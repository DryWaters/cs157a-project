A novel approach to video transition detection based on a two-phase classification strategy LI Shij in LIN Lin and LI Xiaofang Member IEEE School of Computer  Information Engineering Hohai University Nanjing China 210098 Abstract-Video transition detection plays an important role in many tasks of video analysis Aiming at the target application of commercial detection in news video this paper tackles the problem in a unified framework and proposes an alternative classification strategy Our method is made up of two phases In the first stage SVM is employed to classify the transitions into three classes non-transition cut and big-transition In the 
second stage we concentrate on the discrimination of the rapid motion situation and gradual transition which is based on another set of features Apart from the new classification strategy proposed in this paper imbalanced data classification issue is also taken into consideration which has not received sufficient attention by previous work Experimental results show that our method is effective I INTRODUCTION Video transition detection plays an important role in many tasks of video analysis such as commercial detection in news video 1 and highlight extraction from soccer videos 2 Shot transitions are generally classified into two types abrupt cut and gradual fade in/out 
dissolve wipe etc Many methods have been put forward to conduct video transition detection but it is still not completely resolved 3 The existing literature consists of two categories threshold based methods and machine learning based ones Since threshold tuning is very crucial to the success of an algorithm common practitioners and engineers can hardly replicate a practical system based on a published paper Recently several authors have proposed to detect video transitions by using machine learning methods such as Hidden Markov models 4 and support vector machines 5-8 Zhang et al 4 proposed an HMM based shot detection method 
which constructed an HMM for each kind of video transition Ling et al 5 proposed a new method to gradual transition detection based on support vector machines and the extracted features were based on variance projection function of video frames Ren and Singh 6 employed k-nearest neighbor classifier and neural netwoks to predict video transitions In their algorithm a set of 18 features was computed such as block coefficient and cell coefficient Cao and Cai 7 put forward a robust shot transition detection algorithm based on support vector machine in compressed domain which just formed the feature vector from all frames within a 
temporal window each frame represented by six features in compressed domain High accuracy of this algorithm has been reported however it is constrained by the compression method and the encoding quality of the video Ngo 8 proposed to detect dissolves using texture features from temporal slices The above machine leaning-based methods mainly put emphasis on gradual detection while cut detection is still based on some thresholds Aiming at the target application of commercial detection in news video this paper tackles the problem in a unified framework and proposes an alternative classification strategy Our approach is made up of two phases 
In the first stage SVM is employed to classify the transitions into three classes non-transition cut gradual transition and rapid motion The underlying reason is that cut implies an abrupt change of frame similarity non-transition corresponds to nearly no change of the frame content while the third class includes gradual transition and rapid motion which reflect larger dissimilarity between successive frames and thus attract many research efforts In the second stage we concentrate on the discrimination of the rapid motion situation and gradual transition which uses another set of features Apart from the new classification strategy proposed in this paper 
imbalanced data classification issue is also taken into consideration which has not received sufficient attention by previous work The rest of the paper is organized as follows Section II describes the different set of features employed in the two stage classifiers Section III gives out the details of classifiers designing procedures especially the imbalanced data distribution issue in the classification Section IV presents the experimental results and conclusions are drawn in section V respectively II FEATURE EXTRACTION There have been many features put forward by different authors to detect video transitions such as histograms color moments motion vectors and temporal 
slices or visual rhythms In our system we choose different set of features in the different classification procedure A Localfeatures and globalfeatures In the first stage of classification we want to partition the frames in the video into three classes non-transition cut and large-transition which includes rapid motion and gradual transitions The reason for this partition is that it is difficult to separate gradual transition from significant motion when detecting various transitions in videos For this goal there are many features to choose We want to choose those features that are easy to extract and fast to compute 1343 


In recent years visual rhythms have attracted many attentions from different groups 8 9 Since it models the transitions just using the centre lines in horizontal direction and vertical direction the computational cost is very low Furthermore it can catch the local transition details between successive frames Previous work has proved it is efficient to detect cut and wipe 9 A visual rhythm of a video segment is the concatenation of the middle-line of the frames in horizontal or vertical direction Fig 1 shows a horizontal visual rhythm of a piece of news program Ref 9 proposed a string matching based method to detect video transitions which use the longest common subsequence LCSS of two consecutive lines as the similarity measures of two frames They reported good detection results on some test videos Fig 1 A horizontal visual rhythm of a y 1 2 3 C Normalization of the feature series After having chosen the feature sets we extract the feature series in a sliding window whose length is denoted by 2w+1 The parameter w reflects the shortest video transition that can be detected In this paper we set it to 3 For each transition type we want to identify its characteristic patterns in the time-series of video frame features corresponding to the model of the transition type In each sliding window for the first stage of classification we extract the LCSS of the horizontal video slice and LCSS of the vertical video slice and the histogram similarity between successive frames To reduce the influence of absolute values of different features we normalize the feature series using their mean in the sliding window respectively And to differentiate still frames such as anchor man in the studio from rapid motion or gradual transitions we keep the mean as another element of feature in the end feature vector Fig 2 Illustration of piece of news program However like other features visual rhythms are also sensitive to motion and lighting especially significant motion of cameras and objects in the scene Due to the high false positive rates caused by visual rhythms we decide to add histogram similarity to the feature set Histogram similarity of consecutive frames within one shot shows little difference and it is more insensitive to object motion with a constant background 3 It reflects the global visual similarity of the frames which is complementary to the local similarity of visual rhythms So in the first stage of the classifier we use the combination of histogram similarity and LCSS of visual rhythms as the frame features B Block color moments After the first stage classification the remaining problem is to separate gradual transitions from rapid or significant motion There are many research work dedicated to tackle this problem Since our target application is commercial detection in news video we restrict the gradual transition to fade in/out and dissolve 10 11 found that during the gradual transition the mean and variance of consecutive frames looked like parabolic curves But when there were motions in the scene this phenomenon was not so obvious As in 11 we adopt three primary color moments as the frame feature to classify gradual transitions and rapid motion Though the extracted features are the same we resort to the machine learning approach instead of the heuristic rules in 11 which need to tune many thresholds and design many rules We use the first three moments i.e mean standard deviation and skew The following is the computing equations M\(c  I X f\(x,y,c S\(C 2  f\(X y C M\(C K\(c  I L I[f x y c M\(c NX positions of three sub-regions in the frame During the second stage of classification we extract the color moment feature in three sub-regions in the frame and concatenate the features of each frame in the sliding window into a feature series Fig 2 illustrates the positions of the three sub-regions The rational for the selected sub-region is that in the news program there is always a scrolling caption bar in the bottom of the frame and the station logo at the top To 1344 


reduce the influence of the scrolling text and the logos we just compute the color moments in the three sub-regions in the middle part of the frame III THE CLASSIFIER A The overall classification procedure The target application is commercial detection in news video and news story parsing And three main types of transitions are considered in this paper cut fade and dissolve News videos contain a variety of shots ranging from often fast changing and intense-motion commercials to relatively static studio and anchor man shots Fig 3 The procedure of the two-phase classification scheme for video transition detection In this paper we propose a two-stage classification scheme to detect various kinds of video transitions In contrast to classify the frames into cut gradual transition and non-transition we conduct the classification in two phases In the first stage we just classify the frames into cut non-transition and big-transition which include gradual transition and rapid motion The rational behind this scheme is that during the rapid motion and gradual transition the LCSS feature series and histogram similarity series behaves the same which can not give satisfactory discrimination capability Many of the previous work pay much attention to this problem which strives to keep a balance between reducing the number of false positives and improving the detection accuracy of real gradual transitions And in the second stage we employ color moments feature series to separate gradual transitions from rapid motion situations The overall procedure is illustrated in Fig 3 The adopted classifier is SVM as it is one of the most competitive classifiers for small samples problem Since it is very common nowadays we omit the principles in the text and use the LibSVM[12 implementation For the details the interested readers can see the Ref 13 B The imbalanced data classification issue In the real video produced by the broadcast company there are usually not so many cuts or gradual transitions while the non-transition frames are in the dominant Therefore in the machine learning process this will result in the imbalanced data classification issue 14 Previous work on video transition detection based on machine learning methods[5-7 did not take it into consideration in which training samples are selected in a manual way This is not so easy for novice practitioners in this field In Ref 10 Lienhart tackled this problem by using a gradual transition synthesizer which produced thousands of samples of different kinds of video transition However the synthesized samples are not in real programs and there are not so many transition styles in news program As this issue is concerned we adopt another scheme which reduces the non-transition samples by clustering In order to balance the positive and negative samples re-sampling is a common approach 14 which includes under-sampling the majority instances and over-sampling the minority samples If we over-sampling the cut or gradual transitions the likelihood of over-fitting is increased since it makes exact copies of the examples 14 On the other hand the major drawback of random under-sampling is that some useful information for classification is discarded when we throw away many valid instances According to Chawla et al 14 it was found that under-sampling of majority classes was better than over-sampling with replication of minority classes Hence we adopt the under-sampling method to tackle the imbalanced data distribution issue in video transition detection However sequential clustering is utilized instead of random under-sampling which can preserve more useful information for classification Since there are few video transitions in the video we can reduce the sample size of the non-transition class and big-transition class in the first stage of classification and that of rapid-motion class in the second stage of classification In this paper we employ the sequential clustering approach 15 16 The main idea of the clustering algorithm is as follows The training samples are fed sequentially For any new sample if the nearest distance to the centers of the existing cluster centers exceeds a predefined threshold a new cluster is created otherwise it belongs to the cluster whose center is closest to it In this algorithm a threshold needs to be set beforehand In this paper we get it based on the number of the resulting clusters which should be approximately balanced with the peer classes in the classification of different phases More details about the tuning of this threshold are given in Section IV IV EXPERIMENTAL RESULTS To validate our method we conduct several groups of experiments Since our target application is news video parsing and commercial detection in news video we collect tens of clips of news video and commercial segments The 1345 


2006 ACM Press New York NY 201-210 1346 main sources of these clips are from the real programs in our local TV stations We collect 3 full-length news programs broadcasted in 3 different days each of which lasts about 50 minutes In total there are 29 clips of news story and 12 clips of commercials in our experiments among which 19 clips of news and 8 clips of commercials are in the training set while the rest is in the test set Table I shows the detailed statistics of the video materials in our experiments Before reporting the experimental results the evaluation measures should be set In the traditional video retrieval recall and precision are two commonly used measures 5-7 precision gives the measure of correctness of the classifier in detecting the actual video transitions whereas recall gives the measure of the percentage of transitions detected correctly However as mentioned above the video transition detection problem is skewed Another measure is also usually adopted to evaluate the algorithm which is the f-value metric 14 7 f value  1 2  X precision x recall x2 X recall  precision Where 3 corresponds to the relative importance of precision vs recall and in this paper it is set to 1 Three groups of experiments are conducted Firstly we conduct the experiments without under-sampling The SVM is directly trained on the whole set of samples Then we conduct the experiments with two under-sampling schemes one is random sampling and the other is the proposed clustering-based under-sampling As the thresholds in the clustering algorithm we determine them by the resulting clusters and choose the one with the number of resulting clusters is approximately equal to the number of samples from peer classes In the experiments the threshold in the first stage is set to 0.45 and 0.2 in the second stage There are originally 1100 cuts and 343 gradual transitions in the training set while the number of non-transitions reaches 150000 After clustering the number of non-transitions reduces to about 1200 and 1000 for big-transition In the second stage the number of rapid motion is just about 600 Table II shows the detection results of our algorithm The detection result of cuts is from the first stage classification and the result of gradual transition detection is from the second stage The classification result of big-transition in the first stage is input to the clustering algorithm in second stage TABLE I DETAILED ACM international Conference on Multimedia 2006 Segmentation categorization and identification of commercial clips from TV streams using multimodal analysis In Proceedings of the 14th Annual Wang Y Zheng et al STATISTICS OF THE VIDEO MATERIALS IN OUR EXPERIMENTS  of News clips  of Commercial clips cuts Gradual transitions Training set 19 135000 frms 8 23000 frms 1100 343 Test set 10 56000 frms 4 1000 frms 702 161 TABLE II THE DETECTION RESULTS OF THREE GROUPS EXPERIMENTS Different methods Cut Gradual recall precision f-value recall Precision f-value No sampling 73.1 33.9 46.3 55.9 22.1 31.7 Random sampling 80.3 55.0 65.3 67.7 31.8 43.3 Our method 98.5 92.9 95.6 89.4 87.3 88.3 From Table II it can be observed that the proposed method is effective in detecting cuts and gradual transitions in news video Compared with existing work 5-7 our results are competitive and the proposed algorithm is very easy to implement for novice practitioners who can avoid the difficulties of manually selection of training samples and tedious procedures of parameter tuning V.CONCLUSION This paper has proposed a novel approach to detect video transitions in news video which adopts a sequential classification scheme The main contributions lie in two aspects Firstly a new classification strategy is put forward to tackle the problem of discriminating gradual transitions from rapid motion cases And secondly the imbalanced data classification issue is taken into consideration which is explicitly dealt with in machine learning-based methods for the first time Compared with the previous methods ours can keep the practitioners away from the difficulties of manually selection of training samples and tedious procedures of parameter tuning while its accuracy can meet the needs of commercial detection in news programs In future work we hope to investigate more elaborate method to set the clustering thresholds and evaluate other sampling methods for imbalanced data distribution in video analysis ACKNOWLEDGMENT The authors would like to thank Mr Zhang Haiyong from Jiangsu Broadcast Corporation China for his help providing us with the videos used in this paper REFERENCES 1 L Duan,J 


2 Fan and M Tekalp 21 2005 Mingmin Chi Jianping Support Cluster Machine Proceedings of the 24th international conference on Machine learning ICML'07 pp.505-512 Corvallis on Multimedia on Support Vector Machines for Oregon USA 2007 16 S Theodoridis Ren S Singh Using Machine support vector machine in compressed domain Pattern Recogn Lett 28 12 Sep 2007 285-292 7 J Cao and 5599-5604 2005 6 W to detect video transitions Pattern Anal Appl 10 N J Leite 2007 Using string matching A Cai A robust shot transition detection method based on support vector machine In Proceedings of the Eleventh Chih-Chung Chang and Chih-Jen Lin LIBSVM Pattern Recognition 2nd edition Academic Press 2003 1347 transiton detection 2001 4315,pp.219-230 11 Volume 3 R Lienhart Reliable dissolve detection SPIE storage and retrieval for media databases N V Chawla L 0 Hall and A Joshi NY 24-33 15 Bin Li F N Bezerra and 796-807 3 2006 IEEE Computer Society Washington DC 593-596 5 Jian Ling Yi-Qun Lian ICMLC 2005 IEEE Computer Society pp Automatic Video Shot Boundary Detection 1534-1540 8 45-54 10 Pattern Recognition Data Mining and Knowledge Discovery vol 2 no.2 pp C.J.C Burges A Tutorial ACM international Conference NY 2003 Image Processing 2003 12\(7 Mining Chicago Illinois on innovative Computing information and Control August 21 A Hanjalic Shot-boundary detection unraveled and resolved IEEE Trans Circuits Syst Video Techn 12\(2 90-105 2002 4 W Zhang J 121-167 1998 14 Learning IDEAL 2004 C Ngo A robust dissolve detector by Automatic soccer video analysis and R Mehrotra J Z Miadowicz Story Tracking in Video News Broadcasts Ph.D dissertation University of Kansas 2004  12 A Ekin A summarization IEEE Transactions on a library for support vector machines K Koutroumbas Yue-Ting Zhuang A new method for shot gradual using support vector machine In Proceedings of 2001 Software available at http www.csie.ntu.edu.tw cjlin/libsvm 1 3 Xiangyang Xue Wrapper-based computation and evaluation of sampling methods for imbalanced datasets In Proceedings of the 1st international Workshop on Utility-Based Data Lin X Chen et al Video Shot Detection Using Hidden Markov Models with Complementary Features In Proceedings of the First international Conference 283-286 9 1 Jan 2007 ACM Press New York UBDM 05 ACM Press New York 


Lucien Wald 8 has developed terms of reference in data fusion that can be repurposed toward various fusion ontologies Wald derives most of his terms from the remote sensing and defense system applications but we will demonstrate their extension to other fields through domain goal-based and mathematical reasoning The ObjectProcess Diagram OPD is an explicit representation of the semantic decomposition of these terms for generalized fusion Its overarching premise is the treatment and origins of raw information which can take several forms from measurement and signal to observations and verbal reports The relational links that constitute the OPD is given in Figure 3-2 Figure 3-2 Relational links in OPD 0 sa is the same as A code surrogate address of symbol for Decomposes to aggregates to i.k is characterized by exhibits A eSpecializes to generalizes to i instantiated to belongs to the class of Although fusion is the central process in Figure 3-3 it is helpful to include aspects of the supporting system as well as supporting processes which act on the information It is worth noting that there are many types of raw information which can be refined over various processes and environments The hardware elements of the supporting system have been simplified to represent a simple sensor arrangement In reality complex fusion systems are comprised of multiple distributed platforms with thousands of apertures and waveforms Similarly there are many more supporting processes which enable data fusion these hierarchical classification schemes will be discussed later Figure 3-3 Object-Process Diagram for Information Fusion Parallelism and other Taxonomies Rule-based analytics in knowledge-based systems KBS have benefited tremendously from the advent of parallel computers For instance the rules highlighted in Figure 34 under supporting processes can be subjected to rule-level partitioning where classes such as correlation combination or association rules are generated Class-based rule partitioning forms another aspect of ontology development namely one based on fusion algorithm Whereas the Object-Process Diagram above took an information-based approach to ontology design the class methods utilize a hierarchical scheme which imparts a specific decompositional logic or organization Hall develops taxonomy for level 1 processing that is extensible to other levels of fusion This is derived from algorithmsboth statistical and heuristicwhich are based on level 1 fusion object identification Level 1 is specialized into 3 classes positional identity and ancillary support algorithms These are further decomposed by technique or method Positional fusion for instance involves parametric association and estimation of data Identity fusion uses physical models cognitive models and feature-based inference Ancillary support systems which often comprise more than 80 of the fusion effort include numerical libraries data alignment preprocessing database management and man/machine interfaces These are only one instance of a larger class of items 6 


tes ong the many challenges in data fusion there uses objects and relationship links The following taxonomy of identity-classification algorithms was developed by Waltz and Llinas 9 Figure 3-4 Class-Based Ontology for Identity Classification Algorithms Genenc Class vehicle Particular Class ship Figure 3-5 Semantic Network Example ATTRIBUTE-OF PARTICULAR PARTICULAR PROPERTY VALUE Diana's Speed 10 Knots Information-theoretic techniniiles Parametric templat Cluster algorithms Voting fusion Entropy methods Finally parallelism on whether any given entitysignals physical objects aggregates on the assemblage of components whose interrelations on a very different representation scheme Objects as in OPM but takes as further attribute data is generated Figure 3-5 is the layout of connectionist architecture from MacRae's research The semantic network example below illustrates the potential of ontology-based fusion for the Semantic web Like the wireless reconfigurable hardware platform developed at Dartmouth see Figure 2-2\(a use of connectionist parallel architectures for real-time data fusion applications for the Royal Navy Their architecture source providers to end sensor networks to be dynamically discovered composed and integrated with distributed fusion services to support sensor data through the Semantic Web value chain from can enable Diana Fuzzy Set Theorv an indeterminate number of entities depending an m-place relation m>n xI xn an important role in developing ontology for semantic networks MacRae and Byrne 9 studied the can play or behavior The second domain is based are such situations This most closely relates to Level 2 data fusion concerning the implications context Cognitive models for situation assessment are of interest in its attributes characteristics are a location and time which may be points are of interest where the targets themselves a key facet to support rapid propagation of a reflection of this evolving requirement for cognitive models The first domain is based users 4 SYSTEM ENGINEERING FOR DATA FUSION can be addressed by the systems framework hlighted above The first is the basis for selection of mathematical techniques This can have are entities h and k process must take into account the perspectives of various system stakeholders including the user numerical analyst operations researcher and system engineer The specific algorithms available include association methods positional estimation Kalman filtering identity fusion templating voting D-S methods classical Bayesian inference and pattern recognition techniques adaptive neural nets and cluster methods 2 Another challenge is the provisioning of data are two ch are identified by nodes and connected via links of inheritance Sensor data is processed at local active node centers and then propagated along to parallel nodes a web-based semantic network new and challenging missions An easily evolvable ontology will be a priori Increasingly this requires the need for experiential models of cognition and situational assessment in order for the fusion system to provide accurate and meaningful results The partitioning of data fusion into two problem domains is or structureson the reasoning of the agents According to Situation Theory abstract situations or infons are represented by the form P,x I Xn h k p Where P is or an extended region 7 Particular Ship HMS 


p is a polarity or truth-value Real and abstract situations can be distinguished by their polarity A real situation is a set of facts with polarity equal to 1 10 Figure 4-1 illustrates Bowman's cognitive model for a perceptual reasoning machine PRM based on the concept of reinforcement learning Figure 4-1 Perceptual Reasoning Machine PRM for Cognition  Situational Awareness Reinforcement Perceptual Reasoning Cycle each sensor's signal is specific to the transducer design Since discrimination among targets occurs locally before data entry it reduces the load on the fusion processor 2 The next step in concept selection is known as the downselect phase During down-select potential applications are screened according to their fit with the JDL processing criteria This method was developed by Stuart Pugh in the 1980's and helps design teams to decide on optimal strategies for their product goals A Pugh concept selection matrix is provided in the Appendix It ranks applications based on the processing requirements of the data Tracker-Correlator Architectures There are currently 3 general synthetic architectures for data processing which exist today centralized decentralized autonomous and hybrid architectures Refer to Figures 42 through 4-4 11 Figure 4-2 Centralized Fusion Architecture A CENTRALIZED FUSION I Decliralonri The advanced combat direction system ACDS is an example of the centralized fusion architecture 9 The system has external interfaces with existing sensors which maintain different reporting protocols and data links Acoustic IFF ESM and Radar send sensor-level tracks and target reports to the track management processor indicated by the dashed line in Figure 4-2 These reports are fed into the ACDS database which contains tactical status intelligence maps doctrine and tracks Each track-to-track association or identification from the arrival of new observations requires retrieval of the data from a central decision support processor not shown The common thread underlying these location-driven architectures is the notion of a progressive data flow from sensor set A B N through detection classification and end-state Under the centralized scheme the preprocessed output is collected into the data alignment  association through coordination transformation Raw data from a multi-target environment is correlated and then transmitted to the central processing facility A variant of this type of fusion is centralized fusion of feature vector data This construct drives ambiguity out of the system by extracting feature vectors from an image The feature-based method is 8 enarS t Constols The closed loop PRM provides situational feedback to a human perceptual system which optimizes the decisionmaking process This is viewed as a meta-level information management system for resource control that continuously updates domain knowledge as it is acquired Process Selection Comparing Fusion Levels The table in the Appendix sets forth criteria that can be used in selecting a fusion level for a given application These characteristics form part of the selection process that can be used in screening for an appropriate concept The table is divided among four levels of data that approximately correspond to the JDL fusion levels The criteria are based on the type of sensory information being generated individual signals images features etc The table goes on to characterize the content of such information The degree and means of registration are also important factors as the spatial position of sensors determines their ultimate footprint and target-detection space Overlapping footprints ensure that time-dependent phenomena such as a target motion are observed by all sensors at the same time The criteria in Table A can then be used to select the type of architecture which factor into the fusion system design i.e central or sensor-level processing The main difference between these two is that the former processes sensor reports directly and in one place requiring a more complex fusion processor This helps to achieve better accuracy when the multi-sensor data is not generated by independent phenomena Sensor-level fusion is preferred when the signatures are independent There is more cueing of sensors with others in the suite where the optimization of NIIyotk*wwufic  im n 


raw data prp"exn PrNbility of Su silful I Declaration This effectively removes the fusion process the track management processor further downstream Rather than performing sequential estimation at the data level as in the centralized case this architecture provides state vector estimation of position and velocity for an object Although they cull from single-source data decentralized systems tend to degrade the output because there is information loss between the sensor and fusion process An example of a decentralized system is a robot navigation system that can autonomously navigate through a continuous state-space through selective switching between sensors actuators and their effectors legs wheels joints and grippers Intelligent agents such as robots might be designed factoring some goal-state into its decentralized architecture The third generic tracker  correlator architecture is a hybrid system Figure 4-4 which combines state-vector and datalevel fusion processes This confers more flexibility than the previous two cases but imposes a switching cost For instance a dense target environment or complex signal propagation may require centralized fusion for a more accurate assessment of identity The availability of sensors may constrain the user to select autonomous fusion any changeover between the two will impose selection and monitoring costs as well as communication requirements which need to be carefully weighed against the expected benefit In the domain of robotic software architectures the hybrid tracker/correlator would be used to combine elements of reactive and deliberate control Reactive control refers to sensor-driven control that may be used for low-level decision making processes whereas deliberate path planning control refers to global executive decisionmaking Both are necessary for the operation of unmanned land or air vehicles as well as planetary rovers 12 centralized fusion locally fused data track file or decentralized approach and raw/preprocessed data hybrid approach The advantages of the three architectures are summarized below 9 Centralized fusion requires high bandwidth buses to pass the high-rate raw data and powerful central processing capability The autonomous approach tailors the track and classification functions to individual sensor outputs at the expense of an accurate position estimate Finally the hybrid approach selectively transitions between the central and autonomous processes as the situation requires Real world architectures are designed to incorporate more than just the tracker-correlator aspect highlighted above which represents only 20 of the software development effort for a fusion system based on lines of code as a metric 9 Other representations exist for the identity component of Level 1 fusion Levels 2 and 3 fusion adopt among others the blackboard approach described in the following section Finally Level 4 fusion models functions pertaining to communication database management human factors and executive control Hierarchical Architectures A blackboard description of an object hierarchy is an excellent model for building semantic knowledge databases and networks Categories are the building blocks of any large scale knowledge representation scheme 12 The blackboard adaptation illustrates the potential precursor to a semantic network for target identification The blackboard approach uses categorical reasoning to relay the notion of inheritance and parallelism The blackboard approach presents L2 Situation or L3 Threat assessments for an air-land battle It confers flexibility to the designer through static and dynamic representation models that are based on independent modular knowledge sources As rule-based expert systems 9 or measurement sets Targt Clasication L PF Trckn  4a1ing and Control Par."ater C mseIfic.tktoUn ndCt T1 TUat makes these three architectures unique Firstly they All Pr Xnt g C&reatSn-an be considered ontologies of function That is these a t three arrangements represent a taxonomy of trackerI Raking  S P v1 Far motorsn 1 correlator functions In the language of OPM the operand Trackio n 1I\2431asseaIln t c=v t  _ in these architectures one of many types of approaches to the object recognition problem In autonomous or decentralized fusion architecture Figure 4-3 individual tracking and classification functions are assigned to each sensor output Figure 4-3 Decentralized Fusion Architecture Figure 4-4 Hybrid Fusion Architecture C BRID FUSION Gullll   c ai&iriimI pll Ii itloi 1 m LL T 6 kcAstful B AUTONOMOUS FUSION Deection Esration 1 Sonsor Contmls are the 


they are a useful construct for exploratory research and incremental development of a problem Finally hierarchical decompositions help deal with the problem of complexity in mission design by reducing the number of activities through progressive levels of detail Objectoriented representations in action decomposition are stored in plan librariesor databasesthat can be accessed to fit the needs of the mission However hierarchical methods such as blackboard architectures do not always decompose problems correctly In artificial intelligence Al the inability to capture everything in a set of logical rules is known as the qualification problem 13 Consequently blackboard architectures can be expensive to build modify and operate 5 APPLICATIONS Command Control Communications Computers and Intelligence Surveillance  Reconnaissance systems C4ISR provide mature highly evolved examples of data fusion but they are not the only systems which make use of data fusion concepts The objective of a C4ISR system is to provide a comprehensive view of the tactical and strategic battle space through a variety of technologyand intelligence-based media The systems are composed of software hardware and human elements which work in concert to support decision-making processes Fulfillment of C4ISR objectives draws heavily on the utilization of sensor data Increasingly fusion design needs to account for the transition from platform-based systems to networkcentric operations Sense  Respond Logistics SRL Sense  Respond is an emerging domain in military and commercial logistics with powerful implications for the fusion paradigm It is a managerial framework that was originally proposed by researchers at IBM 13 based on the dynamic of change in business security and technology It is comprised of a value net of self-synchronizing partnerships which form and dissolve to adapt to demands in the environment Elements of this value net have been evolved specifically the triumvirate known as operational logistics and intelligence command for military and defense scenarios SRL almost mirrors the data fusion process itself by observing patterns detecting issues and performing root-cause analysis Finally data fusion in robotics plays an important role in factory automation processes such as material handling part fabrication and assembly This can appear in the form of cooperative systems dexterous hands and tele-operation for mining and manufacturing Biometrics Biometrics offers interesting case applications of data fusion principles Biometrics are used in the security or identification sectors where the systems draws on an attribute database to make comparisons and declarations about ownership or identity This is characterized by a subset of the processes used in the JDL model of data fusion object identification collection collation and evaluation The final stage of processing uses relatively simple decision rules to either accept or reject the user based on the attribute data on record Space Expeditions The Haughton-Mars expedition to the Canadian Arctic conducted by MIT in 2005 synthesizes some of these and other considerations in a complex data fusion system design Among other objectives the team set out to determine the efficacy of RFID technology for intelligent agent and asset tracking in support of exploration logistics 14 The International Space Station ISS is currently dominated by manual tracking and barcodes so the hypothesis was that automated web-accessible systems could offer potential savings in time and effort for inventory management Though the concept of RFID tagging was simple it led to complications with respect to optimal antenna installation and tracking of liquids and metallic items The system was equipped with both passive battery-run and active tagging leading to interference in the 915 MHz and 2450 MHz bands This implied that the range data could be reliable under more powerful strictly active RF sensors This would of course require the proper level of EMI shielding of the agents containers and ATVs being tracked In a separate part of the experiment the team developed functional Class of Supply COS ontologies to supplement the Cargo Category Allocations Rates Table CCART used by NASA as well as supply classifications used by NATO and the US Military This was borne out of the need to capture all major items for the new space exploration initiative not already listed in the CCART such as categories for propellant fuels and surface transportation vehicles needed for remote science stations Dynamically indexing these remote logistics categories to a generic upper level ontology that is web-enabled could vastly improve the tracking operation The sensor data being fused could help teams to not only make operational adjustments to the expedition for instance regarding ATV usage but also improve the quality and relevance of the experiments For instance an asset library that is hyperlinked to the web could inform agents of similar expeditions with results/findings openly accessible to the participants Semantic web links would ensure that there is no violation in naming conventions as new logistics applications are discovered Connectionist architectures 10 


would enable the Class of Supply and other ontologies to optimally evolve with remote science networks catering to the diversity of exploration missions around the world 6 CONCLUSIONS This work has introduced some of the formal process models for data fusion which have been developed by the remote sensing and defense communities An overview of sensor and sensor platforms was described as they relate to the concepts and challenges of multi-sensor data fusion The role of fusion in ontological engineering was also discussed specifically as it relates to knowledge databases for fusion Ontology reuse can be a major facilitator for creating blackboard architectures for knowledge representation Systems engineering processes can play an important role in structuring and implementing complex fusion designs beginning with concept selection A Pugh Concept Screening Matrix was developed See Appendix which ranked several potential application areas for their adaptability to C4ISR-type fusion The medical diagnostics domain was found to be the most strongly correlated with the JDL data processing criteria The data may be simply gathered through visual observation thermometers etc or processed using sophisticated sensor machines based on nuclear magnetic resonance acoustic imaging and X-ray imaging In either case there is a clear demand for the high-tech application of fusion software algorithms and hardware sensing devices in this expanding and important field Future research in data fusion might address how ObjectProcess Methodology can be used to better understand problems related to concurrency control in localized sensors and distributed sensor networks It may also include an application of the fusion models developed for commercial implementation in vehicle health monitoring marketing science and logistics or supply chain management Additional research is required into designing data fusion test-beds so that some of the tracker-correlator architectures discussed in this thesis can be evaluated and verified for performance Finally research can be undertaken in developing robust ontologies that will enable the benefits of multi-sensor fusion to be more widely accessible through the Semantic Web 7 APPENDIX This Pugh Concept Selection screen is designed to weight application areas against the processes in the JDL implementation model The baseline reference case is the C4ISR system all other selection criteria are comparatively ranked against the reference case  or  The method was developed to help narrow concepts quickly and to improve them in a structured manner As we can see it is an imperfect approachaccording to the ranking the selection matrix above would not have us develop a fusion system for an intelligent transportation system However all indicators point to this being a worthy implementation of data fusion Kobayashi et al 15 proposed the Kalman approach for fusing measurement data from differential GPS wheel speedometer and optical fibre rate gyro And Mirabadi and Schmid 16 evaluated train speed and measurement through a combination of GPS INS and tachometers 11 


Concept Medical diagnostics features highest case The example cited from Luo et al 17 is the case-based data fusion methods used to support clinical decision support The detection and classification process might involve can grasp the concepts and processes that underpin the widest possible range of fusion applications 12  I Selection Criteria C4ISR Remote Medical Intelligent Condition reference Sensing Diagnostics Transportation Monitoring Level 1 Detection 0 0    Orientation 0 0 0  Classification 0    Identification 0     Level 2 Object Aggregation 0    Event  Activity Aggregation 0 0  0 Contextual Interpretation Fusion 0 0 0 Level 3 Capability Estimation 0 0 Prediction of Enemy Intent 0 0 Identification of Threat 0 0   Multi-perspective Assessment 0  0 0  Offensive  Defensive Analysis 0 0 0 Level 4 Evaluations 0     Fusion Control 0 0 0  0 Source Requirements Processing 0  0   Mission Management 0 0 0  0 Sum s 0 7 7 7 8 Sum O's 16 7 8 1 5 Sum s 0 2 1 8 3 Net Score 0 5 6 1 5 Rank 3 2 1 4 2 Continue Yes Yes Yes No Yes error in Pugh methods In the example above error have mainly to do with the bias of the selection criteria to the remote sensing and defense lexicon of data fusion This highlights the importance of developing upper-level ontologies which as a JDL implementation option which sources of means that it is best suited to leverage the algorithms and processes from the C4ISR reference a cardiac event including ventricular and atrial activity The process being controlled referred to automatic rhythm monitoring through integration of electrocardiogram and hemodynamic signals Although it is a quantitative tool for comparing concepts there is inherent subjective 


REFERENCES 1 Steinberg A 2000 Data Fusion System Engineering International Society of Information Fusion 2]Klein L 1999 Sensor and Data Fusion Concepts and Applications 2nd ed SPIE Optical Engineering Press Bellingham WA 3]Gruber T 1993 A Translation Approach to Portable Ontologies Knowledge Acquisition 5\(2 199-220 4]Boury-Brisset A 2003 Ontology-based Approach for Information Fusion International Society of Information Fusion 5 Martins J and Pinto H 2004 Ontologies How can they be built Knowledge and Information Systems Springer-Verlag London 14 de Weck 0 and Simchi-Levi D 2006 HaughtonMars Project Expedition 2005 Interplanetary Supply Chain Management and Logistics Architectures MITINASA Exploration Systems Mission Directorate and Research  Technology Program NASA/TP-2006214196 15 Kobayashi K Munekata F and Watanabe K 1994 Accurate navigation via differential GPS and vehicle local sensors in Proc IEEE Int Conf Multi sensor Fusion Integration Intell Syst pp 9-16 16 Mirabadi A Mort N and Schmid F 1996 Application of sensor fusion to railway systems in Proc IEEE/SICE/RSJ Int Conf Multi sensor Fusion Integration Intell Syst pp 185-192 17 Luo R Chih-Chen Y and Su K 2002 Multi-sensor Fusion and Integration Approaches and Future Research Directions IEEE Sensors Journal Vol 2 No 2 pp 102119 6 Van Heist G Schreiber A and Wielinga B 1997 Using explicit ontologies in KBS development Int J Hum Computer Stud 46\(2/3 pp 183-292 7 Guarino N 1998 Formal ontology and information systems In Guarino N ed Formal ontology in information systems IOS Press Amsterdam pp.3-15 8 Wald L 1999 Some Terms of Reference in Data Fusion IEEE Transactions on Geoscience and Remote Sensing Vol 37 No 3 9 Waltz E and Llinas J 1990 Multi sensor Data Fusion Artech House Norwood MIA 10 Bowman C 2004 The Dual Node Network DNN Data Fusion  Resource Management DF&RM Architecture AIAA Intelligent Systems Conference Chicago 11 Hall D and Llinas J 1997 An Introduction to Multisensor Data Fusion IEEE Proceedings Vol 85 No 1 12 Russell S and P Norvig 2003 Artificial Intelligence A Modern Approach 2nd ed Pearson Education Inc New Jersey 13 Lin G et al 2005 Transforming the military through sense and respond IBM Business Consulting Services White Paper IBM Global Services Somers NY BIOGRAPHY Atif Mirza is a Senior Consultant with Booz Allen Hamilton He specializes in corporate strategy and operations advisory for clients in the Global Aerospace and Defense Practice Prior to joining Booz Allen Mr Mirza was a graduate student at MIT where he was a System Design and Management Fellow in the Engineering Systems Division He holds a joint SM degree in Engineering and Management from MIT/Sloan and a BEng with Honors in Mechanical Engineering from The University of Edinburgh Scotland 13 


Time 50 350   10 1                   10 0                   10 1  14         300 0.005  s D 2 k,8 Velocity   100 150 200 250 10 2                    0.5 50  Figure 13 \226 Position consistency, filter tuning example 5  C ONCLUSION  Calculating and observing the behavior of covariance consistency at different levels  in the radar signal processing chain represents a very powerfu l tool that can be used to assess the accuracy and softwa re implementation of radar signal-processing algorithms.  Analyzing covariance consistency is applicable to radar systems both in the field and in simulations.  The primary challenge in both arenas comes down to properly accounting for the true target states that contribute to detections, detection primitives measurements, and state estimates For a fielded radar syst em, achieving covariance consistency is usually a s econdary consideration behind achieving and maintaining track s.  Indeed, until recently radar specifications did not even include requirements for covariance consistency.  Recent covariance consistency requirements stem from the fact that the use of radar systems in sensor netting applications is on the rise Currently the combined e ffects of off-beam-center measurements, atmospheric correction, bias correction clustering and centrioding, data association, and filtering on state covariance consistency throughout a target\222s trajectory are not well known.  This is particularly true for radars using wideband waveforms and multiple hypotheses or multiple frame trackers.  Numerical results presented here indicate that algorithms early in the radar signal processing chain can significantly degrad e covariance consistency and that some errors are better tolerated than others For a simulated target in a modeled system, truth relative to some global reference is known. However, transforming truth through different refere nce frames and accounting for changes that occur during various radar processing algorithms is not as simple as it appears.  The techniques in this paper help expose this hidden complexity and provide a framework for discussing and expanding the future development of covariance consistency techniques.  Such future developments include issues related to mapping truth through the convolution operation typically used to simulate wideband signal processing and the fast Fourier transforms typically used in pulse-Doppler processing.  Sophisticated tracking algorithms that carry multiple hypotheses, associate across multiple frames, or weight the association of multiple targets within a single frame pose significant challenges in properly associating truth with state estimates.  Additional work, including an investigation of track-to-truth assignment, is needed before covariance consistency techniques can be applied to these algorithms Another area that needs furthe r analysis is the use of a sliding window to approximate the covariance behavior expected during a set of Monte-Carlo trials.  Various timedependent variables such as the target\222s range and orientation, the transmit waveform, the radar\222s antenna patterns toward the target, missed detections, and false alarms could easily viol ate the assumption that measurement conditions are nearly stationary over the time of the window.  It is importa nt to understand the conditions when this assumption is violated Finally, the examples presented here included a relatively benign arrangement of targets.  Further analysis in dense target environments with the related increase in merged detections, merged measurements, and impure tracks is needed.  Further analysis for targets traveling over different trajectories is also needed Even so, the techniques presented here can be extended to many of these analyses R EFERENCES  1  S. Blackman and R. Popoli Design and Analysis of Modern Tracking Systems Artech House, 1999 2  Y. Bar-Shalom and X. R. Li Multitarget-Multisensor Tracking: Principles and  Techniques YBS Publishing, Storrs, CT, 1995 3  Y. Bar-Shalom, Editor Multi-target-Multi-sensor Tracking: Advanced Applications and  Vol. I Artech House, Norwood, MA, 1990 4  D. B. Reid, \223An Algorithm for Tracking Multiple Targets,\224 IEEE Trans. on Automatic Control Vol. 24 pp. 843-854, December 1979 5  T. Kurien, \223Issues in the Design of Practical Multitarget Tracking Algorithms,\224 in Multitarget-Multisensor Tracking Y. Bar-Shalom \(ed.\43-83, Artech House, 1990 6  R.P.S. Mahler, Statistical Multisource-Multitarget Information Fusion, Artech House, 2007 


 15 7  B.-N. Vo and W.-K. Ma, \223The Gaussian Mixture Probability Hypothesis Density Filter,\224 IEEE Trans Signal Processing Vol. 54, pp. 4091-4104, November 2006 8  B. Ristic, S. Arulampalam, and N. Gordon Beyond the Kalman Filter Artech House, 2004 9  Y. Bar-Shalom, X. Rong Li, and T. Kirubarajan Estimation with Applications to Tracking and Navigation, New York: John Wiley & Sons, pg. 166 2001 10  X. R. Li, Z. Zhao, and V. P. Jilkov, \223Estimator\222s Credibility and Its Measures,\224 Proc. IFAC 15th World Congress Barcelona, Spain, July 2002 11  M. Mallick and S. Arulampalam, \223Comparison of Nonlinear Filtering Algorithms in Ground Moving Target Indicator \(GMTI Proc Signal and Data Processing of Small Targets San Diego, CA, August 4-7, 2003 12  M. Skolnik, Radar Handbook, New York: McGrawHill, 1990 13  A. Gelb, Editor Applied Optimal Estimation The MIT Press, 1974 14  B. D. O. Anderson and J. B. Moore Optimal Filtering  Prentice Hall, 1979 15  A. B. Poore, \223Multidimensional assignment formulation of data ass ociation problems arising from multitarget and multisensor tracking,\224 Computational Optimization and Applications Vol. 3, pp. 27\22657 1994 16  A. B. Poore and R. Robertson, \223A New multidimensional data association algorithm for multisensor-multitarget tracking,\224 Proc. SPIE, Signal and Data Processing of Small Targets Vol. 2561,  p 448-459, Oliver E. Drummond; Ed., Sep. 1995 17  K. R. Pattipati, T. Kirubarajan, and R. L. Popp, \223Survey of assignment techniques for multitarget tracking,\224 Proc  on Workshop on Estimation  Tracking, and Fusion: A Tribute to Yaakov Bar-Shalom Monterey CA, May 17, 2001 18  P. Burns, W.D. Blair, \223Multiple Hypothesis Tracker in the BMD Benchmark Simulation,\224 Proceedings of the 2004 Multitarget Tracking ONR Workshop, June 2004 19  H. Hotelling, \223The generalization of Student's ratio,\224 Ann. Math. Statist., Vol. 2, pp 360\226378, 1931 20  Blair, W. D., and Brandt-Pearce, M., \223Monopulse DOA Estimation for Two Unresolved Rayleigh Targets,\224 IEEE Transactions Aerospace Electronic Systems  Vol. AES-37, No. 2, April 2001, pp. 452-469 21  H. A. P.  Blom, and Y. Bar-Shalom, The Interacting Multiple Model algorithm for systems with Markovian switching coefficients IEEE Transactions on Au tomatic Control 33\(8  780-783, August, 1988 22  M. Kendall, A. Stuart, and J. K. Ord, The Advanced Theory of Statistics, Vol. 3, 4th Edition, New York Macmillan Publishing, pg. 290, 1983 23  T.M. Cover and P.E. Hart, Nearest Neighbor Pattern Classification, IEEE Trans. on Inf. Theory, Volume IT-13\(1 24  C.D. Papanicolopoulos, W.D. Blair, D.L. Sherman, M Brandt-Pearce, Use of a Rician Distribution for Modeling Aspect-Dependent RCS Amplitude and Scintillation Proc. IEEE Radar Conf 2007 25  W.D. Blair and M. Brandt-Pearce, Detection of multiple unresolved Rayleigh targets using quadrature monopulse measurements, Proc. 28th IEEE SSST March 1996, pp. 285-289 26  W.D. Blair and M. Brandt-Pearce, Monopulse Processing For Tracking Unresolved Targets NSWCDD/TR-97/167, Sept., 1997 27  W.D. Blair and M. Brandt-Pearce, Statistical Description of Monopulse Parameters for Tracking Rayleigh Targets  IEEE AES Transactions, Vol. 34 Issue 2,  April 1998, pp. 597-611 28  Jonker and Volgenant, A Shortest Augmenting Path Algorithm for Dense and Sparse Linear Assignment Problems, Computing, Vol. 38, 1987, pp. 325-340 29  V. Jain, L.M. Ehrman, and W.D. Blair, Estimating the DOA mean and variance of o ff-boresight targets using monopulse radar, IEEE Thirty-Eighth SSST Proceedings, 5-7 March 2006, pp. 85-88 30  Y. Bar-Shalom, T. Kirubarajan, and C. Gokberk 223Tracking with Classification-Aided Multiframe Data Association,\224 IEEE Trans. on Aerospace and Electronics Systems Vol. 41, pp. 868-878, July, 2005   


 16 B IOGRAPHY  Andy Register earned BS, MS, and Ph  D. degrees in Electrical Engineering from the Georgia Institute of Technology.  His doctoral research emphasized the simulation and realtime control of nonminimum phase mechanical systems.  Dr. Register has approximately 20 years of experience in R&D with his current employer, Georgia Tech, and product development at two early-phase startups. Dr. Register\222s work has been published in journals and conf erence proceedings relative to mechanical vibration, robotics, computer architecture programming techniques, and radar tracking.  More recently Dr. Register has b een developing advanced radar tracking algorithms and a software architecture for the MATLAB target-tracking benchmark.  This work led to the 2007 publication of his first book, \223A Guide to MATLAB Object Oriented Programming.\224  Mahendra Mallick is a Principal Research Scientist at the Georgia Tech Research Institute \(GTRI\. He has over 27 years of professional experience with employments at GTRI \(2008present\, Science Applications International Corporation \(SAIC Chief Scientist \(2007-2008\, Toyon Research Corporation, Chief Scientist 2005-2007\, Lockheed Martin ORINCON, Chief Scientist 2003-2005\, ALPHATECH Inc., Senior Research Scientist 1996-2002\, TASC, Principal MTS \(1985-96\, and Computer Sciences Corporation, MTS \(1981-85 Currently, he is working on multi-sensor and multi-target tracking and classification bas ed on multiple-hypothesis tracking, track-to-track association and fusion, distributed filtering and tracking, advanced nonlinear filtering algorithms, and track-before-detect \(TBD\ algorithms He received a Ph.D. degree in  Quantum Solid State Theory from the State University of New York at Albany in 1981 His graduate research was also based on Quantum Chemistry and Quantum Biophysics of large biological molecules. In 1987, he received an MS degree in Computer Science from the John Hopkins University He is a senior member of the IEEE and Associate Editor-inchief  of the Journal of Advances in Information Fusion of the International Society of Information Fusion \(ISIF\. He has organized and chaired special and regular sessions on target tracking and classific ation at the 2002, 2003, 2004 2006, 2007, and 2008 ISIF conferences. He was the chair of the International Program Committee and an invited speaker at the International Colloquium on Information Fusion \(ICIF '2007\, Xi\222an, China. He is a reviewer for the IEEE Transactions on Aerospa ce and Electronics Systems IEEE Transactions on Signal Pr ocessing, International Society of Information Fusion, IEEE Conference on Decision and Control, IEEE Radar Conference, IEEE Transactions on Systems, Man and Cybernetics, American Control Conference, European Signal Processing Journal and International Colloquium on Information Fusion ICIF '2007   William Dale Blair is a Principal Research Engineer at the Georgia Tech Research Institute in Atlanta, GA. He received the BS and MS degrees in electrical engineering from Tennessee Technological University in 1985 and 1987, and the Ph.D. degree in electrical engineering from the University of Virginia in 1998. From 1987 to 1990, he was with the Naval System Division of FMC Corporation in Dahlgren, Virginia. From 1990 to 1997, Dr Blair was with the Naval Surface Warfare Center, Dahlgren Division NSWCDD\ in Dahlgren, Virg inia. At NSWCDD, Dr Blair directed a real-time experiment that demonstrated that modern tracking algorithms can be used to improve the efficiency of phased array radars. Dr Blair is internationally recognized for conceptualizing and developing benchmarks for co mparison and evaluation of target tracking algorithms Dr Blair developed NSWC Tracking Benchmarks I and II and originated ONR/NSWC Tracking Benchmarks III and IV NSWC Tracking Benchmark II has been used in the United Kingdom France, Italy, and throughout the United States, and the results of the benchmark have been presented in numerous conference and journal articles. He joined the Georgia Institute of Technology as a Se nior Research Engineer in 1997 and was promoted to Principal Research Engineer in 2000. Dr Blair is co-editor of the Multitarg et-Multisensor Tracking: Applications and Advances III. He has coauthored 22 refereed journal articles, 16 refereed conference papers, 67 papers and reports, and two book chapters. Dr Blair's research interest include radar signal processing and control, resource allocation for multifunction radars, multisen sor resource allocation tracking maneuvering targets and multisensor integration and data fusion. His research at the University of Virginia involved monopulse tracking of unresolved targets. Dr Blair is the developer and coordinator of the short course Target Tracking in Sensor Systems for the Distance Learning and Professional Education Departmen t at the Georgia Institute of Technology. Recognition of Dr Blair as a technical expert has lead to his election to Fellow of the IEEE, his selection as the 2001 IEEE Y oung Radar Engineer of the Year, appointments of Editor for Radar Systems, Editor-InChief of the IEEE Transactions on Aerospace and Electronic Systems \(AES\, and Editor-in- Chief of the Journal for Advances in Information Fusion, and election to the Board of Governors of the IEEE AES Society,19982003, 2005-2007, and Board of Directors of the International Society of Information Fusion   


 17 Chris Burton received an Associate degree in electronic systems technology from the Community College of the Air force in 1984 and a BS in Electrical Engineering Technology from Northeastern University in 1983.  Prior to coming to the Georgia Institute of Technology \(GTRI\ in 2003, Chris was a BMEWS Radar hardware manager for the US Air Force and at MITRE and Xontech he was responsible for radar performance analysis of PAVE PAWS, BMEWS and PARCS UHF radar systems Chris is an accomplished radar-systems analyst familiar with all hardware and software aspects of missile-tracking radar systems with special expertise related to radar cueing/acquisition/tracking for ballistic missile defense ionospheric effects on UHF radar calibration and track accuracy, radar-to-radar handover, and the effects of enhanced PRF on radar tracking accuracy.  At GTRI, Chris is responsible for detailed analysis of ground-test and flight-test data and can be credited with improving radar calibration, energy management, track management, and atmospheric-effects compensation of Ballistic Missile Defense System radars   Paul D. Burns received his Bachelor of Science and Masters of Science in Electrical Engineering at Auburn University in 1992 and 1995 respectively. His Master\222s thesis research explored the utilization of cyclostationary statistics for performing phased array blind adaptive beamforming From 1995 to 2000 he was employed at Dynetics, Inc where he performed research and analysis in a wide variety of military radar applications, from air-to-air and air-toground pulse Doppler radar to large-scale, high power aperture ground based phased array radar, including in electronic attack and protection measures. Subsequently, he spent 3 years at MagnaCom, Inc, where he engaged in ballistic missile defense system simulation development and system-level studies for the Ground-based Midcourse defense \(GMD\ system. He joined GTRI in 2003, where he has performed target tracking algorithm research for BMD radar and supplied expertise in radar signal and data processing for the Missile Defense Agency and the Navy Integrated Warfare Systems 2.0 office.  Mr. Burns has written a number of papers in spatio-temporal signal processing, sensor registration and target tracking, and is currently pursuing a Ph.D. at the Georgia Institute of Technology  


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


