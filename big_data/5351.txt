Elimination Algorithm of Redundant Association Rules Based on Domain Knowledge Jing Zhang School of Computer and Information Hefei University of Technology Hefei, Anhui, China jzhang_zj@163.com Bin Zhang School of Computer and Information Hefei University of Technology Hefei, Anhui, China 365439820@163.com Zihua Wang USTC E-Business Techno logy Co., Ltd Hefei, Anhui, China zhwang@ustcsoft.com Lijun Shi School of Computer and Information 
Hefei University of Technology Hefei, Anhui, China Abstract Many association rule mining algorithms have been developed to extract interesting patterns from large databases However, a large amount of knowled ge explicitly represented in domain knowledge has not been used to reduce the number of association rules. A significant number of known associations are unnecessarily extracted by association rule mining algorithms The result is the generation of hundreds or thousands of noninteresting association rules. This paper presents an algorithm named DKARM, which takes into account not only database itself, but also related domain knowledge, so as to eliminate extraction of known associations in domain knowledge 
Experiments show this algorithm can reasonably eliminate redundant rules, and effectively re duce the number of rules Keywords-Data Mining; Association Rules; Domain Knowledge Redundant Rules I I NTRODUCTION The purpose of association rule mining technique is that discovers the association rules which are novel, useful interesting and hiding in the ite m se association rule mining should find the relation from the new and hiding item. However, most classic algorithms only consider the data itself. Whereas the abundant data related domain knowledge is not regarded as the prio ri knowledge to 
eliminate the known patterns The result is that a lot of redundant rules are created within the mined knowledge. The association rule mining algorithms also extract a lot of known associations, which are regarde d common knowledge in domain, so as to create a lot of redundancy. For eliminating the redundancy, most researches in th e association discovery use the method that brings down the threshold to reduce the number of rules In the knowledge discov ery system, the Domain Knowledge \(DK\defined as th e knowledge that guide and restrict the interesting knowle dge on searching, or is called background knowled 
ge. Usually the tw o definitions are not differentiated. DK refers to the im portant factors or concepts in a specific domain and the relations between these factors and concepts. The knowledge disc overy algorith m shall reasonably make use of the DK, so as to cut down the searching time. In the paper, the DK is understood as the information that is useful for discovering knowledge It is not only the professional knowledge in som e application domains, but also includes some related knowl edge on knowledge discovery and other related background knowle dge and so on. The knowledge discovery shall be based on the DK, and make use of the DK to 
do the targeted knowledge disc overy. On the one hand, it may reduce the range of the searching object and improve the efficiency. On the other hand it may improve the discovery pattern or the interest and reliabil ity of the result. In the meantime, the interest and the reliab ility also relate to DK itself The paper presents an association rule algorithm based on the domain knowledge named DKARM. It not only take into account the DK factor on the data itself, but also improves the classical association rule II T HE A SSOCIATION R ULES M 
INING R ESTRICTED BY D OMAIN K NOWLEDGE Because some items owns the mandatory dependence as is known by everyone among them under guided or restricted by the DK, the rules th at the association mining get always include plenty of the redundant rules. For examples, in the market selling database, {nail, sinker} ow ns the mandatory association and it is a very credible rule. Bu t the user may not be interested to the kind of the rules. Wh at they are really interested in is the rules similarly as {diaper, beer The rules are really interesting 
and can’t be got according to the restriction of the DK. In the traditional association mining pr ocess, most mined rules may exist in the intense relation with the known DK. However, the discovery of the useful and new knowledge can’t make use of the rules. That leads to a resu lt which is the interesting and non-interesting association rules mixed together. Therefore, the users have to distinguish and expl ain them one by one for discovering the interesting pattern Obviously, under the DK guid ing, those association rules obviously token on shall be put out the association mining 
2010 Seventh Web Information Systems and Applications Conference 978-0-7695-4193-8/10 $26.00 © 2010 IEEE DOI 10.1109/WISA.2010.23 13 


This will avoid these rules being seen and extracted by users. In order to diminish the number of the known pattern in the association mining, the paper presents the DKARM algorithm which use the DK as the priori knowledge to eliminate the redundant association rules and the rules derived from themselves A Instance Analysis  fi rstly proposes the mining customer transaction database association rules question among itemset. Its main idea is the recursion way based on the frequent item theory. This is a valid association rule mining algorithm. But it may generate lots of candidate itemsets, and need plenty of times to scan the database. In the process plenty of time is consumed on the exchanging data in the database in the memory Agrawal et al. has proved that the association rules own the following properties Property 1: The subset of the frequent items is also frequent items Property 2: The superset of th e non-frequent items is also non-frequent To the firmly flaw of the Apriori algorithm, J.Han et al proposes a method which doe sn’t generate the candidate frequent items-FP tree algorithm Its m a in policy is di vide and rule. The algorithm main idea is that sets up the structure with no candidate sets based on the FP-tree frequent pattern. It compresses every transaction in the original database to a FPtree, reduces the mining time of the FP-Growth too much, and improves the mining efficiency However, there is also restricting on FP algorithm. When it creates the conditional- pattern base to mine the frequent item if the branch of the tree is too long, the recursion will still take time Discuss the problems of mini ng the mandatory association rule without removing the e xplicit DK guiding through analyzing the instance. Considering a set of elements A,B,C,D,E}, all possible combinations of these elements produce the sets: {AB}, {AC}, {AD}, {AE}, {BC}, {BD BE}, {CD}, {CE}, {DE}, {ABC}, {ABD}, {ABE}, {ACD ACE}, {ADE}, {BCD}, {BCE}, {BDE}, {CDE}, {ABCD ABCE}, {ABDE},{ACDE},{BCDE}, {ABCDE 001 Without considering any threshold, the number of possible subsets is 26, and the maximum number of rules produced with these subsets is 180 002 If consider that the element C and D have a mandatory association. Notice that there are eight sets where C and D appear together \({CD}, {ACD}, {BCD}, {CDE}, {ABCD ACDE}, {BCDE}, {ABCDE}\The eight sets will produce 92 rules, and in every rule, C an d D will appear. The result is that 51.11% of the whole amount of rules is created with the dependence between C and D It is important that we can’t directionally remove C and D from in the preprocessing, because C or D may have an association with A 001 B or E. Nevertheless, we can avoid the combination of C and D in th e same set. This eliminates the possibility of generating rules including both C and D 002 B Association Rules Mining Algorithm Based on DK DKARM In this section, we will use an instance to describe DKARM on how to eliminate the redundant rules under the DK guiding For studying conveniently, the redu ndant rules are restricted to including two items, which is called as the pairs of the redundancy item. Table 1 is the market database transaction table 003 Describe the DKARM Algorithm Input: D: the transaction database; min_sup: minimum support 004 redundant rules under the DK guiding Output: P: the frequen t pattern set after removing the pairs of the redundancy items TABLE I S UPERMARKET TRANSACTION DATABASE TID ITEMSET T100 I1,I2,I5 T200 I2,I4 T300 I2,I3 T400 I1,I2,I4 T500 I1,I3 T600 I2,I3 T700 I1,I3 T800 I1,I2,I3,I5 T900 I1,I2,I3 TABLE II 1ITEMSET COUNT SORT BY DESCENDING AFTER SCAN THE DATABASE A TIME ITEM Support count I2 7 I1 6 I3 6 I4 2 I5 2 TABLE III S UPERMARKET TRANSACTION DATABASE I TEMSET SORTS SUPPORT  TID ITEMSET T100 I2,I1,,I5 T200 I2,I4 T300 I2,I3 T400 I2,I1,I4 T500 I1,I3 T600 I2,I3 T700 I1,I3 T800 I2,I1,I3,I5 T900 I1,I1,I3 The step 1 Scan the database a time to gain 1-candidate sets, and remove the item who se support is less than min_sup to gain all frequent item that is 1-frequent itemsets The item of 1-frequent itemsets sorts support by descending The result is the table 001\012 after the table 000\003\001\011 sort support by descending. And every transaction in the database rearrange 
14 


according to the 1-frequent itemsets order to gain L, as the table 000\003\001\013 shows. This is ready for combine the repeat path The step 2 Scan the database for the second time. Create the FP-tree with the branch number on the base of step 1 Firstly, create the root node of the tree, which is signed by NULL”. And then, scan the database on the second time and insert the transaction of the database to the root node one by one, according to the order of the L. The FP-tree with the branch number is created through the table 3, as the Figure1 shows. For exampl e, node I3:2\(12 the node whose name is I3 and whose support is 2. The branch number is expressed by the string in the brackets. So the “\(12\ the r oot node to the first branch and then from the first bra nch to its second sub-branch TID Support N ode chain NULL I2:7\(1 I1:2\(2 I3:2\(12 I1:4\(11 I4:1\(13 I3:2\(21 I3:2\(112 I5:1\(111 I4:1\(113 I5:1\(1221  I2   7 I1   6 I3   6 I4   2 I5   2 Figure 1 FP tree with branch number The step 3 From the root node beginning, mine the each branch in order to ge t the 2-itemsets which is the combination with the first item of the branch. And then count the support of the item from each branch. Compare the result of counting to the min_sup to get the 2-frequent itemsets. For example in the figure 1, from the “NULL node beginning, the node I2 generates the 2-itemset I2I1:4\(11\:2\(12\+2\(112\I4:1\(13\\(113\I5:1\(11 1221\ and the node I1 generates the 221 1221 According to the node order of the table 001\012 compute the node I3 001 I4 001 I5 like that to generate th e each 2-itemset of themselves. Suppose the min_sup is 2, and we can get the 2-frequent itemsets{I2I1:4\(11\2I 3:2\(12\2\(112\2I4:1\(13 111\1\(1 221\I1I3:2\(112 111 1221 000\021 The step 4 Remove the redundant pairs of the items with the DK guiding to get the set 004 from all 2-frequent itemsets The step 5 According to the pr operty of the Apriori generate the K-order candidate ite m through the K-1-order item. The algorithm is end when there is an order item that is an empty set, and gets the all frequent pattern set P removed the redundant pairs As the 2-itemset reserve the support of each transaction in the each branch, we can get the support of the 3-candidate set to avoid to scanning the database again or searching recursively the FP-tree. For example, owing to the set I2I1:4\(11\ I2I3:2\(12\2\(112 21 frequent itemsets, the 3-itemset I2I1I3} is also frequent And according to the prefix of sharing the branch number we can get its support that is 2 and that is {I2I1I3:2\(112 003 Algorithm illustration The step 1 and step 2 almost keep the consistence with the FP-tree algorithm. However, we increase the branch number in create the FP-tree for getting the information of the more order itemset in the follow-up steps The step 3 is different from the FP-tree in the searching progress. The FP-tree uses the bottom-up method, but we do the combination of the item up-bottom In the step 4, after generati ng the 2-candidate sets, all the redundant element pairs are removed from 2-frequent itemsets in 004 Due to eliminate the redundant pairs of the item from DK in 004 as {C,D}\this does not only avoid to generating the rule C->D, but also avoid to generating all the derived rules\(as {D->C,C->AD}\hese rules include the known mandatory associa tion. According to the property 1 and property 2, the step makes sure that the redundant pairs of the item can’t appear together in the follow-up frequent itemsets, and don’t appear in the association rule. This makes the algorithm be very valid and be independent any the threshold restricting of the minimum support minimum confidence and so on. The algorithm eliminates all candidate set that includes the redundant pairs of items, and is independent any concept levels. This is the character of the DKRAM algorithm that is different from the other removing the redundant rule algorithm In the first two steps, we make use of the FP algorithm to compress every transaction in the original database to a FPtree, and avoid to scanning the database too many times. In the step 5, we also make use of the core idea of the Apriori algorithm. According to the property of the Apriori, we get the more order itemsets through join and pr uning on the base of the 2-itemset. Thro ugh increasing th e branch number of the FP-tree, we reserve the support of each transaction in the each branch as getting the 2-itemsets Therefore, we can get the support of the 3-candidate itemsets, and don’t recursive scan the database again. So the DKARM algorithm avoids the defect of scanning the database too many times, as well as avoids the recursive mining way in the FP-Growth algorithm III E VALUATION Don’t consider the threshold of the minimum support and the minimum confidence. The figure 2 shows the theoretic computational result of the nu mber of rules after eliminating a different number of the redundant pairs of the item. We use the theory to analyze the transaction datab ase which includes the number of the transaction items from 2 to 12, and then respectively discuss the zero, one and two redundancy pairs of the dependences from the set. When the number of the 
15 


redundancy pairs is zero, it is the result without dependence elimination. Notice that in the three cases, as the number of dependences is zero, the number of rules grows exponentially with the number of the element of the set growing, but the elimination of dependences does considerably reduce the number of the rules. Obviously the higher the number of well known dependences is, the more significant will the rule reduction be The percentage reduction of rules produced when zero, one and two pairs of dependences are el iminated is shown in Figure 3. For example, the elimination of one pair generates only 55 of the total number of rules, and eliminates well know rules in 45%. When two pairs are removed, only 30% of the rules are generated, and the reduction increases to 70%. Notice that even if the number of elements increases, these values represent saturation points for these curves Meantime, the operation system is windows XP, CPU Interl T2050 1.60GHz and Memory is 1.00GB. We use java to perform the experiment of the DKARM algorithm. The experiment data are from the transac tion of the AllElectronics  n cludes 9 transactions and 5 items, the RDG1 dataset which includes 100 tran sactions and 10 items generated by weka, and the contact-lenses dat aset which includes 24 transactions and 12 items from weka. We set the support 0.2 and the confidence 0.6 in the experiment. The table 000\003\001\014 shows the result of the experiment Notice the result of the exp eriment from the actual data, and we can find that the number of rul es and time is obviously cut down. To the user who do the data mining, eliminate the number of the known rules under being restricted by the DK which is more important than removing the time as generating the association rule. The main purpos e of the paper is that fully consider the point of the DK guiding in the mining progress However, with eliminating the association pairs of dependence set, it will generate less candid ate items. Therefore, the time of the algorithm certainly eliminate TABLE IV THE EXPERIMENT RESULT OF THE DKARM IV C ONCLUSIONS AND F UTURE W ORK The paper presents a method of mining under the DK guiding, which can be used to eliminate the redundancy association, and make sure the generated rules that is interesting to the user. The threshold restricting of the minimum support etc. doesn’t impact on the elimination. Under the relational domain guiding how to gain the more complicated items dependences as the priori knowledge to guide our follow-up mining wor k, and how to evaluate the interest of the rules after mining, both them need us to continue to study No elimination Eliminate one pair of the redundant items Eliminate two pairs of the redundant items Dataset The number of rules Time\(ms The number of rules Time\(ms The number of rules Time\(ms RDG1 13 99093.7 8 83302.3 7 71713.3 contact-lenses 23 440.6 13 393.8 8 349.8 AllElectronics 10 78.0 6 62.0 4 62.0 000\023 000\023\000\021\000\024 000\023\000\021\000\025 000\023\000\021\000\026 000\023\000\021\000\027 000\023\000\021\000\030 000\023\000\021\000\031 000\025\000\026\000\027\000\030\000\031\000\032\000\033\000\034\000\024\000\023\000\024\000\024\000\024\000\025 0001\000X\000P\000E\000H\000U\000\003\000R\000I\000\003\000,\000W\000H\000P\000V\000\013\000Q\000\014 000\010\000\003\000*\000H\000Q\000H\000U\000D\000W\000L\000R\000Q\000\003\000U\000X\000O\000H\000V 000\024\000\003\000S\000D\000L\000U 000\025\000\003\000S\000D\000L\000U\000V Figure 2. Percentage reduction of rules considering one and two pairs of dependences A CKNOWLEDGMENT This work is supported in part by Teaching and Research Project in Anhui Province \(No.2008jyxm240\ence Development Project of Hefei University of Technology No.2009HGXJ0035 R EFERENCE  Jiawei Han Micheline, Ka m b e r D a ta Mining-concepts and techniques High Education Press. Morgan Kaufman Publishers, 2001  R. Agraw al T Im i e linski, and A. Swa m i  Mining association rules between sets of items in large databases. Proceedings of the ACM SIGMOD Conference on Management of data, pp. 207-216, 1993 000\023 000\024\000\023\000\023\000\023\000\023\000\023 000\025\000\023\000\023\000\023\000\023\000\023 000\026\000\023\000\023\000\023\000\023\000\023 000\027\000\023\000\023\000\023\000\023\000\023 000\030\000\023\000\023\000\023\000\023\000\023 000\031\000\023\000\023\000\023\000\023\000\023 000\025\000\026\000\027\000\030\000\031\000\032\000\033\000\034\000\024\000\023\000\024\000\024\000\024\000\025 number of items\(n Maximum number of assocation rules 0 pair 1 pair 2 pairs Figure 3. Maximum number of rules eliminating zero \(reference one, and two pairs of dependences  J.Han,J.Pei, and Y.Yin.Mining. Frequent patterns without candidate generation. In Proc. 2000 ACM-SIGMOD Int. Conf. Management of Data \(SIGMOD 002 00\, p.p. 1-12, May 2000  Usam a Fayy ad Gr egor y PiatetskySh apir o, and Padhraic S m yth, Fr o m  Data Mining to Knowledge Discovery: An Overview. In: U. Fryyad, G Piatetsky-Shapiro, P.Smyth and R Uthurusamy, eds. Advances in Knowledge Discovery and Data Mining. Menlo Park, California: AAAI Press/The MIT Press 1996, pp. 1-35  Zaki M Generati ng non-redundant association rules, In Proceedings of the 6th ACM \(SIGKDD'2000 Discovery and Data Mining, Boston, Massachussets  USA pp.34-43 2000  B. Liu W   Hsu, and Y. Ma Pruning and summarizing the discovered associations. Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining. San Diego California, pp. 125 134,1999 
16 


Mine the fuzzy association rules by the traditional fuzzy association rules algorithm at first. Then set mfc = 0.6 and min_corr = 1.Use the uniform minimum mfs and set mfs equal to 0.3, 0.2, 0.16, 0.12, 0.10 and 0.08 respectively. It can be seen that, when mfs gets a larger value, as shown in Table 1, the number of the rules extracted from the frequent itemsets is limited. Take for example, while mfs\(k\ = 0.3 small amounts of frequent fuzzy two itemsets, three itemsets and four itemsets are generated. Some meaningful rules especially some longer rules fail to be mined for their higher supports. However, in order to mine more long rules turning more four itemsets and five itemsets into the frequent fuzzy itemsets to get smaller fuzzy supports, which turns out the other way. Take Table 6 for example, when mfs = 0.08, moderate frequent fuzzy five itemsets are generated, but amounts of frequent fuzzy two itemsets and three itemsets are extracted and the number of mined fuzzy rules which may contain lots of redundant and meaningless rules is multiple. The relation between the value of the mfs in traditional fuzzy association algorithm and the rule number of fuzzy association is shown in figure 6. Thus the uniform mfs used in the traditional algorithm has a great effect on the accuracy of the final rule mining if its value is either too big or small. How to find a more moderate fuzzy support threshold is a common problem To solve this problem effectively, a method based on multiple minimum fuzzy supports is adopted. Suppose mfs k\, 0.2, 0.16, 0.12, 0.1, 0 s s h o w n  i n t a bl e 7 each layer has the same rules as the corresponding layer from table 2 to table 5 when k=2,3,4,5 respectively. In this way, not only the large number of meaningless rules is avoided, but some meaningful rules also cannot be removed However, it doesn’t take into account the negative association rules generated in the infrequent itemsets. By comparison of table 7 and table 8, certain infrequent itemsets inFFS\, in which some meaningful negative rules could be generated, can be produced by setting the threshold of the infrequent fuzzy itemsets. But it does not mean that the lower the threshold, the better. Comparing table 8 with table 9, we can see that plenty of fuzzy negative rules, most of which are meaningless and redundant, will be generated when inFFS is the complement of FFS. It is clearly inadvisable TABLE I  FPNAR WHEN MFS  K 0.3 mfs\(k\=[0.3,0.3,0.3,0.3,0.3,0.3 f c  0 6 m i n _ c o r r  1  FFS inFFS PAR NAR1 NAR2 Rules_num k=2 14 0 25 0 25 50 k=3 9 0 43 0 46 89 k=4 1 0 9 0 10 19 k=5 0 0 0 0 0 0 Total 24 0 77 0 81 158 TABLE II  FPNAR WHEN MFS  K 0.2 mfs\(k\=[0.2,0.2,0.2,0.2,0.2,0.2 f c  0 6 m i n _ c o r r  1  FFS inFFS PAR NAR1 NAR2 Rules_num k=2 25 0 38 2 40 80 k=3 22 0 98 0 114 212 k=4 9 0 93 0 114 207 k=5 1 0 26 0 30 56 Total 57 0 255 2 298 555 TABLE III  FPNAR WHEN MFS  K 0.16 mfs\(k\=[0.16,0.16,0.16,0.16,0.16,0.16  m f c 0 6 m i n _ cor r  1   FFS inFFS PAR NAR1 NAR2 Rules_num k=2 26 0 38 4 40 82 k=3 26 0 106 0 127 233 k=4 11 0 104 0 135 239 k=5 2 0 40 0 55 95 Total 65 0 288 4 357 649 TABLE IV  FPNAR WHEN MFS  K 0.12 mfs\(k\=[0.12,0.12,0.12,0.12,0.12,0.12  m f c 0 6 m i n _ cor r  1   FFS inFFS PAR NAR1 NAR2 Rules_num k=2 30 0 38 20 40 98 k=3 29 0 112 7 133 252 k=4 15 0 122 4 167 293 k=5 2 0 40 0 55 95 Total 76 0 312 31 395 738 TABLE V  FPNAR WHEN MFS  K 0.1 mfs\(k\=[0.1,0.1,0.1,0.1,0.1,0.1 m f c  0 6 m i n _ c o r r  1  FFS inFFS PAR NAR1 NAR2 Rules_num k=2 34 0 38 32 40 110 k=3 36 0 121 27 154 302 k=4 18 0 138 10 194 342 k=5 4 0 60 4 97 161 Total 92 0 357 73 485 915 TABLE VI  FPNAR WHEN MFS  K 0.08 mfs\(k\=[0.08,0.08,0.08,0.08,0.08,0.08  m f c 0 6 m i n _ cor r  1   FFS inFFS PAR NAR1 NAR2 Rules_num k=2 36 0 38 38 40 116 k=3 40 0 126 32 164 322 k=4 19 0 142 13 206 361 k=5 4 0 60 4 97 161 Total 99 0 366 87 507 960 TABLE VII  FPNAR WHEN USE MULTI LEVEL FUZZY SUPPORT  mfs\(k\=[0.3,0.2,0.16,0.12,0.1,0  m f c  0  6  m in _co rr 1   FFS inFFS PAR NAR1 NAR2 Rules_num k=2 25 0 38 2 40 80 k=3 26 0 106 0 127 233 k=4 15 0 122 4 167 293 k=5 4 0 60 4 97 161 Total 70 0 326 10 431 767 TABLE VIII  FPNAR WHEN USE MS_FPNAR mfs\(k\=[0.3,0.2,0.16,0.12,0.1,0  m fc 0 6 m in _c or r 1   FFS inFFS PAR NAR1 NAR2 Rules_num k=2 25 11 38 38 40 110 k=3 26 14 106 32 164 302 k=4 15 4 122 13 206 342 k=5 4 0 60 4 97 161 Total 70 29 326 97 507 930 TABLE IX  FPNAR WHEN MFS 0 mfs\(k\=[0.3,0.2,0.16,0.12,0.1  m fc 0  6  m in _cor r 1   FFS inFFS PAR NAR1 NAR2 Rules_num k=2 25 23 38 78 40 156 k=3 26 77 106 378 235 719 k=4 15 92 122 819 564 1505 k=5 4 39 60 702 486 1248 Total 70 231 326 1977 1325 3628 
627 


 Fuzzy rules Fuzzy rules mfs Fuzzy rules Fuzzy rules mfs  Fig.6  mfs and Fuzzy rules in Traditional algorithms Table 10 and Figure 7 reflect the relationship between the minimum correlation coefficient \(min_corr\d fuzzy positive and negative association rules \(FPNAR\where the former is used to remove those frequent fuzzy itemsets of weak correlation. Table 10 presents the number of fuzzy positive and negative association rules, which are generated with min_corr varying from 1 to 2.8 under such preconditions as mfs = [0.3, 0.2, 0.16, 0.12, 0.1, 0.08 d  m f c  0.6.As can be seen from Figure 7, with the min_corr increasing, the number of fuzzy association rules attained shows an obvious decreasing trend, as the minimum correlation coefficient can eliminate the rules which are generated by frequent itemsets of weak correlation, leaving more meaningful rules. So, the minimum correlation coefficient can effectively remove those meaningless or less meaningful rules to ensure that the final mined fuzzy rules are meaningful TABLE X  RELATIONSHIP BETWEEN MIN _ CORR AND FPNAR mfs\(k\=[0.3,0.2,0.16,0.12,0.1,0  m fc 0 6  min_corr= 1.0 1.3 1.5 1.7 2.0 2.3 2.5 2.8 PAR 326 292 249 192 133 106 72 44 NAR 604 522 435 322 193 130 80 48 TotalRules 930 814 684 514 326 236 152 92  Fig.7  relationship between min_corr and FPNAR According to the above analysis, the algorithm is adopted to mine rules from the rock sample data set. If correlation coefficient constraints min_corr = 2, and multiple minimum fuzzy support mfs \(k\3, 0.2, 0.16, 0.12, 0.1, 0 t h en  326 fuzzy rules, including 133 fuzzy positive rules and 193 fuzzy negative rules, are generated after mining. Compared with the traditional fuzzy association rules mining algorithm, this algorithm solves the problem about the selection of fuzzy support, and generates more accurate and effective fuzzy positive and negative association rules VI  CONCLUSION There are broad development space and prospects for the mining fuzzy positive and negative association rules however, there are still many drawbacks in the selection of the membership function and minimum support, and the accuracy of selected parameters directly affects the result of the final mining rules. This paper proposes a novel fuzzy positive and negative association rules algorithm for the first time, and utilize k-means clustering method to determine the membership function, avoiding uncertainty problems which may be brought about in current existed related algorithms that need to identify the membership function subjectively Meanwhile, the multi-level fuzzy support and the correlation coefficient criterion are introduced based on fuzzy positive and negative association rules. In the end, the proposed algorithm is proved to be more effective and accurate in comparison with the traditional algorithm through an experiment REFERENCES 000>\000\024\000  R. Agrawal, T. Imielinski and A. Swami, “Mining association rules between sets of items in large database”, Proceeding of the 1993 ACM SIGMOD International Conference on Management of Data New York: ACM Press, 1993, pp.207-216 000>\000\025\000  R. Agrawal, R. Srikant, “Fast algorithms for mining association rules in large database”, Proceedings of the 1994 International Conference on VLDB, San Francisco: Morgan Kaufmann Publishers, 1994 pp.487-499 000>\000\026\000  J. Han, J. Pei, Y. Yin et al Mining Frequent Patterns without Candidate Generation: a Frequent-Pattern Tree Approach”, Data Mining and Knowledge Discovery, Vol. 8, 2004, pp. 53-87 000>\000\027\000  R. Srikant, R. Agrawal, “Mining Quantitative Association Rules in Large Relational Tables”, In Proc. 1996 ACM–SIGMOD Int. Conf Management of Data, Montreal, Canada: ACM Press, 1996, pp. 1-12 000>\000\030\000  B. Lent, A. Swami, J. “Widow. Clustering association rules”, In Proc 1997 Int. Conf. Data Engineering, Birmingham, England: ACM Press, Apr. 1997, pp. 220-231 000>\000\031\000  T.P. Hong, J.B. Chen, “Processing individual fuzzy attributes for fuzzy rule induction”, Fuzzy Sets Syst, 2000, pp. 127-140 000>\000\032\000  S. Brin, R. Motwani, C. Silverstein, “Beyond Market: Generalizing Association Rules to Correlations”, Processing of the ACM SIGMOD Conference, Tucson, AZ: ACM Press, 1997, pp. 265-276 000>\000\033\000  X. Wu, C. Zhang, S. Zhang, “Efficient Mining of Both Positive and Negative Association Rules”, ACM Trans. On Information Systems Vol.22, 2004, , pp. 381-405 000>\000\034\000  X.J. Dong, S.J. Wang, H.T. Song, “Approach for Mining Positive Negative Association Rules Based on 2-level Support”, Computer Engineering, Vol.31, 2005, pp. 16-18 000>\000\024\000\023\000  X.J. Dong, Z.D. Niu, X.L. Shi et al Mining Both Positive and Negative Association Rules from Frequent and Infrequent Itemsets ADMA 2007. Harbin, China: Spring-Verlag, 2007, pp. 122-133 000\003 
628 


world databases presents a future direction for further research  Figure 1: Execution time v/s Minimum Support  References 1 R. A g r a w a l a nd R. Srik a n t F a s t A l g o rithm s  f o r Mining  Association Rules,” In Proc. of the 20th VLDB Conference  Santiago, pp. 487-499, Chile, 1994 2 R. A g ra w a l T  Im i e linsk i, a nd A. Swami. “Mining Association Rules between Sets of Items in Large Databases.” In Proceedings of ACM SIGMOD pages 207-216, May 1993 3 R. A g ra wa l, R. Srik a n t M in i n g se que ntia l pa t t e r ns In proc. of the 11th International Conference on Data Engineering \(ICDE'95  pages 3-14, March 1995 4 M Blu m R.W  Flo y d   V   P r att, R.L  Riv e st an d R E. T a rjan  Time bounds for selection,” J. Comput. Syst. Sci. 7\(1973\, pp 448-461  Total number of data items Number of elements in MEDIUM Number of elements in LOW 10 139 0 20 3238 127 30 21899 1923 Figure 2: Size of MEDIUM and LOW v/s number of data items 5 Fe r e nc B odo n  A f a s t  A P R I O R I i m plem e n ta tion I n pr oc o f  IEEE ICDM Workshop on Frequent Itemset Mining Implementations \(FIMI'03 Melbourne, Florida, USA, 2003 6 Feren c Bo d o n   A T r i e b as ed A P RIORI Im p l e m en tatio n f o r  Mining Frequent Item Sequences.” In ACM SIGKDD Workshop on Open Source Data Mining Workshop \(OSDM’05 pages 56-65 Chicago, IL,USA, 2005  M  C h en  J Han an d  P  S   Yu  Dat a M i n i n g  A n o v ervi e w  from a Database Perspective IEEE Transactions on Knowledge and Data Engineering vol. 8, no. 6, pp. 866-883, Dec. 1996 8 A r on Culotta A ndre w Mc Ca llum Jona tha n Be tz  I nte g ra tin g probabilistic extraction models a nd data mining to discover relations and patterns in text,” In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics p.296-303, June 04-09, 2006, New York  M M S u fy an Be g P aral l e l an d Di st ri b u t ed Di sco v er y o f  Association Rules In Artifical Intelligence Application Book  Fadzilah Siraj, Eds\ersity Utara Malaysia 10 X ita o Fa n k os Fe ls v  l y i Ste phe n A  Siv o  M onte C a r l o   SAS® for Monte Carlo studies: a guide for quantitative researchers 11 U  Fa y y e d  G  P i a t e t s k y Sha p iro P. Sm y t h a nd R. U t h u ra s a my  eds.\. “Advances in Knowledge Discovery and Data Mining AAAI Press / The MIT Press, 1996 1 W  J F r aw l e y  G  P i at et sk y S h ap i r o an d C M a t h eu s   Knowledge “Discovery In Databases: An Overview. In Knowledge Discovery In Databases eds. G. Piatetsky-Shapiro, and W. J Frawley, AAAI Press/MIT Press, Cambridge, MA., 1991, pp. 1-30 13 V  K u m a r  a nd M. J o s h i T utor ia l o n H i g h P e r f or m a nc e D a t a  Mining,” In proc. of International Conference on High Performance Computing \(HiPC-98 Dec. 1998  a v i d L  O l s o n a n d D e s h e n g Wu   D eci s i on  m a k i ng  with uncertainity and data mining.” In X. Li, S. Wang and Z.Y Dong\(Eds Lecture notes in Artificial Intelligence pp. 1-9 Berlin: Springer\(2005 15  P  W r ig ht. K now le dg e D i s c ov e r y I n D a ta ba s e s  T ools a n d  Techniques ACM  Crossroads Winter 1998   
408 


 7. Reference    Fetzer C,Hagstedt, K,Felb er P  Autom atic Deteciton  and Masking of Non-Atomic Exception Handling International Conference On Dependable Systems and Networks, \(DSN2003\10-116    Y e n S J, Lee Y S. “Mining Interesting  Associatio n  R u l es  and Sequential Patterns”. International Journal of Fuzzy Systems, 2004-6 \(4   Alasf f ar A  H, Deogun J S. “Concept-b a sed Retr iev a l with Minimal Term Sets”. Foundations of Intelligent Systems: 11th Int’l Symposium, Springer, Poland, 2004 114- 122   Qiu Y ong gang,Frei H P  Concept B a sed Quer y   SIGIR’03,2003:16 0-169   Saltom G  W ong A, Y a ng C  S. “A V ector Sp ace Model for Automation Indexing”. Communications of the ACM 2005, 18\(5\-620   Agrawal R, Srikant R. “Fast Algorithm f or Mining Association Rules in Large Databases.” Proceedings of the 20th International Conference on Very Large DataBases Santiago , Chile , 2004   Park J S. “Using A Hash-Based Method with Transaction Trimming forMining Association Rules.” IEEE Transactions on Knowledge and Data Engineering, 2007   Savasere A, Omiecinski E Navathe S  An Ef ficient Algorithm for Mining Association Rules in Large Databases.” Proceedings of the 21st International Conference on Very large Database, Switzerland, 2002  


              


   


                        





