A Novel Parallel Architecture with Fault-Tolerance for Joining Bi-Directional Data Streams in Cloud Xinchun Liu School of Computer Science and Technology University of Science and Technology of China Hefei 230026 China Email liuxch@mail.ustc.edu.cn Xiaopeng Fan Shenzhen Institutes of Advanced Technology Chinese Academy of Sciences Shenzhen,518052 China Email xp.fan@siat.ac.cn Jing Li School of Computer Science and Technology University of Science and Technology of China 
Hefei 230026 China Email lj@ustc.edu.cn 
Abstract 
Nowadays there are more and more data sources such as online stores wireless sensors and stock tickers etc which produce a mass of data as unbounded streams Investigating the potential information from these rich statistical data in real time generates enormous values In most cases it is essential to join two or more data streams in order to nd sufìcient information However as the volume of these data becomes larger joining them in a single machine is not an effective way One of the most practical solutions is to use a shared-nothing cluster built on a cloud to deal with these data streams In this 
paper we provide a novel parallel architecture named DualAssembly-Pipeline\(DAP with fault-tolerance in which we join bi-directional data streams by considering the processing nodes failures Especially virtual machines in a cloud may fail so that fault-tolerance becomes more and more important when users issue a continuous query Our experiments show that our DAP model can effectively join bi-directional data streams in parallel Whatês more when non-adjacent virtual machines fail at the same time all data can be recovered while adjacent ones fail only parts of data can be reinstated Keywords streams join fault-tolerance cloud cluster 
I I NTRODUCTION Recently many applications in big data produce real-time continuous ordered and large data sequences so-called data  Each data stream comes as unbounded and the arrival rate is uncontrolled It is impossible to predict the pattern of these data streams Such applications normally provide replies to uses requests with low latency In traditional database systems all the data are stored in the persistent storage before being processed But in the models of data streams users queries cannot be answered by all the data usually part of the data because of the characteristics of the data streams In data streams the latest tuples are normally much more 
important than previous ones because they reîect the trends of data streams and users pay more attention to them Based on this assumption it is a better choice to apply sliding windows in the processing of data  These windo ws can be classiìed as count-based windows or time-based windows If the window size is measured by the number of tuples such as 10000 tuples it is considered as count-based window If the window size is measured by time such as 15 minutes it is time-based window In complex queries one data stream can only provide partial information In order to deal with complex queries many applications require combining two or more streams with each 
other Data stream joining has attracted more and more attentions in the past years For example 1\ertisement query and click 2 auction[3 3 ehicle tracking In the past decade joining data streams on a single node has been investigated to a certain degree Some key technologies like sliding windo sampling[5][8 sk etching[12][15 his   ha v e been proposed In these models all kinds of computing resources such as CPU memory and network are shared by processing units When a new tuple comes into one sliding window it is not necessary to care about this window for the other stream in the memory because the two data streams share the same memory However when 
the workloads of the data stream system exceed the limitation of a single node some of tuples have to be dropped Therefore it is an important new trend to process big data in a cluster which is composed of computing nodes in a sharednothing  Joining data streams can also be executed in a parallel way in a cluster This can improve the throughput of the data stream system However this brings us new challenges as follows 
Parallelization In a cluster several nodes execute joining simultaneously We must guarantee that each pair of tuples which satisìes the join condition in sliding windows 
 
should be processed And the results of joining appear only once eventually 
Shared-nothing memory In the single-node processing model all the tuples in sliding window are maintained in the shared memory When two tuples perform joining they are transparent for each other in the memory It is unnecessary to consider the locations in the proposed algorithms In the cluster processing model we must guarantee the related tuples for joining which are distributed in the cluster should be collected together Otherwise the query results may be incomplete 
 
2013 International Conference on Cloud Computing and Big Data 978-1-4799-2829-3/13 $26.00 © 2013 IEEE DOI 10.1109/CLOUDCOM-ASIA.2013.27 30 
2013 International Conference on Cloud Computing and Big Data 978-1-4799-2830-9/14 $31.00 © 2014 IEEE DOI 10.1109/CLOUDCOM-ASIA.2013.27 30 


1 2 
A t A r t c A 
002 003 002 003 
002 003 
002 002 002 002 
   
Fault tolerance In order to reduce the cost of computing more and more clusters are built in a cloud Since virtual machines may be not stable it is essential to recover data in running time when some virtual machines fail In this paper we focus on bi-directional data streams joining with fault-tolerance problem where two streams ow in opposite direction Our contributions are described as follows We propose a decentralized parallel architecture called the Dual-Assembly-Pipeline\(DAP model to process twoway data streams joining In this model each node in the cluster processes only part of tuples in the sliding windows All nodes work as workers in the assembly lines We mainly investigate the fault-tolerance problem in such a DAP model and propose an efìcient way to recover data when some nodes fail The rest of the paper is organized as follows In Section II we present the joining model and some related deìnitions In section III we discuss existing stream join architectures and their advantages and disadvantages We present our own architecture in section IV Experiments are carried out in section V We provide related works in section VI Finally we conclude this paper II M ODEL AND D EFINITIONS A data stream  is a large volume of data coming in an unbounded rapid time-varying and unpredictable way Each tuple in a stream has an implied or explicit timestamp When a tuple is produced with a timestamp it has an explicit timestamp otherwise we consider a tupleês arrival time or an incremental number as timestamp In addition there are some signiìcant features of a data stream 1 unbounded size 2 uncontrollable arrival rate 3 unpredictable order Unlike the deìnition of join in the traditional database model the join operator for data streams only considers part of the data tuples because of the above three features Moreover it is impossible for a real-time processing system to store all the tuples In order to overcome these problems one of the key technologies is making use of sliding windows Only the latest tuples are kept in the sliding windows and the join results are obtained by processing these tuples As we mentioned earlier there are two kinds of windows i.e count-based window and time-based window In this paper we select count-based window and assume that one tuple arrives for each time unit We attach an incremental number to each new arriving tuple as its timestamp The terms are deìned in TABLE I Joining two-way data streams is donated as  where is the window size of stream A while is the window size of stream B The join result contains all and which satisìes the condition  In practice the result of the join operator contains two components and  Both TABLE I MEANS OF VARIABLES of the components can be executed respectively by sharing the same memory in the same node Algorithm 1 Joining two-way data streams 1 while true do receive a tuple from stream and calculate  where and into receive a tuple from stream and calculate  where and into Fig.1 shows the detailed procedures of the execution of the join operator When a tuple from stream arrives the join operator rstly triggers a process to purge all the tuples that are out of date in  Then the join operator scans all the tuples in to nd out the ones that satisfy the join condition and execute joining Finally will be inserted into  When a tuple from stream comes the process is similar to the case of  Although the steps in each procedure are very simple we still need to coordinate how to manage the shared memory when and are executed simultaneously a 
a a   a   A W 002\003 B W W W a\002\003b a A W b B W a.attr b.attr A W B W A W B W a A then W W a 002\003 b b W a attr b attr a W b B then W W b 002\003 a a W b attr a attr b W a A W W a W b B a A W B W A W B W 
W A s s t T N T T T k t a\002\003b A B A B 
n A B A B A B A B A B i B B i j j B i j i A m A A m n n A m n m B i B B i A m i A B A B 
                           
window size of stream tuple arriving at time the current time tuples arrival rate in stream A Consumed time for adding a new virtual machine to a cluster transferring time for the backup data in upstream nodeês back window computing time for joining local window size of stream A allocated in each node 002 time consumed by 2 if 3 purge all the tuples that are out of date in 4 scan all the tuples in 5 insert 6 else if 7 pure all the tuples that out of date in 8 scan all the tuples in 9 insert 10 end if 11 end while b Fig 1 Two-Way Data Streams Join 
31 
31 


N N N N N N N N 
A Parallelization in a Cluster 
          2    2                     
m M o O i W j i W W W M O m M SLICE t t T o m t W t T W m SLICE t T,t T o n t T W t T W t T W t T W join A join B a join A sendT uple a a join B join A insertT uple queue A a a a join B scan queue B b join B 
Especially in this paper we focus on the equi-join in the two-way data streams in a cluster built on a cloud In the next section we will review some stream joining architectures for clusters and investigate their advantages and disadvantages III T WO W AY D ATA S TREAMS J OIN A RCHITECTURES One of the most popular architectures for joining in clusters is the master-worker When a data stream o ws into a cluster it is rstly processed by the master node that is responsible for distributing the tuples into the worker nodes Then the worker nodes execute the streams joining It is important to provide scheduling strategies for the master node which have a great impact on the performance of a cluster These strategies should be with the following characteristics The strategies should provide load balance for all the worker nodes in the cluster The related tuples which satisfy the join condition should be allocated to the same worker node as many as possible Since memory is precious in real-time big data processing it is important for the strategies to reduce the redundancy of the tuples in worker nodes A straightforward scheduling strategy is the Hashing method When new tuples come they are disseminated to all the worker nodes according to the joining attributes One of the advantages of this method is that the tuples can be distributed evenly Moreover the tuples with the same attribute value can be allocated in the same node when carrying out the equijoining This strategy makes sure that the tuples satisfying the join conditions can be distributed to the same node Therefore the join results are completed and correct However one of the signiìcant features of data streams is the unpredictability of the tuples arrival rate and their orders It is impossible to know the next tuple forever Some tuples in a data stream may appear frequently in a special period while the others in another data stream may appear frequently in another period This can result in unbalanced system loads if we only apply a simple Hash method Some worker nodes are under an overloaded state while others are underloaded Furthermore a simple hash method also leads to drop some tuples because of the processing speed Thus the joining results may be wrong Another tuple dissemination strategy is to allocate tuples by time slices Firstly the master node selects one stream as the main stream The main stream is allocated to worker nodes by time slices and workloads of the corresponding nodes In each time slice the master node selects one of the worker nodes according to workloads When a tuple from main stream comes the master node distributes it to a node according to the tupleês timestamp When tuples from a non-main stream come they are distributed according to the locations of the tuples from the main stream For example if a tuple from the main stream is allocated to Node  and a tuple from a non-main stream will be allocated to too where  and and are the sizes of the main stream and s sliding windows respectively Because the dissemination of the tuples is based on the workloads of these worker nodes this strategy can guarantee load balance However the disadvantage of the strategy above is that this method has to maintain a complex routing table and one tuple may be routed to multiple worker nodes For example the tuple in the main stream belonging to is distributed to node  Tuple from the non-main stream will be allocated to the node too where  Similarly assume is sent the node  Tuple will also be transferred to  where  Therefore the tuple arrives at is routed to both and  Nowadays more and more clusters are built on clouds because of their convenience and the pay-as-you-go charging model But virtual machines in a cloud may fail sometimes It is important to deal with fault-tolerance problem in the running time for the clusters Keeping tuple copies in other nodes is an efìcient way for fault-tolerance guarantee on the level of nodes In a data stream processing system we can keep a window for tuple copies in upstream nodes in order to recovery data after one node fails In the master-worker model the upstream node of all the worker nodes is the master node Therefore all the copies have to be kept in the master node This is not a reliable way to keep all the tuples in one nodeês memory once the master node fails In the next section we will present a new architecture named Dual-Assembly-Pipeline\(DAP which is suitable for bidirectional data streams joining in a cluster with fault tolerance guarantee In our architecture tuple copies for recovering will be retained in their nearest upstream nodes rather than the master node IV D UAL A SSEMBLY P IPELINE WITH F AULT T OLERANCE In manufacturing industry there are many workers or machines in an assembly line and only parts of the line work will be done by each worker or machine Finally a completed product will be created at the end of the assembly line By this way the efìciency of the production is improved substantially Enlightened by the idea of assembly lines we introduce the pipe-line into data stream joining in a cluster All the nodes in the cluster are considered as workers and the join window can be divided into several small windows Each node only deals with the join operation in one small window In order to improve the efìciency of joining we provide two symmetrical processes and at each node When a tuple from stream A comes invokes to send to its symmetrical cooperator  Then invokes to insert into the queue A After receiving the tuple  will invoke the function to nd expired tuples to purge and make the right tuples to be joined When the tuple from stream B comes the same procedure will be carried out by  After the results are produced they will be sent to a remote document server 
i k j k O M M O i k m k O M j l n l O M O M k l i i i i i i j 
   
 003 002  002 002   
32 
32 


i j i A j B i i i t N A A A B B A A B B A A B A B A B A B A B A B A B A B A B A B A B i A B 
  003 003  004 004       003   003       003  003              
 
join A join B a b a b a W b W t k a B k t a a N N k t N k t N K n K k K K t N n A\002\003B k N k t N k t N t k N k N t k k N N k k k k t N N K K n n K K k K K t N N t K K N N t n K K a b a W t W 
  002 002 002 1 1 002  002 002  002 1 002    1 002 2 max max  1 max 1 002 2 max  max max 002 2 max 002 002   002 
We select MQ as the layer for message passing which acts as the communication protocol for all the processes or hosts MQ hides all kinds of the communication details such as explicit connections reconnection error handling In our assembly line architecture we adopt a one-to-one communication model between and and an N-to-one model between the nodes in a cluster and the document server As Fig.2 a shows the semi-ìnished products go through each worker from one side of the assembly line and all the workers are with xed locations The semi-ìnished products are assembled with each part from each worker Finally the nished products are generated out of the other side of the assembly line There is only one stream in the traditional assembly line described above However to join two-way data streams there are two streams injected into the assembly line at the same time a b c Fig 2 Assembly Lines Obviously the original assembly line is not suitable for joining two-way streams One alternative method is to inject two streams into one side and ow out of the other side In order to obtain a completed result it is necessary to make sure that each pair of tuples and in windows should encounter with each other However when owing towards the same direction tuples from different streams may have different arrival rates and some of them in the sliding window at different nodes may not meet forever such as and in Fig.2 b It is similar for two cars running on the highway to the same direction They will not encounter if the speed of the back one is less than or equal to the front one Fortunately if two cars run in the opposite direction they will meet with each other sooner or later Therefore in joining double data streams two streams are similar to bi-directional highway and tuples are similar to the cars running on it It is guaranteed that two tuples in the windows from different streams must meet with each other if two streams ow into our assembly line from the opposite direction In Fig.2 c the tuple from stream A comes into the window and the tuple from stream B ows into the window  The two tuples will encounter each other at a certain node nally Therefore our proposed dual-assemblypipeline architecture satisìes the demands of joining bi-way data streams in a cluster We deìned the time for joining two tuples as  If the size of the slice in the sliding window of stream B at each node is  the time for is when a new tuple comes If the arrival rate for the tuple is  each node is required to deal with at least new coming tuples per second We obtain that 1 2 If total window size of stream B is  we can obtain the following equation 3 where is the number of needed worker nodes In the corresponding symmetric join  let the window size of stream A be  the arrival rate is  and the same case is for stream B Therefore we can obtain the following equations 4 5 In order to nd an approximate value we enlarge the values 6 Thus the value of is described by the following equation 7 where the window size of stream A and B are and  The value of needed number of worker nodes can be obtained 8 As Eq.\(8 shows the number of computing nodes for joining depends on the joining unit time  the joining window sizes and  and the arrival rates and  The parameter reîects the speed of CPU The higher of each nodeês capability is the less nodes needed are The number of computing nodes is proportional to the window sizes and  If the cost of joining is very high the join operation for each pair of tuples requires more time When a new tuple comes into total join cost will be If 
4 9 1 002 
002 
33 
33 


2 3 2 3 1 1 2 2 1 3 1 3 2 2 2 2 
Algorithm 2 if then end if 
 
1 1 4 4 4 4 5 6 7 6 7 
002 002 002 002 002 002 002 003 003 
B i j i i j j i 
2 produce a copy of 4 send a message to its upstream 
W a A Node a a Node Node a Node Node Node Node Node Node Node Node Node Node Node Node Node Node Node Node 
the size of is large enough the computation capability of the assembly pipeline will not catch up with the arrival rate of the coming streams From Eq.\(8 we discover that one of the straightforward methods is to add more computing nodes to the assembly pipeline This is because the window sizes of the streams are xed so that each node will be assigned with less load when increasing the number of computing nodes In the master-worker model the upstream node of each worker node is the master node In order to recover the data when some nodes fail all copies of the data should be kept in the master node When a node failure appears the master node recovers the data for the restarted node and the recovered data will be re-processed If the arrival rates of the data streams are high enough it is impossible to cache all the tuples in a disk To keep up with the arrival rate a better choice is to cache the backup data in the main memory However this is still limited by the size of memory Unlike the master-worker model there is no master node in our dual-assembly-pipeline architecture Each node plays the same role in the architecture and the upstream nodes are different This linear architecture allows us to distribute the backup copies at different nodes instead of a single one When a tuple is out of date it will be pushed to its downstream node or just discarded Assume that the failure at each node follows the same distribution i.e I.I.D Once a node fails all the tuples in the sliding window of this node will be lost If we only reboot a new node to replace the failed one the result of joining are not completed In order to recover tuples at the failed node we maintain two windows at each node One is the local joining window and the other is the backup window Each tuple in the local joining window is kept a copy in its upstream nodeês backup window When a tuple ows into a node it is immediately inserted into the local joining window to participate the joining operation As described in Algorithm 2 after a tuple slides out of the local joining window its copy is inserted into the backup window located in the current node At the same time a message is sent to its upstream node to remove the copy of this tuple Emit tuple to downstream 1 a tuple from stream is expired in and insert it into its backup window 3 send to the nearest downstream to notify it to purge from the backup window 5 For example in Fig.3 tuple in stream A is purged out of the local joining window in and is transferred to  First of all a copy of should be inserted into the backup window at line 2 Then the tuple is sent to line3 at the same time a message is emitted to line4 Once receiving the signal considers will not be needed any more and removes it from its local backup window When a node fails due to various reasons in a cluster built on a cloud a new virtual machine is added to the cluster to replace the failed one It is a time consuming procedure of starting a new virtual machine from cloud platform However data streams require to be processed as quickly as possible Thus starting new virtual machines straightforwardly from a cloud is not a good idea In order to meet the requirement of joining we build up a resources pool in a cloud where there are always virtual machines available in case of node failures We build a state server to manage all the nodes in the cluster The state server keeps all the states of each node and it detects whether the nodes are live periodically If a node fails the server fetches a new virtual machine from the resources pool and replaces the failed one a before failed b failed c after recover Fig 3 Assembly Line Recovering Procedure In Fig.3 after fails all the tuples on it such as   in stream A and  in stream B are lost The state server sends messages to its upstream nodes In the previous section we mentioned that two streams ow into the cluster in the opposite direction For stream A the nearest upstream node of is  while is the nearest upstream node in stream B After and receive the messages they transfer all the tuples in their backup windows to  stores them in their own local joining windows respectively As Fig.3 c shows all the tuples in the downstream nodes local joining windows are transferred to  too And they are kept in the s backup windows We divide the recovery time into three parts 1 node 
B Fault Tolerance Issue in a Cluster 
34 
34 


E n n n n n n f n n n n f E E E f Ef f f f f n n n n E f f f E E Ef f f E f E f E Ef E Ef f f 
r t c T t A r t c A A r t c A B B r t c B A A r t c A B B r t c B A c c A B B A r t A B A B B A c A B c A A A A B B A B c m m m m m m A B m i A B A 003 B A B A B m m m m A B 
f E f f f f f 
c m  i m  i m  i 002 m  i m  i m  j m  i m  j m m  f  1 
T T T N T T T Node k Node Node N T T T k k N T T T k k N T T T k k N T T T k k T t T N k N k T T k k N k N k t t t T t t N N T n W k k W n n W k W W T n            W W i W W m i W W m i m i m i W W W W   m m W W R 
    
    2 1 2                  002      1    002 002 002 002  1 1 2  2  1  1 2   2  1  1 2 2    1  1  1 2 2   1  2  1 1  1  1 2 2 1 2  1 2 2 1  1 2  4 2  2 1 2 100  4 2 4 100  
recovery time  2 backup tuples transmission time and 3 computing time  In order to recover the results of joining new started virtual machine is required to deal with all the data produced during the procedure In the computing time the VM can join pairs of tuples During the entire recover period stream A pushes tuples into and there are tuples in the s backup window These tuples are joined with the tuples in the local joining window of stream B at  therefore there are 9 joins to be done Also stream B has 10 joins to be done If the objective is to recover all the data at one node the following condition should be satisìed 11 and we can get from Eq 11 12 From Eq 12 when is smaller the computing time is smaller too represents the computing capability of the nodes i.e the more smaller is the more powerful the node is The arrival rate or is proportional to  The relationship among  and can be described as 13 The same relationship exists among  and We consider that or is proportional to  and reciprocal to  In our assembly pipeline architecture when a node fails a new one will take charge of the job at the failed one immediately All the data at the failed node are recovered from its nearest neighbors In practice we can recover all the lost data in theory when the failed nodes are non-adjacent However when two or more nodes which are adjacent fail simultaneously it is impossible to recover all the lost data Because some parts of the backup data retained in these failed nodes are totally lost We assume there are work nodes in a cluster and neighboring nodes fail at the same time The joining results from the tuples on these failed nodes can not be recovered We consider the tuples in the local joining window at a node as one unit For the units and at the node  the number of the units produced by nally is 14 and the number of the units by is 15 Therefore the number of the lost tuple units is 16 If there are nodes fail the total number of the lost tuple units is  However we calculate the tuple unit pair twice because both and are at the failed nodes Thus the repeated number of tuple units is 17 Moreover the lost tuple units at and can be recovered while units at are lost therefore 18 tuple units recovered Since and calculate twice there are number of tuple units can be recovered From the above analysis we know that there are 19 tuple units lost and the recovery rate is 20 From Eq.\(20 when only one node or non-neighboring nodes fail all the data can be recovered However when two or more nodes which are adjacent fail we can only guarantee one of the parts of data can be retrieved V E XPERIMENTES AND E VALUATIONS In this section we describe our DAP architecture to deal with joining bi-directional data streams in a cluster If one or more nodes fail our recovery mechanism is started to recover the joining results immediately by adding new virtual machines A Experiment settings We build up our joining data streams system with fault tolerance on the Han Hai Xing Yun cloud built by University of Science and Technology of China There are two source nodes injecting data streams into a cluster Whatês more we set different number of worker nodes which are responsible for joining operation Our DAP system submits the joining results to a documents server The documents server communicates with the worker nodes through MQ in the REQ-REP model Each Virtual machine is with an Intel\(R Core\(TM Duo 2.4GHZ CPU 4GB memory 40GB disk and runs CentOS6.4 32bit OS All the nodes are connected by 100M Ethernet 
002 1 2 1  1 1  1   1 1  2  1 2 2 
             003 004  003 003                                      
35 
35 


10000 1 1 
n n n 
Fig 4 The Relationship between Time Consuming and Cluster Size Fig 5 Joining Two-way Streams Recover Ratio With One Failed Node Fig 6 Joining Two-way Streams Recover Ratio With Adjacent Failed Nodes The result in Fig.5 shows that when one node fails all the data can be recovered if a new virtual machine in the resource 
 N ame T imestamp P honeN umber   N ame T imestamp Email  Name global joining window local joining window n 
B Capability of Joining Bi-Directional Data Streams C Recovery Ratio 
0.1 0.5 1 1.5 2 x 10 5 0 5 10 15 20 25 30 35 40 45 50 Number of Tuples Time Consuming\(minutes The Relationship between Time Consuming and Cluster Size   1 worker node 3 worker nodes 5 worker nodes 1 2 3 4 5 6 0.96 0.98 1 1.02 1.04 1.06 Number of Failed Nodes Recover Ratio Joining Twoäway Streams Recover Ratio With One Failed Node   2ä3 minutes Case 5ä10 seconds Case 1 2 3 4 5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Number of Failed Nodes Recover Ratio Joining Twoäway Streams Recover Ratio With Adjacent Failed Nodes   5 worker nodes 6 worker nodes 7 worker nodes 
The data sets used in the paper are produced by GenerateData software There are 1000 kinds of different tuples in one stream and 1000 kinds of different tuples in the other stream Each pair of tuples with the same in the joining window will be stored as a result In this section we investigate our DAPês capability in processing joining bi-directional data streams We use ve data sets that contain different number of tuples to test the speedup capability There are two symmetrical modules A and B which are responsible for joining operation at each worker node The two modules carry out Algorithm 1 and communicate with each other through MQ in the PUB-SUB model Both modules submit the results to a document server respectively We set different number of joining worker nodes and we test ve different data sets for 10 times and obtain the average results We set the number of the worker nodes as 1 3 and 5 The cluster with 1 worker model is equal to the stand-alone model and we consider it as a baseline case The size of the for each stream is 10000 When the number of the worker nodes is different the size  uted at each worker node is different too The capability of DAP model is proportional to the number of worker nodes The time consumed by the cluster with worker nodes is of the stand-alone model In Fig.4 the consumed time is a little bigger than of the standalone model The reason results from network latency and the process capacity of the document server In addition the communication model between worker nodes and the document server is REQ-REP which is a relatively slow but stable communication protocol In this section we investigate the recoverability of our DAP architecture in joining bi-directional data streams The cases of node failures can be classiìed into two categories including adjacent nodes failures and non-adjacent nodes failures at the same time When non-adjacent nodes fail the cases can be considered as single node failure In order to reduce the nodes recovery time we prepare some new virtual machines in the resources pool The scheduler running on the state server detects whether the worker nodes exist every second If a node failure is detected the scheduler migrates the tasks on the failed node to a new virtual machine and the recovery procedure is started immediately Otherwise the scheduler does not do anything We carry out the DAP system in a cluster and stop one certain node manually to detect the recovery ratio of the lost data Whatês more we carry out our experiments for different cases in terms of the number of worker nodes 
 
36 
36 


pool can be added into the DAP system in a few seconds But when we set the new virtual machine added after 2-3 minutes a little part of results are lost Because the size of the backup window is limited if adding new virtual machines costs a lot of time the backup window could not keep all arriving tuples some of them have to be dropped The longer time of adding new virtual costs the more results are lost We stop some adjacent nodes at the same time and the result are shown in Fig 6 In this case with increasing the number of failed nodes the recovery ratio is decreasing dramatically This is because the backup data stored in the upstream nodes are lost The data that can be recovered are stored at the upstream node of the rst failed one and the downstream node of the last failed one However we know that the probability that the adjacent nodes fail at the same time is small under the case of I.I.D VI R ELATED W ORKS Our proposed Dual-Assembly-Pipeline architecture is novel for data streams joining in a cluster our system is with a decentralized architecture and all the nodes are with the same functionality In Babcock re vie ws the models and issues in a stream system include deìnitions timestamps sliding windows technology and so on Golab also presents some examples of streaming applications and lists the requirements of stream processing such as selections frequent item queries joins and windowed queries etc Ananthanarayanan and Basker et al build up a special data streams joining system named Photon which provides faulttolerance guarantees in the level of data center in In[6 Gedik proposes a method for dealing with data streams joining in the level of the cell processor In Kang presents the basic steps of a sliding window in joining between streams A and B Moreover he also evaluated the performance of the window joining over unbounded streams Golab investigated the sliding window multi-join processing for continuous queries over data streams and evaluates several algorithms on performance under the assumption that all the sliding windows ts in the memory Gu proposes two methods for multi-way windows streams joining in named Aligned T uple Routing A TR and Coordinated Tuple Routing CTR ATR selects a stream as the main stream periodically Some parts of the main stream such as window  will be routed to one certain node according to the workloads of all nodes other streams tuples related to the time window will be routed to the same node too In CRT each tuple is routed to a minimum set of the leastloaded hosts that can cover all the correlated tuples However Guês mechanism is based on a centralized architecture In order to recover the data after a node failure the backup data may be retained in upstream node In our DAP architecture every nodeês upstream node is its nearest neighboring node VII C ONCLUSION In the paper we propose our Dual-Assembly-Pipeline architecture to carry out bi-directional data streams joining and provide a fault-tolerance mechanism based on it The experiments show that Dual-Assembly-Pipeline architecture can improve the performance of joining two-way streams Moreover when non-adjacent nodes fail all data can be recovered if the new started VMs can be recovered in a few seconds And parts of data can be reinstated if neighboring nodes fail at the same time A CKNOWLEDGMENT This research is partially supported by the National Science Foundation of China No 61202416 Shenzhen Strategic Emerging Industry Development Founds No JCYJ20120615130218295 and the National Key Technology R&D Program under Grant No 2012BAH17B03 R EFERENCES  S Muthukrishnan 
   
t t T 
Data Streams Algorithms and Applications Issues in Data Stream Management Models and Issues in Data Stream Systems Processing Sliding Window MultiJoins in Continuous Queries over Data Streams On Random Sampling over Joins CellJoin a parallel stream join operator for the cell processor Evaluating Window Joins over Unbounded Streams Smapling From a Moving Window over Streaming Data Computing iceberg queries efìciently Space-efìcient online computation of quantile summaries Adaptive Load Diffusion for Multiway Windowed Stream Joins The space complexity of approximating the frequency moments Optimal histograms with quality guarantees Approximate Join Processing Over Data Streams Processing Data-Stream Join Aggregates Using Skimmed Sketchs Memory Limited Execution of Windowed Stream joins Sampling from a moving window over streaming data Photon Fault-tolerant and Scalable Joining of Continuous Data Streams Messaging for Many ApplicationsÖZeroMQ S4 Distributed Stream Computing Platform 
 Foundations and Trends in Theoretical Computer Science Vol 1 No 2\(2005 117-236 2005  Lukasz Golab and M T amer Ozsu  SIGMOD Record Vol 32 No 2 June 2003  Brain Bacock Shi vnath Bab u and etc  ACM PODS 2002 June 3-6 Madison Wisconsin USA  Lukasz Gobal and M T ramer Ozsu  Proceedings of the 29th VLDB Conference Berlin Germany 2003  Surajit Chaudhuri Rajee v Motw ani and V i v ek Narasayya  ACM SIGMOD Record Vol 28 Issue 2 June,NY USA 1999  Bugra Gedik Rajesh R Borda wekar and Philip S Y u  The VLDB journal Volume 18 Issue 2 pp 501-519 2009  Jae w oo Kang Jef fre y F  Naughton and Stratis D V iglas  Data Engineering 2003 Proceedings 19th International Conference on 5-8 March 2003  Brain Babcock Mayur Datar  Rajee v Motw ani  Annual ACM-SIAM Symposium on Discrete Algorithm\(SODA 2002  M F ang N Shi v akumar  and H Garcia-Molina  In Proc of the 1998 Intl Conf on VLDB pages 299310 1998  M Greenw ald and S Khanna  In Proc of the 2001 ACM SIGMOD Intl Conf on Management of Data pages 58-66 2001  Xiaohui Gu Philip S Y u and Haixun W ang  Data Engineering 2007 ICDE 2007 IEEE 23rd International Conference on 15-20 Aprial 2007  N Alon Y  Matias and M Sze gedy   In proc of the 1996 Annual ACM Symp on Theory of Computing pages 20-29 1996  H Jagadish N K oudas etc  In Proc of the 1998 Intl Conf on VLDB pages 275-286 1998  A Das J Gehrk e and M Riedw ald  Proc of SIGMOD 2003  S Ganguly  M Garof alakis and R Rastogi  Proc of EDBT 2004  U Sri v asta v a and J W idom  Proc of VLDB pages 324-335 2004  B Babcock M Datar  and R motw ani  in Proc of the 2002 ACM-SIAM Symp on Discrete Algorithms pages 633-634 2002  Rajagopal Ananthanarayanan V enkatesh Bask er   SIGMOD 13 ACM New York NY USA pp 577-588 2013  P  Hintjens  OêREILY pages 3-4 2013  Neume yer L Robbins B  Data Mining Workshops ICDMW 2010 IEEE International Conference on pages 170-177 2010 
37 
37 


We now evaluate the overhead of the FChain system Table II lists the CPU cost of each key module in FChain We observe that most modules in FChain is light-weight The most computation-intensive module is the abnormal change point selection component which is triggered only when a performance anomaly occurs F Chain also distributes the change point computation load on different hosts and executes them in parallel to achieve scalability The online validation takes about 30 seconds for each component since we need some time to observe scaling impact for deciding whether we have made a pinpointing error However the online validation is only performed on those suspicious components pinpointed by the integrated fault diagnosis module The FChain daemon running inside the Domain 0 of each host imposes less than 1 CPU load and consumes about 3MB memory during normal execution IV R ELATED W ORK Our work is rst closely related to previous black-box fault localization schemes F or example NetMedic p rovided detailed application-agnostic fault diagnosis by learning inter-component impact NetMedic rst needs to assume the knowledge of the application topology To perform impact estimation NetMedic needs to nd a historical state that is similar to the current state for each component However for previously unseen anomalies we might not be able to nd a historical state that is similar to the current state for the faulty components Under those circumstances NetMedic assign a default high impact value whic h sometimes lead to inaccurate diagnosis results as shown in Section III In comparison FChain can diagnose previously unseen anomalies and does not assume any prior application knowledge Oliner et al proposed to compute anomaly scores using the histogram approach and correlates the anomaly scores of different components to infer the inter-component inîuence graph As shown in Section III it is difìcult for the histogram-based anomaly detection to perform online fault localization over suddenly manifesting faults Moreover unrelated components can have indirect correlations caused by workload uctuations which will cause their system to raise false alarms In comparison FChain is more robust to different types of faults and workload uctuations To achieve black-box diagnosis researchers have also explored various passive network trafìc monitoring and analysis techniques such as Sherlock  O ri on 27 S N A P  28 However those analysis schemes can only achieve coarsegrained machine-level fault localization Additionally during our experiments we found that previous network trace analysis techniques cannot handle continuous data stream processing applications due to the lack of gaps between packets for extracting different network ows Project5 and E2EProf performed cros s correl a t i ons bet w een mes s a ge traces to derive causal paths in multi-tier distributed systems WAP5 e x t e nds t h e b l ack-box caus a l p at h a nal y s i s t o support wide-area distributed systems Orion di s c o v e rs dependencies from network trafìc using packet headers and timing information based on the observation that the trafìc delay distribution between dependent services often exhibits typical spikes LWT propos ed t o di s c o v e r t he s i mi l a ri t y of the CPU usage patterns between different VMs to extract the dependency relations hips between different VMs However as shown in our experiments dependency-based fault localization techniques are not robust which can make frequent pinpointing mistakes due to various reasons e.g the back pressure effect in distributed applications common network services pinpointed as culprits Furthermore existing dependency discovery techniques need to accumulate a large amount of trace data to achieve reasonable accuracy Particularly network trace based techniques only support requestand-reply types of applications which fail to discover any dependency in continuously running applications such as data stream processing systems In contrast FChain provides online fault localization which does not require any training data for anomalies or a large amount of training data for normal behaviors FChain is fast which can quickly localize faulty components with high accuracy af ter the performance anomaly is detected A urry of research work has proposed to use end-to-end tracing for distributed system debugging Magpie  i s a request extraction and workload modelling tool that can record ne-grained system events and correlate those events using an application speciìc event sch ema to capture the control ow and resource consumption of each request Pinpoint t ak es a request-oriented approach to tag each call with a request ID by modifying middleware platform and applies statistical methods to identify components that are highly correlated with failed requests Monitor t racks t he reques t s e xchanged b et ween components in the system and performs probabilistic diagnosis on the potential anomalous components X-Trace i s a n integrated cross-layer crossapplication tracing framework which tags all network operations resulting from a particular task with the same task identiìer to construct a task tree Spectroscope can di agnos e p erformance anomal i e s b y comparing request ows from two executions In contrast our approach does not require any instrumentation to the 
G FChain System Overhead Measurements 
System Modules CPU cost 
VM monitoring 6 attributes 1.03 0.09 m illisec onds Normal uctuation modeling 22.9 2 millis econds 1000 samples Abnormal change point selection 602.4 105.2 m illisec onds 100 samples Integrated fault diagnosis 22 1 microseconds Online validation per-component 30 1 seconds TABLE II FC HAIN OVERHEAD MEASUREMENTS  size for the DiskHog fault in Hadoop The reason has been described in Section III-A Generally the look-back window should be long enough to capture the fault manifestation We are currently investigating an adaptive look-back window conìguration scheme by examining the metric changing speed 
     
206 
29 
29 


application or middleware platform to collect request ows Thus it is much easier to deploy FChain in large-scale IaaS clouds Blacksheep correl a t e s t he change poi nt of s y s t em-l e v el metrics e.g cpu usage with the change in count of Hadoop application states i.e events extracted from logs of DataNodes and TaskTrackers to detect and diagnose the anomalies in a Hadoop cluster Kahuna-BB correl a t e s b l ack-box dat a system-level metrics and white-box data Hadoop console logs across different nodes of a MapReduce cluster to identify faulty nodes In comparison FChain is a black-box fault localization system which is application-agnostic without requiring any knowledge about the application internals We believe that FChain is more practical and attractive for IaaS cloud systems than previous white-box or gray-box techniques V C ONCLUSION In this paper we have presented FChain a robust blackbox online fault localization system for IaaS cloud computing infrastructures FChain can quickly pinpoint faulty components immediately after the performance anomaly is detected FChain provides a novel predictability-based abnormal change point selection scheme that can accurately identify the onset time of the abnormal behaviors at different components processing dynamic workloads FChain combines both the abnormal change propagation knowledge and the inter-component dependency information to achieve robust fault localization FChain can further remove false alarms by performing online validation We have implemented FChain on top of the Xen platform and conducted extensive experimental evaluation using IBM System S data stream processing system Hadoop and RUBiS online auction benchmark Our experimental results show that FC hain can achieve much higher accuracy i.e up to 90 higher precision and up to 20 higher recall than existing schemes FChain is light-weight and non-intrusive which makes it practical for large-scale IaaS cloud computing infrastructures A CKNOWLEDGMENT This work was sponsored in part by NSF CNS0915567 grant NSF CNS0915861 grant NSF CAREER Award CNS1149445 U.S Army Research Ofìce ARO under grant W911NF-10-1-0273 IBM Faculty Awards and Google Research Awards Any opinions expressed in this paper are those of the authors and do not necessarily reîect the views of NSF ARO or U.S Government The authors would like to thank the anonymous reviewers for their insightful comments R EFERENCES   A m azon E las tic Com pute Cloud  h ttp://a w s  a m azon com ec2   V i rtual c om puting lab  http://vcl ncs u  e du  P  Barham  A  D onnelly  R I s aacs  a nd R M o rtier   U s ing m agpie f or request extraction and workload modelling in 
 2004  M  Y  Chen A  A ccardi E  K icim an J  L lo yd D  P atters on A  F ox and E Brewer Path-based failure and evolution management in  2004  R F ons eca G  P o rter  R H  K atz S  S h enk e r  and I  S toica X T race A pervasive network tracing framework in  2007  I  Cohen M  G o lds z m i dt T  K elly  J  S ym ons  a nd J  S  Chas e Correlating Instrumentation Data to System States A Building Block for Automated Diagnosis and Control in  2004  I  C ohen S  Z h ang M  G o lds z m i dt J  S ym ons  T  K elly  a nd A  F ox Capturing indexing clustering and retrieving system history in  2005  S  D uan S  Bab u  a nd K  M unagala F a A s ys tem for a utom ating failure diagnosis in  2009  S  K andula R Mahajan P  V erkaik S  A garw al J  P a dhye a nd V  Bahl Detailed diagnosis in computer networks in  2009  A  J  O liner  A  V  K ulkarni and A  A ik en  U s ing c orrelated s u rpris e to infer shared inîuence in  2010  P  Bahl R Chandra A  G r eenber g  S  K andula D  A  M altz and M Zhang Towards highly reliable enterprise network services via inference of multi-level dependencies in  2007  Z  G ong X  G u  a nd J  W ilk es   P RE S S  P Redicti v e E las tic ReS ource Scaling for Cloud Systems in  2010  H  N guyen Y  T a n and X  G u P A L  P ropagation-a w are a nom aly localization for cloud hosted distributed applications in  2011  B Gedik H Andrade K L  W u P  S  Y u and M  D oo SP ADE  t he system s declarative stream processing engine in  2008  A pache H adoop S y s tem   http://hadoop apache  or g/co re   Rice uni v e rs ity bidding s y s tem   http://rubis  objectw eb  o r g   M Ben-Y e huda D  B reitgand M F actor  H  K o lodner  V  K r a v ts o v  and D Pelleg NAP a building blo ck for remediating performance bottlenecks via black box network analysis in  2009  Y  T a n X  G u  a nd H  W a ng  A dapti v e s ys tem anom aly prediction f or large-scale hosting infrastructures in  2010  D  L  M ills   A b rief his t ory o f N T P tim e m e m o irs o f a n i nternet timekeeper  2003  Y  T a n H  N guyen Z  S h en X  G u C V e nkatram ani and D  R ajan PREPARE Predictive Performance Anomaly Prevention for Virtualized Cloud Systems in  2012  M  Bas s e ville and I  V  N ikiforo v   Prentice-Hall Inc 1993  L  Cherkaso v a  K  O zonat N Mi J  S ym ons a nd E  Sm irni  Anom aly application change or workload change towards automated detection of application performance anomaly and change in  2008  P  Barham and e t al   X e n a nd the a rt of virtualization  i n  2003  T he ircache p roject  h ttp://www.ircache.net  H ttperf  h ttp://code google com  p htt p er f  S  K u llback  T h e ku llback-leibler distance  1987  X  Chen M  Z hang Z  M  M a o and P  B ahl  A utom ating n etw ork application dependency discovery experiences limitations and new solutions in  2008  M Y u  A  G reenber g  D  M altz J  Re xford L  Y u an S  K andula and C Kim Proìling network performance for multi-tier data center applications in  2011  M K  A guilera J  Mogul J  W iener  P  R e ynolds  a nd A  Muthitacharoen Performance debugging for distributed systems of black boxes in  2003  S  A g arw ala F  A l e g re K  S chw a n and J  M ehalingham  E 2E P r of A utomated end-to-end performance management for enterprise systems in  2007  P  Re ynolds  J  L  W iener  J  C M ogul M  K  A guilera and A  V ahdat  WAP5 black-box performance debugging for wide-area systems in  2006  R Apte L  Hu K  S chw a n and A  G hosh L ook W ho s T alking Discovering dependencies between virtual machines using cpu utilization in  2010  G Khanna I  L aguna F  A rshad an d S Bagchi Distr ibuted diagnosis of failures in a three tier e-commerce system in  2007  R R S a m b as i v an A  X  Z heng M  D e Ros a  E  K re v at S  W h itm an M Stroucken W Wang L Xu and G R Ganger Diagnosing performance changes by com paring request ows in  2011  J  T a n a nd P  N a ras i m h an  RA M S and B lackS h eep I nferring w h ite-box application behavior using black-box techniques CMU Tech Rep 2008  J  T a n X  P a n E  Marinelli S  K a vulya R  G andhi a nd P  N a ras i m h an Kahuna Problem diagnosis for mapreduce-based cloud computing environments in  2010 
OSDI NSDI NSDI OSDI SOSP ICDE SIGCOMM DSN SIGCOMM CNSM SLAML SIGMOD ICAC PODC Computer Communication Review ICDCS Detection of abrupt changes theory and application DSN SOSP The American Statistician OSDI NSDI SOSP DSN WWW HotCloud SRDS NSDI NOMS 
207 
30 
30 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





