Mining for Useful Association Rules Using the ATMS Yu e X u a n d Yu e f e n g L i School of Software Engineering and Data Communications Queensland University of Technology Brisbane Queensland 4001 Australia  yue.xu,y2.li  qut.edu.au Abstract Association rule mining has made many achievements in the area of knowledge discovery in databases Recent years the quality of the extracted association rules has drawn more and more attention from researchers in data mining community One big concern is with the size of the extracted rule set Very often tens of thousands of association rules are extracted among which many are redundant thus useless In this paper we 223rst analyze the redundancy problem in association rules and then propose a novel ATMS-based method for extracting non-redundant association rules 1 Introduction Association rule mining has become one of the most important and well researched techniques of data mining It aims to extract interesting c orrelations and associations among sets of items in large datasets Two phases are involved in mining association rules extracting frequent itemsets and generating association rules from the frequent itemsets with the constraints of minimal con\223dence The rules whose con\223dence is larger than a user-speci\223ed minimum con\223dence threshold are considered interesting or useful Various approaches have been proposed most f them are focused on developing novel algorithms and data structures to aid ef\223cient computation of such rules 1 6 especially on improving the ef\223ciency of generating frequent itemsets However the quality of the mined association rules hasn\220t drawn adequate attention As a matter of fact very often the resulting rule base can easily contain several thousands of rules among which are many redundancies and thus useless in practice While some efforts have been done on reducing the size of the extracted rule base by de\223ning various interest measures incorporating constraints into mining proce ss or designing speci\223c templates to mine for restricted rules 4 5 8 the u s e les s redundancy in the extracted rule base is still a problem to solve The 223rst phase f association rule mining generates all frequent itemsets The second phase can be viewed as a process of adding new extracted association rules into the current rule set in an accumulating manner The extracted rules work as the beliefs that represent the epistemic state of the decision making system of the domain From the logic point of view when adding a new extracted rule into the rule set the reasoning system needs to maintain a consistent and non-redundant rule set The techniques that enable a reasoning system to update its belief set in dealing with new information is called belief rev ision Many approaches have been proposed in the area of belief revision Among them the truth maintenan ce systems ATMS provide a powerful mechanism In this paper a preliminary investigation of applying the technique of the Assumption-based Truth Maintenance stem ATMS 3 t o association r ule mining is presented Next section will discuss some related approaches The task of association rule ining and the redundancy problem in association rules are discussed in Section 3 Section 4 gives a brief review to the ATMS In Section 5 we present the algorithms that generate non-redundant association rules based on the ATMS technique Section 6 concludes the paper 2 Related Work One approach to address the quality of association rules is to apply constraints to generat e only those association rules that are interesting to users based on some constraints instead of all the association rules 5 and 8  propos ed some algorithms that incorporat e item constraints to the process of generating frequent itemsets Item constraints restrict the items and combinatio n of items that are interesting according to users association rules are generated from those frequent itemsets Also some works have been done on measuring association rules with interestingness parameters These approaches focus on pruning the association rules to get more general or informative association Proceedings of the 2005 International Conference on Computati onal Intelligence for Modelling, Control and Automation, and Inter national Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC\22205 0-7695-2504-0/05 $20.00 \251 2005  IEEE Proceedings of the 2005 International Conference on Computati onal Intelligence for Modelling, Control and Automation, and Inter national Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC\22205 0-7695-2504-0/05 $20.00 \251 2005  IEEE 


rules based on interestingness parameters The approach proposed in 2 i nte g rates v ari ous constraints into mining process including consequent const raint and minimal improvement constraint as well The consequent constraint is used to restrict rules with certain consequent speci\223ed by the user while minimal improvement constraint is used to simplify the antecedents of rules based on item\220s contribution to the con\223dence and therefore prune association rules that have more speci\223c antecedent t o not make more contribution to the con\223dence Another approach is to use a taxonomy of items to extract generalized association rules 4 i.e to generate rules between items ets that belong to different abstract levels in the taxonomy especially between high abstract levels aiming at reducing e number of extracted rules The approaches mentioned above aim at reducing the number of extracted rules but eliminating redundancy of rules is not a focus The approaches proposed in 9 a nd 7 focus on e x t ract i n g non-redundant rul es  B ot h o f t hem make se of e sure of e Galois connection to extract non-redundant rules between frequent cl osed itemsets insdead of rules between frequent itemsets Their results show that any rule between itemsets is equivalent to some rule between closed itemsets Thus many redundant rules can be eliminated One difference between the two approaches is the de\223nition of redundant rules 9 prefers both s horter antecedent and shorter consequent among rules which have the same con\223dence while 7 d e\223 nes non-redundant rules are those which have minimal antecedent and maximal consequent Our de\223nition of non-redundant rules is similar to that of B u t w e u s e a v ery dif ferent mechanis m t o p rune the redundant rules Most importantly our method does not have to calculate the closed itemsets while the method proposed in 7 m us t calculate clos ed items ets and the generators of the closed itemsets as well 3 Association Rule Mining Problems 3.1 Association Rules Let I   I 1 I 2 I m  beasetof m distinct items T be transaction that contains a set of items such that T 002 I  D be a database containing different identi\223able transactions An association rule is an implication in the form of X 003 Y where X Y 004 I are sets of items called itemsets The rule means that X implies Y  Various metrics describe the utility of an association rule The most common ones are the percentage of all t ransactions containing X 005 Y which is called the support and the percentage of transactions containing Y among transactions containing X which is called con\223dence of the rule Association rule mining is to 223nd out association rules that satidfy the prede\223ned minimum support and con\223dence from a given database TID Items 1 ACD 2 BCE 3 ABCE 4 BE 5 ABCE 6 BCE Table 1 A simple database 1-itemsets 2-itemsets 3-itemsets 4-itemsets  A  3/6  AB  2/6  ABC  2/6  ABCE  2/6  B  5/6  AC  3/6  ABE  2/6  C  5/6  AE  2/6  ACE  2/6  E  5/6  BC  4/6  BCE  4/6  BE  5/6  CE  4/6 Table 2 Frequent itemsets minisupp=2/6 The problem is usually decomposed into two subproblems One is to 223nd those itemsets whose occurrences exceed a prede\223ned threshold in the dat abase those itemsets are called frequent itemsets This step is computationally intensive The second subproblem is to generate association rules from those frequent itemset s with the constraints of minimal con\223dence This step is relatively s traightforward Rules of the form X 003 Y  X are generated for itemset Y  for all X 004 Y and X 006  002  We should consider each subset X of Y as an antecedent except for the empty and the full itemset The example in wi l l be us ed t o e xpl ai n our approach throughout the paper The simple database is given in Table 1 which contains 6 transactions The frequent itemsets generated from the database minisupp=2/6 are shown in Table 2 50 association rules which are n in Table 3 are extracted from these frequent itemsets if the minimum con\223dence threshold is set to 2/6 3.2 Redundancy in Association Rules The rules in Table 3 are considered useful based on the prede\223ned minimum con\223dence However after a careful inspection of the rules we can 223nd that some of the rules actually do not contribute new information to the rule set i.e without these rules the rule t doesn\220t lose any information For example rules 1 13 14 and 15 can be derived from rule 37 and all these rules have the same con\223dence Therefore removing rules 1 13 14 and 15 won\220t change the context of the rule set All these rules have the same antecedent and identical con\223dence while rule 37 has the longest consequent which is a superset of the consequent of other rules and thus brings more information than e other rules Therefore we consider rules 1 13 14 and 15 are redundant to rule 37 On the other hand rules 25 32 and 47 have the same consequent as rule 6 does but have a longer Proceedings of the 2005 International Conference on Computati onal Intelligence for Modelling, Control and Automation, and Inter national Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC\22205 0-7695-2504-0/05 $20.00 \251 2005  IEEE Proceedings of the 2005 International Conference on Computati onal Intelligence for Modelling, Control and Automation, and Inter national Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC\22205 0-7695-2504-0/05 $20.00 \251 2005  IEEE 


Rule No Rules Rule No Rules 1 A 002 B 2/3 26 AB 002 C 1 2 A 002 C 1 27 AC 002 B 2/3 3 A 002 E 2/3 28 AC 002 E 2/3 4 B 002 A 2/5 29 AE 002 C 1 5 B 002 C 4/5 30 AE 002 B 1 6 B 002 E 1 31 BC 002 A 1/2 7 C 002 A 3/5 32 BC 002 E 1 8 C 002 B 4/5 33 BE 002 A 2/5 9 C 002 E 4/5 34 BE 002 C 4/5 10 E 002 A 2/5 35 CE 002 B 1 11 E 002 B 1 36 CE 002 A 1/2 12 E 002 C 4/5 37 A 002 BCE 2/3 13 A 002 BE 2/3 38 B 002 ACE 2/5 14 A 002 CE 2/3 39 C 002 ABE 2/5 15 A 002 BC 2/3 40 E 002 ABC 2/5 16 B 002 AE 2/5 41 AB 002 CE 1 17 B 002 CE 4/5 42 AC 002 BE 2/3 18 B 002 AC 2/5 43 AE 002 BC 1 19 C 002 AB 2/5 44 BC 002 AE 1/2 20 C 002 BE 4/5 45 BE 002 AC 2/5 21 C 002 AE 2/5 46 CE 002 AB 1/2 22 E 002 AC 2/5 47 ABC 002 E 1 23 E 002 BC 4/5 48 ABE 002 C 1 24 E 002 AB 2/5 49 ACE 002 B 1 25 AB 002 E 1 50 BCE 002 A 1/2 Table 3 Association rules miniconf=2/6 antecedent to be satis\223ed than rule 6 needs That means rules 25 32 and 47 do not bring more information but require more in order to be 223red In this case we consider rules 25 32 and 47 are redundant to rule 6 The following de\223nition de\223nes such kind of redundant rules De\223nition 3.1 Redundant rules Let X 003 Y and X 002 003 Y 002 are two association rules which have e same con\223dence X 003 Y is said a redundant rule to X 002 003 Y 002 if X 002 002 X and Y 002 Y 002  The following propositions can help to prune out most redundant rules ion 3.1 Let r 1  X 003 Y 1 and r 2  X 003 Y 2 be two association rules r 1 and r 2 have the same antecedent If conf r 1   conf r 2  and Y 1 004 Y 2  then 007 003 b 2 002  004  003 003 004  003 is redundant where 004  Y 1 t Y 2  005 X and 2 002 is the power set of 004  The following example illustrates the correctness of the proposition Suppose we have generated rules 15 and 37 in Table 3 i.e A 003 BC and A 003 BCE  Since the two rules have the same con\223dence we get the following equation supp  ABC  supp  A   supp  ABCE  supp  A  It\220s easy to 223nd that 007 003 b A B C AB AC BC  which is the power set of  A B C t A B C E  without the intersection set the following quation is true supp  ABC  supp  003   supp  ABCE  supp  003  According to the de\223nition of the redundant rules 003 003 ABC  003 is redundant to 003 003 ABCE  003 because the two rules have the same antecedent and con\223dence and the consequent of the 223rst rule is shorter than that of the second rule This gives the result that A 003 BC  B 003 AC  C 003 AB  BC 003 A  AC 003 B and AB 003 C are redundant to the rules A 003 BCE  B 003 ACE  C 003 ABE  BC 003 AE  AC 003 BE and AB 003 CE  respectively Following De\223nition 3.1 we have the proposition 3.2 The correctness f this proposition can be proved directly by De\223nition 3.1 ion 3.2 Let r 1  X 1 003 Y and r 2  X 2 003 Y be two association rules r 1 and r 2 have e same consequent If conf r 1   conf r 2  X 1 004 X 2  then e rule X 2 003 Y is redundant to the rule X 1 003 Y  These propositions will not be helpful if we don\220t have an ef\223cient way to identify rules with identical antecedents or identical consequents In the following sections we present a method that solves this problem by using truth maintenance techniques 4 A Review to de Kleer\220s ATMS As mentioned above the ATMS 3 pro vides an attrictive mechanism to maintain and update the belief set The ATMS itself doesn\220t take part in any problem solving Whenever the problem solving system draws a conclusion or wants to add a new rule it passes the conclusion or the rule to the ATMS The ATMS then incorporates the new information into the belief set and maintains consistent and minimal non-redundant environments to be troduced below for each datum supplied by the problem solver Based on the belief set and the dependency among data nodes the problem solver makes further inferences towards a solution to the domain problem Even though an association rule mining system is not a typical problem solving system that uses an ATMS to con\223rm its inference conclustions the ATMS techniques can be used in association rule mining for avoiding redundancy in the extracted rules Section 5 will present an approach that combines the propositions discussed in Section 3 with the basic ATMS algorithm to extract non-redundant rules In this section we will brie\224y descrebe the ATMS 3 200 Node A node in an ATMS represents any datum suplied by the problem solver The In the association rule mining system presented in this paper each datum is an itemset 200 Assumptions The problem solver speci\223es a set of distinguished nodes which are called assumptions The assumptions are presumed to be true by the problem solver In the case of association rule mining all the items form the set of assumptions Proceedings of the 2005 International Conference on Computati onal Intelligence for Modelling, Control and Automation, and Inter national Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC\22205 0-7695-2504-0/05 $20.00 \251 2005  IEEE Proceedings of the 2005 International Conference on Computati onal Intelligence for Modelling, Control and Automation, and Inter national Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC\22205 0-7695-2504-0/05 $20.00 \251 2005  IEEE 


200 Justi\223cations A justi\223cation is an implication of the form a 1 n a 2  n a n 003 b where each a i is a node Justi\223cations are supplied by the problem solver The intent of the justi\223cation is to inform the ATMS that when the antecedent nodes are derivable then the consequent node is also derivable In the association rule mining algorithm present in this paper each justi\223cation is a potential association rule generated by the algorithm 200 Environments A set of assumptions is called an environment of a node if the node holds under is environment An environment in the association rule mining system is an itemset 200 Label The label of a node cont ains all environments of the node Each environment in e label is a nonredundant set of assumptions from which e node can be derived 200 Context The context of an environment contains all nodes which can be derived from the environment 200 Nogood There is a special node called Nogood in an ATMS Any environment in which contradiction is derived is included in the label of Nogood The ATMS is provided with a set of assumptions and justi\223cations The task of the ATMS is to ef\223ciently determine the label for each node and context for each environment In an ATMS each node is associated with a label and a set of justi\223cations denoted as  node label justif ications   Both the label and justi\223cations for a node can be explained as material implications Given a node n with label  A 1 A 2     B 1 B 2     and justi\223cations   x 1 x 2     y 1 y 2      the meaning of the label of n is that the conjunction of assumptions in each environment makes n true In general each justi\223cation is non-redundant That is deleting any element in a justi\223cation will destroy the implication relation of this justi\223cation to its node For any two justi\223cations of a node usually these two justi\223cations don\220t imply each other If one justi\223cation can be inferred from the other then the effect of this justi\223cation will be covered by the latter one The same rules al so apply to the environments for a node That is ny environment is non-redundant and any two environments of a node have at least one different assumption These are the important features of the ATMS that we will use to 223nd non-redundant association rules de Kleer didn\220t give a formal semantics of the ATMS The behaviour of the ATMS is described by operational procedures Usually supplied with a set of justi\223cations 002 and a proposition p  the ATMS is required by e problem solver to determine whether or not p is believed in 002 The ATMS is incremental After processing all the usti\223cations in 002 the label  p  in node 005 p provides all the environments under which p can be believed If label  p   then p cannot be believed The main ATMS algorithm label-update  a 1 a k 003 p  is to update the labels of nodes with the addition of a justi\223cation Before the algorithm is called it is supposed that the labels of each antecedent node as well as the consequent node of the j usti\223cation have already been created 002 is the set of justi\223cations currently in the ATMS The algorithm label-update is described as follows Algorithm label-update  a 1 a k 002 p  1 L p   x  x  002 i  k i 1 x i x i 003 label  a i   2 for each environment e 003 L p do 3 if e contains any Nogood then remove e from L p  4 if e subsumes any environments in label  p  then remove e from L p  5 endfor 6 if p is 004 then label  p  L p  nogood  nogood 002 L p  return  7 if L p 005  label  p  then label  p  label  p  006 L p add a 1 a k 002 p as a justi\223cation of p 8 for each justi\223cation r 003 002 such that p is one antecedent of r  label-update  r  9 endif Algorithm label-update\(p is a recursive algorithm In order to maintain a consistent data base after updating the label of p  the algorithm will propagate the new label of p to other propositions which take p as one of their antecedents 5 Association Rule Mining based on the ATMS Let 003 be the set of all frequent itemsets The second subproblem in association rule mining can be viewed as a search problem that searches the space S\002 003 327 003 where S   X,Y   X Y b 003 X 004 Y   for a subspace 005 327 005 002S so that each X,Y  b 005 327 005 is considered a useful association i.e X 003 Y  X is a useful rule and each X 002 Y 002  b 003 327 003  005 327 005 is considered useless The traditional rule generation method examines each point X,Y  in S to determine the associations that satisfy the inimum con\223dence constraint The method is straightforward but the resulting association rules contain a great amount of redundancy This section presents a method that examines e most promising points in a dynamicly shrinking space for non-redundant association rules The method uses a simple heuristic to guide the search and applies the ATMS technique to help prune redundancy According to De\223nition 3.1 the rules with shorter antecedent and longer consequent have better Proceedings of the 2005 International Conference on Computati onal Intelligence for Modelling, Control and Automation, and Inter national Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC\22205 0-7695-2504-0/05 $20.00 \251 2005  IEEE Proceedings of the 2005 International Conference on Computati onal Intelligence for Modelling, Control and Automation, and Inter national Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC\22205 0-7695-2504-0/05 $20.00 \251 2005  IEEE 


chance to be non-redundant Based on this heuristic the point X,Y  in S which has the highest value of Y.size/X.size is examined 223rst and the size of S keeps reduced as more redundant rules are identi\223ed and removed from S  X.size denotes the number of items in X  As discussed in Section 4 the label of a node in the ATMS provides all antecedents of the node and the context of an environment provides all consequents of the environment If the con\223dence of any two elements in the label of a node or in the context of an environment is recognized identical one of the two propositions can be used to prune out redundant rules One of e itations of the original ATMS is that it can only infer results with absolute truth values It cannot represent a plausible conclusion with a degree of belief In most cases labels of nodes are very complicated and probability distributions on assumptions are not independent This makes the probability calculation very dif\223cult Fortunately in the case of association rule mining we can use the supports of itemsets to calculate the con\223dence of each rule straightway The algorithms are described below which take S as input and produce a set of non-redundant association rules as output Let h  X Y  denote Y.size/X.size  Algorithm ATMS ARM Input S Output set of justi\223cations 002 1 002  002 2 while S\005  002 3 r X 002 Y  X  h  X Y  max a,b 002S  h  a b   4 002:=\002 006 r  5 labelupdate-ARM r 6 Remove r from S 7 endwhile Algorithm labelupdate-ARM modi\223ed the labelupdate of the original ATMS by incorporating con\223dence calculation and redundancy pruning using the two propositions In the algorithm below e.supp and power  e  denote the support of itemset e and power set of e  respectively It is assumed that the supports have been calculated before executing the algorithm Each element e in a label label  p  is represented as a pair e  conf  meaning that conf is the con\223dence of rule e 003 p  Algorithm abelupdate-ARM  a 1 a k 002 p  1 if node p  or node a i  doesn\220t exist i 1 k 2 then create node p  with e  p   p 1  create node a i  with e  a i   a i 1  3 L p   x  c  x  002 i  k i 1 x i  c   p 006 x  supp/x.supp  x i  c i 003 label  a i   4 for each environment e  c 003 L p do 5 if c<minconf then remove e  c from L p 6 endfor 7 if L p  e  p  or L p  002 then 8 002:=\002  a 1 a k 002 p   S  S a 1 007  007 a k a 1 007 a k 007 p  return 9 label  p  label  p  006 L p 10 if b e i  c i 003 label  p  and e i t p 005  002 then 11 remove e i  c i from label  p  12 if b e i  c i e j  c j 003 label  p   c i  c j  e i n e j 13 then S  S e j p 006 e j  002:=\002  e j 002 p  14 for each e  c 003 label  p  do 15 context  e  context  e  006 p  c  if b p i p j 003 context  e   c i  c j and p i n p j  then for each 003 003 power  004   004  004   p i t p j  006 e do S  S 003,\004 002:=\002  003 002 004  003  16 endfor 17 endfor 18 for each justi\223cation r 003 002 such that p is one antecedent of r  labelupdate-ARM  r  In the algorithm above step 7 is to check whether the newly added rule makes any contribution to the existing label If it doesn\220t the newly added rule should be removed since what it concludes can be implied by other rules Step 10 is to remove implications of the form A 003 AB that are not considered valid accoring to the de\223nition of association rules but might be deduced by the propagation of labels Steps 12 to 13 and steps 14 to 17 implement e propositions 3.2 and 3.1 respectively The algorithm proposed here focuses only on redundancy pruning but doesn\220t address the problem of consistency check For classical logical reasoning a contradiction is a con\224ict among absolute truth values But in the case of plausible ng like association e mining the truth value of a statement is with a degree of uncertainty rather than absolute true or false The consistency check for plausible reasoning is much more complicated than in the case of classical logical reasoning This issue is important but little effort has been made This topic will be disscused in a forthcoming paper The effectiveness and ef\223cency of the algorithm can be demonstrated by the example dataset mentioned in Section 3 Initially the search space S contains 50 elements Based on the heuristic value h  these elements are examined in the following order Proceedings of the 2005 International Conference on Computati onal Intelligence for Modelling, Control and Automation, and Inter national Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC\22205 0-7695-2504-0/05 $20.00 \251 2005  IEEE Proceedings of the 2005 International Conference on Computati onal Intelligence for Modelling, Control and Automation, and Inter national Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC\22205 0-7695-2504-0/05 $20.00 \251 2005  IEEE 


 A,ABCE  ABCE  C,ABCE  E ABCE   A ABC   A ABE   A ACE   BCE  AB  CE  AB,ABCE  CE,ABCE  AB,ABC  CE,BEC  ABC,ABCE  ABCE   After examining the 223rst 5 poi nts we get the following results label  A   A 1   label  BCE   A 2  3   label  B   B 1   label  ACE   B 2  5   label  C   C 1   label  ABE   C 2  5   label  E   E 1   label  ABC   E 2  5   label  BC   A 2  3   context  B   ACE 2  5   context  C   ABE 2  5   context  E   ABC 2  5   context  A   BCE 2  3 BC 2  3  context  A  indicates that two consequents of A have the same con\223dence and one is a subset of the other Acording to Proposition 3.1 step 17 in Algorithm labelupdate-ARM six elements are removed from S  A,ABC  B,ABC  C,ABC  AB,ABC  BC,ABC and AC,ABC  Similarly examining A,ABE and A,ACE makes another 12 points removed from S which include A,ABE  B,ABE   E,ABE   AB,ABE  AE,ABE  BE,ABE  A,ACE  C,ACE  E,ACE  AC,ACE   AE ACE  and CE,ACE  That is after examining e 223rst 7 points in S  18 rules are eliminated and the search space S is therefore reduced by 36 This reduction is signi\223cant not only because the reduced space makes the search more ef\223cient but also because the elimination of redundant rules greatly improves the quality of the extracted association rules The result is shown in Table 4 Out of the 50 potential rules 33 redundant rules are eliminated 17 rules are extracted and considered usefull But two of them rules 45 and 50 which are indicated with 013 in Table 4 are actually redundant but missed to be caught The main reason is because the algorithm takes the advantage of the ATMS that helps examine only the rules with the same antecedent or consequent 6 Conclusion The quality of extracted association rules is getting more and more attention in the area of association rule mining One problem with this concern is the presence of redundancy in the extracted association rules This paper presented a novel method that applies the truth maitenance technique ATMS to the association rule mining The basic idea of the method is to take he advantage of he ATMS\220s labels and contexts to easily y the redundancy among association rules Especially this method allows us to 223nd non-redundant rules without having to calculate the closed Rule No Extracted Rules Redundant Rules 2 A 002 C 1 26 29 48 6 B 002 E 1 32,47 11 E 002 B 1 25 30 35,49 41 AB 002 CE 1 43 AE 002 BC 1 17 B 002 CE 4/5 20 C 002 BE 4/5 8 9 23 E 002 BC 4/5 12 37 A 002 BCE 2/3 1 3,13,14,15,27,28,42 7 C 002 A 3/5 44 BC 002 AE 1/2 31 46 CE 002 AB 1/2 36 50 003 BCE 002 A 1/2 38 B 002 ACE 2/5 4,5,16 18,33,34 39 C 002 ABE 2/5 19,21 40 E 002 ABC 2/5 10,22,24 45 003 BE 002 AC 2/5 Table 4 Extracted Rules and Redundant Rules itemsets Two features of the method allow potential extensions to the current model One is the possibility to incorporate constraints to antecedents or consequents by means of the labels and contexts This can restrict the rules to the ones whose antecedents or consequents satify some constraints speci\223ed by the users The other feature is the use of NOGOOD to check the consistency among the extracted rules References  R Agra w a l and R S r i kant  F ast al gori t h ms for mi ni ng association rules In Proceedings of the 20th International Conference on very large data bases  pages 487\205499 1994  R  J  B ayardo R Agra w al  and D Gunopul os Const rai n t based rule g in large dense databases Data Mining and Knowledge Discovery  4:217\205240 2000  J de Kl eer  A n assumpt i on-based T M S  Arti\223cial ntelligence  28:127\205162 1986  J  Han and Y  F u M i ning multiple-level association rules in large databases IEEE Transactions on Knowledge and Data Engineering  11:798\205804 5 2000  R  T  NG V  L a kshmanan J Han and A Pang E xpl orat ory mining and pruning otimizations of constrained association rules In Proceedings of the SIGMOD conference  pages 13\205 24 1998  N Pasquier  Y  B astide R T aouil and L  L akhal D isco v ering frequent closed itemsets for association rules In Proceedings of the 7th ICDT conference  pages 398\205416 1999  N Pasqui er  R  T aoui l  Y  Bast i d e G S t umme and L  L akhal  Generating a condensed representation for association rules Journal of Intelligent Information Systems  24\(1\:29\20560 2005  R S r i kant  Q  V u and R Agra w a l  Mi ni ng associ at i o n r ul es with item constraints In Proceedings of the KDD Conference  pages 67\20573 1997  M  J  Z aki  G enerat i n g non-redundent associ at i o n r ul es In Proceedings of the KDD Conference  pages 34\20543 2000 Proceedings of the 2005 International Conference on Computati onal Intelligence for Modelling, Control and Automation, and Inter national Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC\22205 0-7695-2504-0/05 $20.00 \251 2005  IEEE Proceedings of the 2005 International Conference on Computati onal Intelligence for Modelling, Control and Automation, and Inter national Conference on Intelligent Agents, Web Technologies and Internet Commerce \(CIMCA-IAWTIC\22205 0-7695-2504-0/05 $20.00 \251 2005  IEEE 


503 MFI that is frequent in more than one node as a global can didate. If a maximal itemset is frequent at all nodes, obvi ously it is also a global maximal frequent itemset. We just need to accumulate its local support counts and put it into the FrequentSet. Such global candidates, however, are very few. Fortunately, even though most itemsets in GC1 appear as local MFls in just one or a few nodes, many of them of ten have their supersets frequent in other nodes. In that case the support counts of the supersets of a candidate allow us to estimate the minimum support count of the candidate in those nodes. For example, suppose that itemset { A ,  B,  G is a local MFI with the local count of 4000 in node I, while A,  B ,  C, E ,  G} and { A ,  B ,  G, K }  are local MFIs in node 2 with local counts of 3800 and 4200, respectively. We can then estimate that the local support count of {A, B ,  G}  in node 2 should he at least 4200. By this way, we can esti mate the minimum support count of any itemset in a node if any of its supersets is frequent in that node. Obviously the estimated minimum support count o f a  candidate is the lagest  support count of all its supersets in that node Subset-Infrequency Based Pruning and Superset Frequency Based Pruning: During thc global mining phase, DMM maintains the following scts: GCk \( k  2 11 FrequentSet and InfrequentSer. They are changing dy namically with the progress of the mining process. The global mining phase continues until GC, is empty, for some k 2 1, to ensure that we will not miss any global maximal frequent itemset. Eventually, FrequentSet will include all the global MFIs. Since DMM uses Frequentset and InfrequentSet to perform the superset-frequency based pruning and the subset-infrequency based pruning maintaining these two sets without any rediindancy is important to make the pruning steps efficient. After each pass in the global mining phase, we determitle whether each global candidate is maximally frequent or not. If a global candidate is frequent, we put it into FrequentSet only if none of its supersets is already in that set. On the other hand, i f  a global candidate is infrequent. we put it into InfrequentSet only if none of its subsets is already in that set. For example, if {A, B, G, H }  is infrequent but A ,  H }  is already in InfrequentSet, we do not insert it into InfrequentSer, because any superset of { A ,  B,  G, H }  will be also pruned by {A,H} when the subset-infrequency based pruning is applied If a global candidate k-itemhet is identified as infrequent we split it into k \(k  - 1 as new candidates. However, some of them may not be a valid candidate for the next global pass if it appears in In frequentSet or has a subset in it. In that case, we need to split the invalid candidate into its largest proper subsets. For ex ample, if {A,  B, C, D }  is infrequent and its subset {A, D is in InfrequentSer, we will continue the splitting until we 2004 IEEE International Symposium on Cluster Computing and the Grid get the following new candidates: { A ,  B, C}, {B ,  C,  D A,B},  \( A ,  C}, { B , D }  and {C,D}.  In practice, these two pruning techniques can make the global candidate set shrink drastically for each pass 3.2.3 Cube-based Communication between Processors To perform the communication between processing nodes efficiently, we impose a logical binary n-cube structure on the processing nodes. Then, the nodes can exchange and merge the local count information through increasingly higher dimensional links between them [4]. In the n-cube there are 2n nodes, and each node has n-bit binary address Also, each node has n neighbor nodes which are directly linked to that node through differcnt dimensional links. For example, there are 8 nodes in a 3-cube structure, and node 0 0 0 001 010 100 through a I st-dimensional link, a 2nd-dimensional link, and a 3rd-dimensional link, respectively. Thus, in the n-cube all the nodes can exchange and merge their local counts in 


all the nodes can exchange and merge their local counts in T I  steps, through each of the n different dimensional links When n = 3, the three exchange and merge steps are step 1: node \( 1 where denotes a don't-care bit step 2: node \( 0 1 step 3: node \(0 1 O  3.3 Pseudo-code of DMM As we assume a homogeneous distributed computing environment where all the processing nodes are the same we just give the pseudo-code of the DMM algorithm running on a node P Local Mining Phase P" applies the sequential Max-Miner algorithm on D' and stores local MFIS into LM n = log, N :  I* N processing nodes are used for mining *I for \( j  = 1;j 5 n;j P' exchange and merge LM' with that of a neighbor node through the jth-dimensional link and the result is stored in LiM GC FrequenrSer= 6 foreach local MFI x in LM if the estimated global support o fx  is above the minimum support superset of x else put x into.GC1 then put x into FrequenrSer unless it contains a I apply the suprrset-frequency based pruning on GCI  504 Global Mining Phase InfrequenrSer= 4 global pass k ,  for k 2 1 while \(GCk # 4 Pi scans D' to count the candidates in GCk for \( j  = I ; j  5 n ; j P' exchange and merge the local counts of GCk members with a neighbor node through the jth-dimensional link foreach candidate x in GCx if the support of x is above the minimum support then put x into FrequentSer unless it contains a superset of x subset of x else put I into InfreqrcenrSer unless it contains a  foreach candidate inserted into InfrequenrSer in the current pass split the infrequent candidate into new candidates i.e., its largest subsets apply the subset-infrequency based pruning on these new candidates those candidates pruned by the subset-infrequency based pruning are put back into InfmquenrSer this process will continue until no new candidate either appears in InfrequenrSer or has any subset in i t 1 apply the superset-frequency based pruning on the new candidates remove those candidates which appear in Frequentset or put the new candidates that passed the two pruning operations into GCI,+~ for the next global pass k + I k have any superset in  it *I 1 GM = FrequentSer I* GM is the set of all maximal frequent itemsets 4 Performance Evaluation Our test platform is an &amp;node Linux cluster system 


Our test platform is an &amp;node Linux cluster system where nodes are connected by a Fast Ethernet switch. Each node has a 800 Mhz Pentium processor, 512 MB memory and a 40 GB disk drive. The processes are communicating using the MPI \(Message Passing Interface The databases used in our experiments are synthetic sales transaction databases generated as in [I]. All parameters used for generating databases are described in Table 1. For all databases, c = 0.5, m = 0.5, U = 0.1, ILI = 2000 and N I  = 1000. Table 2 lists all databases used in our perfor mance evaluation experiments. The size of each database is about 360 MB. When running the parallel algorithm on a database, we need to partition it into local databases. To balance the size of the local databases, each transaction is randomly allocated to a node 2004 IEEE Interna6onal Symposium on Cluster Computing and the Grid ID1 TI 111 ILI N I c m D In order to compare the performance of DMM and Count Distribution, we also implemented Count Distribution on the same platform Number of vansactiom in the database Average size of the transactions Average size of the maximal potentially frequent itemxu Numkraf maximal potentially frequent itemels Number of items Comlaliun level Mean of the comption level Variance of the camption level Table 1. Synthetic database parameters Table 2. Databases 4.1 Improvement of DMM over Count Distribu tion We ran both DMM and Count Distribution on different synthetic databases with different minsup values. If we de tine TCD and TDM,U as the execution times of CD and DMM, respectively, then the speedup of DMM over CD is TCD/TDMA,. In Table 3, the speedup of DMM is shown for different databases listed in the first column and for dif ferent values of minsup listed in the first row. In these ex periments, all 8 nodes in our cluster system were used Table 3. Speedup of DMM over CD \(8-node case As minsup decreases, DMM begins to show more and more improvement in our tests. As shown in Table 3, when the 111 value of the database is large, such as 8 or IO, even if minsup is as high as 0.58, DMM is faster than Count Dis tribution with a speedup above 2.5. It is because a large 11 value results in large frequent itemsets \(i.e., long patterns which benefits DMM. If minsup is less than 0.25%, DMM outperforms Count Distribution considerably DMM uses the local and global mining phases to re duce the overall synchronization and communication re quirement, but the global mining phase still needs several passes over the database and incurs some extra computation overhead. In our cluster system, since the communication speed between nodes is high, the benefit of reduced syn chronization and communication overhead is not enough to offset the effect of extra passes during the global mining phase. However, this feature of DMM may be attractive to some distributed systems where the communication cost is relatively high 4.2 Synchronization Requirement of DMM and Count Distribution We compared the number of synchronizations needed between processing nodes in DMM and Count Distribu 


between processing nodes in DMM and Count Distribu tion. In DMM, the local mining phase needs only one syn chronization. So, the total number of synchronizations is the number of passes needed in global mining phase plus one. Table 4 shows the comparison results. Here, we de fine SDMM and SCD as the numbers of synchronizations needed in DMM and CD, respectively. The first row of the table lists various values of minsup, and the first column lists the names of databases. The values in each entry of the table represents SD,MM :SCD Table 4. Comoarison of svnchronization re quirement When minsup is high, DMM is comparable to or a lit tle bit slower than Count Distribution. We also ran Apriori and Max-Miner for these cases, and found that Max-Miner doesn  t show much improvement over Apriori, either. That is because the high minsup limits the number of frequent itemsets and the length of those frequent itemsets. Thus the effect of look-ahead technique used by Max-Miner is not clearly shown, and naturally DMM has the same result DMM needed just two times of synchronization in the best cases. In other cases, the number of synchronizations needed for DMM was also much smaller than that of CD mainly because DMM requires only one synchronization during the local mining phase 505 2004 iEEE International Symposium on Cluster Computing and the Grid 4.3 Communication Requirement of DMM and Count Distribution In Count Distribution, all nodes have the same set of candidate itemsets in each pass. So, every node needs to exchange the same amount of count information with oth ers. In DMM, nodes need to exchange two types of data candidates and their counts. For the merging of local MFIs to construct the first global candidate set, each node per forms log, N send and log, N receive operations when N processing nodes are used. Since the set of local MFIs in one node may he different from those in other nodes, the amount of data each node sends or receives varies at each communication step. In each global pass, all nodes have the same global candidate set and exchange the same count in formation in log, N steps. To make it simple, we computed the average amount of data each node sends and receives during the whole mining Let  s consider the difference in the meaning of candi dates of the two algorithms as the number of candidates determines how much data need to he exchanged between processing nodes during the mining. In Count Distribution its candidates are the potential frequent itemsets generated as in Apriori. In DMM, after the local mining phase, can didates involved in the communication are just the potential maximal frequent itemsets; i.e., all local MFIs and some of their subsets that are not global MFIs. Compared with the set ofcandidates in Count Distribution, the set of candidates in DMM is very small. Thus, DMM requires much less communication than Count Distribution even though DMM needs to merge the candidates first \(after the local mining phase during the global mining phase When the minsup is very low, Count Distribution tends to discover a large number of short frequent patterns, so that there are a large number of candidates in early passes This results in a very high communication overhead be tween nodes. On the other hand, in  DMM, the increase in the number of short frequent patterns usually results in a small change in the number of local MFIs. Thus, even though low minsup value may affect the local mining phase of DMM, it has a relatively small impact on the communi cation overhead during the global mining phase. Therefore as the minsup decreases, DMM performs better than Count Distribution in terms of communication requirement 


Distribution in terms of communication requirement We implemented two versions of Count Distribution one is using the n-cube communication, and the other is using the all-to-all communication. We compared the av erage amount of data each node communicates with oth ers when we executed DMM and Count Distribution on the T30J08D2954K database with various values of minsup and the results are shown in Figure 4 As shown in Figure 4, the communication overhead of 500 450 400 a 350 B 300 g 200 250 150 s d 10.3 50 0 1 075 0 5  0 4  0 3  025 0 2  015 0 1 Minimum Suppon Figure 4. Comparison of communication re quirement DMM is much lower than that of Count Distribution. Even though DMM needs to cxchange candidates at the end of the local mining phase and some candidates may consist of many items, the total amount ofdata to he transferred is still relatively small, because Count Distribution must exchange the count information for much larger candidate sets. Com pared with Count Distribution using the all-to-all commu nication scheme, DMM demonstrates a big improvement in communication for all cases. Here, we  d like to emphasize that the advantage of DMM in communication requirement comes from its much smaller size of candidate sets and the n-cube communication scheme 4.4 Sensitivity Analysis of DMM In this section, we analyze the characteristics of the DMM algorithm in terms of speedup and sizeup. All tests were performed with a minsup of 0.25 4.4.1 Speedup We measured the speedup of DMM as the number of pro cessing nodes was increased while the database size re mained the same. For the databases listed in Table 2, we kept the same database size of 360 MB, but the database was partitioned into 2,  4, and 8 parts when the number of nodes were 2,4, and 8, respectively Figure 5 shows the execution time of DMM on the 2 node, 4-node, and 8-node systems. To demonstrate the speedup, we also ran the sequential Max-Miner for each database on a single node. As the number of nodes was doubled, the execution time of DMM was reduced by about 40% to 50%. Even though DMM may not achieve the linear speedup, it still shows a good speedup When DMM is executed on the T40110D2256K database using 2 nodes, its execution time is small. This is 506 2004 IEEE International Symposium on Cluster Computing and the Grid 16000 4000 3 12000 I ; iwoo E P 2 6000 w 4wo 2000 5 8000 8 0.7 because, when the number of nodes is small, the datadistri bution characteristic of each data partition is very similar to that of the whole database. So, after the local mining phase the initial global candidate set would be similar to the set of 


the initial global candidate set would be similar to the set of global MFIs. As a result, during the global mining phase the communication and synchronization overhead is low  0 2 4 6 8 1 0 Number of Nodes Figure 5. Speedup of DMM 4.4.2 Sizeup For the sizeup test, we fixed the system to the 8-node con figuration, and distributed each database listed in Table 2 to the 8 nodes. Then, we increased the local database sire at each node from 45 MB to 215 MB by duplicating the initial database partition allocated to the node. Thus, the data distribution characteristics remained the same as the local database size was increased. This is different from the speedup test, where the database repartitioning was per formed when the number of nodes was increased. The per formance of DMM is affected by the database repartitioning to some extent, although it is usually very small. During the sizeup test, the local mining result of DMM is not changed at all at each node The results shown in Figure 6 indicate that DMM has a very good sizeup property. Since increasing the size of local database did not affect the local mining result of DMM at each node, the total execution time increased just due to more disk U 0  and computation cost which scaled almost linearly with sizeup 5 Conclusions In this paper, we proposed a new parallel maximal fre quent itemset \(MFI Max-Miner \(DMM tems. DMM is a parallel version of Max-Miner, and it re quires low synchronization and communication overhead compared to other parallel algorithms. In DMM, Max Miner is applied on each database partition during the lo 0 45 90 135 180 225 270 Amwnt of Data per Node \(ME Figure 6. Sizeup of DMM cal mining phase. Only one synchronization is needed at thc end of this phase to construct thc initial global candi date set. In the global mining phase, a top-down search is performed on the candidate set, and a prefix tree is used to count the candidates with different length efficiently. Usu ally, just a few passes are needed to find all global maximal frequent itemsets. Thus, DMM largely reduces the number of synchronizations required between processing nodes Compared with Count Distribution, DMM shows a great improvement when some frequent itemscts are large \(i.e long patterns employed by DMM for efficient communication between nodes; and global support estimation, subset-infrequency based pruning, and superset-frequency based pruning are used to reduce the size of global candidate set. DMM has very good speedup and sizeup properties References I ]  R. Agrawal and R. Srikant  FdSt Algorithms for Mining As sociation Rules  Pmc. o f f h e  ZOrh VLDB Conf, 1994, pp 487499 2] R. Agrawal and I. C. Shafer  Parallel Mining of Association Rules  IEEE Trans. on Knowledge and Dura Engineering Vol. 8, No. 6, 1996, pp. 962-969 3] R. I. Bayardo  Efficient Mining Long Patlems from Databases  Proc. ofrhe ACM SIGMOD Inf  l Conf on Man ogemenr ofDara, 1998, pp. 85-91 4] S.  M. Chung and J. Yang  A Parallel Distributive Join Al gorithm for Cube-Connected Multiprocessors  IEEE Trans on Parallel and Disrribured Systems, Vol. 7, No. 2, 1996, pp 127-137 51 M. Snir, S. Otto. S. Huss-Lederman, D. Walker, and J. Don gana, MPI: The Complete Reference, The MIT Press, 1996 


gana, MPI: The Complete Reference, The MIT Press, 1996 6] R. Rymon  Search through Systematic Set Enumeralion   Pmc. of3rd Inr  l Con$ on Principles of Knowledge Repre sentation and Reasoning, 1992, pp. 539-550 507 pre></body></html 


sketch-index in answering aggregate queries. Then Section 5.2 studies the effect of approximating spatiotemporal data, while Section 5.3 presents preliminary results for mining association rules 5.1 Performance of sketch-indexes Due to the lack of real spatio-temporal datasets we generate synthetic data in a way similar to [SJLL00 TPS03] aiming at simulation of air traffic. We first adopt a real spatial dataset [Tiger] that contains 10k 2D points representing locations in the Long Beach county \(the data space is normalized to unit length on each dimension These points serve as the  airbases  At the initial timestamp 0, we generate 100k air planes, such that each plane \(i uniformly generated in [200,300], \(ii, iii destination that are two random different airbases, and iv  the velocity direction is determined by the orientation of the line segment connecting its source and destination airbases move continually according to their velocities. Once a plane reaches its destination, it flies towards another randomly selected also uniform in [0.02, 0.04 reports to its nearest airbase, or specifically, the database consists of tuples in the form &lt;time t, airbase b, plane p passenger # a&gt;, specifying that plane p with a passengers is closest to base b at time t A spatio-temporal count/sum query has two parameters the length qrlen of its query \(square number qtlen of timestamps covered by its interval. The actual extent of the window \(interval uniformly in the data space \(history, i.e., timestamps 0,100 air planes that report to airbases in qr during qt, while a sum query returns the sum of these planes  passengers. A workload consists of 100 queries with the same parameters qrlen and qtlen The disk page size is set to 1k in all cases \(the relatively small page size simulates situations where the database is much more voluminous specialized method for distinct spatio-temporal aggregation, we compare the sketch-index to the following relational approach that can be implemented in a DBMS. Specifically, we index the 4-tuple table lt;t,b,p,a&gt; using a B-tree on the time t column. Given a count query \(with window qr and interval qt SELECT distinct p FROM &lt;t,b,p,a&gt WHERE t?qt &amp; b contained in qr The performance of each method is measured as the average number of page accesses \(per query processing a workload. For the sketch-index, we also report the average \(relative Specifically, let acti and esti be the actual and estimated results of the i-th query in the workload; then the error equals \(1/100 set the number of bits in each sketch to 24, and vary the number of sketches The first experiment evaluates the space consumption Figure 5.1 shows the sketch index size as a function of the number of sketches used \(count- and sum-indexes have the same results more sketches are included, but is usually considerably smaller than the database size \(e.g., for 16 signatures, the size is only 40% the database size 0 20 40 60 80 


80 100 120 140 160 8 16 32 number of sketches size \(mega bytes database size Figure 5.1: Size comparison Next we demonstrate the superiority of the proposed sketch-pruning query algorithm, with respect to the na  ve one that applies only spatio-temporal predicates. Figure 5.2a illustrates the costs of both algorithms for countworkloads with qtlen=10 and various qrlen \(the index used in this case has 16 sketches also illustrate the performance of the relational method which, however, is clearly incomparable \(for qrlen?0.1, it is worse by an order of magnitude we omit this technique Sketch-pruning always outperforms na  ve \(e.g., eventually two times faster for qrlen=0.25 increases with qrlen, since queries returning larger results tend to set bits in the result sketch more quickly, thus enhancing the power of Heuristics 3.1 and 3.2. In Figure 5.2b, we compare the two methods by fixing qrlen to 0.15 and varying qtlen. Similar to the findings of [PTKZ02]4 both algorithms demonstrate  step-wise  growths in their costs, while sketch-pruning is again significantly faster The experiments with sum-workloads lead to the same observations, and therefore we evaluate sketch-indexes using sketch-pruning in the rest of the experiments 4 As explained in [PTKZ02], query processing accesses at most two paths from the root to the leaf level of each B-tree regardless the length of the query interval Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE sketch-pruning naive relational 0 100 200 300 400 500 600 700 800 900 0.05 0.1 0.15 0.2 0.25 number of disk accesses query rectangle length 300 0 100 200 400 500 600 1 5 10 15 20 number of disk accesses query interval length a qtlen=10 b qrlen=0.15 Figure 5.2: Superiority of sketch-pruning \(count As discussed in Section 2, a large number of sketches reduces the variance in the resulting estimate. To verify this, Figure 5.3a plots the count-workload error of indexes 


using 8-, 16-, and 32- sketches, as a function of qrlen qtlen=10 error \(below 10 it increases slowly with qrlen used, however, the error rate is much higher \(up to 30 and has serious fluctuation, indicating the prediction is not robust. The performance of 16-sketch is in between these two extremes, or specifically, its accuracy is reasonably high \(average error around 15 much less fluctuation than 8-sketch 32-sketch 16-sketch 8-sketch relative error 0 5 10 15 20 25 30 35 0.05 0.1 0.15 0.2 0.25 query rectangle length relative error 0 5 10 15 20 25 30 35 1 5 10 15 20 query interval length a qtlen=10, count b qrlen=0.15, count relative error query rectangle length 0 5 10 15 20 25 0.05 0.1 0.15 0.2 0.25 relative error query interval length 0 5 10 15 20 25 30 1 5 10 15 20 c qtlen=10, sum d qrlen=0.15, sum Figure 5.3: Accuracy of the approximate results The same phenomena are confirmed in Figures 5.3b where we fix qrlen to 0.15 and vary qtlen 5.3d \(results for sum-workloads number of sketches improves the estimation accuracy, it also leads to higher space requirements \(as shown in Figure 5.1 Figures 5.4a and 5.4b show the number of disk accesses for the settings of Figures 5.3a and 5.3b. All indexes have almost the same behavior, while the 32-sketch is clearly more expensive than the other two indexes. The interesting observation is that 8- and 16-sketches have 


interesting observation is that 8- and 16-sketches have almost the same overhead due to the similar heights of their B-trees. Since the diagrams for sum-workloads illustrate \(almost avoid redundancy 32-sketch 16-sketch 8-sketch number of disk accesses query rectangle length 0 50 100 150 200 250 300 350 400 0.05 0.1 0.15 0.2 0.25 number of disk accesses query interval length 0 50 100 150 200 250 300 350 1 5 10 15 20 a qtlen=10 b qrlen=0.15 Figure 5.4: Costs of indexes with various signatures Summary: The sketch index constitutes an effective method for approximate spatio-temporal \(distinct aggregate processing. Particularly, the best tradeoff between space, query time, and estimation accuracy obtained by 16 sketches, which leads to size around 40 the database, fast response time \(an order of magnitude faster than the relational method average relative error 5.2 Approximating spatio-temporal data We proceed to study the efficiency of using sketches to approximate spatio-temporal data \(proposed in Section 4.1 as in the last section, except that at each timestamp all airplanes report their locations to a central server \(instead of their respective nearest bases maintains a table in the form &lt;time t, plane p, x, y&gt;, where x,y with parameters qrlen and qtlen distinct planes satisfying the spatial and temporal conditions. For comparison, we index the table using a 3D R*-tree on the columns time, x, and y. Given a query, this tree facilitates the retrieval of all qualifying tuples, after which a post-processing step is performed to obtain the Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE number of distinct planes \(in the sequel, we refer to this method as 3DR method introduces a regular res  res grid of the data space, where the resolution res is a parameter. We adopt 16 sketches because, as mentioned earlier, this number gives the best overall performance Figure 5.5 compares the sizes of the resulting sketch indexes \(obtained with resolutions res=25, 50, 100 the database size. In all cases, we achieve high compression rate \(e.g., the rate is 25% for res=25 evaluate the query efficiency, we first set the resolution to the median value 50, and use the sketch index to answer workloads with various qrlen \(qtlen=10 


workloads with various qrlen \(qtlen=10 size \(mega bytes database size 0 20 40 60 80 100 120 140 160 25 50 100 resolution Figure 5.5: Size reduction Figure 5.6a shows the query costs \(together with the error in each case method. The sketch index is faster than 3DR by an order of magnitude \(note that the vertical axis is in logarithmic scale around 15% error observations using workloads with different qtlen Finally, we examine the effect of resolution res using a workload with qrlen=0.15 and qtlen=10. As shown in Figure 5.6c, larger res incurs higher query overhead, but improves the estimation accuracy Summary: The proposed sketch method can be used to efficiently approximate spatio-temporal data for aggregate processing. It consumes significantly smaller space, and answers a query almost in real-time with low error 3D Rsketch number of disk accesses query rectangle length 1 10 100 1k 10k 0.05 0.1 0.15 0.2 0.25 16 14% 15 15% 13 relative error number of disk accesses query interval length 1 10 100 1k 10k 1 5 10 15 20 16 15% 15% 12% 11 relative error a qtlen=10, res=25 b qrlen=0.15, res=25 0 500 1000 1500 2000 2500 25 50 100 number of disk accesses resolution 20% 15% 14 relative error c qrlen=0.15, qtlen=10 


c qrlen=0.15, qtlen=10 Figure 5.6: Query efficiency \(costs and error 5.3 Mining association rules To evaluate the proposed algorithm for mining spatiotemporal association rules, we first artificially formulate 1000 association rules in the form \(r1,T,90 with 90% confidence i randomly picked from 10k ones, \(ii in at most one rule, and \(iii Then, at each of the following 100 timestamps, we assign 100k objects to the 10k regions following these rules. We execute our algorithms \(using 16 sketches these rules, and measure \(i  correct  rules divided by the total number of discovered rules, and \(ii successfully mined Figures 5.7a and 5.7b illustrate the precision and recall as a function of T respectively. Our algorithm has good precision \(close to 90 majority of the rules discovered are correct. The recall however, is relatively low for short T, but gradually increases \(90% for T=25 evaluated in the previous sections, the estimation error decreases as the query result becomes larger \(i.e., the case for higher T 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 precision HT 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 recall HT a b Figure 5.7: Efficiency of the mining algorithm Summary: The preliminary results justify the usefulness of our mining algorithm, whose efficiency improves as T increases Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE 6. Conclusions While efficient aggregation is the objective of most spatio-temporal applications in practice, the existing solutions either incur prohibitive space consumption and query time, or are not able to return useful aggregate results due to the distinct counting problem. In this paper we propose the sketch index that integrates traditional approximate counting techniques with spatio-temporal indexes. Sketch indexes use a highly optimized query algorithm resulting in both smaller database size and faster query time. Our experiments show that while a sketch index consumes only a fraction of the space required for a conventional database, it can process 


required for a conventional database, it can process queries an order of magnitude faster with average relative error less than 15 While we chose to use FM sketches, our methodology can leverage any sketches allowing union operations Comparing the efficiency of different sketches constitutes a direction for future work, as well as further investigation of more sophisticated algorithms for mining association rules. For example, heuristics similar to those used for searching sketch indexes may be applied to improve the brute-force implementation ACKNOWLEDGEMENTS Yufei Tao and Dimitris Papadias were supported by grant HKUST 6197/02E from Hong Kong RGC. George Kollios, Jeffrey Considine and were Feifei Li supported by NSF CAREER IIS-0133825 and NSF IIS-0308213 grants References BKSS90] Beckmann, N., Kriegel, H., Schneider, R Seeger, B. The R*-tree: An Efficient and Robust Access Method for Points and Rectangles. SIGMOD, 1990 CDD+01] Chaudhuri, S., Das, G., Datar, M., Motwani R., Narasayya, V. Overcoming Limitations of Sampling for Aggregation Queries. ICDE 2001 CLKB04] Jeffrey Considine, Feifei Li, George Kollios John Byers. Approximate aggregation techniques for sensor databases. ICDE, 2004 CR94] Chen, C., Roussopoulos, N. Adaptive Selectivity Estimation Using Query Feedback. SIGMOD, 1994 FM85] Flajolet, P., Martin, G. Probabilistic Counting Algorithms for Data Base Applications JCSS, 32\(2 G84] Guttman, A. R-Trees: A Dynamic Index Structure for Spatial Searching. SIGMOD 1984 GAA03] Govindarajan, S., Agarwal, P., Arge, L. CRBTree: An Efficient Indexing Scheme for Range Aggregate Queries. ICDT, 2003 GGR03] Ganguly, S., Garofalakis, M., Rastogi, R Processing Set Expressions Over Continuous Update Streams. SIGMOD, 2003 HHW97] Hellerstein, J., Haas, P., Wang, H. Online Aggregation. SIGMOD, 1997 JL99] Jurgens, M., Lenz, H. PISA: Performance Models for Index Structures with and without Aggregated Data. SSDBM, 1999 LM01] Lazaridis, I., Mehrotra, S. Progressive Approximate Aggregate Queries with a Multi-Resolution Tree Structure. SIGMOD 2001 PGF02] Palmer, C., Gibbons, P., Faloutsos, C. ANF A Fast and Scalable Tool for Data Mining in Massive Graphs. SIGKDD, 2002 PKZT01] Papadias,  D., Kalnis, P.,  Zhang, J., Tao, Y Efficient OLAP Operations in Spatial Data Warehouses. SSTD, 2001 PTKZ02] Papadias, D., Tao, Y., Kalnis, P., Zhang, J Indexing Spatio-Temporal Data Warehouses ICDE, 2002 SJLL00] Saltenis, S., Jensen, C., Leutenegger, S Lopez, M.A. Indexing the Positions of Continuously Moving Objects. SIGMOD 2000 SRF87] Sellis, T., Roussopoulos, N., Faloutsos, C The R+-tree: A Dynamic Index for MultiDimensional Objects. VLDB, 1987 TGIK02] Thaper, N., Guha, S., Indyk, P., Koudas, N Dynamic Multidimensional Histograms 


SIGMOD, 2002 Tiger] www.census.gov/geo/www/tiger TPS03] Tao, Y., Papadias, D., Sun, J. The TPR*Tree: An Optimized Spatio-Temporal Access Method for Predictive Queries. VLDB, 2003 TPZ02] Tao, Y., Papadias, D., Zhang, J. Aggregate Processing of Planar Points. EDBT, 2002 TSP03] Tao, Y., Sun, J., Papadias, D. Analysis of Predictive Spatio-Temporal Queries. TODS 28\(4 ZMT+01] Zhang, D., Markowetz, A., Tsotras, V Gunopulos, D., Seeger, B. Efficient Computation of Temporal Aggregates with Range Predicates. PODS, 2001 ZTG02] Zhang, D., Tsotras, V., Gunopulos, D Efficient Aggregation over Objects with Extent PODS, 2002 Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE pre></body></html 


