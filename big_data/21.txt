1 2 3 4 5 221This research was supported by the DFG project 524/88 and by the Heinz-Nixdorf-Foundation anger baby blue light ocean working mad 353 boy 162 sky 175 dark 647 water 314 hard 132 fear 83 child 142 red 160 lamp 78 sea 233 loafing 99 hate 61 cry 113 green 125 bright 30 blue 111 sleeping 79 rage 49 mother 71 color 66 sun 25 deep 48 playing 65 temper 35 girl 51 yellow 56 bulb 23 waves 48 man 48 


For the calculation of the connective weights a window is shifted from left to right over the text After each shift the connections between all pairs of words occuring in the window are updated Two different types of windows have been compared 1 sentence windows which include all words in a sentence. During the process of learning they 2 continuous windowk which contain a constant number of words and are shifted over the text Seven different types of connections have been computed The corresponding formulas are derived from different assumptions about autoassociative learning In formula 1 the weight between two words i and j is identical to the number of windows or sentences in which both words occur together It is based on the assumption that the associative strength between two words increases by a constant amount whenever they appear in close neigh borhood independent of their overall frequencies This formula gives symmetric weights with a range between zero and the number of windows or sentences are shifted sentence by sentence word by word wij  f  P\(i  j where wi is the associative weight between word i and word j f is the total number of windows and P\(i  j is the probability of words i and j both occuring in a window In formula 2 the association from word i to word j is identical to the conditional probability of word j given i Here the sum of all weights which emanate from a node tends to be the same for all t4e nodes of the network This property is in agreement with the so-called fan effect, i e the observation of Anderson l and others that the amount of activation which spreads out from a node i to a node j is inversely related to the sum of all associations leading from i where P\(i is the probability of word i occuring in a window Formula 3 gives the conditional probability of word i given j Its value decreases whenever j is observed without i This effect would be in accordance with the observation of Barnes  Underwood 2 and others that associative learning is impeded by retroactive inhibition P\(i  j W\224   lJ  P\(j 3 Formula 4 is equal to the probability that i and j CO-occur divided by the probability of their joint co-occurrence in case of independence P\(i  j 20   a\222  P\(i j 4 Taking the logarithm of equation 4 leads us to formula 5 defining 223mutual information\224 which is used by Church  Hanks 3 in a similar context Formulas 1 to 4 yield positive weights Formulas 1 4 and 5 give symmetric weights and formulas 2 and 3 asymmetric weights with a maximum value of 1 Formula 6 is based on the assumption that the connective weight between two words is equal to the difference between an excitatory and an inhibitory relation the latter being reinforced whenever one word appears without the other The derivation for this formula is given in 8 26 


I Formula 7 was suggested in 7 for the construction of a constraint satisfaction network that relates typical features to different types of objects P\(1i  j i   P\(i  j li   w;j  In 7 where P\(i  j is the probability that word i but not j occurs in a window In formulas 1 to 7 the connective weights between two words are independent of the order of succession of these words in the text Many studies of human memory have shown however that after the successive presentation of two words the association of the former to the latter word the SO called forward association will be stronger than its reciprocal the so-called backward association In order to take account of this observation we constructed, for each of the formulas 1 to 7 two variants In the first variant the term P\(i  j has been replaced by P\(i  j the probability that j appears in a window after i and in the second variant P\(i  j has been replaced by P\(i  j where P\(i  j  P\(i  j  P\(i 6 j 3 Prediction of the Associative Responses For the prediction of the associative responses of the Russel  Jenkins study we have compared two assumptions According to the first assumption later on referred to as weight model the associative response should be the word with the highest connective weight to the stimulus In this case the probability that a subject responds with word j to a given stimulus i should be proportional to the connective weight wij According to the second assumption the propagation model the associative stimulus induces a process of propagation where an activation emanates from the stimulus word and spreads out over the lexical network in several steps Hereby the word which receives the highest activity during propagation is considered as the associative response of the system To test this assumption we activate, for each example separately the node corresponding to the stimulus word and propagate this activity according to the following equation Ui\(b  input;\(t  CUj\(t  1 8 j where a;\(t is the activity of word i after step t inputi\(t the external activation of the stimulus word and w;j is the associative weight between word i and word j wii be 0 After each step of propagation the words of the net are ranked according to their activities In 6 it is shown that by the process of propagation not only first order relationships between words direct connections are taken into account but also higher order relationships 4 Results In order to determine the validity of the different predictions we calculated for each example separately the ranks of the words which were given as associative responses In the case of the weight model the lowest rank has been given to the word with the highest connective weight to the stimulus in the case of the propagation model to the word with the highest activity after propagation The measure of the overall quality of a model has been defined as where index 1 denotes the 100 association examples and k the five most frequent responses to stimulus word 1 in the Russel  Jenkins association experiment Nk is the number of persons who responded with word k Rk the rank of word k in our simulation M is the mean rank of the 5 most frequent human responses when looked up in the simulation word list but weighted by the frequency of the responses and averaged over all 100 association examples The range of M is between 1 and 371 If no correlation between predictions and observations existed M would be 185.5 half the number of words in the network 21 


I Formula Ill1213141516171 stimulus c response stimulus  response stimulus t response I M fDroDaeation model 11 49.9 I 4 Formula 1234567 M weight model 47.6 47.6 42.8 42.8 42.8 46.8 42.8 M propagation model 47.6 47.6 35.7 36.8 37.2 37.6 36.9 M weight model 57.9 57.9 54.2 54.2 54.2 59.8 54.5 M propagation model 57.9 56.5 43.3 43.8 49.9 45.6 49.7 70.4 70.4 68.1 68.1 68.1 72.9 68.2 M propagationmodel 70.4 67.8 53.9 53.2 62.3 56.2 62.2  M weight Table 2 Values for M for different formulas with and without propagation The co-occurences were determined on the basis of sentence-wise evaluation, with word order not being considered Table 2 gives the mean ranks M when the weights were computed with different formulas on the basis of sentence-wise co-occurences in Grolier\222s Electronic Encyclopedia \(without regard to word order All formulas give reasonably good predictions of the associative answers and with five of the formulas the predictions improve after propagation It is surprising that the different methods for computing the connective weights produce similar results The same observation has been made in earlier studies where lexical nets have been applied to predict which query-words are used by data-bank searchers in on-line searches 8 With an M of 35.6 formula 3 gives the best prediction of the associative responses This is confirmed when the co-occurrences of word-pairs are determined using windows of constant width and when we take word order into account see table 3 This result supports the assumption that the overall frequency of the response word which is a measure of the amount of retroactive inhibition has effect on the responses in the free association task Table 3 Values for M for different formulas with and without propagation, with order information taken into account The co-occurences were determined using windows of width 18 When the connective weights are learned on the basis of scientific abstracts the predictions become worse When using sentence wise co-occurences and formula 1 we obtain a mean rank M of 69.1 which compares to an M of 49.9 when using the Grolier text corpus This difference might be attributed to the fact that the language used in the abstracts is very specific and highly repetitive 62 54 52 50 I I I I I I I I I I 10 20 30 40 50 60 70 80 WINDOW-SIZE Fig 1 M for different window-sizes Figure 1 shows the values for when the connective weights are computed with windows of different sizes us ing formula 1 The same procedure has also been applied for some of the other formulas The optimal width of the window was always found to be around 18 words For compari son the average sentence length in Grolier\222s Electronic Encyclopedia is 22.8 words Non alpha-characters are counted as words Learning by continuously shifting windows and learning sentence by sentence give similar predictions 


Table 3 gives the values for M when the weights are learned with windows of width 18 with and without regard to word order inside the windows The predictions are better when the direction of the connections corresponds to word order rather than vice versa However the best results are found when word order within the windows is not considered at all This is probably due to the higher number of available observations when word order is neglected 4 Conclusions The evaluation of different sets of assumptions concerning the prediction of word associations from texts leads to the following results 1 Type and length of the text corpus are of importance The larger the text i.e the higher the absolute frequencies of the stimulus or response words the better the predictions are Ten million words should be considered a minimum When we reduced our original vocabulary of 371 words to those 355 words which in Grolier\222s Electronic Encyclopedia have an absolute frequency of 10 or more and removed the 16 association examples where at least one of these words occured the mean rank M improved from 35.6 to 27.5 2 The window size should be around 18 words but the window type is not critical Word order needs not be considered 3 Different formulas for computation of the weights yield similar predictions as long as they take the retroactive inhibition of the response word into account 4 The predictions are improved when second and third-order dependencies between words are considered For this a large vocabulary is advantageous When following these rules our simulation program gives unrivalled predictions of the words people come up with in the free association task References l Anderson J R Cognitive Psychology and its Implications New York Freeman  Co 1985 2 Barnes J M Underwood B J 223Fate\224 of First-List Associations in Transfer Theory Journal of Experimental Psychology 58 1959 97-105 3 Church K W Hanks P Word Association Norms Mutual Information and Lexicography 4 Galton F Psychometric Experiments. Brain 2 1880 149-162 5 Jenkins J J The 1952 Minnesota Word Association Norms In Postman L Keppel G 6 Rapp R Wettler M Simulation der Suchwortgenerierung im Information-Retrieval durch Propagierung in einem konnektionistischen Wortnetz. In Nachrichten fur Dokumentation 41 In Computational Linguistics Volume 16 Number 1 March 1990 Eds Norms of Word Association New York Academic Press 1970 1-38 1990,27-32 7 Rumelhart D E Smolensky, P McClelland J L Hinton G E Schemata and Sequential Thought Processes in PDP Models In Rumelhart D E McClelland J L Eds Parallel Distributed Processing Vol 2 Cambridge MA The MIT Press 1986 7-57 8 Wettler M Rapp R A Connectionist System to Simulate Lexical Decisions in Information Retrieval In Pfeifer R Schreter Z Fogelman F Steels L Eds Connectionism in Perspective Amsterdam Elsevier Science Publishers B.V North-Holland 1989 29 


Theorem 3 The constraints CFreeAC and Ckree,,c are equivalent and anti-monotone. The set SATC,,AC can be eflciently computed using the same method as in CLOSE using SATcFVeeAcm C,,,,~C i.e the output of the generic algorithm with the Constraint c  CFTeeAC A cam A Cm This theorem means that we can find free-itemsets that verify conjunctions of anti-monotone and monotone con straints 4.4 MIN-EX algorithm The MIN-EX algorithm is an extension of the CLOSE al gorithm 4 The concept of closure is extended, providing new possibilities for pruning However we must trade this efficiency improvement against precision the frequency of the frequent itemsets are only known within and bounded error If 6 is an integer let closures\(S be the maxi mal for the set inclusion\superset Y of S such that for every item A E Y  S ISupport\(S U A is at least Support\(S  6 with 6  0 it is the same closure op erator than CLOSE i.e closure0  closure Larger values of S leads to more efficiency improvement and larger errors on the frequencies of itemsets By replacing this new closure operator in the definition of CF we define Cs-Free and C&-FTeeAC and Theorem 3 is true with these new constraints The sets that fulfill C~-F are the so called free sets from 6 Here again one can say that the more you have almost logical association rules that hold in your data rules with confidence close to 1 since only 6 ex ceptions are allowed\the less you have free sets It has been shown that when the frequency of a frequent itemset is approximated by using the frequency of a free set, the error on frequency can remain very low in practice 6  5 An experimental validation We consider an experiment motivated by the search for association rules with negations 5 Only some results con cerning the discovery of generalized sets from with associ ation rules with negations are derived\are given here Notations Let Items  A B  be a finite set of sym bols called the positive items and a set Items of same car dinality as Items whose elements are denotedx B  and called the negative items Given a transactional database 7 over Items let us define a complemented transactional database over Items  Items U Items as follows for a given transaction t E 7 we add to t negative items cor responding to positive items not present in t Generalized itemsets are subsets of Items and can contain positive and negative items Constraints We want to extract frequent itemsets Cf that do not involve only negative items Calpp Calpp\(S is true when S involves at least p positive items This is obviously a monotone constraint First experiments have shown that it was interesting to relax such a monotone con straint i.e accepting more sets in order to give rise to more pruning see 5 for a complete discussion Follow ing that guideline instead of Calpp we used the constraint This constraint enforces gt least p positive attributes a monotone constraint er gt most legative attribute an anti-monotone constraint Let us introduce the collection of constraints that have Calppoamln  Calpp V Camln cdl Cfrep A C&FreeAC A Callpoamln Cd2 Cfreq A C6-FreeACm A Cal2poamln cd3  cf rep A C6-FreeACm A Cal3poamln With these constraints we are able to compare different approaches  e using only the frequency constraint e using the frequency constraint and Calppoamln con e using CfTeg and Calppoamln in conjunction with straint with p  1 2 3 Cfl Cfa and Cf3 CFreeAC or Cd-FreeAC  Datasets We studied the use of these constraints on two dataset The first one is a benchmark the so-called mush room data This dataset is a binary matrix of 8124 rows Each row contains 23 discrete attributes Theses attributes are binarized into exclusive attribute-value pairs This leads to a binary matrix with 119 columns and 23 2231\224 per row When encoding negative items it leads to a matrix with 238 columns whose each row contains 119 2231\224 The second dataset is from the French national institute of statistics INSEE In this dataset each row represents a French town and each column represents a kind of ser vice e.g., bank, insurance company etc a 2231\224 in 223bank\224 column means that there is at least one bank in the town In this dataset, there are about 37000 rows and 59 columns with an average number of 2231\224 per row of 4 When encod ing negative items, it leads to a matrix with 118 columns whose each row contains 59 2231\224 The former dataset is quite small but it is known to be tough due to the high correlation between the attributes and 327 


its density for positive attributes The latter dataset is larger but it is sparse 4 2231\224 per row on average for positive attributes and less correlated. These two different datasets let us compare our approach on different types of datasets Indeed the results show a great difference between these two experiments Experiments The experiments were conducted on a 500 MHz Pentium I11 with 768 MB of memory The value of the frequency threshold y is changed over experiments in order to observe the trend. Logarithmically scaled axes are used The value of the 6 parameter in the Cd-FTee con straint is set to 200 for the mushroom database and to 100 for the INSEE database in this latter database there was only a slight difference in execution time between 6  100 and 6  200 but 6  100 gives more accurate results as explained in Section 4.4 100000 10000 1000 100 Time sec vs Frequency threshold 10 0.2 03 0.4 0 5 0.6 0 7 0.8 0.9 1 Cdl  Cd3  Cf2 t Cd2   Cfl 8 Cf3 Figure 2 Mining generalized sets from mush room dataset Figure 2 shows the results of the experiments on the mushroom dataset with the constraints cfl Cf2 Cf3 Cdl cd2 and Cd3 The extractions using the other constraints Cfreq C1 C2 and C3 were intractable even at high fre quency threshold 95 and with the strongest requirement on the number of positive items C3 On this dataset the use of C~--F~~~~C as opposed to C~--F~~~AC clearly im proves the results With Cf3 a frequency threshold of 65 is reached whereas using Cd3 allows to reach a frequency of 20 within about the same time. Finally on the mushroom dataset CfTes combined with the most favorable case of Calpp still leads to an intractable extraction CfTeq combined with Only C~-F or CFTee does not allow mining at low threshold even with Callp i.e cdl and Cjl we only reach 60 Therefore to mine at reasonable frequency thresholds the conjunction of both techniques \(using constraints on item sets with the Calppoamln family of constraints and 101 ing for 6-free-sets with CFTeeAC or Cd-FreeAC appears mandatory Time sec vs Frequency threshold 10000  mx J I 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 Cd2  Cf2 c2 t Cd3   Cf3 e c3 e Figure 3 Mining generalized sets from INSEE dataset Figure 3 shows the results of the experiments on the IN SEE dataset With the constraints CfTeq C1 Cfl and cdl it is not possible to reach a frequency threshold less than 34 For this frequency threshold, there is only one frequent pos itive attribute. This means that all the itemsets mined at this threshold are composed of the same positive attribute and several negative ones On this dataset we notice that the Cf family of con straint is surprisingly less efficient than the less constrain ing C family We analyzed the output of the algorithm and found that almost no logical rule association rule with a confidence of 1 holds in this dataset The optimization of Cf over C is based on the presence of these rules Even if the use of 6-free-sets \(with Cd2 or Cd3 does not allow to mine at significantly lower thresholds however it speeds up the extraction by an order of magnitude with re spect to C2 or C3 at lower frequency thresholds Finally with this dataset too the approach turns to be valuable 6 Conclusion We study itemset mining under constraints within level wise algorithms Several interesting results have been al ready published the last three years, e.g., about the effective use of anti-monotone constraints or the interest of mono tone constraints The generic algorithm we give in this pa per is a simple generalization of several related algorithms 328 


and enable to emphasize the potential of optimization when considering conjunctions of anti-monotone and monotone constraints Furthermore we provide new results concern ing the computation of free sets under constraints We dis cussed under which conditions it was possible to extend an algorithm like CLOSE for an effective use of constraints An experimental validation has confirmed the added-value of this approach A recent work uses a different approach and proposes to mine frequent itemsets without candidate generation 17 Integrating this new algorithm within our study seems promising Furthermore, frequent sets discovery, and more generally data mining is not limited to independent min ing tasks or queries Knowledge discovery in databases is an iterative process and there are still lots of work to do to optimize sequences of queries There is a major trade-off between fully optimizing each individual query and finding a strategy that makes use of previous mined patterns 8 31 This strategy may be less effective for the first queries but may win for long sequences of related queries i.e the way people actually proceed Acknowledgement The authors thank Artur Bykowski for his contribution to the experimental validation References l R Agrawal T Imielinski and A. Swami Mining associa tion rules between sets of items in large databases In Pro ceedings of ACM SIGMOD Conference on Management of Data SIGMOD\22293 pages 207  216 Washington D.C USA May 1993. ACM  2 R Agrawal H Mannila R Srikant H Toivonen, and A I Verkamo Fast discovery of association rules In Advances in Knowledge Discovery and Data Mining pages 307  328 AAAI Press, Menlo Park CA 1996  E Baralis and G Psaila Incremental refinement of min ing queries In Proceedings of the First International Con ference on Data Warehousing and Knowledge Discovery DaWaK\22299 volume 1676 of Lecture Notes in Computer Science pages 173  182 Florence I Sept 1999. Springer Verlag 4 J.-F Boulicaut and A Bykowski. Frequent closures as a con cise representation for binary data mining In Proceedings of the Fourth PacifAsia Conference on Knowledge Discov ery and Data Mining PAKDD\222OO volume 1805 of Lecture Notes in ArtiJcial Intelligence pages 62  73 Kyoto JP Apr 2000. Springer-Verlag 5 J.-F Boulicaut A Bykowski and B. Jeudy Mining asso ciation rules with negations Technical report, INSA Lyon LISI, F-69621 Villeurbanne Nov 2000 6 J.-F Boulicaut A Bykowski and C Rigotti Approxima tion of frequency queries by mean of free-sets In Proceed ings of the Fourth European Conference on Principles and Practice of Knowledge Discovery in Databases PKDD 22200 volume 1910 of Lecture Notes in ArtiJicial Intelligence pages 75  85 Lyon F Sept 2000. Springer-Verlag  J.-F Boulicaut and B. Jeudy Using constraint for itemset mining should we prune or not In Proceedings Bases de Donne\221es Avange\221es\224 BDA\222OO pages 221  237 Blois F Oct 2000 8 B Goethals and J van den Bussche On implementing interactive association rule mining In Proceedings of the ACM SIGMOD Workshop on Research Issues in Data Min ing and Knowledge Discovery DMKD\22299J Philadelphia USA, May 30 1999 9 G. Grahne L V S Lakshmanan and X Wang Efficient mining of constrained correlated sets In Proc ICDE 2000 pages 512  521 San Diego, USA, 2000 lo L V Lakshmanan R Ng J Han and A Pang Opti mization of constrained frequent set queries with 2-variable constraints In Proceedings of ACM SIGMOD Conference on Management of Data SIGMOD\22299 pages 157  168 Philadelphia, USA, 1999 ACM Press l 13 H. Mannila and H Toivonen. Multiple uses of frequent sets and condensed representations In Proceedings of rhe Sec ond International Conference on Knowledge Discovery and Data Mining KDD\22296 pages 189  194 Portland USA Aug 1996. AAAI Press 12 H Mannila and H Toivonen Levelwise search and bor ders of theories in knowledge discovery Data Mining and Knowledge Discovery 1\(3 258, 1997 13 R Ng L V Lakshmanan J Han, and A. Pang. Exploratory mining and pruning optimizations of constrained associa tions rules. In Proceedings of ACM SIGMOD Conference on Management of Data SIGMOD\22298 pages 13  24 Seattle Washington USA 1998 ACM Press 14 N Pasquier Y Bastide R Taouil, and L Lakhal Closed set based discovery of small covers for association rules In Proc BDA\22299 pages 361  381, Bordeaux, F, Oct 1999 15 N Pasquier Y Bastide R Taouil, and L Lakhal Efficient mining of association rules using closed itemset lattices In formation Systems 24\(1  46 Jan 1999 16 J Pei, J Han and L V S Lakshmanan Mining frequent itemsets with convertible constraints In Proc ICDE\22201 Heidelberg, Germany, 2001 17 J. Pei J Han, and R Mao CLOSET an efficient algorithm for mining frequent closed itemsets In Proceedings of the ACM SIGMOD Workshop on Research Issues in Data Min ing and Knowledge Discovery DMKD\222OO Dallas, USA May 2000  181 R Srikant Q Vu and R Agrawal. Mining association rules with item constraints In Proceedings of the Third Interna tional Conference on Knowledge Discovery and Data Min ing KDD\22297 pages 67  73 Newport Beach California USA 1997. AAAI Press  M J Zaki Generating non-redundant association rules In Proc KDD\222OO pages 34  43 Boston USA 2000 329 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


