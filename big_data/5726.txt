Inîuence Maximization for Big Data Through Entropy Ranking and Min-Cut Agustin Sancen-Plaza Dept of Electrical Engineering and Computer Science CINVESTAV Guadalajara Guadalajara Mexico Email asancen@gdl.cinvestav.mx Andres Mendez-Vazquez Dept of Electrical Engineering and Computer Science CINVESTAV Guadalajara Guadalajara Mexico Email amendez@gdl.cinvestav.mx Abstract As Big Data becomes prevalent the traditional models from Data Mining or Data Analysis although very efìcient lack the speed necessary to process problems with data sets in the range of million samples Therefore the need for designing more efìcient and faster algorithms for these new types of problems Speciìcally from the eld of social network analysis we have the inîuence maximization problem This is a problem with many possible applications in advertising marketing social studies etc where we have representations of inîuences by large scale graphs Even though the optimal solution of this problem the minimum set of graph nodes which can inîuence a maximum set of nodes is a NP-Hard problem it is possible to devise an approximated solution to the problem In this paper we have proposed a novel algorithm for inîuence maximization analysis This algorithm consist in two phases the rst one is an entropy based node ranking where entropy ranking is used to determine node importance in a directed weighted inîuence graph The second phase computes the minimum cut using a novel metric To test the propose algorithm experiments were performed in several popular data sets to evaluate performance and the seed quality over the inîuences Keywords  Inîunce maximization entropy minimum cut I I NTRODUCTION Large scale data processing analysis has brought us new challenges with respect to algorithmic support There are several problems like nding statistics in data streams clustering of information in data streams classiìcation and learning  2 3 where the need of light and ef cient algorithm is necessary For example in pattern recognition there are algorithms where the top-k most important patterns are found instead of all of them Speciìcally in the new world of social networks being able to identify sets of users able to inîuence the rest of the social network has become highly important and necessary For example Inîuence Maximization IM is a classic problem in social analysis and it has become of great interest in data mining machine learning and optimization The original problem was formalized by Domingos and Richardson in and it has the following structure Given a social network a directed graph G  we need to nd m nodes call seeds who inîuence the maximum set of network nodes Because IM is NP-Hard problem most of related w orks 6 7 8 9 are focused in trying to approximate the optimal solution Quality results in the IM problem depends of nding the best seed nodes and there are two kind of seeds nodes static and extra seed nodes Static seed nodes are those which are well know For example if we represent a series of twitter accounts which talk about a famous pop music singer as a graph It is possible to identify fan club leaders as static nodes and the rest of the followers as nodes being inîuenced by the leaders The second type of seed nodes are related to nodes who potentially can inîuence others to change their opinion These type of nodes are called extra seed nodes The focus of this work is to try to nd these extra seed nodes to deìne a solution of the IM problem Even though several methods 6 11 8 12 allo w to process large amounts of information they lack of effective techniques for seed discovery Therefore it is necessary to have more effective methods to obtain faster and more complex answers For example Liu et al uses a greedy technique based on min-cuts that allows nding the most inîuential nodes by computing min-cuts for every node to be a potential extra seed node A drawback of this technique when applied to a large amount of data is its combinatorial nature IM has been recently re-stated as a Big Data problem for marketing purposing in social networks For example traditional greedy techniques based in Kempe et al were adopted to get faster results using parallelizing algorithms In another example Li et al were f acing lar ge datasets problems for which they decide to introduce the conformity term in the network This term means the inclination of one node to be inîuenced by others  In this work the proposed algorithm needs to calculate these two indexes inîuence and conformity Then using these indexes it gets a series of graph partitions to determine local seed nodes because after all inîuence and conformity are local properties Then the initial problem is to nd the extra seed nodes Then nodes need to be classiìed in order to determine the extra seed nodes A popular method is the use of ranking algorithms for this task For example there are traditional ranking algorithms using the degree of the nodes as the populars PageRank  and HITS 15 for web page classiìcation Another 


method uses ranking for labeled directed graph for hyperlinked document ranking There are other methods as Dirichlet PageRank designed to a v oid c yclical structures to identify e.g spammer pages in web map To process large datasets for IM we propose a two-phase algorithm and a bounded greedy algorithm in order to reduce computational time and to obtain good approximations to the IM problem In the rst phase we rank the graph nodes using an entropy ranking to nd a hidden organizational structure to select interesting prominent members  This entropy ranking does not take in account the nodeês degree which makes this ranking more effective for nding extra seed nodes The inîuence entropy is calculated by removing the node from the graph and calculating its effect The second phase uses a well know min-cut in a local fashion to speed up the computational time This idea of using the min-cut is inspired by the Ising model This model w a s originally used for ferromagnetic material behavior analysis in mechanical statistics In Social Network Analysis SNA the Ising model is used to detect community structures In this type of problems the computation of the min-cut for calculating the Ising model leads to a ground state which is a conìguration with a state of low energy In an equivalent way in social networks the ground state is a conìguration with low number of conîicts Finally the proposed method utilizes a greedy pruning method taken from IM traditional methods 8 to obtain the e xtra seed nodes These extra seed nodes are choose from the most ranked nodes which are also attractor seeds Finally in order to prove the proposed algorithm we select a series of datasets to build weighted graphs These datasets are from twitter trending topics hashtags user mentions and retweets to represent the users inîuence In addition we include random graphs for baseline comparison The paper is organized as follows In Section II we formally explain the problem and the required background to understand the proposed algorithm In Section III we describe the proposed algorithm In Section IV we explain how the algorithm was implemented In Section V a series of experiments are described and the results are reported including comparison with other algorithms In this section we show the complexity time improvement due to the two phase algorithm and the bounded greedy algorithm Finally in the conclusion we describe some ideas for future improvements II P ROBLEM D ESCRIPTION AND B ACKGROUND A Graph Construction Graphs are a very used technique for data representation specially in SNA 23 In our case the graphs are used to represent inîuences between social network members For this we deìne a Social Network as a graph G   V E   in which every user is a graph node  V  every link  E  between nodes represents an inîuence Links are deìned as E   i j   V  V  i   j   and we deìne inîuence in one way or direction In other words if there is a inîuence from i to j there is not necessarily the same inîuence from j to i Actually this is the natural way to form userês link followers in twitter Matrix weight W represents degrees of inîuence between nodes W   n  n  N where n   V   W ij means link weight from node i to node j and i   j  For twitter data representation graphs are used to represent twitter followers where users are nodes and links are follow relationships For example if a user a follows user b there are a link from a to b  Follow relationships do not necessary reîects inîuence because for example user a can follow user b but if b never writes a tweet there is no inîuence For inîuence representation we can build it using retweets and mention graphs This means that if user a retweets userês b tweets there is an inîuence from b to a  The same happens for tweet mentions In this way the weight matrix W ij is built by summing the number of times user j mentions or retweets user i those ideas are based in McKelvey and Menczer work 22 Inîuence graphs are constructed to represent speciìc users opinion Given an opinion set   every opinion    belongs to a node  i the nodeês i opinion To t the notation to the min-cut problem model we say if node i has a positive opinion then it is represented by  i  N   When it has negative opinion then  i  N  as is used in F o r e xample we could use an opinion set to indicate if a user knows or does not know a new product B Entropy Based Ranking The node ranking problem is wildly studied in web search document classiìcation citation analysis anomaly detection etc and many of the proposed solutions use graph representation 16 18 The most basic algorithm is the Degree Ranking DG it was mentioned in Kempe et al as a simple strate gy to nd extra seed nodes In this algorithm given the degree value of each node the nodes are sorted by degree values with a expected complexity of O  n log n   if quicksort is used For example if we apply this for twitter followers the node with most in-degree is the most popular node because in-degree represents the number of followers In-degree is represented by a function indegree  i  and the out-degree is represented by function outdegree  i   In this algorithm high out-degree values could represent spammers Although most of the ranking methods are based in metrics related with high node degree this metric does not necessarily implies importance This can be seen in Fig 1 where there are nodes linking two communities both with low indegree  v  and low outdegree  v   This is the reason why we decide to adopt graph entropy  F o r this we are using the entrop y deìnition in 24 which has the following expression Eq 1 H  G P  min x  SatableSet  G  n  v  V p v log  p v  1 This equation involves nding the Stable Set SS This set is the node collection L such that there is no edge between L members It is well know that nding SS is NP-complete problem To overcome this problem Shetty et al proposed to use the equation Eq 2 to compute graph entropy In 


Figure 1 The graph represents twitter user interactions which are formed with hastag israel The brown nodes have low in-degrees and out-degrees but are important because they link two big communities Data was taken from Indiana University Truthy project addition they also proposed an algorithm to determine node effect This metric evaluates the graph when a node is present and re-calculates the evaluation when the node has been removed The nodeês effect is calculated as follows 1 Compute node entropy E  v   2 Compute graph entropy without node to be evaluate EN  v   3 Calculate the effect with effect function Eq 3 H  G P  n  v  V p v log 1 p v  2 ef f ect  v  EN  v  log  EN  v  E  v  3 If these three steps are applied  v  V  all possible effects are calculated Then the nodes can be sorted in descendant order using the ranking/effect generated by the entropy Furthermore entropy could be calculated at different levels  l  Here level number is related to the direct and indirect inîuences between nodes For example l 1 represents direct inîuence i.e node v 0 directly inîuences node v 1 For l 1  we have indirect inîuences i.e for example node v 0 inîuences node v 1  and this in turn inîuences v 2  This means that there is an indirect inîuence of level two from v 0 to v 2  How the probability graph distribution P is calculated depends on analysis level For l  n  we need to count every single path of size n for each node v  Probability of node v is equal to all paths of length l  which contain v  divided by the total number of graph paths of length l   l  p v 0    v 0 v 1   E    v l  1 v l   E W l  1 l   l  v  G l  0 4 A nal note it does not make sense to analyze l with large values because inîuence is lost as you increase levelês value C Min-cuts for Inîuence Maximization Min-cuts are mainly used in social network analysis for graph reconstruction Suppose that we ha v e a directed weighted graph G that represents social networkês inîuences between users V by edges E  weight matrix W  and we know all members opinion about certain subject s  If all opinions are of two types positive or negative then we have two sets N  for positive opinion nodes and N  for negative opinion nodes In addition N  and N  are disjoint sets Therefore the min-cut can be deìned as a function that c  N  N   min  i,j   N   N   W ij 5 Now suppose we want to reconstruct the graphês node opinion set and we know some speciìc opinions represented by subsets    N  representing positive opinion and    N  representing negative opinion These subsets are called seed nodes and they have the characteristic that they never change their opinion no matter what We can use these seed sets and a min-cut algorithm to obtain the original opinion sets N   and N   which are known as source and sink sets In addition if we compare N  and N   subsets obtained from the min-cut algorithm from the original graph opinions we can get an error e  deìned as the number of nodes in N  which are misclassiìed In other words they are in N  after min-cut This error e can be reduced if we increase   size Then we can use min-cuts for IM with a dependable seed sets to infer the other member opinions in a v ery accurate manner This nal inference is determined depending on which side of the min-cut the node is In order to use the min-cut algorithms in the IM problem we need to specify in addition to seed sets the source s and sink t nodes The computation of the min-cut is accomplished by using a ow algorithm F o r e xample if we use the EdmondsKarp algorithm the running time is in O  VE 2   This is highly recommended for sparse graphs as the ones generated by social networks Finally we only need to determine the source and sink nodes There are two strategies to determine these nodes  The rst strategy takes the most representative node belonging to a community This can be done if we have some knowledge about the graph For example in the karate club problem if we do not choose as a source/sink the most representative nodes and select them randomly the results may give us a very different result For this selection we can use ranking algorithms to determine them we can select one node with high values in the positive opinion side and one for the negative opinion side 


Figure 2 The rst graph represents the source and sink election by high degree in their communities The second graph represents the extra node placement for source and sink nodes  In the second strategy we create extra source and sink nodes These nodes do not represent any real graph member it is only used to have a starting node and a ending node This technique is used in ow problems Fig 2 Once we have s and t  we need to link them with static seeds nodes with s  N  and t  N   For this we use the following directed weights  W sv      v       W v  t     v     Where set  is deìned as   012    These rules are from multi-source/multi-sink ow problem In addition they are used to not allow N  nodes does not go to the N  subset References to all this notation can be seen in Table I An example of an algorithm that uses these ideas of seed source and sink nodes is the Greedy Placement GP algorithm  This algorithm can nd e xtra seed nodes by e v aluation of node inîuence in running time of O  mV 2 E 2   where m is the number of seeds sought This is the base algorithm for comparisons in the experiment Section V III P ROPOSED M ETHOD In order to analyze the IM problem for Big Data we need to have low computational complexity algorithms due to TABLE I N OTATION USED IN THIS PAPER  Notation Description G Directed weight graph V Set of nodes vertex in G E Set of edges links in G W Node weights matrix N  Set of nodes classiìed as positive N  Set of nodes classiìed as negative   Positive seed nodes   Negative seed nodes    Positive extra seed nodes   Negative extra seed nodes  i Opinion of the ith node outdegree  v  Function to count number of out edges indegree  v  Function to count number of in edges H  G P  Entropy of G probability distribution P s Source node t Sink node c  N  N   Cut function GP Greedy placement algorithm DG Degree based algorithm EMinG Entropy based algorithm the size of the problems Therefore it is necessary to have fast algorithms that are suitable to obtain high quality results However most of the proposed approximation algorithms for the IM problem are based on greedy methods over combinatorial interpretations of the IM problem 15 12 8 This mak es them vulnerable to runaway computation with exponential bounds To address this we propose a novel algorithm which has two phases In the rst phase an analysis is carried out using node entropy ranking because is faster than use greedy approaches These greedy approaches measure every single node inîuence even when using min cut analysis This implies to compute min-cut as many time as nodes have the graph In the second phase the min-cut algorithm Edmonds-Karp is used locally to obtain an approximation in the opinions of the graph A Data Preparation To apply the proposed algorithm we need to decide in which part of the graph we are going to rank the nodes It could be in the entire graph or only a set of nodes This happens because local ranking of a node is not equal than the entire graph ranking In our speciìc problem we need to know which m nodes in N   could attract more N  nodes to N   The local ranking will be done in the N  side in order to reduce the number of operations To get N   the min-cut needs to be calculated After the N  subset is obtained we rank the respective nodes B Ranking Phase In this phase we want to identify prominent nodes This characteristic must be related to an individual power of persuasion Therefore the ranking serves as data preparation The top most m ranked nodes can be the extra seed nodes      Entropy ranking is only calculated in N  subgraph to reduce the number of operation Graph entropy is calculated by the formula 2 and node effect with 3 The algorithm for this phase is in Algorithm 1  Complexity of ranking is directly related to the number of nodes in the graph The computation of probability is made in O  N   l  and we only are going to compute inîuence level for l 1 and l 2  To compute effect is necessary to compute graph entropy which is caluculated with complexity O  N   2   Finally sorting of the nodes is done in O  N  logN   complexity and addition of nodes is done in O  N   complexity During experimentation it was discovered that a good level of inîuence in this phase was l 1 and l 2  An increase of the level values may result in loss of time without improving the quality of the extra seeds selected C Cut Phase The min-cut splits the graph in two sets N  and N   Because Min-cut is computed locally we need to establish source and sink nodes Locally this refers to obtain a local min-cut between these nodes and not the global min-cut Then every node i  N  is classiìed as a positive and every node j  N  is classiìed as a negative For our model the m nodes 


 Algorithm 1 Ranking algorithm Require Directed Weight Graph G  1      2 N   mincut  G  3 Combine all N  nodesasasupernode 4 entropy N   H  N  P N   5 for all v  i in N  do 6 G   N   v  i 7 entropy G   H  G  P G   8 ef f ect  i  entropy G  log  entropy G  E  v  i  9 end for 10 Sort ef f ect  11 for j 1 to  N   do 12 if v  j     then 13       012 v  j  14 end if 15 end for 16 return    Algorithm 2 Cut algorithm EMin Require Extra seed set     1 j 1 2 for i 1 to m do 3       j  4 while    N  do 5 j  j 1 6       j  7 end while 8 W s    9 N  mincut  G  10 Combine all N  nodesasasupernode 11 end for 12 return with high entropy ranking are chosen Then the following algorithm algorith m2\s applied As we see in the cut algorithm in line 4 there is a validation that guarantees a special case when a node with high entropy value was attracted previously to N  by other node In Fig 3 we illustrates the situation The algorithm deals with the situation in the following way The attracted nodes are not added to    because all the possible nodes which could be attracted by this node are also includes when they are members of N  side In the ranking and cut algorithms in line 3 and 10 respectively we note a special operation which combines all nodes in the N  side in a single node Supernode is a technique in graph theory to reduce the number of operations In this technique nodes in N  related by an edge to N  nodes are called cut nodes The technique takes all edges between positive cut nodes to negative cut nodes and fused them in one edge which is the sum of all weights of cut nodes This is also done for negative cut nodes linked to positive cut nodes All weights between N  nodes are ignored because it does not make sense to analyze N  interactions and they are also in the side that we need to increment Finally the supernode always will be the source node The algorithm time complexity consists of Min-cut comFigure 3 Special case representation In the left graph there are two top most ranked nodes in N   In the right after adding most ranked node to the N  side Also the second most ranked node is attracted because it is not necessary to take this node as extra seed plexity and the entropy effects calculations It needs to compute Min-cut over using the seeds generated by the entropy ranking Then the cut algorithm complexity is O  m  VE 2   D Adding Greedy to Entropy As we mention earlier greedy techniques may lead us to a spend more time to obtain results in addition to have low quality results Still we want to exploit the greedy methods main characteristic the reduction of the combinatorial part of the IM as an optimization problem Thus we can prune the number of combinations to compute high quality seeds We can take m  m  top ranked nodes and test which attract more nodes where m  is an extra search space to nd best extra seeds The value for m  could be determined by experimental tests In addition the traditional greedy algorithm is deìned when m  m    N    In Eq 6 we deìne EMinG algorithm as an optimization equation where    m  m  are top m  m  ranked nodes The nal algorithm with the new greedy part is deìned in algorithm 3    j argmax i  N     mincut  N      m  m   6 Algorithm 3 Cut algorithm with Greedy EMinG Require Extra seed set     1 j 1 2 for i 1 to m do 3 for j 1 to m  m  do 4 a  mincut  N       j  5 if a  max then 6 max  a 7       j  8 end if 9 end for 10 W s    11 Combine all N  nodesasasupernode 12 end for 13 return The complexity of EMinG is directly related with the number of seeds to be found For example Min-cut is computed m  m  m   times then we get a O  m  m  m   VE 2  complexity for the algorithm For IM m is very small compared with the number of nodes in the graph For example with m 5  


as we will see in section V we can attract 1033 nodes in a 7500 nodes graph Finally the proposed algorithm is a combination of mincut and ranking algorithms with greedy technique over nodes which can potentially be inîuential with this we can reduce time over greedy methods and get more quality extra seeds IV E XPERIMENT D ESIGN The proposed algorithm was applied over twitter data sets which are constructed from certain subjects and also it was applied over random build graphs We run all the algorithms with the same seed nodes discovered in the entropy ranking phase   and    In addition to the same source s node sink t node and those which were chosen from speciìc studied graphs A Algorithms for Comparison The proposed algorithm will be compared with two speciìc algorithms the DG algorithm and the GP algorithm DG was chosen to be compared with the algorithm in order to get an extra seed set     As the experiments will show the proposed algorithm is able to obtain more quality results better extra seed nodes when compared with the DG algorithm Although with the GP algorithm it is possible to obtain higher quality results the proposed algorithm is able to obtain faster results than GP when the size of the data sets increases We also use random seed that were chosen heuristically together with a single discount algorithm The single discount algorithm heuristic consist in a simple degree discount heuristic where each neighbor of a newly selected seed discounts its degree by one This is used in cascade models B Twitter Graphs We use twitter data sets from Truthy project Those graphs from politics social movements and news are used to try to understand how communication spreads on twitter This project recollects tweets for the last nine months and packed them in data sets available to the public Using graphês software visualization such Gephi it is possible to recognize some of the nodes preferences at the graphs Then it is possible to use them as a seed nodes positive and negative We also recollected data from twitter for marketing datasets where the users are related with a certain subject The subject could be hashtag or keyword and the user relations are found by mentions or retweets C Random Graphs Although some random graphs generators could not represents any real problem we are going to use a randomly generated datasets to compare the testing algorithms against the proposed algorithm in order to see how they behave as the graph increses in size degree at the nodes etc  For this random graphs are generated with well know algorithm BarabasiAlbert generator  The importance of using random graphs is because these can be conìgured to modify the average degree of the nodes sparsity and of course we can handle the amount of nodes to be used Modifying these parameters we can create graphs to represent real possible behaviors V E XPERIMENTS AND R ESULTS The algorithms were implemented on Intel Core I7 3.4 GHz with 8Gb RAM and this scenario was the same for all algorithm comparisons A Attracted Nodes As we mention earlier most of the traditional IM algorithms are focused in maximizing the amount of nodes which can change their opinion Under this assumptions it is clear GP is going to have the best results But as we saw in the last section it is possible to obtain most of the inîuential nodes by reducing the number of operations with EMinG In the  israel dataset Fig 4 GP and EMinG got the same results because high quality nodes are also included in top m  m  ranked nodes In this case m    V   7 was chosen for testing on all experiments In  immigration we got almost the same results with both algorithms The variation was of at most 15 extra seeds For example node N4620 is not of high entropy but it is inîuential Single discount had acceptable results For the rst and second experiments we got 1910 and 2005 nodes respectively In the random graph while looking for 30 extra seed nodes with GP we were able to attract 1171 nodes with EMinG we got 1066 Using the single discount algorithm we got 794 with DG we got 549 and with Random we got 240 In the fourth experiment DG and Random got the worst results Furthermore in the randomly generated graph it was more notorious the not so good results for DG and random seed placement For random seeed placemente seeds were selected at random and the second phase of EMinG was used to nd attrected nodes and extra seeds The main difference between the rst and third experiments is the out-degree node average In  israel is 1.21 in  immigration is 1.14 in random graph experiment is 2.48 and  syria 1.18 Although the rst second and third expriments Fig 4 were designed to show the performance of the algorithms the fourth experiment was done over a Big Data set 1,000,000 nodes in order to show the scalability of the algorithms Thus we noticed that with GP an EMinG we got 671310 and 650010 extra attracted nodes respectively It is more with m 30 DG and Random got small increases Finally the single discount algorithm had incremental results but they were much worse when compared with ones by GP and EMinG B Running Time The nal benchmark was done over the running time Fig 5 It was possible to see that DG and random seed placement were the fastest algorithms at the four datasets with respect to EMinG but it was only with an advantage of seconds For large amount of data EMinG was faster than GP The interesting fact was that the Single discount algorithm had almost the same results in time when compared with EMinG For example at the  israel graph in order to nd 5 extra seeds there were 300 seconds of difference between both algorithms DG and EMinG Still the nodes quality for DG were not as good as in EMinG We had a similar result for for the random graph With DG we obtained 858 attracted nodes with EMinG 1033 and 245 for random placement When we 


Figure 4 Number of attracted nodes over  israel 4,000 nodes  immigration 6,307 nodes Barabasi-Albert 7,500 nodes and  syria 1,000,000 graphs increased the number of seeds to 30 and used DG we got 1878 attracted nodes Instead with EMinG while looking for 25 extra seeds the algorithm got 1989 attracted nodes Finally with random seed only placemente there were only 1221 attracted nodes Now the GPês results were slower than EMinG This is because with GP we need to evaluate every single nodeês inîuence Then while looking for a small number of extra seeds it can be seen that it has lower running time But with 10 or more extra seeds to nd GPês time complexity increases faster than any other algorithm For example to nd 30 seeds in the Barabasi-Albert random graph GP runned for 11171.76 seconds against EMinG that runned only for 4814.04 seconds in addition that GP got only 9 more attracted nodes For the same experiment the single discount algorithm runned for 4579.09 seconds which is less than EMinG but with 111 attracted nodes less For  syria graph EMinG attracted each node in 0.83 seconds versus GP that attrected each node in 2.31 seconds Of course DG and random seed placement are faster than EMinG but as we saw in the Big Data Graph if we look for more than 30 extra seeds there are no signiìcant changes As we can see EMinGês computational time is related with the graph size because entropy ranking is calculated over all negative opinion nodes Finally The additional time in EMinG is for the greedy choice on top of the m  m  nodes VI C ONCLUSION AND F UTURE W ORK The inîuence maximization problem will continue to be studied from different points of view because the increasing interest in studying the interaction among members of social networks This is increasingly having a heavy impact in application development and gaming industry The proposed algorithm is based on simple algorithms and other simple techniques in order to be able to implement an highly scalable application for Big Data Of course the algorithm is still only trying to get an approximation to the optimal solution However the experiments showed the accuracy and high quality of the results of the proposed algorithm Additionally one the characteristics of the algorithm is the use of local information for increasing speed when dealing with large data sets In addition incorporating new ranking algorithms is quite simple because this phase was designed independently in the second phase of the proposed algorithm Also heuristics can be incorporated such as the ones applied in greedy algorithms to reduce the number of operations without loss of quality or the use of heuristics to choose the seed nodes In a future implementation many of the operations could be parallelized In addition we are working in developing new methods to analyze IM and mining large data streams to get faster results Finally for future applications we want to apply the proposed algorithm in no-social datasets to measure its effectiveness For example in biological sensor communication or networking datasets A CKNOWLEDGMENT We appreciate the nancial support given by CONACYT We want to thank computer science laboratory members for the tips reviews and recommendations over experiments and algorithms that were proposed Finally we want to thank people who are developing new algorithms new methods and new software which are the basis of this work R EFERENCES  M Shindler  A  W ong and A W  Me yerson F ast and accurate kmeans for large datasets in Advances in Neural Information Processing Systems 24  J Shawe-Taylor R Zemel P Bartlett F Pereira and K Weinberger Eds 2011 pp 2375Ö2383 


Figure 5 Running time over  israel 4,000 nodes  immigration 6,307 nodes Barabasi-Albert 7,500 nodes and  syria 1,000,000 graphs  Z Li H Ning L Cao T  Zhang Y  Gong and T  S Huang Learning to search efìciently in high dimensions in Advances in Neural Information Processing Systems 24  J Shawe-Taylor R Zemel P Bartlett F Pereira and K Weinberger Eds 2011 pp 1710Ö1718  N Halk o P  G Martinsson and J A T ropp Finding structure with randomness Probabilistic algorithms for constructing approximate matrix decompositions SIAM Rev  vol 53 no 2 pp 217Ö288 May 2011  A Metw ally  D  Agra w al and A E Abbadi  A n inte grated ef cient solution for computing frequent and top-k elements in data streams ACM Trans Database Syst  vol 31 no 3 pp 1095Ö1133 Sep 2006  P  Domingos and M Richardson Mining the netw ork v alue of customers in Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining  ser KDD 01 New York NY USA ACM 2001 pp 57Ö66  D K empe J Kleinber g and E T ardos Maximizing the spread of inîuence through a social network in Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining  ser KDD 03 New York NY USA ACM 2003 pp 137 146  S Liu L Y ing and S Shakk ottai Inîuence maximization in social networks An ising-model-based approach in Communication Control and Computing Allerton 2010 48th Annual Allerton Conference on  2010 pp 570Ö576  W  Chen Y  W ang and S Y ang Ef cient inîuence maximization in social networks in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining  ser KDD 09 New York NY USA ACM 2009 pp 199Ö208  W  Chen A Collins R Cummings T  K e  Z  Liu D Rincn X Sun Y Wang W Wei and Y Yuan Inîuence maximization in social networks when negative opinions may emerge and propagate in Proceedings of the Eleventh SIAM International Conference on Data Mining SDM 2011 April 28-30 2011 Mesa Arizona USA  2011 pp 379Ö390  W  Chen W  Lu and N Zhang T ime-critical inîuence maximization in social networks with time-delayed diffusion process CoRR  vol abs/1204.3074 2012  J T ang J Sun C W ang and Z Y ang Social inîuence analysis in large-scale networks in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining  ser KDD 09 New York NY USA ACM 2009 pp 807Ö816  H Li S S Bho wmick and A Sun Cinema conformity-a w are greedy algorithm for inîuence maximization in online social networks in Proceedings of the 16th International Conference on Extending Database Technology  ser EDBT 13 ACM 2013 pp 323Ö334  K Ohara K Saito M Kimura and H Motoda Ef fect of in/out-de gree correlation on inîuence degree of two contrasting information diffusion models in Proceedings of the 5th international conference on Social Computing Behavioral-Cultural Modeling and Prediction  ser SBPê12 Berlin Heidelberg Springer-Verlag 2012 pp 131Ö138  L P age S Brin R Motw ani and T  W inograd The pagerank citation ranking Bringing order to the web Stanford InfoLab Technical Report 1999-66 November 1999 previous number  SIDL-WP-1999-0120  J M Kleinber g  Authoritati v e sources in a h yperlink ed en vironment  J ACM  vol 46 no 5 pp 604Ö632 Sep 1999  K P  Chitrapura and S R Kashyap Node ranking in labeled directed graphs in Proceedings of the thirteenth ACM international conference on Information and knowledge management  ser CIKM 04 ACM 2004 pp 597Ö606  F  Chung A Tsiatas and W  Xu Dirichlet pagerank and trust-based ranking algorithms in Proceedings of the 8th international conference on Algorithms and models for the web graph  ser WAWê11 Berlin Heidelberg Springer-Verlag 2011 pp 103Ö114  J Shetty and J Adibi Disco v ering important nodes through graph entropy the case of enron email database in Proceedings of the 3rd international workshop on Link discovery  ser LinkKDD 05 ACM 2005 pp 74Ö81  D M Greig B T  Porteous and A H Seheult Exact Maximum A Posteriori Estimation for Binary Images 1989  L Onsager  Crystal statistics i a t w o-dimensional model with an order-disorder transition Phys Rev  vol 65 pp 117Ö149 Feb 1944  S.-W  Son H Jeong and J D Noh Random eld ising model and community structure in complex networks The European Physical Journal B Condensed Matter and Complex Systems  vol 50 no 3 pp 431Ö437 2006  K R McK elv e y and F  Menczer   T ruthy enabling the study of online social networks in CSCW Companion  A Bruckman S Counts C Lampe and L G Terveen Eds ACM 2013 pp 23Ö26  A DêAndrea F  Ferri and P  Grifoni An Ov ervie w o f Methods for 


Virtual Social Networks Analysis in Computational Social Network Analysis  ser Computer Communications and Networks A Abraham A.-E Hassanien and V Sn  ael Eds London Springer London 2010 ch 1 pp 3Ö25  J K orner  Fredman-k olmo s bounds and information theory   SIAM J Algebraic Discrete Methods  vol 7 no 4 pp 560Ö570 Oct 1986  T  Leighton and S Rao Multicommodity max-îo w min-cut theorems and their use in designing approximation algorithms J ACM  vol 46 no 6 pp 787Ö832 Nov 1999  M Bastian S He ymann and M Jacomy  Gephi An open source software for exploring and manipulating networks 2009  A.-L Barabasi and R Albert Emer gence of scaling in random networks Science  vol 286 no 5439 pp 509Ö512 1999 


application or middleware platform to collect request ows Thus it is much easier to deploy FChain in large-scale IaaS clouds Blacksheep correl a t e s t he change poi nt of s y s t em-l e v el metrics e.g cpu usage with the change in count of Hadoop application states i.e events extracted from logs of DataNodes and TaskTrackers to detect and diagnose the anomalies in a Hadoop cluster Kahuna-BB correl a t e s b l ack-box dat a system-level metrics and white-box data Hadoop console logs across different nodes of a MapReduce cluster to identify faulty nodes In comparison FChain is a black-box fault localization system which is application-agnostic without requiring any knowledge about the application internals We believe that FChain is more practical and attractive for IaaS cloud systems than previous white-box or gray-box techniques V C ONCLUSION In this paper we have presented FChain a robust blackbox online fault localization system for IaaS cloud computing infrastructures FChain can quickly pinpoint faulty components immediately after the performance anomaly is detected FChain provides a novel predictability-based abnormal change point selection scheme that can accurately identify the onset time of the abnormal behaviors at different components processing dynamic workloads FChain combines both the abnormal change propagation knowledge and the inter-component dependency information to achieve robust fault localization FChain can further remove false alarms by performing online validation We have implemented FChain on top of the Xen platform and conducted extensive experimental evaluation using IBM System S data stream processing system Hadoop and RUBiS online auction benchmark Our experimental results show that FC hain can achieve much higher accuracy i.e up to 90 higher precision and up to 20 higher recall than existing schemes FChain is light-weight and non-intrusive which makes it practical for large-scale IaaS cloud computing infrastructures A CKNOWLEDGMENT This work was sponsored in part by NSF CNS0915567 grant NSF CNS0915861 grant NSF CAREER Award CNS1149445 U.S Army Research Ofìce ARO under grant W911NF-10-1-0273 IBM Faculty Awards and Google Research Awards Any opinions expressed in this paper are those of the authors and do not necessarily reîect the views of NSF ARO or U.S Government The authors would like to thank the anonymous reviewers for their insightful comments R EFERENCES   A m azon E las tic Com pute Cloud  h ttp://a w s  a m azon com ec2   V i rtual c om puting lab  http://vcl ncs u  e du  P  Barham  A  D onnelly  R I s aacs  a nd R M o rtier   U s ing m agpie f or request extraction and workload modelling in 
 2004  M  Y  Chen A  A ccardi E  K icim an J  L lo yd D  P atters on A  F ox and E Brewer Path-based failure and evolution management in  2004  R F ons eca G  P o rter  R H  K atz S  S h enk e r  and I  S toica X T race A pervasive network tracing framework in  2007  I  Cohen M  G o lds z m i dt T  K elly  J  S ym ons  a nd J  S  Chas e Correlating Instrumentation Data to System States A Building Block for Automated Diagnosis and Control in  2004  I  C ohen S  Z h ang M  G o lds z m i dt J  S ym ons  T  K elly  a nd A  F ox Capturing indexing clustering and retrieving system history in  2005  S  D uan S  Bab u  a nd K  M unagala F a A s ys tem for a utom ating failure diagnosis in  2009  S  K andula R Mahajan P  V erkaik S  A garw al J  P a dhye a nd V  Bahl Detailed diagnosis in computer networks in  2009  A  J  O liner  A  V  K ulkarni and A  A ik en  U s ing c orrelated s u rpris e to infer shared inîuence in  2010  P  Bahl R Chandra A  G r eenber g  S  K andula D  A  M altz and M Zhang Towards highly reliable enterprise network services via inference of multi-level dependencies in  2007  Z  G ong X  G u  a nd J  W ilk es   P RE S S  P Redicti v e E las tic ReS ource Scaling for Cloud Systems in  2010  H  N guyen Y  T a n and X  G u P A L  P ropagation-a w are a nom aly localization for cloud hosted distributed applications in  2011  B Gedik H Andrade K L  W u P  S  Y u and M  D oo SP ADE  t he system s declarative stream processing engine in  2008  A pache H adoop S y s tem   http://hadoop apache  or g/co re   Rice uni v e rs ity bidding s y s tem   http://rubis  objectw eb  o r g   M Ben-Y e huda D  B reitgand M F actor  H  K o lodner  V  K r a v ts o v  and D Pelleg NAP a building blo ck for remediating performance bottlenecks via black box network analysis in  2009  Y  T a n X  G u  a nd H  W a ng  A dapti v e s ys tem anom aly prediction f or large-scale hosting infrastructures in  2010  D  L  M ills   A b rief his t ory o f N T P tim e m e m o irs o f a n i nternet timekeeper  2003  Y  T a n H  N guyen Z  S h en X  G u C V e nkatram ani and D  R ajan PREPARE Predictive Performance Anomaly Prevention for Virtualized Cloud Systems in  2012  M  Bas s e ville and I  V  N ikiforo v   Prentice-Hall Inc 1993  L  Cherkaso v a  K  O zonat N Mi J  S ym ons a nd E  Sm irni  Anom aly application change or workload change towards automated detection of application performance anomaly and change in  2008  P  Barham and e t al   X e n a nd the a rt of virtualization  i n  2003  T he ircache p roject  h ttp://www.ircache.net  H ttperf  h ttp://code google com  p htt p er f  S  K u llback  T h e ku llback-leibler distance  1987  X  Chen M  Z hang Z  M  M a o and P  B ahl  A utom ating n etw ork application dependency discovery experiences limitations and new solutions in  2008  M Y u  A  G reenber g  D  M altz J  Re xford L  Y u an S  K andula and C Kim Proìling network performance for multi-tier data center applications in  2011  M K  A guilera J  Mogul J  W iener  P  R e ynolds  a nd A  Muthitacharoen Performance debugging for distributed systems of black boxes in  2003  S  A g arw ala F  A l e g re K  S chw a n and J  M ehalingham  E 2E P r of A utomated end-to-end performance management for enterprise systems in  2007  P  Re ynolds  J  L  W iener  J  C M ogul M  K  A guilera and A  V ahdat  WAP5 black-box performance debugging for wide-area systems in  2006  R Apte L  Hu K  S chw a n and A  G hosh L ook W ho s T alking Discovering dependencies between virtual machines using cpu utilization in  2010  G Khanna I  L aguna F  A rshad an d S Bagchi Distr ibuted diagnosis of failures in a three tier e-commerce system in  2007  R R S a m b as i v an A  X  Z heng M  D e Ros a  E  K re v at S  W h itm an M Stroucken W Wang L Xu and G R Ganger Diagnosing performance changes by com paring request ows in  2011  J  T a n a nd P  N a ras i m h an  RA M S and B lackS h eep I nferring w h ite-box application behavior using black-box techniques CMU Tech Rep 2008  J  T a n X  P a n E  Marinelli S  K a vulya R  G andhi a nd P  N a ras i m h an Kahuna Problem diagnosis for mapreduce-based cloud computing environments in  2010 
OSDI NSDI NSDI OSDI SOSP ICDE SIGCOMM DSN SIGCOMM CNSM SLAML SIGMOD ICAC PODC Computer Communication Review ICDCS Detection of abrupt changes theory and application DSN SOSP The American Statistician OSDI NSDI SOSP DSN WWW HotCloud SRDS NSDI NOMS 
207 
30 
30 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





