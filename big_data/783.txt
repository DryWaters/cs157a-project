g 
Xiaodong Yu Daniel DeMenthon and David Doermann 
f 
Abstract 
Institute for Advanced Computer Studies University of Maryland College Park 
Training a classi\002er for object category recognition using images on the Internet is an attractive approach due to its scalability However a big challenge in this 
Support Vector Data Description for Image Categorization From Internet Images 
xdyu,daniel,doermann umiacs.umd.edu 
approach is that it is dif\002cult to automatically obtain sets of negative samples that are guaranteed to be free of positive samples In this paper we propose to address this challenge with a Support Vector Data Description SVDD classi\002er An SVDD classi\002er does not need negative images in training It computes a hypersphere around the potentially good images in the feature space and uses this boundary to distinguish images of target visual category from outliers Evaluation on standard test sets shows that we are able to achieve competi 
tive classi\002cation performance using the contaminated training images from the Internet without the need for large datasets of negative examples 
Recently learning object categories from images on the Internet has gained attention in the computer vision community 1 4 Compared to the co n v entional approach that uses training samples collected manually learning from the Internet images seems much more scalable Given any visual category we could submit the category name to an Internet image search engine 
1 Introduction 
such as Google Image Search and then thousands of images are available for training However although it is rather easy to obtain positive images from Internet it is nontrivial to obtain acceptable sets of negative images We can type 223car\224 in a search engine and expect to receive images related to cars But it is dif\002cult to explicitly search for images that do not contain cars One may consider typing several random nouns in the search engine and using the returned images as negative images for training How 
ever we need to manually check these images to make sure that there are no instances of the target category in them Otherwise the positive samples labeled as negative will contaminate the classi\002er Due to this issue conventional two-class classi\002ers such as SVM may not work well for the problem of training classi\002ers using Internet images We note that what is important in decision making at test time is the boundary between positive data and negative data Thus if we can de\002ne a boundary of positive 
data it can be used as a classi\002er This is called a oneclass problem in machine learning Following this line of thought in this paper we propose to train an image classi\002er using only the positive samples returned by the image search engine Our contribution is to show that we are able to obtain competitive classi\002cation results using the disparate training images from the Internet without the need for large datasets of negative examples The background of the proposed approach is presented in Section 2 Implementation details are de 
scribed in Section 3 Finally we present the experimental results in Section 4 
As discussed in Section 1 we formulate the problem of learning an object category from Internet images as a one-class problem The object category we are interested in is called the target class and the images not belonging to this category are called outliers Our goal is to train a classi\002er that could estimate a boundary useful for separating images of the target class from 
2 Approach 
the outliers In this paper this is achieved by using the Support Vector Data Description SVDD classi\002er introduced by Tax and Duin T o mak e this paper more self-contained we now summarize their approach in the 978-1-4244-2175-6/08/$25.00 \2512008 IEEE 


027 3 subject to X 0 x x x 030 y y and 0 037 0 k in 1 The parameter 1 2 2 2 2 2 2 2 2 1 1 1 1 2 2 1 2 2 1 2 2 2 2 2 2 1 2 N i N i i i i i i i i i i 013 i;j i j i j i i i i i i i i i i i i i i i i i i N 027 i N 027 i i i j i j i j k k k i i i i i i N We use the min 030 x 6 0 0 x  If is classi\002ed as an outlier Note that in 3 030 R 013  rest of this section For the mathematical details of this approach please refer to Tax and Duin's seminal paper 6 Suppose we are given a dataset  To improve the generality of the classi\002er some remote samples in the training set are allowed to be located outside the sphere but larger distances to the center should be penalized Thus slack variables 1 subject to and accounts for possible errors and the parameter is within the sphere or on the boundary there is no error and the corresponding is equals to to the boundary of the sphere Using Lagrangian multipliers we obtain the dual problem  If is within the sphere the inequality constraint in 2  is satis\002ed and the corresponding Lagrangian multiplier  is zero Otherwise for on the sphere  has to be enforced and the Lagrangian multipliers will become non-zero i.e or  All samples with positive are called Support Vectors of the SVDD Given a new sample This section describes the implementation of our approach We 002rst discuss the bag-of-word image representation and our similarity measurement Then we describe the kernel used in our approach as well as its properties and the method used to choose the parameters in our experiments distance which is de\002ned as kernel in the SVDD frame w ork There are tw o free parameters to adjust the kernel width and the tradeoff parameter in\003uences the complexity of the boundary and the number of the support vectors Tax and Duin sho w that for v ery small v alue of  3 is optimized when all samples become support vectors with equal  With the increase of  the D 3 Implementation Details X  013 1 b r r r r z 013  1 037 030 030 027   N 027 030 N 027  013 013 013 N 027 013  R     x   x  y   y  H x  z is the training set The main idea of SVDD is to obtain a spherically shaped boundary around the training dataset such that the sphere can enclose as many samples as possible while having minimum volume The sphere is characterized by its center 0 030 5 In a way analogous to the Support Vector Classi\002er a 003exible boundary description can be obtained by choosing an appropriate kernel function x  we construct a 033 R and 030 with the lowest false positive rate on the validation dataset is selected as the optimum value Note that in contrast with two-class classi\002ers the validation dataset does is not involved in the training but it is only used to select an optimal parameter The kernel width k 027 lies inside the hypersphere it belongs to the target class otherwise N 013 H R can be replaced by a kernel function min R k k 1 1 validation dataset Kernel and Parameters Selection R X 033 X X X For a given image we 002rst extract PCA-SIFT descriptors after con v ertin g it to gray scale The PCA-SIFT descriptors are then vectorquantized using a pre-computed visual word codebook including 1000 visual words Finally each image is represented by a histogram of the visual words To compare two histograms is an upper bound on the fraction of training points outside the estimated region and a lower bound on the fraction of support vectors A large g 2 X 025 000 024 025 001 001 024 000 000 001 000  030 030  x y H x f x 4 Solving the dual optimization problem yields c 027 0 z  The minimization of the sphere volume is achieved by minimizing its square radius  otherwise  or beyond the sphere   the equality constraint  we compare its distance to the center of the sphere with the radius of the sphere  we use the  which includes 1000 background images The k is the squared distance from  where x 027 controls the trade-off between the hypersphere volume and the errors If r r r r x R  x  1       1   0       b           1 2     and radius square are introduced and the minimization problem is formulated as is the number of samples and 030 000 001 allows more points to be assigned outside the sphere which corresponds to a larger rejection rate and a larger number of support vectors To select the best H  Image Representation x 033 2 where x 027 0 013 013 013 c c 024 only appear in the form of inner products with other points Hence the inner product x x x x x x x x x 033 


We obtain this dataset with a similar method to Fergus's Google validation set  W e translate se v en cate gory names into 002 v e languages German French Dutch Italian and Chinese and then use them along with the English version as queries for Google Image Search The top 20 images are downloaded and images in different languages are combined into a single training set Thus this training set contains 120 images for each category This subset will be used to train the classi\002ers in our approach Caltech test set Google raw set Google training set 0 1 Table 1 Comparison of the proposed approach with other weakly supervised training approaches 4 Experiments 4.1 Datasets 027 027 2 2 2  we initialize It consists of a large number of images returned by Google's image search using the category name It may be contaminated by images unrelated to the category and the proportion of good images ranges from 18.1 to 63.4 Details about this dataset are given in This subset will be used in the Search Engine Improvement tests 3 4.2 Classi\002cation Experiments 4.3 Search Engine Improvement Experiments 033  It w as manua lly g athered and the pose of the object is quite constrained within these frames This subset will be used as the test set in the classi\002cation experiments 2  033 in our experiments Now the SVDD has a large number of support vectors When we increase 15.5 16.0 16.2 10.5 12.3 6.2 17.7 recall that is the lower bound on the fraction of support vectors Beyond this point the volume of the sphere may be too large and we will risk enclosing too many outliers boundary is smoother the volume of the enclosed region is increased the number of support vectors decreases Given a with a small value   the fraction of the support vectors decreases The optimal is selected when the fraction of support vectors 002rst reaches the speci\002ed Our experiments used part of Fergus's ICCV'05 datasets under the same setting as in plus our o wn datasets The resulting dataset contains seven different object categories Five of these are from Caltech datasets Airplane A Car Rear C Leopard L Face F and Motorbike M Two additional categories are Guitar G and Wrist Watch W For each category three subsets of data were compiled and they are described as follows 1 In these experiments classi\002ers are trained using the Google training set For each category images in the Caltech test set are mixed with an equal number of images from the Caltech background set The task is to correctly classify each image as belonging to the class or as background image The results of this experiment are illustrated in Table 1 compared with other weakly supervised learning approaches Within three models trained from Google images SVDD achieves the best performance in four categories and TSI-pLSA in three categories For 223Airplane\224 SVDD performs almost as well as TSI-pLSA We attribute the superior performance of TSI-pLSA to their use of spatial information Indeed compared to the pLSA approach which also employs the bag-of-words image representation our approach is competitive In the images in the Google ra w set are manually labeled as Good Intermediate and Junk based on their relativeness to the speci\002c category where Good images show dominant object without major occlusion while Junk images are totally unrelated to the category and the Intermediate images are those in between In tests we take the Good images as positive images and the Intermediate and Junk images as negative images The trained models are used to re-rank the images in the Google raw set We then compare the trained models to Google Image Search by the recall-precision curves In Figure 1 we compare the precision at 15 recall for the raw Google images SVDD pLSA and TSIpLSA SVDD achieves signi\002cant improvement over raw Google Image Search on precision in most categories and achieves higher precision than pLSA in four categories pLSA TSI SVDD 2 5 Supervision None None None Label Label A 24.7 15.7 7.0 11.1 C 21.0 20.5 9.7 8.9 F 20.3 20.7 3.6 6.5 G 17.6 31.8 L 15.0 13.0 10.0 M 15.2 13.5 6.7 7.8 W 21.0 19.9 033 027 


CVPR IJCV CVPR  pages 1816\2261823 2005  R Fer gus P  Perona and A Zisserman Object Class Recognition by Unsupervised Scale-Invariant Learning In  volume 2 pages 264\226271 2003  Y  K e and R Sukthankar  PCA-SIFT A More Distincti v e Representation for Local Image Descriptors In  volume 2 pages 506\226513 2004  L.-J Li G W ang and L Fei-Fei OPTIMOL automatic Object Picture collecTion via Incremental Model Learning In  pages 1\2268 2007  A Opelt M Fussene gger  A Pinz and P  Auer  W eak Hypotheses and Boosting for Generic Object Detection and Recognition In  volume 2 pages 71\22684 2004  D T ax and R Duin Support V ector Data Description  54\(1 2004  A T orralba R Fer gus and Y  W eiss Small Codes and Large Databases for Recognition In  2008  J Zhang M Mar szalek S Lazebnik and C  Schmid Local Features and Kernels for Classi\002cation of Texture and Object Categories A Comprehensive Study  73\(2 2007 5 Conclusion and Future Work References ICCV CVPR of the target class from outliers The results are competitive with state-of-the-art pLSA model Though both the pLSA approach and the SVDD approach can learn from contaminated training data they provide different solutions to this problem The pLSA approach focuses on coherence among visual words while SVDD emphasizes consistency in the transformed space It would be interesting to see if we can combine these two methods into a single framework and thus obtain bene\002ts from both CVPR ECCV Machine Learning We proposed an approach for learning object categories from Internet images in a weakly supervised manner This is achieved by the Support Vector Data Description which uses the smallest hypersphere around the target class in the feature space transformed by a kernel as a boundary to distinguish the images Figure 2 and Figure 3 compare the top 20 images returned by the Google Image Search and those by the SVDD respectively The colored letters in the top-left corner of each image show the ground-truth labels 223G\224 in green  Good image 223I\224 in blue  Intermediate image 223J\224 in red  Junk image Notice the improved image quality when compared to the raw Google images in Figure 2 with 15 good images in SVDD re-ranked set vs ten good images in Google raw set Figure 2 and Figuer 3 compare the top 20 images returned by the Google Image Search and those by the SVDD respectively The colored letters in the top-left corner of each image show the ground-truth labels 037 Figure 1 Improvement in precision at 15 recall over raw Google ranking with SVDD in comparison with pLSA model and TSI-pLSA model Figure 2 Top ranked images by Google Image Search for the 223Guitar\224 category Figure 3 Top ranked images by the SVDD classi\002er trained from our Google training set for the 223Guitar\224 category 2  R Fer gus L Fei-Fei P  Perona and A Zisserman Learning Object Categories from Google's Image Search In 


one relationship between are getting smaller and 0.11317 0.12090 0.10844 0.09526 0.00268 0.03162 The uniformly low P-values for all tests indicate a violation of the null hypothesis of homoskedasticity i.e a constant variance error term This is not surprising given the large differences in scale between missions in the data set It does not mean that coefficients developed using ordinary least squares regression a method we frequently will use will be biased but that their standard errors may be underestimated thus potentially inflating the significance of some variables It is something we need to be aware of though not necessarily control increasing that variable X led to a proportionally faster or slower increase in costs For example assume that costs were found to be proportional to Xy i.e X raised to Y power If Y is equal to 1 then as X increases costs increase at the same rate If Y is greater than 1 as X increases costs increase more quickly The greater the value of Y the higher the increase in costs associated with the same increase in variable X Conversely of course if Y is less than 1 as X increases costs increase more slowly We also considered whether the rate of change of each variable X with respect to costs changed over time That is we considered the Y which produces the greatest correlation between XY and cost changes over time by looking at the correlation between cost and various XY throughout various windows of time Correlation Matrix Table 1 offers basic insight into which factors tend to drive others with correlations above 2 underlined An example of this approach for mass is shown in table 2 It seems that in the beginning of the sample the best correlations were achieved with Mass raised to low powers Over time the optimal power on Mass has risen albeit with COST mass maxwatts designlife instruments launch Planetary envelope heogeo deployables maxdata bands buslegacyweaker payloadlegacyweaker 1.00 0.60 0.37 0.35 0.12 0.13 0.22 0.42 0.05 0.06 0.34 0.22 0.09 0.24 1.00 0.48 1.00 0.19 0.60 1.00 0.02 0.11 0.08 0.04 0.22 0.23 0.16 0.18 0.20 0.59 0.39 0.22 0.11 0.09 0.04 0.08 0.02 0.00 0.32 0.12 0.02 0.12 0.24 0.30 0.05 0.10 0.03 0.06 0.02 0.04 Table 1 Correlation matrix for data set The matrix shows pair-wise correlations between variables shown versus bus legacy There is we began systematically inspecting the correlation between cost and each of the variables a sufficient number of observations Raising variables to various powers provided an indication not only about whether cost and were related but also whether mass tended to increase much faster than cost increased Toward the end of the sample cost still increased as slowly Ceteris paribus mass appears to be increasing in importance as a driver of cost One explanation is that while materials are getting more expensive It is possible that was essentially replicated with log-transformed variables 5 1.00 0.27 0.15 0.05 0.12 0.13 0.17 0.11 0.20 0.27 1.00 0.18 0.02 0.26 0.02 0.13 0.12 0.08 0.15 1.00 0.10 0.07 0.03 0.11 0.03 0.12 0.12 1.00 0.05 0.03 0.04 0.07 0.03 0.13 1.00 0.07 0.05 0.04 0.10 0.19 1.00 0.23 0.10 0.11 0.11 1.00 0.13 0.11 0.03 1.00 0.11 0.08 1.00 0.42 1.00 CHI-SQUARE TEST STATISTIC E**2 ON YHAT 2.509 E**2 ON YHAT**2 2.406 E**2 ON LOG\(YHAT**2 2.577 E**2 ON LAG\(E**2 ARCH TEST 2.783 LOG\(E**2 ON X HARVEY TEST 26.918 ABS\(E ON X GLEJSER TEST 19.759 D.F P VALUE 1 1 1 1 10 10 more slowly than mass but not nearly more powerful they we possessed in mass and cost will exist within over time on either of its axes Among the findings above are getting lighter and electronics are strong associations between cost and mass cost and size envelope power maxwatts and design life and weak payload I a smaller negative association between launch year and cost Single Variable Correlations with Cost To explore the explanatory power of the data a given variable X a slight decline post 2000 From these results it may be concluded that early on a one to a decade This finding 


mass and cost is high in almost all periods studied and seems to be rising over time However its power term is always less than same tendencies over time the power mass.3 mass.4 mass.5 0.3083 0.2991 0.2895 0.6261 0.6185 0.6062 0.5897 0.5808 0.5687 0.6329 0.6202 0.6063 0.8747 0.8754 0.8702 0.8924 0.9078 0.9156 0.8721 0.8853 0.8934 0.7967 0.7955 0.7924 0.7179 0.7233 0.7233 mass.6 0.2797 0.5901 0.5541 0.5914 0.8610 0.9169 0.8973 0.7879 0.7183 mass.7 mass.8 0.2699 0.2603 0.5711 0.5503 0.5376 0.5198 0.5756 0.5590 0.8492 0.8364 0.9130 0.9056 0.8980 0.8960 0.7820 0.7750 0.7093 0.6970 Table 2 Time-phased power vs correlation table for mass Particularly significant variables Mass is the most relevant variable when it as a cost estimating variable being uncertain about causality it could be argued that duration is strictly driven by cost and mass and costs is positive and fairly high which remains true when taking into account volume and watts Additionally  The linear effect of were generally ten years long in increments of five years 0.6546 0.6444 1985 1995 0.8524 0.8672 1990 2000 0.8375 0.8690 1995 2005 0.8261 0.8527 2000  0.7925 0.7958 on watts a bigger driver of the cost of construction of we chose not to include development months as an input Less significant variables Design life is seems to be declining When design life is increased costs increase although by a lesser proportion Data rate has increased roughly in proportion to cost throughout much of the sample period and this correlation has been quite high Toward the end of the sample data rate has ceased to have much explanatory power Pointing accuracy has had seems to have increased faster than cost However this hasn't been true of all time periods and the explanatory or a regular pattern of increase a series of regressions were run over various time periods using these variables The time periods chosen so that enough observations would be included The previous correlation study indicated that on them both We also tried varying the power term in each window a  MASSX  b  VOLY  c  WATTSZ However we did not find patterns worth reporting APPROACHES SURVEYED Given the challenging nature of this work several estimating methods a breadth first effort evaluating each approach to over time though beyond this we evaluated widely varied methods 6 mass.1 mass.2 1964-1975 were pursued in some explanatory power with respect to costs throughout much of the sample and accuracy was achieved For example COST  mass and volume seems to have been rising excepting the last chronologic window tested  Watts's explanatory power is increasingly uncertain when both mass is increasing This indicates that while power generation is becoming are accounted for Mass is the primary explanatory variable to over time the correlation between case The correlation between us to judge whether it merited further investigation All involved 0.3244 0.3169 1970 1980 0.6254 0.6285 19751985 mass upon adjusted cost or decrease in importance These include  Apogee  Number of instruments  Newtons in-orbit propulsion impulse  Deployables number of  Communication Bands number of  Degree of autonomy  Number of participating science organizations Variable Changes Over Chronologic Windows Since single-variable correlations indicated that Mass Volume and Watts so that a best fit seems to have are that this will continue to be the one This means that though an increase in mass leads to an increase in costs the increase is proportionally less However the difference between their rates of change is disappearing This is not surprising given the increased sophistication of materials and miniaturization of components Judging by correlations alone size envelope is probably the next most important variable in explaining costs and it so not appropriate all years 0.6901 0.7068 as Mass albeit less pronounced Watts also appears to be extremely important but were the most important variables a spacecraft the cost of providing that power per watt is decreasing rapidly Development Months also appear to be strongly correlated with costs However a variable which a strong correlation a one unit change in a one unit change in volume upon adjusted cost a lesser extent volume and watts a degree sufficient for was highly correlated with costs early in the sample but whose explanatory power seems to have been dropping excepting the last chronologic window tested  The linear effect of seems to be dependent some manner of trending to anticipate change comes to explaining cost and indications seems to decrease yet the correlation between watts and some of the 0.5961 0.5949 1980 1990 power of pointing accuracy has been dropping since the 1970s No regular patterns found Other tested factors did not show either 


 In 1  Accuracy jn\(1  Accuracy 2 In 1  Accuracy Tech Years TechYears1 4 x ccdh 1 X ccdh2  development months development months1.2 ddevelopment months1.8 In\(development month 2 In deve opment month 1.8 planetary number of science orgs x number of sci orgs.8  number of sci orgs1.2  number of sci orgs1.6 jIn number of sci orgs In number of sci L u.u C Figure 6 Coefficient values when stepped in for a stepwise regression conducted over several time windows Coefficient trend determination through stepwise regression Several attempts were made to manually develop explanatory models and then determine whether coefficients in those models changed over time No strong trend in coefficient values could be discerned based on our manual efforts We also conducted a stepwise analysis where the full range of variables was exposed to a stepwise regression method within progressive chronologic windows of the data The stepwise algorithm chose particularly significant variables from among those offered We then examined the result to determine not only which variables were regularly stepped weighted average of various spacecraft technical characteristics 3 We produced a variation on this to see whether weightings might regularly vary over time in such a way that they could be trended to estimate missions lying in the future The variables we chose for this were DryMass Envelope Watts Impulse in Physical Size Power Newtons  of Bands CC&DH GN&C GN&C Redundancy Redundancy Sensors Design Autonomous  of  of Science Life Design Instruments Organizations Type of Mission Comm Surveill Science Mgt 7 9 In\(1  Accuracy orgs 4 In number of sci orgs 8 In number of sci orgs A cap 1  cap.2 cap.6 cap.8  capl.4 cap2 intIO.4 intl.8  intl1.2 A intl1.6 comm Mgtl 4 In\(1  Mgt In 1  M gt 8 ln\(1  Mgt nni We will present the following methods with particular attention given to the most successful one  Coefficient trend determination through static models and stepwise regression  Complexity index by automatically weighted inputs  Complexity index by analogy  Neural networks  Automated windowed coefficient trending  Boosting using binomial logit classifiers  Weighted combinations of simple models in but also whether a trend could be discerned for their coefficients Figure 6 shows the result of this analysis Again no strong trends could be inferred Complexity Index by Automatically Weighted Inputs For our next approach we adopted a common formula for estimating spacecraft cost cost a  mass complexity One published complexity metric has been formulated as a 10000.00 1000.00 100.00 Cu 40 _ 100 _ 0 1 00 1 0.10 mass.6 deployables design lifel.8 Complexity watts wattsO.4 watts 1.6 Inwatts newtons newtonsO.9 newtonsA1 1 newtons2 In\(1  newtons In\(1  newtons ln\(1  newtons ln\(1  newtons data data.6 ln\(1  data ln\(l  data x Accuracy^O.3 X AccuracyA1.7 I 


Data Rate Pointing accuracy Year 10 _I  Ill on_ _X5 4 31 Deployables Figure 7 shows OD Figure 7 Weights some extent is means were unlikely to be accurate Complexity By Comparison A metric capturing overall spacecraft complexity is was carried out a complexity index solved to minimize predictive was normalized by percent ranking error for the test set As time varies seen that optimal weights change according to seemingly predictable trends This to error was minimized The optimization algorithm also required starting points for weights and or any other a very seductive goal For cost estimating purposes spacecraft a spacecraft can be are listed in the z axis The heights of the bars represent the optimal weights found to minimize forecast were used to calibrate models that then forecast missions so that forecast a variety of settings so that data from previous chronologic windows a deterministic path For this reason a simple LEO orbit to missions requiring complex flight dynamics pointing knowledge etc The payload an artifact of the calibration process which used prior periods solved weights over the 2001-2005 timeframe this process across chronologic windows it one such outcome of this sliding calibration of weights Chronologic windows progress CD OC o C           r 0 O~~~~~~~~D00 O over time for factors contributing to error Each of these variables across the full prior sample Weightings were determined using a Generalized Reduced Gradient nonlinear optimization algorithm available in Microsoft Excel's Solver Weightings were solved were tested including a naive beginning setting all weights to 1 and using prior periods solved weights as the starting point The method was repeated with progressively more recent chronologic windows The first window covered missions from 1964-1975 and the last from 1991-2000 In this way we hoped to observe trends that could be extrapolated across the x axis as the starting point The chart above shows the smoothest transition of weights over time while other weight starting points resulted in less smooth transitions Over any 10-15 year period as can be observed the weights are highly unlikely to maintain we felt that future estimates using a complexity adjustment derived from reduced gradient methods are otherwise fairly difficult to describe Complexities in the mission profile can vary from can also vary from essentially off-the-shelf equipment to those having a low technology readiness level at time of conception A single metric summarizing the relative complexity of versus 8 Developme Apogee Miles Apogee Planetary nt Months Mission Dummy  of  of Primes Budget Int'l Project Customers Capped Adjusted Final Cost CD rI I I across the width of the page while factors 


others could carry critical information going beyond relatively blind factors such as mass power and number of instruments We therefore made several attempts to generate a complexity metric The complexity by comparison method requires a human analyst's assessment of the relative complexity of a mission versus others We used a tool that facilitates relative assessments within a relatively automated framework A relatively naive analyst made the assessments as we wanted to develop this approach on a worst-case basis to gauge the minimum estimating performance of this approach Comparisons were made against past missions only to replicate what an analyst would face when trying to predict the complexity of a future mission Through trial and error a model was obtained of this form ln\(cost  constant  ln\(months since 1960  attenuated variable 1  attenuated variable 2 Attenuated Variable 1  sum of two smallest of ln\(mass ln\(watts and ln\(complexity Attenuated Variable 2  sum of two largest of ln\(lifetime number of deployables and number of instruments Where complexity was estimated using the comparisons method Results were produced within chronologic windows starting in 1989-1994 using regressions calibrated Forecast Start Year 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 As can be seen in table 3 as time goes on an increasingly reasonable estimate band was obtained so that by the 20042008 timeframe using outside authorities predicted cost for the last few missions 5000 of estimates lay within 24%60%.6 While the results were fairly good versus other methods we were concerned with the average analyst's ability to make relative complexity assessments particularly given the uncertain nature of future projects We also were concerned that the better recent results were unwittingly due to the analyst's even passing knowledge of the current state of the art For a model intended to estimate contemporary missions these results are good news but our focus is on the future Neural Analysis Neural networks are an estimating method based on a network of interconnected nodes which adaptively reweight so that a set of inputs best approximates an expected output From the Wikipedia entry In more practical terms neural networks are non-linear statistical data modeling or decision making tools They can be used to model complex relationships between inputs and outputs or to findpatterns in data In testing this approach we divided our sample into a training set for calibrating the network and a test set for evaluating its predictive accuracy Neural networks train or adapt over epochs The wisdom of the neural network community is to avoid overtraining by stopping the neural Absolute  Error End Year 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2008 Observations 23 25 27 29 36 45 47 45 46 46 40 34 31 30 25 19 mm 0.090o 4.00o 4.340o 0.430o 0.36 0.3 1 4.20 5.70o 0.23 0.990o 0.02 4.16 14.98 0.070o 0.90o 4.0 1 25th pct 23.63 79.4000 136.83 15.78 10.21 27.610 68.72 276.85 55.63 19.311 18.84 28.52 56.02 26.32 32.55 23.9900 median 35.9000 141.44 187.30 46.510 47.12 67.19 120.99 395.87 87.25 38.10 46.95 43.350 67.61 46.65 47.63 40.3500 75th pct 87.84 256.36 327.70 101.60 101.21 132.111 217.12 626.50 194.111 86.45 96.95 57.94 74.990 63.34 67.49 59.3700 max 740.16 2453.47 3033.5300 1106.22 1258.14 760.75 1050.42 5529.24 2386.48 2586.72 1753.64 1626.94 522.98 220.73 177.75 438.33 average 85.48 294.15 364.01 107.97 98.85 110.77 196.02 656.36 211.42 158.28 125.177 108.24 77.32 56.57 55.45 62.110 Table 3 Results for an estimating method including user comparisons of relative spacecraft complexity over a ten year period ending 15 years before 6 For comparison with another analogy-based study though conducted with a different time horizon and using subsystem rollup rather than purely system-level estimating the reader may wish to consult Kellogg Mahr and Lobbia 4 9 


our normalization process which basically embedded the effect of time within the factors being input to Neural networks normally so an important step network run based on the test set's root mean squared error RMSE An illustration of the process is given in figure 8 RMS Error vs Training Time 0.22 0.16 0.11 Training Error 2"""I  Testing Error I I I I I I I I I I I I I I I I I I 200 400 600 800 10 Epochs Figure 8 Root mean square of prediction error over training epochs for both training and test sets Our empirical analysis indicated that a resilient propagation or RPROP network worked best Qwiknet is a commercial software implementation of this method Its help states This is an adaptive learning rate method where weight updates are based only on the sign of the local normalizing our data essentially converting it from an extrapolative problem to an interpolative one The normalization we undertook was transforming each field to its percentage rank vis-a-vis the observed minimum and maximum for that factor For example a design life of 24 months might lie in the 25th percentile for all observed design life and so on Training was carried out using 52 missions from the 1960s through 1996 and testing occurred on 58 missions from 1996 through 2007 We tried to allow as much time as possible to elapse between the latter part of the training set and the latter part of the test set to gauge whether predictive accuracy decayed over time Results are shown sorted by cost in figure 9 and chronologically in figure 10 The graph above shows that MREs are large and negative below approximately 50M actual mission cost but then rapidly come within 5000 and closer perhaps again rising in the high cost range somewhere above 500M The correlation between estimates and actual values is approximately 0.82 In producing an actual model for use it may be possible to introduce an artificial trend correction indexed to cost which offsets the moderate trend observed above Sorted chronologically virtually no trend can be seen as Neural Network MRE By Cost Cost Figure 9 was in 10 o LM M 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 was good used entirely logistic activation functions within three news for being able predict future missions and may have layers been due to Neural network magnitude of relative error sorted by actual cost gradients not their magnitudes Our network topology estimates go farther out up to 11 years This as classification including pattern recognition or function approximation They may tend to over-fit and we took seem better suited to the model interpolative applications such 


Automated Windowed Coefficient Trending Earlier we attempted to find explanatory models whose coefficients changed over time in a predictable way This would then let us predict future coefficient values for these models and apply these trended models towards future estimates The problem was that we were not able to find such stable models at least manually We automated our search for stable models by building a system we refer to as the Automated Coefficient Trend Search ACTS tool A patent is currently pending for the method implemented in this software Against the Far Out data set ACTS is designed to automatically permute fields run chronologically-windowed regressions with the chosen fields establish linear and nonlinear trends for the resulting coefficients and then use these trended coefficients to estimate a future set of data ACTS also automatically tries up to seven different transformations on any given dependant or independent variable The length of calibration windows and the increment between them are automatically varied ACTS establishes most promising coefficient trends and also rates the performance of predictions done using trended-coefficient models With so many parameters transformations windows length and intervals to evaluate ACTS needed to search a huge problem space Despite assembling a cluster of 8 computers to run ACTS if necessary for months the problem still required pruning Each run of ACTS was therefore restricted to producing models with a different number of variables between 3 and 15 We quickly found that the best models contained very few parameters due to over-fitting on wider models permitting us to pare down the problem Neural Netv 1.00 0.75 0.50 0.25 I  M The system produced literally thousands of forecasts for each project If the distribution of project costs and project costs estimates were known a confidence interval could be computed using the cost estimates and a method such as Bayesian model averaging used to integrate them 5 Not having a priori knowledge of this distribution we chose instead to use the inter-quartile range of the estimates that is we looked at whether the estimate fell between the first and third quartile of the estimates The top and bottom 25 were disregarded because there are always outliers and it was presumed the system could work well even when many of the individual estimates were not very good Given the very large number of estimates produced by the system literally thousands or tens of thousands depending on the settings we could afford to squander a few Results were best with only three variables per equation In eight out of 31 projects 26 of the projects the project's actual costs fell within the inter-quartile range of the forecasts of costs for that project produced by the ACTS models For 30 out of 31 projects the inter-quartile range of estimates was made up of over a thousand estimates while for the 3 1st project only 446 estimates were part of the inter-quartile range The significance of these numbers is revealed in the next paragraph When equations with four variables were used only one of the true project costs fell within the inter-quartile range of forecasts produced by ACTS When more than 4 variables were used none of the true project costs fell within the interquartile range From this we concluded that ACTS performed best when only 3 variables were included in each model Using more variables appears to produce over-fitted models Aork MRE By Lauch Month Chronologic 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Launch Month Since 1960 Figure 10 Neural network magnitude of relative error sorted chronologically 11 space 


incapable of accurate forecasts Of course the wider the inter-quartile range of forecasts the more likely it is that the true forecast will fit within that range But how parsimonious is the range As figure 11 shows in about 55\2600 of the inter-quartile ranges created by ACTS the 3rd quartile of estimates is 50%0 larger than the Ist quartile of estimates For about 90\2600 of the ranges created the difference between the I't and 3rd quartile is less than 75\2600 and for 97\2600 of the projects the top of the range is 100 or less than the bottom of the range 45\2600 of the true project costs falling into the new range When the top and bottom of the range were changed by 50\2600 the range estimates encompassed 77\2600 of the true project costs In the end however we felt that the adjusted ranges were too wide for practical use Adaboosting using binomial logit classifiers Short for adaptive boosting the Adaboost algorithm 6 is a type of machine learning algorithm where a set of classifiers are adjusted to favor previous misclassifications These classifiers are actually other estimating algorithms Percentage of Time 3rd Quartile no more than XO/o Greater than 1st Quartile 120 100 80 60 40 20 0 25 50 75 100 Figure 11 Variation in ACTS-produced estimates measured between 3rd and 1st quartile Percentage of time True Cost Falls in Range Produced by ACTS 3 variable ersion 90 8070 60 5040 30 20 10 l 0 Interquartile  or10  or25  or50  or75 Figure 12 The effect of widening the estimating range measured by  of estimates now lying within Of the eight projects that fell within the average percentage difference between the 1st and 3rd quartile was 55 with median of 52 This indicated that the inter-quartile range might be too restrictive We therefore proceeded to widen the range of cost estimates for each project multiplying the ISt quartile estimate by 1 X%0 and the 3rd quartile estimate by 1  X using various Xs In each instance we compared actual project costs to estimates produced and determined how frequently the actual fell into the range of estimates Results are summarized in figure 12 When the top and bottom of the range were changed by 10 32 of the actual estimates fell within the new range Extending each of the range boundaries by 25 resulted in Adaboost works by iteratively weighting observations so that poorly classified ones are given more weight during the next iteration With the algorithm focused on improving estimates for exceptional cases it is somewhat more sensitive to noisy data and outliers though less likely to over fit We chose to test this approach due to the latter property The boosting algorithm seemed structured particularly for binary decisions though we are seeking to estimate a continuous variable cost We therefore staged the method by estimating cost bands so that the estimate for each band reverted to a binary decision The algorithm would separately estimate whether an observation was 12 


our evaluation method tested potential models using either ordinary least squares least absolute above or below X dollars then Y dollars then Z such that instead namely that we were examining a proportion rather X  Y  Z than a binary outcome but this was impractical to use and we judged that the problem was not strongly biased towards The boosting algorithm still required a classifier and model either context For a classifier we implemented binary logit78 a discrete Adaboost MRE By COST Lu was similar to ACTS but with 0 1.00 0.75 0.50 0.25 0.00 0.25 0 50 0 75 1 00 1 25 1 50 1 75 2 00 Cost Figure 13 Adaboost magnitude of relative error sorted by actual cost Adaboost MRE CHRONOLOGICALLY Year Figure 14 Adaboost magnitude of relative error sorted chronologically choice model specifically intended for a binary dependant variable is the cost above or below this point There may have been some support for using the probit model 7 This was done by a statistical programmer with output checked against the statistics program STATA 8 we obtained from this first stage system We also attempted to use OLS as a classifier but our experiments showed no significant effect from adding relatively orthogonal i.e unrelated models Since using more than one model did not make a difference boosting using OLS was little more than a single OLS model with estimates divvied up in bands no dynamic re-weighting of classifiers and thus no boosting effect to speak of To obtain models some changes and improvements and again put were discarded and resolved by the latter 13 Lu 1.00 0.75 0.50 0.25 0.00 0 0.50 0.75 1 00 1.25 1 50 1 75 2 00 error which better discounts outliers and weighted least squares The models which we built another windowed test system that our computing cluster to use trying to find good ones from were then used to bootstrap the binary logit method although coefficients a sea of potential transformations This time 


This exercise required a number of decisions about how many classifiers to use how many bands to estimate and how many variables should be used in the models All these choices were made empirically Models with 3 variables were marginally more accurate than those with 4 More models seemed generally better to a point so we used 99 models We also used 22 cost bands checking whether an estimate was above or below 12M 35M 44M and so on through 519M It is interesting to note that there was no data scarcity-induced limit on the number of bands we could have used since each was estimated using all observations lying above and below and not just between bands As with the neural network training was carried out using 52 missions from the 1960s through 1996 and testing occurred on 58 missions from 1996 through 2007 The trend in estimate deviation would again permit us to gauge whether predictive accuracy decayed over time Results are shown sorted by cost in figure 13 and chronologically in figure 14 Figure 13 shows that MREs are large and negative for missions at 56M actual mission cost and below Above this point results are mixed although not as poor as at the very low range There seems to be no degradation in predictive accuracy as projects increase in scale The correlation between estimates and actual values is approximately 0.66 Sorted chronologically in figure 14 no trend can be seen as estimates go farther out up to 11 years as was seen with the neural network Again this may have been due to the normalization process in which we mapped each variable to its percentage ranking based on future and past minimum and maximum values basically embedding the effect of time within the factors being input to the model It also may be due to a relatively stable era Weighted combinations of simple models A persistent problem in multivariate estimation methods which has been mentioned is that a model may be produced that over fits the observations used for calibration It would thus not be adequately generalized to estimate out of that sample Techniques such as jack-knifing and outlier removal can mitigate this We chose another approach developing models that were little more than simple rules of thumb and then combining these in a weighted manner so each would have a say in the estimate The weightings were formulated using Excel's Solver to minimize estimation error on a set observations lying 15 to 25 years before the test set We refer to this method as continuous boosting because as with the Adaboost method it too involves a mix of classifiers that are combined according to their performance against a set of test observations Unlike Adaboost which solves a binary problem the algorithms are here estimating on a continuous scale Also unlike Adaboost the models are re-weighted rather than the observations to maximize prediction Before presenting results it is worth noting that we ran a control test where all variables from the simple rules of thumb were combined into a single model and run on the same windows performance was nowhere near as good We chose the models by hand guided by the criteria that they should be relatively orthogonal rely on differing parameters so they could each contribute a unique influence to the combined result Figure 15 shows that MREs are mostly positive though for lower cost missions errors are regularly negative and large Continuous Boost MRE By COST M M 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Cost Figure 15 Continuous boosting magnitude of relative error sorted by actual cost 14 


CONCLUSIONS Given that this project was intended to estimate missions lying 10-15 years out we structured it differently than one intended to estimate contemporary projects A variety of conventional techniques were not used as we felt they would over fit training observations and thus not be suitable for prediction We also were not sure which approach would work so we tried many We heavily favored ensemble methods where models are combined because we surmised that any one model could not be guaranteed to have the best view of the future However a single neural network ultimately yielded the most competitive results Another finding was that methods built upon simple models such as with three variables generally did work best not surprisingly because they were less likely to over fit calibration data A graph of the three best methods is shown in figure 18 with estimates for the same missions sorted by cost For the neural network calibration occurred until just before the results shown for continuous boosting calibration occurred no more recently than 15 years prior to the results and for Adaboost calibration also occurred up until the results shown A graph sorted by year is not shown we think it instructive that no results seemed to degrade over time As mentioned but obvious from figure 17 the neural network performs best followed by Adaboost and then continuous boosting It is interesting to note that the three cases in which the neural network goes haywire and predicts too low also correspond to worst performances for the Adaboost method pointing to exceptional data points We will be further investigating these regularly errant results and other outliers which may result in estimating improvements ACKNOWLEDGEMENTS This work was carried out under Small Business Innovation Research contract FS9453-05-C-0023 with the Air Force Research Lab The authors wish to gratefully acknowledge Judy Fennelly and later Ross Wainwright our technical points of contact at AFRL for their continuous support and encouragement Our contract officer Timothy Provencio also provided invaluable assistance Our critical seed stock of data was provided by Joseph Hamaker who previously was Director of NASA Headquarters Cost Analysis Division and now is Senior Cost Analyst with SAIC Ainsley Chong and Dale Martin USAF Ret also lent considerable assistance with data gathering APPENDIX A MISSIONS COLLECTED Active Cavity Radiometer Irradiance Monitor Satellite Active Magnetospheric Particle Tracer Explorer Adeos Advanced Communications Technology Satellite Advanced Composition Explorer Alexis Amos-I AMSC-1 Anik El Anik E2 Applications Technology Satellite-I Applications Technology Satellite-2 Applications Technology Satellite-5 All MREs Lu 1.00 0.75 l 0.50l 0.25 l 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75-2.00oi v 1   Cost o N X  X~~~~~Cl Figure 16 Comparison of estimating error for three best methods 15 


Applications Technology Satellite-6 Aqua Argos Atmospheric Explorer AURA Aurora 2 Calipso Cassini Cassini Spacecraft  Huygens Probe Chandra X Ray Observatory CHIPSat Clark Clementine CloudSat COBE Columbia 5 Contour CRRES DART Dawn DBS-1 Deep Impact Flyby Spacecraft  Impactor Deep Space 1 Deep Space 2 Defense Meteorological Satellite Program-5D Defense Meteorological Satellite Program-5D3 Defense Support Program DSCS 3 FIO DSCS 3 F7 DSCS I DSCS-II DSCS-IIIA DSCS-IIIB Dynamics Explorer-I Dynamics Explorer-2 Earth Observing Satellite 1 Earth Radiation Budget Experiment EchoStar 5 Extreme Ultraviolet Explorer Far Ultraviolet Spectroscopic Explorer FAST FLTSATCOM 6 Galaxy 5 Galaxy 11 Galaxy Evolution Explorer Galileo Orbiter  Probe Gamma Ray Large Area Space Telescope GE 1 GE 5 Genesis GFO 1 Globalstar 8 Glomr GOES 3 GOES 9 GOES N GPS-1 GPS-IIR GPSMYP GRACE Gravity Probe-B GRO/Compton Gamma Ray Observatory GStar4 Hayabusa HEAO-1 HEAO-2 HEAO-3 HESSI-II High Energy Transient Explorer-II HETE HST ICESat Ikonos IMAGE IMP-H Inmarsat 3-F5 Intelsat K INTELSAT-II INTELSAT-IV International Ultraviolet Explorer Iridium James Webb Space Telescope Jason 1 JAWSAT KEPLER KOMPSAT LANDSAT1 LANDSAT-4 LANDSAT-7 Lewis Lunar Orbiter Lunar Prospector Magellan Magsat Mariner-4 Mariner-6 Mariner-8 Mariner1 0 MARISAT Mars Exploration Rover Mars Express/Beagle 2 Mars Global Surveyor Mars Observer Mars Odyssey Mars Surveyor 2001 Orbiter Mars Pathfinder  Sojourner Rovers Mars Polar Lander Mars Reconnaissance Orbiter Mars Telecommunication Orbiter Mars Climate Orbiter Messenger Meteor Mid-course Space Experiment MightySat Milstar 3  Adv EHF Model-35 Morelos NATO III Near Earth Asteroid Rendezvous NEAR Shoemaker New Horizons 16 


NIMBUS NOAA 8 NOAA 15 NPOESS Preparatory Project Orbital Maneuvering Vehicle Orbcomm Orbiting Carbon Observatory Orbiting Solar Observatory-8 Orbview 2 Orion 1 P78 Pas 4 Phoenix Pioneer P-30 Pioneer Venus Bus/Orbiter Small Probe and Large Probe Pioneer-I 0 Polar QuikSCAT Radarsat Reuven High Energy Solar Spectroscopic Imager REX-II Rosetta Instruments Sage III SAMPEX Satcom C3 Satcom C4 SBS 5 SCATHA Seastar SMART-1 SNOE Solar Dynamics Observatory Solar Maximum Mission Solar Mesosphere Explorer Solar Radiation and Climate Experiment Solar Terrestrial Relations Observatory Space Interferometry Mission Space Test Program Small Secondary Satellite 3 Spitzer Space Telescope SPOT 5A Stardust  Sample Return Capsule STEPO STEPI STEP3 STEP4 STRV ID Submillimeter Wave Astronomy Satellite Surfsat Surveyor Swift Gamma Ray Burst Exporer Synchronous Meterological Satellite-I TACSAT TACSAT 2 TDRS F7 Tellura Terra Terriers Tethered Satellite System Thermosphere Ionosphere Mesosphere Energetics and Dynamics TIMED TIROS-M TIROS-N TOMS-EP Total Ozone Mapping Spectrometer TOPEX TRACE Triana Tropical Rain Measuring Mission TSX-5 UFO I UFO 4 UFO 9 Ulysses Upper Atmosphere Research Satellite Vegetation Canopy Lidar VELA-IV Viking Viking Lander Viking Orbiter Voyager Wilkinson Microwave Anisotropy Probe WIND WIRE XMM X-Ray Timing Explorer XTE XSS-iO XSS-ii APPENDIX B FIELDS COLLECTED Mission Mission Scenario Mission Type Launch Year Launch Vehicle Bus Model Bus Config Bus Diameter Location Expected Life Flight Profile Flight Focus Technical Orbit Currency Total Cost Including Launch Unit Sat Costs Average Sat Cost Production Costs Launch Costs Operations Cost Orbit Weight Wet Weight Dry Weight Max Power EOL Power BOL Power  of Batts 17 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


