Adapting CakeDB To Integrate High-Pressure Big Data Streams With Low-Pressure Systems Peter Membrey and Keith C.C Chan Department of Computing Hong Kong Polytechnic University Hong Kong SAR China Email 
  
cspmembrey,cskcchan comp.polyu.edu.hk Yuri Demchenko System and Network Engineering Group University of Amsterdam Amsterdam The Netherlands Email y.demchenko@uva.nl 
Abstract 
Big Data continues to be one of the hottest topics in the computer science eld and itself takes many forms One way Big Data manifests is in the form of streams These streams 
can be generally deìned by their update frequency and the bandwidth they consume They can however be further deìned by the characteristics of the data they carry The producers of these streams are generally tuned to perform a given role such as moving large quantities of data with low latency which can often be at odds with the requirements of a given consumer In many cases the logistics of consuming such a stream can make the task impractical This paper discusses the concept of data streams as sequential data sets and having different pressures The paper demonstrates through a use case of a nancial trading company and a High Performance Compute Cluster how different applications require different pressures and why it is necessary to be able to scale 
down high pressure streams for low pressure applications without impacting the applications that require the full high pressure feed and the high pressure feed itself A proposed system for classifying streams and related consumers is discussed as well as the concept of conîation as it applies to these data streams Features in the prototype stream oriented database CakeDB that support adapting high-pressure streams to low-pressure applications are then discussed and further work is identiìed 
I I NTRODUCTION Big Data continues to be one of the hottest topics in the computer science eld today Database research is increasingly looking to shared nothing solutions with the ability to scale out over many nodes This approach effectively uses the 
power of cloud computing and the relatively low cost compute cycles to create fast databases that scale in a near linear fashion However this approach to database design has a severe limitation that of physical memory As long as the value derived from the dataset exceeds the overhead of storing it a distributed memory model can easily be appropriate However with terabytes of low value data data that does not bring immediate value for its storage this approach becomes an unrealistic solution With the cost of fast hard disk storage plummeting it is worth investigating disk based storage solutions Most computers have disk storage space many times that of the physical memory in the machine By considering the value and type of data being stored the need for a different 
type of database becomes more evident This paper builds upon previous work describing different types of Big Data and introduces the concept of dif ferent types of Data Streams DS that a Big Data system may include these being Low Pressure Medium Pressure and High Pressure feeds respectively.A use case describing a nancial trading company highlights the sorts of issues that a modern distributed system can face and demonstrates how the situation is getting increasingly more complex and difìcult to manage The paper then touches on the design for a prototype stream oriented database currently called CakeDB with a focus on the components that allow for low latency and high throughput with respect to mediating between streams of differing pres 
sures The key features in CakeDB that allow for these are discussed followed by a discussion of future work The remainder of this paper is as follows First two use cases are discussed along with the requirements for a proposed system that would satisfy their needs This is followed by the proposed deìnitions for Big Data Streams and the different categories of consumer A solution is then proposed detailing the implemented features that would fulìl the requirements A summary then follows along with a discussion on future work II U SE C ASES S YSTEM R EQUIREMENTS When considering Big Data Streams it can be easy to 
overlook the requirements of the consuming applications It is common for producers to publish data at a rate that suits the producerês needs This could quite conceivably be a case of send as much data as quickly as possible However the downstream applications that actually consume this feed often have varied requirements and although some may be required to handle the full data stream others are only interested in parts of it or perhaps simply wish to store the data stream with no need or even capacity to handle the load in real time This section details two use cases one of a nancial trading company that has multiple high throughput data streams but a variety of applications that utilize that data stream in very different ways This challenge is not limited to scenarios with 
diverse requirements but can also be found in compute clusters where it is quite possible that a given data stream may exceed for a short or potentially long time the ability of the cluster to process it The ability then to adapt high throughput data streams so that they may be consumed appropriately by a given application is of considerable interest First it is necessary to categorize different types of data stream by the impact they have on applications and requirements rather than by using the generally accepted approach of using bandwidth Second it is 
2013 International Conference on Cloud Computing and Big Data 978-1-4799-2829-3/13 $26.00 © 2013 IEEE DOI 10.1109/CLOUDCOM-ASIA.2013.33 414 
2013 International Conference on Cloud Computing and Big Data 978-1-4799-2830-9/14 $31.00 © 2014 IEEE DOI 10.1109/CLOUDCOM-ASIA.2013.33 414 


necessary to categorize different types of consumer so that they can be matched up to the data streams that were previously deìned Together these sets of deìnitions make describing a particular scenario or use case signiìcantly easier Financial Exchanges are at the centre of world nance They provide regulated and structured places for trading a wide range of instruments such as stocks bonds futures options warrants and commodities In exchange for providing these regulated trading platforms the Exchange charges commission to market participants generally based on the amount of volume traded To ensure they have an environment conducive to increasing trading volume Exchanges are constantly improving their technology so that orders sent to the Exchange can be processed faster and market information such as prices and trades can be disseminated fairly and efìciently to all participants Quiet markets can easily generate 10 million trade updates in a trading day and a state of the art Exchange can handle 10 billion trades a day That does not include the orders placed that were never lled and those orders could easily be an order of magnitude higher Although each Exchange is slightly different there is a trend to move towards order-based price feeds Snapshot-based price feeds publish the best price and available volume for a given instrument and can include various levels of price depth below the best price There is no calculation or processing done by the consumer and the price update can be consumed directly However these feeds are relatively slow because the Exchange must calculate the current best price and levels of price depth from their internal order book This puts considerable load on the Exchange and effectively limits how fast price updates can be published Often instruments are published in a cycle that is measured in milliseconds Orderbased price feeds however are very different Rather than the Exchange calculating the best price it simply forwards any orders that it accepts to all the other market participants It is then down to each participant to take the order update apply it to the order book and from that calculate the prices that were previously provided by the Exchange in the snapshotbased feed This approach greatly increases the amount of data that consumers need to process but effectively turns the Exchange into a routing device These feeds tend to publish data in real time with gaps between updates being measured in microseconds The continued improvements in performance at the Exchange have meant that the computer systems used by market participants have needed similar upgrades and enhancements This includes infrastructure such as computer hardware newer and faster machines improved network infrastructure low latency switches and bre optic inter-connects and software enhancements ability to consume the newer and faster feeds in real time Whichever participant can receive data from the exchange make a trading decision and if necessary send an order to the Exchange in the least time will have a very signiìcant and most likely very proìtable advantage over other participants It is now considered standard in the industry to benchmark such systems in nanoseconds Market data is only one of many potential data sources in such a trading company and most will have distributed trading platforms of some kind The components of such a system have differing requirements and capabilities in terms of the data each can handle and the ability to adapt high speed data streams into a form that these disparate applications can consume remains a signiìcant challenge Figure 1 highlights how different systems may inter-operate in a given trading system Fig 1 Trading System Message Flow Financial companies are under increasing stress to manage and process more data from a myriad of sources The ability to arbitrate between streams of different categories is a big concern as applications may not increase in performance but the demands of a given stream will High Performance Computing solutions that take advantage of commodity hardware have matured greatly in recent years Systems such as the High Performance Computing Cluster HPCC and similar systems such as Google s MapReduce  4 Hadoop 5 6 SCOPE 7 Sector/Sphere 8 Datomic all use commodity hardw are in their respecti v e solutions However regardless of which technology is chosen one of the challenges such a system faces is being able to process High Pressure Data Streams HPDS where the data coming into the cluster can exceed the clusters ability to process it Solutions such as Redis would not be appropriate in this case as if the cluster is unable to process the data in real time the backlog will build up in Redis If the stream is always on there would never be an opportunity for the cluster to catch up Apache Kafka would be a superior choice for this challenge as it is designed speciìcally for handling such streams However it does not provide database features and so while it could buffer data it would not be able to provide support for more complex queries such as requesting data over a time range CakeDB however supports basic database queries that are time bound and also supports functionality to enable streaming data both conîated and non-conîated Such a solution would not only allow data to be buffered into the HPCC cluster itself but would also allow for CakeDB to store data for historical queries as well This approach gives the best of both worlds where only simple database functionality is required but the ability to handle diverse and high speed data streams and make them available to a range of differing consumers is a requirement 
A Use Case  Financial Trading Company B Use Case  High Performance Computing 
415 
415 


III P ROPOSED MECHANISMS  SOLUTIONS Data Streams DS are generally categorized by the amount of data that the stream generates every second This gure generally given in megabits per second allows for effective resource allocation from a network level but does not give a great deal of information about the DS itself When considering Big Data Streams BDS it is often beneìcial to consider the type of data being sent and how exactly that data is being sent For example a multicast UDP stream will behave quite differently to a TCP stream Streams are effectively serial in that one message or event arrives at a given time Even streams with multiple sources ultimately arrive one behind the other in sequential order Whether the stream is considered to contain messages such as in a market price feed or events data capture points from remote sensors these individual elements are the fundamental component of a stream The general deìnition of a stream can be expanded to better describe a BDS First the size of a speciìc message in the stream determines how much data will be received in each message The bigger the message the greater the load on the infrastructure although for various reasons such as compression smaller messages could be more CPU intensive The second deìning factor is the time between the messages This is signiìcant because a message thatês twice as large but takes twice as long to arrive is usually less intensive on the infrastructure than smaller updates arriving at much higher frequency High Pressure Streams HPS are those that put signiìcant stress on infrastructure and consuming devices and applications The individual message size can be large as data is packed to increase throughput often to the size of the network MTU and the time between each packet being sent can generally be measured in microseconds These streams are currently found in many nancial exchanges where in many cases a single computer is unable to process the entire feed and it must be broken down into segments called partitions Individual streams that may not normally meet this category can be considered HPS if they combine data from other sources For example two Medium Pressure Streams being sent to one destination could when combined be enough to rate as an HPS Medium Pressure Streams MPS also stress infrastructure but generally do so in only one of the two dimensions highlighted earlier Either the packet size is large but the transmit frequency is low or the transmit frequency is high with much smaller packets These streams can generally be consumed by most systems in its entirety although care needs to be taken to ensure that applications consuming this feed do not become overloaded Although MPS are less intensive it is still possible for applications to either build a backlog or to start failing under load Low Pressure Streams LPS are streams that place no real stress on the infrastructure and the vast majority of applications would have little difìculty consuming and processing this feed This is the level that most general applications particularly those with business logic would operate at These applications would be network bound rather than CPU or memory bound Regardless of the type of data stream there is often the requirement to store it for future use This can be especially challenging with current database technologies as those that can handle black box data or highly structured data such as JSON tend to leverage physical memory in order to provide high performance thresholds As physical memory is signiìcantly less than available hard disk storage the need to scale-out such systems that is add additional nodes greatly increases their cost and is therefore unable to fully utilize available disk storage This is especially important when dealing with High Volume Low Value HVLV data Therefore it is not only the handling of the streams that needs to be considered but also how such streams are ultimately going to be stored Stream consumers can be broken down into four types regardless of the type of stream that is being consumed Real time non-conîated RTNC consumers need to be able to process each message received in the stream within a speciìed time window In addition each message in the stream must be processed This is the most intensive form of consumer as not only must updates be handled as quickly as possible but it is a requirement for each and every update to be processed For these consumers either failing to meet the time window or dropping a message designates a failure of the system Such systems would include order based market price data Whilst Real time conîated RTC consumers have the same time constraints as RTNC consumers there is no requirement for every message to be processed Some systems only care about the most recent update If a message is being processed and a new message then arrives for the same feed such a consumer may cancel processing the current message and process the new message instead These systems are common such as snapshot based market price data where only the current price is of interest as only the current price can be acted upon Therefore in order to ensure that the most current value is available such consumers should drop updates that are not relevant or can be skipped Non-real time non-conîated NRTNC consumers have no implicit constraints on how fast each message must be handled but they are required to process each and every message Such systems are usually ancillary in nature For example an application that stores the real-time feeds discussed previously would need to see each message but as it is only storing them there is no requirement for the process to be inherently fast 
A High Pressure Streams B Medium Pressure Streams C Low Pressure Streams D Storing the data E Types of stream consumer F Real time non-conîated G Real time conîated H Non-real time non-conîated 
416 
416 


I Non-real time conîated J Conîation 1 Passive Conîation 2 Active Conîation K Proposed implementation 1 Optimized binary format 2 Natural Order Storage 
Non-real time conîated NRTC consumers only care about the latest values and have no constraints on the time required to process them For example a system that updates a price on a web page once every minute neither needs to process the data within a limited time frame and nor does it need to process each message in the feed It is only interested in the latest price Conîation allows an application to effectively consume a data stream that would otherwise overwhelm it As discussed in the previous section when the full feed cannot be processed in real time either more time is needed or the effective message rate must be decreased By conîating a data stream messages are effectively dropped but in a way that does not impact the usage of the stream This is similar in concept to MP3 compression where signiìcant data is lost but the overall impact on the use of the data stream is minimized Passive Conîation is where a data stream is conîated by the client rather than by the sender For example with a polling based mechanism a client with a suitably equipped server would be able to request the latest message process it and then again request the latest message Any data received in between those two requests is effectively dropped from the clients point of view This is often desirable because it means the client can effectively rate limit the data stream to meet its own capacity to process it However it does add additional complexity in that the client must be able to handle certain situations such as when the latest message has not updated since the last request Active Conîation is where a data stream is conîated by the sender Conîation can be done in several ways the simplest being that of a timed pulse With this approach the sender processes message as normal but does not actually send them When the timer is triggered the sender will then publish whatever the current value is With a pulse cycle of 100ms this would limit a given stream to 10 updates per second regardless of how many messages were actually arriving at the sender for processing Conîation could also be done on individual components inside the data stream For example in a single stream there might be ten different message types These could all be conîated separately In addition some message types may not be conîated at all A more application speciìc form of conîation is where the sender applies logic to determine if a message should be sent or if it should be dropped and replaced with a newer message For example if message two arrives before message one is sent message one is dropped and only message two will actually be published As discussed in a related paper CakeDB is a stream oriented database designed speciìcally to handle high-pressure low-latency data streams T o enable the database to handle such workloads a new database design was necessary to meet these speciìc requirements Thus the following features were key to CakeDBês original design Does not block the client Has low impact on the client Can accept data at high speed Takes full advantage of available resources Copes with burst trafìc Can operate on a single machine Queries are executed in a reasonable time In addition to these features CakeDB also uses a simple binary format for both network communication and data storage and stores data in Natural Order NO on disk These features enabled CakeDB to outperform MongoDB on raw insert performance and also mak e it possible for Cak eDB to act as an intermediary for handling high-pressure data streams with low-pressure consumers CakeDB was optimized for throughput and minimal overhead Figure 2 shows the structure of a message and these are stored one after the other in natural order Fig 2 CakeDB Format Data File Figure 3 shows a similar structure is used in the index les This structure is very efìcient for determining the closest byte offset to a given time stamp This in turn allows data les of many hundreds of gigabytes to be accessed with a minimal performance penalty Fig 3 CakeDB Format Index File CakeDB implements a Natural Order Storage NOS system that stores data in the order it was received This means that when the dataset is also naturally ordered  such as prices from a stock market  they are inherently stored in the correct order that is events stored after a given event must also have occurred after that event This approach makes streaming data to disk very efìcient and allows for high throughput and minimum overhead The database simply buffers data in FIFO order and streams to disk when either a given amount of time has passed or a given amount of data has built up in the buffer Fig 4 Natural Order Storage 
       
417 
417 


3 Adopting Erlang L Commands 1 Get Last Message 2 Get All Messages Since 
Erlang was designed initially in 1986 by a small team at Ericsson Lead by Joe Armstrong the team developed the language which was later open sourced in 1998 to address key issues that Ericsson were experiencing in developing software for their telecommunications hardware that of stability and reliability in systems known to be susceptible to errors Armstrong s approach tak es the opposite approach of shared-everything designs which use threads and shared memory as he believed them to be too tightly interconnected to allow faults to be properly isolated Instead a process based message passing model similar to the actor model w as belie v ed to be more than suf cient 13 This design coupled with a powerful scheduling model makes Erlang exceptionally good at making full use of multi-core systems as this development model prevents any shared state and therefore there is very little locking in Erlang Although the message passing paradigm is not unique to Erlang it is still a relatively rare approach to solving the concurrency problem Scala written for the JVM Java Virtual Machine is one such implementation Ho we v er the challenges that are faced when trying to implement an actor based model on top of a thread based model can be signiìcant and there are limits to what such languages as Scala can accomplish Erlang has a custom VM that was written in C and was designed speciìcally from the ground up to support this paradigm This combined with libraries for implementing Erlang clusters in C and Java mak e it v ery easy to integrate legacy applications into an Erlang cluster These features made Erlang the perfect candidate for CakeDB CakeDB implements the following commands which provide the basis for stream conîation CakeDB acts as a pressure regulator between the incoming high-pressure feed and the various low-pressure consumers Because CakeDB acts as an intermediary it effectively detaches slower applications from the critical path which in turn protects upstream applications from back-pressure caused by slow consumers Consumers that require only the latest update from a given stream can use the Get Last Message command whereas those that require the full feed just in smaller chunks can use the Get All Messages Since command The Get Last Message command returns the latest message on a given stream This is very useful for applications where only the most recent value is of importance and where historical data all data other than the most recent is of no interest This command is very efìcient as it does not need to access the index or data le stored on disk In CakeDB the last message on each stream is held in memory and so it can reply immediately to this command The Get All Messages Since command takes a given timestamp each message is guaranteed to have a monotonically increasing timestamp and returns all messages since that time It does not include the message that has that particular timestamp This is useful for applications that need to process the whole feed but are unable to handle the full feed in real time With this approach CakeDB serves up data in chunks that the client can control This query does use the index and data les but as the most recent writes are still usually in the cache data is still usually served from memory IV R ELATED W ORKS There are two key requirements for handling such data streams There needs to be a way to store them and there needs to be a way to manage those streams so that they may be effectively consumed by the relevant applications Combining these requirements shifts the focus away from a speciìc database technology This makes designing implementing and even understanding the issues surrounding Big Data usage a signiìcant challenge Databases are not generally designed for this sort of workload State of the art databases such as those created by Michael Stonebraker and his fellow researchers H-Store  and C-Store 20 of fer v ery impressi v e performance over distributed datasets However they are not designed to stream data i.e push updates to multiple clients and although something similar could be implemented this would not yield optimum performance In addition their database structures are relational in nature and as such do not support either binary blob or highly structured NoSQL datasets which is a useful feature when dealing with unstructured or proprietary data feeds Datomic treats time as a rst class citizen and supports NoSQL datasets however its schemas still need to be determined in advance and arenêt as exible when structured data such as JSON must be handled There are two main choices for when handling this style of data becomes necessary MongoDB and CouchDB 22 CouchDB stores data in native JSON format and takes a unique approach to indexing data Where RDMS systems have static data and dynamic queries CouchDB implements dynamic data and static queries leading to f ast reads only when inde x es are available CouchDB also has a REST REpresentational State Transfer over HTTP interface which makes querying less efìcient Insert performance even with batch jobs is relatively low at around 700 updates per second In contrast MongoDB is more like traditional SQL databases It is possible to do dynamic queries and due to its BSON Binary JSON storage format offers JSON-like structures but with very fast search and parse times However being memory mapped physical memory is a highly limiting factor Once data exceeds this limit performance drops to the point of being unusable Although these systems could potentially handle and store the required data although initial research suggests that there are severe bottlenecks when trying to do so there is still no native way of streaming that data to clients The same is true for key-value store systems such as memcached and Riak Whilst highly performant and distributed in nature they were not designed to handle this sort of workload A potential alternative is Redis which is generally considered an evolution of memcached It of fers publish and subscribe support PUB/SUB which does give the ability to handle real-time streaming data However like MongoDB it is a memory-based solution although it can backup this store to disk and it attempts to deliver all queued messages to a given consumer This causes problems where a consumer cannot keep up with the data stream and Redis is then required to start buffering undelivered messages in memory In many cases a consumer is unable to catch up and ultimately this will cause Redis to run out of memory and fail 
418 
418 


Kafka is an Apache project that was designed to unify multiple messaging systems into a single system The design speciìcally placed throughput rather than features as the primary design constrain A k e y design dif ference between Kafka and other solutions such as Redis was the design decision to leverage a disk based system for holding data after considering research that demonstrated disk based systems when implemented appropriately can offer faster access times than a memory based system By using disk space as the primary storage medium Kafka removed physical memory as a restriction for the amount of data that could be stored By simply tracking where a client was in relation to a particular feed it was possible to allow even slow consumers to consume large data streams However Kafka is not designed as a permanent storage system it aggregates data streams rather than stores them As such streams can often be very large themselves even though Kafka might provide a solution for managing the streams in the initial case where slower applications need to be able to manage fast data streams it does not solve the issue of storing this data for historical purposes archiving or a repository of data for future analysis An ideal solution then would implement features found in existing database technology such as the permanent storage of data and the ability to query it along with features found in streaming solutions CakeDB is a prototype database that uniìes features from both elds to provide a simple database that supports the following use cases V S UMMARY F UTURE D EVELOPMENTS Big Data Streams are generally geared towards suiting the requirements of the producer rather than that of the consumer Indeed it is usually the producer that deìnes the format and requirements for a given data stream However there are many cases where high-pressure data streams need to be consumed by applications that cannot fulìl the expectations of the producer low-pressure consumers and so an intermediary is required to regulate the pressure between the two As data streams increase in throughput complexity and number a simple way of deìning these streams and the related consumers becomes necessary Support for active conîation is currently being added to CakeDB This feature allows clients to specify the number of updates per second that they can support and then streams the data to them in real time Although CakeDB was originally conceived primarily as a system for storing large amounts of data the need to be able to adapt that data in real time as well as for historical uses is driving its continued research and development R EFERENCES  P  Membre y  K C Chan and Y  Demchenk o  A disk based stream oriented approach for storing big data in 
 2013 pp 56Ö64  A Middleton Hpcc systems Introduction to hpcc high-performance computing cluster  2011  J Dean and S Ghema w at Mapreduce a e xible data processing tool   vol 53 no 1 pp 72Ö77 2010   Mapreduce simpliìed data processing on lar ge clusters   vol 51 no 1 pp 107Ö113 2008  J V enner   Apress 2009  T  White  OêReilly 2012  R Chaik en B Jenkins P 036 A Larson B Ramsey D Shakib S Weaver and J Zhou Scope easy and efìcient parallel processing of massive data sets  vol 1 no 2 pp 1265Ö1276 2008  R Grossman and Y  Gu Data mining using high performance data clouds experimental studies using sector and sphere in  ACM 2008 pp 920Ö927  A Kiel Datomic-a functional database  2013  F  Cesarini and S Thompson  OêReilly Media Incorporated 2009  J Armstrong Making reliable distrib uted systems in the presence of software errors Ph.D dissertation KTH 2003  C He witt  Actor model for discretionary  adapti v e concurrenc y    2010  J Armstrong Ho w erlang vie ws the w orld and what we ha v e learned in the last 25 years of programming distributed systems  2011   erlang   vol 53 no 9 pp 68Ö75 2010  M Odersk y  P  Altherr  V  Cremet B Emir  S Maneth S Micheloud N Mihaylov M Schinz E Stenman and M Zenger An overview of the scala programming language Technical Report IC/2004/64 EPFL Lausanne Switzerland Tech Rep 2004  B Hausman T urbo erlang Approaching the speed of c   pp 119Ö135 1994  J Larson Erlang for concurrent programming   vol 52 no 3 pp 48Ö56 2009  Y  Demchenk o P  Grosso C de Laat and P  Membre y   Addressing big data issues in scientiìc data infrastructure in  2013 pp 48Ö55  R Kallman H Kimura J Natkins A P a vlo A Rasin S Zdonik E Jones S Madden M Stonebraker Y Zhang  H-store a highperformance distributed main memory transaction processing system  vol 1 no 2 pp 1496Ö1499 2008  M Stonebrak er  D Abadi A Batkin X Chen M Cherniack M Fer reira E Lau A Lin S Madden E OêNeil  C-store a columnoriented dbms in  VLDB Endowment 2005 pp 553Ö564  P  Membre y  E Plugge and T  Ha wkins  Apress 2010  J Lennon  Apress 2009  J Anderson J Lehnardt and N Slater   OêReilly Media 2010  J Petrovic Using memcached for data distribution in industrial environment in  IEEE 2008 pp 368Ö372  C M Hayden E K Smith M Denche v  M Hicks and J S F oster  Kitsune Efìcient general-purpose dynamic software updating for c in  ACM 2012 pp 249Ö264  J Kreps N Narkhede and J Rao Kafka A distrib uted messaging system for log processing in  2011  K Goodhope J K oshy  J Kreps N Narkhede R P ark J Rao and V Y Ye Building linkedinês real-time activity data pipeline  vol 35 no 2 pp 33Ö45 2012  A Jacobs The pathologies of big data   vol 52 no 8 pp 36Ö44 2009 
Collaboration Technologies and Systems CTS 2013 International Conference on White paper LexisNexis Risk Solutions Communications of the ACM Communications of the ACM Pro Hadoop Hadoop the deìnitive guide Proceedings of the VLDB Endowment Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining Erlang programming A concurrent approach to software development CoRR abs/1008.1459 EPTCS 58 Communications of the ACM Implementations of logic programming systems Communications of the ACM Collaboration Technologies and Systems CTS 2013 International Conference on et al Proceedings of the VLDB Endowment et al Proceedings of the 31st international conference on Very large data bases Deìnitive Guide to MongoDB Beginning CouchDB CouchDB The Deìnitive Guide Time to Relax Systems 2008 ICONS 08 Third International Conference on Proceedings of the ACM international conference on Object oriented programming systems languages and applications Proceedings of the NetDB IEEE Data Eng Bull Communications of the ACM 
419 
419 


not possible under other visualization frameworks because they lacked the ability to succinctly express the differences between the different overplotting treatments V I MPLEMENTATION The ability to interactively switch between different Abstract Rendering encodings is a signiìant advantage of Abstract Rendering over other libraries Efìciency changing rest directly on implementation decisions Abstract Rendering has been implemented in Java and Python This section describes the implementation in Java The Python implementation is similarly structured but differs in the parallelization strategy relying heavily on vectorization through NumPy The Abstract Rendering implementation follows the definition provided in Section II Aggregate info and transfer functions are directly represented though aggregate functions are slightly modiìed to provide efìcient execution in out-of-core environments The select function is replaced with a Renderer class that controls the overall data access order This includes access not just to the underlying dataset as the select function implies but also to the aggregate values i.e the order and frequency that the x/y values appear in Renderers fall into two general categories by pixel and by glyph Bypixel renderers perform selection essentially as described in Section II This strategy is efìcient when used with datasets that are spatially arranged such as a quad-tree Any given glyph will be accessed once for each pixel the glyph touches Many data structures do not efìciently handle the highly spatial nature of these queries and thus a pixel-oriented rendering strategy is not effective A glyph-based rendering strategy takes the opposite tack Each glyph is visited exactly once and each pixel may be updated multiple times This enables efìcient rendering when using non-spatial data structures because each glyph is visited in an order convenient to its container However the different iteration order means that synthetic values i.e the s xy values are typically updated multiple times The aggregation function does not receive a list of info function results as described in Section II Instead it receives one info result and a pre-existing aggregate value To compensate for this difference aggregate functions must provide a zero value and must be commutative/associative if deterministic execution is desired The zero is used to initialize the synthetic data space When presented with a list of info results the operator receives all of its operands at once and thus commutativity and associativity are not signiìcant The implementation supports a number of parallelization strategies and data conìgurations A full analysis is beyond the scope of this paper In brief thread-level parallelism and vector-based parallelism have been explored GPU distributed memory and streaming data conìgurations have also been investigated All use the components described   Figure 10 Scaling behavior of Abstract Rendering as processor count increases Scaling behavior is near linear as processors as are added but shows no additional improvements with hyper-thread processors processors 8-15 are hyperthreads and improve by less than 5 vs 8 cores above augmented with various helper functions to facilitate data access or partial result combination The out-of-core conìguration is used in performance analysis Section VI and to handle the Kiva data set Section IV-B VI P ERFORMANCE Section III described several Abstract Rendering phrasings demonstrating its expressive capabilities To be practical the framework must be performant as well as expressive An simple characterization of runtime performance was done with an eight physical core machine Two adjacency matrix visualizations were used The rst was the Kiva dataset described Section IV-B The second dataset is an adjacency-list of links found on Wikipedia receiving the same treatment The Wikipedia data set includes 153 million edges representing the links of the largest connected cluster if the category system is ignored Both data sets were binaryencoded adjacency-lists stored in a memory mapped le The le contents were streamed off disk and rendered in a glyphparallel strategy These datasets were used because the data volume is sufìcient to require out-of-core processing but require simple analysis to create a visual representation Figure 10 presents the average performance over 10 executions while keeping the core count xed In general more processors are more helpful but hyperthreading is not The scaling characteristic is similar between the two datasets but the difference shows the overhead of abstract rendering in general Even though this Wikipedia data set is four times larger than the Kiva data set the overall runtime is only three times longer on average Overall the performance numbers are generally supportive of interactive visualization applications 15 


VII F UTURE W ORK Current Abstract Rendering implementations closely tie the bin-elements with the display resolution and region This decision introduces view-dependent effects View-dependent effects are used advantageously in high-deìnition alpha composition but may not always be desirable Developing techniques for avoiding these effects and guidelines for their usage is an ongoing effort Section V described implementation considerations There are several unexplored options that may lead to more efìcient implementations or to implementations that run in more complex runtime environments Options include distributed memory or efìcient GPU execution The idea of binning is shared inMens Abstract Rendering can be applied to more than just overplotting High-deìnition alpha composition rests on the idea of measuring pixel-level information This same idea can be applied to screen-space metrics for visualization evaluation 17 Such applications are also being e xplored The idea of the transfer function comes from scientiìc visualization However years of research into transfer functions has yielded many interesting techniques Mixed rendering styles and context-aware highlighting are strong candidates for exploration Additionally as noted in Section I Z-ordering creates an implicit volume-like space Some volume-based techniques from scientiìc visualization maybe more directly applicable by more literally applying this metaphor VIII C ONCLUSIONS Visualizing large data sets inevitably runs into overplotting issues By considering the rendering process as binning Abstract Rendering provides a means to unify many overplotting techniques Furthermore those techniques can be encoded succinctly at compile time and executed efìciently at runtime R EFERENCES  S K Card J Mackinlay  and B Shneiderman Readings in Information Visualization Using Vision to Think  Morgan Kaufman 1999  M Bostock and J Heer  Proto vis A graphical toolkit for visualization IEEE Transactions on Visualization and Computer Graphics  vol 15 no 6 pp 1121Ö1128 2009  M Bostock V  Ogie v etsk y  and J Heer  D3 DataDriven Documents IEEE Trans Visualization  Comp Graphics Proc InfoVis  2011 A v ailable http vis.stanford.edu/papers/d3  J A Cottam Design and implementation of a stream-based visualization language Ph.D dissertation Indiana University 2011  L W ilkinson The Grammar of Graphics  2nd ed New York Springer-Verlag 2005  H W ickham  A layered grammar of graphics  Journal of Computational and Graphical Statistics  vol 19 no 1 pp 3Ö28 March 2010  G S Da vidson B Hendrickson D K Johnson C E Meyers and B N Wylie Knowledge mining with VxInsight Discovery through interaction Journal of Intelligent Information Systems  vol 11 no 3 pp 259Ö285 1998  J A Cottam and A Lumsdaine Extended assortitivity and the structure in the open source development community in International Sunbelt Social Network Conference  International Network for Social Network Analysis January 2008 A v ailable http://www.insna.org/PDF/Awards/awards  ms 2007.pdf  C Muelder  F  Gygi and K.-L Ma V isual analysis of inter process communication for large-scale parallel computing IEEE Transactions on Visualization and Computer Graphics  vol 15 no 6 pp 1129Ö1136 Nov 2009 Available http://dx.doi.org/10.1109/TVCG.2009.196  National Historical Geographic Information System V ersion 2.0 University of Minnesota Minneapolis MN 2011  A v ailable http://www nhgis.or g  T  Porter and T  Duf f Compositing digital images  SIGGRAPH Comput Graph  vol 18 no 3 pp 253Ö259 Jan 1984 A v ailable http://doi.acm.or g/10.1145 964965.808606  J Johansson P  Ljung M Jern and M Cooper  Re v ealing structure within clustered parallel coordinates displays in Proceedings of the Proceedings of the 2005 IEEE Symposium on Information Visualization  ser INFOVIS 05 Washington DC USA IEEE Computer Society 2005 pp 17 Available http://dx.doi.org/10.1109/INFOVIS.2005.30  H Hagh-Shenas V  Interrante C Heale y  and S Kim Weaving versus blending a quantitative assessment of the information carrying capacities of two alternative methods for conveying multivariate data with color in Proceedings of the 3rd symposium on Applied perception in graphics and visualization  ser APGV 06 New York NY USA ACM 2006 pp 164Ö164 A v ailable http://doi.acm.org/10.1145/1140491.1140541  T  E Oliphant Guide to NumPy  Provo UT Mar 2006  A v ailable http://www tramy us  Z Liu B Jiang and J Heer  immens Real-time visual querying of big data Computer Graphics Forum Proc EuroVis  vol 32 2013 A v ailable http vis.stanford.edu/papers/immens  J W  T u k e y and P  A T u k e y  Computer Graphics and Exploratory Data Analysis An Introduction in Proceedings of the Sixth Annual Conference and Exposition Computer Graphics  Fairfax VA Nat Computer Graphics Association 1985 pp 773Ö785  A Dasgupta and R K osara P ar gnostics Screen-space metrics for parallel coordinates IEEE Transactions on Visualization and Computer Graphics  vol 16 no 6 pp 1017Ö1026 Nov 2010 A v ailable http dx.doi.org/10.1109/TVCG.2010.184 16 


state of innovation stakeholder  node PQ It  s a balanced node Based on this, we could calculate the  node PQ Calculation process is: set different inn ovation stakeholders state i U  and j U Value of ij 000T can be get from  innovation time difference. Innovation stakeholdersí social effect and industrial effect can be obtained upon ij B  and ij G set according to relation between innovation stakeholders  Model 4.1 points out  that the value of Gij  directly affects social benefits and sector benefits. Large Gij  can lead to increasing benefits of the entire industry and the entire social growth Bij reflects big organizationís impact on businesses. Only strengthening the inter agent association within big organization and enhancing the str ategic partnership between enterprises can jointly promote the development of the entire industry, and bring more social benefits, so that each agent can be improved   5 Summary This paper puts forward the concept of the big organization based on the CSM t heory. It introduces the basic implication of the big organization and theoretical framework of the big organization including: the big organization's perspective  overall perspective, dynamic perspective, and new resource perspective; the big organizat ionís sense  the purpose of the organizational structure is innovation, organizational activities around the flow of information, breaking the traditional organizational structure, encouraging self run structure, and blurring organizational boundaries; the big organizationís platform  the platform ecosystem of the big organization ; the big organizationís operation mode  borderless learning mode, and cluster effect; the big organizationís theory  active management theory  leading consumers, and culture  entropy reduction theory  negative culture entropy and humanistic ecology theory  inspiring humanity, and circuit theory  a virtuous circle, and collaborative innovation theory  collaborative innovation stakeholder. This paper also discusses culture entropy reduction theory of the big organization  negative culture entropy, and coordinated innovation theory  innovation stakeholders collaboration. Culture entropy change model and collaborative in novation model are constructed   The research has just begun for the big organization. It also needs further improvement but remains the trend of the times   Reference  1  Gordon Pellegrinetti, Joseph Bentsman. Nonlinear Control Oriented Boiler Modeling A Benchmark Problem for Controller De sign [J  I E E E tr a n s a c tio n s o n c o n tr o l s y s te m s te c h n o lo g y 2 0 1 0  4 1\57 65  2  Klaus Kruger, Rudiger Franke, Manfred Rode Optimization of boiler start up using a nonlinear 457 


boiler model and hard constraints [J  E n e r gy 201 1 29   22 39 2251  3  K.L.Lo, Y.Rathamarit  State estimation of a boiler model using the unscented Kalman filter [J  I E T  Gener. Transm. Distrib.2008 2 6\917 931  4  Un Chul Moon, Kwang. Y.Lee. Step resonse model development for dynamic matrix control of a drum type boiler turbine system [J IE E E  T ra nsactions on Energy Conversion.2009 24 2\:423 431  5  Hacene Habbi, Mimoun Zelmat, Belkacem Ould Bouamama. A dynamic fuzzy model for a drum boiler turbine system [J  A u to m a tic a 2 0 0 9 39:1213 1219  6  Beaudreau B C. Identity, entropy and culture J   J o ur na l  o f  economic psychology, 2006, 27\(2 205 223  7  YANG M, CHEN L. Information Technique and the Entropy of Culture J  A cad e m i c E x ch a n g e  2006, 7: 048  8  ZHANG Zhi feng. Research on entropy change model for enterprise system based on dissipative structure J  Ind ustrial  Engineering and  Management 2007, 12\(1\ :15 19  9  LI Zhi qiang, LIU Chun mei Research on the Entropy Change Model for Entrepreneurs' Creative Behavior System Based on Dissipative Structure J  C h i n a S of t S c i e n c e  2009   8  1 62 166   458 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





