  
    
 
  


         


            


    


       


        


  7 we show that it is possible to construct a social network graph from analysis of unstructu red text.  Next, we describe a series of community detection experiments with terror networks embedded in clutter.  Finally, we show some preliminary results of social network filtering using a synthetic terrorism scenario Figure 5 shows the overall process of generating the social network graph from transactions.  A weighted multiplex graph is created where the li nk types are defined by the social network transaction types.  Weights are provided by the content extraction algorithm as a confidence measure of the observation.  For example, the natural language processing algorithm may be able to determine with high confidence that two people know each other but may be less confident about a business relationship between them.  Each of these link types can be thought of as a separate graph which can be fed into social network algorithms in part or as a whole The relationship types between individuals ought to give important clues to the structure and nature of the network but most social network algorithms do not handle these kinds of multiplex graphs directly.  Our current implementation flattens the network into a single weighted graph, which allows us to use a variety of readily available SNA algorithms  Figure 5: Multiplex social network graph generation  Of particular interest are community detection algorithms where nodes of a graph are grouped into distinct communities.  Among these is the Girvan-Newman algorithm [Girvan  whi c h di vi des t h e graph i n t o  communities by repeatedly removing edges with high edge betweenness, a measure of which edges lie on the shortest path between many nodes.  The Girvan-Newman algorithm does not scale well to large graphs as the expensive edge betweenness metric must be recomputed after every edge removal.  Faster algorithms based on the concept of network modularity have been introdu ced more recently [Newman   Modularity m easures th e quality of a selection of communities within a given network.  We have found Carnegie-Mellon\222s Organizational Risk Analysis tool Carley and t h e general graph anal y s i s  package iGraph [Csardi  t o be useful i n our experi m e nt s.  OR A provides a good interface for visualization with some capability to perform analysis, while iGraph contains a more complete set of algorithms that will work on a variety of different graph formats Experiments using Information Extraction from Text Our basic hypothesis is that one can extract a reasonable social network from unstructured content using automated analysis tools.  In this section, we explore the use of several different techniques for generating social networks from a corpus of text data.  Related work includes the AutoMAP system [Die where l i nk associ at i on i s  assum e d by  sentence co-occurrence.  While this simple method yields reasonable results, AutoMAP relies on a manual process for entity detection and disambiguation and is not practical for use on large amounts of unseen data.  More complex methods are presented in [McCallum where net w orks and individuals are simultaneously analyzed and clustered using topic discovery techniques.  Our approach takes the middle ground of applying state-of-t he-art entity and relation extraction and building social networks from the resulting structured content Our test corpus consists of 60 articles on a recent terrorist event, totaling about 200,000 words.  A labeled social network was generated by a subject matter expert using only these 60 documents.  This network will serve as 223truth\224 for our analysis.  Th e truth graph contains places organizations, and events as well as individuals.  For this study, we consider only person-to-person links and only the core individuals directly involved in the terrorist event.  Our error metric will consist of precision and recall measures on the set of person-to-person links our algorithms return as compared to the truth graph.  Precision is the number of hypothesized links that are correct divided by the total number of hypothesized links.  Recall is the number of hypothesized links that are correct divided by the total number of truth links in the graph.  Thus precision indicates how accurate the system is; while recall gives an indication of how much is missed Azahari Husin Bank Noordin Mohammed Iwan Darmwan Chemical Supply Company Teacher knows Cleric knows knows Transactions mentor communicates knows Azahari Husin Bank Noordin Mohammed Iwan Darmwan communicates Chemical Supply Company Teacher Cleric comm Azahari Husin Bank Noordin Mohammed Iwan Darmwan communicates Chemical Supply Company Teacher Cleric comm Azahari Husin Noordin Mohammed Iwan Darmwan Chemical Supply Company Teacher Cleric mentor mentor Azahari Husin Noordin Mohammed Iwan Darmwan Chemical Supply Company Teacher Cleric mentor mentor Relations Multiplex Metrics Clustering Azahari Husin Noordin Mohammed Iwan Darmwan communicates knows mentor mentor comm Target Social Network Azahari Husin Bank Objects Relations Noordin Mohammed Iwan Darmwan communicates Chemical Supply Company purchase transact Teacher knows Cleric knows knows mentor mentor comm Azahari Husin Bank Objects Relations Noordin Mohammed Iwan Darmwan communicates Chemical Supply Company purchase transact Teacher knows Cleric knows knows mentor mentor comm  
 


  8  Figure 6: Block diagram describing derivation of social networks from text The overall system used to generate social networks from text is shown in Figure 6.  The collection of source documents is run through state-of-the-art natural language processing software to identify named entities and possibly events and relations.  We used BBN Identifinder [Bikel  to form a baseline for our system Identifinder will tag named entities, including people, places, and organizations from a given source text.  However, it will not perform within document co-re ference resolution.  Coreference resolution involve s the mapping of multiple expressions to the same real-world concept or entity.  This can include pronoun references or descriptive phrases \(e.g the President of the U.S.A. a nd George Bush are the same person.\We used a more complex system known as SERIF Statistical Entity & Relation Information Finding\ also from BBN [Boschee t o perform furt her anal y s i s on the text, including within document co-reference and event and link extraction.  The resulting set of entities then need to be resolved across documents in order to account for differences in spelling and naming conventions.  In our original experiment with the Identifinder output, this was performed manually.  For the SERIF output, this was accomplished via an automatic document clustering algorithm that uses as featur es: sentence context from each entity mention and a spelling distance measure on the core name.  The resulting clusters indicate which entities are the same across documents.  While the automatic crossdocument co-reference worked well, we hand corrected a few entries in order to have a valid comparison against the manual resolution from the BBN Identifinder output  In order to determine if a link exists between two people we first consider sentence co-occurrence.  If two people are mentioned in the same sentence, then it is likely that there is some link between then, although the negative is also possible.  We also compare this simplistic technique with SERIF\222s relation finding algorithms, which reports both the evidence of a relation as well as the type as determined from the content The results from our experiment are shown in Table 2.  In the first line, we use only Identifinder named entity tagging output with sentence co-occurre nce to establish links While precision is reasonable, the low recall suggests that many links are missing.  Next, we use only the relations and events between persons given by the SERIF tool.  The results indicate the SERIF returns a small set of more accurate links.  Next, we a pply sentence co-occurrence link detection to the SERIF output which gives the best overall performance.  Some errors are made, as evident by the lower precision scores, but the recall increases dramatically The addition of within document pronoun reference in SERIF is largely responsible for this gain over the Identifinder system.  In the last line, we take the union of all links between the SERIF specific relations and the sentence co-occurrence relations.  In this case, precision drops as several more incorrect links are added.  While the SERIF system with sentence co-occurre nce yields the best overall performance, the combination of the two provides useful content information such as the type of relation \(family business, etc.\between entities.  Additionally, the high precision of SERIF allows us to infer certain relations with higher confidence.  All of this information can be used to improve additional social network analysis, including community detection Table 2 \226 Results from social network analysis from text Entity and Link Detection Method  Precision Recall Identifinder \(Sente nce Co-occurrence 0.73 0.42 SERIF \(Relations & Events 0.77 0.32 SERIF \(Sentence Co-occurrence 0.70 0.64 SERIF \(Relations, Events, & Sentence Cooccurrence 0.67 0.64  Experiments using Simulated Attack and Clutter In this section, we describe community detection experiments on simulated data from our Jakarta example The simulated background clutte r is generated according to the R-MAT algorithm and the number of nodes and edges are varied across runs.  The simulated terrorist cell is Source Document Collection      205 Named Entity Detection \(Identifinder/SERIF Within Document Co-reference Resolution \(SERIF 205 205 Cross-Document Co-reference Resolution Link Analysis 
 


  9 embedded in the clutter network such that the terrorist actors communicate with the clutter actors but no new edges between the terrorist actors are introduced In our experiment, we performed community detection on the simulated graph and analyzed the results as follows.  We search through the detected co mmunities and select the one that has the highest number of terror cell actors, who, for the purposes of this experiment, are known in advance.  We then count the number of terrorist and clutter actors in this community.  Each clutter actor represents a false alarm, and each terrorist actor represents a positive detection.  Given these counts, we can once again calculate precision and recall measures on the detected community.  Because of the random nature of the graph generation process, we average the results across many runs The results of this experiment are shown in Table 3.  We vary the number of nodes for the clutter network from 16 to 256 nodes and keep the edge count at twice the number of nodes.  We calculate precision and recall as described above for both the Girvan-Newman and Newman modularity community detection algorithms.  For smaller graphs the community detection performs quite well with high precision scores.  However, as the graph gets larger precision scores begin to drop dramatically.  In the GirvanNewman algorithm, recall remains about the same regardless of graph size.  This indicates a fixed percentage of missed terrorist actor nodes.  The Newman modularity algorithm shows a higher recall measure in general, which increases with the number of nodes.  This higher recall is advantageous for our application as a filtering mechanism to pare down the community size before intent recognition.  In this case it is probably better to include more clutter nodes lower precision\her than miss terrorist actor nodes \(low recall\wever, we are still far from perfect performance indicating that more research is needed in this area Table 3 \226 Community Detection of Target Group in Random Clutter Clutter Parameters Group Detection Metrics  Girvan-Newman Betweenness Newman Modularity Nodes Edges Precision Recall Precision Recall 16 32 0.93 0.69 0.67 0.79 32 64 0.86 0.65 0.67 0.83 64 128 0.79 0.64 0.58 0.89 128 256 0.53 0.66 0.48 0.96 256 512 0.23 0.64 0.35 0.97  Experiments with Ali Baba Data The final experiment involves the synthetic Ali Baba data The Ali Baba data set has been used by other researchers  For t h e Al i B a ba dat a set we used scenario 1 which consists of 800 synthesized documents that replicate intelligence reports of suspected terrorist activity in southern England.  The documents are labeled as either being part of the scenario or as clutter.  We created discrete transactions for each synthetic document using an in-house annotation tool.  We use these hand-annotated transactions to build a multiplex social network and perform community detection using the Girvan-Newman algorithm The transactions are then filtered into overlapping \223buckets\224 of transactions based on the detected communities.  For each community, we select all tr ansactions that contain at least one of the individuals in that community.  Given that we know the members of the target cell in the Ali Baba scenario, we can calculate the percentage of \223truth\224 transactions present in each bucke t of transactions.  In this case, the truth set of transactions contains all transactions where at least one of the target cell actors is present in the transaction Table 4 \226 Community Detect ion Results for Ali Baba Community ID  Percentage of Target Transactions Covered 1 82.5 2 15.9 3 0.9 4 0.6 5 12.9 6 2.4 7 5.4 8 0.6 9 0.9 10 1.8  The total Ali Baba data set consists of 785 total actors with 25 of them belonging to the core target cell \(including aliases, etc.\total transactions, with approximately 337 of these belonging to the \223truth\224 set of target cell actors.  In this experiment, we use a weighted sum of multiplex relations to create a single weighted link between two actors.  This allo ws weak relations such as 
Is_Aware_Of to have less effect on the final set of communities while strong relations have greater effect.  The weights were hand tuned but could be determined automatically on a held-out social network.  After running the community detection algorithm on this weighted network, there were 89 detected communities.  Most of these communities were isolated from the rest of the graph and only contain two to three actors 
 


  10 The results of the transaction filtering based on community detection are shown in Table 4 for the 10 largest detected communities.  The first community contains the largest percentage of target cell actor s, and therefore the largest percentage of target transactions.  There are two other communities that contain more than 10% of target transactions, but we would expect the intent recognition algorithm to reject these as th e set of transactions would contain mostly clutter 6  I NTENT R ECOGNITION IR This section describes our work on intent recognition, in which we focus on detection of target scenarios in a transaction stream Problem Definition and System Framework Intent recognition is part of the overall system framework presented in Section 2.  The goal of intent recognition is to provide an indication to an analyst of which threats may be present in a transaction stream The exact process of intent recognition could potentially involve several tasks\227 detection of known or hypothetical target scenarios prioritization of target scenarios, and interpretation of the resulting detection In this work, we focus on detection of target scenarios.  We assume that we have a known target scenario.  The transaction stream is observed for some percentage of the entire scenario; e.g., we observe 25% of the transactions Typically, this 25% would be from the beginning of the scenario.  We then have a ta rget scenario detector which produces a score that is compared to a threshold, and a scenario-present or clutter-only decision is produced Detection of terrorist scenarios is implemented using a standard statistical train/test methodology.  Our basic setup with a Support Vector Machine \(SVM\based detector is shown in Figure 7.  In the figure, background clutter and the scenario are generated using the TADL simulation we described in Section 4.  We interleave clutter and scenario represented by C and S, respectively, producing a combined transaction stream.  We then truncate the output to some percentage of the scenario.  Finally, this stream is introduced into the SVM detector which has SVM scenario models Training of a target scenario de tector is straightforward with the setup in Figure 7.  To produce true trials, we turn off clutter and then run the TADL simulation multiple times Since the TADL interpreter is stochastic, multiple distinct outputs will be produced.  To pr oduce false trials, we turn off the scenario model and use only clutter.  Once we have false trials and true trials, we can train a detector to produce a likelihood ratio or a posterior probability p scenario|xact The features and classifier structure are, of course, the challenge, and we describe a preliminary approach in the next section As a performance criterion for the detection task, we propose the use of standard ROC or detection error tradeoff DET\curves.  These curves illustrate tradeoffs between probability of false alarm and probability of miss.  Although we do not expect the performance criterion to produce an absolute measure of effectiven ess of an intent recognition system, it should show relative gains for different system implementations and also provide a measure for contrastive experiments SVM Techniques for Training and Recognition  As a recognition system for detection of scenarios, we selected a support vector machine.  An SVM f\(x  Cristianini i s a t w o-cl ass cl assi fi er const r uct e d from  a sums of a kernel function K\(x,y  
002 and b are trained parameters For classification, a class decision is based upon whether the value f\(x is above or below a threshold Observe Partial Scenario Recruitment Resources Reco nnaissance Interleave Signal and Clutter S C 205 C S C 205 C S Simulated Background Clutter Simulated Scenario from TADL Support Vector Machine Transaction s SVM Models Class 1 f\(x\<0 Cl ass 0 f x 0 f x 0 Class 1 f\(x\<0 Cl ass 0 f x 0 f x 0   Figure 7: Setup for Scenario Training Recognition using Simulated Data 
   N i ii bxxKxf 1  
002  where the x i are support vectors from the training set, and i 
 


  11 The choice of an SVM for recognition is based upon multiple considerations.  First, at a top level, the use of the simulation models for scenario and clutter should be optimal for recognition.  But, using these models for recognition will not create a r obust detection system.  For instance, in real situations scenarios can be reordered subject to their dependencies.  Using the generation model to detect a rearranged scenario in this case will result in a low detector score and probably a miss by the detector Therefore, separating the detection framework from the simulation framework is critical in our modeling.  Other reasons for choosing an SVM are its flexibility in incorporating multiple feat ure types, good detector performance, and a well-developed tool set We consider two types of modeling for the SVM.  First, we used bag-of-events \(BOE\odeling.  The features in this situation are the counts of n grams of events in a transaction stream.  By event, we mean the predicate name only and not its arguments.  For instance, for a predicate Meets\(Bob,John we only record only the fact that Meets  takes place and not the specific actors.  The second type of SVM modeling uses bag-of-eve nts and bag-of-arguments combined together SVM BOEA In this case, since we are using a typed ontology, we only use ngrams of types that cover a general scenario.  For instance, in our experiments we do not include names of specific people in our n gram representation, since they could be arbitrarily renamed.  An example in this case, is that if the predicate was Recon\(Bob,White House then the output ngrams \(for unigram events and bigram arguments\d be Recon_White, Recon_White_House, Recon_House  The SVM kernel for both approaches is based upon a linearized likelihood ratio kernel presented in Campbell Not e t h at i n bot h t y pes of SVM  m odel s  we only have partial information about the sequence ordering because of n grams.  This technique ensures some robustness to scenario reordering The difficulty, in general, with designing classifier features based on predicates and their arguments is that the representation space is large and has both discrete and continuous aspects \(e.g., names of individuals, ages of individuals\so, there is a tradeoff between features representing specific versus generic aspects of an entity For instance, naming specific terrorist targets such as buildings or people may be of interest in detecting some scenarios.  In other scenarios, we may be looking only for generic aspects of targets\227nationality, ownership infrastructure role, etc.  These issues are certainly a topic of future research Experiments In our first set of experiments, we constructed a scenario from the Jakarta Embassy bombing that occurred on Sept. 9 2004.  The basic outline of events was taken from approximately 50 open source new articles.  We found that a reasonable timeline of events could be gleaned from open sources, but that specific tr ansaction were difficult to document.  In some cases, details from court records highlighted in articles provided interesting insight.  From these open sources, we constructed a series of hypothetical transactions consistent with the scenario and coded a simulation model in TADL Experiments were performed using the setup in Figure 7 For our first set of experiments, we generated scenario transactions using a uniform prior for clutter transactions We generated simulated training and test data using the TADL interpreter.  An SVM with a unigram BOE model was used as a detector.  Initially, we considered various percentages of observation of the scenario and interleaving We swept the percentage of the scenario observed P from 0 to 100%.  We also swept the duty cycle D 227the percentage of scenario transactions to clutter \(as shown in the interleaving in Figure 7\\227up to 25%.  Note that the interleaving is done randomly.  The equal error rate \(EER Pmiss=Pfa\parameters for a clutter network of 1000 actors is shown in Figure 8.  Several observations can be made from this figure.  First, for D and P greater than about 20% we are getting good performance Second, duty cycle appears to be the more significant parameter in the simulation process  As part of our experiments, we also tested the effect of the prior probability distribution for the clutter model on recognition performance.  We tried to match the prior for clutter transactions closer to the actual scenario.  As expected, this created a more challenging detection task.  A DET curve comparing uniform clutter and more challenging clutter is shown in Figure 9 We remark at this point that the simulations for the Jakarta scenario were a proof of concept for intent recognition D   d u t y  c y c l e  P   p e r c e n t a g e s c e n a r i o  EER  Figure 8: EER performance of the Jakarta SVM Scenario Detector with varying parameters 
 


  12 One difficulty with a known scenario is that it is essentially 223linear\224; i.e., there is very little variation in the ordering of events.  Also, the data simulation is known and matches the experimenters\222 pre-conceived concept of the problem; i.e there is no red-teaming in the process  As a next step, we considered other sources of data.  We found that the Ali Baba simulated data set provided an interesting second way of testing our system.  As in our social network analysis experiments, we use the synthetic documents from scenario 1 of the Ali Baba data set, which includes labeling of scenario vs clutter for each transaction For our experiments, we formed two teams.  One team took the Ali Baba documents and hand-annotated transactions using the TADL ontology.  Another team constructed a TADL simulation based only upon a high-level overview of the scenario.  The goal in this case was to see if a scenario could be detected given only a minimal top-level description Our experiments used the TADL simulator to generate data and train an SVM scenario detector as in Figure 7.  Initially we constructed a clutter model based upon event priors from the Ali Baba data set.  For the SVM, we used a BOEA model where the events were unigrams and the arguments were all ngrams up to 4.  Person arguments were excluded from the BOEA modeling Results for the detection using various subsets of the annotated document sequence are shown in Figure 10.  Note that the Ali Baba data set has only one scenario instance, so in order to generate multiple trials we had to Monte Carlo sample the scenario.  The sampling was always performed with a time window covering the required duration \(e.g 25%\utter only was desired then the terrorist scenario documents were removed from the sequence.  From the figure, we note that as we observe more of the scenario the EER drops.  With about 85% observation of the scenario the detection performance is good  We performed another set of experiments where we sampled a small amount of the clutter \(about 5%\and then used this as training data for the false class for the SVM Training data for the true trials was done using the TADL simulated output.  The results of this set of experiments are shown in Figure 11.  We see that the detection performance improves dramatically Uniform Clutter EER=4 More Realistic Clutter EER=25  Figure 9: Jakarta simulated scenario detection with different clutter generation models P=0.6  D=0.1   Figure 11: Ali Baba Scen ario Detection with clutter sampled from the Ali Baba data set  Figure 10: Ali Baba Scenario Detection with a simulated event prior 
 


  13 The Ali Baba and Jakarta experiments illustrate several points.  First, modeling of the prior for clutter should be a key part of the detection process.  Theoretically, if we have a large data source, the parameters of the clutter model can be learned automatically.  The example of using samples of the Ali Baba data illustrates if we have an \223oracle\224 prior then good detection is possible \(compare Figure 10 and Figure 11 ments illustrate that the process of simulation and recognition can be decoupled The scenario detector does not have to be of the same complexity as the simulation.  In fact, a simpler detector may be more robust.  A third point illustrated by our experiments is that significant wo rk is needed in features for simulation and recognition.  Our BOEA model uses very specific features that may not generalize well.  One way to think of the process is as a query task.  We would like to give TADL examples that produce detectors that generalize to unseen situations Relation to prior work Our work in intent recognition is related to prior work in plan or goal recognition.  We have highlighted some references for this area in Section 4.  Our current approach is statistical and is thus distinct from classical plan recognition.  Our general methodology is comparable to the ASAM system [Pattipati06 b u t o u r sp ecific d etectio n  strategy using an SVM is distinct from the prior work in this area 7  O NGOING AND F UTURE W ORK  LL-SNAIR Corpus Development One of the main difficulties we found when first considering this work is the lack of truth-marked data for development.  Ideally, having raw data and markings such as a social network structure, ro les, and events and clutter in a scenario is critical to benchmarking and developing algorithms.  Although some data is available, it is usually incomplete; for instance, a soci al network may be available but the documents it was derived from are not.  One goal for future work is to develop truth-marked corpora for algorithm development and validation of technologies.  We plan to create corpora using open source scenarios and documents with a mix of simulated and real data Future R&D Directions The area of social network anal ysis with respect to intent recognition contains many opportunities for additional research.  In particular, we will consider community detection algorithms that allow actors to be members of more than one group as well as provide probabilistic features of group membership fo r intent recognition.  This should help avoid the pitfalls of making hard decisions early in the pattern recognition process.  Dynamic features of social networks, particularly those which can aid in intent recognition, represent additi onal research opportunities In the area of intent recogniti on, there are many areas of possible future research.  For simulation, we plan to create more sophisticated scenarios and tools to address the issue of partial ordering of events.  For recognition, we want to combine social network analysis, entity attributes, etc. into the intent recognition process to improve generalization 8  S UMMARY AND C ONCLUSIONS  This paper introduced a framework of social network analysis and intent recognition from multimedia input.  We demonstrated our framework in the context of multiple sources of data\227actual text documents, simulated threat scenarios, and the Ali Baba data set.  Also, we showed there is a significant need for corpor a with truth-marked social network structure and threat s cenarios.  Overall, further research on this area should focus on building both data sets real and simulated\gorithms for recognition to understand the challenges and benchmark performance A CKNOWLEDGEMENTS  Our work is greatly indebted to the guidance of Zachary Lemnios of MIT Lincoln Laboratory and Robert Popp of National Security Innovations.  Both of these individuals provided key inputs and ideas to guide our research process R EFERENCES   H Tu, S. Singh, K. Pattipati and P. Willett, \223Detecting, Tracki ng  and Counterac ting Terrorist Networks via Hidden Markov Models,\224 IEEE  Aerospace Conference, Big Sky, MT, March 2004  Blayl  N B l ay l o ck and Jam e s Al l e n, \223Generat i ng artificial corpora for plan rec ognition,\224 in Liliana Ardissono Paul Brna, and Antonija Mitrovic, editors,  User Modeling 2005, number 3538 in Lecture Note s in Artificia l Intelligence pages 179-188. Springer, Edinburgh, July 24-29 2005  Bikel  D. M B i kel R L. Schwart z and R M  Weischedel. 1999. An algorithm that learns what\222s in a name Machine Learning, vol. 34, no. 1-3  Boschee E. B o schee, R W e i s chedel and A. Zam a ni an Automatic Information Extrac tion, Proceedings of the 2005 International Conference on Intelligence Analysis, McLean VA, 2-4 May 2005  Brachma R J. B r achm a n and H J Levesque Knowledge Representation and Reasoning, Morgan Kaufmann Publishers, 2004  Campbell W M C a m pbel l J. P. C a m pbel l  T P Gleason, D. A. Reynolds, and W Shen, \223Speaker Verification using Support Vector Machines and High-Level Features,\224 
 


  14 IEEE Trans. Audio, Speech a nd Language Processing, Sept 2007, vol. 15, no. 7, pp. 2085-2094  Carley een. \223Destabilizing Terrorist Networks, \223Proceedings of the 8th International Command and Control Research and Technology Symposium conference held at the National Defense War College Washington DC. Evidence Based Research, Track 3 Electronic Publication  Carley C a rl ey Kat h l een & C o l u m bus, Dave DeReno, Matt & Reminga, Jeffrey & Moon, Il-Chul. \(2008 ORA User's Guide 2008. Carnegie Mellon University, School of Computer Science, Institute for Software Research Technical Report CMU-ISR-08-125  Chakrabarti  D C h akrabart i Y. Zhan, and C  Faloutsos. R-mat: A recursive model for graph mining. In SDM, 2004  Coffm Thay ne R C o ffm an Sherry  E M a rcus Dynamic Classification of Groups Through Social Network Analysis and HMMs,  2004 IEEE Aerospace Conference Proceedings  C P. R C ohen and C T. M o rri son 223The HATS simulator,\224 Proc. of the 2004 Winter Simulation Conference 2004  Csardi G C s ardi T Nepusz The i g raph soft ware package for complex network research, - InterJournal Complex Systems, 2006  Die Di esner, Jana C a rl ey  Kat h l een 2005 Exploration of Communication Networks from the Enron Email Corpus. Proceedings of the Workshop on Link Analysis, Counterterrorism and Security, SIAM International Conference on Data Mining 2005, pp. 3-14. Newport Beach CA, April 21-23, 2005., 3-14  Doddingt G. Doddi ngt on A M i t c hel l   M   Pryzbocki, L. Ramshaw, S. Strassel, R. Weischedel, The Automatic Content Extraction \(ACE\gram Tasks, Data and Evaluation, Proceeding of LREC 2004 Conference on Language resources and Evaluation    D A. Gerdes, C Gl y m our, and J. R a m s ey  223Who\222s Calling? Deriving Organization Structure from Communication Records,\224 in Information Warfare and Organizational Decision Making, ed. Alexander Kott, 2007  Gi Gi rvan M and Newm an M E. J., Proc. Nat l  Acad. Sci. USA 99, 7821-7826 \(2002  Howe04  Howe D., \223Planning Scenarios E x e c u t i v e  Summaries,\224 The Homeland Security Council, 2004  Jakarta 2004 Wikipedi 2232004 Aust ral i a n Em bassy  Bombing,\224http://en.wikipedia org/wiki/2004_Australian_em bassy_bombing.\224   Proxi m i t y 4.3 QGraph Gui d e Novem b er 2007 http://kdl.cs.umass.edu/proximity/documentation/QGraphGui de.pdf  Kum a gi, ed., \223N ine Cautionary Tales,\224 IEEE Spectrum, Sept. 2006, pp. 36-45  McCallum Joi n t Group and Topi c Di scovery  from  Relations and Text. Andrew McCallum, Xuerui Wang and Natasha Mohanty, Statistical Network Analysis: Models Issues and New Directions, Lecture Notes in Computer Science 4503, pp. 28-44, \(Book chapter   D Jensen Relational Dependency Networks. Journal of Machine Learning Research. 8 \(March, 2007\ 653-692 http://jmlr.csail.mit.edu/papers/volume8/neville07a/neville07a pdf  Newman M E. J. Newm an \(2006 odul ari t y and community structure in networks". Proc. Natl. Acad. Sci USA 103: 8577\2268582    223Aut om at i c C ont ent Ext ract i on 2008 Eval uat i on Plan,\224 http://www.nist.gov/speech/tests/ace  Oliv J. Ol i v e Gl obal  Aut onom ous Language Exploitation \(GALE\ description http://www.darpa.mil/ipto/p rograms/gale/gale.asp  Popp  R   Popp and J. Yen \(edi t o rs Em ergent  Information Technologies and Enabling Policies for CounterTerrorism \(in this comprehensive reference see especially chapter 2, Hidden Markov Models and Bayesian Networks for Counter-Terrorism by K. Pa ttipati, et. al.\IEEE Press 2006  Popp R. Popp, K. Pattipati, P. W illett, D. Serfaty, W   Stacy, K. Carley, J. Allanach, H. Tu and S. Singh 223Collaborative Tools for Counte r-Terrorism Analysis,\224 IEEE Aerospace Conference, Big Sky, MT, March 2005   Pattipati P.K. Willett, J. Allanach, H Tu and S. Singh, \223Hidden  Markov Models and Bayesian Networks for Counter-terrorism,\224  R. Popp and J. Yen editors\Emergent Information Technologies  and Enabling Policies for Counter Terrorism, Wiley-IEEE Press,  May 2006, pp. 27-50  Sandl  
Todd Sandler and Kevin Siqueira, \221\221Games and Terrorism: Recent Developmen ts,\222\222 Simulation & Gaming Sep 2003; vol. 34: pp. 319 - 337   Donat, J. Lu, K. Pattipati, and P Willett, \223An Advanced System for Modeling Asymmetric 
 


  15 Threats,\224 IEEE International Conference on Systems, Man and Cybernetics, October 2006  B IOGRAPHY  Clifford J. Weinstein leads the Information Systems Technology Group at MIT Lincoln Laboratory and is responsible for initiating and managing research programs in speech technology, machine translation, and information assurance. He received S.B., S.M and Ph.D. degrees in electrical engineering from MIT. He has made technical contributions and carried out leadership roles in research programs in speech recognition, speech c oding, machine translation speech enhancement, packet speech communications information system assuran ce and survivability, integrated voice/data communication networks, digital signal processing, and radar signal processing. In 1993, Dr Weinstein was elected to th e Board of Governors of the IEEE Signal Processing Socie ty. From 1991-93, he was chairman of the IEEE Signal Pr ocessing Society's Technical Committee on Speech Processing. In 1976-78, he was chairman of that Society's Technical Committee on Digital Signal Processing. In 1993, Dr. Weinstein was elected as a Fellow of the IEEE for tec hnical leadership in speech recognition, packet speech and integrated voice/data networks. From 1986-1998, Dr. Weinstein was U.S technical specialist on the NATO RSG10 \(now IST-01 Speech Research Group, in which capacity he authored a comprehensive NATO report and journal article on opportunities for applications of advanced speech technology in military systems. From 1989-1994, he was chairman of the coordinating committee for the DARPA Spoken Language Systems Program, which was the major U.S. research program in speech recognition and understanding, and which involved coordinated efforts of a number of leading U.S. research groups. From 1999-2003 he served on the DARPA Information Sciences and Technology \(ISAT\ Panel, a group which provides DARPA with continuing assessments of the state of advanced information science and technology, and its relationship to DoD issues  Dr. Campbell is a technical staff member in the Information Systems Technology group at MIT Lincoln Laboratory.  He received his PhD in Applied Mathematics from Cornell University in 1995 Prior to joining MIT Lincoln Laboratory, he worked at Motorola on biometrics, speech interfaces, wearable computing, and digital communications.  His current research interests include machine learning, speech pro cessing, and social network analysis  Brian Delaney member of the Technical Staff at MIT Lincoln Laboratory where he has worked on statistical methods for machine translation of speech input as well as more recent efforts in social network analysis.  He received his Ph.D degree in Electrical Engineering from the Georgia Institute of Technology in 2004. He is a member of the IEEE  Gerald C. O\222Leary received S.B and S.M. degrees in Electrical Engineering and the Electrical Engineer degree from M.I.T in 1964 and 1966.  From 1966 to 1971, he worked at MITRE Corp in the Advanced Radar Techniques Department.  From 1971 to 1977, he worked for Signal Processing Systems Inc. in the area of programmable digital processors for communications applications.  In 1977 he joined the Technical Staff of M.I.T. Lincoln Laboratory.  There he has served as Associate Leader of the Information Systems Technology Group from 1984 to 1998 and of the Tactical Communications Systems Group from 1998 to 2000.  He has worked in the areas of satellite communications, speech processing, digital networking and information security He is currently a Senior Staff in the Information Systems Technology Group.  He is a member of the IEEE  
 


  16  
 


 17 Satellite Communications Division  


engineering systems, intelligent control of industrial processes, neurotechnology and cardiotechnology, and unmanned systems. His research work has been sponsored by government and industry and has published over 250 technical papers in his area of expertise. He is the lead author of a book on "Intelligent Fault Diagnosis and Prognosis of Engineering Systems" published by Wiley in 2006. He is the recipient of the Georgia Tech Interdisciplinary Activities award and the ECE Distinguished Professor award Romeo de la Cruz del Rosario, Jr. is the Chief of the Electronics Technology Branch at the U.S. Army Research Laboratory. He also serves as the Army Technology Objective ATO P&amp;D Operational Readiness and Condition Based Maintenance He received the B.E.E. degree from the Catholic University of America, Washington, D.C., and the M.S.E. and Ph.D degrees in Electrical and Computer Engineering from the Johns Hopkins University, Baltimore, MD. Since 1991 he has been an engineer at the Harry Diamond Laboratory then U.S Army Research Laboratory working in several areas including high power microwave technology characterization &amp; modeling of heterostructure RF devices and fabrication and failure analysis of electron devices and circuits  pre></body></html 


movies. In the case of the volume of critical reviews however, there was no big difference between mainstream and non-mainstream movies. WOM and critical reviews were usually positive H1 and H2 tested the relationship between WOM and weekly box office revenue, and the results supported the hypotheses. The volume of WOM was positively related to weekly box office revenue, while the valence of WOM had no significant effect. H3 H4a, and H4b tested the impact of critical reviews, and the results also supported the hypotheses except H4b The volume and valence of critical reviews had no consistent significances to weekly box office revenue H3 H4b. Table 7 showed that the number of critical reviews was statistically significant to aggregate box office revenue \(H4a for the attitude of critical reviews \(H4b the result more detail, an additional test was performed using only those factors related to critical reviews as independent variables. The result of the additional test supported H4, but the signs were reversed, i.e. positive critical reviews had minus signs, and negative critical reviews had plus signs. This reversed signs imply that the preference of critical reviewers is very similar to that of normal moviegoers. H5s and H6s tested the different effects of WOM and critical reviews on mainstream and non-mainstream movies. The result failed to determine that WOM give different impact on mainstream and non-mainstream movies, so H5a and H5b were rejected. H6, however, was supported, i.e the effects of critical reviews were different for mainstream and non-mainstream movies. There were no significant relationships between critical reviews and aggregate box office revenue in mainstream movies. For non-mainstream movies, however, the volume of critical reviews and the percentage of negative critical reviews were significant. Nonmainstream movies have fewer sources from which consumers can get information, and this might explain the results The above findings lead to several managerial implications. First, producers and distributors of movies could forecast weekly box office revenue by looking at previous weeks? volume of WOM. It does not matter what attitude people have when they spread WOM, the important factor is its volume. Therefore producers and distributors need to develop an appropriate strategy to manage WOM for their movies For example, the terms related to WOM marketing such as buzz and viral marketing are easily found Second, for the distributors who usually distribute less commercial and more artistic movies, and consequently have a smaller market compared to the major distributors, critical reviews can impact their movies box office revenues in a significant way. There are usually fewer sources for information for nonmainstream movies than mainstream movies, and so small efforts could leverage the outcomes. Finally, for those who are dealing with mainstream movies, the finding that the valence of WOM and critical reviews do not have significant relationship with box office revenue can have certain implications. Particularly, the attitude of critical reviews showed reversed effects Therefore, they may need to concentrate on other features rather than attitude of moviegoers or critical reviews, such as encouraging moviegoers to spread WOM This study contributes to the understanding of the motion picture industry, especially the relationship between box office revenue and WOM including critical reviews. There are existing studies that already 


critical reviews. There are existing studies that already dealt with similar issues, but this study has some differentiated features compare to prior studies. First the data used in this study was collected from South Korea, while most of the relevant studies usually focus on the North American market. This helps to provide the opportunity to understand the international market especially the Asian market, even though South Korea is a small part of it in terms of the motion picture industry. Second, movies were categorized to two groups, i.e. mainstream and non-mainstream and this study attempted to determine how WOM impacts these categories differently by testing several hypotheses In this study, there are also several limitations that could be dealt with in future research. First, using box office revenue as a dependent variable is more meaningful for distributers rather than producers. Due to there is close correlation between box office revenue and number of screens, one of producers? main concerns is how many screens their movies can be played on. Moreover, DVD sales are also important measurement for success of movies these days, and so it also could be a dependent variable. Therefore, it could be possible to give more fruitful managerial implications to various players in the motion picture industry by taking some other dependent variables Second, in this study, movies were categorized simply as mainstream and non-mainstream movies, but there could be further studies with diverse techniques of movie categorizations. For example, it would be possible to study the varying influence of WOM or critical reviews on different genres or movie budgets Third, an interesting finding of this study is that positive critical reviews could have negative relationship with box office revenue while negative critical reviews could have positive relationship. This study tried to provide a reasonable discussion on the issue, but more studies could be elaborate on it  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 References  1] Dellarocas, C., The Digitization of Word of Mouth Promise and Challenges of Online Feedback Mechanisms Management Science, 2003. 49\(10 2] Bone, P.F., Word-of-mouth effects on short-term and long-term product judgments. Journal of Business Research 1995. 32\(3 3] Swanson, S.R. and S.W. Kelley, Service recovery attributions and word-of-mouth intentions. European Journal of Marketing, 2001. 35\(1 4] Hennig-Thurau, F., et al., Electronic word-of-mouth via consumer-opinion platforms: What motivates consumers to articulate themselves on the internet? Journal of Interactive Marketing, 2004. 18\(1 5] Fong, J. and S. Burton, Electronic Word-of-Mouth: A Comparison of Stated and Revealed Behavior on Electronic Discussion Boards. Journal of Interactive Advertising, 2006 6\(2 6] Gruen, T.W., T. Osmonbekov, and A.J. Czaplewski eWOM: The impact of customer-to-customer online knowhow exchange on customer value and loyalty. Journal of Business Research, 2006. 59\(4 7] Garbarino, E. and M. Strahilevitz, Gender differences in the perceived risk of buying online and the effects of receiving a site recommendation. Journal of Business Research, 2004. 57\(7 8] Ward, J.C. and A.L. Ostrom, The Internet as information minefield: An analysis of the source and content of brand information yielded by net searches. Journal of Business Research, 2003. 56\(11 


9] Goldsmith, R.E. and D. Horowitz, Measuring Motivations for Online Opinion Seeking. Journal of Interactive Advertising, 2006. 6\(2 10] Eliashberg, J., A. Elberse, and M. Leenders, The motion picture industry: critical issues in practice, current research amp; new research directions. HBS Working Paper, 2005 11] S&amp;P, Industry surveys: Movies and home entertainment 2004 12] KNSO, Revenue of Motion Picture Industry 2004 Ministry of Culture, Sports, and Tourism, 2004 13] Duan, W., B. Gu, and A.B. Whinston, Do Online Reviews Matter? - An Empirical Investigation of Panel Data 2005, UT Austin 14] Zhang, X., C. Dellarocas, and N.F. Awad, Estimating word-of-mouth for movies: The impact of online movie reviews on box office performance, in Workshop on Information Systems and Economics \(WISE Park, MD 15] Mahajan, V., E. Muller, and R.A. Kerin, Introduction Strategy For New Products With Positive And Negative Word-Of-Mouth. Management Science, 1984. 30\(12 1389-1404 16] Moul, C.C., Measuring Word of Mouth's Impact on Theatrical Movie Admissions. Journal of Economics &amp Management Strategy, 2007. 16\(4 17] Liu, Y., Word of Mouth for Movies: Its Dynamics and Impact on Box Office Revenue. Journal of Marketing, 2006 70\(3 18] Austin, B.A., Immediate Seating: A Look at Movie Audiences. 1989, Wadsworth Publishing Company 19] Bayus, B.L., Word of Mouth: The Indirect Effects of Marketing Efforts. Journal of Advertising Research, 1985 25\(3 20] Faber, R.J., Effect of Media Advertising and Other Sources on Movie Selection. Journalism Quarterly, 1984 61\(2 21] Eliashberg, J. and S.M. Shugan, Film critics: Influencers or predictors? Journal of Marketing, 1997. 61\(2 22] Reinstein, D.A. and C.M. Snyder, The Influence Of Expert Reviews On Consumer Demand For Experience Goods: A Case Study Of Movie Critics. Journal of Industrial Economics, 2005. 53\(1 23] Gemser, G., M. Van Oostrum, and M. Leenders, The impact of film reviews on the box office performance of art house versus mainstream motion pictures. Journal of Cultural Economics, 2007. 31\(1 24] Wijnberg, N.M. and G. Gemser, Adding Value to Innovation: Impressionism and the Transformation of the Selection System in Visual Arts. Organization Science, 2000 11\(3 25] De Vany, A. and W.D. Walls, Bose-Einstein Dynamics and Adaptive Contracting in the Motion Picture Industry Economic Journal, 1996. 106\(439 26] Bagella, M. and L. Becchetti, The Determinants of Motion Picture Box Office Performance: Evidence from Movies Produced in Italy. Journal of Cultural Economics 1999. 23\(4 27] Basuroy, S., K.K. Desai, and D. Talukdar, An Empirical Investigation of Signaling in the Motion Picture Industry Journal of Marketing Research \(JMR 2 295 28] Neelamegham, R. and D. Jain, Consumer Choice Process for Experience Goods: An Econometric Model and Analysis. Journal of Marketing Research \(JMR 3 p. 373-386 29] Lovell, G., Movies and manipulation: How studios punish critics. Columbia Journalism Review, 1997. 35\(5 30] Thompson, K., Film Art: An Introduction. 2001 McGraw Hill, New York 31] Zuckerman, E.W. and T.Y. Kim, The critical trade-off identity assignment and box-office success in the feature film industry. Industrial and Corporate Change, 2003. 12\(1 


industry. Industrial and Corporate Change, 2003. 12\(1 27-67 32] KOFIC, Annual Report of Film Industry in Korea 2006 Korean Film Council, 2006 33] Sutton, S., Predicting and Explaining Intentions and Behavior: How Well Are We Doing? Journal of Applied Social Psychology, 1998. 28\(15 34] Basuroy, S., S. Chatterjee, and S.A. Ravid, How Critical Are Critical Reviews? The Box Office Effects of Film Critics Star Power, and Budgets. Journal of Marketing, 2003. 67\(4 p. 103-117  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 





