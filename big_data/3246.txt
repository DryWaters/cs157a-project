Cognitive Modeling of Symbolic-like Relationships with the Adaptive Neural Network Associator \(ANNA Rainer Spiegel University of London and Universitv of Cambridze Goldsmiths College Department of Computing New Cross London SE14 6NW UK r.s~ieeel~!gold.ac.uk Abstract-In their influential articles 1]-[2 Science 283 1999 Marcus, Vijayan, Bandi Rao and Vishton \(pp 77-80 and Pinker pp 40-41 argue that a prominent model of associative learning the simple recurrent network SRN 3 would fail to simulate rule-learning by seven-month-old infants Furthermore 
the authors argue for the consideration of rule based symbolic explanations Subsequently several authors proposed variations of the simple recurrent network that were better able to model the infant data but Marcus argued that these models would themselves implement hidden rule mechanisms Moreover he was able to show in a further experimental test that one of these models predicted exactly the opposite of what was found in an infant learning experiment My paper proposes ANNA a new recurrent neural network architecture that is fully based on associative 
mechanisms i.e ANNA does not implement rules ANNA succeeds to simulate the infant results This includes Marcus\222 recent experimental tests I will therefore argue that the results of Marcus dd not necessarily prove that infants make use of rules though they might apply rules Moreover ANNA shows that rule/symbolic like relationships can at least sometimes arise out of associations 1 INTRODUCTION It has often been asked whether the assumption of rule based symbolic mechanisms is 
necessary to explain the formation of abstract representations in human learning It has also been asked whether associative learning processes, a type of learning processes that is not based on symbolic rules are able to explain the formation of abstract representations alone In associative leaming models connections between units representing items concepts words etc are formed through simple associations Connection strengths \(=weights between units are changed based on a general learning rule as well as repeated exposure to the material that the model is supposed to learn This general 
learning rule is often an error correcting algorithm such as the delta rule In Psychology the delta rule has become very popular because it was able to simulate many I Wolfson College and Experimental Psychology Barton Road Cambridge CB3 9BB UK rs272liijcam.ac.uk 0-7803-7898-9/03/$17.00 02003 IEEE 2146 behavioral phenomena Associative learning models thus have a tradition in psychology and are analogous to many connectionist modelslneural networks because the same learning principles apply to many neural networks Psychologists therefore often consider neural networks as examples of 
associative learning The simple recurrent network also known as Elman-network 3 has become a particularly prominent model in psychology and turned out to be one of the most cited papers in Cognitive Science Psychology and Psycholinguistics 4 p 25 This model was able to simulate many human learning processes which my colleagues and myself were also able to confirm e.g 5]-[8 The question arises whether associative learning models such as the SRN or other neural networks would be fully able to explain human learning behavior and other phenomena in Cognitive Science 
This point of view is considered a possibility by many authors e.g 9]-[IZ in particular when taking a developmental perspective i.e when studying infant learning behavior In this case it was often suggested that infant learning may be guided by statistical processes alone which can be emulated with associative models/neural networks Steven Pinker Gary Marcus and his colleagues do not agree with the notion that infant learning can be explained by associative principles alone  4 Although none of them denies the existence of associative mechanisms in human learning they argue that rule-based processes are also required in order to explain particular experimental 
data on infant learning An overview of their experimental task and their results will be provided later when I attempt to model them with ANNA Steven Pinker 2 further argues more generally ahout the human mind and brain, e.g that 223the theory of symbol processing seems better suited to explaining the brain\222s ability to handle complex ideas and the aspects of language that communicate them,\224 p 41 Subsequently many scientists have questioned whether the infant data provide enough evidence in favor of assuming rule-based symbolic processes e.g   After 


summarizing the results of Marcus and his colleagues I I will give an overview of the skeptical arguments provided by these other authors 13]-[16 Subsequently 1 will refer to a novel experiment by Marcus 4 that was able to defend his position He showed that another particular model lacking algebraic-like rules could not explain his data Following this I will provide a detailed overview of my novel model which 1 named ANNA 1 will show that this associative model, which does not incorporate any algebraic-type rule is able to simulate all the results considered here 11 SUMMARY OF THE EXPERIMENTS BY MARCUS VIJAYAN BANDI RAO AND VISHTON 1999 These experiments have shown that 7-month-old infants show longer attention to sequences of phonemes with unfamiliar structure than to sequences of phonemes with familiar structure The authors suggest that these infants were able to learn sequences of phonemes at an early stage in development, because they were bored by sequences that had the same grammar as the sequences they had been trained on and attended longer to sequences with a novel grammar Infants were assigned to two conditions which either consisted of an ABA or an ABB condition In both conditions, they received a two minutes long speech sample containing repetitions of three phoneme sequences that followed one of the grammars In the ABA grammar these were 16 different sequences of phonemes such as \223li ti li\224 223ni gi ni\224 etc In the ABB grammar these were 16 different sequences of phonemes such as 223li ti ti\224 223ni gi gi\224 After the training phase was over infants were given a test phase where they received 12 novel sequences of phonemes that they had not experienced during the training phase e.g 223WO fe WO\224 or 223we fe fe\224 One half of the test trials corresponded to the grammar the infants had earlier been familiarized with whilst the other half of the test trials were constructed with the other grammar i.e the one that the infants had not been trained on The authors found that infants attended significantly longer to the sequences of phonemes they had not been trained on This suggests that they have recognized the novelty of the grammar in the test phase and that they have habituated to the grammar from the training phase. The authors interpret their findings in the following way They argue that infants have leamed an algebraic rule of the grammar rather than having relied on the statistical regularities of the training set They carried out additional experiments to strengthen their assumption that this was indeed the case i.e that infants have relied on an abstract rule rather than on leaming sequences of particular phonetic features and rather than on habituating to a specific single property that makes these grammars different l In both additional experiments they argued against these possibilities and in favor of the abstract rule mechanism. They further argue that a model like the simple recurrent network SRN would not be able to simulate. the infant results This is because the SRN in its classical form 3 cannot transfer to novel sequences of phonemes The SRN can leam the sequences of phonemes it has been trained on but when novel phonemes are applied it cannot transfer the underlying rule of the trained sequences to the novel sequences. Because the SRN which is based on associative learning mechanism and not on algebraic rules as well as basic statistical systems based on transitional probabilities cannot simulate the infant results the authors argue 223that 7-month-old infants can represent extract and generalize abstract algebraic rules\224 I p 77 With algebraic rules they mean algebra-type rules that represent relations between variables the variables would be placeholders for the phonemes in their experiment Subsequently this interpretation was challenged by a number of authors who either proposed modifications of the SRN or altemative models that were successful in simulating the infant data A brief summary of these papers and the replies by Marcus will be provided now and more detailed accounts can be found elsewhere  111 ALTERNATIVE INTERPRETATIONS The interpretation of Marcus and colleagues was criticized by Altmann and Dienes 13 because even before Marcus and his colleagues published their paper I Altmann Dienes and Gao had successfully designed a neural network that elaborated on an SRN 17 This model subsequently turned out to be successful in terms of simulating the infant data It makes use of batch weight update and weight freezing mechanisms of a core set of weights as well as mapping mechanisms between the trained set of stimuli and the novel set of stimuli This way the model is required to learn the mapping to and from the encoding formed by the SRN As a response to these comments, Marcus acknowledged that be had misunderstood their model 18 He was able to defend his interpretation however by arguing that this elaboration of the SRN was a purpose-built model incorporating an extemal mechanism to tell the model what to do e.g when to freeze particular weights\These mechanisms may go well beyond associative learning Even more striking is the following argument 4 pp 64-65 Marcus trained their model on the following problem An artificial phoneme sequence of the type ABA was supposed to transfer to a sequence with the same ABA grammar but different phonemes In the test phase the model was also tested on sequences where the phonemes were reversed e.g if WO fe WO occurred in training, it was now tested on the grammatical fe WO fe and the ungrammatical fe WO WO The model performed better on the ungrammatical phoneme sequences than on the grammatical ones Marcus and his colleague Bandi Rao also tested infants on this task and found that children showed exactly the opposite result This may suggest that infants learn to represent the underlying rule In 2747 


contrast the model had performed better on the ungrammatical sequences. Marcus argues that the reason for this is that the third phoneme in the ungrammatical sequences had been the same as the third phoneme in the grammatical ones during the training phase e.g WO fe WO vs fe WO WO whilst fe WO fe has no exact overlap with WO fe WO apart from the abstract rule Seidenberg and Elman I41 also challenged the interpretation that Marcus\222 results would prove the learning of abstract algebraic rules by 7-month-old infants They successfully modified the SRN so that it was able to simulate the infant data As a response to their contribution, Marcus  argued that Seidenherg and Elman had extended the SRN by adding an external teacher, which would do nothing else than implement an algebraic rule into their model After looking at Seidenberg and Elman\222s model in detail I was not able to find any evidence for Marcus\222 arguments as no rule of the type Marcus\222 argues was actually built into the model Nevertheless I would argue that their model may go beyond the principles of pure associative learning This is because they added a pre-training phase to their network and the model was given the chance to learn about similarity and dissimilarity of stimuli The weights from the pre-training phase were frozen and these weights later contributed to the performance of the model i.e during the test phase According to Marcus  p 66 and 19 the implementation of rules also holds true for the SRN variation designed by Negishi IS who made use of statistical algorithms to capture the continuous vowel structure of the phonemes from the infant experiment. Based on the information given in the journal Science I cannot assess whether Negishi\222s modification of the SRN is based on rule-hased principles or not The reason for this is because the implementation of his model is not publicly available in the link provided by Science Magazine not even with an online license to Science\Another neural network that successfilly simulated the infant data was provided by Shultz 16 He proposed cascade-correlation neural networks that add hidden units as training progresses These types of networks also apply curvature and slope information from the error surface when making weight changes In addition Shultz applied analog encoding of the stimuli e.g all the phonemes that could represent the As in ABA and ABB were coded by unique numbers e.g the phoneme ga was coded as number 1 the phoneme li as number 3 the phoneme ni as number 5 and the phoneme fa as number 7 Similarly the phonemes that could represent the Bs in ABA and ABB were also coded by unique numbers e.g ti was coded as number 2 na was coded as number 4 gi was coded as number 6 and la was coded as number 8 The novel phonemes in the test phase are interpolated between these numbers e.g the phoneme WO would for instance receive an average value of 2.5 Albeit capable in terms of simulating the infant data Marcus 4 criticizes that 223crucial to the success of the model is the coding scheme\224 and \223Shultz uses each node as a variable that represents a particular position in the sequence\224 p 65 i.e Marcus comes to the conclusion that this model and the previously mentioned successful models implement operations over variables, which he argues to be the same as an algebraic rule After having summarized alternative interpretations I would like to provide my own model ANNA which is clearly distinct from all of the previous approaches as it is only based on what psychologists consider associative processes and as it does not implement operations over pre-specified variables IV THE ADAPTIVE NEURAL NETWORK ASSOCIATOR Like many models in the previous section my approach was also based on the SRN and associative processes so I start by considering theoretical issues ahout associative learning first ANNA uses the same learning algorithm \(backpropagation and the same coding typically chosen in an SRN, which is considered an associative model by Steven Pinker 2 223Marcus has also demonstrated that a kind of associative network frequently toutet as a ruleless model of language learning J Elman\222s Simple Recurrent Network does not discriminate the patterns in the way these infants do\224 p 41 Apart from these basic associative mechanisms, ANNA uses other associative mechanisms where activation is spread to those units where error values become minimal \(these are no pre-specified units or variables, and this process could apply to any units in the network This process will he clarified later Error computation is arguably one of the most important mechanisms in modern associative learning theories and has for instance been proposed in the Rescorla Wagner Learning Rule e.g ZO This goes beyond classical theories of associationism, which are summarized by Steven Pinker 2 223In this passage from his 1748 Enquiry Concerning Human Undersfanding David Hume summarizes the theory of associationism. The mind connects things that are experienced together or look alike and generalizes to new objects according to their resemblance to known ones. Replace Hume\222s \223ideas\224 or 223sensible qualities\224 with 223stimuli\224 and \223responses,\224 and you get the behaviorism of Ivan Pavlov John Watson, and B.F Skinner. Replace the ideas with 223neurons\224 and the associations with 223connections,\224 and you get the neural network models of D.O Hebb and the school of cognitive science called connectionism\224 p 40 This statement about associationism does not account for the variety of associative learning theories that exist nowadays. Moreover it does not refer to the fact that many modern associative learning theories compute errors, e.g the Rescorla-Wagner Leaming Rule the McLaren Kaye and Mackintosh Theory the Adaptively Parametrised Error 2748 


Correcting System \(APECS etc ANNA stands in line with error computing approaches and is therefore not the same as the classical school of associationism from the eighteenth century I will now start by summarizing ANNA'S basic mechanisms in a simplified form ANNA makes use of the fact that the basic SRN 3 can learn the statistical regularities of the phoneme sequences. Therefore, ANNA has an SRN sub-component As in the SRN both trained and novel phonemes are coded as vectors In ANNA there are connections from the vectors representing the trained items to the vectors representing the novel items Here is one example: Take the sequence li ti li where li is coded I 0 0 0 and ti is coded 0 1 0 0 An SRN would easily be able to learn that li predicts ti which in turn predicts 1 Thus the predicted activity of the second step in the sequence would be similar to 0 I 0.9,O 1,O 1 and the prediction of the third step would be something like 0.9 0.1 0.1 0.1 One would then present the novel inputs WO 0 0 1,O\andfe 0 0 0 1 to ANNA and let them establish connections between the vector components in the trained phonemes and the vector components in the novel phonemes where the delta  error values would become minimal The first phoneme li can only be associated with the phoneme WO because both of them occur at the first position in the sequence In this case the delta value between vector component 1 in li I 0 0 0 and vector component 3 in WO 0 0 1 0 would be minimal and these components could therefore be associated m vs 1  0=1 for the rest of the comparisons Similarly vector components 2,3 and 4 in li and vector components 1,2 and 4 in WO would be equated as well because of their minimal delta values 0-O=O multiple equations do not make a difference because these components all have minimal activities The same associations can be formed between vectors ti 0 I 0 0 andfe 0 0 0 l that both occur at the second step of the sequence. Given that li ti predicts li as the third phoneme in the sequence and because there is an association between li and WO and between ti and fe the sequence WO fe could predict a WO as its third element because it would produce the corresponding output for the final phoneme of the trained sequence li ti li at its respective position in the novel sequence WO fe WO Instead of outputting something like li 0.9 0.1 0.1 O.l the novel sequence could output a WO 0.1 0.1 0.9 0.1 due to the previous associations for which the delta values have become minimal and due to the spreading activation from vector component 1 to vector component 3 The same is possible if one reverses the training sequences in testing e.g if training on wofe WO and testing onfe wofe vs fe WO WO In this case, vector component 1 could stand for WO 0.9 0.1 0.1 0.1 and vector component 2 for fi 0.1 0.9,0.1,0.1 Such an operation could still he considered associative, as it is not an operation over pre-specified variables hut rather a general error computation and a spread of activation to wherever the error values become minimal The example chosen here was to enable better understanding However the critical reader may recall that there was not only one trained phoneme sequence and one novel phoneme sequence in the Marcus et al study Rather there were 16 phoneme sequences in the training set and there were 12 novel sequences 6 grammatical ones and 6 ungrammatical ones However the principle remains the same If one out of the 16 phoneme sequences from the training set \(you can pick one at random is associated with a novel sequence and activation is spread to wherever the error value becomes minimal the result will be the same. To explain in detail how this process is implemented into ANNA it might he helpful to have a look at the basic architecture of ANNA \(Figure 1 Output vector Output vector components to components to predict next predict next phoneme of phoneme of sequences sequences t I weights 2 weights I Context units Phonemes at time t I Phonemes t Fig I The adaptivc neural nchvork associafor ANNA As mentioned before ANNA has an SRN sub-component These are the input units trained phonemes the context units the hidden units and the left side of the output units i.e the output vector components of the trained sequences The SRN learns sequential information by predicting the next phoneme in the sequence The SRN has an extra layer of context units and copy-back connections from the hidden units lo the context units As a consequence the context units store exact copies of the previous hidden units activities The context units provide the SRN with a temporal memory 2149 


Apart from the copy-hack connections all other connections in the SRN are adjustable This is done with the Backpropagation learning algorithm 21]-[24 The extra components of ANNA can be found on both the input and output level The curved lines underneath the input level indicate where ANNA establishes connections between the vector components in the trained phonemes and the vector components in the novel phonemes As mentioned before this happens where the delta  error values become minimal. The curved lines above the output level are exactly identical to the curved lines underneath the input level In order to make this figure more comprehensible I have drawn 2 types of curved lines The one underneath the input level and the one above the output level The curved lines above the output level spread activation from the vector components of the trained phonemes to the vector components of the novel phonemes for which the delta  error values have become minimal at the input level i.e if the vector representing A I 0 0 0 was associated with C 0 0 1,O at the input level a prediction of output A through vector component 1 e.g 0.88 0.1 0.12 0.1 1 will spread to component 3 e.g 0.12 0.1 0.88 0.1 I at the output level In turn the activity of component 3 will spread to component 1 at the output level Instead of exchanging the activities via spreading activation, ANNA could also perform an average computation for the not-activated units and just spread activation in one direction from component 1 to component 3 The result would be the same: The remaining units would stay inactive The following simulation study will show that ANNA can deal with what is considered an algebraic rule by Marcus and his colleagues. As can be inferred from ANNA\222s architecture the learning takes place in the SRN and the spreading activation and error computation between trained and novel items takes place in the testing session. Therefore the parameters of the SRN are only of interest for the learning session. The model was trained for 10,000 training trials i.e it received 10,000 presentations of either the ABA grammar or the ABB grammar. The learning rate was 0.1 there was no momentum term and the number of hidden units was IO In order to measure the consistency of the simulations each of them was repeated 25 times after implementing them with different random seeds and it was assessed how many times and to what extent ANNA would transfer to novel items with the same grammar as the one that was experienced during the training phase All in all there were 50 simulation studies 25 times training ANNA on ABA and testing transfer to novel items with grammar ABA and 25 times training ANNA on ABB and testing transfer to novel items with grammar ABB\The most widely accessible form of Backpropagation 24 was chosen and based on past considerations i.e I71 and 24 bias values for each hidden and output unit were changed in a similar way as the weights Target values were 0.9 and 0.1 instead of 1 and 0 A justification for these target values can also be found in 24 Iv SIMULATION RESULTS In each of the simulations ANNA perfectly predicted the correct transfer to novel sequences in both the ABA grammar and the ABB grammar. As a consequence there were 25 successful transfers and 0 unsuccessful transfers in the ABA condition and 25 successful transfers and 0 unsuccesshl transfers in the ABB condition. When performing statistical chi\222 tests this yields highly significant results chi\222\(df=l p<.OOI and chiz\(df=1 p<.OOI when applying the Yates correction for continuity due to the fact that there is only one degree of freedom \(some statisticians suggest to apply the Yaks correction for continuity, because otherwise they fear the chi\222 test might lead to progressive decisions After the highly significant finding and after carrying out the Yates correction for continuity this scenario can safely be excluded After having seen that ANNA transfers in each of the simulation studies it will be of interest to what extent ANNA shows transfer This was very much dependent on the SRN because the SRN was the network where the learning of trained items took place The interesting finding was that the SRN perfectly learned to match the target values i.e with vector component values of 0.9 if the target value was 0.9 and with vector component values of 0.1 if the target was 0 I This happened for each of the output units in each of the 25 simulations on the ABA grammar, and it also happened for each of the output units in each of the 25 simulations on the AB9 grammar Because there is no decay term after ANNA performs the error computation between trained and novel items at least in ANNA\222s first version which is presented here ANNA shows perfect and symbolic-like transfer to the novel items i.e the vector components in the novel sequences also have activities of 0.9 and 0.1 Since ANNA does this perfect transfer for each of the simulations the averages at the respective locations are also 0.9 and 0.1 and their standard errors are 0 not just if li fi i maps on WO fe WO but also if WO fe WO is reversed to fe WO fe Such a performance is typically only known for symbolic, rule-based models An overview of these results is given in Table I TABLE 1 RESULTS ON ANNA\222S TRANSFER TO NOVEL PHONEMES WITH THE SAME UNDERLYlNFGR*MMARASTHETRAMED PHONEMES If one aims for worse performance in order to simulate human data one could simply include a decay term into ANNA that decreases activation when it is spread between vector components. One might criticize that ANNA cannot 2750 


transfer after the first step in the sequence, because if a novel item WO is presented for the first time ANNA cannot predict the novel itemfe Both phonemes have never been presented to the model before so there cannot be any learning at the first step But the same would happen in humans Humans who hear a novel phoneme for the first time cannot predict the next novel phoneme if the grammar is based on three phonemes as in ABA and ABB Where it gets interesting is when wofe have been presented and ANNA has to predict what is to follow next Like the infants ANNA can then perform in a quasi-symbolic way to predict WO completing wofe WO in the ABA condition andfe completing wofefe in the ABB condition Iv CONCLUSIONS ANNA shows that the results by Marcus and his colleagues can in principle be solved by a neural network that is purely based on error computation Therefore I would argue that Marcus\222 results do not prove that infants apply algebraic rules Marcus may in turn argue that my model implicitly makes use of rules However ANNA only performs rulelsymbolic-like, and the rule is discovered through general error computation rather than operations over placeholders or pre-specified variableslvector components General error computation is considered associative and very popular in psychology for more than 30 years e.g ZO In the light of ANNA\222S results I think it is premature lo argue that symbol processing is generally better suited as done in 2 p 41 I do not deny the existence of rule-based processes in humans, but 1 argue that the present results on phoneme learning cannot prove them in infants This is because it is possible to construct models like ANNA that are not based on algebraic rules or operations over variables but nevertheless succeed to simulate the data In adults, on the other hand, there might be more evidence for rule-based processes that associative models cannot account for 8 These rule-based processes seem to co-exist along with associative processes, as various results have revealed 5 8 25 26 At least my arguments for a hybrid rule-baseUassociative system to explain human learning stands in line with Marcus\222 and Pinker\222s general assumption that both rules and associations are necessary REF ER EN c E s I G.F Marcus S Vijayan S Bandi Rao, P.M. Vishton, \223Rule Learning by Scven-Month-Old Infana,\224Scienee vol 283,77-80 1999 2 S Pinker 223Out of thc Minds of Babes,\224 Science vol 283,40-41 1999 3 J.L Elman 224Finding stmcturc in timc,\224 Cognirive Science vol 14 pp 179-211 1990 4 G.F Marcus The Algebraic Mind lnlcgrofing Counnecrionism ond Cogmirive Science Cambridge MA MIT-Prcss, 2001 5 R Spicgcl and I.P.L McLaren, \223Rccurrent Neural Networks and Symbol Grounding,\224 Proceedings of the lnlernalionoi Join1 INNS/IEEE Conference on NeurolNetwonCS pp. 320-325,2001 6 R Spiegel M Suret M.E Le Pelley and I.P.L McLaren 223Analping State Dynamics in a Recurrent Neural Network.\224 Proceedings of he IEEE World Congress on Compulolionol Intelligence Conjerence International Join1 lNNS/IEEE Conference on Neural Networkr pp 834-839,2002 7 R Spiegcl M.E Le Pelley M Suret I.P.L Mchen 223Combining Fvrry Rules and a Neural Network in an Adaptive System,\224 Proceedings oflhe IEEE World Congrrrs on Compulolionoi lmeiligence Conference IEEE Internotional Conference on Furry Syslem pp 340-345.2002 8 R Spiegcl Humon ond Machine Learning of Spolio-Temporal Sequences An Experimental and Compurarional Invesligolion PhD Thesis Univcrsity of Cambridge UK 2002 9 M.H Christiansen and N Chater 223Toward a Connectionist Model of Recursion in Human Linguistic Performance,\224 Cognilhe Science vol 23 pp 157-205 1999 IO J.L Elman E.A Bates M.H Johnson A Karmiloff-Smith D Parisi K Plunkett Relhinking Innolencrs A Conneclionisl Perspeclive on Dwelopmenl Cambridge, MA: MIT-Press 1996 Ill K Plunkelt J.L Elman Exercises in Rethinking Innateness A Handbookfor Connectionlsl Simuialiom Cambridge MA MIT-Press 1997 I21 P McLcad K Plunkett E.T Rolls lnrroduclion IO Conneclionisl Modeiiing ofcognitive Prococesses Oxford Oxford University Press 1998 I31 G.T.M Almann and 2 Dienes 223Rule Lcaming by Seven-Month-Old Infants and Neural Networks.\224 Science vol 284 p 875 1999 iih technical comments and online version of Science I41 M Seidenberg and J.L Elman 224DO infants leam grammar with algebra or statistics?\224 Science vol 284 p 433 1999 in leners and online version of Scievcc Magazine and htl~:llcrl.ucsd.cdul-elmanlPaocrsiMVRVsii~.~html 1151 M Negishi 223Do infants leam grammar with algebra or statistics?\224 Science vol 284 p 433 1999 in letters and online version of Science Magazine I61 T.R. Shultz 223Rule learning by Habihlation can be Simulated in Neural Networks,\224 Proceedings of le Twenty-Firs1 Annual Conjerence of he Cognilive Science Society pp. 665-670 1999 I71 2 Dienes G.T Altmann and S.J Gao 224Mapping across Domains Without Feedback A Neural Network Model of Transfer of Implicit Knowledge,\224 CogniliveScience vol 23 pp 53-82 1999 I81 G.F Marcus 223Rulc Lcaming by Sewn-Month-Old Infants and Newal Networks,\224 Science vol 284 p 875 1999 in technical comments and online version of Science Magazine 1191 G.F Marcus 223Do infants leam grammar with algebra or slatislid\223 Science vol 284 p 433 I999 in letters and online vcrsion of Science Magazine 1201 RA Rcacorla and A.R Wagner 223A theory of Pavlovian conditioning Variations in thc effectiveness of reinforcement and non-reinforcement,\224 in Ciarsical condirioning If Currenl research and theory A.H Black and W.F Prokasy E Ncw York Appleton-Cenhlry-CraIs 1972, pp. 64-99 21 P Werbos Beyond regression new molsforpredicrion and analysis in behaviorsciences PhD Thcsis Harvard University, Cambridge MA 1974 22 Y Le Cun 224Une Proctdure dApprentisrage pour Reseau i Seuil Asymduique,\224 Cognilivo 8s A la Fronlicre de I\222lnleiiigence Ami$cieile des Sciences de io Conaissonce des Neurosciences pp 599-604 1985 1231 D.B Parker Learning logic Technical Repan TR-47 Centcr for Computational Research in Economics and Managemcot Science MIT Cambridge MA 1985 1241 D.E Rumelhan G.E Hinton and R.J Williams 223Learning lntemal Rcpresentatians by Error Propagation,\224 in Poraliei dimibured proeesring vol I D.E Rumelhan and J.L McClelland E Cambridgc MA MIT Press 1986 pp 318-362 I251 R Spiegel and I.P.L McLarcn 223Hum Scquence Leaming Can Associations Explain Everything?\224 Proceedings of the Twenty-Third Annual Conference ofrhe Cognilive Science Society pp 976-981,200l 1261 R Spiegel and I.P.L McLaren 223Abstract and associatively-based representations in human sequence learning,\223 Phil Trans Roy Soc London vol B in press to appear in 29 July 2003 issue 215 1 


r 0 5  freq  genid  I 1 C 2   inh  I 1 C 1   inh  I 2 C 2  I 1  I 2  r 0 6  cand  I sum  C    freq  P I  C   Figure 8 Mo died rules that sim ulates m ultiset seman tics in Datalog rules r 5 through r 6 to obtain rules r 0 5 through r 0 6 as sho wn in gure 8 In these rules w eha v e used a system dened function called genid  that returns a unique iden tier ev ery time it is called Notice that the rules r 3 and r 4 in R ULES are the only rules that are recursiv e and that they are safe F urthermore it is imp ortan t that w e main tain a set seman tics while w e complete pro cessing these t w o rules b ecause w e need unique deriv ations of the meet irreducible elemen ts in inh  The mo died system R ULES no w b eha v es as exp ected and computes the correct supp ort for item sets in an y kno wledge base K  including our example kno wledge base T  It is imp ortan t to note here that the articial x w e ha v e prop osed ab o v e to sim ulate m ultiset op eration in set based framew ork through the use of genid  function is not necessary in man y systems including CORAL and RelationLog deductiv e database systems F or example CORAL supp orts m ultiset relations bags through multiset declaration Finally  grouping using set v alued terms are also allo w ed in CORAL and RelationLog 8 Wh y the System W orks The reader ma yha v e noticed that the R ULES system did not rely on generating candidate item sets in the w a y apriori has to Unlik e apriori it also do es not rely on a lev el wise computation Instead it uses a few critical observ ations that man y systems fail to notice 4  W e summarize belo w t w o critical observ ations that w e exploit in our system These observ ations follo w from the formal prop erties of transaction kno wledge bases that w e ha v e presen ted in section 5 and section 2  Item sets that are large can be computed from the database in t w o principal w a ys Either they app ear as transactions in the kno wledge base or they are computable from the transactions as follo ws Item sets in the transaction table that are not related b y a subset sup erset relationship in tersect with eac h other to pro duce in tersection virtual no des in the item set lattice meets These in tersection no des in turn in tersect un til they b ecome me et irr e ducible elements  Only a subset of these in tersection no des will b e large item sets These elemen ts can b e generated from the kno wledge base b y computing the least xp oin t of the pairwise in tersection of the elemen ts in the transaction kno wledge base Hence there is no need to generate 4 Zaki and sev eral others also ha v e made similar observ ations in their w ork on closed sets and concept lattices But there are imp ortan t dierences b et w een our observ ations and the manner in whic h w e utilize these observ ations His observ ations and tec hniques rely on a searc h based algorithm for CHARM 25 whic h is essen tial in order to compute the so called closed sets and th us ha v e to b e completely pro cedural an y candidate item sets articially as the w a y apriori do es  All other p ossible item sets are either not large item sets or are redundan t and can b e computed from the other large item sets found in the t w o t yp es of sets computed as ab o v e These observ ations can be in tuitiv ely understo o d from the example belo w Consider another kno wledge base T as sho wn in gure 9 tr ans  t 1 a   tr ans  t 1 b   tr ans  t 1 c   tr ans  t 2 a   tr ans  t 2 b   tr ans  t 2 d   tr ans  t 3 d   tr ans  t 3 e   tr ans  t 4 a   tr ans  t 4 c   tr ans  t 4 d   tr ans  t 5 a   tr ans  t 5 b   tr ans  t 5 c   Figure 9 A new kno wledge base T  Application of rules r 1 and r 3 will pro duce the gr oup and inh facts sho wn in gure 10 group  t 1  f a b c g   inh  f a b c g  40  group  t 2  f a b d g   inh  f a b d g  20  group  t 3  f d e g   inh  f d e g  20  group  t 4  f a c d g   inh  f a c d g  20  group  t 5  f a b c g   Figure 10 Execution trace of T  F ollo wing the con v en tions of lattice building in previous sections w e construct the K mapping in gure 11 for the item set lattice corresp onding to the example kno wledge base T in gure 9 Notice that in gure 11 no de ab 0 3 is an in tersection of no des abc 2 2 and abd 1 1 whic h inherits the transaction coun t of all its ancestors 2 from abc and 1 from abd  to record its total coun t as 3 Notice that its transaction coun t is still zero as it is a virtual no de not app earing in T and becauseitw as created through an in tersection Recall that the total coun t of a virtual no de cannot b e less than an yof its paren ts from whic hitw as created In fact it is alw a ys higher than its paren ts coun t refer to lemma 5.2  de 1 1 ac 0 3  acd 1 1 ad 0 2  0 3 c  0 2 d                     abc 2 2 abd 1 1 0 bd 0 1 cd 0 1 ab 0 3 null 5 0 0 3 b 0 1 e 0 4 a bc 2 acd 1 1 ad 0 2 0 3 c 0 2 d transaction nodes Intersection of transaction nodes Redundant nodes Meet irreducible element l-envelope 40 l-envelope 60 Figure 11 K mapping of the kno wledge base T  F urthermore the in tersection of the transaction no des abc 2 2  abd 1 1 and acd 1 1 could not cross the lev el of 2 item sets i.e ab  ac and ad  as in tersections alw a ys pro duce the meet alw a ys the largest p ossible common subset of 7 


the paren ts In particular the in tersection of these three no des cannot yield a 0 4  T o pro duce a 0 4  w e need to tak e another round of in tersection of the new in tersection no des pro duced in the rst round from T  It turns out that a 0 4 is a meet irreducible elemen tinthe K mapping of T  and hence no further in tersection in v olving a 0 4 is required In general w e need to compute the least xp oin t of the pairwise in tersection pro cess to compute all the in tersection no des and stop only when the set generated at the nal stage are all meet irreducible elemen ts Suc h a least xp oin t computation will only generate no des ab  ac  ad  a and d with abd and acd actually in the rst round In particular the least xp oin t will nev er compute the no des with b  c and bc as sho wn under the so called l-en v elop e in gure 11 Recall that ev ery no de under this en v elop e is a large item set Consequen tly  the l-en v elop e in this example assumes a 40 supp ort for large item sets But notice that the no de b 0 3 has an iden tical total coun t with one of its non-redundan t paren t ab 0 3  Hence b 0 3 is a redundan tnode and th us not computing or generating this no de do es not result in the loss of an y information b ecause w e can infer b 0 3 from ab 0 3  in case w e need to Since w e do not ha v e to generate the redundan t large item sets the R ULES systems w orks just ne But if w e wish to create all the large item sets similar to apriori w e m ust add another rule to ac hiev e this goal as w e do not explicitly compute them as a view d lar ge  The addition of the follo wing rule whic hessen tially copies the coun tof a large item set I to all its subsets X if X do es not exist as a large item set already  will do the tric k r 9  d lar g e  X C   lar g e  I C  X  I  lar g e  X C 2  There is a subtle issue that w e w ould lik e to poin t out here Consider the K mapping sho wn in gure 12 corresp onding to another kno wledge base T not sho wn from whic h w e ha v e remo v ed all the redundan t no des and sho wn only the transaction and in tersection virtual no des A t a rst glance one ma y think that it is p ossible to compute the total coun t of no des or item sets in a lev el wise manner and sa v e time b y not redoing certain computations F or example consider computing the total coun t of no de abc and recall that initially  the no de abc will read as abc 2 0  Assume that w e compute abc 2 3 from abc 2 0 and abcd 1 1 b y adding the total coun tof abc and abcd  Recall that abc 2 0 re\015ects the fact that abc app ears t wice in the database whereas abc 2 3 represen ts the fact that abc app ears t wice as a database transaction and app ears once 3-2=1 as a sub item in another transaction i.e abcd Let us assume for a momen t that w e compute the total coun t of ev ery no de in this fashion starting from no de abcd in a lev el wise fashion  compute the total coun tof eac hnode b y adding the total coun ts of all its paren ts No w for the third lev el from the top to compute the no de coun ts for ac  w e add the total coun tof its paren ts 3+2 giving 5 But as can b e seen from the gure coun t 5 is not really accurate This discrepancy resulted b ecause w e added total coun ts of paren ts instead of the transaction coun ts of ancestors to compute the total coun t of the no de ac  Notice that the coun t corresp onding to ac in abcd w as accoun ted for t wice in no de ac via t w o distinct branc hes as sho wn Similarly ifw e con tin ue with the same sc heme w e will compute a 0 11 for no de a instead of a 0 5 whic h in realit y is the correct total coun tfor a  1 ac 1 ac a 1 a 1 a 1 a 1 a 1                  abd 1 2 null 5 0 0 5 a 1 1 abcd abc 2 3 acd 1 2 ab 0 4 ac 0 4 ad 0 3 Figure 12 K mapping of a new database T sho wing incorrect inheritance of transaction coun t if total coun t of paren ts are used to compute total coun toflo w er lev el no des instead of transaction coun t Our rule system w ork ed correctly b ecause w e either inherited the transaction coun ts in the fr e q rules from a no de that is related via subset-sup erset relationship or b y rst generating the in tersection no de once initializing the transaction coun t to zero rule r 4   and using this in tersection no de to inherit the transaction coun ts whic h no w is in a subset-sup erset relationship with its ancestors Finally  w e added the transaction coun ts with a grouping op eration follo w ed b y a coun t op eration whic hb y denition is the total coun tfor an ynode 8.1 Breaking the Barrier of Pro ceduralit y W ew ould lik e to highligh t here that the three observ ations w e ha v e made early in this section w ere critical in dev eloping a mo del theoretic and declarativ ec haracterization of the large item set computing pro cess as it did not dep end on pro cedural concepts suc h as candidate generation The observ ation that w e only need to generate and test the intersection no des help ed us visualize the pro cess as a sort of Cartesian pro duct of the kno wledge base with itself and compare eac h transaction tuple with the other tuples in the kno wledge base and see if they w ere unrelated b y subsetsup erset relationships Recall that suc h pairs are p oten tial con tributors to an in tersection no de The least xp oin tof the in tersection pro cess help ed b ecause w e kno w that w e ha v e computed all the meet irreducible elemen ts b y no w and no other in tersection no des exists There are sev eral w orks that ha v ein v estigated the issue of declarativ e asso ciation rule mining using SQL 7  23  19  16  11 Most of these w orks sp ecially 23  19 attempt to sim ulate apriori in SQL giving rise to a complicated and a wkw ard metho d They do not exploit the inheren t declarativ e prop erties of transaction databases as w eha v e iden tied in this pap er The inheren t pro ceduralit yoftheir prop osed expressions app ears to be a ma jor b ottlenec k While it is ob viously p ossible to dev elop op erators that hide the complexit y of these expressions the system nonetheless is a wkw ard unnatural and pro cedural whic h ma y ha v e eciency related dra wbac ks F urthermore b y sp ecifying the seman tics in pro cedural terms they compromise the query optimization asp ects of the system The reason for this loss of opp ortunit y is the fact that the pro cess has already b een co ded in to the declarativit y of SQL and th us database system m ust no w consider only lo cal optimization 8 


of the query expression without ha ving the global view of the in ten tion There is a big c hance that the enco ded pro cedure ma y not b e the b est w a y to compute the rules dep ending on the database instance F urthermore as w e understand it their prop osals require p oten tially large n um ber of name generation for relations and attributes The names that are needed are usually database dep enden t and th us p ossibly cannot b e gathered at query time An additional pro cess needs to b e completed to gather those v ariables b efore actual computations can b egin 5  9 Optimization Issues While it w as in tellectually c hallenging to dev elop a declarativ e expression for asso ciation rule mining from deductiv e databases there are sev eral op en issues with great promises for resolution In the w orst case the least xp oin tneedsto generate n 2 tuples in the rst pass alone when the database size is n  Theoretically  this can happ en only when eac h transaction in the database pro duces an in tersection no de and when they are not related b y subset-sup erset relationship In the second pass w e need to do n 4 computations and so on The question no w is can w e a v oid generating and p erhaps scanning some of these com binations as they will not lead to useful in tersections F or example the no de b 0 3 in gure 11 is redundan t A signican t dierence with apriori lik e systems is that our system generates all the item sets top do wn in the lattice without taking their candidacy as a large item set in to consideration Apriori on the other hand do es not generate an y no de if their subsets are not large item sets themselv es and thereb y prunes a large set of no des Optimization tec hniques that exploit this so called an ti-monotonicit y prop ert y of item set lattices similar to apriori could mak e all the dierence in our setup The k ey issue w ould b e ho ww e push the selection threshold minim um supp ort inside the top do wn computation of the no des in the lattice in our metho d F or the momen t and for the sak e of this discussion let us consider a higher supp ort threshold of 60 for the database T of gure 9 No w the l-en v elop e will b e the one sho wn in ligh ter dashed lines in gure 11 and the no des under this line will b e the large item sets Notice that no ww eha v eto discard no des ad 2 0 and d 0 2 to o This raises the question is it p ossible to utilize the supp ort and condence thresholds pro vided in the query and prune candidates for in tersection an y further Ideas similar to magic sets transformation 3  24 ma y be b orro w ed to address this issue The only problem is that pruning of an y no de dep ends on its supp ort coun t whic h ma y come at a later stage By then all no des ma y already ha v e b een computed and th us pushing selection conditions inside aggregate op erator ma y b ecome non-trivial Sp ecial data structures and indexes ma y also aid in dev eloping faster metho ds to compute ecien t interse ction joins that w e ha v e utilized in this pap er W e lea v e these questions as op en issues that should be tak en up in the future F ortunately though there has been a v ast b o dy of researc h in optimizing Datalog programs including recursiv e programs suc h as the one w e ha v e used in this pap er and hence the new questions and researc h 5 Recall that their prop osal requires one to express the mining problem to the system using sev eral queries and up date statemen ts that utilizes information ab out the database con ten ts to ac hiev e its functionalit y  c hallenges that this prop osal raises for declarativ e mining ma y exploit some of these adv ances Needless to emphasize a declarativ e metho d preferably a formal one is desirable b ecause once w e understand the functioning of the system w e will then be able to select appropriate pro cedures dep ending on the instances to compute the seman tics of the program whic hw e kno wis in tended once w e establish the equiv alence of declarativ e and pro cedural seman tics of the system F ortunately  w e ha v e n umerous pro cedural metho ds for computing asso ciation rules whic h complemen t eac h other in terms of sp eed and database instances In fact that is what declarativ e systems or declarativit y buy us  a c hoice for the most ecien t and accurate pro cessing p ossible 10 Conclusion Our primary goal for this pap er has b een to demonstrate that mining asso ciation rules from an y rst-order kno wledge base is p ossible in a declarativ ew a y  without help from an y sp ecial to ols or mac hinery  and that w e can no wha v ea v ery in tuitiv e and simple program to do so W eha v esho wn that it is indeed p ossible to mine declarativ ekno wledge b y exploiting the existing mac hinery supp orted b ycon temp orary inference engines in programming languages e.g Prolog or kno wledge base systems e.g RelationLog XSB LD L  CORAL All w e require is that the engine b e able to supp ort set v alued terms grouping aggregate functions and set relational op erators for comparison functionalities whic hmostofthesesystemscurren tly supp ort W e ha v e also demonstrated that our formalism is grounded on a more mathematical foundation with formal prop erties on whic h the seman tics of the R ULES system rely  W e ha v e also raised sev eral op en issues related to eciency and query optimization whic h should b e our next order of business As future researc h w e plan to dev elop optimization tec hniques for mining queries that require non-trivial lo ok ahead and pruning tec hniques in aggregate functions The dev elopmen ts presen ted here also ha v e other signican t implications F or example it is no w p ossible to compute c hi square rules 4 using the building blo c ks pro vided b y our system Declarativ e computation of c hi square rules to our kno wledge has nev er b een attempted for the man y pro cedural concepts the computation of c hi square metho d relies on In a separate w ork 2 w e sho w that the coun ting metho d prop osed in this pap er can be eectiv ely utilized to generate the exp ectations needed in order to compute suc h rules rather easily  These are some of the issues w e plan to address in the near future The motiv ation imp ortance and the need for in tegrating data mining tec hnology with relational databases has b een addressed in sev eral articles suc h as 12  13 They con vincingly argue that without suc h in tegration data mining tec hnology ma y not nd itself in a viable p osition in the y ears to come T o b e a successful and feasible to ol for the analysis of business data in relational databases suc htec hnology m ust b e made a v ailable as part of database engines and as part of its declarativ e query language Our prop osal for declarativ e mining bears merit since it sheds ligh t on ho w rst order databases can be mined in a declarativ e and pro cedure indep enden t w a y so that the optimization issues can b e delegated to the underlying database engine Once suc h argumen ts are accepted sev eral systems 9 


related issues b ecome prime candidates for immediate atten tion F or example traditionally database systems supp orted declarativ e querying without the necessit y to care ab out the pro ceduralit y of the queries In this pap er w eha v e actually demonstrated that asso ciation rule mining can b e view ed as a Datalog query  It is immediate that a direct mapping from the Datalog expressions presen ted in this pap er to SQL can be dev elop ed with no problem at all W e can then rely on ecien t database pro cessing of the query in an optimized fashion Hence w ecomeclose to the essence of the visions expressed b y the leading database researc hers and practioners 12  References 1 Rak esh Agra w al and Ramakrishnan Srik an t F ast algorithms for mining asso ciation rules in large databases In VLDB  pages 487{499 1994 2 Anon ymous A declarativ e metho d for mining c hisquare rules from deductiv e databases T ec hnical rep ort Departmen t of Computer Science Anon ymous Univ ersit y USA F ebruary 2001 3 C Beeri and R Ramakrishnan On the po w er of magic In Pr o c e e dings of the 6th A CM Symp osium on Principles of Datab ase Systems  pages 269{283 1987 4 Sergey Brin Ra jeev Mot w ani and Craig Silv erstein Bey ond mark et bask ets Generalizing asso ciation rules to correlations In Pr o c A CM SIGMOD  pages 265 276 1997 5 D Chimen ti et al The LD L system protot yp e IEEE Journal on Data and Know le dge Engine ering  2\(1 90 1990 6 Jia w ei Han Jian P ei and Yiw en Yin Mining frequen t patterns without candidate generation In Pr o c A CM SIGMOD  pages 1{12 2000 7 Marcel Holsheimer Martin L Kersten Heikki Mannila and Hann uT oiv onen A p ersp ectiv e on databases and data mining In Pr o c of the sixth A CM SIGKDD Intl Conf  pages 150{155 Mon treal Queb ec 1995 8 Flip Korn Alexandros Labrinidis Y annis Kotidis and Christos F aloutsos Ratio rules A new paradigm for fast quan tiable data mining In Pr o c of 24th VLDB  pages 582{593 1998 9 Brian Len t Arun N Sw ami and Jennifer Widom Clustering asso ciation rules In Pr o c of the 3th ICDE  pages 220{231 1997 10 Mengc hi Liu Relationlog At yp ed extension to datalog with sets and tuples In John Llo yd editor Pr oc e e dings of the 12th International L o gic Pr o gr amming Symp osium  pages 83{97 P ortland Oregon Decem ber 1995 MIT Press 11 Rosa Meo Giusepp e Psaila and Stefano Ceri An extension to SQL for mining asso ciation rules Data Mining and Know le dge Disc overy  2\(2 1998 12 Amir Netz Sura jit Chaudh uri Je Bernhardt and Usama M F a yy ad In tegration of data mining with database tec hnology  In Pr o c e e dings of 26th VLDB  pages 719{722 2000 13 Amir Netz Sura jit Chaudh uri Usama M F a yy ad and Je Bernhardt In tegrating data mining with SQL databases In IEEE ICDE  2001 14 Ra ymond T Ng Laks V S Lakshmanan Jia w ei Han and Alex P ang Exploratory mining and pruning optimizations of constrained asso ciation rules In Pr o c A CM SIGMOD  pages 13{24 1998 15 Jong So o P ark Ming-Sy an Chen and Philip S Y u An eectiv e hash based algorithm for mining asso ciation rules In Pr o c A CM SIGMOD  pages 175{186 1995 16 Karthic k Ra jamani Alan Co x Bala Iy er and A tul Chadha Ecien t mining for asso ciation rules with relational database systems In Pr o c e e dings of the International Datab ase Engine ering and Applic ations Symp osium  pages 148{155 1999 17 R Ramakrishnan D Sriv asta v a and S Sudarshan CORAL  Con trol Relations and Logic In Pr o c of 18th VLDB Confer enc e  pages 238{250 1992 18 Konstan tinos F Sagonas T errance Swift and Da vid Scott W arren XSB as an ecien t deductiv e database engine In Pr o c of the A CM SIGMOD Intl Conf  pages 442{453 1994 19 Sunita Sara w agi Shib y Thomas and Rak esh Agra w al In tegrating mining with relational database systems Alternativ es and implications In Pr o c A CM SIGMOD  pages 343{354 1998 20 Ashok a Sa v asere Edw ard Omiecinski and Shamk an tB Nav athe An ecien t algorithm for mining asso ciation rules in large databases In Pr o c of 21th VLDB  pages 432{444 1995 21 Pradeep Sheno y  Ja y an t R Haritsa S Sudarshan Gaura v Bhalotia Ma y ank Ba w a and Dev a vrat Shah T urb o-c harging v ertical mining of large databases In A CM SIGMOD  pages 22{33 2000 22 Abraham Silb ersc hatz Henry F Korth and S Sudarshan Datab ase System Conc epts  McGra w-Hill third edition 1996 23 Shib y Thomas and Sunita Sara w agi Mining generalized asso ciation rules and sequen tial patterns using SQL queries In KDD  pages 344{348 1998 24 J D Ullman Principles of Datab ase and Know le dgeb ase Systems Part I II  Computer Science Press 1988 25 Mohammed J Zaki Generating non-redundan t association rules In Pr o c of the 6th A CM SIGKDD Intl Conf  Boston MA August 2000 1 0 


OM OM 006 OD8 01 012 014 016 018 02 022 False alarm demity Figure 9 Percentage of tracks lost within 200 seconds using three-scan assignment with PD  0.9 TI  O.ls Figure 11 T2  1.9s and T  Is ij  20 and 0  0.1 24 1 22  20  E fls 0  8l 16 0 n 14  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 T1/12 PD Average track life of three-scan assignment with PD varying TI  0-ls T2  1.9s T  Is X  0.02 ij LO and   0.1 mareuvenng index Figure 12 Percentage of lost tracks of 4-D assipment in 200 seconds with maneuvering index varying X  0.01 Ti  0.1 T2  1.9s and T  IS PD  0.98 Figure 10 Percentage of lost tracks of 4-D assignment in 200 SeoDllCls with TI and T2 varying PD  0.98 X  0.02 q 20 and 0  0.1 4-1607 


Figure 13 Average gate size for Kalman filter Figure is relative as compared to nq and curves are parametrized by ij/r with unit-time between each pair of samples 1.2 Iy I 1.1 0.5 I A CRLB for he unifm samiina I  0.4 0.35 d 3 03 i7 3 0.25 0 0.M 0.04 0.06 008 0.1 0.12 0.14 0.16 0.18 0.2 False A!am DemW V I    Figure 14 CramerRao Lower Boundfor Mean Square Error of uniform and nonuniform sampling schemes with Ti  O.ls T2  1.9s T  IS PD  0.9 ij  5 and U  0.25 1 unifon sampling r-ls ked i non-uniform sampling loge inlewi I ti non-uniform sampling shod interva I 0.9 0.8 I Figure 15 MSE comparison of three-scan assignment with Ti and T2 varying I'D  1 X  0.01 ij  20 and U  0.1 4-1608 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


