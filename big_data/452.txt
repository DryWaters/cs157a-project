Association Rule Mining in Peer-to-Peer Systems 000 Ran Wolff and Assaf Schuster Technion 226 Israel Institute of Technology Email 001 ranw,assaf 002 cs.technion.ac.il Abstract We extend the problem of association rule mining 226 a key data mining problem 226 to systems in which the database is partitioned among a very large number of computers that are dispersed over a wide area Such computing systems include GRID computing platforms federated database systems and peer-to-peer computing envi 
ronments The scale of these systems poses several dif\036culties such as the impracticality of global communications and global synchronization dynamic topology changes of the network on-the-\037y data updates the need to share resources with other applications and the frequent failure and recovery of resources We present an algorithm by which every node in the system can reach the exact solution as if it were given the combined database The algorithm is entirely asynchronous imposes very little communication overhead transparently tolerates network topology changes and 
node failures and quickly adjusts to changes in the data as they occur Simulation of up to 10,000 nodes show that the algorithm is local all ru les except for those whose con\036dence is about equal to the con\036dence threshold are discovered using information gathered from a very small vicinity whose size is independent of the size of the system 1 Introduction The problem of association rule mining ARM in large transactional databases was 336rst introduced in 1993 1 The input to the ARM problem is a database in which objects are grouped by context An example would be a list 
of items grouped by the transaction in which they were bought The objective of ARM is to 336nd sets of objects which tend to associate with one another Given two distinct sets of objects 003 and 004 wesay 004 is associated with 003 if the appearance of 003 in a certain context usually im\005 This work was supported in part by Microsoft Academic Foundation plies that 004 will appear in that context as well The output 
of an ARM algorithm is a list of all the association rules that appear frequently in the d atabase and for which the association is con\336dent ARM has been the focus of great interest among data mining researchers and practitioners It is today widely accepted to be one of the key problems in the data mining 336eld Over the years many variations were described for ARM and a wide range of applications were developed The overwhelming majority of these deal with sequential ARM algorithms Distributed association rule mining DARM was de\336ned in 2  n o t lo n g after t h e d e 336n itio n o f 
ARM and was also the subject of much research see for example 2 5 15 7   In recent years database systems have undergone major changes Databases are now detached from the computing servers and have become distributed in most cases The natural extension of these two changes is the development of federated databases 320 sys tems which connect many different databases and present a single database image The trend toward ever more distributed databases goes hand in hand with an ongoing trend in large organizations toward ever greater integration of data For example health maintenance organizations HMOs envision their medical 
records which are stored in thousands of clinics as one database This integrated view of the data is imperative for essential data analysis applications ranging from epidemic control ailment and treatment pattern discovery and the detection of medical fraud or misconduct Similar examples of this imperative are common in other 336elds including credit card companies large retail networks and more An especially interesting example for large scale distributed databases are peer-t o-peer systems These systems include GRID computing environments such as Condor 10 20,000 comput ers   s peci 336 c area comput i n g s ys  
tems such as SETI@home 12  1,800,000 comput ers  or UnitedDevices 14 2,200,000 comput ers   g eneral pur pose peer-to-peer platforms such as Entropia 6 60,000 peers and 336le sharing networks such as Kazza 1.8 million peers Like any other system large scale distributed systems maintain and produce operational data However 1 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


in contrast to other systems that data is distributed so widely that it will usually not be feasible to collect it for central processing It must be processed in place by distributed algorithms suitable to this kind of computing environment Consider for example mi ning user preferences over the Kazza 336le sharing network The 336les shared through Kazza are usually rich media 336les such as songs and videos participants in the network reveal the 336les they store on their computers to the system and gain access to 336les shared by their peers in return Obviously this database may contain interesting knowledge which is hard to come by using other means It may be discovered for instance that people who download The Matrix also look for songs by Madonna Such knowledge can then be exploited in a variety of ways much like the well known data mining example stating that 322customers who purchase diapers also buy beer\323 The large-scale distributed association rule mining LSD-ARM problem is very different from the D-ARM problem because a database that is composed of thousands of partitions is very different from a small scale distributed database The scale of these systems introduces a plethora of new problems which have not yet been addressed by any ARM algorithm The 336rst such problem is that in a system that large there can be no global synchronization This has two important consequences for any algorithm proposed for the problem The 336rst is that the nodes must act independently of one another hence their progress is speculative and intermediate results may be overturned as new data arrives The second is that there is no point in time in which the algorithm is known to have 336nished thus nodes have no way of knowing that the information they possess is 336nal and accurate At each point in time new information can arrive from a far-away branch of the system and overturn the node\325s picture of the correct result The best that can be done in these circumstances is for each node to maintain an assumption of the correct result and update it whenever new data arrives Algorithms that behave this way are called anytime algorithms  Another problem is that global communication is costly in large scale distributed systems This means that for all practical purposes the nodes should compute the result through local negotiation Each node can only be familiar with a small set of other nodes 320 its immediate neighbors It is by exchanging information with their immediate neighbors concerning their local databases that nodes investigate the combin ed global database A further complication comes from the dynamic nature of large scale systems If the mean time between failures of a single node is 20,000 hours 1  a system consisting of 1 This 336gure is accepted for hardware for software the estimate is usually a lot lower 100,000 nodes could easily fail 336ve times per hour Moreover many such systems are purposely designed to support the dynamic departure of nodes This is because a system that is based on utilizing free resources on nondedicated machines should be able to withstand scheduled shutdowns for maintenan ce accidental turnoffs or an abrupt decrease in availability when the user comes back from lunch The problem is that whenever a node departs the database on that node may disappear with it changing the global database and the result of the computation A similar problem occurs when nodes join the system in mid-computation Obviously none of the distributed ARM algorithms developed for small-scale distributed systems can manage a system with the aforementioned features These algorithms focus on achieving parallelization induced speedups They use basic operators such as broadcast global synchronization and a centralized coordinator none of which can be managed in large-scale distributed systems To the best of our knowledge no D-ARM algorithm presented so far acknowledges the possibility of failure Some relevant work was done in the context of incremental ARM e.g 13 and s i m i l a r a l gori t h ms  I n t hes e works the set of rules is adjusted following changes in the database However we know of no parallelizations for those algorithms even for small-scale distributed systems In this paper we describe an algorithm which solves LSD-ARM Our 336rst contribution is the inference that the distributed association rule mining problem is reducible to the well-studied problem of distributed majority votes Building on this inference we develop an algorithm which combines sequential association rule mining executed locally at each node with a majority voting protocol to discover at each node all of the association rules that exist in the combined database Du ring the execution of the algorithm which in a dynamic system may never actually terminate each node maintains an ad hoc solution If the system remains static then the ad hoc solution of most nodes will quickly converge toward an exact solution This is the same solution that would be reached by a sequential ARM algorithm had all the databases been collected and processed If the static period is long enough then all nodes will reach this solution However in a dynamic system where nodes dynamically join or depart and the data changes over time the changes are quickly and locally adjusted to and the solution continues to converge It is worth mentioning that no previous ARM algorithm was proposed which mines rules not itemsets on the 337y This contribution may affect other kinds of ARM algorithms especially those intended for data streams 9  The majority voting protocol which is at the crux of our algorithm is in itself a signi\336cant contribution It requires no synchronization between the computing nodes 2 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


Each node communicates only with its immediate neighbors Moreover the protocol is local in the overwhelming majority of cases each node computes the majority 320 i.e identi\336es the correct rules 320 based upon information arriving from a very small surrounding environment Locality implies that the algorithm is scalable to very large networks Another outcome of the algorithm\325s locality is that the communication load it produces is small and roughly uniform thus making it suitable for non-dedicated environments 2 Problem De\336nition The association rule mining ARM problem is traditionally de\336ned as follows Let 000 002 004 006 b n 006 f n 017 017 017 n 006 024 026 be the items in a certain domain An itemset is some subset 027 031 000  A transaction 033 is also a subset of 000 associated with a unique transaction identi\336er 034 000 037  A database 037  is a list that contains  037   transactions Given an itemset 027 and a database 037         033  027 n 037  2 is the number of transactions in 037  which contain all the items of 027 and 3  5 6  027 n 037  2 002       033  027 n 037  2   037    For some frequency threshold  B D 006 F 3  5 6 B K we say that an itemset 027 is frequent in a database 037  if 3  5 6  027 n 037  2 Q D 006 F 3  5 6 and infrequent otherwise For two distinct frequent itemsets 027 and S  and a con\336dence threshold  B D 006 F V  F W B K wesaytherule 027 Z S is con\336dent in 037  if 3  5 6  027  S n 037  2 Q D 006 F V  F W  3  5 6  027 n 037  2  We will call con\336dent rules between frequent itemsets correct and the remaining rules false The solution for the ARM problem is a c 037  e the list of all the correct rules in the given database When the database is dynamically updated that is transactions are added to it or deleted from it over time we denote 037  g the database at time 033  Now consider that the database is also partitioned among an unknown number of share nothing machines nodes we denote the partition of node  at time 033 037  i g  Given an infrastructure through which those machines may co mmunicate data we denote c  e g the group of machines reachable from  at time 033 We will assume symmetry i.e j l c  e g m  l c j e g Nevertheless c  e g may or may not include all of the machines The solution to the large-scale distributed association rule mining LSD-ARM problem for node  at time 033 is the set of rules which are correct in the combined databases of the machines in c  e g  we denote this solution a c  e g  Since both c  e g and 037  o g of each j l c  e g are free to vary with time 320 so does a c  e g  It is thus imperative that  calculate not only the eventual solution but also approximated ad hoc solutions We term qa c  e g the ad hoc solution of node  at time 033  It is common practice to measure the performance of an anytim e algorithm according to its recall  r t i v x y z r t i v x   r t i v x   and its precision  r t i v x y z r t i v x   z r t i v x   We require that if both c  e g and 037  o g of each j l c  e g remain static long enough then the approximated solution qa c  e g will converge to a c  e g  n other words both the precision and the recall of  converge to a hundred percent Throughout this work we make some simplifying assumptions We assume that node connectivity is a forest 320 for each  and j there could be either one route from  to j or none Trees in the forest may split join grow and shrink as a result of node crash and recovery or departure and join We assume the failure model of computers is stop and that a node is informed of changes in the status of adjacent nodes 3 n ARM Algorithm for Large-Scale Distributed Systems As previously described our algorithm is comprised of two rather independent components Each node executes a sequential ARM algorithm which traverses the local database and maintains the current result Additionally each node participates in a d istributed majority voting protocol which makes certain that all nodes that are reachable from one another converge toward the correct result according to their combined da tabases We will begin by describing the protocol and then proceed to show how the full algorithm is derived from it 3.1 LSD-Majority Protocol It has been shown in 11 t hat a di s t ri b u t e d A R M al gorithm can be viewed as a decision problem in which the participating nodes must decide whether or not each itemset is frequent However the algorithm described in that work extensively uses broadcast and global synchronization hence it is only suitable for small-scale distributed systems We present here an entirely different majority voting protocol 320 LSD-Majority 320 which works well for large-scale distributed syst ems In the interest of clarity we describe the protocol assuming the data at each node is a single bit We will later show how the protocol can easily be generalized for frequency counts As in LSD-ARM the purpose of LSD-Majority is to ensure that each node converges toward the correct majority Since the majority problem is binary we measure the recall as the proportion of nodes  whose ad hoc solution is one when the majority in c  e g is of set bits or zero when the majority in c  e g is of unset bits The protocol dictates how nodes react when the data changes a message is received or a neighboring node is reported to have detached or joined 3 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


The nodes communicate by sending messages that contain two integers 000 002 004 006 b  which stands for the number of bits this message reports and t 004 f which is the number of those bits which are equal to one Each node 004 will record for every neighbor r  the last message it sent to r 320 016 t 004 f 020 021 023 000 002 004 006 b 020 021 030 320 and the last message it received from r 320 016 t 004 f 021 020 023 000 002 004 006 b 021 020 030  Node 004 calculates the following two functions of these messages and its own local bit 033 020 034 t 020 037  021 020    t 004 f 021 020    000 020 037  021 020    000 002 004 006 b 021 020  033 020 021 034 t 004 f 020 021 037 t 004 f 021 020   0 000 002 004 006 b 020 021 037 000 002 004 006 b 021 020 2 Here 3 020 is the set of edges colliding with 004  t 020 is the value of the local bit and 000 020 is for now one 033 020 measures the number of access set bits 004 has been informed of 033 020 021 measures the number of access set bits 004 and r have last reported to one another Each time t 020 changes a message is received or a node connects to r or disconnects from r  033 020 is recalculated 033 020 021 is recalculated each time a message is sent to or received from r  Algorithm 1 LSD-Majority Input for node 004  The set of edges that collide with it 3 020  Abit t 020 and the majority ratio    The algorithm never terminates Nevertheless at each point in time if 033 020 8  then the output is  otherwise it is   De\336nitions 033 020 034 t 020 037  021 020    t 004 f 021 020    000 020 037  021 020    000 002 004 006 b 021 020 B  033 020 021 034 t 004 f 020 021 037 t 004 f 021 020   0 000 002 004 006 b 020 021 037 000 002 004 006 b 021 020 2 Initialization For each r 004 E 3 020 set t 004 f 021 020  000 002 004 006 b 021 020  t 004 f 020 021  000 002 004 006 b 020 021 to  Set 000 020 034   On edge r 004 recovery  Add r 004 to 3 020 Set t 004 f 021 020  000 002 004 006 b 021 020  t 004 f 020 021  000 002 004 006 b 020 021 to   On failure of edge r 004 E 3 020  Remove r 004 from 3 020  On message 016 t 004 f 023 000 002 004 006 b 030 received over edge r 004  Set t 004 f 021 020 034 t 004 f  000 002 004 006 b 021 020 034 000 002 004 006 b On change in t 020  edge failure or recovery or the receiving of a message For each r 004 E 3 020 If 000 002 004 006 b 020 021 037 000 002 004 006 b 021 020 034  and 033 020 8  or 000 002 004 006 b 020 021 037 000 002 004 006 b 021 020 O  and either 033 020 021 Q  and 033 020 O 033 020 021 or 033 020 021 8  and 033 020 Q 033 020 021 Set t 004 f 020 021 034 t 020 037  V 020 X Y 021 020    t 004 f V 020 and 000 002 004 006 b 020 021 034 000 020 037  V 020 X Y 021 020    000 002 004 006 b V 020 Send 016 t 004 f 020 021 023 000 002 004 006 b 020 021 030 over r 004 to r Each node performs the protocol independently with each of its immediate neighbors Node 004 coordinates its majority decision with node r by maintaining the same 033 020 021 value note that 033 020 021 034 033 021 020  and making certain that 033 020 021 will not mislead r into believing that the global majority is larger than it actually is As long as 033 020 8 033 020 021 8  and 033 021 8 033 021 020 8   there is no need for 004 and r to exchange data They both calculate the majority of the bits to be set thus the majority in their combined data must be of set bits If on the other hand 033 020 021 O 033 020 then r might mistakenly calculate 033 021 8  because it has not received the updated data from 004  Thus in this case the protocol dictates that 004 send r a message  t 020 037  V 020 X Y 021 020    t 004 f V 020 023 000 020 037  V 020 X Y 021 020    000 002 004 006 b V 020 _  Note that after this message is sent 033 020 021 034 033 020  The opposite case is almost the same Again if  O 033 020 021 8 033 020 and  O 033 021 020 8 033 021  then no messages are exchanged However when 033 020 O 033 020 021  the protocol dictates that 004 send r a message calculated the same way The only difference is that when no messages were sent or received r knows by default that 033 020 Q  and 004 knows that 033 021 Q   Thus unless 033 020 8   004 does not send messages to r because the majority bits in their combined data cannot be set The pseudocode of the LSD-Majority protocol is given in Algorithm 1 It is easy to see that when the protocol dictates that no node needs to send any message either 033 021 8  for all nodes r E h 004 i j or 033 021 Q  for all of them If there is disagreement in h 004 i j  then there must be disagreement between two immediate neighbors in which case at least one node r must send data which will cause 000 002 004 006 b 020 021 037 000 002 004 006 b 021 020 to increase This number is bounded by the size of h 004 i j  hence the protocol always reaches consensus in a static state It is less trivial to show that the conclusion they arrive at is the correct one This proof is too long to include in this context In order to generalize LSD-Majority for frequency counts 000 020 need only to be set to the size of the local database and t 020 to the local support of an itemset Then if we substitute l n 006 q r t u for   the resulting protocol will decide whether an itemset is frequent or infrequent in h 004 i j  Deciding whether a rule v w y is con\336dent is also straightforward using this protocol 000 020 should now count in the local database the number of transactions that include v  t 020 should count the number of these ansactions that include both v and y and  should be replaced with l n 006 z 002 006   Deciding whether a rule is correct or false requires that each node run two instances of the protocol one to decide whether the rule is frequent and the other to decide whether it is con\336dent Note however that for all rules of the form  w     only one instance of the protocol should be performed to decide whether  is frequent The ength of the protocol lies in its behavior when 4 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


the average of the bits over 000 001 003 004 is somewhat different than the majority threshold 005  De\336ning the signi\336cance of the input as 006 b n f r 016 017 021 b 022 024 006 b n f r 016 017 025 b 027 030  we will show in section 4.1 that even a minor signi\336cance on the scale of 031 033 035 030 issuf\336cient for making a correct decision using data from just a small number of nodes In other words even a minor signi\336cance is suf\336cient for the algorithm to become local Another strength of the protocol is that during static periods most of the nodes will make the correct majority decision very quickly These two features make LSDMajority especially well-suited for LSD-ARM in which the overwhelming majority of the candidates are far from signi\336cant 3.2 Majority-Rule Algorithm LSD-Majority ef\336ciently decides whether a candidate rule is correct or false It remains to show how candidates are generated and how they are counted in the local database The full algorithm must satisfy two requirements First each node must take into account not only the local data but also data brought to it by LSD-Majority as this data may indicate that additional rules are correct and thus further candidates should be generated Second unlike other algorithms which produce rules after they have 336nished discovering all items ets an algorithm which never really 336nishes discovering al l itemsets must generate rules on the 337y Therefore the candidates it uses must be rules not itemsets We now present an algorithm 320 MajorityRule 320 which satis\336es both requirements The 336rst requirement is rather easy to satisfy We simply increment the counters of each rule according to the data received Additionally we employ a candidate generation approach that is not levelwise as in the DIC algorithm 4  w e p erio d i cally c onsider all the correct rules regardless of when they were discovered and attempt to use them for generating new candidates The second requirement mining rules directly rather than mining itemsets 336rst and producing rules when the algorithm terminates has not to the best of our knowledge been addressed in the literature To satisfy this requirement we generalize the candidate generation procedure of Apriori 3  A pri o ri generat e s candi dat e i t e ms et s i n t w o w a ys  Initially it generates candidate itemsets of size 030  037   for every     Later candidates of size   030 are generated by 336nding pairs of frequent itemsets of size  that differ by only the last item 320   037    and   037  2  320 and validating that all of the subsets of   037   6  2  are also frequent before making that itemset a candidate In this way Apriori generates the minimal candidate set which must be generated by any deterministic algorithm In the case of Majority-Rule the dynamic nature of the Algorithm 2 Majority-Rule Input for node 001  The set of edges that collide with it 8   The local database       A C D F G    A I J A K   Initialization Set I L 037 N P Q 037   S K J D V W W     For each D  I set D 035 001 _ a D 035 c J 001 A g a 033 and D 035 005 a   A C D F G For each D  I and every j 001  8  set D 035 001 _  m a D 035 c J 001 A g  m a D 035 001 _ m  a D 035 c J 001 A g m  a 033 Upon receiving 037 D 035  s 6  001 _ 6 c J 001 A g  from a neighbor j If D a N  Q y S z I add it to I If c  a N P Q   y S z I add it too Set D 035 001 _ m  a  001 _  D 035 c J 001 A g m  a c J 001 A g On edge j 001 recovery Add j 001 to 8  Forall D  I set D 035 001 _  m a D 035 c J 001 A g  m a D 035 001 _ m  a D 035 c J 001 A g m  a 033 On failure of edge j 001  8   Remove j 001 from 8   Main Repeat the following for ever Read the next transaction 320 207  If it is the last one in    iterate back to the 336rst one For every D a N  Q y S  I which was generated after this transaction was last read If  214 207 increase D 035 c J 001 A g If   y 214 207 increase D 035 001 _ Once every  transactions Set 216\217 000 001 003 220 a the set of rules D a N  Q y S  I such that 222  224 D 225 226 033 and or D  a N P Q   y S 222  224 D  225 226 033 For every D a N  Q y S  216\217 000 001 003 220  such that  a P and    if D  a N X 240 037   Q 037   S z I insert D  into I with D  035 001 _ a D  035 c J 001 A g a 033  D  035 005 a   A I J A K and D  035  s a unique rule id For each c  a N  Q y  037    S 6 c 2 a N  Q y  037  2  S  216\217 000 001 003 004 such that   243  2 if c 244 a N  Q y  037   6  2  S z I and 246  244  y 250 D 244 a N  Q y  037   6  2  240 037  244  S  216\217 000 001 003 004 add D 244 to I with D 244 035 001 _ a D 244 035 c J 001 A g a 033  D 244 035 005 a D  035 005 and D 244 035  s a unique rule id For each D a N  Q y S  I and for every j 001  8  If D 035 c J 001 A g  m  D 035 c J 001 A g m  a 033 and 222  224 D 225 226 033 or D 035 c J 001 A g  m  D 035 c J 001 A g m  255 033 and either 222  m 224 D 225 243 033 and 222  224 D 225 255 222  m 224 D 225 or 222  m 226 033 and 222  224 D 225 243 222  m 224 D 225 Set D 035 001 _  m a D 035 001 _  257 260  262 263 m  264 265 r D 035 001 _ 260  and D 035 c J 001 A g  m a D 035 c J 001 A g  257 260  262 263 m  264 265 r D 035 c J 001 A g 260  Send 037 D 035  s 6 D 035 001 _  m 6 D 035 c J 001 A g  m  over j 001 to j 5 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


system means that it is never certain whether an itemset is frequent or a ule is correct Thus it is impossible to guarantee that no super\337uous candidates are generated Nevertheless at any point during execution 000  it is worthwhile to use the ad hoc set of rules 001\002 004 005 007 b  to try and limit the number of candidate rules Our candidate generation criterion is thus a generalization of Apriori\325s criterion Each node generates initial candidate rules of the form t 013 r 017 021 for each 017 023 025  Then for each rule t 013 030 023 001\002 004 005 007 b it generates 030 036 r 017 021 013 r 017 021 candidate rules for all 017 023 030  In addition to these initial candidate rules the node will look for pairs of rules in 002 004 005 007 b which have the same lefthand side and right-hand sides that differ only in the last item 320 030 013   r 017  021 and 030 013   r 017  021  The node will verify that the rules 030 013   r 017  5 017  021 036 r 017 7 021 for every 017 7 023   are also correct and then generate the candidate 030 013   r 017  5 017  021  It can be inductively proved that if 001\002 004 005 007 b contains only correct rules then no super\337uous candidate rules are ever generated using this method The rest of Majority-Rule i s straightforward Whenever a candidate is generated the node will begin to count its support and con\336dence in the local database At the same time the node will also begin two instances of LSDMajority one for the candidate\325s frequency and one for its con\336dence and these will determine whether this rule is globally correct Since each node runs multiple instances of LSD-Majority concurrently messages must carry in addition to  005  and  A 005 B 000  the identi\336cation of the rule it refers to D E 017 F  We will denote G H I D K and G H L I D K the result of the previously de\336ned functions when they refer to the counters and N of candidate D  Finally D E N is the majority threshold that applies to D Weset D E N to O 017 B S D U V for rules with an empty left-hand side and to O 017 B Y A B Z for all other rules The pseudocode of Majority-Rule is detailed in Algorithm 2 4 Experimental Results To evaluate Majority-Rule\325s performance we implemented a ulator capable of running thousands of simulated computers We simulated 1600 such computers connected in a random tree overlaid on a   _   grid We also implemented a simulator for a stand-alone instance of the LSD-Majority protocol and ran simulations of up to 10,000 nodes on a    _    grid The simulations were run in lock-step not because the algorithm requires that the computers work in locked-step 320 the algorithm poses no such limitations 320 but rather because properties such as convergence and locality are best demonstrated when all processors have the same speed and all messages are delivered in unit time We used synthetic databases generated by the standard tool from the IBM-quest data mining group 3 W e gener ated three synthetic databases 320 T5.I2 T10.I4 and T20.I6 320 where the number after T is the average transaction length and the number after I is the average pattern length The combined size of each of the three databases is 10,000,000 transactions Other than the number of transactions the change we made from the defaults was reducing the number of patterns This was reduced so as to increase the proportion of correct rules from one in ten-thousands to one in a hundred candidates Because our algorithm performs better for false rules than for correct ones this change does not impair out the validity of the results 4.1 Locality of LSD-Majority and Majority-Rule The LSD-Majority protocol and consequently the Majority-Rule algorithm are local algorithms in the sense that a correct decision will usually only require that a small subset of the data is gathered We measure the locality of an algorithm by the average and maximum size of the environment of nodes The environment is de\336ned in LSDMajority as the number of input bits received by the node and in Majority-Rule as the percent of the global database reported to the node until system stabilization Our experiments with LSD-Majority sh ow that its locality strongly depends on the signi\336cance of the input d e g h i j k m e n o d e g h i j k q e r   Figure 1\(a describes the results of a simulation of 10,000 nodes in a random tree over a grid with various percentages of set input bits at the nodes It shows that when the signi\336cance is s  E  i.e  w x or w w x of the nodes have set input bits the protocol already has good locality the maximal environment is about 1200 nodes and the average size a little over 300 If the percentage of set input bits is closer to the threshold a large portion of the data would have to be collected in order to 336nd the majority In the worst possible case when the number of set input bits is equal to the number of unset input bits plus one at least one node would have to collect all of the input bits before the solution could be reached On the other hand if the percentage of set input bits is further from the threshold then the average environment size becomes negligible In many cases different regions of the grid may not exchange any messages at all In Figure 1\(c these results repeat themselves for Majority-Rule Further analysis 320 Figure 1\(c 320 show that the size of a node\325s environment depends on the signi\336cance in a small region around the nodes I.e if the inputs of nodes are independent of one another then the environment size will be random This makes our algorithms fair nodes\325 performance is not determined by its connectivity or location but rather by the data 6 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


 0 1000 2000 3000 4000 5000 6000 10 20 30 40 45 50 55 60 70 80 90 Environment size Percentage of set bits a 20 40 60 80 100 10 20 30 40 50 60 70 80 90 100 b 0 10 20 30 40 50 60 70 80 90 100 0.1 0.05 0 0.05 0.1 Percent of DB gathered Rule significance c Worst-case Average Figure 1 The locality of LSD-Majority a and of Maj ority-Rule c depends of the signi\336cance The distribution of environment sizes b depe nds on the local signi\336cance and is hence random 4.2 ence and Cost f Rule In addition to locality the other two important characteristics of Majority-Rule are its convergence rate and communication cost We measure convergence by calculating the recall 320 the percentage of rules uncovered 320 and precision 320 the percentage of correct rules among all rules assumed correct 320 vis-a-vis the number of transactions scanned Figure 2 describes the convergence of the recall a and of the precision b In c the convergence of stand-alone LSD-Majority is given for various percentages of set input bits To understand the convergence of Majority-Rule one must look at how the candidate generation and the majority voting interact Rules which are very signi\336cant are expected to be generated early and agreed upon fast The same holds for false candidates with extremely low signi\336cance They too are genera ted early because they are usually generated due to noise which subsides rapidly as a greater portion of the local database is scanned the convergence of LSD-Majority will be as quick for them as for rules with high signi\336cance This leaves us with the group of marginal candidates those that are very near to the threshold these marginal candidates are hard to agree upon and in some cases if one of their subsets is also marginal they may only be generated after the algorithm has been working for a long time We remark that marginal candidates are as dif\336cult for other algorithms as they are for Majority-Rule For instance DIC may suffer from the same problem if all rules were marginal then the number of database scans would be as large as that of Apriori An interesting feature of LSD-Majority convergence is that the number of nodes that assume a majority of set bits always increases in the 336rst few rounds This would result in a sharp reduction in accuracy in the case of a majority of unset bits and an overshot above the otherwise exponential convergence in the case of a majority of set bits This occurs because our protocol operates in expanding wavefronts convincing more and more nodes that there is a certain majority and then retreating with many nodes being convinced that the majority is the opposite Since we assume by default a majority of zeros the st wavefront that expands would always be about a majority of ones Interestingly enough the same pattern can be seen in the convergence of Majority-Rule more clearly for the precision than for the recall Figure 3 presents the the communication cost of LSDMajority vis-a-vis the percentage of set input bits and of Majority-Rule vis-a-vis rule signi\336cance For rules that are very near the threshold a lot of communication is required on the scale of the grid diameter For signi\336cant rules the communication load is about ten messages per rule per node However for false candidates the communication load drops very fast to nearly no messages at all It is important to keep in mind that we denote every pair of integers we send a message In a realistic scenario a message will contain up to 000 002 004 004 bytes or about 000 007 004 integer pairs 5 Conclusions We have described a new distributed majority vote protocol 320 LSD-Majority\320 which we incorporated as part of an algorithm 320 Majority-Rule 320 that mines association rules on distributed systems of unlimited size We have shown that the key quality of our algorithm is its locality 320 the fact that information need not travel far on the network for the correct solution to be reached W e have also shown that the locality of Majority-Rule translates into fast convergence of the result and low communication demands Communication is also very ef\336cient at least for candidate rules which turn out not to be correct Since the overwhelming majority of the candidates usually turn out this way the communication load of Majority-Rule depends mainly 7 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


 0 0.2 0.4 0.6 0.8 1 0.01 0.1 1 10 100 Recall Number of database scans a  T5  T10  T20   0 0.2 0.4 0.6 0.8 1 0.01 0.1 1 10 100 Precision Number of database scans b  T5  T10  T20  Percent of Nodes 60 80 100 40 20 0 10 100 1000    1 10 20 30 40 45 48 52 55 60 70 80 90 c Steps Figure 2 Convergence of the recall and precisi on of Majority-Rule and of LSD-Majority 0 10 20 30 40 50 60 70 80 1 0.5 0 0.5 1 Average number of per node messages Rule significance a 0 10 20 30 40 50 60 70 80 10 20 30 40 45 50 55 60 70 80 90 Number of messages Percentage of set bits b Figure 3 Communication characteristics of Major ity-Rule a and of LSD-Majority b Each message here is a pair of integers on the size of the output 320 the number of correct rules That number is controllable via user supplied parameters namely 000 002 004 006 b n f and 000 002 004 017 020 004 021  References 1 R  A g r a w a l  T  I mie lin sk i a n d A  N  S w a mi M in in g a ssociation rules between sets of items in large databases In Proc of the 1993 ACM SIGMOD Int\222l Conference on Management of Data  pages 207\226216 Washington D.C June 1993 2 R  A g r a w a l a n d J  S h a fe r  P a ra lle l m in in g o f a sso c i a tio n rules IEEE Transactions on Knowledge and Data Engineering  6\:962 226 969 1996 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules In Proc of the 20th Int\222l Conference on Very Large Databases VLDB\22294  pages 487 226 499 Santiago Chile September 1994 4 S  B r i n R  Mot w ani  J Ul l m an a nd S  T s ur  D ynami c itemset counting and implication rules for market basket data SIGMOD Record  6\(2\:255\226264 June 1997 5 D  C heung J Han V  Ng A  F u and Y  F u A f ast d i s tributed algorithm for mining association rules In Proc of 1996 Int\222l Conf on Parallel and Distributed Information Systems  pages 31 226 44 Miami Beach Florida December 1996 6 E nt r opi a http://www.entropia.com   E  H S  Han G Karypi s and V  K umar  S cal abl e paral l e l data mining for association rules IEEE Transactions on Knowledge and Data Engineering  12\(3\:352 226 377 2000 8 J  L  L i n a nd M H Dunham Mi ni ng associ at i o n r ul es Anti-skew algorithms In Proceedings of the 14th Int\222l Conference on Data Engineering ICDE\22298  pages 486\226 493 1998 9 G  S  M anku and R  M ot w a ni  A ppr oxi mat e f r e quenc y counts over data streams In Proceedings of the 28th International Conference on Very Large Data Bases VLDB\22202  Hong Kong China August 2002  T  C  P r oj ect  http://www.cs.wisc.edu condor   A S c hust e r a nd R  W o l f f  C o mmuni cat i onef 036 c i e nt di stributed mining of association rules In Proc of the 2001 ACM SIGMOD Int\222l Conference on Management of Data  pages 473 226 484 Santa Barbara California May 2001  S e t i  home http://setiathome.ssl berkeley.edu   S  T homas and S  C hakra v art h y  Increment a l m i n i n g o f constrained associations In HiPC  pages 547\226558 2000 14 Un ite d d e v ic e s in c  h ttp www u d  c o m/h o m e  h t m  M J Z a ki  S  P ar t h asar at hy  M  O gi har a  a nd W  L i  P ar allel algorithms for discovery of association rules Data Mining and Knowledge Discovery  4\:343\226373 1997 8 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


 0 5 10 15 20 25 30 35 40 10 15 20 25 30 average number of items in transactions T I=6 D=200K  o f d ata p rocessed 0 50 100 150 200 250 300 350 400 450 500 ti m e m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 5 Pruning and CPU time varying T 0 2000 4000 6000 8000 10000 12000 14000 10 15 20 25 30 average number of items in transactions T I=6 D=200K number o f r andom I  Os SG-table SG-tree Figure 6 Random I/Os varying T 0 5 10 15 20 25 30 35 40 6121824 average length of large itemsets I T=30 D=200K  o f d ata p rocessed 0 50 100 150 200 250 300 350 400 450 500 ti m e m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 7 Pruning and CPU time varying I 0 2000 4000 6000 8000 10000 12000 14000 6121824 average length of large itemsets I T=30 D=200K numbe r of ra ndom I  O s SG-table SG-tree Figure 8 Random I/Os varying I 0 2 4 6 8 10 12 14 16 T=10,I=6 T=20,I=12 T=30,I=18 T=40,I=24 T=50,I=30 Varying T and I I/T=0.6 D=200K  o f d ata p rocessed 0 20 40 60 80 100 120 140 160 tim e m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 9 Pruning and CPU time 336xed I/T 0 200 400 600 800 1000 1200 1400 1600 1800 2000 T=10,I=6 I=12 T=30,I=18 I=24 T=50,I=30 Varying T and I I/T=0.6 D=200K numbe r of ra ndom I  Os SG-table SG-tree Figure 10 Random I/Os 336xed I/T 0 1 2 3 4 5 6 7 100 00 300 400 500 Data set cardinality T=10 I=6  o f d ata p rocessed 0 10 20 30 40 50 60 70 80 90 ti m e m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 11 Pruning and CPU time varying D 0 10 20 30 40 50 60 0 1 to 3 4 to 10 11 to 20 20 distance of nearest neighbor T30.I18.D200K  o f d at a p rocessed 0 100 200 300 400 500 600 700 800 time m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 12 Pruning and CPU time var 000 000 003 005 007 t  83  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


shows a pattern similar to the CPU cost as in the previous experiments During the experiments we observed that queries having a close nearest neighbor were processed fast using both structures whereas for cases with distant neighbors the SG\226tree was signi\036cantly faster than the SG\226table We validated this observation by running 1000 queries on the T30.I18.D200K dataset and averaging the query costs for various distance ranges of the nearest neighbor Figure 12 shows the average pruning an d CPU cost for 036ve distance ranges When the distance is small search is fast for both methods actually for distances in the range 1\2263 the SG\226 table outperforms the SG\226tree However the distant cases are handled much faster by the SG\226tree showing that this access method is more robust to 221outlier\222 queries As a general conclusion from this set of experiments the SG\226tree is a more ef\036cient and robust access method than the SG\226table in addition to its other inherent advantages dynamic data handling independence to hard-wired constants In the next subsection we compare the indexes for other query types on both synthetic and real data 5.4 Real data nd other queries Figures 13 and 14 show the performance of the indexes for 000 NN queries on the T30.I18.D200K synthetic dataset and the CENSUS dataset respectively for various values of 000  The results for each experimental instance were averaged over 100 queries In both 036gures for small to medium values of 000 the SG\226tree is signi\036cantly faster than the SG\226 table When 000 is large  001 003 005 005 005  the fraction of the data that need to be visited becomes too large for the indexes to be useful This is due to the fact that the search space becomes less appropriate for search For example when 000 t 003 005 005 005 005 we observed that the average distance of the 000 th neighbor is very large 31.81 for T30.I18.D200K and 18.06 for CENSUS and very close to the average distance of all transactions from f  This is due to the 221dimensionality curse\222 effect 3 o ften o b s erv e d i n h ig h d i men s io n a l search problems Observe that the SG\226tree is less sensitive to this effect since its performance degenerates at a smaller pace especially for the real dataset We also compared the indexes for similarity range queries Figures 15 and 16 The same datasets and queries as before are used and the distance threshold from the query varies from 2 to 10 For r t 020  the SG\226table outperforms the SG\226tree on the synthetic dataset In all other cases the tree is much faster Observe that on the real dataset in particular for both 000 NN queries and range queries the performance difference quite large in favor of the tree This indicates that the structure can perform very well in real life cases  0 10 20 30 40 50 60 70 80 90 100 1 10 100 1000 10000 k-nn search varying k T30.I18.D200K  o f d ata p rocessed 0 200 400 600 800 1000 1200 1400 time\(msec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 13 021 NN queries T30.I18.D200K 0 10 20 30 40 50 60 70 80 90 100 1 10 100 1000 10000 k-nn search varying k CENSUS  o f d ata p rocessed 0 100 200 300 400 500 600 tim e m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 14 022 NN queries CENSUS 5.5 Dynamic data changes In this experiment we compare the structures simulating a case where the nature of the data changes dynamically We generated a synthetic dataset T10.I6.D100K and built an SG\226table and SG\226tree for it We then gradually updated the structures by inserting batches of 100K transactions each with the same characteristics i.e T=10 I=6 but putting different seeds to the random generator i.e the large itemsets used were different for each batch We ran nearest neighbor queries on the two structures after each insertion phase The queries for phase 023 after batch 023 has been inserted 024 026 023 026 032  are generated as follows For each query i a random number 033 from 1 to 023 is chosen and ii the generator parameters i.e large itemsets for batch 033 are used to produce the query For example a query for the phase where the dataset contains 300K data is generated using randomly one of the generators of batches 1 2 or 3 Figure 17 shows the average pruning ef\036ciency and CPU time of the two structures Initially both have similar performance but as more data with different characteristics are inserted into the structures the performance of the SG\226table degenerates since it is optimized for the 036rst 100K data  84  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


 0 5 10 15 20 25 30 35 40 246810 similarity range queries varying epsilon T30.I18.D200K  o f d at a p r o cessed 0 50 100 150 200 250 300 350 400 tim e m s e c  SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 15 Range queries T30.I18.D200K 0 10 20 30 40 50 60 70 80 246810 similarity range queries varying epsilon CENSUS  o f d ata p r o cessed 0 50 100 150 200 250 300 350 400 ti me\(msec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 16 Range queries CENSUS On the other hand the SG\226tree is robust to updates and exhibits very good query performance since each batch contains skewed data generated from a different collection of large itemsets 6 Conclusions and Future Work We presented a hierarchical indexing method for similarity search in sets and categorical data The SG\226tree is a disk-based height-balanced tr ee that organizes 036xed-length bitmaps and is appropriate for various query types We have shown how several branch-and-bound methods which apply on R\226tree-like structures can be adapted for ef\036cient similarity search on the SG\226tree Extensive experimental evaluation has shown that the SG\226tree is in most cases much faster than the SG\226table a previous hash-based index The advantages of the SG\226tree can be summarized as follows 000 It is ef\036cient and robust to various data types both categorical and set data and characteristics cardinality density dimensionality It is a versatile structure that can be used for several query types 000 The tree is dynamically adapted to updates and re0 2 4 6 8 10 12 100 200 300 400 500 Dataset cardinality T=10 I=6  o f d ata p rocessed 0 20 40 60 80 100 120 140 160 180 200 tim e m sec SG-table\(%data SG-tree\(%data SG-table\(time SG-tree\(time Figure 17 NN search after dynamic updates quires no preprocessing of the data Thus it can be useful for analyzing data which change dynamically over time 001 It relies on no hardwired constants and requires no tuning using a-priori de\036ned parameters 001 It is a disk-based paginated data structure so it can operate with limited memory resources and dynamically changing memory resources Caching policies previously used for the B 002 226tree and the R\226tree can be seamlessly applied on this structure There are several directions for extending the current work In our study we used hamming distance as the similarity metric However the SG\226tree can also be de\036ned tuned and searched for other set theoretic similarity metrics For example if the Jaccar d coef\036cient is used the lower distance bound in fact the upper similarity bound for nearest neighbor search can be de\036ned by 003 005 007 b n f 016 007 020 021 023 024 026 030 003 005 007 b n f 026  We plan to test the effectiveness of the structure using alternative metrics Another direction or future work is to study methods for bulk-loading SG\226trees instead of inserting the data oneby-one We can adapt categor ical clustering algorithms 12 for t hi s purpos e Anot her a pproach i s t o s o rt t h e transactions using gray codes as key in analogy to using space-\036lling curves for bulk-loading multidimensional data to an R\226tree 17  A lternati v ely  hashing t echniques can be used to group similar signatures together The resulting 221globally-optimized\222 tree could have much better quality characteristics while being built faster In a reverse direction we can investigate whether the SG\226tree can be used for clustering large dynamic collections of set and categorical data The cost of existing methods is at least 035 n   026 and the tree could be used to derive good clusters much faster e.g by merging the leaf nodes using their signatures as guides Finally we plan to empirically test the ef\036ciency of the tree to the query types discussed in Section 4.2 In  85  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


addition for some data types search can be further optimized For example if the indexed categorical data have 223xed-dimensionality 000 we know that the area of each indexed signature is 223xed to 000  We can use this property to derive stricter lower bounds for the directory node entries 001  instead of the rather relaxed 002 004 006 006 t 013 r 001 020 022 004 025 027 For this example a better bound is 002 004 006 006 t 013 r 001 020 022 004 025 027 033 t 000 037    t 001 020 022 004 025 r 013 027 027  We plan to study such search optimizations using domain properties or statistics from the indexed data References  C  C  A ggarw al  J  L  W ol f and P  S  Y u A N e w Method for Similarity Indexing of Market Basket Data SIGMOD Conference  pages 407\205418 1999  R  A gra w al and R  S ri kant  F as t A l gori t h ms for M i n ing Association Rules in Large Databases VLDB Conference  pages 487\205499 1994  K  S  B e y er  J  G ol ds t e i n  R  R amakri s hnan and U Shaft When Is 215Nearest Neighbor\216 Meaningful International Conference on Database Theory  pages 217\205235 1999  T  B ri nkhof f H.-P  K ri e g el  a nd B  S e e g er  E f 223 ci ent Processing of Spatial Joins Using R-Trees SIGMOD Conference  pages 237\205246 1993  A  C orral  Y  Manol opoul os  Y  T heodori d i s  a nd M Vassilakopoulos Closest Pair Queries in Spatial Databases SIGMOD Conference  pages 189\205200 2000  A  P  d e V ries N  M amoulis N  N es a nd M K e r sten Ef\223cient k-NN Search on Vertically Decomposed Data SIGMOD Conference  pages 322\205333 2002  U  D eppisch S-T r ee A D ynamic B alanced Signature Index for Of\223ce Retrieval ACM SIGIR Conference  pages 77\20587 1986  V  G aede a nd O G 250 unther Multidimensional Access Methods ACM Computing Surveys  30\(2\170\205231 1998  V  G ant i  J  Gehrk e  a nd R  R a makri s hnan C A C T US 205 clustering categorical data using summaries ACM SIGKDD Conference on Knowledge Discovery and Data mining  pages 73\20583 1999  D Gi bs on J  M Kl ei nber g  a nd P  R a gha v a n C l us tering Categorical Data An Approach Based on Dynamical Systems VLDB Conference  pages 311\205322 1998  A Gi oni s  D Gunopul os  a nd N K oudas  Ef 223 c i e nt and Tunable Similar Set Retrieval SIGMOD Conference  2001  S  Guha R  R as t ogi  a nd K S h i m  R OC K A R obust Clustering Algorithm for Categorical Attributes International Conference on Data Engineering  pages 512\205521 1999  A Gut t m an R T rees  A Dynami c I nde x S t r uct u re for Spatial Searching SIGMOD Conference  pages 47\205 57 1984  S  Hel m er and G  M oerk ot t e  A S t udy of F our Inde x Structures for Set-Valued Attributes of Low Cardinality Technical Report University of Mannheim  number 2/99 1999  G R Hjaltason a nd H Samet Distance Bro w sing in Spatial Databases TODS  24\(2\265\205318 1999  A K J a i n and R  C  D ubes  Algorithms for Clustering Data  Prentice-Hall 1988  I Kamel a nd C  F a louts o s  Hilbert R tree An Improved R-tree using Fractals VLDB Conference  pages 500\205509 1994  F  K o rn N  S i d i r opoul os  C  F al out s o s  E S i e g el  a nd Z Protopapas Fast Nearest Neighbor Search in Medical Image Databases VLDB Conference  pages 215\205 226 1996  N K oudas a nd K C  S e vci k  H i g h D i m ens i onal S i m i larity Joins Algorithms and Performance Evaluation International Conference on Data Engineering  pages 466\205475 1998  N R ous s opoul os  S  K el l e y  and F  V i n cent  Neares t Neighbor Queries SIGMOD Conference  pages 71\205 79 1995  Y  S a kurai  M  Y os hi ka w a  S  U emura and H  K oj i m a The A-tree An Index Structure for High-Dimensional Spaces Using Relative Approximation VLDB Conference  pages 516\205526 2000  The U C I KDD Archi v e ht t p    kdd.i c s  uci  edu 23 R W e b e r  H.-J S ch ek  a n d S Blo tt A Q u a n titative Analysis and e Study for SimilaritySearch Methods in High-Dimensional Spaces VLDB Conference  pages 194\205205 1998  86  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


