MULTI-SENSES AND MULTI-DEPENDENCIES DISCOVERY AMONG WORDS Tang Jie Li JuanZi Wang Ke-Hong Cai Yue-Ru Knowledge Engineering Group Department of Computer Tsinghua University, P.R.China 100084 j tang02@mails.tsin~iua.edu.cn Ijz@keg.cs.tsiiighua,edu.cn ABSTRACT Word sense and word dependency benefit many applications Manually constructed lexicon usually serves 
as a source for word sense and word dependency However there always requires very expensive work and extensive time consumption to compile it simultaneously senses and relationships anlong words in these kinds of lexica are always missed Automatically compilied lexica are under developing the main obstacle is to discover multi-senses and multi-dependencies among words In this paper 
we propose a new method combining an improved ISODATA Clustering algorithm with Association Rule mining to answer the question With the recursively clustering algorithm lower frequency senses are discovered As well a approach for refinement is put forward to improve the precision Experiments indicate that the approach presented here provides preferable outputs Keywords Word Sense Word Dependency 
Clustering Algorithm 1 INTRODUCTION With the development of software intelligence electronic lexicon is becoming a more and more important tool for many applications such as information retrieval NLP\(Natnra1 Language 0-7803-7902-0/03/$17.00 2003 IEEE 132 Processing\and machine leaning Two main parts in lexicon are word senses and word dependencies Word senses are the meanings 
of the word usually interpreted as a serial of synonyms or other similar words Word dependencies are the relationships between words such as subset of hypemyms of meronyms of and causal dependencies etc Traditional thesaurus are often constructed manually e.g WordNet Miller et al.19901 It leads to very expensive 
work and extensive time consumption. At the same time maintenance of these lexica is also difficult For example WordNet classifies retiring into four senses zmnssertive 1tnass7rming receding past but misses the sense of alone Neither could WordNet deal with the unknown word e.g LSI an 
abbreviation for Latent Semantic Indexing Many researchers are working on automatically compiling lexica Landauer et al.1990 Lin 1998 Pereira et al 19931 The work is based on the known Distributional Hypothesis Harris 19851 which supposed that words occurring in the same context tend to be similar Accordingly various methods to acquire senses are proposed 
however rarely addresses the multi-senses discovery Lin put forward cluster algorithm CBC to discover multi-senses Lin 1998 Patrick 20021 But without rigorous refinement these methods also lead to some frustrating output For instance Americnn is learned as a close word to retiring in their lexicon 


Relationship discovery is often defmed to fmd interest pattems for specific applications. Past work mainly focused on shopping data such as market basket problem Introduced into electronic lexicon the interest pattems can be interpreted as word Y is very likely to be presented in the context containingX A  In this paper we propose a new approach to learn multi-senses and multi-dependencies among words Content of this paper is structured as followings Section 2 gives the survey of the related work Section 3 describes our method to acquire multi-dependencies and multi-senses among words The evaluation and experiment are given in Section 4 and 5 Finally we conclude the paper with a discussion 2 RELATED WORK Andrei Mikheev and Steven Finch design KAWB to acquire knowledge from test Mikheev et al.19971 The workbench compromises a set of computational tools for uncovering intemal structure in natural language texts Two main stages i.e text representation and text analysis are included in their workbench to implement incremental process including a rough automatic extraction organization of semantic regularity and a computer supported analysis of extracted data Alexander Maedche and Raphael Volz also put fonvard an Ontology Extraction and Maintenance Framework to accelerate ontology construction and simplifv the ontology maintenance Alexander et al.20011 They employed a hierarchical cluster algorithm to acquire the word meanings, and adopted association rule algorithm to obtain relationships anlong words Faure and Nedellec presented a cooperative machine learning system, ASIUM, which acquires taxonomic relations and sub-categorization frames of verbs based on syntactic input Faure et al 19981 Thc ASIUM systcm hierarchically clustcrs nouns based on the verbs that they are syntactically related with and vice versa Thus with the cooperative operation they extended the lexicon the set of concepts and the concept hierarchy Hahn and Schnattinger introduced a methodology for the maintenance of domain specific taxonomies They built up a framework Syndicate for natural language understanding Hahn et al.20001 But in their methods they failed to extract multi-senses among words Landauer and Dumais employed the LSI Latent Semantic Indexing to acquire latent word senses Z Nor did they address the multi-senses discovery. Dekang Lin and Patrick Pantel put forward clustering algorithm CBC to discover multi-senses from words 3,6 They use a set of tight clusters called committee to define the center for the clusters assign other words to their most close committee and remove the intersecting features from words after each cluster pass and then re-cluster the new matrix to find lower frequency senses In this paper we put forward a framework Concepts Network Leaming Workbench CNLW In this workbench we intend to construct a lexicon automatically with multiple word senses and multi-dependencies among words  3 CNLW ARCHITECTURE In this section we elaborate the architecture of CNLW The systematic model can be described as a 4-tuple CNL W  LexicalParser DependencyMiner SenseMiner Re Jiner Where LexiculPnrser parses the text corpora segments the sentences into words and removes the stop words then constructs item-by-item matrix DependencyMiner mines dependencies among words SenseMiner is used to discovcr multi-sets 133 


similarity words for each word and result in similar word sets with similarity value Rejner is a very important component which compares the result of SenseMiner with WordNet and refines the results CNLW architecture is depicted in figure 1 II c Lexical Parser lmtrix I I b-l?t Redidisplay Figure 1  Shucture of CNLW CN is the leaniing result naniely concepts network We defuie CN as CN  Cs Ds WSS  where Cs is a set of words Ds is a set of dependencies Wss is the collection of senses for each word 3.1 Multi-Dependencies Discovery Association rule mining is a powerful tool in data mining It is usually used to mine item relationships for shopping baskets An early approach due to Agrawal, Imielinski and Swami [Agrawal et al.19931 was to find a set of items that occur together often i.e high support and also have the property that one item often occurs in a transaction containing the other items i.e high confidence Its measure for interest is based on the conditional probability After this mining processing relationships can be discovered A relation can be described as R  Id Anlecedeiit,Coiisequeiil Support Conjdence Re lationship Where Antecedent and Consequent are two word sets in the relation which can be interpreted as Antecedent  Consequent Support is the relation weight Conjdence stands for the reliability of the relation Relationship denotes the type of the relation For Example as for the CRANFIELD document Experiments  Perform cone 65.3% sup: 7.2 Kinematic  veloci conf 73.4 sup 6.4 High, speed  flight conf66.3 5.8 collection  Type of dependency between two word sets is required to be specified manually As for Chinese corpora seventy four dependency categories derived from HowNet are used to choose As for English corpora dependency categories are based on FrameNet 3.2 Multi-senses Discovery We employ an improved clustering algorithm to yield multi-senses for words First a word similarity matrix is generated according to original corpora Every value in the matrix indicates the similar degree between two words Then multi-passes clustering is applied on the matrix For each pass an appropriate meaning for every word is yielded Then corresponding features are removed from its feature vectors for the next clustering pass We use a classic similarity measure namely cosine coefficient to compute the similarity matrix c  w wi  wen is the feature vector of word i where wU is computed by TF*IDF Similarity between ci and cj is defined as 134 


As for partitional clu$tering seed selection is the key Various selections would result in different outputs We derive an idea from ISODATA algorithm to realize dynamic tuning K Ravi et a1.19991 Based on the intersecting features removing we improve the algorithm with character of multi-pass clustering Followings are the details of the algorithm Input words similar matrix minimum size of cluster N partitional and merging thresholds y 9 maximum merging clusterj count in each L maximum iterative times T maximum senses thresholdMs  Step 1 K  J  Select K-seeds randomly Let these seeds as the centroids of clusters Step 2 foreach\(words i foreach\(c1usterj compute sim\(cml.,c I1 em is the centroid of clusterj Step 3 foreach\(c1usters assign word to its closest cluster If cozmtelements\(cl~ster i remove\(chister I reclassify\(words in it Step 4 Compute the new centroids for each clusters Step 5 Compute error for each cluster Compute distance for every two clusters dis  Ikm  eml II Step 6 foreach\(cl2rsters if en  y partition the cluster i foreach\(c1uster pii-wises if dis  7 add the painvise to merge list merge the first min\(L,sizeof\(list painvise clusier Step 7 compute new cluster centroids if \(iterative times Tor no update to clusters goto Step 8 else goto Step 2 Step 8 foreach\(c1usters remove\(out1ier labelByFeatures\(c1uster RemoveIntersectingFeatures\(word cluster foreach\(word Step 9 if maxfsenses  Ms Goto Step 1 else retum In Step 8 outliers for each cluster will be removed from the cluster by remove\(out1ier labelByFentures\(j is to label cluster by its high scored features RemoveIntersectingFea~~s\(j is to remove intersecting features from word vector The ji lfeahtre score indicating how much a feature contributes to the cluster. It is defined as follow Features with higher score are chosen for the cluster label and then remove them from the feature matrix for next pass After the clustering algorithm lower frequency senses could also be found Follow is an example of the clustering result after one run problem case, solutions   molecules onygen atoms   large, measurements small   basis important, limited considerable   long injiniteb short, recently  ISODATA is a solution for dynamic tuning K for clustering. But it doesn't address features scoring for clustcrs In CBC intersccting featurcs are also 135 


removed from word to find multi-senses the features for each clusters are predefined by the committee 3,6 Contrarily, we define the features for clusters according to the clustering result, which provides an alternative method 3.3 Refinement It is necessary to refine the result from the previous step. We compare the output of our system with the WordNet to improve the precision WordNet is an electronic dictionary organized as a concept directed graph with nodes i.e Synset representing a set of synonymous words and arcs between nodes representing senses relationships such as hypemym meronym etc. The similarity between synset sI and s can be defined as 6 Where p\(s is the probability of a randomly chosen word referring to a synset or any synsets below it s is the common hypemym concept of both s and sI In this way, we compute similarity between words and elements in its each sense represented by a word cluster\Similar value below a threshold 6 usually set to 0.15 would be regarded as noise and be removed from its sense If word dose not exist in WordNet keep it as a new word, e.g LSZ 4 EXPERIMENT Derived from information retrieval community precision and recall are used for the evaluation Adopting them to knowledge learning, we could get follo\\vs Where wsA is the CN learned by this workbench c is the CN built manually In our experiment we compare output with WordNet to get precision and recall CNLW can deal with both English corpora and Chinese corpora Our experiments are based on hvo English data sets CRANFIELD documents from aeronautical system papers Distribution 1 O of the Reuters-21578 text categorization test collection As for CRANFIELD documents,,we focus on domain senses discover for words Afier removing the stop words, the resulting test set consists of 8588 words and the average number of feature is 670.3 As for test collection of Reuters-21578 more common senses of words are concemed After removing the stop words the result consists of 12783 words with 722.6 average features Table 1 is the output of the dependencies discovering, table 2 is the sense mining result. Figure 3 shows the precision and recall for the senses learning Average Average Average depend confid support Encies 18765 6.73 38.4 CRAN FIELD 30849 2.4 6.41 39.1 Reuters 21578 Table 1 Output of Dependency mining Test Data count 10.8 12.3 9.7 Table 2 Output of senses discovey In table 2 average similarity denotes average similarity between word and its senses 136 


30 35 40 45 50 55 60 65 70 75 80 Figure 2 Precision-Recall Evaluation Result 5 CONCLUSIONS With the rapid development of WWW and computer technique data and information have been growing so large that to deal with them manually is intractable Knowledge learning especially from unstructured data becomes more important to the nex generation of software. In this paper, a method to discover multi-dependencies and multi-senses among words is provided With a multi-passes clustering algorithm senses even lower frequency can be discovered Research and experiments show that this method can reduce the cost of building up the CN greatly However further work is still needed to provide a more practicable system Based on this paper we will improve on these aspects 1 Highly accurate and efficient approaches are still required to improve the performance of the learning processing 2 More practicable evaluation methods need to be developed References I Miller G WordNet An online lexical database Intemational Journal of Lexicography MIT Press 1990 2 Landauer T K Dumais S T A solution to Plato\222s problem The Latent Semantic Analysis theory of the acquisition induction and representation of knowledge Psychological Review 104 1990:211-240 3 Dekang L. Automatic retrieval and clustering of similar words In Proceedings of  Pereira et al Distributional clustering of English words ACL-93 183-190 5 Harris Z Distributional structure In Katz J J ed The Philosophy of Linguistics New York Oxford University Press 1985: 26-47  Patrick P Dekang L 2002 Discovering Word Senses from Text In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining 2002:613-619  A Mikheev S Finch A Workbench for finding structure in text In Proceedings of the 5th Conference on Applied Natural Language Processing, March 1997: 372-379 8 Alexander D Maedche Ontology Learning for the Semantic Web Kluwer Academic Publishers 2002,l 9 D Faure, C. Nedellec. A corpus-based conceptual clustering method for verb frames and ontology acquisition In LREC workshop on adapting lexical and corpus resources to sublanguages and applications, Granada Spain 1998 IO U Hahn M Romacker Content management in the syndicate system  how technical documents are automatically transformed to text knowledge bases Data  Knowledge Engineering 2000 Ill R Agrawal T Imielinski and A Swami Mining association rules between sets of items in large databases In Proceedings of the ACM SIGMOD Intemational Conference on the Management of Data May 1993: 207-216 12 Ravi T V Chidananda Gowda K An ISODATA clustering procedure for symbolic objects using a distributed genetic algorithm Elsevier Science Inc 1999:659-666 COLINGIACL-98: 768-774 351137-159 137 


Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


Figure 8 Visual interface for Moridou system Search EngineTest Page 0 UI 0 5 5 Keyword plealet Figure 9 Prototype system in hcterogeneous environment 283 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


