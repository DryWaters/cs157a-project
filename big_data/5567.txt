 226 2013 IEEE International Solid-State Circuits Conference ISSCC 2013 / SESSION 12 / NON-VOLATILE MEMORY SOLUTIONS / 12.9 12.9 Unified Solid-State-Storage Architecture with NAND Flash Memory and ReRAM that Tolerates 32◊ Higher BER for Big-Data Applications Shuhei Tanakamaru 1,2 Masafumi Doi 1 Ken Takeuchi 1 1 Chuo University, Tokyo, Japan 2 University of Tokyo, Tokyo, Japan Unified solid-state storage \(USSS niques: reverse-mirroring \(RM ERS and error-masking \(EM ABER memory is enhanced by 32◊, or endurance or data-retention time effectively 
extends by 4.2 or 34◊, respectively. ABER is defined to realize BER after ECC below 10 15  Big-data enterprise storage is escalating demand for SSD, because of SSDês high speed, low power and small form factor. A conventional storage system has four basic levels of hierarchy: server, disk array, SSD, and NAND flash. ECC and redundancy provide reliability. For performance, in addition to reliability, stripe and mirror everything \(S.A.M.E  mirroring ent layers. Since controllers at each level are optimized independently of each other, the resulting system has inefficient duplications and overhead The developed unified storage controller integrates all ECCs and redundancy functions \(Fig 12.9.1 encoder. Next, the page-RAID sequencer generates parity-bits based on XOR calculation. Then, data is written to both the primary and mirrored NAND by the reverse-mirroring unit. During read, data from the primary and mirrored NAND is transferred to the error-reduction synthesizer. Next, the error-location information is transferred from the ECC decoder to the error-masking unit, which keeps a record of previous error-location information, and then the corrected data is read out. The ReRAM and NAND hybrid SSD has been previously pro 
posed to eliminate the data fragmentation [2]. In this work, ReRAM also improves memory system reliability. ReRAMês scalability below 50nm, nonvolatility, speed, page re-writability, and high endurance are needed for storage of the memory systemês mirroring information, parity and error record [3 Figure12.9.2 depicts RM. In program disturb, measured BER in the upper page is smaller than that in the lower page. In data retention, measured BER in the smaller page number \(cells close to the source-line larger page number. The key idea of RM is that the data stored in a higher BER cell of the primary NAND is stored in a lower BER cell of the mirrored NAND. To minimize errors for both program disturb and data retention, RM unit modifies the write address of both lower/upper pages and page number for the mirrored NAND, to minimize BER within the primary and mirrored data pair. For example the data is allocated in Lower Page 0 Upper Page 255 Upper Page 1 LowerPage 254 of primary/mirrored NAND, respectively \(Fig.12.9.2\(a is limited to writing in ascending order from Page 0 
ReRAM temporarily stores the re-ordered write data, and then the mirrored NAND is written. During read either the primary or mirrored NAND can be selected, based on whichever is expected to have fewer errors. As a result, the program-disturb and data-retention BER decreases by 69% and 41%, respectively \(Fig.12.9.2\(b Figure12.9.3 describes ERS. Measurement of program-disturb error shows that in the lower page, 1  0 errors dominate with no observable 0  1 errors; and in the upper page, 0  1 errors occur more than 1  0 errors \(Fig.12.9.3\(a Figure12.9.3\(b page data in the primary NAND is stored in the upper page of the mirrored NAND, which means that the dominant error directions \(1  0 or 0  1 error primary NAND and mirrored NAND are opposite. In order to reduce bit errors and maintain RM compatibility, write data to the mirrored NAND should be bitflipped. During read, primary NAND and mirrored NAND data are compared. If 
mismatch, ERS outputs ç1é or ç0é when the lower or upper page of primary NAND is selected, respectively. Based on the trend for error direction in the upper/lower page, the mismatch can be corrected. False decision may also occur, but rarely, as this would require a simultaneous bit error in both the primary and mirrored NAND. As a result, with RM and ERS, the measured BER decreases by 91%, which corresponds to a 100/\(100-91\=12◊ ABER improvement Figure 12.9.4 provides 2X and 1Xnm NAND measurements. Immediately after writing, program disturb error is dominant. In that case, RM w/ ERS achieves 91% and 85% BER reduction in 2X and 1Xnm NANDês, respectively Fig.12.9.4\(a 2X and 1Xnm NANDês, respectively \(Fig.12.9.4\(b Page-RAID is explained in Fig.12.9.5. RAID-4 like protection is realized without performance degradation; data is protected in the bitline direction in addition to conventional ECC \(Fig.12.9.5\(a in a NAND string and storing the result in ReRAM. Only one parity page is needed per NAND chip, because the unified storage controller programs blocks in round-robin with page-level logical/physical address translation. When an ECC failure is flagged, the victim page can be recovered by calculating XOR of the other existing pages in the NAND block. The block-parity in ReRAM is updated 
every time when a page is written in a NAND block. Repeated parity updating is feasible because high endurance, page over-write is available in ReRAM [2 When the NAND block is almost full, the block-parity data is transferred from the ReRAM to the last page of the NAND block During one block NAND write, the block-parity in ReRAM is overwritten  N page 1 N page is the number of pages in a block. Considering NANDês endurance \(4000 cycles\◊4000=10 6 cycles, which can be realized by using program verification and optimizing the program pulse shapes [5]. If MLC or SLC NAND were to be used for the parity buffer, the write time overhead and required capacity would be unacceptably large \(Fig.12.9.5 \(b parity buffer capacity is less than 0.1% of the storage \(NAND page-RAID, ABER of NAND is increased by 45% \(Fig.12.9.5\(c Figure12.9.6 shows EM. Over time, memory cell V TH decreases, and once an error occurs, it is irrecoverable. In error-recording sequence, the error address 
is marked as ç1é in the error-location table, then stored to ReRAM with compression. In error-masking sequence, errors are masked by comparing the read-data and pre-recorded error-location table \(Fig.12.9.6\(a percent, the number of error flag ç1éês in the error-location table is small. If the NAND BER is 1%, the error-location table can be efficiently compressed to 16 by run-length encoding \(Fig.12.9.6\(b by 67%, corresponding to 3◊ ABER improvement \(Fig.12.9.6\(c Figure12.9.7 depicts the photograph of the measured storage. The USSS with RM, ERS, page-RAID and EM increases ABER by 32◊ compared with the conventional mirroring scheme, which corresponds to an increase in write/erase endurance of 4.2◊, or increase in retention time of 34◊. ABER of NAND is 6.7 assuming 40b correction per 1KB ECC. The presented techniques can also be applied with other ECC schemes such as asymmetric coding [4] and LDPC codes 6 Acknowledgements This work is partially supported by CREST/JST. The authors appreciate T.O Iwasaki, K. Johguchi, S. Hachiya, and K. Miyaji for their support References 1] çOracle and RAID Usage http://www.dba-oracle.com/oracle_tips_raid_usage.htm 2] H. Fujii et al., çx11 Performance Increase, x6.9 Endurance Enhancement 93% Energy Reduction of 3D TSV-Integrated Hybrid ReRAM/MLC NAND SSDs 
by Data Fragmentation Suppression IEEE Symp. VLIS Circuits pp. 134-135 2012 3] ç64Mbit 50nm ReRAM Prototype http://www.elpida.com/ja/news/2012/01-24r.html 4] S. Tanakamaru et al., ç95%-Lower-BER 43%-Lower-Power Intelligent SolidState Drive \(SSD Algorithm ISSCC Dig. Tech. Papers pp. 204-205, 2011 5] K. Higuchi et al., çInvestigation of Verify-Programming Methods to Achieve 10 Million Cycles for 50nm HfO 2 ReRAM IEEE Int. Memory Workshop pp.119122, 2012 6] S. Tanakamaru et al., çOver-10x-Extended-Lifetime, 76%-Reduced-Error Solid-State Drives \(SSDs Recovery Scheme ISSCC Dig. Tech. Papers pp. 424-425, 2012 978-1-4673-4516-3/13/$31.00 ©2013 IEEE 


 227 DIGEST OF TECHNICAL PAPERS ISSCC 2013 / February 19, 2013 / 4:45 PM Figure 12.9.1: Concept of the unified solid-state storage \(USSS the issues of unnecessary duplications and inefficient functions in the conventional scheme caused by the copying of the big-data between many layers Figure 12.9.2: Reverse-mirroring \(RM and re-ordering the large/small page numbers into the mirrored NAND memory, 69% BER reduction is experimentally demonstrated Figure 12.9.3: Error-reduction synthesis \(ERS BER is reduced up to 91 Figure 12.9.5: Page-RAID. XOR parity of the programmed data is calculated vertically, temporarily stored in ReRAM, and finally written to the last page of the block Figure 12.9.6: Error-masking \(EM Figure 12.9.4: Measured results of conventional mirroring, RM w/o ERS, RM w/ ERS for 2X and 1Xnm NAND flash memories Conventional storage  Proposed unified solid-state storage \(USSS   A B C D RAID A B C D A B C D A B E F A R T Z Server 0 Server 1 Server N 1 Host ECC Redundancy Mirror server ECC Redundancy Mirror array/drive NAND flash memory Solid-State Drive Redundant server   ECC Redundancy Spare space SSD controller RAID controller Primary Mirror Unified storage controller Host ReRAM Mirroring buffer Parity buffer Error-location table 1 Reverse-mirroring RM 2 Error-reduction synthesis ERS 3 Page-RAID 4 Error-masking EM NAND A B C NAND A B C Mirroring RAID-1   Primary Mirror    0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0 64 128 192 256 Data number in a block Data retention BER Worst BER for Conv Worst BER for RM 41 Lower page Upper page   SSL BL Upper page Lower page P.D. BER DSL WL 0 WL 1 WL 126 WL 127 Program data Page 0 Data 0  Page 2 Data 2  Page 252 Data 252  Page 254 Data 254  Page 1 Data 1  Page 3 Data 3  Page 253 Data 253  Page 255 Data 255    SSL BL DSL WL 0 WL 1 WL 126 WL 127 Page 254 Data 1  Page 252 Data 3  Page 2 Data 253  Page 0 Data 255  Page 255 Data 0  Page 253 Data 2  Page 3 Data 252  Page 1 Data 254  P.D. BER Lower Upper Lower Upper 2Xnm, 506 hours @ 85 C  Primary NAND  Page D.R BER RM Conv  Mirrored NAND  Page D.R BER Pair Pair Write from Page 0     0 0.004 0.008 0.012 0.016 0 64 128 192 256 Data number in a block Program disturb BER Worst BER for Conv Worst BER for RM 69 Conv RM Mirroring buffer ReRAM Page position inversion    a b   0 0.005 0.01 0.015 0.02 12345678 Program disturb BER 2Xnm L 1 L 2 L 3 L 4 U 1 U 2 U 3 U 4 L Lower page U Upper page   Read primary NAND data Read mirrored NAND data Data match Yes Output primary data No Comparison Decision Most "1"-data is correct Most "0"-data is correct a b Read method Write primary NAND data Write mirrored NAND data Reversemirroring Bit-flip Write request from host Write method Is lower/upper page selected in primary NAND Output 1 Output 0 Upper page Lower page Error direction 1 2 Flip-back ex.\ "1 0 1 1 ex.\ "0 1 0 0 ex.\ "1 0 1 1 ex.\ "0 1 0 1  Error ex.\ "1 0 1 0  W/E: 10k V TH of cells 1 1 0 1 0 0 1 0 0 1 error Upper page 1 0 error Lower page 1 0 error Upper page Conventional mirroring \(Pl ace same data to same page of primary/mirrored NAND Reverse-mirror ing with error-reduct ion synthesis \(RM w/ ERS Reverse-mirror ing without error-reduct ion synthesis \(RM w/o ERS 0 0.002 0.004 0.006 0.008 0.01 0.012 0.014 0246810 Worst program disturb BER of a page in a block Write/erase cycles 0 2k 4k 6k 8k 10k 2Xnm 0 0.05 0.1 0.15 0246810 Worst data retention BER of a page in a block Write/erase cycles 0 2k 4k 6k 8k 10k 2Xnm 506 hours @ 85 C 69 91 Program disturb Data retention 0 0.005 0.01 0.015 0.02 0246 Worst program disturb BER of a page in a block Write/erase cycles 02k4k6k 1Xnm 0 0.02 0.04 0.06 0.08 0.1 0246 Worst data retention BER of a page in a block Write/erase cycles 02k4k6k 1Xnm 194 hours @ 85 C 45 85 Program disturb Data retention 56 41 14 30 a b 10101000 11011100 011010 XOR \(Page-RAID BL direct ion ECC \(Conv WL direct ion     00011110 01101010 ReRAM Parity buffer Write block-parity to NAND  User data  ECC for ReRAM ECC 0.01 0.1 1 10 100 1.00E+03 1.00E+04 1.00E+05 1.00E+06 1.00E+07 1.00E+0 8 10 8 10 7 10 6 10 5 10 4 Required parity buffer ratio to storage     1E 17 1E 15 1E 13 1E 11 1E-09 1.00E-03 1.00E-0 2 10 9 10 11 10 13 10 15 10 17 12345678910 Raw BER of NAND \(x10 3   Market re quirement    45 40bit correct ion per 1KByte + pr oposed page-RAID N page 1 NAND block N page Page number in a block \(e.g 256 WL 0 WL 1 WL 126  WL 127 10 3   ReRAM NAND \(SLC NAND \(MLC x1/100   Acceptable write/erase cycle of parity buffer BER after ECC Block-parity a b c     0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0 100 200 300 400 0 0 1 0 0 1 1 0 Errorlocation flag 1 Data compression Errorlocation data   Write to ReRAM Error-location table   Step1 Step2 Step3 Efficiently comp ressed to 16% by run-length encoding when BER is 1   Read from NAND Error-recor ding sequence Error-mask ing sequence ECC decode Error location is obtained Data output Read from NAND Compress error location see c Write error location to ReRAM Read error location from ReRAM Decompress error location ECC decode Data output Error-elimination a b c Retention time \(hour BER of NAND flash memory Errormasking 67 55 Conv 2Xnm 85 C Errorrecor ding sequence Errormasking sequence NAND page size No error 0 12 


 2013 IEEE International Solid-State Circuits Conference  978-1-4673-4516-3/13/$31.00 ©2013 IEEE  ISSCC 2013 PAPER CONTINUATIONS Figure 12.9.7: Photograph of the measured storage and summary of this work  0.0001 0.001 0.01 1 10 100 1000  Prop. ABER for D.R 1.3 Conv. ABER 0.21 x2 Acceptable W/E cycles x34 Acceptable D.R. time Data retention time \(hour 2Xnm 85 C  x6.2 Data retention BER 1 0.1 0.01 for P.D. error for D.R. error   0.1 1 10 123456 ECC     RM ERS PageRAID EM ECC   PageRAID EM ECC  PageRAID ECC Conv x32 40bit correction per 1KByte ECC  Mirroring Conv w/o Mirroring w/ Mirroring  x6.2 6.7 1.3 0.21  x1.45 x3 x2.6 x12  0.001 0.01 0.1 1 10 0102030  Program disturb BER Prop. ABER for P.D 6.7 Conv. ABER \(0.21 x4.2 Acceptable W/E cycles 2Xnm 0 10k 20k 30k Write/erase cycles  x32 ECC     RM ERS PageRAID EM Acceptable raw BER ABER\ of NAND Extension board for storage NAND flash Storage controller board Extension board for storage NVMs Storage controller chip 


          000\012\000\010\000\020\001Ê\000\007\000\003\000\026\000ñ\000É\000â\000ã\000ê\000â\000\003 000\017\000É\000õ\000á\000î\000\003 000\021\000á\000ô\000\003\000\005\000\014\000\003 000\027\000ë\000ë\000é\000ï\000\003\001¨\000\003 000\025\000á\000í\000ë\000î\000ñ\000ï\000\003 000\025\000á\000É\000é\000\003\000ñ\000ã\000è\000á\000\003\000Ü\000É\000ñ\000É\000\003 000à\000á\000á\000Ü\000ï\000\003 000\012\000\010\000\020\001Ê\000\007\000\003\000\017\000ë\000â\000ã\000Ö\000É\000é\000\003 000é\000É\000õ\000á\000î\000\003 000\011\000É\000Ö\000ñ\000ë\000î\000õ\000\003\000\026\000ë\000ó\000î\000Ö\000á\000\003\000\026\000õ\000ï\000ñ\000á\000è\000ï\000\003  000\010\000\014\000\003\000é\000ë\000â\000ï\001·\000\003\000Ñ\000ã\000ñ\000è\000É\000í\000ï\001·\000\003\000ï\000õ\000ï\000ñ\000á\000è\000\003\000é\000ë\000â\000ï\001·\000\003\000\003 000ñ\000á\000ï\000ñ\000\003\000Ü\000É\000ñ\000É\000\003  Current State Future State Additions and Considerations 000\030\000ê\000ñ\000É\000í\000í\000á\000Ü\000\003\000\011\000É\000Ö\000ñ\000ë\000î\000õ\000\003\000\026\000ë\000ó\000î\000Ö\000á\000\003\000\026\000õ\000ï\000ñ\000á\000è\000ï\000\003 000\025\000á\000É\000é\000\003\000ñ\000ã\000è\000á\000\003\000É\000ê\000Ü\000\003 000É\000ó\000ñ\000ë\000è\000É\000ñ\000á\000Ü\000\003 000É\000ê\000É\000é\000õ\000ñ\000ã\000Ö\000ï\000\003 000\010\000ò\000É\000é\000ó\000É\000ñ\000ã\000ê\000â\000\003\000Ö\000ë\000è\000í\000î\000á\000ï\000ï\000ã\000ë\000ê\001·\000\003 000\014\000ê\001Ê\000è\000á\000è\000ë\000î\000õ\000\003\000É\000ê\000É\000é\000õ\000ï\000ã\000ï\000\003\000É\000ê\000Ü\000\003 000ä\000É\000î\000Ü\000ô\000É\000î\000á\000\003\000ï\000ë\000é\000ó\000ñ\000ã\000ë\000ê\000ï\000\003 000\021\000á\000ô\000\003 000\004\000ê\000É\000é\000õ\000ï\000ã\000ï\000\003 000\027\000ë\000ë\000é\000ï\000\003  000\014\000ê\000ñ\000á\000â\000î\000É\000ñ\000á\000Ü\000\003\000\014\000Ü\000á\000ê\000ñ\000ã\000ñ\000õ\000\003 000\020\000â\000è\000ñ\000\003 Fig. 2. The GEM-D Model 223 ASMC 2013 


 B  The Hadoop Derivative In order to scale to the petabytes of unstructured data \(not modeled in tables with well defined rows and columns Google introduced the Map Reduce paradigm that later was made available in the Hadoop Open Source project.  This has been extended with tools like HBase, Pig, Hive, etc. which improve the usability and reduce the complexity of the Map Reduce coding.  Using commodity hardware as a foundation Hadoop provides a layer of software that spans the entire grid turning it into a single system. Hadoop based solutions are provided by companies like Cloudera, Hortonworks and MapR C  Massive In Memory Database While the previous two models offer ways to scale to support larger amounts of data, a new model attempts to make larger amounts of data available in real time by placing the database in a highly available cluster of servers that keep all the data in memory.   This eliminates the need for indexing and I/O on the traditional drive storage is completely eliminated Some systems, such as SAP Hana, also flush data to disk for recovery scenarios. It is by far the fastest solution for critical structured data but comes with price tag D  Solid State Disk \(SSD There is yet another improvement to any of the other Big Data solutions or even the traditional model that addresses the speed of access. The hard drive storage itself can be moved to solid state.  These ìflash drivesî can be leveraged to all or just a subset of the data. SSD based solutions can be used with Hadoop systems to address the big data problems. Companies like Fusion-io, NetApp, EMC, etc. provide SSD based solutions  V  W HERE D O WE GO FROM H ERE  The path for the next level of data management is not yet defined.  Reporting and analysis has already moved beyond the single system or data set.  The factory data volumes are expanding rapidly.  The reader can see from Fig. 3 that there are significant tradeoffs with using these solutions.  The appliance covers a subset of the traditional RDBMS whereas the Hadoop paradigm covers new territory A  A call for action We appeal to our vendors to work together to leverage Big Data strategies within their product offerings.  Several things can be done to help including   Provide schemas that are portable and not locked into specific RDBS providers   Provide Map-Reduce stubs that can be grouped together with other such routines.  This applies to tool vendors log files, test file output and otherwise untapped data today   Leverage SOA architectures using interchangeable message buses for communication   Work together to offer a standard nomenclature of objects to better tie system data together   Expect to work with new analytic tools for analysis and reporting not requiring proprietary reporting platforms and perhaps providing components or models for BI solutions B  Big Data Environment We are looking to provide vendors the opportunity to test their Big Data platforms in our lab.  Our state of the art test environment has been an environment where vendors have introduced hardware and software solutions and see how they work together with other factory applications   Fig. 3. Comparison chart of Big Data technologies  VI  C ONCLUSION  We are entering a new realm of data management Solutions will perhaps take several forms.  As the complexity of our needs scale, we need our suppliers to move away from stand alone proprietary infrastructure, reporting and offer plugand-play components A CKNOWLEDGMENT  The authors would like to thank the Data Integration and IT DBA teams at GLOBALFOUNDRIES in deploying GEM-D and making the current solutions a reality    224 ASMC 2013 


TABLE I  T ECHNOLOGY P ROS AND C ONS  Technology Pros Cons Appliance RDBMS    Very mature. Long history of successful installation   Can be used by multiple user types, from Business users using reporting tools through SQL novices and expert users   Vendors like Teradata support data volume in petabyes   Custom hardware accelerates query and transformations   Fault Tolerance is only at transaction level cannot survive node failure   Supports only structured data   Homogenous hardware - all nodes in the installation must be the same   Disk based data storage. Limited real-time analysis capabilities compared to in-memory technologies   OLTP and OLAP layers are separate   More expensive compared to open source solutions Hadoop    Data volumes in Petabytes   Open source   Commodity hardware and inexpensive   Fault tolerant, designed to survive multiple node failures   Supports both structured and unstructured data. Data can be operated on in native format   Heterogeneous hardware, the nodes in the installation can be different   Currently it only has batch processing capabilities. Real-time data processing is still under works   Requires programming skills to work with Smaller pool of individuals capable of performing these tasks   Relatively new technology. The source is continuously under development with new features being added In Memory DB   In-memory. Real-time analysis   Single foundation for OLTP+OLAP   No need for Indexing   Data volume in terabytes. Doesn't scale to petabyte size currently   Very expensive. Enterprise level toolset Proprietary hardware and software   Limited fault tolerance capabilities SSD   Very high performance   No spinning Disks   Consumes less power and space   Leverage existing software architecture and data base systems, improving virtually any disk-based solution   Very expensive. About ten times that of a standard hard drive   Limited no. of writes, thus limiting the life of the device   Cannot scale out   Relatively less reliable    R EFERENCES  1  J Manyika, M Chui, B Brown, J Bughin, R Dobbs, C Roxburgh, A H Byers, McKinsey Global Institute Publication "Big data: The next frontier for innovation, competition, and productivity 2  J G Kobielus, ìThe Forrester Waveô: Enterprise Hadoop Solutions Q1 2012 3  A McClean, R. C. ConceiÁ„o, M OíHalloran, "A Comparison of MapReduce and Parallel Database Management Systems ICONS 2013, The Eighth International Conference on Systems  4  Big Data Now: 2012 Edition" OíReilly Media, Inc  5  http://blogs.gartner.com/merv-adrian/2013/02/10/hadoop-and-di-aplatform-is-not-a-solution  6  http://www.saphana.com/community/blogs/blog/2012/04/30/whatoracle-wont-tell-you-about-sap-hana  7  http://www.dbms2.com/2012/08/20/in-memory-hybrid-memorycentric  8  http://hadoop.apache.org  9  http://en.wikipedia.org/wiki/MapReduce    http://en.wikipedia.org/wiki/Big_data    http://www.datanami.com/datanami/2012-0213/big_data_and_the_ssd_mystique.html  225 ASMC 2013 


Michael Armbrust, Armando Fox and Rean Griffith et al. A view of cloud computing [J  C o m m un i c a t i ons of th e A C M Vol 53 No 4, A p ri l 2010 2 Patel Patel, Ranabahu Ajith and Sheth Amit. Service Level Agreement in Cloud Computing [C I n C l ou d W o rk s h op a t OOPSL A  2 0 0 9   3 Mell P., Grance T. The NIST De\002nition of Cloud Computing. National Institute of Standards and Technology, Information Technology Laboratory, USA 4 D. Hyde. A Survey on the Security of Virtual Machines. Dept. of Comp Science, Washington Univ., 2009 5 Arvind Seshadri, Mark Luk, Ning Qu, Adrian Perrig: SecVisor: a tiny hypervisor to provide lifetime kernel code integrity for commodity OSes C S O S P 2 0 07 33 53 5 0   6 Fengzhe Zhang, Jin Chen, Haibo Chen, Binyu Zang CloudVisor retrofitting protection of virtual machines in multi-tenant cloud with nested virtualization [C S O S P 2 0 11 20 32 1 6  7 Maurice Gagnaire, Felipe Diaz, et al. Downtime statistics of current cloud solutions. IWGCR 2012 8 Qiang Guan, Song Fu. Auto-AID: A data mining framework for autonomic anomaly identification in networked computer systems C  I P CCC 2 0 1 0  7 3 8 0  9 Ripal Nathuji, Aman Kansal and Alireza Ghaffarkhah. Q-clouds managing performance interference effects for QoS-aware clouds [C  EuroSys 2010:237-250  Yongmin Tan, Hiep Nguyen, Zhiming Shen, Xiaohui Gu, Chitra Venkatramani, Deepak Rajan. PREPARE: Predictive Performance Anomaly Prevention for Virtualized Cloud Systems [C   I C DC S 2012:285-294  Yongmin Tan, Xiaohui Gu, Haixun Wang. ALERT:Adaptive system anomaly prediction for large-scale hosting infrastructures [C  PODC 2010: 173-182  Qiang Guan, Ziming Zhang, Song Fu. Ensemble of Bayesian Predictors and Decision Trees for Proactive Failure Management in Cloud Computing Systems [J J C M 7 1  5 2 61 2 01 2    Qiang Guan, Chi-Chen Chiu, Ziming Zhang, Song Fu. Efficient and Accurate Anomaly Identification Using Reduced Metric Space in Utility Clouds [C A S 20 1 2  2 0721 6   Husanbir S. Pannu, Jianguo Liu, Song Fu. AAD: Adaptive Anomaly Detection System for Cloud Computing Infrastructures [C   SR DS  2012:396-397  Song Fu. Performance Metric Selection for Autonomic Anomaly Detection on Cloud Computing Systems [C L O B ECO M 2 0 11 15   Daniel Dean, Hiep Nguyen, and Xiaohui Gu. UBL: Unsupervised behavior learning for predicting performance anomalies in virtualized cloud systems  A C M I C A C  2 012   Chengwei Wang, Vanish Talwar, Karsten Schwan, Parthasarathy Ranganathan. Online detection of utility cloud anomalies using metric distributions [C   NOM S 20 10 96 103   Stefano Ferretti, Vittorio Ghini, Fabio Panzieri, Michele Pellegrini, Elisa Turrini: QoS-Aware Clouds [C  I E E E CL OUD 2010:321-328  Deepayan Chakrabarti, Ravi Kumar, Andrew Tomkins  Evolutionary clustering [C   A C M SI GKDD I n t e r n a t i ona l C o n f e r e n c e on K n ow l e d g e  Discovery & Data Mining \(KDD\ 2006:554-560  Yun Chi, Xiaodan Song, Dengyong Zhou, Koji Hino, Belle L Tseng On evolutionary spectral clustering [J  A C M T r ans act io ns o n  Knowledge Discovery from Data, Vol. 3, No. 4, 2009  Yun Chi, Xiaodan Song, Dengyong Zhou, et al. Evolutionary spectral clustering by incorporating temporal smoothness [C  A C M SI GKDD  International Conference on Knowledge Discovery & Data Mining KDD\, 2007:153-162  Ulrike Luxburg. A tutorial on spectral clustering [J   S t at ist ics a n d  Computing \(SAC\\(4\7    
 
occasion, the labels were determined based on the upper SLO Thus, other attributes might interfere with its precision. We would improve it in the future works VI C ONCLUSION  In this paper, we proposed a framework named eCAD from an evolutionary view to identify anomalies in the IaaS cloud. To illustrate our framework, we established a VICCI cloud equipped with HADOOP benchmarks. Our experiment reveals that the evolutionary clustering method can identify cloud anomalies more precisely than its counterparts especially for the traditional static clustering approach Moreover, our framework can also infer the reasons for the above anomalies As immediate future work, we plan to simulate more scenarios with other injected errors. With the accumulation of the anomalies, they might come into being a whole anomaly cluster, which would also be solved within our following researches R EFERENCES  1 
                      
334 
334 


14.0 12.0 10.0 8.0 6.0 4.0 2.0 0.0 
021\017\013\003\021#\012\004 020\021\017\004 022\017\007\023 023\005 017\011\012\013\021\032 017\005\004\007\017 023\007\012\004\021\032\005\012\021 Execution Time Ratio of Primary-backup Paradigms Comparing to HDFS Mapreduce Jobs Running on OAMS and Others 
Figure 6 Overall measurement V CONCLUSION AND FUTURE WORK In this paper a highly metadata service OAMS was proposed for big data storage Different from traditional primary-backup paradigms OAMS depends on more than one standby to take over the active in cluster le system It is based on the built-in shared storage pool and employs a series of protocols to tolerate multiple points of failures OAMS achieves an automatic state transition in the form of hot standby Besides it supports server self-recovery and dynamical addition for standbys at runtime Measurements show that OAMS can obviously improve the system reliability while keeping the performance with little degradation In the future we intend to optimize and expand the policy such as supporting failover in the cluster le system with multiple metadata servers A CKNOWLEDGMENT This work is supported by the National High-Tech Research and Development Program of China under grant numbered 2011AA01A203 2013AA013204 also supported by the National HeGaoJi Key Project under grant numbered 2013ZX01039-002-001-001 and Strategic Priority Research Program of the Chinese Academy of Sciences under grant numbered XDA06030200 R EFERENCES  J Dean and S Ghema w at Mapreduce Simpliìed data processing on large clusters in 
BackupNode HA with NFS HA with QJM OAMS with 1A2S OAMS with 1A1S1J OAMS with 1A2J 
Sixth Symposium on Operating System Design and Implementation OSDI 04 Seventh Symposium on Operating System Design and Implementation OSDI 06 19th Symposium on Operating Systems Principles SOSP 03 26th IEEE Transactions on Computing Symposium on Mass Storage Systems and Technologies Mass Storage Systems and Technologies in Cooperation with the 17th IEEE Symposium on Mass Storage Systems ACM Computing Surveys Proc of the 2011 ACM SIGMOD International Conference on Management of data Proc of the Summer USENIX conference 13th Symposium on Operating Systems Principles Research report Systems Research Center Digital Equipment Corporation Markov Chains Principles of Distributed Database Systems 
 San Francisco USA Dec 2004 pp 137Ö150  F  Chang J Dean S Ghema w at W  C Hsieh D A Wallach M Burrows T Chandra A Fikes and R E Gruber Bigtable A distributed storage system for structured data in  Seattle USA Nov 2006 pp 205Ö218  S Ghema w at H Gobiof f and S T  Leung The Google le system in  New York USA Oct 2003 pp 29Ö43  K Shv achk o H K uang S Radia and R Chansler  The Hadoop distributed le system in  Incline Village USA May 2010 pp 1Ö10  A Barry  J Brasso w  R Cattelan A Manthei E Nygaard S V Oort D Teigland M Tilstra M OKeefe G Erickson and M Agarwal Implementing journaling in a Linux shared disk le system in  Maryland USA Mar 2000 pp 351Ö378  T  Haerder and A Reuter  Principles of transaction-oriented database recovery  vol 15 no 4 pp 287Ö317 1983  F  Haas P  Reisner  and L Ellenber g The DRBD user s guide LINBIT Information Technologies GmbH 2009  D Borthakur  J S Sarma J Gray  K Muthukkaruppan N Spiegelberg H Kuang K Ranganathan D Molkov A Menon S Rash R Schmidt and A Aiyer Apache Hadoop goes realtime at Facebook in  Athens Greece Jun 2011 pp 1071Ö1080  R Sandber g D Goldber g S Kleiman D W alsh and B Lyon Design and implementation of the Sun network lesystem in  Portland USA Jun 1985 pp 119Ö130  Cloudera homepage Online A v ailable http://www.cloudera.com  Apache hadoop Online A v ailable http://hadoop.apache.or g  Apache BookK eeper  Online A v ailable http://zookeeper.apache.org/bookkeeper  W  Lin M Y ang L Zhang and L Zhou P aciìca Replication in log-based distributed storage systems Technical Report MSR-TR-2008-25 Microsoft Research Tech Rep 2008  B Lisk o v  S Ghema w at R Gruber  P  Johnson L Shrira and M Williams Replication in the Harp le system in  California USA Oct 1991 pp 226Ö238  G Sw art A Birrell A Hisgen and T  Mann  A v ailability in the Echo le system in  Citeseer 1993  J R Norris  Cambridge University Press 1998  L Lamport The part-time parliament  Research Report 49 Systems Research Center Digital Equipment Corporation Tech Rep 1989  M T   Ozsu and P Valduriez  Springer 2011  Apache ZooK eeper  Online A v ailable http://zookeeper.apache.org 
since OAMS has highly improved the reliability of metadata service while having little effects on performance        
1294 
1294 


607 


608 


  11 that it will be able to meet all of the Van Allen Probes communications goals with its intended ground segments A CKNOWLEDGEMENTS  This work was performed with the support of the Radiation Belt Storm Probes mission under NASA\222s Living with a Star program. The authors would like to thank Rick Fitzgerald and Kim Cooper, Van Allen Probes project managers at JHU/APL for supporting this work. There are many at JHU/APL who contributed to the development and verification of the RF system. Significant technical contributions were made by: Christopher Haskins, Bob Wallis, Matthew Angert, Laurel Funk, Joe Sheehi, Wesley Millard, Norman Adams, Lloyd Ellis, Sheng Cheng, John Daniels, Phillip Huang, Avi Sharma, Carl Herrmann, David Jones, Brian Bubnash, Melanie Bell, Horace Malcom Michael Pavlick, Mark Bernacik, Christopher Deboy, Bob Bokulic, Sharon Ling, Albert Hong, Erik Hohlfeld, Judy Bitman, William Dove and Tony Garcia. Significant contributions were also made by the USN and TDRSS compatibility test teams  R EFERENCES  1 eck D. G.; Mau k  B  H.; Greb o w sk y  J  M.; Fo x  N J, \223The Living With a Star Radiation Belt Storm Probes Mission and Related Missions of Opportunity 224 American Geophysical Union, Fall Meeting 2006   h o rs k i y  A Y., Mauk B. H., Fox N. J Sibeck D G., Grebowsky, J. M., \223Radiation belt storm probes Resolving fundamental physics with practical consequences,\224 Journal of Atmospheric and SolarTerrestrial Physics Vol. 73, Issues 11-12, July 2011 Pages 1417-1424   S. Bu s h m a n M. Bu tler, R C o n d e, K. Fretz, C  Herrmann, A. Hill, R. Maurer, R. Nichols, G. Ottman M. Reid, G. Rogers, D. Srinivasan, J. Troll, B. Williams 223Radiation Belt Storm Probe Spacecraft and Impact of Environment on Spacecraft Design,\224 Proceedings of the 2012 IEEE Aerospace Conference, Big Sky Montana USA, March 3-10, 2012   opelan d D.J DeB o y C  C R o y s ter, D.W., Dov e  W.C., Srinivasan, D.K,. Bruzzi, J.R., Garcia, A., "The APL 18.3m station upgrade and its application to lunar missions," Aerospace Conference, 2010 IEEE , vol., no pp.1-10, 6-13 March 2010    Figure 10. FER/BER performance for all downlink modes for RF GSE, SCF, USN, and TDRSS 


  12  iv as a n D. K., A r ti s  D  A Bak er, R  B., Stil w e ll, R   K., Wallis, R. E., \223RF Communications Subsystem for the Radiation Belt Storm Probes,\224  Acta Astronautica vol 65, issue 11-12, December 2009, Pages 1639-1649   k i n s  C B., Mi llard, W P 223 M u l t i Ban d  So f t w a re Defined Radio for Spaceborne Communications Navigation, Radio Science, and Sensors,\224 2010 IEEE Aerospace Conference, March 2010  k i n s  C B., Mi llard, W P A d a m s  N. H Sri n i v a s a n  D. K., Angert, M. P., \223The Frontier Software-Defined Radio: Mission-Enabling, Multi-Band, Low-Power Performance,\224 61st  International Astronautical Congress IAC-10.B2.5.11, October 2011 8  Crowne, M.J.,  Haskins, C. B., Wallis, R. E.,  Royster D.W, \223Demonstrating TRL-6 on the JHU/APL Frontier Radio for the Radiation Belt Storm Probe mission,\224 2011 IEEE Aerospace Conference, March 2011  o ckw ood, M. K K i n n i s o n  J., F o x  N C o n d e, R  Driesman, A., \223Solar Probe Plus Mission Definition,\224 63rd  International Astronautical Congress, IAC 12.A3.5.2, October 2012   i t m a n J  223An I n D ept h  L o o k at t h e R a dio Freq u e n c y    Ground Support Equipment for the Radiation Belt Storm  Probes Mission,\223 IEEE Autotestcon, 2011, September 2011  d a m s  N.H., Bi t m a n J C opela n d D. J Sri n ivas a n  D  K.,  Garcia. A., \223RF Interference at Ground Stations Located in Populated Areas,\224 2013 IEEE Aerospace Conference, March 2013  B IOGRAPHY  Matthew J. Crowne is a member of the Senior Professional Staff of the RF Engineering group in JHU/APL\222s Space Department. He received his B.S from Johns Hopkins University in 2000 and his M.S. from the same university in 2009, both in electrical engineering Matthew joined JHU/APL in 2007 where he has been working on the development of radios for spaceflight communications systems. Prior to joining JHU/APL, he worked for Integrated Defense Systems Inc., where he developed solid state power amplifiers for electronic warfare and communication systems. Matthew was the integration and test lead for the Van Allen Probes RF communication system and is currently working on the Solar Probe Plus mission   Dipak K. Srinivasan is the supervisor of the RF Systems Engineering Section in the JHU/APL Space Department. He received his B.S. and M.Eng. in electrical engineering in 1999 and 2000 in electrical engineering from Cornell University, and an M.S. in applied physics from The Johns Hopkins University in 2003. Dipak joined the APL Space Department in 2000, where he has served as the lead RF Integration and Test Engineer for the CONTOUR and MESSENGER spacecraft and lead mission system verification engineer for the New Horizons project. He is currently the Lead RF Telecommunications Systems Engineer for the MESSENGER and Van Allen Probes missions and chairs technical sessions at the annual International Astronautical Congress  Darryl W. Royster is a member of the Senior Professional Staff in the RF Engineering Group at JHU/APL.  He led compatibility testing for the Van Allen Probes, STEREO, and MESSENGER missions.  Previously he was the System Engineer for the Satellite Communications Facility at JHU/ APL and the lead RF Integration and Test Engineer for the STEREO spacecraft.  Prior to joining the JHI/APL Space Department in 2001, Mr. Royster designed cellular and land mobile radio products for Ericsson, GE and Motorola.  He received his B.S. and M.S. in electrical engineering from Virginia Polytechnic Institute and State University in 1982 and 1984, respectively  Gregory L. Weaver joined the Senior Professional Staff of JHU/APL in 2003 and works within the RF Engineering Group of the Space Department.  He is a technologist with extensive background in the technical and business aspects of the frequency control industry and has held positions as a senior design engineer, technical manager and marketing strategist over a 25 year career history, including vice president positions with Bliley Technologies Inc. and the former Piezo Crystal Company. He received his M.S in Technology Management from the University of Pennsylvania in 1993 and his B.S. in Physics from Dickinson College in 1982.  He is a licensed professional engineer in the state of Pennsylvania, member of the IEEE and the UFFC Societ y.  He has contributed to the technical proceedings of the IEEE International Frequency Control Symposium, Precise Time and Time Interval Systems and Application Meeting and the European Frequency and Time Forum   


  13 Daniel Matlin is an Associate Professional Staff at JHU/APL and a member of the RF engineering group in the Space department.  He went through a dual Bachelors/Masters program at Johns Hopkins University graduating with his Bachelor of Science in Electrical Engineering in 2008 and his Masters of Science in Engineering from the Electrical Engineering department in 2009.  As a student he specialized in RF systems design.  Mr. Matlin started at the JHU/APL in February of 2010 and in his short time with the lab has been privileged to work on various tasks supporting the RBSP program, including supporting a successful launch and early operations.  Mr. Matlin assisted in the qualification testing for the flight DSP slices as well as the integrated flight transceivers.  He also carried out electrical testing and flight qualification of the newly designed Hypertronics stacking connectors as well as components and cables used for the RF subsystem  Nelli Mosavi is an EMC and RF Engineer in the JHU/APL Space Department, RF Systems Engineering section. She received a B.S. degree in Electrical Engineering from Oakland University Michigan in 2004 and an M.S. in Electrical Engineering from The Johns Hopkins University in 2010. She is currently working toward her Ph.D. at the University of Maryland Baltimore County. She joined APL in 2009 and has since been working on RF and EME issues on the Van Allen Probes mission. Nelli previously worked for SENTEL Corporation, General Motors, DENSO International, and Molex Automotive   


APPENDIX 3ñ RESULTS \(SEM I-PROFESSIONAL DSLRS     Run by TFDEA add-in ver 2.1 Frontier Type Orientation 2nd Goal Return to Scale Avg RoC Frontier Year MAD Dynamic OO Max CRS 1.124802 2008 1.394531 Input\(s Output\(s SOA products at Release SOA products on Frontier RoC contributors Release before forecast Release after forecast 22166527 DMU Name Date Efficiency_R Efficiency_F Effective Date Rate of Change Forecasted Date 1 Nikon D100 2002 1 1.66666667 2007.000000 1.107566 2 Olympus E1 2003 1 1.666666667 2007.000000 1.136219 3 Pentax *ist D 2003 1 1.358024691 2007.000000 1.079511 4 Nikon D20 0 2005 1 1.2 2007.000000 1.095445 5 Canon EOS 5D 2005 1 1.664796311 2007.730028 1.205269 6 Pentax K10D 2006 1 1 2006.000000  7Nikon D30 0 2007 1 1 2007.000000  8 Olympus E3 2007 1 1 2007.000000  9 Sony Alpha DSLR A70 0 2007 1 1 2007.000000  1 0 Nikon D70 0 2008 1.46 1.46 2007.000000  11 Canon EOS 5D Mark II 2008 1.065464119 1.065464119 2008.000000  12 Sony Alpha DSLR A90 0 2008 1 1 2008.000000  13 Olympus E3 0 2008 1.02 1.02 2007.000000  1 4 Pentax K20D 2008 1 1 2008.000000  15 Nikon D300s 2009 1.142857143 0.874450785 2007.000000  2008.140742 16 Canon EOS 7D 2009 1 0.754166667 2007.000000  2009.399022 17 Sony Alpha DSLR A85 0 2009 1 0.774820627 2008.000000  2010.169290 18 Pentax K-7 2009 1 0.772738276 2006.503130  2008.695302 19 Olympus E5 201 0 1.466133763 1.173333333 2007.000000   2 0 Pentax K-5 201 0 1.009024674 0.776190476 2007.000000  2009.15427 0 21 Nikon D80 0 2012 1 0.686950618 2008.000000  2011.192776 22 Canon EOS 5D Mark III 2012 1.115010291 0.930769231 2007.502755  2008.112786 23 Pentax K-5 II 2012 1 0.632075669 2006.839705  2010.740375 24 Sony Alpha SLT A99 2012 1.009662059 0.854117647 2007.640496  2008.981286 Results 2129 2013 Proceedings of PICMET '13: Technology Management for Emerging Technologies 


