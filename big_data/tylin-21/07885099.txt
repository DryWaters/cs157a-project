IEEE TRANSACTIONS ON IMAGE PROCESSING VOL 26 NO 6 JUNE 2017 2721 Multi-Scale Multi-Feature Context Modeling for Scene Recognition in the Semantic Manifold Xinhang Song Student Member IEEE  Shuqiang Jiang Senior Member IEEE  and Luis Herranz Member IEEE Abstract  Before the big data era scene recognition was often approached with two-step inference using localized intermediate representations objects topics and so on One of such approaches is the semantic manifold SM in which patches and images are modeled as points in a semantic probability simplex Patch models are learned resorting to weak supervision via image labels which leads to the problem of scene categories co-occurring in this semantic space Fortunately each category has its own co-occurrence patterns that are consistent across the 
images in that category Thus discovering and modeling these patterns are critical to improve the recognition performance in this representation Since the emergence of large data sets such as ImageNet and Places these approaches have been relegated in favor of the much more powerful convolutional neural networks CNNs which can automatically learn multilayered representations from the data In this paper we address many limitations of the original SM approach and related works We propose discriminative patch representations using neural networks and further propose a hybrid architecture in which the semantic manifold is built on top of multiscale CNNs Both representations can be computed signicantly faster than the Gaussian mixture models of the original SM To combine multiple scales spatial relations and multiple features we formulate rich 
context models using Markov random elds To solve the optimization problem we analyze global and local approaches where a topñdown hierarchical algorithm has the best performance Experimental results show that exploiting different types of contextual relations jointly consistently improves the recognition accuracy Index Terms  Scene recognition semantic manifold semantic multinomial multi-scale context model Markov random eld convolutional neural networks I I NTRODUCTION S CENES e.g coast mountain ofﬁce  are abstract semantic entities composed of many less abstract and localized ones e.g sky rock table car  Accurate scene recognition Manuscript received July 21 2016 revised January 31 2017 accepted March 7 2017 Date of publication Mar ch 22 2017 date of current version 
April 11 2017 This work was supported in part by the National Natural Science Foundation of China under Grant 61532018 and Grant 61322212 in part by the Beijing Municipal Co mmission of Science and Technology under Grant D161100001816001 in part by the Lenovo Outstanding Young Scientists Program and in part by Na tional Program for Special Support of Eminent Professionals and National Program for Support of Top-notch Young Professionals Corresponding author Shuqiang Jiang X Song and S Jiang are with Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences CAS Institute of Computing Technology CAS Beijing 100190 China are also with University of Chinese Academy of Sciences Beijing 100049 China e-mail xinhang.song@vipl.ict.ac.cn s huqiang.jiang@vipl.ict.ac.cn L Herranz is with the Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences CAS Institute of Computing Technology CAS Beijing 100190 China e-mail luis.herranz@vipl.ict.ac.cn Color versions of one or more of the gures in this paper are available 
online at http://ieeexplore.ieee.org Digital Object Identiﬁer 10.1109/TIP.2017.2686017 
Fig 1 Scene category co-occurren ces in scene recognition a images from the highway top row and tallbuilding bottom row categories of the 15 scenes dataset b regions labeled with a mid-level vocabulary c patches labeled and their corresponding semantic multinomial resulting from weakly-supervised learning with scene labels Note how the isolated patches have similar content e.g road walls cars which introduces ambiguity and uncertainty in the estimated scene category shown in the semantic multinomial descriptor 
is challenging because it implies reasoning from low-level visual features to high-level scene categories While scene categories can be modeled directly using descriptors speciﬁc for scenes e.g GIST  C ENTR IS T  2 t h i s l a r g e s emant i c gap makes difﬁcult to discriminate between a large number of scene categories A more common approach is to split the reasoning in two or more steps with smaller semantic gaps e.g features to objects objects to scenes   4 T hus  a local intermediate representation is deﬁne d over a vocabulary of mid-level concepts or themes Figure 1a-b shows an example of two images and their regions with the corresponding mid-level concepts To avoid explicitly annotating regions with mid-level labels some approaches use latent representations such as topic models  a nd di s c ri mi nat i v e part s  8]–[10 b ut t h e challenge is now to discover them while learning both models jointly The semantic manifold SM 11 u s es an in ter m ed iate 
representation based on the semantic multinomial SMN  in which patches are also rep resented in terms of scene categories i.e patches are no longer represented with mid-level concepts such as sky  road  trees  but scenes such as coast  street  see Figure 1c Patch models are learned in a weakly-supervised way using only image labels i.e the scene category and thus bypassing the problems of midlevel annotations and discovering latent representations However this weak supervision creates a speciﬁc problem of 1057-7149  2017 I EEE Personal u se is perm itted but republication/redistri bution requires IEEE permission See http://www.ieee.org/publications_standards/publications/rights/index.html for more information 


2722 IEEE TRANSACTIONS ON IMAGE PROCESSING VOL 26 NO 6 JUNE 2017 ambiguity which we will refer to as scene category co-occurrences 1  F ort unat e l y  t hes e cat e gory co-occurrences appear in patterns that are consistent across the images in the same category so they can be modeled and separated from accidental co-o ccurrences i.e noise in the semantic representation with an additional classiﬁer The way of aggregating patch SMNs into image SMNs is also tricky since it should emphasize consistent category co-occurrence patterns so they can be modeled robustly  13 However the SM framework still has several limitations In particular previous works   13 onl y m odel g l obal co-occurrences in image SMNs while category co-occurrence patterns also appear locally see Figure 1c In this paper we focus on local co-occurrence p atterns in patch SMNs Our motivation is to exploit in an unsupervised way contextual relations to reinforce consistent co-occurrence patterns and remove accidental ones i.e noise In this way the classiﬁer can learn a more robust model fro m cleaner SMN descriptors A second limitation of current SM frameworks is the SMN representation itself based on GMMs Patch SMN models are learned independently for each category which makes them not very discriminative At the same time they do not scale well to datasets with many categories Here we propose using discriminative SMN representations based on neural networks which are learned for all categories jointly and since they share intermediate layers they can scale much more easily to large number of categories In particular the contributions of this paper are  Analysis of the limitations of the original semantic manifold framework and its variants Section III  Neural network-based discriminative SMNs to address the problem of efﬁciency and lack of discriminative capability of the original GMM-SMNs Section IV  Context models to exploit spatial multi-feature and multi-scale relations between SMNs with the objective of emphasizing consistent scene co-occurrence patterns and removing accidental ones We formulate it in a Markov random eld MRF framework analyzing different context models and ways to solve the optimization problem Section V The research in this paper is an extension of our previous work 14  w h i ch f o cu ses o n e x p l o itin g s p a tial a n d m u ltifeature context on GMM-SMNs However in this paper we signiﬁcantly extend that framework including discriminative SMNs both shallow and deep architectures multi-scale context and a hierarchical message passing algorithm to solve the optimization problem We also include more detailed analysis of the limitations of previous works and extended experiments that achieve state-of-the-art scene recognition performance II R ELATED W ORK For convenience we include Table I with several abbreviations used across the paper and the related references 1 In t he authors u s e the t erm contextual co-occurrences to refer to consistent and thus desirable co-occu rrence patterns Here we refer to them as scene category co-occurrences to emphasize that they are high-level categories rather than low or mid-level co-occurrences We also want to avoid confusion with other type of context such as the spatial neighborhood multi-scale or inter-feature relations TABLE I A BBREVIATIONS U SED IN THE P APER   A Intermediate Representations for Scene Recognition A number of methods include mid-level representations using explicit classiﬁers Vogel and Schiele  propos ed a vocabulary with nine local concep ts to model natural scenes Object bank  22 i s a s emant i c repres ent a t i o n t hat e ncodes the response at different spatial locations of a number of pretrained object classiﬁers Classemes  are i nt ermedi ate semantic representations based on a set of 2659 basis classes These methods require training these intermediate classiﬁers explicitly with the corresponding mid-level annotations and often exploit large amounts of external training data e.g ImageNet web images to learn these mid-level classiﬁers Latent topic models are also a popular approach in which mid-level concepts are unknown and need to be discovered They are often modeled using variants of latent Dirichlet allocation LDA   25 H o w e v er  m os t L D A ha v e been shown to capture irrelevant general regularities rather than the semantic regularities of interest due to poor supervision  Spatial context can be included to model the global layout and enforce local coherence in the topics 26 R ecently  Li and Guo propos ed a p at ch-bas ed l a t e nt frame w o rk which jointly learns the contextual representation and the classiﬁcation model Most latent topic models are generative and usually do not scale well to large scale datasets More recent variants exploit discriminative parts which are unknown and discovered during learning   28 A l t e rnat i v el y  some variants  30 l earn t he mi d-l e v e l r epres e nt at i ons using dictionary learning B Semantic Multinomial The contextual multinomial CMN   31 us es t h e semantic multinomial SMN as intermediate representation for patches Patch SMNs are learned via weak supervision using scene labels common for all patches in each given image To address the ambiguity i.e scene category co-occurrences caused by this weak supervision a second classiﬁer i.e contextual model in  31 m odel s t h e scene from SMNs and obtains the nal classiﬁcation Note that this process has three advantages compared with other intermediate representations a no explicit mid-level vocabulary is required not even a latent one b consequently no expensive mid-level annotations are necessary and c still requires training models for the two stages but in contrast to latent representations these two stages can be trained 


SONG et al  MULTI-SCALE MULTI-FEATURE CONTEXT MODELING FOR SCENE RECOGNITION IN THE SEMANTIC MANIFOLD 2723 TABLE II C OMPARISON OF SMN-B ASED A PPROACHES TO S CENE R ECOGNITION P ROPOSED M ETHODS A RE E MPHASIZED IN B OLD     independently instead of jointly which is more complex making training much more s calable to large datasets The original CMN approach uses generative models combining Gaussian mixture models GMMs and Dirichlet mixture models DMMs The semantic manifold SM i s a variant using the negative geodesic distance NGD kernel which is suitable for the geometry of the semantic manifold i.e a simplex enabling discriminative classiﬁcation with kernel SVMs instead of DMMs Additionally the SM can be extended with spatial pyramid matching SPM  for better classiﬁcation i.e SPMSM However DMMs and kernel SVMs are still limited to relatively small datasets For large scale classiﬁcation the SPM framework uses an approximate embedding of the NGD kernel a v o i d i n g computing the kernel matrix but at the cost of some accuracy Recent extensions include unsupervised modeling  and better embeddings such as the kernelized contextual noise lter KCNF 19  H o w e v er  t h e se m o d e ls still h a v e lim itatio n s  which we address in this paper and describe in more detail in Section III-B C Deep Features Deep convolutional neural networks CNNs  trained with large datasets   34 ar e the current state-of-theart feature representations achieving impressive recognition performance in many visual recognition tasks including object and scene classiﬁcation   34 Recently several weakly supervised frameworks   have been proposed to detect and recognize objects Oquab et al  propos e t o  ne t une pret rai n ed C N N s with multiple regions where a g lobal max-pooling layer is used to select the regions for ne tuning Durand et al 35 extend this idea by selecting both useless negative and useful positive regions with a mixed maximum and minimum pooling layer These weakly-supervised works focus on objects requiring a pooling layer to select the most salient regions for object detection while in this paper we focus on scene recognition us ing all the patches for CNN ne tuning and motivated from previous works using weak supervision on shallow features e.g GMM-SMNs   13 Combining CNN features extracted at multiple scales can further improve the accuracy of scene classiﬁcation   39  40 due t o t h e w i d e r ange of obj ect s a ppeari n g in scenes Our framework also combines multiple scales and deep CNN features Earlier Gong et al  e x t r act C N N activations from patches and encode them into multi-scale feature vectors using VLAD Wu et al  e x tract deep features from a set of region proposals which are then pooled into the scene representation The semantic Fisher vector SFV approach  41 us es F i s h er v ect ors  42 t o encode the output of the CNN Note that the output of the softmax layer is a probability distribution so it can be regarded as a SMN lying on the 1000-dimensional semantic space of the training categories i.e objects categories from ImageNet ILSVRC12 However this intermediate semantic space is different from the nal scene semantic space In contrast the semantic space in our case is common for both intermediate and scene representa tions leading to scene category co-occurrences see Table II III T HE S EMANTIC M ANIFOLD A Scene Category Co-Occurrences The semantic multinomial SMN descriptor s   s 1   s M  T 12 r e p r esen ts th e p r o b a b ility s w  P  w  x  that a patch or image with visual feature x e.g SIFT color histogram kernel des criptors b elongs to each scene category w  consisting of M scene categories in total The term semantic space refers to the probability simplex where SMNs lie on Since only image labels are available patch models are learned using weak-supervision via image labels see Fig 3a In particular patches are modeled with GMMs P GMM  x  w  trained independently for each category w i.e only with patches from images in category w  Using the Bayes rule each component of the SMN descriptor is obtained as the posterior probability s w  P GMM  w  x   P GMM  x  w  P  w   P  x   We will refer to SMNs obtained in this way as GMM-SMNs  Patch SMNs in a given image are then aggregated into a single image SMN using their geometric average  or 


2724 IEEE TRANSACTIONS ON IMAGE PROCESSING VOL 26 NO 6 JUNE 2017 Fig 2 Scene recognition framework in test we resize the input images to different L sizes of V kinds of visual feature voting 11  W eak s u p e r v isio n d u r in g t r a in in g c r eates a p r o b lem of ambiguity in the resulting image SMNs For instance patches containing pieces of sky can be found in images from many categories e.g coast  mountain  highway  open country  Since the visual content is very similar all those patches may have visual features with similar distribution but depending on the particular training image the label will be different Thus for an unknown test patch the SMN descriptor will estimate certain probability in all those related categories This can be seen as scene categories co-occurring in the SMN descriptor Rasiwasia et al  obs erv e d t hat s ome co-occurrence patterns are consistent across image SMNs in the same scene category referring to them as contextual co-occurrence s masked by other undesirable co-occurrence patterns regarded as contextual noise  Thus scenes can be modeled from these patterns and hence the need for a second classiﬁer referred in  as contextual classiﬁer  B Limitations The original contextual multinomial and semantic manifold have several limitations that make them not competitive with the state-of-the-art in scene recognition a Category-speciﬁc patch GMMs are redundant and not discriminative  The reason is that they are trained independently per category so each GMM ignores the other GMMs Moreover since all GMMs from all categories need to be evaluated to obtain patch representations this also makes them inefﬁcient for a large number of categories i.e the time to compute an image SMN grows as O  NM   b Image SMNs obtained with GMMs are very noisy at both patch and image level which leads to limited recognition performance since the image classiﬁer cannot learn a suitable model from noisy data While aggregating them into image SMNs can reduce the contextual noise there is no speciﬁc method to lter contextual co-occurrences out from co-occurrence noise c Previous works CMN and SPM\M only exploit global contextual co-occurrences i.e those available in image SMNs ignoring local ones at patch level However contextual co-occurrences are essentially local and Fig 3 Weakly-supervised learning of patch SMN models a GMM-SMN b NN-SMN c CNN-SMN local modeling can signiﬁcantly improve the recognition performance While Song et al  e xpl oi t co-occurrences in patches still ignore the contextual relations between patches which are key to remove co-occurrence noise Addressing these limitations we include the following modiﬁcations see framework in Fig 2 in the semantic manifold framework  Neural network-based patch SMNs Addressing limitation a we replace category-speciﬁc GMMs by a suitable multi-layer neural network model for patch SMNs In contrast to GMMs a single model is learned jointly for all categories in a discriminative way Furthermore representations in intermediate layers are shared so they can scale much better to a larger number of categories 


SONG et al  MULTI-SCALE MULTI-FEATURE CONTEXT MODELING FOR SCENE RECOGNITION IN THE SEMANTIC MANIFOLD 2725 TABLE III C OMPARISON B ETWEEN P AT C H SMN M ODELS FOR MIT I NDOOR 67 S CENE C ATEGORIES   We study both shallow and deep architectures the latter requiring pre-training with external data  Multi-scale multi-feature contextual model We address limitations b and c with a hierarchical contextual model that exploits spatial multi-feature and hierarchical relations between patches at different scales In contrast to w e i ncl ude e xpl i c i t hi erarchi cal rel a t i ons bet w een scales and propose a message passing algorithm to better optimize the model IV D ISCRIMINATIVE S EMANTIC M ULTINOMIALS A Neural Network Based Representations A GMM can be considered as a multi-layer model with two levels of trainable parameters i.e the parameters of each Gaussian and then the weights to combine them As shown earlier each model is learned independently for each category so they are not trained to discrim inate between them Besides P GMM  x  w  is trained to t the feature distribution of x for category w  which also includes those parts that are not discriminative Thus GMMs tend to require more parameters than a discriminative approach In order to obtain more discriminative SMN descriptors we replace GMMs with a multi-layer neural network NN with the same depth two layers The neural network consists of two fully connected layer and one softmax layer see Fig 3b where the output P NN  w  x  can be used directly as SMN we refer to them as NN-SMNs  The network is still weakly-supervised via image labels However instead of having M independent GMMs we have a single NN that jointly models all categories thus being able to minimize a discriminative loss By sharin g intermediate layers we can also learn more expressive models with comparable number of parameters Table III shows a signiﬁcant increase in the accuracy with NN-GMMs In addition to discriminability NN-GMMs are more efﬁcient and scalable to datasets with many categories since only the last fully connected layer depends on the number of categories The rest of the architecture can remain unchanged In contrast GMM-SMNs require training a new GMM for each additional category i.e the cost grows linearly with M  while NN-SMNs grow sublinearly This results in signiﬁcant speed-ups e.g around ten times faster for 67 categories see Table III B CNN Based Representations We can further integrate the visual feature extraction stage as an additional layer\(s in a deeper model In GMM-SMNs and NN-SMNs the visual feature extraction stage is handcrafted i.e engineered not trainable while GMMs and the NN are trainable Deep CNNs replace this rst stage by several trainable convolutional layers Similarly to NN-SMNs we can use the output of a CNN architecture as P CNN  w  x   where now x are directly RGB pixels The downsize is that these CNN models are signi cantly deeper with many more parameters Since the training data is often limited we train the SMN model in two steps First the CNN is pre-trained with a large dataset e.g ImageNet ILSRVC2012 Places In practice we just reuse pretra ined models In order to adapt to the number of target scene categories we replace the classiﬁer implemented as the last fully connected layer by another fully connected one co nveniently resized We train this new layer i.e new classiﬁer and also ne tune the previous fully connected one e.g fc7 in VGGNet As input we use the patches extracted at the corresponding scale and as label we use the scene category of the corresponding image i.e weakly supervised training as in GMM-SMNs and NN-SMNs We refer to SMNs obtained with this method as CNN-SMNs see Figure 3c Table III shows a signiﬁcant gain compared with previous methods The main reasons are the deeper model a larger patch size and being trained end-to-end at patch level Note however that the CNN heavily relies on external large datasets used for pre-training In addition CNN-SMNs are signiﬁcantly faster Although the CNN model is more complex it processes much fewer patches Besides we implement patch extraction as convolutions i.e as a fully convolutional network which is very efﬁcient by reusing intermediate results in overlapping patches Finally visual feature extraction of kernel descriptors KDES is considerably slow since it is performed in the CPU while the other operations are performed in the GPU V C ONTEXT M ODEL A Local Category Co-Occurrences and Contextual Relations A critical part in the SM framework is the contextual modeling of category co-occurrences to obtain robust classiﬁcation The original CMN and SPM\M approaches model category co-occurrences after aggregating patch SMNs into image SMNs Thus they are limited to global co-occurrence patterns in image SMNs While it can indeed address the ambiguity resulting from weak-supervision image SMNs are still very noisy representations due to the fact that signiﬁcant information is lost in this aggregation process However these methods ignore that category co-occurrences are essentially local and sparse  a s s ho w n i n F i gure 1 c 


2726 IEEE TRANSACTIONS ON IMAGE PROCESSING VOL 26 NO 6 JUNE 2017   Fig 4 Feature-speciﬁc patch SMNs as probability maps a input image from the 15 scenes dataset category tallbuilding  b GMM-SMNs c NN-SMNs Each row represents SMNs obtained from three different visual descriptors Furthermore there are several types of contextual relations in both patch SMNs and image SMNs that we can exploit to further reduce the co-occurrence noise and emphasize consistent co-occurrences a Class-speciﬁc patterns Category co-occurrences appear with similar patterns in images from the same class This is essentially the motivation of the contextual modeling in CMN  and  S P M  M  11 approaches  w hi ch model scenes from these patterns in image SMNs Song et al 19 further exploit local co-o ccurrences and sparsity in an unsupervised way to lter co-occurrence noise in patch SMNs They use dictionary coding in a bag-of-words fashion which ignores explicit spatial relations between neighbors b Local spatial relations Neighboring patches are likely to depict parts of similar concepts e.g sky  Similarly their patch SMNs are likely to have similar co-occurrence patterns c Inter-feature relations Since SMNs are semantic representations lying on the same semantic simplex regardless of the input visual feature different visual features generate complementary co-occurre nce patterns after learning SMN models d Multi-scale relations In a multi-scale setting where images are resized to differen t scales patches extracted from similar regions but different scales will still have certain similarity so the corresponding SMNs and co-occurrence patterns will have too We exploit them for the case of CNN-SMNs By properly exploiting jointly all these contextual relations between patch SMNs consistent patterns can be emphasized while noisy accidental ones can be removed In this paper we propose a context model that join tly addresses the four types of contextual relations Inaﬁrstapproach,weassumeasinglescaleandaset V of complementary features in our experiments V   gradient  shape  color  for GMM-SMN and NN-SMN and Fig 5 Contextual models a multi feature combination b 4-connected spatial grid model and c multi-feature spatial grid model V  IM  PL  for CNN-SMN corresponding to the SMNs obtained with ImageNet-CNN and Places-CNN respectively adapted to the target scene categories as explained earlier Each feature v  V generates a set of local visual descriptors I  v  x v  1  x v  N   x v  n  X  v and I  I  1   I   V    represents all the features in the image Now we assume that P v  x v  n  w  is the feature-speciﬁc patch model for feature v  learned independently in the same way as in the single feature case Thus we can deﬁne the feature-speciﬁc patch SMN of the patch n and the feature v as s v n   s v  n 1  s v nM  T  Figure 4 shows an example with three feature-dependent patch SMNs In this gure we can observe how certain regions are noisier than others in some features We can also observe certain patterns across categories category co-occurrences across features inter-feature relations and between neighboring patches spatial relations B Global Models 1 Single Scale Model Since our objective is to keep consistent co-occurrences and rem ove accidental noise from patch SMNs we formulate our contextual model as a denoising problem using a Markov Random Field MRF Considering rst a single feature and a 4-connectivity grid the resulting model is shown in Figure 5b The objective is to 


SONG et al  MULTI-SCALE MULTI-FEATURE CONTEXT MODELING FOR SCENE RECOGNITION IN THE SEMANTIC MANIFOLD 2727 maximize the joint probability over the set of observed SMNs and denoised SMNs deﬁned as P   s 1   s N  s 1  s N   1 Z exp   E   s 1   s N  s 1  s N  where Z is the partition function to normalize the probability Thus the problem is equivalent to minimizing the global energy of the network modeled as E   s 1   s N  s 1  s N    n g   s n  s n      n  n   g   s n   s n     H   s n  1 where  s n is the unknown denoised SMN of patch n in contrast to the original s n nd  n  n   represents pairs of connected patches We model the energy as distance between SMNs A suitable choice for the distance between points in simplices is the geodesic distance GD g  s  s    W e c hos e i t o v e r the KL divergence used in  becaus e KL di v e r g ence is asymmetric and in the semantic manifold framework GD has been proved effective  F i n al l y  w e i ncl ude a r e gul ari zat i o n term H  s    M w  1 s w log  s w   which is the entropy of s  This term is included to penalize too at SMNs which would lead to uninformative patches without co-occurrence patterns to model Considering now a multi-feature setting all featuredependent SMNs and the corresponding denoised SMNs lie on the same semantic space Multi-feature combination can be easily achieved using some kind of pooling e.g weighted average as in Figure 5a but it would ignore spatial relations In contrast the previous MRF model can be easily extended to jointly consider multiple features using the model in Figure 5c The corresponding energy is E   s 1   s N  s  1  1  s  1  N  s   V   1  s   V   N     n  v  V g   s n  s v n      n  n   g   s n   s n     H   s n  2 2 Hierarchical Model Considering now a multiscale setting with L scales we can further extend the MRF model to connect the patches at scale l  1 with the patches at scale l  1  L in a hierarchical fashion The size of patches increases from scale l  1toscale l  L  A global hierarchical model using 4-connectivity is illustrated in Fig 6a The resulting joint energy for an architecture with L scales is E   s 1  1    s N 1  1    s 1  L    s N L  L   s  1  1  1  s  1  1  N 1  s  L   V   1  s  L   V   N L    L  l  1 N l  n  1  v  V g   s n  l   s  l v n     l  L   n  n   g   s n  l    s n   l     L  l  2   n  n p  g   s n  l    s n p  l  1     H   s n  3 where N l is the number of patches in scale l and n p in  n  n p  represents the neighbor in previous scale To solve the minimization problem we can consider different alternatives commonly used in computer vision problems        Fig 6 Contextual models a global b local and c extended local such as image segmentation However we must emphasize the differences of our problem with image segmentation In our case we are not interested in estimating the label of each patch but in the probabilities in SMNs as scene features Thus algorithms designed to nd the MAP labeling e.g graph cuts are not easy to adapt to our problem In the following subsections we describe different ways to address the optimization problem using different simpliﬁcations C Local Models 1 Hierarchical Iterated Conditional Modes The global hierarchical model can become very complex and difﬁcult to optimize particularly for the multi-scale scenario In order to reduce the optimization complexity we use a local approximation inspired by the Iterated Conditional Modes ICM algorithm  I n t hi s a pproxi mat i on for a gi v e n p at ch  s n  l  the rest of the  s n    n  l  are considered observed and xed Thus the contextual model becomes local to  s n  l  and it is only necessary to consider a few connections For example the complex model of Fig 6a is simpliﬁed to Fig 6b We use a hierarchical ICM algorithm that minimizes the global energy by scanning the patch es and minimizing each local model one by one updating the value of the corresponding  s n  l   For multiple scales the scanning order is extended to include the multiple scales following the top-down direction see Algorithm 1 This algorithm can be seen as coordinatewise gradient descent and converges to a local optimum The local energy for  s n  l  is computed as E   s n  l     l  n     v  V g   s n  l   s  l v n      n  h  h  B  l  n g   s n  l    s h  l       n  q  q  Q  l  1  n g   s n  l    s q  l  1     H   s n    l  n  012 s  l v n  v  V   012 s  l v h  h  B  l  n   v  V   012  s q  l  1   q  Q  l  1  n  4 where B  l  n contains the neighbors in scale l and Q  l  1  n contains the related patches from the previous scale l  1 This local problem can be solved using gradient descent 


2728 IEEE TRANSACTIONS ON IMAGE PROCESSING VOL 26 NO 6 JUNE 2017 Algorithm 1 Hierarchical ICM The gradient corresponding to patch n at scale l is  E    s n  l     l  n   s n    v  V    s n  l   s  l v n      n  h  h  B  l  n    s n  l    s h  l       n  q  q  Q  l  1  n    s n  l    s q  l  1     H   s n  5 where   x  y    g  x  y   x   y 2  x 015 1    x  y  2 An advantage of this local model is that the complexity and computational cost are greatly reduced We can easily extend this model to include relations with other neighboring SMNs both observed and latent without increasing signiﬁcantly the complexity see Fig 6c Note that these new neighboring relations are not part of the original global model of Fig 6a Including these extended relations in the global model and solving the optimization problem would be very difﬁcult since the extended connections destroy the factorization into pairwise factors requiring higher order factors and the corresponding energy terms in the formulation 2 Scale-Wise Message Passing Algorithm A different view of the hierarchical ICM is as neighboring patches sending  Fig 7 Optimization using message passing a one step of hierarchical ICM b-c two steps of the top-down scale-wise message passing algorithm for scale l and l  1 update messages to the current patch and then moving to the next one see Fig 7a A message passing algorithm consists of update messages and a schedule for the updates Algorithm 1 has the problem that each update may depend on both updated and not updated values Here we study different message passing strategies as an alternative First we consider sending update messages directly in the global model All nodes receive and send messages simultaneously in parallel which are then updated at the same time as  s n  l   s n  l   s n  l   The update is computed as  s n  l     v  V msg  n  l v  n  l       n  h  h  B  l  n msg  h  l   n  l       n  q  q  Q  l  1  n msg  q  l  1   n  l     H   s n  6 with msg  h  l   n  l     E    s n  l     l  n   s n  Note that each message solves a local optimization problem as in previous section Since the information from top scales of CNNs is usually more reliable we devise a scale-wise message passing algorithm Algorithm 2 that propagates the information from previous scales in a hierarchical fashion rather than optimizing jointly the global model The experiments will show that this strategy has better performance The algorithm sends update messages within the nodes of a given single layer including messages from the previous scale In the next step all the nodes in that scale are considered observed and the next scale is processed in the same way Fig 7b and c represent two steps of this algorithm D Embedding and Pooling After processing patch SMNs with the hierarchical context model we aggregate them into image SMNs using average pooling and the decision is simply the category with the maximum probability in the image SMN Note that in this case the ambiguity due to weak supervision still remains Alternatively patch SMNs can be encoded and pooled prior to the contextual classiﬁer   19 s ee F i g 2 I n p art i c ul ar we use the KCNF embedding w h i c h e xpl oi t s bet t e r l ocal category co-occurrences 


SONG et al  MULTI-SCALE MULTI-FEATURE CONTEXT MODELING FOR SCENE RECOGNITION IN THE SEMANTIC MANIFOLD 2729 Algorithm 2 Top-Down Message Passing Algorithm VI E XPERIMENTS A Experimental Setup 1 Datasets The proposed methods are evaluated on three small datasets 15 scenes   24 cont ai ns 4485 i m ages across 15 scene categories LabelMe  c ons i s t s of 8 out door scene categories with a total of 2600 images UIUC-Sports 5 consists of 1585 images labeled into 8 complex sport scene categories Following the settings in previous works we use 100 100 and 70 images for training respectively We also evaluate the proposed methods on larger datasets including MIT67  and S U N 397 45 M IT67 cont ai ns 15620 i m ages of 67 indoor scene classes SUN397 consists of 397 categories with 108762 images in total In the case of MIT67 Indoor and SUN397 the training/testing conﬁgurations are provided by the original authors Finally we also include an evaluation on the very large Places365-standard dataset  cons is ting of 365 scene categories and 1,803,460 training images with the number of images per class varying from 3,068 to 5,000 We follow the public training/validation split for evaluation 2 Shallow Patch SMNs We evaluate GMM-SMNs and NN-SMNs in the multi-feature setting with one scale and the proposed context models As local visual descriptor we use three variants of kernel descrip tors g radi ent  s h ape a nd color KDES All local visual descriptors are extracted on a regular dense grid of 16  16 pixels stride 8 pixels resulting in 30  30 patch level local descriptors on a 256x256 image For GMM-SMNs we train GMMs with 512 mixtures for each scene category For NN-SMNs we use a network with two fully connected layers including one hidden layer with 512 nodes Note that this network has comparable amount of parameters to the model with 512 GMMs 3 Deep Patch SMNs We use the VGG CNN architecture pre-trained either with ImageN et or Places 18 r eplacing the size of the last fully convolutional layer fc8 to meet Fig 8 Region size and sparse parameter evaluation the number of categories Then we ne tune the previous fully convolutional layer fc7 and train fc8 with the target datasets Since the size of the patches is xed in this architecture 224  224 pixels  we extract features in four scales obtained by resizing the input image to 224  224 320  320 448  448 and 640  640 pixels scales 1 2 3 and 4 respectively With these sizes we obtain 1  1 4  4 8  8 and 14  14 patches per scale respectively B Context Models With Shallow SMNs a Baseline and proposed methods We evaluate the proposed context models within the SM framework but integrating KCNF encoding F or G MM-S M N s and NN-SMNs we also include spatial pyramid matching w i t h four levels 1  1 2  2 3  3 4  4 Using the previous method as baseline we consider four variations of the proposed context model  Multi-feature context MF multiple features are combined in the semantic space with average pooling corresponding to the model in Figure 5a  Spatial context  single feature exploiting neighboring spatial relations see Figure 5b Obtained by minimizing Eq 1 when only one feature is used  Multi-feature spatial context MFS combines multiplefeatures of the target patch and neighboring spatial relations i.e see Figure 5c Obtained by minimizing Eq 2 in the multi-feature case  Extended multi-feature spatial context EMFS also includes multiple-features from additional patches in the neighborhood Figure 6c with single scale 


2730 IEEE TRANSACTIONS ON IMAGE PROCESSING VOL 26 NO 6 JUNE 2017 Fig 9 Output patch SMNs of the image in Fig 1a category tallbuilding af ter the context model and the effect of en tropy regularization a GMM-SMN s and b NN-SMNs The spatial neighborhood is 7  7 patches 1 Neighborhood Size and Entropy Regularization We evaluate the impact of the size of the spatial neighborhood which is critical in our context model We use the 15 scenes dataset and the EMFS model xing   1   V  and   1   B  l  n   The results are illustrated in Figure 8 We evaluate different neighborhoods including the 4-connectivity spatial neighborhood and other dense neighborhoods of size L  L patches 3  3 corresponds to 8 neighbors We can observe that larger neighborhoods can effectively reinforce consistent patterns and lter accidental ones However too large neighborhoods cannot capture properly local co-occurrence patterns From our experiments a good trade-off is 7  7 patches Entropy regularization is also important to capture category co-occurrence patterns properly We evaluate  in a range from 0 to 0.2 with a step of 0.05 Figure 9 shows that without entropy regularization    0 the performance is lower Note that NN-SMNs require lower penalty than GMM-SMNs We obtained the best performance for L  7and   0  1  0  05 for GMM-SMN and NN-SMN respectively so for the rest of the experiments we use this conﬁguration Figure 9 illustrates how the proposed method is able to effectively combine the three feature-speciﬁc patch SMNs from Figure 4 into smoother multi-feature patch SMNs The regularization term prevents from excessive smoothing that can wash out the true class-speciﬁc co-occurrence patterns that we want to preserve 2 Context Models We compare the different variations of the proposed method on the three small datasets to show how different types of context models improve the accuracy Table IV shows that the classi cation accuracy increases consistently when we include additional contextual relations in the context model Combining multiple features helps with a TABLE IV A CCURACY  OF GMM-SMN/NN-SMN FOR D IFFERENT C ONTEXT M ODELS I NDICATES I MPLEMENTED BY US I NSTEAD OF R EPORTED             gain around 1.1-2.5%/1.3-1.4 GMM-SMN/NN-SMN over the best single feature Using s patial relations varies from no gain to modest gains around 1%/3.3 However combining both can increase an additional 0.5-1%/0.7-0.8 over only multi-feature context The extended multi-feature spatial context contributes with an additional 0.4-2.2%/1.1-2.6 gain by incorporating multip le features from the neighboring patches The total gain with the extended context model over the baseline is around 2.6-5.7%/3.0-4.5 Note also that NN-SMNs typically obtain slightly be tter performance compared 


SONG et al  MULTI-SCALE MULTI-FEATURE CONTEXT MODELING FOR SCENE RECOGNITION IN THE SEMANTIC MANIFOLD 2731 TABLE V C OMPARISON ON MIT67 D ATA S E T      with GMM-SMNs and both consistently beneﬁt from contextual modeling 3 Comparison With Related Works We compare our method with other works using mid-level semantic representations such as latent topics object bank  22  27 and classemes   47 M os t o f t hes e approaches cannot be used in large scale datasets so we separate comparisons for small datasets and larger datasets a Small datasets Table IV compares the results reported by the authors in their corresponding references Although a completely fair comparison with reported results is not possible due to different implementations features and other parameters our framework at least seems to be competitive in the three evaluated datasets Comparing with previous methods based on SMNs and co-occurrence modeling such as CMN SPMSM and KCNF is of particular interest The proposed method which also exploits mu ltiple features and richer contextual relations achieves better performance than those methods We also compare with non-semantic representations by directly modeling categories from the same low-level kernel descriptors concatenated to combine them with and a SVM and spatial pyramid We observe that our method also achieves better results b Large datasets We evaluate the proposed methods on the larger MIT67 and SUN397 datasets The results are shown in Tables V and VI respectively NN-SMNs achieve better performance than GMM-SMNs especially for MIT67 The gains due to richer context models are much higher than in smaller datasets with signiﬁcant gains of 11%/9.5 and 15%/9.1 GMM-SMNs/NN-SMNs over the best single feature baseline respectively This suggests that contextual relations become much more important important as the number of scene categories increases resulting in much noisier and sparser co-occurrence patterns Exploiting the context to emphasize representative category co-occurrence patterns can greatly help to improve the recognition performance Other mid-level semantic representations such as object bank and meta-classes exploit larger amounts of external data TABLE VI C OMPARISON ON SUN397 D ATA S E T      TABLE VII A CCURACY  OF D IFFERENT A DAPTATIONS ON MIT I NDOOR 67   e.g ImageNet web images to model the mid-level classiﬁers The proposed method outperforms them without resorting to external data but still falls short compared with discriminative parts  w hi ch i s part i c ul arl y ef fect i v e f or indoor scenes where certain objects can be very discriminative However this method cannot scale to larger datasets such as SUN397 We also include other approaches based on lower level representations such as bag-of-words coding  and the Fisher vector T he latter achie v e s b etter accurac y  b ut at the cost of a much higher dimensional feature resulting from a much denser grid for sampling local features C Context Models With Deep SMNs and Multiple Scales 1 Patches vs Full Images We use the CNN-SMNs as described in Section IV-B extracting two complementary features that depend on the pre-training dataset i.e either ImageNet-CNN or Places-CNN In addition we consider multiple scales which are determined by the size the input image is resized for a xed patch size of 224  224 pixels When adapting the CNN to a particular target scene dataset this adaptation can be performed using full size images resized to the patch size i.e 224  224 pixels or using patches extracted at the particular scale As Table VII shows the latter is a better approach since patches used for adaptation and during test have similar scale distributions 2 Single Scale We rst compare the hierarchical ICM and the message passing MP algorithms in a single scale setting We compare the accuracy and the total energy for different spatial neighborhoods Since the total energy depends on the number of edges and they depend on the size of the neighborhood it is difﬁcult to compare neighborhoods with different size For better comparison we normalized the energy and set 


2732 IEEE TRANSACTIONS ON IMAGE PROCESSING VOL 26 NO 6 JUNE 2017 Fig 10 Comparison between ICM and MP on MIT Indoor 67 on scale 3 448  448 a\y b normalized pairwise energy,\(c time cost TABLE VIII C OMPARISON B ETWEEN I NTEGRATED AND S CALE W ISE MP M ODELS ON MIT67 IN A CCURACY       1   3  V      1   3  B  l  n      1   3  Q  l  1  n   and   0 in Eq 4 and 5 which we found work well in practice Fig 10b shows how the energy of ICM decreases quickly to the minimum value in around 16 iterations However it increases with more iterations probably due to the asynchronous updating scheme also causi ng the accuracy to decrease In contrast MP passes messages synchronously and then updates the values of each node simultaneously As a result the energy decreases more slowly but consistently although the absolute value of the of the energy is slightly higher and the accuracy increases sli ghtly However a drawback is that is slower than ICM 3 Multiple Scales and Message Passing In the next experiment we evaluate three varian ts of the proposed multi-scale MP algorithm on MIT Indoor 67 with just one CNN or combining two both ImageNet-CNN and Places-CNN The integrated variant optimizes all the nodes at the same time and then combines the scales The top-down and bottom-up variants are scale-wise and progressively update a given scale based on the previous scale In general the top-down strategy performs better than the others since the top scale more global obtains the best singlescale performance so using it as initial step leads to a better solution The results of the same experiment for SUN397 are shown in Table IX The proposed architecture combining ImageNet-CNN Places-CNN at three scales achieves a remarkable 69.3 of accuracy comparable to human performance as reported in  I n t hi s cas e i ncl udi ng s cal e 4 decreases the performance so we do not include it in the next experiment 4 Encoding Methods and Other Works In the previous experiments there is no supervised contextual classiﬁer e.g SVM nor any particular encoding The scene prediction is obtained basically pooling patch CNN-SMNs Now we also consider the full SM framework which includes TABLE IX A CCURACY  OF M ODELING J OINT C ONTEXTS ON SUN397    encoding and SVM classiﬁcatio n see Figure 2 We selected the architectures with best performance from previous experiments 3 scales for MIT67 and 4 scales for SUN397 both with ImageNet-CNN and Places-CNN and encode the CNN-SMNs using various encodings SM  F V  15 EMK  LLC 55 F o r E MK and LLC w e us e d i c t i onari es of 1000 words and for FV we use 50 GMMs and then reduce the dimensions to 4096 using PCA following T he res u l t s are shown in Table X The gain using encoding+SVM is more signiﬁcant for MIT67 than for SUN397 and for single scale than for multiple scales In particular for SUN397 a marginal 0.1 gain is achieved over the best performance We also compare with other works in Table X some using AlexNet and some using VGG architectures In the next section we evaluate our approach on the recent dataset Places365 T hus  w e can als o us e C NNs pretrained on this dataset in our framework and we report some results using an extended framework with in addition to ImageNet-CNNs and Places-CNNs includes Places365CNNs This setting obtains state-of-the-art performance 72.6 for SUN397 D Evaluation on Places365 Evaluation on Places365 is difﬁcult due to the size of the dataset In this case we use the original crops for adaptation instead of patches and 3 scales the amount of 


SONG et al  MULTI-SCALE MULTI-FEATURE CONTEXT MODELING FOR SCENE RECOGNITION IN THE SEMANTIC MANIFOLD 2733 TABLE X C OMPARISON TO THE S TATE OF THE A RT   I NDICATES O UR I MPLEMENTATION          TABLE XI A CCURACY  OF V ALIDATION D ATASET ON P LACES 365   data resulting for smaller patches is too large for training However even with these settings the results of our framework with multi-scale multi-CNN context modeling obtains 57.1 top 1 accuracy outperforming the best in baseline by 2.2 We can also compare with a simple average pooling across scales and CNNs and where our model still has a gain of 1.6 In general evaluation on Places or Places365 is not reported in the vast majority of papers about scene recognition and even most recent works typically use off-the-shelf CNNs trained on Places or Places365 but do not evaluate on those datasets For Places365 we are only aware of the result of Zhou et al  w hi ch w e i m pro v e by 1.9  Note that their setting would be closer to our scale 1 256  256 pixels but with some differences  a v erages 10 crops 4 corners  central  mirror while we use only four 2  2 patches VII C ONCLUSIONS Although recently relegated in favor of deep learning methods intermediate representations have played an important role in automatic scene recognition The semantic manifold framework addresses the problem of modeling scene categories from visual features with a combination of weak supervision and pooling that avoids mid-level annotations while inference can be easily modeled in two independent steps in contrast to most methods that learn latent representations This framework suffers from the speciﬁc problem of scene category co-occurrences thus requiring speciﬁc solutions In this paper we revisit the semantic manifold approach and tackle several of the limitations not addressed in previous works   13  19 W e i dent i f y t he ori g i n al pat c h S MN models based on GMMs i.e GMM-SMNs as an important bottleneck in terms efﬁciency and accuracy resulting from the training stage that learns patch SMN models independently for each category We show that replacing them by NN-SMNs based on neural networks and learned jointly for all the categories produce much faster and more discriminative SMNs Modeling category co-occurrences properly is the other critical stage Previous methods ignore local contextual relations which are very helpful for this purpose SMN representations in the semantic manifold have the unique characteristic that patches and images are repre sented in the same semantic space independently of the visual feature used as input Exploiting this property we combine multiple features and scales and integrate spatial multi-feature and even multi-scale relations between neighboring patch SMNs into a joint context model showing that in this way we can discover consistent co-occurrence patterns and lter out noisy ones making things easier for the classiﬁer whic h can focus on modeling scenes in terms of these cleaner patterns In particular we use a multifeature multi-scale Markov random eld formulation with a speciﬁc entropy regularizer Although still far from CNNs and some methods using the proposed NN-SMNs and an extended 


2734 IEEE TRANSACTIONS ON IMAGE PROCESSING VOL 26 NO 6 JUNE 2017 context model our framework can signiﬁcantly improve the recognition performance of the previous semantic manifold approach and its variants We further recast convolutiona l networks as sophisticated SMNs implemented as weakly supervised adaptation of a pre-trained network and inte grate them as semantic features in the proposed framework This hybrid approach achieves state-of-the-art scene recognition accuracy even without the contextual classiﬁer R EFERENCES 1 A  O li v a and A  T or r a lba M odeling t he s h ape o f t he s cene A holis tic representation of the spatial envelope Int J Comput Vis  vol 42 no 3 pp 145–175 2001 2 J  W u a nd J  M  Rehg Centr is t A v is ual d es cr iptor f or s cene categorization IEEE Trans Pattern Anal Mach Intell  vol 33 no 8 pp 1489–1501 Aug 2011 3 J  V ogel a nd B S c hiele S em antic m odeling o f n atur al s cenes f o r content-based image retrieval Int J Comput Vis  vol 72 no 2 pp 133–157 Apr 2007  L  L i H Su E  P  X ing and L  F ei-Fei  Object bank A h ighlevel image representation for scene classiﬁcation  semantic feature sparsiﬁcation in Proc NIPS  2010 pp 1378–1386 5 L  J  L i and L  F eiF ei  W h at w her e and w ho Clas s i f y ing e v e nts b y scene and object recognition in Proc ICCV  2007 pp 1–8 6 C  W ang D  Blei a nd L  F e iF e i S im ultaneous im age c las s i  cation and annotation in Proc CVPR  2009 pp 1903–1910  Z  N iu G  H ua X  G ao a nd Q T i an  Conte x t a w a re topic m odel f or scene recognition in Proc CVPR  2012 pp 2743–2750  A  Q uattoni and A  T orralba Recognizing indoor s cenes   in Proc CVPR  2009 pp 413–420 9 C  D oer s ch A  G upta and A  A  E f r o s  M idl e v el vis u al elem ent discovery as discriminative mode seeking in Proc NIPS  2013 pp 494–502  M J uneja A  V edaldi C  V  J a w ahar  a nd A Z i s s e rm an  Blocks that shout Distinctive parts for scene classiﬁcation in Proc CVPR  2013 pp 923–930  R K w itt N  V a s c oncelos  a nd N  Ras i w a s i a S cene r ecognition o n t he semantic manifold in Proc ECCV  2012 pp 359–372  N Ras i w a s i a P  J  Moreno and N  V as concelos   Bridging the g ap Query by semantic example IEEE Trans Multimedia  vol 9 no 5 pp 923–938 Aug 2007  N Ras i w a s i a a nd N V a s c oncelos   Holis tic conte x t m odels for v is ual recognition IEEE Trans Pattern Anal Mach Intell  vol 34 no 5 pp 902–917 May 2012  X  S ong S  J i ang and L  H erranz  Joint multi-feature spatial context for scene recognition in the semantic manifold in Proc CVPR  Jun 2015 pp 1312–1320  M  D i xit S  Chen D  G ao N  R as iw as ia a nd N  V a s c oncelos   S cene classiﬁcation with semantic Fisher vectors in Proc CVPR  2015 pp 2974–2983  J  Bes a g On t he s t atis tical analys is of dirty p ictures   J Roy Statist Soc Ser B  vol 48 no 3 pp 259–302 1986 17 O R u ssa k o v sk y et al  ImageNet large scale visual recognition challenge Int J Comput Vis  vol 115 no 3 pp 1–42 2015 Available http://dx.doi.org/10.1007/s11263-015-0816-y  B Z hou A L a pedriza J  Xiao A  T orralba and A  O li v a   L earning deep features for scene recognition using places database in Proc NIPS  2014 pp 487–495  X Song S J i ang L  Herra nz Y Kong and K Zheng Category co-occurrence modeling for large scale scene recognition Pattern Recognit  vol 59 pp 98–111 Nov 2016 A v a ilable http://www.sciencedirect.com/science/article/pii/S0031320316000406  S L azebnik C Schm id a nd J  Pon ce Beyond bags of features Spatial pyramid matching for recognizing natural scene categories in Proc CVPR  2006 pp 2169–2178  L  Bo X  R en a nd D F ox K ernel d es criptors for v is ual r ecognition  in Proc NIPS  2010 pp 244–252  L  Z h ang X Z h en a nd L  Shao  L earning object-to-clas s k ernels for scene classiﬁcation IEEE Trans Image Process  vol 23 no 8 pp 3241–3253 Aug 2014  A Ber g am o a nd L  T o rres a ni  Cl assemes and other classiﬁer-based features for efﬁcient object categorization IEEE Trans Pattern Anal Mach Intell  vol 36 no 10 pp 1988–2001 Oct 2014  L  Fei-Fei a nd P  Perona  A B ayes ian hierarchical model for learning natural scene categories in Proc CVPR  2005 pp 524–531  N Ras i w a s i a a nd N V a s c oncelos  Latent Dirichlet allocation models for image classiﬁcation IEEE Trans Pattern Anal Mach Intell  vol 35 no 11 pp 2665–2679 Nov 2013  X W a ng and E  G rim s on Spatial l atent Dirichlet allocation in Proc NIPS  2007 pp 1577–1584  L  J  L i H Su Y  L im  a nd L  Fe i-Fei Object bank An object-level image representation for high-level visual recognition Int J Comput Vis  vol 107 no 1 pp 20–39 2014  X  Bai C Y a o and W  L iu  S t r o k e lets  A lear ned m ultis cale m idl e v el representation for scene text recognition IEEE Trans Image Process  vol 25 no 6 pp 2789–2802 Jun 2016  X  W a ng B W a ng X  Bai W  L i u and Z  T u M axm ar gin m ultipleinstance dictionary learning J.Mach.Learn.Res  vol 28 no 3 pp 846–854 2013  G S X ie X  Y  Z h ang S Y a n and C  L  L i u Hybrid CNN a nd dictionary-based models for scene recognition and domain adaptation IEEE Trans Circuits Syst Video Technol  to be published O A v a ilable http://ieee xplore i e e e  o r g d ocu m ent 7362 156  doi 10.1109/TCSVT.2015.2511543  N Ras i w a s i a a nd N V a s c oncelos   Holis tic conte x t m odeling u s i ng semantic co-occurrences in Proc CVPR  2009 pp 1889–1895  A Krizhe vs k y  I  S uts k e v er  a nd G E Hinton ImageNet classiﬁcation with deep convolutional neural networks in Proc NIPS  2012 pp 1106–1114  J  Deng A Ber g  a nd L  Fei-Fei L ar ge graph c ons truction f or s calable semi-supervised learning in Proc ICML  2011  J  Donahue et al  DeCAF A deep convolutional activation feature for generic visual recognition in Proc ICML  2014 pp 647–655  T  Dura nd N T home  a n d M  C ord  W E L DON W e a k l y supe rvi s e d learning of deep convolutional neural networks in Proc CVPR  Jun 2016 pp 4743–4752  M Oquab L  Bottou I L a pte v  a nd J Sivic Is object localization for free?—Weakly-supervised learning with convolutional neural networks in Proc IEEE Conf Comput Vis Pattern Recognit CVPR  Jun 2015 pp 685–694  X L i et al  Deepsaliency Multi-task deep neural network model for salient object detection IEEE Trans Image Process  vol 25 no 8 pp 3919–3930 Aug 2016  H  Bilen a nd A  V e daldi W eakly s uper v is ed deep detection n etw o r k s   in Proc IEEE Conf Comput Vis Pattern Recognit CVPR  Jun 2016 pp 2846–2854  Y  G ong L  W a ng R G uo and S  L azebnik Multi-s cale o rderles s pooling of deep convolutional activation features in Proc ECCV  2014 pp 392–407  R W u  B  W ang W  W a ng and Y  Y u H ar v e s ting d is cr im inati v e m eta objects with deep CNN features for scene classiﬁcation in Proc ICCV  2015 pp 1287–1295  Q W a ng P  L i  W  Z uo and L  Z hang RAID-G Rob u s t es tim ation o f approximate inﬁnite dimensional Gaussian with application to material recognition in Proc CVPR  Jun 2016 pp 4433–4441  F  Perronnin J  Sánchez and T  M en sink Improving the Fisher kernel for large-scale image classiﬁcation in Proc ECCV  2010 pp 143–156  D G L o we  Dis tincti v e i m a ge feat ures from scale-invariant keypoints Int J Comput Vis  vol 60 no 2 pp 91–110 2004  D  Z h ang X  Chen a nd W  S  L ee T e x t c las s i  cation w ith k e r n els o n the multinomial manifold in Proc RDIR  2005 pp 266–273  J  X i ao J  H ays  K  A  E h inger  A  O l i v a and A  T or r a lba S U N database Large-scale scene recognition from abbey to zoo in Proc CVPR  2010 pp 3485–3492  B Z hou A Khos la A  L apedriza A T o rralba and A  O li v a   2016 Places An image database for deep scene understanding On Available https://arxiv.org/abs/1610.02055  L  T o rres a ni M  S zum m e r  and A  F itzgibbon E f  cient object cate gory recognition using classemes in Proc ECCV  2010 pp 776–789  M  P a nde y a nd S  L azebnik S cene r ecognition a nd w eakly s uper v is ed object localization with deformable part-based models in Proc ICCV  2011 pp 1307–1314  G  L  O l i v eir a  E  R  N as cim e nt o A W Vieira and M F M Campos Sparse spatial coding A novel approach to visual recognition IEEE Trans Image Process  vol 23 no 6 pp 2719–2731 Jun 2014 


SONG et al  MULTI-SCALE MULTI-FEATURE CONTEXT MODELING FOR SCENE RECOGNITION IN THE SEMANTIC MANIFOLD 2735  L  Xie Q T i an M  W ang and B  Z hang Spatial pooling o f h eterogeneous features for image classiﬁcation IEEE Trans Image Process  vol 23 no 5 pp 1994–2008 May 2014  Z  W a ng J  Feng S Y a n and H  X i L inear dis t ance coding for i m a ge classiﬁcation IEEE Trans Image Process  vol 22 no 2 pp 537–548 Feb 2013  J  Sanchez F  Perronnin T  Mens i nk and J Verbeek Image classiﬁcation with the Fisher vector Theory and practice Int J Comput Vis  vol 105 no 3 pp 222–245 2013  J  Xiao K  A  E hinger  J  Hays  A  T orralba and A  O li v a   SUN database Exploring a large collection of scene categories Int J Comput Vis  vol 119 no 1 pp 3–22 2014  L  Bo and C  S m i nchis e s c u E f  cient m atch k e r n el betw een s e ts of features for visual recognition in Proc NIPS  2009 pp 135–143  J  W a ng J  Y a ng K  Y u  F  L v  T  H u ang and Y  G ong L ocalityconstrained linear coding for image classiﬁcation in Proc CVPR  2010 pp 3360–3367  H Hu G  T  Z hou Z  Deng Z  L i ao a nd G Mori  L earning s t ructured inference neural networ ks with label relations in Proc CVPR  Jun 2016 pp 2960–2968  K  S i m o n y an and A  Z is s e r m an  V e r y deep con v olutional n etw o r k s f or large-scale image recognition in Proc ICLR  2015 A v a ilable https://arxiv.org/abs/1409.1556  S  Y a ng and D  R am anan  Multi-s cale r ecognition w ith D A G CN N s   in Proc ICCV  2015 pp 1215–1223  M D Dixit a nd N V a s c oncelos   Object based scene representations using Fisher scores of local subspace projections in Advances In Neural Information Processing Systems  vol 29 D D Lee M Sugiyama U V Luxburg I Guyon and R Garnett Eds Red Hook NY USA Curran Associates Inc 2016 pp 2811–2819 Xinhang Song received the B.S degree from the School of Computer and Information Technology Beijing Jiaotong University Beijing China in 2011 He is currently pursuing the Ph.D degree in computer science with the Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences Beijing His current research interests include image processing large-scale image retrieval image semantic understanding multimedia content analysis computer vision and pattern recognition Shuqiang Jiang SM’08 is currently a Professor with the Institute of Computing Technology Chinese Academy of Sciences and also a Professor with the University of CAS He is also with the Key Laboratory of Intelligent Information Processing CAS His current research interests include multimedia processing and semantic understanding pattern recognition and computer vision He has authored or co-authored over 100 papers on the related research topics He was supported by the New-Star program of Science and Technology of Beijing Metropolis in 2008 the NSFC Excellent Young Scientists Fund in 2013 and the Young top-notch talent of Ten Thousand Talent Program in 2014 He is a Senior Member of the CCF and a member of the ACM He received the Lu Jiaxi Young Talent Award from Chinese Academy of Sciences in 2012 and the CCF Award of Science and Technology in 2012 He was the General Chair of ICIMCS 2015 the Program Chair of ICI MCS2010 the Special Session Chair of PCM2008 and ICIMCS2012 the Area Chair of PCIVT2011 the Publicity Chair of PCM2011 the Web Chair of ISCAS2013 and the Proceedings Chair of MMSP2011 He has also a TPC member for about 20 well-known conferences including the ACM Multimedia  CVPR ICCV ICME ICIP and PCM He is an Associate Editor of the IEEE Multimedia Multimedia Tools and Applications He is also the Vice Chair of the IEEE CASS Beijing Chapter the Vice Chair of the ACM SIGMM China chapter Luis Herranz received the Ingeniero de Telecomunicación degree from the Universidad Politécnica de Madrid Madrid Spain in 2003 and the Ph.D degree in computer science and telecommunication from the Universidad Autónoma de Madrid in 2010 From 2003 to 2010 he was with the Escuela Politécnica Superior of the Universidad Autónoma de Madrid as a Researcher and a Teaching Assistant From 2010 to 2011 he was with Mitsubishi Electric Research And Development Center Europe U.K He is currently a Post-Doctoral Research Fellow with the Institute of Computing Technology Chinese Academy of Sciences Beijing China His current research interests include multimedia signal processing content summarization and adaptation multimedia indexing and retrieval and scene recognition 


2332-7790 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/TBDATA.2017.2680460, I\EEE Transactions on Big Data A 16 a b c particles nonthe quality aluated ely 224non-testable\224 approach are system be the to The uilding its more and MRs enough alidated R D W K quality data ork and data scienti\002c learning In topics poor prediction utes and detail timeliness metadata accuauditability  Gao Xie and T ao ha v e gi v en an o v ervie w of the issues of data where the y de\002ned big data quality assurance techniques Although for the health eb orthiy such history source and sources information Finding the duplicated information quality as for duplication Data 002ltering is an approach data Samza which is adopted and al an electronic proposed using learning training reduce the algorithm data poor data Due to the massi v e scale of big data automated choice learning grated easily data for domain task The process is to reduce the irrele v ant performance selection correlation predict CFS 
 


2332-7790 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/TBDATA.2017.2680460, I\EEE Transactions on Big Data A 17 for images In this practical based in images  Ho w the feature selection w ould impact the classi\002cation reported  More adv anced feature selection approaches such as the in can be introduced into the frame w ork e images samin images challenge to problem  Man y dif ferent approaches ha v e been proposed to address as results the are Ho we v er  none of these that most eloping MRs 12 Metamorphic testing w as 002rst Chen al  for testing non-testable systems bioinformatics sysaultrelations A recent has compilers  Metamorphic testing has been applied for testi ng a lar ge ASA and also successfully engines Baidu Ho we v er  the quality of reported are w information In this paper  metamorphic in results w are tests xity SUT  Combinatorial technique 53 used for testing softw are for are C N the learning were to classi\002cation the for confusion learning important it data our data techniques A T King and xperi#1262933 e Corporation research R S  V  Gudi v ada R Raeza-Y ates and V  Ragha v an 223Big data Promises and 224 Computer 2015  Y  Bengio 223Learning deep architectures for ai 224 ends Learning 2009  Apache 2016 Hadoop Online A v ailable http://hadoop.apache.or g  V  Gudi v ada D Rao and V  Ragha v an 223Renaissance in database 224 IEEE Computer 2016  J Zhang Y  Feng M S Moran J Lu L Y ang al of 224 ess 2013  R M and T  Poggio 223Models of object recognition 224 oscience 2000  K Jacobs  J Lu and X Hu 223De v elopment of a dif f raction imaging 224 Lett 2009  2016 Adda project Online A v ailable https://github com/addateam adda  T  Y  Chen S C Cheung and S Y iu 223Metamorphic testing a ne w CS98and 1998  J Ding D Zhang and X Hu 223 An application of metamorphic testing in metamorphic ICSE 2016  U Kane w ala and J M Bieman 223T esting scienti\002c softw are A system\224 gy 56 2014  S Se gura G Fraser  A Sanchez and A Ruiz-Cort 264 on 224 Engineering  2016  2016 Mongodb  Online A v ailable https://www mongodb com  2016 Mongochef Online A v ailable http://3t.io/mongochef  M Y urkin and A Hoekstra 2014 User manual for the discrete 1.3b4 A v ailable https team/adda/tree/master/doc  C Hsu C.-C Chang and C.-J Lin 223 A practical guide to support v ector 2003  Y  LeCun Y  Bengio and G Hinton 223Deep learning 224 e 521 2015  R Haralick 223On a te xture-conte xt feature e xtract ion algorithm for in Society ol 650\226 657 
 


2332-7790 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/TBDATA.2017.2680460, I\EEE Transactions on Big Data A 18  K Dong Y  Feng K Jacobs J Lu R Brock al 223Label-free 224 Biomed ess 2011  R M Haralick K Shanmug an and I H Dinstein 223T e xtural features 224 Cybern SMC-3 1973  S K Thati J Ding D Zhang and X Hu 223Feature selection and analin ion ance 2015  J Dixon and J Ding 223 An empirical study of parallel solution for glcm 2016  T  Kanungo D Mount N Netan yahu C Piatk o R Silv erman and im\224 hine ence 2012  M A Hall 223Correlation-based feat ure selection for machine learning 224 wzealand 1999  A Krizhe vsk y  I Sutsk e v er  and G E Hinton 223Imagenet classi\002cain al systems and 1097\2261105  E Gibne y  223Google ai algorithm masters ancient g ame of go 224 e   M Moran 223Correlating the morphological and light scattering prop 2013  R P an Y  Feng Y  Sa J Lu K Jacobs and X Hu 223 Analysis 224 ess  2014  X Y ang Y  Feng Y  Liu N Zhang L W ang al e fraction 224 ess 7 2014  M Zhang 223 A deep learning based classi\002cation of lar ge scale biomed2016  Y  Feng N Zhang K Jacobs W  Jiang L Y ang al 223Polarization w 224 A 2014  C.-C Chang and C.-J Lin 2016 Libsvm Online A v ailable csie.ntu.edu.tw 030 cjlin/libsvm  2016 Caf fe project Online A v ailable http://caf fe.berk ele yvision.or g  J Mayer and R  Guderlei 223 An empirical study on the selection of good in e C06 475\226484  U Kane w ala J M Bieman and A Ben-Hur  223Predicting metamorphic approach 224 and Reliability 2015  J Ding T  W u J Q Lu and X Hu 223Self-check ed metamorphic testing in on vement apore 2010  W  E W ong and A Mathur  223Reducing the cost of mutati on testing 224 e pp 1995  Y  Jia and M Harman 223 An anal ysis and surv e y of the de v elopment of 224  649\226678 2011  L Cai and Y  Zhu 223The challenges of data quality and data quality 224 ournal 1 2015  J Gao C Xi e and C T ao 223Big data v alidation and quality assurance in Service\(SOSE 433\226441  X Dong E Gabrilo vich K Murph y  V  Dang W  Horn C Lug aresi 224  938\226949 2015  X Y in J Ha n and P  S Y u 223T ruth disco v ery with multiple con\003icting 224 Data  2008  C H W u and Y  Song 223Rob ust and distrib uted web-scale near dup in IEEE Data 2606\226 2611  2016 Apache samza Online A v ailable http://samza.apache.or g  J A Saez B Kra wczyk and M W ozniak 223On the in\003uence of class 002ltering 224 ence 590\226609 2016  M Y ousef D S D M 250 223Feature for 224 bioinformatics 2016  F  Min Q Hu and W  Zhu 223Feature sel ection with test cost constraint 224 Reasoning 167\226 2014  H A L Thi H M Le and T  P  Dinh 223Feature selection in machine function 224 Learning 2015  H Liu F C K uo D T o we y  and T  Chen 223Ho w ef fecti v ely does problem?\224 on Engineering 2014  V  Le M  Afshari and Z Su 223Compiler v alidation via equi v alence in amming on Kingdom 216\226226  M Lindv all D Ganesan R rdal and R E W ie g and 223Metamorphic in 37th Engineering 129\226 138  Z Zhou S Xiang and T  Chen 223Metamorphic testing for softw are 224 e Engineering 2016  C Nie and H Leung 223 A surv e y of combinatorial testing 224 CM y 2011 CE O HERE Ding Computer has Computer in Nanjing 2004 r ed His the He by CM Hu  ada East  
 


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access   19     en-US  en-US  en-US  en-US 52 en-US  en-US  en-US  en-US en-US e en-US ti en-US en-US  en-US  en-US en-US  en-US 4 en-US en-US  en-US  en-US  en-US 53 en-US  en-US  en-US  en-US en-US  en-US  en-US DA en-US en-US  en-US  en-US  en-US  en-US 54 en-US  en-US  en-US  en-US en-US e en-US n en-US  en-US en-US  en-US v en-US en-US  en-US  en-US  en-US 55 en-US  en-US  en-US  en-US en-US k en-US en-US thm en-US en-US  en-US ron en-US  en-US 0 en-US en-US  en-US  en-US 5 en-US 6 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US n en-US  en-US  en-US  en-US 57 en-US  en-US  en-US  en-US en-US ti en-US en-US T en-US  en-US n en-US en-US  en-US  en-US  en-US  en-US  en-US 58 en-US  en-US  en-US  en-US en-US  en-US Pre en-US en-US  en-US t en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 3 en-US en-US 1 en-US  en-US  en-US  en-US 59 en-US  en-US  en-US t en-US  en-US  en-US n en-US en-US  en-US  en-US en-US  en-US OS en-US  en-US 2 en-US en-US  en-US  en-US  en-US 60 en-US  en-US  en-US  en-US en-US  en-US ti en-US  en-US t en-US en-US  en-US 4 en-US en-US  en-US  en-US  en-US 61 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US ess en-US  en-US  en-US 0 en-US en-US  en-US  en-US  en-US 62 en-US  en-US  en-US  en-US en-US X en-US en-US ng en-US en-US s en-US  en-US en-US  en-US  en-US i    en-US x en-US en-US e en-US en-US i en-US en-US r en-US en-US is en-US en-US 2 en-US en-US Dec en-US en-US 6  en-US  en-US  en-US 63 en-US  en-US  en-US i en-US  en-US en-US  en-US  en-US en-US h en-US i   Av a i l a bl e   en-US e en-US en-US is en-US en-US ng en-US en-US a en-US en-US nd en-US en-US b en-US en-US r en-US en-US the en-US en-US net en-US en-US of en-US en-US 0 en-US en-US Dec en-US en-US 6  en-US  en-US  en-US 64 en-US  en-US  en-US BI en-US en-US  en-US en-US e en-US  en-US  en-US en-US er en-US  en-US i    en-US du en-US en-US es en-US en-US new en-US en-US ai en-US en-US nd en-US en-US net en-US en-US of en-US en-US s en-US en-US ves en-US en-US 6 en-US en-US 0 en-US en-US Dec en-US en-US 6  en-US  en-US  en-US 65 en-US  en-US  en-US  en-US en-US Su en-US  en-US  en-US en-US a en-US  en-US 9 en-US en-US  en-US  en-US  en-US  en-US 66 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US en-US s en-US  en-US  en-US  en-US  en-US 67 en-US  en-US  en-US  en-US en-US  en-US a en-US en-US ve en-US en-US a en-US  en-US 383 en-US en-US  en-US  en-US  en-US 68 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US ess en-US  en-US 7 en-US en-US  en-US  en-US  en-US 69 en-US  en-US  en-US  en-US en-US  en-US vey en-US en-US  en-US ri en-US s en-US  en-US 77 en-US en-US  en-US  en-US  en-US 70 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 71 en-US  en-US  en-US o en-US  en-US  en-US ne en-US en-US  en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US 72 en-US  en-US  en-US  en-US en-US  en-US n en-US e en-US  en-US en-US  en-US n en-US  en-US 6 en-US en-US  en-US  en-US  en-US 73 en-US  en-US  en-US  en-US en-US  en-US  en-US hy en-US  en-US en-US  en-US n en-US  en-US 6 en-US en-US  en-US  en-US  en-US 74 en-US  en-US  en-US  en-US en-US  en-US t en-US en-US I en-US  en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US 75 en-US  en-US  en-US  en-US en-US  en-US ty en-US en-US  en-US en-US  en-US f en-US en-US  en-US  en-US  en-US  en-US 76 en-US  en-US  en-US a en-US  en-US en-US ti en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 96 en-US en-US  en-US  en-US  en-US 77 en-US  en-US  en-US  en-US en-US  en-US n en-US en-US 0 en-US 8 en-US en-US  en-US  en-US ess en-US  en-US 85 en-US en-US  en-US  en-US  en-US 78 en-US  en-US  en-US  en-US en-US  en-US n en-US en-US  en-US  en-US  en-US  en-US  en-US 79 en-US  en-US  en-US hen en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US 4 en-US en-US  en-US  en-US  en-US 80 en-US  en-US  en-US  en-US en-US ng en-US en-US  en-US  en-US  en-US en-US s en-US  en-US  en-US 6 en-US en-US  en-US  en-US  en-US 81 en-US  en-US  en-US N en-US  en-US  en-US en-US  en-US  en-US en-US 2011 en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 82 en-US  en-US  en-US  en-US  en-US en-US o en-US  en-US  en-US en-US 2011 en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 83 en-US  en-US  en-US  en-US en-US  en-US s en-US  en-US  en-US  en-US  en-US 9 en-US en-US  en-US  en-US  en-US 84 en-US  en-US  en-US  en-US en-US  en-US en-US  


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access  20      en-US  en-US en-US  en-US  en-US  en-US 4 en-US en-US  en-US  en-US  en-US 85 en-US  en-US  en-US F en-US  en-US en-US  en-US  en-US en-US  en-US 0 en-US  en-US 5 en-US en-US  en-US  en-US  en-US 86 en-US  en-US  en-US h en-US  en-US  en-US l en-US en-US  en-US en-US  en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 87 en-US  en-US  en-US  en-US en-US ti en-US en-US  en-US  en-US d en-US en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 88 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US 6 en-US  en-US 263 en-US en-US  en-US  en-US  en-US 89 en-US  en-US  en-US  en-US en-US tem en-US en-US OTA en-US  en-US 6 en-US  en-US 6 en-US en-US  en-US  en-US  en-US  en-US 90 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US m en-US  en-US 87 en-US en-US  en-US  en-US  en-US  en-US 91 en-US  en-US  en-US e en-US  en-US  en-US en-US  en-US  en-US en-US  en-US m en-US  en-US 87 en-US en-US  en-US  en-US  en-US  en-US 92 en-US  en-US  en-US  en-US en-US  en-US a en-US  en-US  en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 93 en-US  en-US  en-US  en-US en-US  en-US r en-US en-US  en-US  en-US en-US  en-US 195 en-US en-US  en-US  en-US  en-US 94 en-US  en-US  en-US Wei en-US en-US xi en-US ng en-US en-US eng en-US en-US  en-US en-US  en-US k en-US en-US to en-US en-US  en-US  en-US en-US ess en-US  en-US 1 en-US en-US  en-US  en-US  en-US 95 en-US  en-US  en-US  en-US en-US  en-US c en-US r en-US en-US  en-US  en-US en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 96 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US r en-US  en-US  en-US 9 en-US en-US  en-US  en-US  en-US 97 en-US  en-US  en-US  en-US  en-US  en-US en-US e en-US en-US  en-US r en-US en-US  en-US  en-US en-US  en-US en-US  en-US  en-US  en-US  en-US m en-US en-US s en-US en-US 3 en-US  en-US 3 en-US en-US  en-US  en-US  en-US 98 en-US  en-US  en-US a en-US en-US  en-US en-US  en-US ter en-US en-US  en-US  en-US en-US l en-US  en-US  en-US  en-US  en-US 99 en-US  en-US  en-US a en-US  en-US en-US  en-US  en-US  en-US en-US  en-US BE en-US  en-US 1 en-US en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US e en-US  en-US  en-US  en-US  en-US 1 en-US  en-US  en-US e en-US en-US  en-US ter en-US en-US  en-US  en-US  en-US  en-US 2 en-US  en-US  en-US e en-US en-US  en-US ter en-US en-US the en-US  en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US  en-US ter en-US en-US  en-US ti en-US en-US n en-US en-US  en-US n en-US  en-US 3 en-US en-US  en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US  en-US th en-US  en-US  en-US ter en-US en-US  en-US  en-US en-US s en-US  en-US 7 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US en en-US en-US  en-US n en-US  en-US  en-US en-US 4 en-US  en-US 4 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US e en-US en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US 4 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US  en-US ter en-US en-US  en-US tem en-US en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US  en-US en-US  en-US a en-US en-US  en-US en-US  en-US S en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US  en-US  en-US en-US ter en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 72 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US en-US  en-US n en-US en-US a en-US  en-US 5 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US W en-US  en-US en-US  en-US  en-US nty en-US en-US  en-US ON en-US  en-US 1 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US  en-US Se en-US  en-US en-US  en-US l en-US  en-US 334 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US Int en-US en-US y en-US  en-US  en-US  en-US 249 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US en-US  en-US d en-US en-US  en-US 1 en-US 1 en-US 52 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US  en-US te en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US en-US r en-US  en-US 34 en-US en-US  en-US  


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access   21     en-US  en-US 8 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US i en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US s en-US  en-US  en-US  en-US 2 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US 1 en-US 77 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US en-US d en-US en-US a en-US en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US 2 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US o en-US  en-US en-US  en-US  en-US en-US s en-US  en-US 0 en-US en-US  en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US s en-US  en-US  en-US 0 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 3 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US en-US a en-US  en-US 321 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US K en-US en-US t en-US en-US  en-US en-US  en-US  en-US s en-US en-US dy en-US en-US matics en-US  en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US  en-US res en-US  en-US  en-US  en-US 9 en-US  en-US  en-US R en-US n en-US en-US  en-US en-US  en-US  en-US en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US n en-US en-US  en-US en-US n en-US  en-US  en-US  en-US en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US en-US  en-US  en-US  en-US man en-US en-US  en-US  en-US 3 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US en-US  en-US en-US  en-US d en-US en-US s en-US 2 en-US  en-US 46 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US ti en-US en-US ti en-US en-US  en-US n en-US en-US n en-US  en-US 1 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US  en-US n en-US en-US n en-US  en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US  en-US en-US 2 en-US  en-US ron en-US  en-US 351 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US Semi en-US en-US  en-US  en-US en-US  en-US S en-US s en-US en-US  en-US  en-US  en-US 2 en-US en-US  en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US ex en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 9 en-US  en-US  en-US nt en-US en-US  en-US en-US  en-US  en-US a en-US  en-US en-US  en-US r en-US  en-US 7 en-US en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US en-US  en-US n en-US en-US EEE en-US  en-US 6 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US s en-US  en-US 7 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US en-US ti en-US en-US  en-US n en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US ene en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 7 en-US en-US  en-US  en-US 4 en-US  en-US  en-US en-US  en-US ti en-US en-US  en-US en-US  en-US  en-US 8 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US i en-US  en-US  en-US en-US  en-US dy en-US en-US  en-US n en-US y en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US k en-US en-US to en-US en-US  en-US  en-US en-US IE en-US  en-US  en-US 0 en-US en-US  en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US ter en-US en-US n en-US en-US  en-US l en-US en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 9 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US  en-US  en-US n en-US en-US  en-US en-US  en-US  en-US  en-US 7 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US k en-US en-US to en-US en-US  en-US the en-US  en-US en-US  en-US  en-US 4 en-US en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US en-US  en-US n en-US en-US  


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access  22      en-US Inte en-US  en-US  en-US 132 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US k en-US en-US to en-US en-US n en-US en-US  en-US I en-US en-US  en-US  en-US  en-US 69 en-US en-US 8 en-US  en-US  en-US  en-US 2 en-US  en-US  en-US r en-US en-US  en-US en-US k en-US en-US to en-US en-US  en-US  en-US en-US r en-US  en-US 1 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US t en-US en-US n en-US en-US n en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US  en-US i   en-US m en-US  en-US  en-US 5 en-US  en-US  en-US  en-US  en-US en-US k en-US en-US to en-US en-US  en-US tr en-US  en-US n en-US en-US  en-US  en-US 7 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US en-US a en-US i   en-US d en-US en-US a en-US en-US c en-US en-US es en-US 2 en-US en-US n en-US en-US 7  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US dy en-US en-US  en-US n en-US y en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US  en-US en-US  en-US vey en-US en-US  en-US  en-US 13 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US n en-US en-US  en-US n en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US 2010 en-US  en-US  en-US 7 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US en-US  en-US o en-US n en-US en-US  en-US  en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 4 en-US  en-US  en-US B en-US  en-US en-US A en-US ti en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 6 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US e en-US en-US s en-US  en-US  en-US  en-US en-US  en-US n en-US  en-US 1 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US f en-US en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US  en-US e en-US en-US  en-US  en-US  en-US 1 en-US 68 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US f en-US en-US  en-US ew en-US en-US e en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US en-US i en-US  en-US  en-US en-US  en-US n en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US h en-US en-US er en-US i   Av a i l a bl e   en-US p en-US en-US ten en-US en-US hn en-US y en-US en-US ends en-US en-US l en-US en-US the en-US en-US l en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US en-US  en-US en-US re en-US  en-US 6 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US y en-US en-US  en-US e en-US en-US  en-US  en-US  en-US en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US en-US re en-US  en-US 6 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US en-US t en-US  en-US earn en-US  en-US 9 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 34 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US Lef en-US  en-US en-US  en-US  en-US ti en-US en-US  en-US en-US V en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US Pro en-US  en-US  en-US  en-US 34 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US ng en-US en-US ti en-US en-US  en-US  en-US  en-US en-US  en-US 1 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US O en-US en-US M en-US  en-US en-US l en-US en-US  en-US  en-US en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US N en-US s en-US  en-US en-US  en-US  en-US 9 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US  en-US to en-US en-US  en-US en-US n en-US  en-US  en-US  en-US 5 en-US  en-US  en-US D en-US  en-US  en-US en-US nt en-US en-US  en-US en-US  en-US en-US  en-US  en-US  en-US  en-US 6 en-US en-US  en-US 2 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US edes en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US t en-US  en-US en-US  en-US  


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access   23     en-US  en-US 22 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US t en-US en-US  en-US vey en-US en-US  en-US  en-US ne en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US  en-US  en-US tbed en-US en-US  en-US en-US  en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US G en-US  en-US en-US o en-US en-US  en-US en-US  en-US  en-US  en-US 0 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US e en-US en-US ent en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US a en-US en-US z en-US en-US  en-US dez en-US en-US z en-US en-US  en-US en-US  en-US a en-US  en-US  en-US en-US  en-US 26 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US a en-US en-US a en-US en-US o en-US en-US  en-US a en-US en-US  en-US en-US o en-US en-US F en-US en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 8 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US ti en-US en-US  en-US  en-US  en-US en-US  en-US en-US  en-US 2015 en-US b en-US 5 en-US  en-US 9 en-US en-US  en-US  en-US  en-US  en-US 5 en-US  en-US  en-US o en-US en-US  en-US en-US  en-US  en-US en-US n en-US  en-US  en-US 45 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US k en-US  en-US  en-US en-US  en-US  en-US  en-US 0 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US r en-US  en-US 86 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US  en-US y en-US en-US by en-US en-US  en-US en-US  en-US s en-US  en-US 1 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US  en-US dez en-US en-US  en-US en-US n en-US en-US  en-US en-US  en-US n en-US y en-US en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US en-US  en-US tem en-US en-US  en-US ti en-US en-US ve en-US en-US  en-US  en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US ty en-US en-US  en-US en-US  en-US  en-US  en-US 5 en-US  en-US 2 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US en en-US en-US  en-US  en-US  en-US en-US ess en-US  en-US 831 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US  en-US en-US ti en-US en-US  en-US ne en-US en-US  en-US e en-US en-US  en-US  en-US 26 en-US en-US  en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US ve en-US  en-US  en-US en-US  en-US rk en-US  en-US 7 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US  en-US en-US Sy en-US  en-US  en-US en-US  en-US  en-US  en-US es en-US  en-US 1 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US  en-US  en-US r en-US rks en-US  en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US n en-US  en-US  en-US 9 en-US 0 en-US  en-US 411 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US  en-US tems en-US en-US  en-US  en-US  en-US  en-US n en-US  en-US en-US  en-US v en-US es en-US  en-US  en-US 3 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US en en-US en-US  en-US  en-US en-US  en-US en-US  en-US  en-US  en-US 0 en-US  en-US  en-US m en-US en-US  en-US  en-US th en-US en-US  en-US en-US  en-US  en-US  en-US 7 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 4 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US ng en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US  en-US rk en-US  en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 8 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US t en-US dy en-US en-US n en-US s en-US  en-US  en-US 9 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US  


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access  24      en-US  en-US i en-US  en-US en-US  en-US  en-US 9 en-US  en-US 2 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US  en-US n en-US en-US n en-US  en-US t en-US 7 en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US n en-US en-US E en-US ess en-US  en-US 858 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US ess en-US  en-US 1 en-US en-US  en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US  en-US  en-US o en-US en-US  en-US en-US in en-US  en-US  en-US  en-US l en-US en-US  en-US g en-US  en-US 0 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US Ben en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US 7 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US f en-US  en-US  en-US dy en-US en-US  en-US 11 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US  en-US en-US T en-US en-US  en-US o en-US  en-US en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US z en-US en-US  en-US en-US ti en-US en-US  en-US  en-US  en-US en-US sort en-US  en-US  en-US 5 en-US  en-US  en-US  en-US 8 en-US  en-US  en-US z en-US en-US  en-US en-US  en-US f en-US en-US the en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 121 en-US en-US  en-US  en-US  


