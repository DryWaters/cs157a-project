Cloud Predicting Systems a Education echnology Japan a@mmm.muroran-it.ac.jp a Center ersity Japan a@cmc.osaka-u.ac.jp Murata Science ersity Japan murata@ist.osaka-u.ac.jp Abstract 227 ursting of enting es longweb ed a web both of the numerical of and or cost s erms ediction I N centers them approach cost is sting  In in and sts one for total time of ybrid adjust orkloads vided demand of between required ely includes applications  In the second cate gory  future w orkload is unkno wn to constraints resource in applications a video streaming service and production systems 6 Our tar get f alls into is basis errors  studies a of y applied W e thus reallocate not  orking ork although ph ysical serv ers ha v e a longer e computing ysical long-term ate an prediction based virtual course contrithe total the ard platform by websites Sect we method approach conclusions 
2017 International Conference on Computing, Networking and Communication\s \(ICNC 978-1-5090-4588-4/17/$31.00 \2512017 IEEE 
 


Fig approach Fig approach O F C D B G A H einman we deapplication resources in ysical application ery system hour the the VMs or is data for data  M F H D C D S M response cloud in VMs Model are which by t  t 1  001 T  has slots t 2 I P T t x t A system n t n 0 t application centers 025 t 025 0 t center ely  025 t  025 0 t  x t  N t  003 t system processing rate  n t 024 N t 025 t 024 003 t  T II C M center c ps Cost  22.8 n vm 2 c ec  16/kWh p ps 550W e 002cient 0.6 12 b center c vm Cost 0.732/hour c tr 0.14/GB d bytes website c management c st Personnel  1250/h n st 100 013 scale  013 024 1 15 0.6 V V  N t processing  003 t each of w slots application platform C to VMs F public VMs U management O er horizon e minimize C  T X t 1  aF  N t n t  a 0 U  n 0 t 025 0 t  O  N t n 0 t   1 where a the in and a 0  1 as F  N t n t  c ps 030 N t n vm 031  c ec p ps 022 1 000 e  030 n t n vm 031  e n t n vm 023  2 where renting l N t n vm m ph for ers where l n t n vm m ph ysical on n t VMs 2017 International Conference on Computing, Networking and Communication\s \(ICNC 
 


Second as U  n 0 t 025 0 t  c vm n 0 t  c tr d\025 0 t  3 ontransferring storages wide-area public system the rate is O  N t n 0 t  c st 022 1 n st  N t  n 0 t  023 013  4 where support the economics scale erformance rethus public centers q for  r q and r q 0 threshold r c Here q the  R in center probability  n t rate  025 t  r c not  q 5 by notation  parameter alue to r q 024 r c 020 R  n t   025 t r q  025 q 100 021  8 t  5 r q 0 024 r c 020 R  n 0 t   025 0 t  r q 0  025 q 100 021  8 t  6 In  n t and  n 0 t alues   025 t and  025 0 t actual q  r q and r q 0 xceed r c errors ediction model to predict the request operator B by x t  x t 000 1 series x t transformed series y t 1 000 B  d 1 000 B s  D x t by the d the D thThis y t a ws y t  p X i 1 036 i B i y t  q X j 1 022 j B j  017 t 7 where 036 i 022 j and 017 t term ws 017 t 030 N 0 033 2  the the  033 means y t 1 030 N  y t 1 033 2  when y t  h as y t  h  P 1 034 0  034 017 t  h 000 034 where  034 is and  0 1  y t  h ws y t  h 030 N  y t  h 033 2 P h 000 1 034 0  2 034   ution response slot t model asynwithout aiting Let r  r 0  and 026 at t rate ution function R as R  n t 025 t r 1 000 031  n t 025 t  e 000  n t 026 000 025 t   r 000 r 0   r 025 r 0   8 where 031  n t 025 t  at t as 031  n t 025 t  n t 032 n t t n t  n t 000 032 t   n t 032 n t t n t  n t 000 032 t   n t 000 1 X l 0 032 l t l   000 1 032 t  025 t 026  9 This  M R R E A N VM each w size xt w time f N t  h j h 1  001 w g  N t 1  001  N t  w   counted using n t  h and n 0 t  h  h 1  2  001 w  on the w rate f  x t  h j h 1  001 w g of  n t 1 and n 0 t 1 prediction  x t 1 and N t 1 e E N system the long Settings Datasets o systems 017 2014 30,000 web 017 1998 website called a consumer web 2017 International Conference on Computing, Networking and Communication\s \(ICNC 
 


2 model are the 32-GB memory each were assumed to be used on a three-year to in c ps  600  000  3 002 365 002  er to EC2 The processing  026 requests/s  e 026 that as for  c st to per month  30 002 24  In addition a 2  and a 0 1  25  in Objecti v e 1 W e of 1 aints 6 probability q of  r c s Furthermore the  r 0 in and  ediction of i.e 24 002 168 slots series x t of y t 1 000 B  1 000 B 168 log 10 x t arithmic and weeks i.e 24 002 504 alues of p and q of A  p 1 q  16 Results Rate the time ursty prediction percentage as 1 T P T t 1 j x t 000  x t j x t  168-timewed gular ge xi.e both erage Center ws the  N t consumer of by by C ent  This C ent assigned the ate Fig web Fig errors Fig of N t minimized when N t till N t 4  ime the the  C  C ent  Each ideal are cost than threshold r c transformed q one ger 0.15 slight of in pool response The xceeding in resulting thus the response con\002dence indicates still 7 responsebound w ut of C ent onethe 2017 International Conference on Computing, Networking and Communication\s \(ICNC 
 


Fig cost Fig web Fig error cost Cost mean the point that using al al ratio predictions the to the cost the onethe wing erted bounds upper equilibrium while N t alue around N t 3 These center the rate C N which data determine the ge upper onethe the the the the impose e instances R S  J Barr  223Cloudb ursting h ybrid application hosting 224 https://a ws.amazon 2008  H.-Y  Chu and Y  Simmhan 223Cost-ef 002cient and resilient job life-c ycle in IPDPS  327\226336  M Hosein yF arahabady H Samani L Leslie Y  C Lee and A Zomaya 224 in ICPP 419\226428  T  Guo U Sharma P  Sheno y  T  W ood and S Sahu 223Cost-a w are cloud 224 hnol  2014  H Zhang G Jiang K Y oshihira and H Chen 223Proacti v e w orkload 224   2014  M Bjorkqvist L Chen and W  Binder  223Cost-dri v en service pro visionin SOCA pp 1\2268  VMw are Inc 223vsphere and vsphere with operations management 224 html 2016  C Dixon D Olshefski V  Jain C DeCusatis W  Felter  J Carter  de\002ned 224 Res  2014  J W einman 223Hybrid cloud economics 224 Comput 3 2016  Dell Inc 223Po werEdge R430 224 http://www dell.com/jp/b usiness/p 2015  T ok yo Electric Po wer Compan y Holdings Inc http://www tepco.co.jp 2015  D W ong and M Anna v aram 223Knightshift Scaling the ener gy propor in 45th O 119\226130  Amazon.com Inc 223Amazon Elastic Compute Cloud EC2 224 http://a ws 2015  A Greenber g J Hamilton D A Maltz and P  P atel 223The cost of a 224 Computer w 2008  F  T  Moore 223Economies of scale Some statistical e vidence 224 The Economics 1959  R J Hyndman and G Athanasopoulos 223F orecasting principles and 2015  R Jain Analysis John 1991  The Internet T raf 002c Archi v e 2231998 w orld cup web site access  2014  J D McCabe Second Edition 2003 2017 International Conference on Computing, Networking and Communication\s \(ICNC 
 


ii 
Global address space is shared among all virtual machine and this may lead to security threads while having centralized memory access  Another big disadvantage of global address space is every time when any process or VM get detach memory address a signal is sent to all the connected devices so latency may be possible 
An issue brought about by shared file system is the absence of arrangement separation Different applications can have clashing necessities for framework wide setup settings  Shared library conditions can be particularly dangerous since cutting edge applications use numerous libraries and frequently distinctive applications require diverse variants of the same library At the point when introducing different applications on one working framework the expense offramework organization can surpass the expense ofthe product itself  C Inter process Communication lPe  i Hypercalls  Hypercalls in hypervisor behaves like system calls in operating system It is an interface in between guest operating system Virtual Machine and host 
This memory model solves all problems arose in shared memory access model by allocating isolated memory units among virtual machines But one big disadvantage of this model is it is not scalable 
Shared Memory Access  
Distributed and Network File systems 
Utilization of Storage Devices  Server virtualization joins couple of servers on storage devices this frequently make bottlenecks as virtual machines VMs vie for storage resource Desktop virtualization can likewise make bottlenecks since such a large number of desktops keep running on a solitary host Along these lines effectively overseeing virtual servers regularly requires a joint effort from storage and also desktop and server administrators To help information stockpiling experts capitalize on these testing situations we've gathered our main five tips on overseeing stockpiling in a virtual server environment  iv 
iii 
There are several models for memory management in hypervisor virtualization Likewise 
Utilization of Memory Units  
Distributed Memory Access  
 


Virtualized hardware I nterfac 
Guest App l icat ion s G u est operat i ng syste m 
operating system  There are certain issues with hypercalls inter process communication mechanism like  Granularity  Security and speed Granularity  Hypercalls increase the problem of granularity by increasing communication ratio than processing ratio as multiple guest operating system requests for resources to host operating system installed over hardware layer  Security  hypercalls are exceedingly privileged  correctness should be in the bag  It may be problem to sniff data using hypercalls from one guest operating system to other guests or host operating system by IPC So it may be necessary that the call must be well equipped with cryptographic algorithm Speed  Normally system calls are very fast in communication but Hypercalls are little bit slower as an application is executing on virtual machine and needs a resource to complete the process so respective VM will request for the particular resource to host OS or Virtual machine manager and it will take long time to process 
operating  SYSrem A PI Host Operating System Hardware I nterfaces 
Hypervisor H W Fig.3lpe Ring 3 Hos t 
I/O ca ll 
Ring 1 driver Ring 0 b 
OS 
Appl i cat i on modified  Gues 
OS 
Virtual Driver 
 


containers as Virtual Machines Individual can install configure and run diverse apps libraries just as it would be on any Operating system But in this scenario anything executing within a container can only perceive resources that are assigned to it There are two flavor of system containers identical and multiple  In identical containers are designed to run multiple programs and services whereas application containers are planned to box up and run a particular service Simple single service as a container is known as application container The container host all the required libraries and supporting files with that particular application/service to execute in that particular domain This feature help to process fast and avoid granularity that not all the time application communicate with the host operating system 
Container there is an array of homogeneous containers as in figure a but in multi-OS container a choice is for user to host different operating systems containing different containers of their nature Application Container  
containers are virtual atmosphere that distribute the kernel of the host operating system but offer user space isolation  For all useful reasons one can believe of 
Containerization Another virtualization technique is known as Containerization Container is a light weight process that runs inside an operating system root kernel as it is host Containerization does extremely well in executing all kind of micro services-based architecture that is appropriate more and more for cloud-native web apps Containers are of two types  System Container 
as as as as 
Characteristics of Container 
1 As a customer you condition containers not hardware virtual machines or even bare metal servers  2 Those containers run on native systems not in a VM 3 Each container is an identical examine on the network to which it is attached by its hold IP stack sovereign of its meticulous host containers must not be group in the host's system 4 The container hypervisor has experienced and believed security isolation for containers nasty or wrecked code in one container must not be able to break other containers as it was possible in VMs  5 As a public cloud client he/she has to pay by the container not for the infrastructure and surely not for a bunch of VMs they force to run on 
 


Benefits of Container 
The principle of least privilege or the principle of least authority it is mechanism is a model in security endorsing negligible user profile rights on computers and based on users job requirements  And sharing same instance among diverse programs is always risky  If any of the program is disturbed then it disturb few more programs or the entire programs in the same instance like covert storage channel Suppose the security of one program is compromised than all other programs sharing same instance or state will be affected This concept brings 
SECTION IV THREAD MODEL 
Hypervisor virtualization techniques do not satisfy the problems discussed in this section So need another virtualization technique to address the problem and in this report I advocate containerization Literature Review 
The main theme which enforces to work on the title cited above is 
SECTION III MOTIVATION 
1 Fast Runtime performance near bare metal speed 2 Flexible Have facility to Containerize a system and Containerize application\(s 3 Inexpensive Most containers are freeware or open source 4 Ecosystem Due to its benefit and characteristics it is growing rapidly and getting enough popularity 5 Light Weight It is light weight process as other processes running concurrently on operating system Just enough OS Minimal per container penalty Section IV Background In this area we talk about dangers and vulnerabilities in CC some normal dangers and vulnerabilities are highlighted in detail and record and administration seizing  Powerlessness alludes to a shortcoming or blemish in a framework that can be abused by an occasion called an assault Hashizume et al 2013 Tianfield 2012  A danger see Figure 1 is an abuse ofany known powerlessness that can bring about genuine loss of information and data Modi et al 2013  These dangers and vulnerabilities are seen as a hindrance for associations wishing to embrace cloud administrations Hashizume et al 
least common mechanism principle  
 


2013  The Cloud Security Alliance CSA has highlighted beat dangers to CC Alva et al 2013  Information Breaches and Loss  These are exceptionally extreme dangers in view of cloud administrations shared design  Information misfortune can happen if information are being erased without having reinforcement or if there is any catastrophe and no appropriate components are embraced to recuperate it Khorshed et al 2012  Privacy uprightness and accessibility of client's information are put at hazard Alva et al 2013 an assailant has an opportunity to capture a client's record points of interest by taking client qualifications furthermore data An aggressor may utilize different techniques for example phishing DoS and Man-in themiddle assault to take client accreditations Khorshed et al 2012 and afterward utilize them to spy on a client's exercises and exchanges  More refined assaults include mixes of these techniques for monetary pick up  A few strategies for which fruitful assaults might be accomplished include Unreliable Application Programing Interfaces APls A CSP offers framework programming and stage administrations to clients and gives them access to administrations through Application Programing Interfaces APls Khorshed et al 2012 Chhabra and Dixit 2015 Ayala et al 2013  Clients oversee administrations they devour by utilizing these APls Modi et al 2013 for instance PaaS APls give get to and usefulness to cloud SaaS APls give availability to applications laaS APls gives control and dispersion over particular cloud assets Classification trustworthiness and validation of a cloud administration are in particular subject to how APls are planned and created Hashizume et al 2013 Vaquero et al 2011 Ayala et al 2013  Accordingly it is the obligation of the architect and designer to make it troublesome for malevolent clients to endeavor potential vulnerabilities  For instance plan defects of the APls to the fundamental SSL libraries Georgiev et al 2012 give influence to a client with noxious purpose to execute unsafe activities Srinivasan et al 2012  Noxious Insider  Confidentiality uprightness and accessibility might be damaged if client information mishandle happens from somebody with approved get to yet with vindictive purpose Alva et al 2013 The vindictive insider danger may get to be distinctly clear in light of the fact that of an absence of straightforwardness in CSP procedures and 
 


methodology and the level of get to each worker needs to client information Modi et al 2013 Srinivasan et al 2012  Manhandle of Cloud Resources  CSPs give free trials of laaS to clients with a specific end goal to persuade them to buy this administration later on  Be that as it may because of an absence of control practiced by the CSP clients may accept the open door to perform malignant exercises utilizing the capable equipment stage gave by the CSP Khorshed et al 2012 Modi et al 2013  Besides such equipment gives them opportunity to split encryption keys Modi et al 2013 Zhang et al 2013  Shared Technology Vulnerabilities Virtualization gives multi-tenure through the sharing of equipment assets like CPU centers abnormal state reserve stockpiling gadgets what's more and system interface cards among various occupants Ayala et al 2013  The hypervisor gives a stage to virtualization and is in charge of separation between various VMs running on a mutual server By abusing vulnerabilities or design blemishes an assailant can increase unapproved access to a hypervisor to control a cloud stage and adventure other clients VMs Khorshed et al 2012 Modi et al 2013  For instance foundation sharing is one center cloud benefit but since of equipment sharing impediments needs essential assurance systems to ensure client arrange movement information and different applications This gives an aggressor an opportunity to seize client accreditations listen in on data also and control other clients VMs Khorshed et al 2012  Side Channel Attack The present day 
microchip has various levels of reserve Levell Level 2 and Level 3 In a processor with different centers and three levels of store every processor center has its own isolate L1 and L2 reserve while L3 store is shared among everyone of 
x64 
 


s ide ha nn el 
In fe rr ed codepat h The fundamental building block of a FlushReload assault is as per the following  A lump is a cacheline-sized adjusted locale in the physical memory that is mapped into the foe's address space  For instance if a cacheline is of size 64B then every address that is a various of 64B characterizes the piece beginning at that address  225 Flush The enemy flushes pieces containing particular directions situated in a memory page it offers with the casualty out of the whole reserve progression counting the shared last-level reserve utilizing the clflush direction  225 Flush Reload interim  The enemy sits tight for a predetermined interim while the last\255 level store is used by the casualty running on another CPU center 225 Reload  The enemy times the reload of the same pieces into the processor A speedier reload proposes these lumps were in the last level reserve as were executed by the casualty amid the Flush Reload interim a slower reload proposes something else 
Flash Reload 
Cache pattern cl ass i fi ca t ion Ana ly sis st age 
pr ob i ng 
of SVM-c l ass i fied 
Code-p ath 
reassemb ly 
the centers of the processor Yarom and Falkner 2013 Kim et al 2012 Zhang et al 2011  In this way the common design of L3 reserve among various centers makes an open door for a vindictive client to misuse it and dispatch a side channel assault  In an average situation when the processor needs information it checks L1 L2 and L3 reserves separately  On the off chance that the asked for information are available at any level then the information are perused and a reserve hit happens  Be that as it may if information are not accessible in any reserve level then a store miss happens and the processor gets information from RAM and places it in the store prepared for the following call In the occasion of a reserve miss the processor may likewise exploit spatial region by getting information from close reserve areas since it is normal that information in close store areas are going to be gotten to soon Irazoqui et aI 2014a 2014b  Additionally information brought from RAM also put in store are in settled measured pieces called reserve lines and a reserve passage is made amid this procedure  Co res id ent 
l ab e ls 
M easure men t s tag e Phase 1 m ea su reme n ts 
Phas e 3 No i se re d uct i on Frag ments of code path Phase 4  
Sequences 
VM s  Ph ase 2 f 
ll-<ache 
 


Experiment Setup Proposed attack scenario 
SECTION V EXPERIMENTS 
We allude to the lumps being Flush Reloaded by the advertisement versary as being observed since Flush-Reload basically screens access to information in the lump  Flushing a piece by means of clflush thus observing that piece should be possible without knowing the physical address of the lump since clflush takes the lump's virtual address for this situation in the adversary's address space as its operand  We call a quicker reload amid the Reload stage a watched occasion or perception  We additionally embrace ideas from measurable order and utilize the term false negative to allude to missed perceptions of the casualty's entrance to the checked piece and false positives to allude to watched occasions that are brought about by reasons other than the casualty's entrance to the observed piece We characterize a Flush-Reload convention in which the foe procedure screens a rundown of lumps at the same time and over and again until taught generally  It will first attempt to Reload the principal lump record the reload time and Flush it promptly thereafter At that point it will rehash these means on the second lump the third and so on until the last lump in the rundown  At that point the foe will sit tight for a precisely computed day and age before beginning once again from the primary piece so that the interim between the Flush and Reload of a similar lump is of an objective level 3 from the Reload of the main lump to the end of the holding up period is called one Flush-Reload cycle  An outline of the Flush-Reload convention is appeared in following Fig  
 
 chi k 2 R eI oa din g  chi nk 1  Rel o a di ng I dl e l ooping l 
Here we execute experiments on two systems  One system is a laptop and the second system is a dedicated server facilitating different services  We have installed windows server 2012 on both machines  First of all we used NMap to find OS finger printing the 
FI ng 
chi Id 
AUJS  ng 
 ch k 2 225 us bR eload Cycl e 
 


as 
using the Nassus we found the vulnerabilities available in VMs  Then generated side channel attack on both machines using flush  reload attack to get cryptographic keys and peeks inside the VMs whereas docker provides much more isolation and it was quite difficult to get all keys and peeking mechanism in docker Algorithm Flush Reload Hardware Specification System 1 System2 Use Laptop Dedicate Server 
Windows Server 2012 CPU Intel\256 Core\231 i3-4010U CPU  Intel Xeon 2.S Ghz 1.70Ghz Cores 02 Physical 04 Physical 04 Logical OS Logical RAM 4GB SGB 
 


x86-64 
L1 128 KB 256KB L2 512 KB 01 MB L3 03 MB 08MB Supporting Software Name Function Ubuntu and Windows Ubuntu and Windows server 2008 are being used to Server 2008 host multiple servers on Virtual machines Web Server Docker Container and Docker Microsoft Hyper-V NMap Nessus Docker Container cape a bit of Software in an entire file system that contains everything expected to run code runtime framework instruments framework libraries anything that can be introduced on a server This ensures the product will dependably run the same paying little respect to its surroundings  MS Hyper-V Microsoft Hyper-V codenamed Viridian and some time ago known as Windows Server Virtualization is a local hypervisor it can make virtual machines on 
frameworks running Windows Nmap is a security app used to find has and benefits on a PC organize hence making a IIguide ll of the system Nessus is a remote security checking device which filters a PC and raises a caution in the event that it 
 


225 225\225 225 225 1 111 225 225 
i 
GnuPG and Open SSL Results finds any vulnerabilities that noxious programmers could use to access any PC you have associated with a system GnuPG is an entire and free execution of the OpenPGP standard as characterized by RFC4880 otherwise called PGP  GnuPG permits to encode and sign your information and correspondence highlights a flexible key administration framework and additionally get to modules for a wide range of open key indexes  
 
26 
tt 26 
c e 
140 120 g 0 25 
System A 180    O penSSLD 9 7LR 
E Ubgcryp t 1 6LR             29 
Ubgcrypt 1 6FR 
 
 
_ OpenSS LD 9 7FR _ OpenSSLD  9.7FR 180    O penSSLD 9 7LR O pen SSL1 0.1 FR   III   Op enSSL1 0.1 LR 160 PolarSS L 1.3  3FR   0   Po l arSS L1 3 3LR 
B 
27 29 Encr ypt ions   Ill   O penS SL1 0.1 LR 160 PolarSSL1  3.3FR   0   PolarSS L1  3.3LR Ubgcrypt1 6FR Ubg c rypt1  6LR 27 Encr y pt i ons  2 System 
  
28 28 
140 25 
 


Conclusion 
1 
References 
1 
machine 8 https www.toptal.com/linux/separation-anxiety-isolating-your-system-with-linu\x-namespaces 9 Isolation in Cloud Computing and Privacy-Enhancing Technologies Suitability of PrivacyEnhancing Technologies for Separating Data Usage in Business Processes Prof Dr Noboru Sonehara Prof Dr Isao Echizen Dr SvenWohlgemuth National Institute of Informatics 2-1-2 Hitotsubashi Chiyoda-ku Tokyo sonehara@nii.ac  jp  10  Performance Isolation and Fairness for Multi-Tenant Cloud Storage David Shue Michael J Freedman and Anees Shaikh Princeton University ylBM TJ Watson Research Center 
An Updated Performance Comparison of Virtual Machines and Linux Containers Wes Felter Alexandre Ferreira Ram Rajamony Juan Rubio IBM Research Austin TX fwmf apferrei rajamony rubiojg@us.ibm.com 2 A Unified Operating System for Clouds and Manycore fos David Wentzlaff Charles Gruenwald III Nathan Beckmann Kevin Modzelewski Adam Belay Lamia Youseff Jason Miller and Anant Agarwal 3 Containers and Cloud From LXC to Docker to Kubernetes DAVID BERNSTEIN 4 Containers and Clusters for Edge Cloud Architectures a Technology Review Claus Pahl Irish Centre for Cloud Computing and Commerce IC4  Lero the Irish Software Research Centre Dublin City UniversityDublin 9 Ireland 5 Containerisation and the PaaS Cloud Claus Pahl 6 http://www.slideshare.net/BodenRussell/kvm-and-docker-lxc-benchmarking-w\ith-openstack 7 http://stackoverflow com q uestio ns/1604 7306/how-is-docker d ifferent from-a-no rma I-vi rtua 
It has been observed from experiments that container provides much more isolation among multiple users multi-tenants in cloud virtualization as compared to virtual machines Taking the example of Docker container which is light weight more secure and fast processing virtualization technique and getting much more familiarity due to its characteristics Also Container provides isolation at every instance of virtualization like at process level at file system level network level and at inter process communication lPe level 
 


