Proceedings of the Third International Conference on Machine Learning and Cybernetics, Shanghai 26-29 August 2004 MINING MAXIMAL FREQUENT ITEMSETS FOR LARGE SCALE TRANSACTION DATABASES RAN XIA l,\222 WE1 YUAN I,\222 SHENG-CHAO DING\222 JUAN LIU\222,\222 HUAI-BE1 ZHOU 221Advanced Research Center for Science  Technology, Wuhan University, P R China Computer School, Wuhan University P R China E-MAIL: xiaranxr@tom.com, liujuan@whu,edu.cn Abstract We pmnt a graph-based algorithm MFIminer for mining maximal fquent itemsets 
MFI from transaction databases Our method is especially effident in large transaction databases hecause the performance is not sensitive to the quantity of transactions MFIminer adopts a directed association graph to guide the mining task efficiently It uses the technique of depth-first traversal and complete graph checking to achieve reduction of seamhing time Perfonnance study shows that MFMner outperfom MinMax an algorithm to find 
MFI in both speed and scalability property Keywords association des maximal frequent itemset frequent itemset data mining 1 Introduction Mining frequent itemsets FI or patterns plays a fundamental and essential de in mining association rules which is widely applied in many fields such as consumer market-basket analysis and infemng rules from gene expression data I 21 Apriori and support-confidence framework which is the basic of many algorithms generated recently for mining association rules were originally proposed by 
Agrawal et al 3,4 It was well known that frequent itemsets mining often results in a very large quantity of sets including subsets as well as supersets which lead to a remarkable reduction of both efficiency and effectiveness Therefore people generated concepts of Frequent Closed Itemsets FCI\and Maximal Frequent Itemsets MFI and found the following relationship holds MFZ E FCZ FZ lo Therefore many people focus on FCI mining 5 6 71 or MFI 8 91 mining altemativly Recently due to the strong demand on 
efficiency of practical applications, many itemset-mining algorithms are proposed MinMax SI generates MFI based on depth-first traversal and iterative Furthermore it combines a vertical tidset representation of the database with effective pruning mechanisms GFCG 7 mines FCI by constructing an association graph to represent the frequent relationship between items and employing a recursion to achieve depth-first travel of the graph In this paper we present a novel MFI mining algorithm MFlminer which is based on an association graph with different construction from GFCG 7 It searches the graph in depth-first style Performance evaluation proves that it has a 
remarkable efficiency in very large transaction databases The organization of the paper is as follows The conceptual ideas of mining maximal frequent itemsets and the corresponding graphs are discussed in Section 2 An analysis of the components of our algorithm, MFIminer, is presented in Section 3 Section 4 reports the performance comparison of our method with MinMax SI as well as the scalability study In Section 5 we conclude our work and discuss some future research directions 2 Preliminaries In this section we describe related concepts of maximal frequent itemsets and graph 
Z   i i     i 1 is a complete set of distinct items and a transaction database D is composed of a set of transactions Each transaction T is a set of items such that T C I  and is associated with a unique aansaction identity rid  An itemset is a non-empty set of some items in Z  and k  itemset is 
an itemset containing k items A transaction T is said to contain an itemset X if X c T  The support of itemset X is defmed as the ratio of transactions containing X in T  n itemset X is afrequent itemset 2003 if its support donated as sup\(X  holds sup\(x 2 minsup where 0-7803-6403-2/04/$20.00 WOO4 IEEE 1480 


Proceedmgs of the Third International Conference on Machine Learning and Cybernetics Shanghai 26-29 August 2004 min sup is a given minimum support threshold We can also say that itemset x is frequent if x is a FI A FI X is a closed one frequent closed itemset FCI if there exist no itemset x\222 such that x\2223 x and every transaction containing X also contains x\222  A FI X is a maximal one maximal jkquenr iternset MFI if no superset of it is frequent For example let I 1,2,3,4,5 and D is described as Table 1 Let the minimum support threshold be 1 the set of all MFI is\(1,2,3,4,5 the set of all FCI set of all FI consists of the set 1,2,3,4,5 and its all non-empty subsets 25  1  31 subsets totally Table 1 A database of Transaction is 1,2,3,4,5 1,3 1,3,5 1,4,5 2,4,5 1  and the Itemset 245 135 12345 006 145 A graph is a pair G  V,E of sets where the elements of V are the vertices or nodes or points of the graph G  and the elements of E are its edges If edges of G are ordered pairs a b  where a b are inV  we call G a digraph or directed graph The vertex a is the initial vertex of the edge and b the terminal vertex Two vertices a b of G are said to he adjacent or neighbors if\(a,b  usually written asab is an edge of G  If all the vertices of G are painvise adjacent then G is complete Let G=\(V,E andG\221=\(V\222,E\221 if V\222CV and E\222G E holds Gis a subgraph ofG  or G is a supergraph of G written as GG G  G is said to be a maximal complete graph if there is no supergraph of it is complete  According to each edge vi v  of graph G  if there exists a value w\(vi,vj  usually written as wg for short we say G is a weighted graph and this value is the weight of G  In this paper MFIminer algorithm generates an association graph which is directed and weighted In this graph each vertex presents a unique item, and then every edge and the weight on it indicate the relationship of the two items We assume that items in each transaction are kept sorted in their lexicographic order When item i and item j appear in the same transaction and i  j holds a directed edge from item i to item j is constructed adding 1 to wi  which is initialed to 0 before the construction of graph In an association graph, a subgraph G\222 V\222,E\222 presents a subset where items are exact vertices in the vertex setv\222  Figure 1 shows an example of an association graph constructed from the transaction database in Table 1 To distinguish from the weight value vertices 1,2,3,4,5 are denoted as vl,v2,~3,V4,~5 separately  VI v2 v5 1 v3 v4 Figure 1 The association graph 3 Algorithm Description In this section we present an analysis of different components of MFIminer 223Every subset of a frequent itemset must be frequent\224 141 lays an essential principle for mining frequent itemsets In the association graph, this principle could be described as 223a subset may be a MFl if its corresponding subgraph is a maximal complete graph\224 Therefore the problem of mining maximal frequent itemsets can be decomposed into two subproblems 0 Construct an association graph according to the given transaction database and prune those edges having weights smaller than the minimum support from this Generate all maximal complete graphs from pruned one to be the candidate of MFl and check them to see whether they are frequent graph 0 1481 


Proceedings of the Third International Conference on Machine Learning and Cybernetics Shanghai 26-251 August 2004 3.1 The Construction of Adation Graph To draw an association graph of a given transaction database is to update the weight of edges for every transaction. Given a transactionT  i,,i2;..,ik we will add directed edges imin I\(m,n=1,2;..,k m<n where i points to in  simultaneously adding value 1 to w corresponding to every edge i,in  Figure 2 demonstrates the process of creating a graph of transaction database in Table 1 step by step Practically, all the edges and their weights present the support of 2-itemsets each of which consists of two items joined by the edge To narrow the search space, all edges having weights smaller than the minimum support should be moved out Let minsup=3  Figure 3 shows the pruned graph Different from MFIminer MinMax 7 makes vertices present the item\222s bit-vector which reveal appearance of items in some transactions by setting relevant bits in the bit-vector to value 1 However generally, computers assign 16 bits to an integer variable and 32 bits to a long integer one that is to say a bit-vector can store at most 32 transactions\222 appearance information about a item It is well known that thousands of transactions are included in VI 0 bit-vector to normal array vi r 3 rk v4 Fiyre 3 Pruned association grnph MFIminer adopts the relatively simple graph structure to avoid the capability limitation and uses an adjacent linked list as the data storage structure to achieve efficient storage 3.2 Searching for Maximal Complete Graph If a new vertex is added to a complete grapbG and this new vertex is adjacent to every vertex of G  inferred from the concept of complete graph the flew graph is also a complete one MFTminer searches the pruned graph in depth-first way and every time it travels to a new vertex U 223I 223I 9 Y 223I Figure 2 The construction of graph most databases If a series of bit-vectors together describe the whole appearance information for one item, every time the algorithm visits the item the exact bit in an exact bit-vector should be located which is not easy to control and is time expensive to estimate and locate Therefore large magnitudes of transaction will weak the superiority of it will check whether the new graph including U is a complete one If it is, the algorithm will add U to original graph G and then further to one on the other hand if it is not the will not include the G\222 and go to another vertex at the same time all vertices that must start from U to reach will never be visited in this 1482 


Proceedings of the Third International Conference on Machine Learning and Cybernedcs Shanghai 26-29 August 2004 Parameter Avg Length  Records Items travel According to this principle if no more vertices could be added toG algorithm will return G as a maximal complete graph. Table 2 describes the pseudocode of MFlsubminer a function used to mine all maximal frequent itemsets which starts from a node appointed And then MFlminer can find all MFl in the whole graph by using MFIsubminer to every node in this graph completecheck\(m currentMFl will check whether the node m is adjacent to every node in currentMFI That is to say completecheck\(m currentMFl will check whether the graph G generated by including m into the original complete one G  is also complete. maxcheck\(surrentMFI will check whether currentMFI is a subset of any MFl in MFlset frequenctcheck\(currentMi3 will check whether the support of currentMFI higher than min sup  T 1 O.14.D 1 OK T20.16.D 1 OK 10 20 10K 10K loo0 loo0 4 Performance Evaluation In this section we present a performance comparison of our method with MinMax XI is well as the scalability study All the experiments were performed on a 2.02GHz PC machine with 256 MB memory and Windows XP installed All the programs are written in C language and compiled in MicrosoftNisual C+t 6.0 We test two methods on various datasets including two synthetic datasets generated hy the standard procedure described in 4 and a real one connect-4 used in 121 Table 3 describes the parameters of the synthetic test data sets Table 3 Parameter Settings \(synthetic datasets Table 2 Pseudocode of MFlsubminer Overview  MFIsubmina mines the exact set of maximal frequent itemsets having Starting node c as the first item in the itemset Pseudocode  MFIsubminer\(Starting node c MHset for\(m  c    for\(currentMI3  Q    m.flag=l set all nodes n.flag  0 where n  m if m has no subsequence node or all nodes.flag  I break if completecheck\(m currentMFTj  Ocontinue Add m to currentMFI Curkt node m  the first nodea in m's subsequence having a.flag  0 I if m has no subsequence nodes or all of them have been visited  if m  c return MFIset if maxcheck\(currentMFl  1 and frequentcbeck\(currentMFI  I add currentMFI to MFIset m  backtrack to the node right before m in currentMF1 I I I First in order to evaluate the speed property we run two programs on these three datasets by changing the minimum support threshold Figure 4 5 and 6 shows that 1483 


Proceedings of the Third International Conference on Machine Learning and Cybernetics Shanghai 26-29 August 2004 due to construct an association graph and prune it with minimum support threshold MFIminer has better performance than MinMzy in synthetic datasets as well as real one And then to test the scale up property we generate a series of synthetic datasets having the same average size of the transaction and average size of the maximal potentially large itemsets 5 transactions and 2 items separately hut different database size varying from 10K to 50K transactions Figure 7 present that the performance of MFIminer is not sensitive to the quantity of transactions The good performance of MFIminer lays in the employment of a directed association graph which guides the mining task efficiently. Depth-first traversal makes the algorithm avoid generation of MFI level by level promoting MFIminer to mine maximal frequent itemsets directly And complete graph checking help MFIminer decrease the time wasted in mining little itemsets so that less nodes will be backtracked in the whole process Figure 4 Performance Study on T1O.M.DIOK Figure 5 Performance Study on T20.16.DlOK Figure 6 Performance Study on CoMect-4 Figure 7 Scales up Property Study 5 Conclusion Generally, mining complete set of frequent itemsets is not efficient due to the large number of itemsets including subsets and supersets Mining Maximal Frequent Itemsets provides a practical alternative in large databases mining In this paper we present MFIminer an algorithm for mining Maximal Frequent Itemsets efficiently in large transaction databases The performance evaluation on some synthetic data sets and a real data set proved that this method is better than MinMax in both speed and scales up property in large databases Currently MFIminer has been successfdly employed in mining association rules In the future other methods of data mining like clustering and classification will be combined into MFlminer to make it fit for more applications including discovery of causality between microarrays and cancer 1484 


Proceedings of the Third Intemadonal Conference on Machine Learning and Cybernetics Shanghai 26-29 August 2004 Acknowledgment This work is supported by the National Natural Science Foundation of China\(60301009 and Chenguang project of Wuhan city\(211121009\partially supported by The Foundation of Young Scholars of Ministry of Education of China\(l50118 References I Chad Creighton and Samir Hanash 223Mining gene expression databases for association rules\224 Bioinfonnatics Vol 19, No I pp 79-86,2003 121 C Becquet S Blachon B Jeudy, J-F. Boulicaut, and 0 Gandrillon 223Strong-association-rule mining for large-scale geneexpression data analysis: a case study on buman SAGE data\224, Genome Biology 2002,3\(12 3 R Agrawal T Imielinski and R Srikant 223Mining association rules between sets of items in large databases\224 SIGMOD May 1993 4 R Agrawal and R Srikant 223Fast Algorithms for Mining Association Rules\224 Proc of the 20th Int\222l Conference on Very Large Databases Santiago Chile September 1994 5 Jim Pei Jiawei Han and Runying Mao 223CLOSET An Eficient Algorithm for Mining Frequent Closed Itemsets\224 Proceedings of the ACM SIGMOD Workshop on Research Issues in Data Mining and re~ear~hOO67.1-0067.16 Knowledge Discovery DMKD\222OO Dallas USA May 2ooO 6 Jianyong Wang Jiawei Han, and Jian Pei. \223CLOSET searching for the best strategies for mining frequent closed itemsets\224 In KDW3 pages 236--245,2003 171 Li Li Donghai Zhai and Fan Jm 223A graph-based algorithm for frequent closed itemsets mining\224 Proceedings of the 2003 Systems and Information Engineering Design Symposium 2003 8 Hui Wang, Qinghua Li, Chuanxiang Ma and Kenli Li 223A Maximal Frequent Itemset Algorithm\224 RSFDGrC 2003 484490  D Burdick M Calimlim J Gehrke 223MAFIA A Maximal Frequent Itemset Algorithm for Transactional Databases\224 In Proceedings of the 17th BIBLIOGRAPHY 63 International Conference on Data Engineering pages 443-452 Heidelberg Germany, April 2001 IO Mohammed Javeed Zaki and Karam Gouda 223Fast vertical mining using diffsets\224 KDD pp 326-335 2003 Ill N Pasquier Y Bastide R Taouil and L Lakhal 223Discovering frequent closed itemsets for association rules\224 In Proc 7th Int Conf Database Theory ICDT\222 99 pp 398-416 Jerusalem Israel January 1999 12 R J Bayardo Efficiently mining long pattems from databases In hc 1998 ACM-SIGMOD Int Conf Management of Data SIGMOD 98 pp 85-93 Seattle, Washington June 1998 1485 


ered frequent 2-itemsets and the Apriori downward property is utilized to generate the minimal number of their candidate calendar patterns. Finally, all frequent itemsets and their cal endar patterns are discovered in one shot. Calendar-based temporal association rules are then obtained. Experimental results have shown that our method is more efficient than others References 11 R. Agrawal and R. Srikant. Fast Algorithms for Min ing Association Rules. In Proceedings of the Inrema tional Very Large Database Conference , pages 487 499,1994 2 ]  J. Han, G. Dong, and Y. Yin. Efficient Mining of Par tial Periodic Patterns in Time Series Databases. In Pro ceedings of the Inremational Conference on Data En gineering, pages 106-1 15, 1999 3] C. H. Lee, C. R. Lin and M. S. Chen. Sliding-Window Filtering: An Efficient Algorithm for Incremental Min ing. In Pmceedings of the ACM 10th Intemational Conference on Information and Knowledge Manage ment, pages 263-270,2001 4] Y. Li, P. Ning, X. S. Wang and S .  Jajodia. Dsicovering Calendar-based Temporal Association Rules. Data and Knowledge Engineering, Vo1.44, No.2, pages 193-21 8 2003 5] B. Ozden, S. Ramaswamy, and A. Silberscbatz. Cyclic Association Rules. In Proceedings of the 15th Inter national Conference on Data Engineering, pages 41 2 421,1998 6] S. Ramaswamy, S. Mahajan, and A. Silberschatz. On the Discovery of Interesting Patterns in Association Rules. In Proceedings of the International Very Large Database Corference , pages 368-379,1998 7] J. F. Roddick and M. Spiliopoulou. A Survey of Tem poral Knowledge Discovery Paradigms and Methods IEEE Trans. Knowledge and Data Engineering, Vol 14, Issue 4., pages 75C!-767,2002 3127 pre></body></html 


decreased. However, refer to Fig.6, it brings the following problem  110 100 010 100 100 011 001 111 100 011 100 34 33 34 DSD u u u                         u              101 100 001 100 100 010 011 111 100 011 100 


100 34 33 34 DSD u u u                          u             Fig.6: Over Hiding problem of setting  1 in S No matter the left-hand or right-hand equation, the support of {1, 2} in D' is 0. That is, item 1 and item 2 never appear toge ther, and they are mutual exclusive! This situation almost never happens in the normal database. The attackers may interest in this situation and infer that {1, 2} is hidden deliberately. To hide the sensitive patterns, only need to make their supports smaller than minimum support and need not to decrease their support to 0. To solve the problem, we inject a probability ? which is called Distortion probability into this approach. Distortion probability is used only when the column j of the sanitization matrix S contains only one  1  i.e. Sjj = 1 0 1 d   m k k j i k  S D  m j n i j i  d d d d   1  1     D  i j  h a s   j probability to be set to 1 and 1  j probability to be set to 0 Lemma 1: Given a minimum support ?, and a level of confidence c. Let {i, j} be a pattern in Marked-Set, nij be the support count of {i, j}. ? is the Distortion probability of column j Without loss of generality, we assume that Sij  1. If ? satisfies    D n i j  u  u  V U   a n d    


   c x n xijnx D x ij t          u    1 0 UU V where D| is the number of transactions in D, we can say that we are c confident that {i, j} isn  t frequent in D Proof: If probability policies are not considered and Sij  1, that is, while a row \(transaction according to the matrix multiplication discussed in the previous section, D'tj will be set to 0. We can view the nij original jtiD jtjtjt ijn DDD 21     f o r  e a c h  r o w  t i  c o n t a i n s   i   j   i n  D  a s  realizations of nij independent identically distributed \(i.i.d om variables ijn X X X      2 1      e a c h  w i t h  t h e  s a m e  d i s t r i b u  tion as Bernoulli distribution, B \(1 X i    1  a n d  t h e  f a i l u r e  i s  d e f i n e d  a s  X i    0     i    1  t o  n i j   T h e  p r o b  ability ? is attached to the success outcome and 1?? is attached to the failure outcome. Let X be a random variable and could be viewed as the number of transactions which include {i, j} in D such that jin     2 1    T h u s   t h e  d i s t r i b u t i o n  o f  X is the same as Binomial distribution, X?B \(nij              otherwise nx x n XP ij xnxij x ij 0 1,0     U the mean of X = nij?, and the variance of X = nij?\(1 If we want to hide {i, j} in D', we must let its support in D' be smaller than ?. Therefore, the expected value of X must be s m a l l e r  t h a n     D u V    T h a t  i s     m u s t  s a t i s f y     D n i j  u  u  V U   Moreover, the probability of the number of success which is 


Moreover, the probability of the number of success which is equal to or smaller than     1     u  D V   s h o u l d  b e  h i g h e r  t h a n  c  Therefore, U must satisfy    c x n xijnx D x ij t           u    1 0 UU V  If ? satisfies the two equations, we can say that we are c confident that {i, j} isn  t frequent in D When nij &gt; 30, the Central Limit Theorem \(C.L.T used to reduce the complexity of the equation and speed up the execution time of the sanitization process Moreover, if several entries in column j of S are equal to  1 such as Sij  1, Skj  1, Smj  1  etc, several candidate Dist ortion probabilities such as iU , kU , mU , etc are gotten. The Dis tortion probability of column j, ? j, is set to the minimal candid ate Distortion probability to guarantee that all corresponding pair-subpatterns have at least c confident to be hidden in D 3.3. Sanitization Algorithm Fig.7: Sanitization Algorithm According to the probability policies discussed above, the sanitization algorithm D'n  m = Dn  m  Sm  m is showed in Fig.7 Notice that, if the sanitization process causes all the entries in row r of D' equal to 0, randomly choose an entry from some j Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE where Drj = 1, and set D'rj = 1. It is to guarantee that the size of the sanitized database equals the size of the original database 4. Performance Evaluation 4.1. Performance Quantifying There are three potential errors in the problem. First of all some sensitive patterns are hidden unsuccessfully. Secondly some non-sensitive patterns cannot be mined from D'. And the third, new artificial patterns may be produced. However, the third condition never happens in both of our approach and SWA Besides, we also introduce the other two criteria, dissimilarity and weakness. All of the criteria are discussed as follows Criterion 1: some sensitive patterns are frequent in D'. This condition is denoted as Hiding Failure [10], and measured by     DP DP HF H H , where XPH represents the number of patterns contained in PHwhich is mined from database X Criterion 2: some non-sensitive patterns are hidden in D'. It is also denoted as Misses Cost [10], and measured by      DP DPDP MC 


MC H H H      w h e r e        X P H  r e p r e s s ents the number of patterns contained in ~PH which is mined from database X Criterion 3: the dissimilarity between the original and the sanitized database is also concerned, and it is measured by      n i m j ij n i m j ijij D DD Dis 1 1 1 1    Criterion 4: according to the previous section, Forward Inference Attack is avoided while at least one pair-subpattern of a sensitive pattern is hidden. The attack is quantified by        DPDP DPairSDPDP Weakness HH HH    where DPairS is the set of sensitive patterns whose pair subsets can be completely mined from D 4.2. Experimental Results Table 1: Experiment factor The experiment is to compare the performance between our approach and SWA which has been compare with Algo2a [4 and IGA [10] and is so far the algorithm with best performances published and presented in [12] as we know. The IBM synthetic data generator is used to generate experimental data. The dataset contains 1000 different items, with 100K transactions where the average length of each transaction is 15 items. The Apriori algorithm with minimum support = 1% is used to mine the dataset and 52964 frequent patterns are gotten Several sensitive patterns are randomly selected from the frequent patterns with length two to three items to be seeds With the several seeds, all of their supersets are included into the sensitive patterns set since any pattern which contains sensitive patterns should also be sensitive 0 0.05 0.1 0.15 0.2 0.25 0.3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern H id 


id in g F ai lu re SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern W e ak n es s SP SWA Fig.8: Rel. bet. RS and HF. Fig.9: Rel. bet. RS and weakness Fig.8, Fig.9, Fig.10, Fig.11 and Fig.12 show the effect of RS by comparing our work to SWA. Refer to Fig.8, because the level of confidence in our sanitization process takes the minim um support into account, no matter how the distribution of the sensitive patterns, we still are C confident to avoid hiding failure problems. However, there is no correlation between the disclos ure threshold in SWA and the minimum support. Under the sa me disclosure threshold, if the frequencies of the sensitive patte rns are high, the hiding failure will get high too. In Fig.9, beca use our work is to hide the sensitive patterns by decreasing the supports of the pair-subpatterns of the sensitive patterns, the val ue of weakness is related to the level of confidence. However according to the disclosure threshold of SWA, when all the pair subpatterns of the sensitive patterns have large frequencies, it may cause serious F-I Attack problems. Hiding failure and wea kness of SWA change with the distributions of sensitive patterns 0 0.1 0.2 0.3 0.4 0.5 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern M is se s C o st SP SWA 0 0.005 0.01 0.015 0.02 0.025 0.03 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern D is si m 


m il ar it y SP SWA Fig.10: Rel. bet. RS and MC.  Fig.11: Rel. bet. RS and Dis In Fig.10 and Fig.11, ideally, the misses cost and dissimilar ity increase as the RS increases in our work and SWA. However if the sensitive pattern set is composed of too many seeds, a lot of victim pair-subpatterns will be contained in Marked-Set. And as a result, cause a higher misses cost, such as x = 0.04 in Fig.10 Moreover, refer to the turning points of SWA under x = 0.0855 to 0.1006 in Fig.10 and Fig.11. The reason of violation is that the result of the experiment has strong correlation with the distri bution of the sensitive patterns. Because the sensitive patterns Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE are chosen randomly, several variant factors of the sensitive patt erns are not under control such as the frequencies of the sensiti ve patterns and the overlap between the sensitive patterns if the overlap between the sensitive patterns is high, some sensitive patterns can be hidden by removing a common item in SWA Therefore, decrease the misses cost and the dissimilarity 0 0.5 1 1.5 2 2.5 3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern E x ec u ti o n T im e h  SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set H id in g F a il u re SP SWA 


SWA Fig.12: Rel. bet. RS and time. Fig.13: Rel. bet. RL2 and HF Fig.12 shows the execution time of SWA and our approach As shown in the result, the execution time of SWA increases as RS increases. On the other hand, our approach can be separated roughly into two parts, one is to get Marked-Set which is strong dependent on the data, the other is to set sanitization matrix and execute multiplication whose execution time is dependent on the numbers of transactions and items in the database. In the experi ment, because the items and transactions are fixed, therefore, the execution time of our approach is decided by the setting of Marked-Set which makes the execution time changing slightly 0 0.05 0.1 0.15 0.2 0.25 0.3 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set W ea k n es s SP SWA 0 0.1 0.2 0.3 0.4 0.5 0.6 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set M is se s C o st SP SWA Fig.14: Rel. bet. RL2 and weakness. Fig.15: Rel. bet.RL2 and MC 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set D is si m il a ri ty SP SWA Fig.16: Rel. bet. RL2 and Dis Fig.13, Fig.14, Fig.15and Fig.16 show the effect of RL2 Refer to Fig.13 and Fig.14, our work outperforms SWA no matter what RL2 is. And our process is almost 0% hiding failure 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


