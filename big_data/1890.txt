Fuzzy Reasoning Implemented by Neural Networks Jiriilioiig Nie uiid D A Litiketis Department of Automatic Control  Systems Engineering University of Sheffield, Sheffield S1 3JD U.K Abstract Viewing the given rule-base as defining a global linguistic association constrained by fuzzy sets approximate reasoning is implemented here by a Backpropagation Neural Network BNN with the aid of the fuzzy ste tbeory By paying particular attention to the mpability of generalization of the BNN the underlying principles have been examined in detail using two examples The simulation results not only indicate the feasibility of the BNN-based approach but also reveal 
some deeper similarities which exist in the two methods which may have some important implications for future studies of fuzzy control 1 Introduction Fuzzy or approximate reasoning AR employed by rule-based fuzzy expert systems or fuzy logic controllers can be regarded as a process by which a set of imprecise conclusions is deduced from a collec tion of imprecise premises l Although the rcasoning can be carried out by fuzzy logic-based re3soning algorithms here we consider another possibility of implementation of AR by neural networks. The motiva tion comes mainly from the fact that the goals of AR and mapping 
neural networks are very similar, that is to perform some kind of approximation or intcrpolation, although the representation the information they are dealing with is different In particular we claim that neural networks inherently possess some fuzzi ness, the source of approximation in AR which is displaycd in the network via net-computing in the form of generalization This view is of pnrticular importancc in the net-based fuzzy control as will be discussed in section 5 Instead of seeking a structure mapping from a fuzzy reasoning system to a neural network this paper is intended to find 1 
functional mapping from the fuzzy logic-based algorithm to the network-based approach By viewing the given rule-base as defining a global linguistic association constrained by fuzzy sets the approximate reasoning is implemented by Backpropagation Neural Networks BNN with the aid of fuzzy ste theory By paying particular attention to the capability of generalization of the BNN the underlying principleshave been examined in dctnil using two examples a small demonsmion at the linguistic level and a more realistic problem of multivariable fuzzy control of blood pressure The simula tion results not only indicate the feasibility of the BNN-based approach, but also reveal some deeper simi larities 
which exist in the two methods which inay hive some important implications for future studies of fuzzy control In addition this work may be considcred as another appliction example of the BNN in the case of continuous ouputs and on a relatively larger scale in the second exmple,the BNN has 26 inputs and 16 outputs with a total of 2013 weights and thresholds\Furthermore, the work may provide evidence to support the argument that net-computing is in fact knowledge representation as claimed very recently by Pa0 2 2 Formulation of the problem Assume that the system has 
n inputs and m outputs denotcd by XI X2;-Xn and YI.Y2 Y Further more, it is assumed that L IF situafiori THEN acfiort  rules, and n inputs in the IF pat and m outputs in the THEN part are connected by linguistic connectives ALSO and AND respectively Then the problem may be described as follows Given rules RULE ALSO RULE ALSO  ALSO RULEL where RULE has the form IF XI is 4 AND X2 is A AND 
 AND X is A THEN YI is B AND Y2 is A AND  AND U is 4  j=1,2  L Given input datx XI is CI AND X2 is C2 AND  AND X is C To find output daw Y1 is DI AND Y2 is D2 AND  AND U is D where X and Y are linguistic 
vnri;ibles whose values we taken from the discourses of universe which are defined on the correspondin$ discourses, and rcpresent some fuzzy concepts such as big medium and small etc More prcciscly fuzzy subsets A Ci B/I Dk are characterized by the corresponding membership functions Aj\(ii  I/;+[o 11  Ci\(u I/i+[o 11  v vk 0.1 U Vk with Ui=\(~;l 142 e U and VL=\(V k2 vkrk Ai Ci Bi Dk are fuzzy Subsets 0-7803-05594 92 53.00 Q 1992 IEEE 11-702 


and Dr\(vd  v O,I The above problem can be solved using fuzzy-logic-based algorithms in the following two ways The first one may be called the relational mauix method The basic idea is first to create a relational matrix R based on the given L rules using some logical operators Then the outputs for a specific input is determined by Zadeh's compositonal rule of inference The other solution is a stnightforward one in the sense that the algorithms closely follow the inferencing procedures used by data-driven reasoning systems and which take the fuzzy variables into account 3 The basic idea is first to treat the L rules one-by-one and then to combine the individual results to give a global output 3 Solutions using neural networks Although the Bacclrpropagation Ncuml Network BNN the Basis Function Network BFW and Pro bability Function Network PFW are good candidates for our purpose, the well-known BNN 4 structrure has been chosen for the current study In what follows  we will formulate the BNN and AR from the functional viewpoint so as to obtain a formal equivalence between them From the mapping perspective the goal of the BNN is to perform an approximate implementation of an unkown mapping  from a compact set Pc R an NI demensional Euclidean space to an NO demen sional Euclidean space  fl  RNo by an approximator  consisting of layered and massively con nected processing units Assume that the topology of the network is specified Then constructing  is equivalent to determining the parameters of the BNN based on a set of selected examples More specifically assume that we are given a set of examples  U v1    U  with Up being drawn from fi and 9 being supposed to be satislied with the unknown function  i.e 9  U Denote all the weights and thresholds in the structure-specified BNN as a panmeter vector w Furthermore the P desired and actual outputs cahculated from the BNN at P discrete sample points are denoted as  U U e 41'\(d and 4.3 U w 2*\(u2 w  U w respectively Then the problem can be simply for mulated as to select w in such a way that a specific error fuction, say a quadratic one 3s denoted by N 1 is minimized provided that the strucure of the BNN is specified a priori We can expect with some confidence that the approximator constructed from only a finite set of points in fl will work satisfactorily over the whole space of P Based on the idea discused above the problem of the approximate reasoning presented in section two may be solved if it is formulated in the same way as in the BNN that is to construct an inference engine Y based on the given L rules and to genenlize to unseen situations in the domain of interest by directly manipulating the engine Clearly a given rule is malogus to an example in the BNN L IF-THEN state ments may designate an implicit and global relationship between the situation set and action set To be more specific denote the situation variable X XI X2  X md the action variable Y  YI Yz  Y I X and Y take linguistic labels, represented by fuzzy sets A and Bi defined on the corresponding universes Vi and Vr as their values for example X=Xp AY A  Aa Further let Y\(X Y*\(Xp,w be the desired action and the actual action calculated from the inference engine Y with respect to P situations Xp p19 where W denotes a set of parameters deter mining the  Then by analogy to equ 1 the problem may be reformulated to select W in such a way that 1 2 E\(w   I 1$\(4  U.W I I 2 1 E\(w  2 I IY\(X  Y'\(X,W I I is minimized such that an action Y=[D D2  D will be approximately deduced from the constructed inference engine Y when a new situation X=[CI CZ  C is encounted Thus the Y accomplishes an approximate implimenution of linguistic mapping from one linguistic set of the situation domain to another linguistic set of the action domain It is obvious from the above discussions that the AR problem can be solved by the network method if the linguistic values can be mnslatcd into numerical forms suitable for the use of the BNN and the numerical outputs from the BNN can be interprcted as liguistic labels. The graded membership function is the first choice for the former tnnsfomntion aid some methods for linguistic approximation may be used for the latter conversion It is noted that althougli the reasoning system itself has n iuputs and m outputs the BNN using the fuzzy set representation will have sI+s2  s,,inputs and rl+r2    r outputs with 11-703 


si  rk being the cardinality of Ui aid V on which fuzzy sets are delined Therc exist other possibilities for the hbeVnumencal value tmnsformation one of which will be discussed in section 5 Klle*ls KIZe  ACO   1.0 24.761 MAP 0.6636 76.38 f K~~~~P  sTZ+I sT2i-I 4 Reasoning capability a linguistic study The main objective of the study in this section is two-fold i.e to investigate the capability of the BNN-based reasoning system in the linguistic levcl and to examine the effect of the number of hidden units upon this capability by means of the simulation on a small but typical problem Suppose we are given five rules each of which has the form of IF X is A THEN Y is B with the pair A B being specified as PB NB PM, NM ZR ZR NM PM and NB PB where PB PM ZR NM and NP stand for Positive-Big Positive-Medium Zero Ncgative-Medium and Negative-Big respectively For simplicity we assume that all the fuzzy sets representing the above linguistic labels are defined on the same discourses of universe i.e U=V=\(-4,-3.-2,-1,0,1.2,3,4 and all the membership func tions are of triangular form located at 4 2 0 2 4 The BNN consists of one input and one output layer with 9 input and 9 output units, corresponding to the cardinalities of U and V and one hiden layer with a variable number of units By setting the initial weights to be uniformly distributed on OS OS the net was trained by the presentation of five rules with the learning rate being 0.8 and the number of hidden units being 5 15 30 and 50 respectively The tmining process was stopped when the sum of squared error within one cycle was less than 0.0005 Then the BNN was tested using the following two sets of linguistic labels The first set of linguistic inputs used so-callcd linguistic hedges to modify the basic labels in the rules Here two hedges very and nwre or less were used and defined as very A A2 and more or less A AoJ Thus ten different inputs could be used to test the performance For example if very-positive medium is presented to the BNN the expected outputs should be very-riegative-medium Instead of merely altering the shapes of the basic labels as above the second set of linguistic inputs was concerned with changing the centnl values with respect to the bllsic labels representing a much harder situation than the previous one since the inputs are substantially different from that in the given rules It is convenient to interpret the linguistic labcls as fuzzy numbers meaning linguistically that  X is about U  With the same trained BNN Fig 2 gives the results corresponding to the expected outputs of about 3 1, -1 3 At first sight one may think that the results are not as good as would be expected However if we calculate the area under the different curves and accept this quantity as a global measure of the perfor mance, the results are remarkably good For example when the expected output is about 1 the absolute differences between the expected and actual mas calculated using a weighted sum are 0.1552 0.1141 0.1142 and 0.0534 corresponding to 5 15 30 and 50 hiddcn units respectively It should be noted that the generdization performance of the BNN does not monotonously increase with an increase in the number of the hidden units To clarify this point we adopt another global measure namely the centre of gravity COG which is an important quantity in fuzzy control where it is used to produce a non-fuzzy output For the present problcm the averaged differences between the expected and actual COG were 0.1239 0.0912 0.1109 and 0.0965 corresponding to 5 15.30 50 hidden units respec tively indicating that at least in the current context the rmoning performance could be degraded with too few or too many hidden units 4f ASNP 3 This problem has been investigated by the authors 3 using the logic-based fuzzy control approach For the purpose of comparisons we employ the same architecture as used in 31 except that here the fuzzy 11-704 


controller is implemented using the BNN-based method The control system as shown in Fig.3 consists of two separated control loops CO/DOP and MAP/SNP with the aid of a simple compensator to reduce the interactive effects For simplicity the two controllers exh comprising two linguistic inputs error e and change-inerror ce and one liguistic output U are assumed to be idcntical and thcrcfore only one of them is described Suppose that dl the discourses of unvcrse have the sane form consisting of 13 integers ranged from 6 to 0 to 6 and that seven Linguistic hbcls negative-big \(medium, small\zero, and positive-big medium small are used They are dclined by the me tmgular form with the central values located at 6.4 2.0.2.4.6 respectively Thus the resultant BNN has 26 inputs and 13 outputs If one hidden hyer with 50 units is used the BNN will have a total of 2013 adjustable parameters Two phases off-line training and onrline application are needed Using the given 33 rules as shown in Table 1 the BNN was mined with 50 hidden units and a leaning rate of 0.02 Although the actual out puts of the BNN are in the interval OJ in xcordcnce with the range of a sigmoid nonlinear active func tion it has been found that a faster convergency could be achieved if linear active functions in the output layer are used as illustrated in Fig.4 To investigate the elfects of the number of rules upon the reasoning performance a set of 16 rules selected from 33 rules as marked with  in Table 1 was also tnined As expected the sum of the squared error decreases more quickly as shown in Fig.4 Because the measured error e change-in-error ce aid the required control U are numerid the e and ce have to be fuzzified into fuzzy sets and the fuzzy scts output of the BNN must be defuzzified into a real number during the application stage Here singleton fuzzification and COG defuzzification methods m employed However if the linguistic labels are viewcd as fuzzy numbers and the BNN is trained with the central values of the fuzzy numbers only the complexity of the controller is greatly reduced and the efficiency in speed and storage is dnmaticaly increased due to the fact that the number of inputs and out put is reduced to 2 and 1 and neither fuzzilication nor defuzzification is needed To verify this possibility the BNN with 15 hidden units was trained and tested using 33 and 16 rules respectively The results are reported next After appropriate training the BNN was used as the controller in the system The capability of the generalization of the BNN was evaluated indirectly by the control performances measured by two fre quently used indices integnl of square of the error ISE and integral of time and absolute error product ITAE for each output We considered four cases the BN"s with linear active functions in the output layer mined by fuzzy sets with 33 arid 16 rules \(dcnotcd by FS-33 and FS-16 and by fuzzy numbers with 33 and 16 rules FN-33 and FN-16 Tiible 2 shows the simulation results when the system was subject to square-like inputs as shown in Fig 6 The results obtained using the logical-based method are also included in Table 2 for comparison purpose Due to space limitation only the output responses in the case of FS-33 are presented in Fig.5 The following conclusions can be drawn from the results a the BNN possesses a high ability to generahe a very useful property for fuzzy reasoning This is pmicularly demonstrated by the case of FS-33 and FS-16 where the measured singleton inputs were very different from the Wining inputs;\(b it is possible to use fewer rules to obhin a valid generalization as illustrated by FS-16 and FN-16 c perhaps the most important conclusion from the rcsults is that the BNN trained by the fuzzy numbers can produce performances 3s good 3s or even better than, the one trained by the fuzzy sets as indicated by the case of FS33 wrt FN-33 or FS-16 wrt FN-16 d the similar pcrfomances obtained by the logic-based and the BNN-based reasoning approaches suggest the feasibility of the BNN for the application of fuzzy control 6 Conclusion By paying particular attention to the capabilily of generalization of the BNN it has been demon stmted that a forward-chaining fuzy reasoning system with parallel rule-bases cm be implemented within the framework of neural networks. The studies into the BNN-based fuzzy controller suggest that, besides a seeming resemblence between rules and patterns in the logic-based and BNN-based approaches, there exists a deeper similarity in the information processing aspect in them namely fuzziness vs. distributiveness The authors believe that this suggestion has some important implications for the development of fuzzy control under the hework of neural networks Some studies along these lines are currently in progress REFERENCES I L.A Zrrdeh Outline of a new approach to the a~nlysis of of complex systems and decision processes IEEE Trans Syst Mun Cybern 3  pp 2844 1973 11-705 


Y.-H Pm Sr D J Sobajic Neural nelworks arid knowlcdge engineering IEEE Trans Knowledge Md Dufa Engineering 3 pp 185-197 1991 D A Linkcns  Junhong Nic A unificd rcd tiinc approxiinate rewning approach for use in intelligent can wok Part 1  2 subtnittcd 1991 D Rumelhart G Hinton and R Wiliiuns Lcvning internal rcprescntation by error propagation in Parallel Dis tributed Processing  Vol 1 Rumellivt CG McClellhd Eds MIT press.1986    0.8   0.6  0.8 0.6 0.4 0.2 0.4 e 0 2  4    I a 1  Fig.1 The linguistic inputs of very  more or less hidden units 30 hidden units  15   0.8 0.6 0.4 0.2    l about-+3 about-+l n I  0.8 0.6 0.4  3-2-1 0 12 3 4 about 1 about--3 Fig.2 The linguistic inputs of about n-706 


I I Fig.3 BNN-based fuzzy control system FN-16 MMC  l6rules-Nonlinear ____ 16rules-Linear  33rules-Nonlinear   33rules-Linear 16.78 810.27 3.50 558.87 16.77 808.43 3.48 557.06 Training Number\(*50 Fig.4 SSE vs training cycles Table 2 Control Performances NUMBERS OF SAMPLE Fig.5 Output responses of the system with net of FS-33 Table 1 Control rules cl NB\(NM\(NS ZR IPS iPM IPB 1 E I  16 rules II-707 


 I                  222                                I I m 2s a Io 40 I 20 10 I..sdlima Irnssap Fig 7 Extracted measusement trajectories for experiment I compare with Figure 6 Fig 10 Range VI time for detected rems for experiment 2 Returns that are not grouped into a mjectary are discarded 0.5 Oi I 5 4 8 2 lrnMonirnl Fig 8 Estrmated tralectory and map for experiment 1 c        221          2236     I A*+-&s     2       ti i if t  o 4 2 0 1 1 6 I 10 I2 1 16 e Fig 9 Canesian projection of detected returns for experiment 2 i yI 40 I a 10 Wa*Ium,_ Fig 11 Extracted measusemem trajectories far experiment 2 IS 6 Fig 12 Raw data estimated sensor uajecton and object locations far experiment 2 969 


appear complex and hard-to-interpret when considered via Cartesian projection are much easier to explain us ing segmented echo trajectories The system incorporates automatic fixation which we believe is an essential part of this type of sonar interpretation approach This has been applied for example to good success in Kuc's work on object recognition 14 The problem of balancing multiple fixations is an unexplored problem \(for example observation of known objects to provide good navigation vs observation of unknown objects for mapping Future work will address 1 3-D tracking 2 the incorporation of spectral information and 3 extension to rough surfaces The primary motivating application for this work detection of undersea buried mines via low frequency synthetic aperture sonar is inherently a 3-D problem Recently we have obtained a large amount of data at the GOATS 2002 experiment using a synthetic aperture sonar that provides this information via use of two eight-element arrays The formulae presented in this paper apply in two dimensions the three dimensional equivalents are presently being derived and tested and initially results are very promising The issues of exploiting spectral information and cop ing with rough surfaces are inter-related The study of echo reflection from rough surfaces has been studied by Bozma and Kuc 3 The matching of new data to previ ously mapped features \(for recognition andlor navigation purposes is tremendously important This paper has not addressed this issue, hut it is anticipated that the trajectory segmentation method will be a valuable tool for early processing as input to recognizing previously mapped features Acknowledgments The authors are grateful to the entire MIT GOATS 2002 team, including H Schmidt P Newman M Bosse D Eickstedt J Edwards W Xu T.C Liu R. Damus J Morash S Desset J Dryer C Bohner and M Grund The authors also thank P Damhra for his efforts in creation of the robotic gantry for the testing tank This research has been funded in part by NSF Career Award BES-9733040, the MlT Sea Grant College Program under grant NA86RG0074 project RCM-3 and the Office of Naval Research under grants N00014-97-0202 and N00014-02-C-02 10 VIII REFERENCES I W Au The Sonar of Dolphins New York Springer Verlag 1993 121 B Barshan and R Kuc Differentiating sonar reflections from corners and planes by employing an intelligent sen sor IEEE Transactions on Panern Analysis and Machine Intelligence PAh41-12\(6 lune 1990 31 0 Bozma and R Kuc Characterizing pulses reflected from rough surfaces using ultrasound J Acoustical Society of America 89\(6 lune 1991 141 0 Bozma and R Kuc Single sensor sonar map building based on physical principles of reflection In Pmc IEEE Int Workshop on Intelligent Robots and Systems 199 1 5 V Braitenberg Vehicles: Experiments in Synthetic Psy chology The MIT Press 1984 61 R A Brooks Cambrian Intelligence The Early History of the Nw AI The MIT Press 1999  J Edwards, H. Schmidt and 1 LePage Bistatic synthetic apenure target detetection and imaging with an auv IEEE J Ocean Engineering 26\(4 2001 181 A Freedman A mechanism of acoustic echo formation Acustica 12\(1 1962 9 W E L Grimson Object Recognition by Computer The Role of Geometric Constraints MIT Press 1990 With contributions from T Lozano-Perez and D P Hut tenlocher IO A Heale and L. Kleeman Fast target classification using sonar In Pmc IEEE Inr Workshoo on Intellinent Robots and Systems pages 14461451 2601  Ill B K P Horn Robot Vision MIT Press 1986 I121 L Kleeman and R Kuc Mobile robot sonar for target localization and classification. Technical Repolt ISL-9301 Intelligent Sensors Laboratory Yale University, 1993 Perception as Bayesian Inference 1996 Fusing binaural sonar information for object recognition In IEEWSICWRSJ International Conference on Multisensor Fusion and Integrarion for Intelligent Sys tems pages 127-135 1996 I51 R Mann and A Jepson Non-accidental features in learning In AAAI Foll Symposium on Machine Learning in Vision 1993 161 D Mm Vision New York W H Freeman and Co 1982 I71 B A Moran I I Leonard and C Chryssostomidis Curved shape reconstruction using multiple hypothesis tracking IEEE J Ocean Engineering 22\(4 1997 I81 I N Newman Marine Hydrodynamics The MIT Press 1977 191 H Peremans K Audenaen and C J M Van A high resolution sensor based on tr-aural perception IEEE I131 D C bill and W Richards editors I41 R Kuc  Transactions on Robotics And Automation 9 1 Feb 1993 USA 20 W Richards editor Natural Computation The MIT Press 1988 PI1 W Richards I Feldman and A. Jepson From features to perceptual categories In British Machine Vision Confer ence 1992 22 R J Rikoski John I Leonard and Paul M Newman Stochastic mapping frameworks In IEEE International Conference on Robotics and Automation 2002 1231 K D Rolt Ocean platform and signal processing effects on synthetic aperture sonar performance Master's thesis MIT Februaty 1991 High-Level Vision Object Recognition and Visual Cognition The MIT Press 1996 24 S Ullman 970 


I Plenary Panel Session J Future Directions in Database Research  456 Chair Surajit Chaudhuri Microsoft Corporation Panelists Hector Garcia-Molina Stanford University Hank Korth, Bell Laboratories Guy Lohman IBM Almaden Research Center David Lomet Microsoft Research David Maier Oregon Graduate Institute I Session 14 Query Processing in Spatial Databases I Chair Sharma Chakravarthy University of Florida Processing Incremental Multidimensional Range Queries in a Direct Manipulation Visual Query Environment  458 High Dimensional Similarity Joins Algorithms and Performance Evaluation  466 S Hibino and E Rundensteiner N Koudas and K.C Sevcik Y Theodoridis E Stefanakis and T Sellis Cost Models for Join Queries in Spatial Databases  476 Mining Association Rules Anti-Skew Algorithms  486 J.-L Lin and M.H Dunham Mining for Strong Negative Associations in a Large Database of Customer Transactions  494 A Savasere E Omiecinski and S Navathe Mining Optimized Association Rules with Categorical and Numeric Attributes  503 R Rastogi and K Shim Chair: Anoop Singhal AT&T Laboratories S Venkataraman J.F Naughton and M Livny Remote Load-Sensitive Caching for Multi-Server Database Systems  514 DB-MAN A Distributed Database System Based on Database Migration in ATM Networks  522 T Hara K Harumoto M Tsukamoto and S Nishio S Banerjee and P.K Chrysanthis Network Latency Optimizations in Distributed Database Systems  532 I Session 17 Visualization of Multimedia Data I Chair Tiziana Catarci, Universita di Roma 223La Sapienza\224 W Chang D Murthy A Zhang and T.F Syeda-Mahmood Global Integration of Visual Databases  542 X 


The Alps at Your Fingertips Virtual Reality and Geoinformation Systeps  550 R Pajarola l Ohler P Stucki K Szabo and P Widmayer C Baral G. Gonzalez and T.C Son Design and Implementation of Display Specifications for Multimedia Answers  558 1 Session 18 Management of Objects I Chair: Arbee Chen National Tsing Hua University P Boncz A.N Wilschut, and M.L. Kersten C Zou B Salzberg, and R Ladin 0 Wolfson S Chamberlain S Dao L Jiang, and G. Mendei Flattening an Object Algebra to Provide Performance  568 Back to the Future Dynamic Hierarchical Clustering  578 Cost and Imprecision in Modeling the Position of Moving Objects  588 ROL A Prototype for Deductive and Object-Oriented Databases  598 A Graphical Editor for the Conceptual Design of Business Rules  599 The Active HYpermedia Delivery System AHYDS using the M Liu W Yu M Guo and R Shan P Lang W Obermair W Kraus and T Thalhammer PHASME Application-Oriented DBMS  600 F Andres and K. Ono S Chakravarthy and R Le S Mudumbai K Shah A Sheth K Parasuraman and C Bertram ECA Rule Support for Distributed Heterogeneous Environments  601 ZEBRA Image Access System  602 Author Index  603 xi 


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


