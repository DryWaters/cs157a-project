Frequent Subgraph Discovery Michihiro Kuramochi and George Karypis Department of Computer Science/Army HPC Research Center University of Minnesota Minneapolis MN 55455  kuram, karypis} @cs.umn.edu Abstract rectly modeled with the traditional market-basket transac As data mining techniques are being increasingly ap plied to non-traditional domains existing approaches for finding frequent itemsets cannot be used as they cannot model the requirement of these domains An alternate way of modeling the objects in these data sets is to use graphs Within that model the problem of finding 
frequent pat terns becomes that of discovering subgraphs that occurfre quently over the entire set of graphs In this paper we present a computationally efJicient algorithm for finding all frequent subgraphs in large graph databases We evalu ated the performance of the algorithm by experiments with synthetic datasets as well as a chemical compound dataset The empirical results show that our algorithm scales lin early with the number of input transactions and it is able to discover frequent subgraphs from a set of graph trans actions reasonably fast even though we have to deal with 
computationally hard problems such as canonical labeling of graphs and subgraph isomorphism which are not neces sary for traditional frequent itemset discovery 1 Introduction tion approaches An alternate way of modeling the various objects is to use undirected labeled graphs to model each one of the object's entities-items in traditional frequent itemset discovery-and the relation between them In particular each vertex of the graph will correspond to an entity and each edge will correspond to a relation between two entities In this model both vertices and edges may have labels asso 
ciated with them which are not required to be unique. Using such a graph representation the problem of finding frequent patterns then becomes that of discovering subgraphs which occur frequently enough over the entire set of graphs The key advantage of graph modeling is that it allows us to solve problems that we could not solve previously For instance, consider a problem of mining chemical com pounds to find recurrent substructures We can achieve that using a graph-based pattern discovery algorithm by creat ing a 
graph for each one of the compounds whose vertices correspond to different atoms and whose edges correspond to bonds between them We can assign to each vertex a label corresponding to the atom involved \(and potentially its charge and assign to each edge a label corresponding to the type of the bond and potentially information about their relative 3D orientation\Once these graphs have been created, recurrent substructures across different compounds become frequently occurring Related Work Developing algorithms that discover all frequently occurring subgraphs in a large graph database is particularly challenging 
and computationally intensive as graph and subgraph isomorphisms play a key role through out the computations The power of using graphs to model complex datasets has been recognized by various researchers in chemical do main 18 17,5,4 computer vision  12 141 image and ob Efficient algorithms for finding frequent itemsets-both sequential and non-sequential-in very large transaction databases have been one of the key success stories of data mining research 2 1 22 9 31 We can use 
these itemsets for discovering association rules for extracting prevalent patterns that exist in the datasets or for classification Nev ertheless, as data mining techniques have been increasingly applied to non-traditional domains such as scientific, spa tial and relational datasets, situations tend to occur on which we can not apply existing itemset discovery algorithms be cause these problems are difficult to be adequately and cor This work was supported by NSFCCR-9972519 HA-9986042 ACI 9982274 by Army Research Office contract DmAAG55-98-1-0441 by the 
DOE ASCI program,.and by Army High Performance Computing Re ject retrieval 6 and machine learning  10,201 In particu lar, Dehaspe et al 5 applied Inductive Logic Programming search Center contract,number DAAH04-95-C-0008 Access to comoutine ILp t Obtain frequent patterns in the toxicology evalua Y facilities was provided by the Minnesota Supercomputing Institute tion problem 18 ILP has been actively used for predicting 0-7695-1 119-8/01 117.00 0 2001 IEEE 313 


carcinogenesis 17 which is able to find all frequent pat terns that satisfy a given criteria It is not designed to scale to large graph databases however and they did not report  any statistics regarding the amount of computation time re quired Another approach that has been developed is using a greedy scheme 20 101 to find some of the most prevalent formance of FSG on the same chemical compound dataset used by AGM Our results show that FSG is able to find all the frequent connected subgraphs using a 6.5 minimum support in 600 seconds 2 Frequent Subgraph Discovery find all frequent induced subgraphs in a graph database that satisfy a certain minimum support constraint A subgraph G  Vs E of G  V E is induced if E contains all the edges of E that connect vertices in V AGM finds all frequent induced subgraphs using an approach similar to that used by Apriori  which extends subgraphs by adding one vertex at each step Experiments reported in  1 13 show Notation D t k-\(sub gk Ck Fk cl\(gk Description A dataset of graph transactions A transaction of a graph in D A sub with k edges A k-subgraph A set of Candidates with k edges A set of frequent k-subgraphs A canonical label of a k-graph gk datasets and it required 40 minutes to 8 days to find all fre quent induced subgraphs in a dataset containing 300 chem ical compounds as the minimum support threshold varied from 20 to 10 Our Contribution In this paper we present a new algo rithm named FSG for finding all connected subgraphs that appear frequently in a large graph database Our algorithm finds frequent subgraphs using the same level-by-level ex pansion adopted in Apriori 2 The key features of FSG are the following 1 it uses a sparse graph representation which minimizes both storage and computation 2 it in creases the size of frequent subgraphs by adding one edge at a time, allowing to generate the candidates efficiently 3 it uses simple algorithms of canonical labeling and graph isomorphism which work efficiently for small graphs and 4 it incorporates various optimizations for candidate gen eration and counting which allow it to scale to large graph databases We experimentally evaluated FSG on a large number of synthetic graphs that were generated using a framework similar to that used for market-basket transaction genera tion  For problems in which a moderately large number of different types of entities and relations exist FSG was able to achieve good performance and to scale linearly with the database size In fact FSG found all the frequent con nected subgraphs in less than 500 seconds from a synthetic dataset consisting of 80000 graphs with a support threshold of 2 For problems where the number of edge and ver tex labels was small, the performance of FSG was worse as the exponential complexity of graph isomorphism dom inates the overall performance We also evaluated the per The key restriction in our problem statement is that we are finding only subgraphs that are connected This is mo tivated by the fact that the resulting frequent subgraphs will be encapsulating relations \(or edges\between some of entities \(or vertices of various objects Within this con text connectivity is a natural property of frequent patterns An additional benefit of this restriction is that it reduces the complexity of the problem as we do not need to con sider disconnected combinations of frequent connected sub graphs In developing our frequent subgraph discovery algo rithm, we decided to follow the structure of the algorithm Apriori used for finding frequent itemsets 2 because it achieves the most effective pruning compared with other algorithms such as GenMax dEclat 22 and Tree Projec tion I The high level structure of our algorithm FSG is shown in Algorithm 1 Edges in the algorithm correspond to items in traditional frequent itemset discovery Namely as these algorithms increase the size of frequent itemsets by adding a single item at a time our algorithm increases the size of frequent subgraphs by adding an edge one by one FSG initially enumerates all the frequent single and double edge graphs Then based on those two sets it starts the main computational loop During each iteration it first generates candidate subgraphs whose size is greater than the previous frequent ones by one edge Line 5 of Algorithm 1 Next it counts the frequency for each of these candidates and The algorithm presented in this paper can be easily extended to di rected graphs 314 


prunes subgraphs that do no satisfy the support constraint Lines 7-1 1 Discovered frequent subgraphs satisfy the downward closure property of the support condition which allows us to effectively prune the lattice of frequent sub graphs Algorithm 1 fsg\(D 0 Frequent Subgraph I F t detect all frequent I-subgraphs in D 2 F2 t detect all frequent 2-subgraphs in D 3 kt3 4 while Fk  0 do 5 Ck t fsg-gen\(Fk 6 7 gk.count t o 8 9 IO 11 12 ktk+l 13 return F F2    Fk--2 for each candidate  E Ck do for each transaction t E D do if candidate 8 is included in transaction t then gk count t gk count  1 F t gk E ck I gk.count 2 01~1 In Section 2.1 we briefly review some background is sues regarding graphs Section 2.2 contains details of can didate generation with pruning and Section 2.3 describes frequency counting in FSG 2.1 Graph Representation Canonical Labeling Sparse Graph Representation Our algorithm uses sparse graph representation to store input transactions, in termediate candidates and frequent subgraphs Each one of the transactions, candidates and discovered frequent sub graphs is stored using adjacency-list representation while our canonical labeling described in Section 2.1 is based on adjacency matrix representation Thus after determining canonical label for a subgraph we convert its canonical ad jacency matrix back into adjacency lists. This adjacency-list representation saves memory when input transaction graphs are sparse and speeds up computation Canonical Labeling Because we deal with graphs not itemsets, there are many differences between our algorithm and the traditional frequent itemset discovery A difference appears when we try to sort frequent objects In the tra ditional frequent itemset discovery we can sort itemsets by lexicographic ordering Clearly this is not applicable to graphs To get total order of graphs we use canonical labeling A canonical label is a unique code of a given graph  16,7 A graph can be represented in many different ways, depending on the order of its edges or vertices Nev ertheless, canonical labels should be always the same no matter how graphs are represented, as long as those graphs have the same topological structure and the same labeling of edges and vertices By comparing canonical labels of graphs we can sort them in a unique and deterministic way regardless of the representation of input graphs We denote a canonical label of a graph g by cl\(g It is easy to see and Isomorphism that computing canonical labels is equivalent to determin ing isomorphism between graphs, because if two graphs are isomorphic with each other, their canonical labels must be identical Both canonical labeling and determining graph isomorphism are not known to be either in P or in NP complete 7 We use a straightforward way of determining a canoni cal label. Using ajuttened representation of the adjacency matrix of a graph by concatenating rows or columns of an adjacency matrix one after another we construct a list of in tegers Regarding this list of integers as a string we can obtain total order of graphs by lexicographic ordering To compute a canonical label of a graph we try all the permu tations of its vertices to see which ordering of vertices gives the minimum adjacency matrix. To narrow down the search space we first partition the vertices by their degrees and la bels, which is a well-known technique called vertex invari ants 16 Then we try all the possible permutations of ver tices inside each partition. Once we determine the caninical adjacency matrix we convert our adjacency-list represen tation of the subgraph so that the ordering of vertices in the canonical adjacency matrix is reflected All compar isons of subgraphs are done with the canonically reordered adjaceny-list representation Further details can be found in 13 Isomorphism In our algorithm we need to solve both graph isomorphism and subgraph isomorphism Graph iso morphism is a problem to determine whether given two graphs g1 and g2 are isomorphic namely, to find a map ping from a set of vertices to another set Automorphism is a special case of graph isomorphism where 91  92 which means to find a mapping from a graph to itself Subgraph isomorphism is to find an isomorphism between 91 and a subgraph of g2 In other words it is to determine if a graph is included in the other larger graph A well-known algo rithm for subgraph isomorphism is proposed in 19 As suggested in 7 graph isomorphism can be directly solved in practice, although it is not known to be either in P or in NP-complete On the other hand subgraph isomorphism has been proved to be in NP-complete Thus there is no scalable algorithm to solve it When the size of graphs is small such as 10 vertices or less however it is also known that subgraph isomorphism can be feasible even with a sim ple exhaustive search 7 191 We solve graph isomorphism by a simple way which is starting from a single vertex in one graph to try to find a mapping to one of the vertices in the other graph that is consistent with the labeling Then we keep the same pro cess by adding vertices one by one until either we find a complete mapping or we end up with exhausting the search space When we seek for the next mapping we have to be careful to keep the consistency of edge and vertex la bels We can reduce the search space more if there are more 315 


labels are assigned to edges and vertices which leads to restriction against mapping This approach can solve both graph and subgraph isomorphism 2.2 Candidate Generation In the candidate generation phase, we create a set of can didates of size k  1 given frequent k-subgraphs Can didate subgraphs of size k  1 are generated by joining two frequent k-subgraphs In order for two such frequent k-subgraphs to be eligible for joining they must contain the same k  1 We will refer to this common k  1\among two k-frequent subgraphs as their core Unlike the joining of itemsets in which two frequent Ic size itemsets lead to a unique k  1 itemset, the join ing of two subgraphs of size k can lead to multiple sub graphs of size k  1 This is due to three reasons First the resulting two k  1 produced by the join ing may differ in a vertex that has the same label in both k-subgraphs Figure l\(a is such an example This pair of graphs g;f and gt generates two different candidates gz and g The second reason is because a core itself may have multiple automorphisms and each automorphism can lead to a different k  1 An example for this case is shown in Figure l\(b in which the core-a square of 4 vertices labeled with vo-has more than one automorphism which result in 3 different candidates of size 6 Finally two frequent subgraphs may have multiple cores as depicted by Figure l\(c a By vertex labeling b By multiple automorphisms of a single core c By multiple cores Figure 1 Three different cases of candidate joining The overall algorithm for candidate generation is shown in Algorithm 2 For each pair of frequent subgraphs that share the same core the fsg-join is called at Line 6 to gen erate all possible candidates of size k  1 For each of the candidates the algorithm first checks if they are already in C If they are not, then it verifies if all its k-subgraphs are frequent If they are fsg-join then inserts it into C k+l otherwise it discards the candidate Lines 7-16 The algo rithm uses canonical labeling to efficiently check if a partic ular subgraph is already in Ck or not The key computational steps in candidate generation are 1 core identification 2 joining and 3 using the down ward closure property of a support condition to eliminate some of generated candidates A straightforward way of implementing these tasks is to use subgraph isomorphism graph automorphism and canonical labeling with binary search respectively The amount of computation required by the first step however, can be substantially reduced by keeping some information from the lattice of frequent sub graphs Particularly if for each frequent k-subgraph we store the canonical labels of its frequent IC  1 then the cores between two frequent subgraphs can be deter mined by simply computing the intersection of these lists Also to speed up the computation of the automorphism step during joining we save previous automorphisms associated with each core and look them up instead of performing the same automorphism computation again The saved list of automorphisms will be discarded once C has been gen erated Note we need to perform selfjoin that is two graphs gt and g in Algorithm 2 are identical It is necessary because for example, consider transactions without any labels that is each transaction in the input is an undirected and un labeled graph Then we will have only one frequent 1 subgraph and one frequent 2-subgraph regardless of a sup port threshold because those are the only allowed struc tures and edges and vertices do not have labels assigned From those F1 and F2 where IF1 I  IF21  1 to generate larger graphs of Ck and Fk for k 2 3 the only way is the self join 2.3 Frequency Counting Once candidate subgraphs have been generated FSG computes their frequency The simplest way of achieving this is for each subgraph to scan each one of the transaction graphs and determine if it is contained or not using sub graph isomorphism. Nonetheless having to compute these isomorphisms is particularly expensive and this approach is not feasible for large datasets In the context of frequent itemset discovery by Apriori, the frequency counting is per formed substantially faster by building a hash-tree of can didate itemsets and scanning each transaction to determine which of the itemsets in the hash-tree it supports Develop ing such an algorithm for frequent subgraphs however is 316 


Algorithm 2 fsg-gen\(Fk Candidate Generation Ck t 0 foreachpairofgF,g E Fk,Z jsuchthatcl\(gf cl\(g for each edge e E 9 do create a k  1 of 91 by removing an edge e gt t gt  e if gf is included in  then gf and g share the same core T t fsg-join\(gt gf  for each g E Tk do test if the downward closure property holds for  flag t true for each edge fi E g do hf t g;+1  fi if hf is connected and hf Z Fk then flag t false break if flag  true then ck+l  Ck+1 U gk+l return k Notation DI IT1 111 ILI challenging as there is no natural way to build the hash-tree for graphs For this reason FSG instead uses Transaction ID TID\lists proposed by 23,21,22 In this approach for each frequent subgraph we keep a list of transaction iden tifiers that support it Now when we need to compute the frequency of gk we first compute the intersection of the TID lists of its frequent k-subgraphs If the size of the in tersection is below the support gk+l is pruned, otherwise we compute the frequency of g k+l using subgraph isomor phism by limiting our search only to the set of transactions in the intersection of the TID lists Parameter The total number of transactions The average size of transactions in terms of the number of edges The average size of potentially frequent subgraphs in terms of the number of edges The number of potentially 3 Experiments We performed a set of experiments to evaluate the per formance of FSG There are two types of datasets we used The first type was synthetically generated and allowed us to study the performance of FSG under different conditions The second type contains the molecular structures of chem ical compounds which is used to evaluate the performance of FSG for large graphs All experiments were done on 650MHz Intel Pentium I11 machines with,2GB main memory running the Linux operating system 3.1 Synthetic Datasets For the performance evaluation we generate synthetic datasets controlled'by a set of parameters shown in Table 2 The basic idea behind our data generator is similar to the one used in 2 but simpler First we generate a set of 1151 potentially frequent con nected subgraphs whose size is determined by Poisson dis tribution with mean 111 For each frequent connected sub graph its topology as well as its edge and vertex labels are chosen randomly It has a weight assigned which becomes a probability that the subgraph is selected to be included Table 2 Synthetic dataset parameters I frequent subgraphs I The number of edge and vertex labels N in a transaction. The weights obey an exponential distribu tion with unit mean and the sum of the weights of all the frequent subgraphs is normalized to 1 We call this set of ILI frequent subgraphs a seed pool The number of distinct edge and vertex labels is controlled by the parameter N In particular N is both the number of distinct edge labels as well as the number of distinct vertex labels Next we generate ID1 transactions The size of each transaction is a Poisson random variable whose mean is equal to IT1 Then we select one of the frequent subgraphs already generated from the seed pool by rolling an ILI sided die Each face of this die corresponds to the proba bility assigned to a potential frequent subgraph in the seed pool If the size of the selected seed fits in a transaction we add it If the current size of a transaction does not reach its selected size we keep selecting and putting another seed into it When a selected seed exceeds the transaction size we add it to the transaction for the half of the cases and discard it and move onto the next transaction for the rest of the half The way we put a seed into a transaction is to find a mapping so that the overlap between a seed and a transac tion is maximized In the following experiments we use the combinations of the parameters shown in Table 3 Table 3 Parameter settings Parameter Values 3,5,7,10 N 3.5.10.20.40 Table 4 shows the amount of time required by FSG to find all the frequent subgraphs for various datasets in which we changed N 111 ITI and o In all of these experiments the number of transactions ID1 was fixed to 10000 and the number of potential frequent subgraphs ILI was set to 200 If the average transaction size IT1 is smaller than that of potential frequent subgraphs 111 we omitted such combina tions because we can not generate transactions In some 317 


cases we aborted computation because the running time was too long or because the main memory was exhausted which are denoted by dashes in the table By looking at the table we can observe a number of in teresting points regarding the performance of FSG for dif ferent types of datasets First, as the number of edge and vertex labels N increases, the amount of time required by FSG decreases For example when IS  2 N  3 11  3 and 12\2221  10 it takes 143 seconds, while the run ning time drops to 16 seconds for N  20 This is because as the number of edge and vertex labels increases there are fewer automorphisms and subgraph isomorphisms which leads to fast candidate generation and frequency counting Also by having more edge and vertex labels we can effec tively prune the search space of isomorphism because they work as constraints when we seek for a mapping of vertices Second as the size of the average transaction IT1 increases the overall running time increases as well The relative in crease is higher when N is small than when N is large. For example, going from 17\2221  5 to IT1  40 under the set ting of N  5 111  3 and T  2 the running time increases by a factor of 20, whereas for the same set of pa rameters when N  40 the increase is only by a factor of 4 The reason for that is again having many edge and vertex labels effectively decreases the number of isomor phisms and the search space With small N and large ITI we can not narrow down efficiently the search space of sub graph isomorphism for frequency counting and the running time increases drastically. Third, as 111 increases the overall running time also increases Again the relative increase is smaller for larger values of N and smaller values of IT1 by the same reason described above To determine the scalability of FSG against the number of transactions we performed an experiment in which we used DI  10000 20000 40000 and 80000 with ILI  200 111  5 and IT1 ranging from 5 to 40 These results are shown in Figure 2 As we can see from the figure FSG scales linearly with the number of transactions 3.2 Chemical Compound Dataset We obtained a chemical dataset from This was originally provided for the Predictive Toxicology Evalua tion Challenge  181 which contains information on 340 chemical compounds in two separated files The first file named atoms pl contains definitions of atoms in com pounds. For example 223atm\(d1 dl 1 c 22 0.133 means that a chemical compound dl has an atom whose identi fier is dll of element carbon of type 22 and with par tial charge 0.133 The other file bonds pl provides bonding information between atoms A line in the file 223bond\(d1 dll dl2 7 for instance, states that in the com pound dl its atoms dll and dl2 are connected by a type 7 bond. There are 4 different types of bonds and 24 different Figure 2 Scalability on the number of transaction atoms, and there are 66 atom types We converted the data into graph transactions Each compound becomes a transaction. Thus, there are 340 trans actions in total. Each vertex corresponds to an atom, whose label is made of a pair of the atom element and the atom type We did not include partial charge to vertex labels be cause those values were not discretized Each edge is placed for every bond Edge label directly corresponds to the bond type By the conversion, there are 4 edge labels and 66 ver tex labels produced in total The average transaction size was 27.4 in terms of the number of edges and 27.0 in terms of the number of vertices Because the number of edges is very close to that of vertices, this dataset is sparse There are 26 transactions that have more than 50 edges and ver tices The largest transaction contains 214 edges and 214 vertices The experimental results by FSG for finding frequent subgraphs are shown in Figure 3 Figure 3\(a shows the running time required for different values of support thresh old and Figure 3\(b displays the number of discovered fre quent subgraphs on those support levels With z  7 the largest frequent subgraph discovered has 13 vertices With the support threshold T below lo both the run ning time and the number of frequent subgraphs increase exponentially FSG does well even for 7 support as it requires 600 seconds AGM a frequent induced subgraph discovery algorithm, required about 8 days for 10 and 40 minutes for 20 with almost the same dataset on 400MHz PC ll Comparing the performance of FSG on this dataset against those on the synthetic datasets we can see that it requires more time for this chemical dataset, once we take into account of the difference in the number of transactions This is because in the chemical dataset edge and vertex labels have non-uniform distribution As we decrease the minimum support, larger frequent subgraphs start to appear which generally contain only carbon and hydrogen and a 318 


Table 4 Running times in seconds for synthetic data sets We omitted parameter combinations where 11  12'1 because transaction size is too small for potential frequent subgraphs A dash in the table means we had to abort the computation for the set of parameters because of either memory exhaustion or taking too long time N 111 12'1 335 10 20 40 355 10 20 40 3 7 10 20 3 10 10 20 143 434 RunningTime[sec u=2 u=1 12 22 30 40 112 390 18 32 51 102 189 736 66 4512 5817  6110  1953  40  8290    20 40 255 10 20 2 7 10 20 2 10 10 20 U  2 10 16 20 35     27 52 25 1 2246   40  557 6203   40      20 20 I88 10 10 190 20 40 ime[secl U  1 17 25 40 98 18 51 119 246 816 I506 3199   53 71 196 279 N 111 IT1 u=2 u=l 10 16 28 20 34 38 RunningTime[sec u=2 I u=1 10 20 40 4055 10 20 40 40 7 10 20 40 40 10 10 20 27 44 44 47 84 89 20 28  29 60 55 131 177 234 197 1236 861 5273 2456 9183 9687    3271 10520 20 5 5 10 single bonding type. Essentially with U  lo this dataset becomes similar to the synthetic datasets where N  2 3.3 Summary of Discussions We summarize the characteristics of FSG performance First FSG works better on graph datasets with more edge and vertex labels During both candidate generation and frequency counting what FSG essentially does is to solve graph or subgraph isomorphism Without labels assigned determining isomorphism of graphs is more difficult to solve because we can not use labeling information as con straints to narrow down the search space of vertex mapping We can confirm it by comparing the results in Table 4 with various values of the number of edge and vertex labels N Second, the running time depends heavily on the size of frequent subgraphs to be discovered If input transactions contain many large frequent patterns such as more than 10 edges the situation corresponds to the parameter setting of 111  10 where FSG will not be likely to finish its compu tation in a reasonable amount of time The same thing hap pened with the chemical dataset with a support threshold less than 10 If we compare Figure 3\(a and Figure 3\(b we notice the running time increases at a higher rate than the number of discovered subgraphs does as we decrease the minimum support With a lower support criteria we start getting larger frequent subgraphs and both candidate generation and frequency counting become much more ex pensive On the other hand as for the cases of 111  5 in 10 19 20 51 u=2 u=1 10 20 25 20 40 20 7 10 48 117 182 233 193 804 Table 4 FSG runs fast The result of the chemical dataset is consistent with it For example if we use U  10 for the chemical dataset FSG spends 28 seconds to get 882 fre quent subgraphs in total The largest frequent graphs among them have 11 edges, and there are only 10 such frequent 11 subgraphs discovered Another important factor is the size of a transaction If the average size of transactions becomes large, frequency counting by subgraph isomorphism becomes expensive re gardless of the size of candidate subgraphs. Traditional fre quent itemset finding algorithms are free from this problem They can perform frequency counting simply by taking the intersection of itemsets and transactions As of the number of transactions FSG requires running time proportional to the size of inputs under the same set of parameters This is the same as frequent itemset discovery algorithms 4 Conclusion In this paper we presented an algorithm FSG for finding frequently occurring subgraphs in large graph databases that can be used to discover recurrent patterns in scientific, spatial and relational datasets Our experimen tal evaluation shows that FSG can scale reasonably well to very large graph databases provided that graphs contain a sufficiently many different labels of edges and vertices 319 


8000 7000  6000 25000 I 000 3000 2000 1000 a i     a Minimum support o and running time 25001 b Minimum support U and the number of discovered fre quent subgraphs Figure 3 Performance with the chemical compound dataset Acknowledgment We deeply thank Professor Takashi Washio Professor Hiroshi Motoda and their research group at the Institute of Scientific and Industrial Research Osaka University and Mr Akihiro Inokuchi at Tokyo Research Laboratory IBM Japan Ltd for providing the source code of AGM and use ful comments References l R C Agarwal C C Aggarwal V V V Prasad and V Crestana A tree projection algorithm for generation of large itemsets for association rules IBM Research Report RC21341 1998 2 R Agrawal and R Srikant Fast algorithms for mining as sociation rules In Proc the 20th VLDB pages 487-499 Morgan Kaufmann 1994 3 R Agrawal and R Srikant Mining sequential patterns In Proc the Ilth ICDE pages 3-14 IEEE Press, 1995 4 R N Chittimoori, L B Holder, and D J Cook Applying the SUBDUE substructure discovery system to the chemical toxicity domain In Proc the 12th Int Florida AI Research Society ConJ pages 90-94 1999 5 L Dehaspe H Toivonen, and R D King Finding frequent substructures in chemical compounds In Proc the 4th ACM SIGKDD KDD-98 pages 30-36 AAA1 Press 1998 6 D Dupplaw and P H Lewis Content-based image retrieval with scale-spaced object trees In Proc SPIE Storage and Retrieval for Media Databases volume 3972 pages 253 261,2000 7 S Fortin The graph isomorphism problem Technical Re port TR96-20 Department of Computing Science, Univer sity of Alberta, 1996 8 M R Carey and D S Johnson Computers and Intractabil ity A Guide to the Theory of NP-Completeness W H Free man and Company New York 1979 9 J Han J Pei, and Y Yin Mining frequent patterns without candidate generation In Proc ACM SIGMOD 2000 0 lo L Holder, D Cook and S Djoko. Substructure discovery in the SUBDUE system In Proc the Workshop on Knowledge Discovery in Databases pages 169-1 80 1994 I I A Inokuchi T Washio, and H Motoda An apriori-based al gorithm for mining frequent substructures from graph data In Proc PKDD'OO pages 13-23,2000 12 H Kalviainen and E Oja Comparisons of attributed graph matching algorithms for computer vision In Proc. STEP-90 Finnish ArtiJicial Intelligence Symposium pages 354-368 1990 13 M Kuramochi and G. Karypis Frequent subgraph discov ery Technical Report 01-028 Department of Computer Sci ence University of Minnesota, 2001 I41 D A L. Piriyakumar and P Levi An efficient A based algorithm for optimal graph matching applied to computer vision In GRWSIA-98 1998  151 http://oldwww.comlab.ox.ac.uk/oucVgroups/machlearn I61 R C Read and D G Corneil The graph isomorph disease Journal of Graph Theory 1:339-363 1977  171 A Srinivasan, R D King S Muggleton and M J E Stern berg Carcinogenesis predictions using ILP In Proc the 7th Int Workshop on Inductive Logic Programming volume 1297, pages 273-287 Springer-Verlag, Berlin 1997  181 A Srinivasan, R. D. King S H Muggleton and M Stern berg The predictive toxicology evaluation challenge In Proc the 15th IJCAI pages 1-6 Morgan-Kaufmann 1997 19 J R Ullman An algorithm for subgraph isomorphism Journal ofthe ACM 23\(1 1976 20 K Yoshida and H Motoda CLIP: Concept learning from in ference patterns Artificial Intelligence 75 1 1995  M J Zaki Scalable algorithms for association mining Knowledge and Data Engineering 12\(2 22 M J Zaki and K Gouda Fast vertical mining using diffsets Technical Report 01-1 Department of Computer Science Rensselaer Polytechnic Institute 2001 23 M J Zaki and C.-J. Hsiao CHARM An efficient algorithm for closed association rule mining,. Technical Report 99-10 Department of Computer Science Rensselaer Polytechnic Institute 1999 320 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


