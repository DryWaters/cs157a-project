Maintaining unlinkability in group based P2P environments Joan Arnedo-Moreno Estudis dêInform  atica Multim  edia i Telecomunicacions Universitat Oberta de Catalunya Barcelona Spain jarnedo@uoc.edu Jordi Herrera-Joancomart   Escola T  ecnica Superior dêEnginyeria Universitat Aut  onoma de Barcelona Campus de Bellaterra Spain jherrera@deic.uab.cat Abstract In the wake of the success of peer-to-peer networking privacy has arisen as a big concern Even though steps have been taken in order to attain an anonymous communications channel all approaches consider the overlay network as a single entity and none of them take into account peer group based environments In this paper we describe a method in order to maintain unlinkability in group membership authentication attempts when using peer groups relying on web-of-trust Using this method it is not possible to ultimately pinpoint a peerês identity despite the constraints of a group membership scenario 1 Keywords peer-to-peer P2P security peer group anonymity unlinkability distributed systems web-of-trust ring signatures I I NTRODUCTION The adoption of peer-to-peer P2P technologies has become a promising solution to share distributed resources Usually P2P environments are conceptualized as a global overlay network without any kind of logical segmentation as far as resource availability is concerned However under some circumstances it is desirable to segment the network into sets of peers which share common interest or services creating peer groups  The main reasons for such segmentation range from restricting access to some resources to creating a scoping domain A lot of research efforts in the eld of P2P have mainly focused towards strictly functionality issues such as scalability efìcient message propagation across the network or access to distributed resources At present time the maturity of P2P research eld has pushed through new problems such as those related with security One of the desired security properties for a P2P system is anonymity allowing users to connect to the P2P network without exposing their identity protecting their privacy and escaping censorship Initiatives such as Freenet FreeHa v en  or T or 3 pro vide mechanisms to deplo y fully anon ymous P2P networks In such systems it is not possible to easily guess the source of messages transmitted across the network 1 This work was partially supported by the Spanish MCYT and the FEDER funds under grant TSI2007-65406-C03-03 E-AEGIS and CONSOLIDER CSD2007-00004 ARES funded by the Spanish Ministry of Science and Education A general survey on anonymity in P2P systems may also be found in Unfortunately the aforementioned initiatives do not take into account peer group based environments These proposals rely on the assumption of a at network where peers do not form groups and then anonymity is focused on messaging resource access and publication However as we already mention network segmentation in different groups could be useful in some applications but it would be interesting to still preserve user anonymity That means it will be necessary to identify which peers are group members without disclosing its identity or being able to trace peer interactions In such scenario an additional mechanism for anonymously proving peer group membership is necessary The goal and main contribution of this paper is to provide some degree of anonymity to peer groups allowing peers to prove membership to each other without disclosing their actual identity Speciìcally the proposed method is concerned with identity unlinkability even though members of a peer group may know the identity of its current members it is not possible to trace authentication attempts to a speciìc identity or tell which have been initiated by the same peer This is achieved with the help of ring signatures 6 Our proposal is based on a web-of-trust scenario since such models hea vily tak e into account peer equality and decentralization Such features are important regarding the anonymity problem since on one hand they avoid that a single peer becomes too powerful and is able to compromise the rest of the group memberês anonymity and on the other hand each member manages its own data The proposal assumes that peers are already provided with anonymous transport having the capability to anonymously exchange messages at a lower layer using any of the initiatives that already exist such as 2 3 Otherwise the point of anonymously proving peer group membership becomes moot as the source peer identiìer is sent across the network in plain text This paper is organized as follows Section II brieîy describes how group membership access control is usually attained in a peer group and the challenges it poses when applied to an anonymous environment Following section III provides an overview of ring signature scheme Section IV presents how unlinkability may be attained despite the 
2009 18th IEEE International Workshops on Enabling Technologies: Infrastructures for Collaborative Enterprises 1524-4547/09 $25.00 © 2009 IEEE DOI 10.1109/WETICE.2009.24 140 


constraints of a peer group environment Finally section V summarizes the paper contributions and outlines further work II R ELATED WORK In this section we review the related work regarding peer group membership mechanisms and the existing proposals regarding peer anonymity Peer group membership approaches have been discussed in the scientiìc literature although not always have been referred with the same name or taken into account the characteristics of a P2P environment The most basic approaches use a symmetric key model similar to that of ad hoc networks 9 This is an ob vious solution for peer group access control as proof of membership is achieved via the direct usage of this token However its main drawbacks are key management and distribution The shared key must be transmitted to new members of the group via an out-of-band secure channel and changing such key is equivalent to recreating the group from scratch Current group membership approaches mainly focus on asymmetric cryptography Every peer generates its own publicprivate key pair which is used in order to authenticate to other peers Most of such approaches 11 12 13  rely on a Certiìcation Authority CA which generates certiìcates to group members binding their public key to their identity Those certiìcates may be used as proof of peer group membership providing simplicity to peer group management as a single trusted entity takes care of everything However the distributed nature of P2P networks tends to avoid relying on a fully centralized CA using instead threshold cryptography to split the CAês private key between different peers A different approach specially suited to P2P is to rely on a web-of-trust instead of a single entity such as a CA This approach also prevents a single peer from becoming the only group manager by making use of the systemês selforganization and enforcing peer equality A proposal speciìcally based on group membership though not anonymously is presented in 17 All peers act as a fully functional CA vouching for other peers group membership by signing certiìcates to them Any peer P i vouching for some other peer P j s membership is considered as its patron  Peers test group membership by nding trust paths or certiìcate paths A trust path is a chain of multiple certiìcates which validates some subjectês public key starting from the validating entity and ending in the subject to be validated Peer group membership approaches with the speciìc goal to maintain anonymity exist 20 Unfortunately  these approaches are entirely based on a centralized model rather than a P2P architecture where a single entity controls group membership Another recent proposal tak es into account the idiosyncrasies of peer group scenarios However its main goal is maintaining linkability in such a way that it is possible to identify that different authentication attempts have been performed by the same peer which is just the opposite of our goal The problem of anonymous proof of peer group membership is also very similar to that of generic anonymous authentication 23 and anon ymous identiìcation in ad hoc en vironments 25 Ho we v er  there s a signiìcant dif ference in our base scenario In the cited proposals group formation is made within the context of a user population where either a single entity generates some common knowledge to be shared within the group equivalent to a symmetric key approach or a at membership hierarchy exists as it is the case in a CA approach In contrast our work approaches a web-of-trust based environment where each peer is fully autonomous and trust relationships without a single root entity must be taken into account III R ING S IGNATURES The notion of a ring signature scheme is related to that of group signature In the latter  a trusted group manager predeìnes certain groups of users and distributes special keys to their members Each member can use these keys to anonymously generate signatures which look indistinguishable to other group members In contrast the former does not need a group manager This is a highly desirable feature in a selforganized environment such as P2P Ring signatures are useful when the members are autonomous and do not want to rely on other peers They are signer-ambiguous and provide no way to revoke the anonymity of the actual signer It is only necessary to assume that each peer group member already holds a private/public key pair of some standard signature scheme A ring signature is generated by the actual signer declaring an arbitrary set of possible signers which includes himself and computing the signature by himself using only his private key and the others public keys In this scheme the set of possible signers is called a ring The ring member who produces the actual signature is the signer and each of the other ring members is a non-signer The signer does not need the consent or assistance of the other ring members to put them in the ring only knowledge of their public keys is needed Two procedures are deìned  002  ring  sign  m P K 1 PK 2   P K n SK i  produces a ring signature 002 for the message m  given the public keys PK 1 PK 2   P K n of the n ring members together with the private key SK i of the i-th member the actual signer for 1 002 i 002 n   ring  verify  m 002  accepts a message m and a signature 002 and outputs either true or false 002 includes the public keys of all possible signers  PK 1 PK 2   P K n  Veriìcation satisìes the usual soundness and completeness conditions but since ring signatures are signer-ambiguous the veriìer is unable to determine the identity of the actual signer in a ring of size n  probability is not greater than 1 n  This limited anonymity is unconditional since e v en an inìnitely powerful adversary cannot link signatures to the same signer Ring signatures are also particularly efìcient since generating or verifying a ring signature costs the same as generating or verifying a regular signature plus an extra multiplication or 
141 


two for each non-signer This means that the scheme is still useful even when the ring cardinality is very high IV A UTHENTICATION UNLINKABILITY WITH RING SIGNATURES The following notation will be used in this section  PK i  Peer P i s public key  SK i  Peer P i s secret private key  Cert i  One of peer P i s certiìcates containing PK i It must be noted that in a web-of-trust P i may hold several certiìcates obtained from different patrons However they all always contain PK i   E PK i  x   A string x encrypted using the public key of peer P i  Group access control using asymmetric cryptography by means of digital certiìcates is normally performed through trust paths When some peer P n wants to prove to some other peer P 1 that he is part of the group  P 1 authenticates P n  all certiìcates conforming a trust path between P n and P 1 are transmitted A trust path from peer P 1 to peer P n is a list of certiìcates  Cert 1  Cert 2  Cert 3   Cert n   where P 1 signed Cert 2  P 2 signed Cert 3  etc up to Cert n  Consequently P n may be considered a group member by P 1 only if such trust path exists However using this approach the public key PK n becomes the main constraint in order to achieve unlinkability between authentication attempts Even in the case that Cert n does not contain any information regarding P n s identity the public key can be regarded as a unique identiìer or pseudonym and only some degree of pseudonymity is ultimately achieved In order to solve this problem we introduce the concept of trust tree TT between two peers which will be used as a means for authentication instead of a trust path in order to solve this issue A trust tree between peers P i and P j  TT i j  is deìned as a set of certiìcates  Cert 1  Cert 2    Cert n  with the following properties  003 i 004 1   n  such that Cert i is P i s certiìcate  003 j 004 1   n  with j 005  i such that Cert j is P j s certiìcate  006 k 004 1   n  there exists a trust path from Cert i to Cert k  As a result it is guaranteed that a trust path exists from Cert i to Cert j  The certiìcate of P j in TT i j is unknown to anybody but its creator P j  It is only guaranteed that PK j is in some certiìcate within TT i j  Also note that in contrast with trust paths Cert j doesnêt need to be the edge one it may be anyone within TT i j  As a result given a peer group more than one possible TT j i may exist between two peers TT i j can be envisioned as a certiìcate hierarchy which comprises several trust paths as shows the example in gure 1 Since TT i j is a set of certiìcates it contains the set of public keys PK TT i j  PK 1   PK i   PK j   PK n Asa result it is feasible to apply a ring signature scheme where P j Fig 1 Trust Path and possible Trust Tree between peer P i and P j may specify the TT i j as the set of possible signers Choosing a ring signature scheme is worthwile in a P2P environment as shown in section III A Authentication process In order for P j to anonymously proof group membership to P i  a valid TT i j must be generated by P j in advance The authentication process then follows as shown in gure 2 1 A session identiìer sid  is chosen by P j and sent encrypted to P i  2 P i generates a pseudorandom nonce r  which is sent in response to P j  P i stores both sid and r as an authentication transaction in progress 3 P j generates 002  a ring signature of r using its own private key and the set of public keys in TT i j  4 P j sends to P i the values sid and 002  which includes both the signature algorithm result and the related the set of public keys PK TT j i  5 P i uses sid to identify the transaction retrieves r and checks the signatureês correctness Fig 2 Peer group membership authentication protocol If ring  verify validates P i checks the validity TT i j  as it will be shortly explained In case that TT i j is valid 
142 


P j s peer group membership is considered proved to P i  Short term access as a group member is granted using E PK i  sid  in any further messages During this access interactions are linked via sid but the identity of P j is never disclosed However P j may reset the identiìer by re-authenticating Any authentication attempt remains unlinked to the previous ones In order to check the validity of a trust tree TT i j  for any certiìcate from some peer P k 004 TT i j  the trust path from P i to P k must exist The TT validation process is considered valid if all trust paths between Cert i and any other certiìcates are valid which means that any subject within the TT is a peer group member During this validation process P i stores in a local cache correct trust paths Just certiìcate subjects are stored not the whole certiìcate This cache can be used in further TT validations in order to speed up the process by rst looking up if a trust relationship between P i and some other peer has already been checked in previous TT validations And advantage of keeping this cache is the fact that it can be used in the validation process of any TT within the peer group In this proposal each possible signer for a given TT each certiìcate subject within the TT  becomes the anonymity set for each authentication attempt The signer s identity becomes hidden within all subjects in the TT  however since during the validation process it has been guaranteed that all subjects within the TT are group members it can also be guaranteed that the signer is a group member A TT is used instead of a simple trust path in order to both dynamically expand the cardinality of the anonymity set and avoid clearly pinpointing the actual signer which is usually the owner of the edge certiìcate Using a standard trust path as a ring signature signer set is not desirable since its cardinality once established will usually remain static Since the degree of unlinkability relies on the number of possible signers in some instances such cardinality may become too narrow to be considered acceptable TT s take advantage of some other peerês long trust paths in order to increase the cardinality of the signer set Furthermore at each authentication attempt between P i and P j  the later is not bound to always using exactly the same TT  P j is able to generate different TT s which are considered valid resulting in diverse anonymity sets which may be used at authentication attempts Even in the worst case scenario where P i directly trusts P j  an acceptable TT may still be produced It is enough to include some other peers which conform trust paths from A  even though B does not appear in those paths as shown in gure 3 as an example Even in this scenario it is still possible to chose which is the cardinality of the anonymity set up to the full peer groupês cardinality B Trust tree generation A TT cannot be generated by arbitrarily collecting and setting together member public keys which is the assumption in a basic ring signature approach since it must be ensured that all the included public keys conform some trust path from Fig 3 Possible TT i j where P i directly trusts P j the authenticating peer No unconnected certiìcate may be included There are several methods to obtain the necessary certiìcates in order to generate a TT by taking advantage of a webof-trust based peer group membership operation However it is highly desirable that certiìcate retrieval is performed along standard network operation since the impact of using TT s on network performance is minimized and most important other peers then cannot know whether certiìcate retrieval is being requested in order to compose TT s or just in order to routinely operate Feasible methods for some peer P j to retrieve certiìcates from other group members other than just directly requesting them are  Whenever P i is requested a certiìcate by someone interested in becoming group member acting as a patron following the approach in The generated certiìcate may be stored for later use for TT creation An advantage of this option is that all certiìcates will be useful for TT generation since all of them conform a valid trust path from P j  Using this method is also an incentive to become a patron since the most certiìcates P j generates the easier it is for him to generate large anonymity sets  In web-of-trust environments peers already know its patronês certiìcates  In onion routing anonymous networks which are the most popular ones the sender establishes the message path by retrieving the certiìcate of each peer in the desired path This is necessary to create each message encryption layer.That means that some method to directly retrieve other peerês certiìcates must exist An advantage of this method is that both onion routing operation and TT generation make use of the same information The proposal in in f act deìnes a protocol which speciìcally looks up trust paths within the peer group and is able to retrieve the certiìcates of other group members This capability allows to retrieve any certiìcate within the peer group and easily join trust paths in order to create TT s As shown at the beginning of the authentication process in 
143 


subsection IV-A TT s may be generated in advance It does not need to be created at the precise moment before initiating authentication Peers may slowly generate a large TT as they operate within the peer group and learn about new certiìcates Different subsets from a large TT  which could amount to the whole peer group in the best case scenario may be used at each authentication attempt as shown in gure 4 There are two main reasons for not using the whole information and just using subsets avoiding sending large TT s across the network which may impact performance and slow TT validation and preserving the security of the scheme as will be explained in section IV-C Fig 4 Using a large TT to generate smaller TT s C Security analysis In this section attacks on anonymity are analyzed This analysis will focus only on the authentication process since an anonymous transport method is assumed as stated in section I For that reason all strengths and weaknesses of the anonymous networks will be inherited Common attacks in anonymous networks such as the predecessor attack for e xample are not discussed being completely concerned with message relay  Trusted tree reuse A big concern in this proposal is the reuse of TT s If the same TT is always used it will ultimately become like an identiìer and it will be trivial to link different sessions For that reason the same TT should not be reused in different sessions This problem is solved with the generation of a bigger TT and then only using smaller subsets However the proposed scheme still minimizes this attack to some degree since it cannot be guaranteed that the same trusted tree received several times by the authenticating peer at different group membership authentication attempts was sent from the exact same peer Two peers may generate the same TT  and still follow the properties listed in section IV since a given TT may be used by any peer whose public key is contained in such TT   Timing attack In a timing attack an authenticating peer may try to link different sessions by comparing response timings in the authentication process message round trip time Although anonymous networks already take this attack into account this problem can also be solved by purposely delaying responses a random amount of time in the authentication protocol  Intersection attack This attack is complementary to that of TT reuse The authenticating peer may try to compare different TT s used by the same peer intersect all of them trying to decrease the number of suspects An underlying anonymous network does constrain this attack only to the authenticating peer it cannot be attempted by a middle point peer However this attack is countered since it is not possible for an attacker to know which TT s come from the same peer in order to compare them All authentication attempts as well as sessions are completely independent and no information is sent along each one in order to relate different attempts to each other Given a set of different TT s in which an speciìc public key appears in all of them it cannot be guaranteed that some of them came from the same peer V C ONCLUSIONS AND FURTHER WORK A method to maintain unlinkability between authentication attempts in peer groups based on web-of-trust has been presented This is achieved by using ring signatures and introducing the concept of trust trees as the means order to transport public keys as well as providing an anonymity set for the signer which may be constructed according to its own needs and may be increased or changed over time The main contribution of this proposal is the capability of each individual peer to chose its trust anchors into the peer group instead of being forced to use a speciìc one such as a CA or group manager We consider that this freedom of choice is extremely important in an environment where privacy is highly valued Furthermore it is faithful to a pure P2P approach relying on peer autonomy and self-organization Furthermore the proposed method also nicely meshes into peer group operation in web-of-trust based scenarios in two different ways First of all trust tree generation does not force the use of additional protocols taking advantage of standard data exchanges in order to retrieve public keys This fact improves its performance and minimizes threats from a passive attacker Second the link between trust tree size and the anonymity set directly rewards those peers who decide to act as patrons within the group providing an incentive peers which act as such In fact in a web-of-trust based environment it is very important that as many peers as possible agree to act as patrons Currently research is at its initial stages the anonymous authentication model being just set Further work includes implementing the model in order to assess its behaviour and 
144 


performance under a real group based P2P middleware A good candidate for this is JXTA R EFERENCES  W ile y B Clark e I Sandber g O and T W  Hong Freenet A distrib uted anonymous information storage and retrieval system Lecture Notes in Computer Science  p 46 2002  Freedman M.J Dingledine R and D Molnar  The free ha v en project Distributed anonymous storage service Lecture Notes in Computer Science  p 67 2001  Mathe wson N Dingledine R and Syv erson P  T or The second generation onion router Proceeding of the 13th USENIX Security Symposium  August 2004  X Ren-Y i Surv e y on anon ymity in unstructured peer to-peer systems Journal of Computer Science and Technology  vol 23 no 4 pp 660 671 July 2008  T auman Y  Ri v est R.L Shamir A Ho w to leak a secret Lecture Notes in Computer Science  vol 2248 2001  Harn L Ren J Generalized ring signatures 2001  S Garìnk el Pgp Pretty good privacy  OêReilly and Associates Inc 1994  IEEE Standard speciìcations for wireless local area netw orks  1999 http://standards.ieee.org/wireless  J Katz R Ostro vsk y  and M Y ung Ef cient passw ord-authenticated key exchange using human-memorable passwords Advances in CryptologyEUROCRYPT 2001  pp 475Ö494 2001  J Luo J.P  Hubaux and P Th Eugster  Dictate Distrib uted certiìcation authority with probabilistic freshness for ad hoc networks IEEE Transactions on Dependable and Secure Computing  vol 2 pp 311 323 2005  S Y i and R Kra v ets Moca Mobile certiìcate authority for wireless ad hoc networks The 2nd Annual PKI Research Workshop PKI 03  2003  L Zhou and Z.J Haas Securing ad hoc netw orks  IEEE Network Journal  vol 13 pp 24Ö30 1999  J K ong P  Zerfos H Luo S Lu and L Zhang Pro viding rob ust and ubiquitous security support for mobile ad-hoc networks International Conference on Network Protocols ICNP  pp 251Ö260 2001  Sun Microsystems Project JXT A 2001 http://www jxta.or g  A Shamir  Ho w to share a secret Commun ACM  vol 22 pp 612 613 1979  Joan Arnedo-Moreno and Jordi Herrera-Joancomart   Providing collaborative mechanism for peer group access control in Proceedings of the Workshop on Trusted Collaboration  2006 pp 1Ö6 IEEEPress  Joan Arnedo-Moreno and Jordi Herrera-Joancomart   Collaborative group membership and access control for jxta in COMSWAREê08 Proceedings of 3rd International Conference on COMmunication System softWAre and MiddlewaRE  2008 IEEEPress  CCITT  The directory authentication frame w ork recommendation 1988  P atrick Tsang Man Ho Au Apu Kapadia and Sean Smith Blacklistable anonymous credentials Blocking misbehaving users without ttps in Proceedings of CCS 2007  2007  Peter C Johnson Apu Kapadia P atrick P  Tsang and Sean W  Smith Nymble Anonymous ip-address blocking in Proceedings of the Seventh Workshop on Privacy Enhancing Technologies PET 2007  2007  P atrick Tsang and Sean Smith Ppaa Peer to-peer anon ymous authentication Applied Cryptography and Network Security  pp 55Ö74 2008  Franklin M Boneh D  Anon ymous authentication with subset queries in ACM CCS 1999  1999  Stadler M Camenisch J Ef cient group signature systems for lar ge groups in Crypto 1997  1997  Persiano G De Santis A Di Crescenzo G Communication-ef cient anonymous group identiìcation in In 5th ACM Conference on Computer and Communications Security  1998  Nicolosi A Dodis Y  Kiayias A and Shoup V   Anon ymous identiìcation in ad hoc groups in Advances in Cryptology EUROCRYPT 2004  2004 vol 3027 pp 609Ö626  Chaum D and V an He yst E Group signatures Advances in Cryptology EUROCRYPT 91  vol volume 547 of Lecture Notes in Computer Science 257-265 1991  Khontopp M Pìtzamnn A  Anon ymity  unobserv ability  and pseudonymity a proposal for terminology in Designing Privacy Enhancing Technologies International Workshop on Design Issues in Anonymity and Unobservability  2001 pp 2Ö9 Springer-Verlag  Le vine B.N Wright M.K Adler M and Shields C The predecessor attack An analysis of a threat to anonymous communications systems ACM Trans Inf Syst Secur  vol 7 no 4 pp 489Ö522 2004 
145 


operations Fast Lossless Compression Algorithm Five lossless compression algorithms were compared in terms of compression ratio by Aaron Kiely at JPL see Table 3 The data compression algorithms were tested using calibrated 1997 16 bit AVIRIS data from the Jasper Ridge Lunar Lake and Moffett Field datasets The table shows the average number of bits/sample to which the data was reduced The Fast Lossless algorithm developed by Matt Klimesh at JPL had the lowest number of output bits per sample and the overall best performance The average compression ratio achieved by the Fast Lossless algorithm for these datasets was 3.1:1 Traditional algorithms that have previously been used on hyperspectral data produced a higher average bits/sample and less effective compression ratios F or detailed information on the Fast Lossless Compression algorithm please see 12 Table 3  Comparison in terms of resulting bits per sample after processing by compression algorithms Initially all samples were initially expressed with 16 bits In addition to supplying an advantage in terms of compression ratio over other algorithms the Fast Lossless Compression Algorithm is also well suited for implementation on an FPGA due to the algorithm's low number of calculations per sample The memory requirements are also low requiring only enough memory to hold one spatial-spectral slice of the data on the order of hundreds of Kbytes Figure 4 shows the data 003ow from the instrument to the solid state recorder in the case of the VSWIR instrument Data from the sensor is initially calibrated to re\003ectance values which can be analyzed by the cloud detection algorithm Calibration data is sent to the solid state recorder The cloud detection algorithm generates a cloud map that becomes an input to the compressor The cloud map can be thought of as a matrix of cells that correspond to a spatial location on the image Each cell holds one bit to tell the compressor if the current pixel is a cloud pixel or a non cloud pixel The cloud map is then sent to the solid state recorder If the compressor is compressing a cloudy pixel the pixel is initially lossily compressed then additional lossless compression is applied The initial compression works by 002rst averaging the 214 spectral bands to 20 and reducing each sample from 14 to 4 bits If the compressor is compressing a non cloud pixel only the lossless compression algorithm is applied The compressed data stream is sent to the solid state recorder where it will be stored until it can be transmitted to a ground station The compression algorithm will be implemented on a 002eld programmable gate array FPGA Figure 4  Data Flow through the compressor for the VSWIR instrument Hardware Implementation The motivation for hardware implementation of the onboard compression algorithm is speed Hardware compression may perform as much as 002ve times faster than software compression for data of this type T o implement the algorithm in hardware it is translated from C code executed as a list of software instructions to a hardware description language executed as a propagation of signals through circuits The low complexity and minimal computational requirements of the Fast Lossless compression algorithm make it very well suited to be implemented using an FPGA Work to implement the Fast Lossless Compression Algorithm on an FPGA is being done at JPL The algorithm was tested using a Xilinx Virtex 4 LX160 FPGA mounted on a custom made evaluation board and preliminary test results demonstrated a throughput data rate of 33 Msamples/sec at a system clock rate of 33MHz The algorithm used approximately ten percent of the chips available programmable logic and interconnect resources The VSIWIR instrument will collect samples at a maximum rate of 60 Msamples/sec the TIR will collect at a maximum of 10 Msamples/sec for a combined maximum sample rate of 70 Msamples/sec which should be within the capabilities of the board FPGAs also provide appealing features such as high density low cost radiation tolerance and functional evolution As science goals change or as spacecraft capabilities are limited the ability of an FPGA to be reprogrammed from earth allows for the functional evolution of hardware thought the 7 


life of the mission For the HyspIRI mission this means that if a new compression algorithm with a higher compression ratio or a more accurate cloud detection algorithm with better cloud classi\002cation is developed the new technology could be implemented on the satellite with relative ease In March 2008 Xilinx Inc the main manufacturer of space ready FPGAs has released the Radiation-Tolerant Virtex-4 QPro-V Family providing good options for radiation-tolerant FPGAs with the speed and processing power required for the HyspIRI mission Thus based on a comparison of a v ailable compression algorithms and the HyspIRI missions requirements an FPGA implementation of the Fast Lossless Compression Algorithm is the recommended compression scheme for the mission 4 C OMMUNICATIONS A RCHITECTURE Once the data has been compressed it will be transmitted to a series of ground stations using a given communications protocol X-band Ka band TDRSS and LaserComm architectures were compared in terms of their ability to accommodate the volume of data to be transmitted their cost and mass and their heritage level An X-band architecture utilizing three ground stations was chosen Data Flow Modeling In order to design the communications system it was necessary to determine the desired data volume per unit time that must be transmitted from the satellite to the ground To do this a data 003ow model was created to calculate the data rate and volume that would be created aboard the spacecraft as seen in Figure 5 As the data 003ow diagram Figure 5 shows the model takes into account the satellite orbital parameters the desired sensor con\002guration swath width trackwise and swathwise pixel resolution spectral resolution number of bands intensity resolution bits per band compression ratio and a model of Earth's geography Geography is important since the sensors data output rates will vary according to the sensors target Land/Shallow Water LSW Cloud or Ocean To determine the percentage of time the sensors will view the features a map was overlaid onto a Satellite Orbit Analysis Program SOAP model of the satellite developed by Francois Rogez at JPL The map determined the satellite's sensor mode 226 whether it should record with LSW or Ocean resolutions By running this model over a 19-day period at 5minute resolution as to account for the entire repeat cycle the percentage of time the satellite would collect data from the designated regions LSW or Ocean was identi\002ed Since the locations of clouds are unpredictable a statistical average was used to predict the time the sensor would be viewing clouds With an 11 AM local descending node the expected cloud cover is approximately 30 Estimating a 2-in-3 detection success rate for pixels that are cloudy we estimate 20 of LSW areas will be labeled as cloud Note that clouds will only be identi\002ed on the VSWIR instrument when it is viewing LSW regions Some key elements of the model are shown in Table 4 Table 4  Important values and assumptions in the Data Flow Model The bits per orbit for each of the instrument and modes were then summed over one day Note that the VSWIR is considered to be on for half the orbit the actual duty cycle would be less due to the minimum 20 016 sun angle Therefore the model will slightly overestimate the data taken over the ocean The results of the model are shown below in Table 5 with comparative data volumes shown in Figure 6 One day was chosen as the time unit because it allows for a reasonably accurate estimate over multiple orbits thereby reducing 003uctuations in data generation due to non-uniform geography The data volumes shown are data outputs for the instruments A 10 overhead amount was added to account for metadata calibration data cloud map and packetization This average downlink data volume per day of 5,097 Gbits was used to size the communication system Communications Architecture Selection The main driver in the selection of HypsIRI's communication architecture was the large data downlink rate required Because the HyspIRI mission aims to provide consistent lowrisk operations redundancy and heritage were considered to be major selection factors Cost and masses of various architectures were also considered Table 7 shows a comparison between the architectures considered Ka band frequencies between 18-40 GHz are often used in communications satellites These frequencies are chosen be8 


Figure 5  Simpli\002ed operation of the data 003ow model Table 5  Data sum over one day of normal science operations The added 10 overhead accounts for the required added metadata Table 6  Link Budget for X-band to Svalbard GS cause of their ability to provide high gain as well as high data rates with smaller physical structures The Ka band was ruled to be unfeasible because most ground stations do not support Ka band and the use of Ka would mean introducing a new standard to multiple ground stations while modem upgrades to the exisiting X-band receiving systems were ruled more 9 


Figure 6  Comparative sizes of the data volumes Note that LSW VSWIR and LSW/Cloud TIR represent 98 and 97 of the overall data volume respectively cost-effective 223TDRSS is a communication signal relay system which provides tracking and data acquisition services between low earth orbiting spacecraft and NASA/customer control and/or data processing facilities\224 TDRSS while not supporting as high rates as can be achieved by X-band is capable of continuously operating for long link times   20 minutes per orbit TDRSS was investigated due to its long link times and its convenient ground station location at White Sands However using TDRSS with the current return link data rate would require more power from HyspIRI as well as more complicated structural requirements due to the need for a larger antenna to meet Effective Isotropic Radiative Power EIRP requirements on the HyspIRI-to-TDRSS-side LaserComm is a relatively new technology under development in labs including JPL LaserComm has the potential to transmit data at extremely fast rates up to 10 Gbps making it attractive to the HyspIRI mission While the link speed has been demonstrated the link rate does not seem to be supportable due to the data rate capabilities of the Solid State Recorder SSR Faster storage media must be developed before the satellite can support such a high data rate It also remains the most expensive of the communications system options X-band was deemed to be the best solution overall for HyspIRI The X-band system is estimated to be the least expensive of the three options while having excellent heritage in the DigitalGlobe WorldView-1 satellite both for data rate and data volume X-band also pro vides HyspIRI with the ability to reduce the data rate as GeoEye-1 does to utilize un-upgraded ground stations and increase the data envelope should the upgraded ground stations at 800 Mbps fail Additionally many high-latitude ground stations such as Svalbard and Fairbanks Alaska 20 21 are compatible with the X-band frequencies X-band TDRSS LaserComm Power Required 200 W 320 W 40-60 W Data Rate 800 Mbps 300 Mbps 6 Gbps Data Margin 28.5 13.3 23.7 Cost 9.5M 11.2M 20-30M Table 7  Comparison of the different communications architectures considered Based on these characteristics an X-band-based architecture was baselined for the mission X-band Architecture Details High data rate X-band heritage\227 Two LEO imaging satellites were benchmarked to 002nd a starting point for the design of the X-band communications system The 002rst is the DigitalGlobe WorldView-1 satellite launched in September of 2007 Using two 400-Mbps X-band radios it achieves an overall data rate of 800 Mbps The other is the GeoEye-1 Satellite which will be launched in September of 2008 which is speci\002ed to be capable of 740 Mbps and 150 Mbps The DigitalGlobe WorldView-1 is of special interest because of its data rate The satellite is speci\002ed to collect 331 gigabits of data per orbit which is extremely close to the 343 gigabits per orbit expected to be collected by HyspIRI mission Spacecraft Segment for X-band\227 To achieve 800 Mbps transmission the satellite would use two 400 Mbps radios simultaneously To accomplish this at the same band the signals would be linearly and perpendicularly polarized The WorldView-1 satellite which operates at 800 Mbps uses the L-3 Communications T-722 X-band radio which operates between 8 and 8.4 GHz The radio has an output power of 7.5 W with an OQPSK modulation scheme which increases bitrate from BPSK without introducing error associated with higher-order PSK T o determine if this system w ould be feasible for HyspIRI a link budget was created as can be seen in Table 6 After determining the desired acquisition location the link geometry see Table 8 was used to conduct link budget sizing using a sample link budget from David Hansen at JPL The budget was set up to produce a minimum link margin of 6 dB with the available 7.5 W that is speci\002ed by the T-722 radio The b udget is for one radio each radio requiring data rates of 400 Mbps since the other will be identical but perpendicularly polarized the budget applies for both radios As this rough link budget shows Svalbard ground station is theoretically capable of 10 dB more gain than is necessary to 10 


GS Link Elevation 10 016 above horizon Max Slant Distance 1984 km Off-Nadir Gimballing Angle 64 016 Table 8  Link Geometry for 623 km Note the Gimballing angle must be 64 degrees this has been ruled feasible with a 0.25m-dia parabolic antenna as noted in Table 6 make the link Therefore there is room to reduce the power decrease the size of the antenna or use smaller receiver antennas on the ground and gain access to more ground stations while maintaining a robust link Physically the radio system will require precise gimballing and rotational control to align the polarization with the receiver antenna at the ground station The SA-200HP bus that is baselined for the mission has suf\002cient spacecraft pointing knowledge to supply for the 0.5 016 pointing error assumed in the link budget The mass of the X-band system is shown below in Table 9 Item Mass Radio 3.9 kg each Antenna and Gimballing 2.5 kg S-band Rx 2.5 kg Coax Switch 0.4 kg Total Mass 12.2 kg Table 9  Mass Budget for X-band Architecture Because the two radios will be downlinking from the same pool of onboard data it is necessary to create a 002le splitting scheme to downlink the data see Figure 7 Since the instrument compression will be reset every several lines for error containment the data that is processed will be separated by geographical segments These segments will each correspond to a 223\002le bundle\224 which contains the instrument data calibration data and cloud map data associated with that segment's geographical location Metadata in the 002les will be used to designate the location the 002le bundle correlates to Each 002le bundle will be assigned to a radio so that at any one moment two 002le bundles are transmitted together During transmission the next 002le bundle will then be queued for the next available radio Since each 002le bundle contains metadata about its location the data can be pieced back together at the ground station Ground Segment for X-band\227 Multiple NASA ground stations support X-band communications with current antennas These ground stations include high-latitude ground stations Svalbard Fairbanks TrollSat McMurdo TERSS and many others These ground stations have dual-polarizing parabolic antennas which would be necessary to support the downlink radio frequency structure While this band is supported at Figure 7  Each radio will downlink a single 223File Bundle at a time these stations no station supports data rates higher than 150 Mbps while most support approximately 100 Mbps Therefore to use the 800 Mbps link the ground stations must be upgraded from their current speed with new electronics and data storage For the purposes of communications system sizing it was assumed that with a combination of ground stations it would be possible to provide an average of 10 minutes of downlink time per orbit This could be accomplished by upgrading two to three high latitude ground stations As a contingency option it should be feasible to decrease the data rate of HyspIRI's communications system to 100 Mbps to utilize more ground stations for additional coverage time and data volume or as a backup in case one of the high-rate ground stations is unavailable Data Volume for X-band Communications\227 At 800 Mbps transfer rate with an average of 10 minutes of contact time per orbit the X-band system will be able to downlink 7,125 Gb while the satellite should generate 5,097 Gb on average This scenario yields an overall data margin of 28.5 Cost of X-band Communications\227 The estimated cost for Xband communications is shown in Table 10 Satellite communications costs are taken from the last TeamX report conducted in 2007 Ground station costs are estimates after consulting Joseph Smith at JPL Dedicated data line costs were estimated at 1M each over 3 years 11 


Table 10  Estimated cost of the X-band system 5 G ROUND S TATION S ELECTION As mentioned previously there are several ground stations that are compatible with X-band communications frequencies It was necessary to determine how many and which ground stations should be used in order to accommodate the HyspIRI mission's large data downlink volume Analysis Methods Twenty ground stations were considered in this analysis It was assumed that the ground stations would be upgraded to accommodate the 800 Mbs data rate required by the HyspIRI mission A driving factor in the ground station selection process was the amount of onboard data storage available currently 1.6Tb based on Team X A Satellite Orbit Analysis Program SOAP scenario was created to provide the downlink durations over the entire 19-day period for each instrument PERL scripts created by Francois Rogez at JPL translated the downlink durations into the amount of data being stored onboard the spacecraft based on the data downlink and uplink rates While the onboard storage capabilities of the spacecraft limited the possible ground station options other factors also in\003uenced the selection of the ground station con\002guration To account for these factors a decision making tool was created With respect to the onboard storage the decision making tool took into account the peak volume the slope volume vs time and the average volume of the data stored over the 19 day repeat cycle In addition to these characteristics the con\002gurations were also evaluated based on the cost associated with upgrading the ground stations the assumed reliability and the political control of each ground station The objective function is shown in Equation 1 The T subscript represents the target and the A subscript represents the actual value The objective function target values are given below in Table 11 obj f unc  013 M axP eak T 000 M axP eak A M axP eak T  f Slope T 000 Slope A Slope T  037 AvgV al T 000 AvgV al A AvgV al T  016  Reliability 000 1  017  P oliticalControl 000 1  036 numGS 000 AvgV alGS AvgV alGS 1 where 013  f  037  016  017  036  1 2 Table 11  Target Values of Objective Function Attributes Four scenarios were created by varying the values of the weighting coef\002cients used in Equation 1 see Table 12 The scenarios were run using one two and three ground stations The ground station combination that minimized the objective function was then chosen as the baseline con\002guration Results It was found that the HyspIRI mission's data volume is too large to be accommodated by just one ground station For the scenarios generated using just one ground station the amount of data stored onboard the spacecraft increased linearly over the 19 day collection cycle clearly violating the 1.6Tb onboard data storage constraint The amount of data collected by the HyspIRI mission is also too large to be accommodated by two ground stations again the amount of data stored onboard increases almost linearly over the 19 day repeat cycle With three ground stations however there are several feasible ground station combinations for this mission Of the 20 possible ground station combinations there are 11 solutions where the onboard data storage volume varies sinusoidally see Figure 8 Note that the trial numbers correspond to the ground station numbers shown in Table 13 However once the 1.6Tb onboard data storage volume constraint is included in this analysis there are only two feasible ground station con\002gurations see Figure 9 The con\002gurations that meet the onboard data storage requirements are Fairbanks McMurdo and TrollSat and Fairbanks Svalbard and TrollSat 12 


Figure 8  Close-up Time series of amount of data stored onboard for scenarios using three ground stations upgraded to 800 Mbps Figure 9  Three ground stations upgraded to 800Mbps solutions with 1.6Tb onboard storage constraint 13 


Table 12  Objective Function Coef\002cient Scenarios Sensitivity Analysis Given that only two combinations of three ground stations do not exceed the onboard data storage limits it is necessary to understand the sensitivity of our results to the onboard data storage limits This was accomplished by varying the weighting coef\002cients of the objective function see Table 12 Table 14 shows the results for each of the scenarios Table 14 shows that for every scenario TrollSat was included in the best ground station combination Fairbanks was the second-most favored ground station with McMurdo and Svalbard tied for third place Sweden and Tasmania were never chosen for the best combination This does not mean that they were not feasible solutions However with the given weighting functions they were never chosen as the best solution The optimal ground station con\002guration from Scenario 2 Fairbanks McMurdo and Trollsat was baselined for this mission due to its emphasis on the 1.6Tb maximum onboard data storage constraint Preliminary analysis was conducted into the sensitivity of the system to missing a downlink opportunity This analysis was conducted for the ground link scenario experienced on the second day of the spacecraft's 19 day repeat cycle Also this analysis does not look at the possibility of loosing contact with several different ground stations over the course of a day Preliminary results are shown in Table 15 and a more detailed discussion can be found in It can be seen that virtually all of the cases considered exceed the 1.6Tb of available onboard storage More work needs to be done in this area to adequately address the risk of missing downlink opportunities throughout the mission Table 13  Ground Station Reference Numbers More advanced HyspIRI mission design conducted at JPL by Team X has now expanded the available onboard data storage to 3Tb 6 S CIENCE A NALYSIS In order to more fully understand the requirements associated with the data throughout the mission it was necessary to de\002ne data levels both on-board the spacecraft and on the ground By de\002ning the form and scienti\002c signi\002cance of the data at each stage a greater understanding of the data system's role was gained Additionally by gaining a greater understanding of HyspIRI's data products it was possible to identify missions with which HyspIRI could produce complementary science products Data Level De\002nition De\002nitions for data levels follow see Figure 10 Level 0 Raw Data HyspIRI will collect two types of raw data the science data from the two sensors and the calibration data As the sensors collect photons they will output analog signals resulting from the light intensity levels as well as the sensors material properties The calibration data is obtained through lunar blackbody solar and dark level calibration Lunar and blackbody calibration will be conducted by satellite pointing Solar calibration will be conducted by re\003ecting solar light off the instrument cover Dark level calibration will be conducted by closing the instrument cover Ephemeris data will be obtained through the onboard GPS and star-sensors The calibration data will be needed on-board the spacecraft in order to conduct the preliminary calibration required for cloud detection Level 1A Roughly Calibrated to Re\003ectance Data Table 14  Objective Function Results 14 


Ground Station Number of Passes Missed Max Required On-board Storage Fairbanks 1 2 Tb All 8 2.7 Tb Trollsat 1 1.5 Tb All 10 3.7 Tb Svalbard 1 1.7 Tb All\(10 3.8 Tb Table 15  Amount of storage required when downlink opportunities are missed These preliminary results were computed on the second day of the spacecraft's 19 day repeat cycle All passes missed indicates that all the passes over the ground station were missed for a given day Figure 10  The data level de\002nitions are presented The chart 003ows from left to right top to bottom through levels 0 1A 1B 1C 2 3 and 4 These visual examples belong past mission imagery Visual examples for levels 1A 2 and 3 belong to AVIRIS data and calibration charts while visual examples for Level 4 belongs to ASTER data Once the data is converted to digital number values it will be calibrated using the dark level calibration values to correct for sensor offsets due to material 003aws During dark level calibration the sensors should not detect any signal thus any signals from the sensor indicate an erroneous offset This dark-level-calibrated data must then be converted from sensor readings to spectral radiance values This conversion process takes inputs from solar and satellite zenith and azimuth angles radiometric information stored onboard in coef\002cient appendices and top-of-atmosphere effects scattering The data will be tagged with universal collection time and latitude and longitude Radiance values that have been atmospherically corrected can then generate re\003ectance values This conversion must be completed onboard because re\003ectance allows for the detection of clouds for the VSWIR instrument Level 1B Compressed Packetized Data Level 1B data consists of compressed packetized data Cloudy pixels undergo a lossy compression and a lossless compression while non-cloudy land-and-shallow-water pixels undergo only lossless compression Ocean pixels will undergo lossy compression Because there is little scienti\002c value gained from VSWIR images of clouds cloud maps are generated to enable higher compression for clouds In the case of the TIR all land-and15 


shallow-water data regardless of cloud presence will be subject to lossless compression Compressed Level 1A data is then packaged alongside the ephemeris metadata to become Level 1B data which is downlinked to ground stations Packetization is done such that communications errors causing the loss of one packet will not affect the data integrity of other packets Level 1C Decompressed Depacketized Data Once data packets are received on ground the data is depacketized and decompressed The packets are then assembled to create full-resolution continuous data sets as Level 1C data This data is backed up and will be stored permanently as is described below Level 2 Atmospheric Corrected Data In order to generate Level 2 data instrument measurement values previously calibrated are resampled remapped and recalibrated with more sophisticated and accurate methods Data is also geometrically calibrated and orthorecti\002ed Further corrections may involve cosmic ray compensation and bad pixel replacement Spectral signatures are identi\002ed A series of high-level information physical optical radiance and other derived elements can be viewed at this level Spectral signatures derived from re\003ectance surface radiance temperature emissivity and composition are available at this level Level 3 Mapped Data Geophysical parameters are mapped onto uniform space grids located in space and time with instrument location pointing and sampling Volcanic eruptions landslides wild\002res natural resources drought soil and vegetation type will be mapped Gridded maps are created for each image Files may be located regionally A series of overlapping data per orbit may be encountered All 002les will be made available including overlapping 002les Level 4 Temporal Dependant Data As the instruments 002nish their repeat cycles they will produce global frames of science products 5 days per frame for the TIR and 19 days for the VSWIR With these frames it is possible to characterize the earth in terms of the science data products every repeat cycle These frames will be compiled based on collection time producing a 4-D model of the earth which can be used to view local or global changes as a function of time This 4-D model would be made available on a monthly seasonal or yearly basis Synergy with Other Missions The HyspIRI mission will produce science products for various monitoring and early warning programs If combined with science products from other missions the HyspIRI science products can serve for even greater societal bene\002t and scienti\002c interest HyspIRI's science products can be combined with missions that will be collecting data simultaneously around 2013-2016 as well as with past and future missions and ground-based measurements These will permit the creation of high-impact science products It is possible that the HyspIRI mission could collaborate with the Aerosol and Cloud Ecosystems ACE mission which is also motivated by the NRC Decadal Survey By collaborating with ACE 226 a mission scheduled launch between 2013 and 2016 226 more accurate predictions of aquatic ecosystem responses to acidi\002cation and organic budget changes can be obtained from disturbance effects on distribution biodiversity and functioning of ecosystems HyspIRI science products as well as ocean CO 2 sinking and acidity changes ACE science products Also correlation between ecosystem changes and ocean organic measurements can be studied by combining disturbance effects on distribution biodiversity and functioning of ecosystems HyspIRI science products and oceanic organic material budgets ACE science product Through collaboration with other mission's the impact of HyspIRI's data products will be increased HyspIRI's many science products produced at the various data levels will add to the large volume of data that must be stored once the data is sent to the ground Additionally clear de\002nitions of the science products and data levels will enable the creation of an ef\002cient and accessible ground data storage system 7 D ATA S TORAGE AND D ISTRIBUTION Long-term data and science product storage is becoming an increasingly relevent issue in data system design Afterall the worth of the mission is based on the accessibility and usability of the data once it has been sent to the ground The combined data collection rate of HyspIRI's two instruments exceeds 1.5TB/day and nearly 0.56 PB/year The raw data ingest rate alone would increase the current Earth Observing System Data and Information System EOSDIS daily archive growth by 48 not including the storage of science products The HyspIRI Adaptive Data Processing System HIIAPS is a proposed data processing architecture based on the Atmospheric Composition Processing System ACPS with minor modi\002cations HIIAPS would handle the challenge of archiving processing and distributing such an immense amount of data while keeping costs low Lessons Learned from EOSDIS The Earth Observing System EOS Data and Information System EOSDIS began development and implementation in the early 1990s and has been fully operational since 1999  28 29 EOSDIS is the primary information infrastructure that supports the EOS missions The responsibilities of EOSDIS could originally be divided into two sectors mission operations and science operations 27 28  Mission Operations handles satellite 003ight operations data capture and initial processing of raw data Most of the Mission Operations responsibilities are now handled by Earth Science Mission Operations ESMO 32 Sci16 


ence Operations processes distributes and archives the data  27 28 31 N ASA Inte grated Services Netw ork NISN mission services handle the data transfer between the mission operations system and the science operations systems 30 There are several valuable lessons that were learned from the operation and continued evolution of EOSDIS The Goddard Earth Sciences GES Distributed Active Archive Center DAAC and Langley Research Center LaRC Atmospheric Science Data Center ASDC evolutions emphasize reducing operations costs by maintaining only a single processing system increasing automation through the use of simpler systems reducing engineering costs through the replacement of COTS commodity-off-the-shelf products with commodity products and increasing accessibility by increasing disk usage by phasing out existing tape based systems The par allel development of Archive Next Generation ANG 257 e and Simple Scalable Script-Based Science Processing Archive S4PA at GES DAAC and LaRC ASDC DAAC demonstrate that NASA continues to encourage individual entities such as the DAACs and Science Investigator-led Processing Systems SIPSs to suggest and develop new approaches A summary of the lessons learned from the development and continued operation of EOSDIS from 223Evolving a Ten Year Old Data are brie\003y listed belo w 017 Having a program requirement for continuous technology assessment establishes a culture of innovation 017 An on-going small-scale prototyping effort within operational elements is important for exploring new technologies and hardware 017 Involvement by all affected parties is essential to evolution planning 017 It is necessary to collect and analyze system usage metrics e.g request patterns for various data products in planning for the future 017 Flexible management processes accommodating innovation speed and ef\002ciency are essential for increasing agility in development despite the higher perceived risks 017 A risk management strategy must be included in planning for evolution 017 Transitions affecting the user community need to be carefully managed with suf\002cient advance noti\002cation All of these lessons point toward an independently operated system built for continuous evolution As has been stated before HyspIRI generates extremely large amounts of data A highly scalable and adaptive system would be the most cost effective approach to handle the processing archiving and distribution needs of the mission The presence of a culture of innovation would facilitate the necessary continuous reassessment of the technology present at HIIAPS Such technology reassessments would facilitate a smooth continuous integration of new processing and storage components in both the hardware and software realms Thus initial development costs could be reduced by eliminating the need to purchase all storage media at once and instead purchasing storage media gradually as costs and performances improve and as they are needed To be able to do this in the most cost effective manner a system usage analysis should be conducted in order to properly size the storage and processing systems For such a system there should be a detailed plan in place for the migration of data from old to new storage media as the older storage media becomes obsolete or nonfunctional Keeping all affected parties involved is especially relevant to the formatting of the data which will be discussed later Involvement of these parties combined with continuous smallscale prototyping speeds the generation and integration of new ideas and also serves as a risk management measure to prevent changes being made to the system that would be detrimental to the user community Involvement of these parties also helps to determine how access to data should be kept online and how that data might be most easily accessed online HyspIRI Adaptive Processing System HIIAPS HyspIRI Adaptive Processing System pronounced 223high-apps,\224 is a proposed stand alone data processing system which would handle the data processing storage and distribution requirements of the HyspIRI mission It would be based on ACPS with only the minor modi\002cations that occur in adapting a previous system to the slightly different needs of another mission JPL's Team X states that 223Science team has responsibility for all storage and analysis    Data is made available online to full science team for providing atmospheric corrections and producing products\224 An A CPSlike processing system would ful\002ll these requirements HIIAPS would be a centralized dedicated SIPS and DAAClike system that would be co-located with the instrument and science teams The MODIS Adaptive Processing System MODAPS has demonstrated that co-locating the science team with the processing center reduces cost by saving time on the development and integration of new algorithms  It is also important to note that HIIAPS w ould ha v e no commercial or proprietary parts thus evading the issues of software licensing A diagram of the HIIAPS data processing context is shown in Figure 11 All storage and analysis for the HyspIRI mission would be done on site Level 1C data is archived and distributed at HIIAPS HIIAPS will also process and archive Level 3 and below data leaving higher level products to academia and the private sector Data products would be sent directly to the HypsIRI science team and an offsite archive This offsite archive could be a DAAC or another data processing center that serves as a risk management precaution in the event of a natural disaster at the HIIAPS site Like MODAPS HIIAPS would employ data pool technology to save on storage and processing costs Higher level smaller data products and recently created products would be kept in the data pool while more cumbersome products like Level 1C data would be produced on demand Also like previous systems HIIAPS would provide custom data products in the form of virtual products that are made to order By building upon the knowledge base 17 


Figure 11  HIIAPS Data Processing Flow of previous processing systems HIIAPS would both reduce the costs and risk of creating a new processing center Storage Media Disk vs Tape Initially EOSDIS used tape-based storage Now it is slowly transitioning to disk based storage 31 34 T ape is easily transportable has faster read/write speeds but it is sequential access storage and a nearline media 36 37 On the other hand disk has a higher storage capacity per unit makes the data easily accessible online experiences more frequent technology upgrades but has higher electrical costs and is more prone to error 36 37 Disk storage is also more convenient to use because it is random access and data can be kept online as opposed to nearline or of\003ine all the time thus shortening data retrieval times It was been shown that disk storage is more expensive in the short term but in the long term the cost difference between disk and tape systems seems to converge This combined with the f act that ACPS the Ozone Monitoring Instrument Data Processing System OMIDAPS and MODAPS are all disk based storage leads to the conclusion that HIIAPS should also be an entirely disk based system especially with the Kryder's Law disk storage densities double every 23 months taken into account Data Formats As the previous section states it is imperative that HyspIRI data is made available to as many non-remote sensing scientists as possible Part of achieving this goal will be accomplished by distributing the data in a format that is easy to use A standard format would be helpful because it would guarantee that everybody could read that 002le format However there is no single standard format for geospatial data  N ASA  s EOSDIS currently uses HDF-EOS5 which is good for processing but not everyone can easily read the HDF 002le format 40 41 Ev en though there is no standard some formats are more easily accessible than others Using a GIS Geographic Information Systems format like a 003at georeferenced JPEG or PNG image would make HyspIRI data readily accessible to an already large user base and to those who do not use a GIS program The Open GIS Consortium Inc R r OGC is working to solve this formatting issue and towards Open GIS 43 223Open GIS is the full integration of geospatial data into mainstream information technology What this means is that GIS users would be able to freely exchange data over a range of GIS software systems and networks without having to worry about format conversion or proprietary data types Working with the OGC or at least staying in touch with their progress would help to maximize interoperability and accessibility Based on this HyspIRI should have at least two standard data formats One would be HDF-EOS5 because that is the standard NASA EOS format and HIIAPS can draw on pervious NASA experience for it The second would be a GIS ready format like a georeferenced JPEG or PNG to increase accessibility to HyspIRI data This section looked at how HyspIRI might 002t into the EOSDIS framework several previous processing systems and touched on accessibility and interoperability concerns HyspIRI should take advantage of the present EOSDIS infrastructure However HIIAPS should be a stand alone processing center so that HyspIRI's large data volume does not overtax the existing EOSDIS system There are three heritage systems for HIIAPS MODAPS OMIDAPS and ACPS each an the evolution of the next There has been a steady shift from tape-based storage to disk-based storage over the years Thus HIIAPS should use a disk-based storage system In addition to storage and processing distribution is also a concern for the HyspIRI mission HyspIRI data should be made available at no charge to as many people as possible in a manner accessible to as many people as possible Some of the challenges are choosing a data format reaching non-remote sensing scientists and providing the data in an easy to 002nd manner Using Google Earth TM and supplying HyspIRI data products in more than one format will help to solve this challenge 8 C ONCLUSION An end-to-end data system has been described for the HyspIRI mission a high data volume Earth-observing mission With the HyspIRI mission ambitious science objectives are driving the development of a cutting-edge data handling system both on-board the spacecraft and on the ground 18 


The HyspIRI mission utilizes innovative techniques to both reduce the amount of data that must be transmitted to the ground and accommodate the required data volume on the ground The infrastructure and techniques developed by this mission will open the door to future high data volume science missions The designs presented here are the work of the authors and may differ from the current HyspIRI mission baseline A CKNOWLEDGMENTS This research was carried out at the Jet Propulsion Laboratory California Institute of Technology and was sponsored by the Space Grant program and the National Aeronautics and Space Administration R EFERENCES  K W ar\002eld T  V  Houten C Hee g V  Smith S Mobasser B Cox Y He R Jolly C Baker S Barry K Klassen A Nash M Vick S Kondos M Wallace J Wertz Chen R Cowley W Smythe S Klein L Cin-Young D Morabito M Pugh and R Miyake 223Hyspiri-tir mission study 2007-07 002nal report internal jpl document,\224 TeamX 923 Jet Propulsion Laboratory California Institute of Technology 4800 Oak Grove Drive Pasadena CA 91109 July 2007  R O Green 223Hyspiri summer 2008 o v ervie w  224 2008 Information exchanged during presentation  S Hook 2008 Information e xchanged during meeting discussion July 16th  R O Green 223Measuring the earth wi th imaging spectroscopy,\224 2008  223Moore s la w Made real by intel inno v ation 224 http://www.intel.com/technology/mooreslaw/index.htm  T  Doggett R Greele y  S Chein R Castano and B Cichy 223Autonomous detection of cryospheric change with hyperion on-board earth observing-1,\224 Remote Sensing of Environment  vol 101 pp 447\226462 2006  R Castano D Mazzoni N T ang and T  Dogget 223Learning classi\002ers for science event detection in remote sensing imagery,\224 in Proceedings of the ISAIRAS 2005 Conference  2005  S Shif fman 223Cloud detection from satellite imagery A comparison of expert-generated and autmatically-generated decision trees.\224 ti.arc.nasa.gov/m/pub/917/0917 Shiffman  M Griggin H Burk e D Mandl and J Miller  223Cloud cover detection algorithm for eo-1 hyperion imagery,\224 Geoscience and Remote Sensing Symposium 2003 IGARSS 03 Proceedings 2003 IEEE International  vol 1 pp 86\22689 July 2003  V  V apnik Advances in Kernel Methods Support Vector Learning  MIT Press 1999  C Bur ges 223 A tutorial on support v ector machines for pattern recognition,\224 Data Mining and Knowledge Discovery  vol 2 pp 121\226167 1998  M Klemish 223F ast lossless compression of multispectral imagery internal jpl document,\224 October 2007  F  Rizzo 223Lo w-comple xity lossless compression of h yperspectral imagery via linear prediction,\224 p 2 IEEE Signal Processing Letters IEEE 2005  R Roosta 223Nasa jpl Nasa electronic parts and packaging program.\224 http://nepp.nasa.gov/docuploads/3C8F70A32452-4336-B70CDF1C1B08F805/JPL%20RadTolerant%20FPGAs%20for%20Space%20Applications.pdf December 2004  I Xilinx 223Xilinx  Radiation-hardened virtex-4 qpro-v family overview.\224 http://www.xilinx.com/support/documentation data sheets/ds653.pdf March 2008  G S F  Center  223Tdrss o v ervie w  224 http://msp.gsfc.nasa.gov/tdrss/oview.html 7  H Hemmati 07 2008 Information e xchanged during meeting about LaserComm  223W orldvie w-1 224 http://www digitalglobe.com/inde x.php 86/WorldView-1 2008  223Sv albard ground station nor way.\224 http://www.aerospacetechnology.com/projects/svalbard 7 2008  223Satellite tracking ground station 224 http://www.asf.alaska.edu/stgs 2008  R Flaherty  223Sn/gn systems o v ervie w  224 tech rep Goddard Space Flight Center NASA 7 2002  223Geoe ye-1 f act sheet 224 http://launch.geoeye.com/launchsite/about/fact sheet.aspx 2008  L-3 Communications/Cincinnati Electronics Space Electronics 7500 Innovation Way Mason OH 45040 T-720 Transmitter  5 2007 PDF Spec Sheet for the T720 Ku-Band TDRSS Transmitter  L-3 Communications/Cincinnati Electronics Space Electronics 7500 Innovation Way Mason OH 45040 T-722 X-Band  7 2007 PDF Spec Sheet for the T-722  J Smith 07 2008 Information e xchanged during meeting about GDS  J Carpena-Nunez L Graham C Hartzell D Racek T Tao and C Taylor 223End-to-end data system design for hyspiri mission.\224 Jet Propulsion Laboratory Education Of\002ce 2008  J Behnk e T  W atts B K obler  D Lo we S F ox and R Meyer 223Eosdis petabyte archives Tenth anniversary,\224 Mass Storage Systems and Technologies 2005 Proceedings 22nd IEEE  13th NASA Goddard Conference on  pp 81\22693 April 2005 19 


 M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Evolving a ten year old data system,\224 Space Mission Challenges for Information Technology 2006 SMC-IT 2006 Second IEEE International Conference on  pp 8 pp.\226 July 2006  S Marle y  M Moore and B Clark 223Building costeffective remote data storage capabilities for nasa's eosdis,\224 Mass Storage Systems and Technologies 2003 MSST 2003 Proceedings 20th IEEE/11th NASA Goddard Conference on  pp 28\22639 April 2003  223Earth science data and information system esdis project.\224 http://esdis.eosdis.nasa.gov/index.html  M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Evolution of the earth observing system eos data and information system eosdis\\224 Geoscience and Remote Sensing Symposium 2006 IGARSS 2006 IEEE International Conference on  pp 309\226312 31 2006Aug 4 2006  223Earth science mission operations esmo 224 http://eos.gsfc.nasa.gov/esmo  E Masuoka and M T eague 223Science in v estig ator led global processing for the modis instrument,\224 Geoscience and Remote Sensing Symposium 2001 IGARSS 01 IEEE 2001 International  vol 1 pp 384\226386 vol.1 2001  M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Earth observing system eos data and information system eosdis 227 evolution update and future,\224 Geoscience and Remote Sensing Symposium 2007 IGARSS 2007 IEEE International  pp 4005\2264008 July 2007  D McAdam 223The e v olving role of tape in the data center,\224 The Clipper Group Explorer  December 2006  223Sun microsystems announces w orld s 002rst one terabyte tape storage drive.\224 http://www.sun.com/aboutsun/pr/200807/sun\003ash.20080714.2.xml July 2008  223P anasas 227 welcome 224 http://www panasas.com  R Domikis J Douglas and L Bisson 223Impacts of data format variability on environmental visual analysis systems.\224 http://ams.confex.com/ams/pdfpapers/119728.pdf  223Wh y did nasa choose hdf-eos as the format for data products from the earth observing system eos instruments?.\224 http://hdfeos.net/reference/Info docs/SESDA docs/NASA chooses HDFEOS.php July 2001  R E Ullman 223Status and plans for hdfeos nasa's format for eos standard products.\224 http://www.hdfeos.net/hdfeos status HDFEOSStatus.htm July 2001  223Hdf esdis project.\224 http://hdf.ncsa.uiuc.edu/projects/esdis/index.html August 2007  223W elcome to the ogc website 224 http://www.opengeospatial.org 2008  223Open gis Gis lounge geographic information systems.\224 http://gislounge.com/open-gis Christine M Hartzell received her B.S in Aerospace Engineering for Georgia Institute of Technology with Highest Honors in 2008 She is currently a PhD student at the University of Colorado at Boulder where she is researching the impact of solar radiation pressure on the dynamics of dust around asteroids She has spent two summers working at JPL on the data handling system for the HyspIRI mission with particular emphasis on the cloud detection algorithm development and instrument design Jennifer Carpena-Nunez received her B.S in physics in 2008 from the University of Puerto Rico where she is currently a PhD student in Chemical Physics Her research involves 002eld emission studies of nanostructures and she is currently developing a 002eld emission setup for further studies on nano\002eld emitters The summer of 2008 she worked at JPL on the HyspIRI mission There she was responsible for the science analysis of the data handling system speci\002cally de\002ning the data level and processing and determining potential mission collaborations Lindley C Graham is currently a junior at the Massachusetts Institute of Technology where she is working towards a B.S in Aerospace Engineering She spent last summer working at JPL on the data handling system for the HyspIRI mission focusing on developing a data storage and distribution strategy 20 


David M Racek is a senior working toward a B.S in Computer Engineering at Montana State University He works in the Montana State Space Science and Engineering Laboratory where he specializes in particle detector instruments and circuits He spent last summer working at JPL on compression algorithms for the HyspIRI mission Tony S Tao is currently a junior honor student at the Pennsylvania State University working towards a B.S in Aerospace Engineering and a Space Systems Engineering Certi\002cation Tony works in the PSU Student Space Programs Laboratory as the project manager of the OSIRIS Cube Satellite and as a systems engineer on the NittanySat nanosatellite both of which aim to study the ionosphere During his work at JPL in the summer of 2008 Tony worked on the communication and broadcast system of the HyspIRI satellite as well as a prototype Google Earth module for science product distribution Christianna E Taylor received her B.S from Boston University in 2005 and her M.S at Georgia Institute of Technology in 2008 She is currently pursing her PhD at the Georgia Institute of Technology and plans to pursue her MBA and Public Policy Certi\002cate in the near future She worked on the ground station selection for the HyspIRI mission during the summer of 2008 and looks forward to working at JPL in the coming year as a NASA GSRP fellow Hannah R Goldberg received her M.S.E.E and B.S.E from the Department of Electrical Engineering and Computer Science at the University of Michigan in 2004 and 2003 respectively She has been employed at the Jet Propulsion Laboratory California Institute of Technology since 2004 as a member of the technical staff in the Precision Motion Control and Celestial Sensors group Her research interests include the development of nano-class spacecraft and microsystems Charles D Norton is a Principal Member of Technical Staff at the Jet Propulsion Laboratory California Institute of Technology He received his Ph.D in Computer Science from Rensselaer and his B.S.E in Electrical Engineering and Computer Science from Princeton University Prior to joining JPL he was a National Research Council resident scientist His work covers advanced scienti\002c software for Earth and space science modeling with an emphasis on high performance computing and 002nite element adaptive methods Additionally he is leading efforts in development of smart payload instrument concepts He has given 32 national and international keynote/invited talks published in numerous journals conference proceedings and book chapters He is a member of the editorial board of the journal Scienti\002c Programming the IEEE Technical Committee on Scalable Computing a Senior Member of IEEE recipient of the JPL Lew Allen Award and a NASA Exceptional Service Medal 21 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207ñ216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intíl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intíl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





