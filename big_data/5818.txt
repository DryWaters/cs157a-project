  Association rules learning technique for knowledge mining about scheduling algorithm performance Martin Dubois DI, University of Quebec at Montreal Montreal, Canada heavenc@cryptoheaven.com  Mounir Boukadoum DI, University of Quebec at Montreal Montreal, Canada boukadoum.mounir@uqam.ca Abstract With the advent of increasingly higher numbers of processors on-chip, task scheduling has become an important concern in system design, and research in this area has produced substantial and diversified knowledge. As a result the efficient management and taping of this knowledge has become a concern in itself This paper addresses the issue of how to effectively extract performance information about a scheduling algorithm in the context of a set of applications, by learning the association rules between the applications attributes and the algorithmsê performance metrics. The new methodology that is presented serves to both increase the designerês knowledge about a particular scheduling algorithm and compare algorithms   Index Terms s cheduling, data mining, knowledge, list heuristics directed acyclic graph 1  I NTRODUCTION Todayês applications increasingly require high performance and low power consumption in a smaller area As semiconductor techno logy goes towards smaller geometries and architectures become larger, the typical approach to system design has become to partition applications into simple and/or complex interacting tasks and tools are needed to help the designers Task scheduling is one aspect that has been well studied 1, 2, 3 The user ha s a n appl i c a t ion a nd a runt i m e  architecture, and the problem is to optimize the execution process according to one or more constraints. Typically, the application is represented by a directed acyclic graph DAG\here each node is executed on one processor and each arc is a communication link between two nodes Several methods exist to schedule node execution given a set of processors [1, 4 The t e chni ques usua lly f o cus on optimizing a limited number of parameters such as speed number of processor, or communication contention However, there exist several methods to compute the cost of a node or ar nd t h e differ e n t ap pro ac hes ca n have different performances for the same algorithm, depending on the scheduling policies for the DAG Many scheduling methods have been reported in the literature. As each of them has its own parameters and can have numerous sub-algorithms \(defined as variations with different properties and strengths\ choosing the optimal one for an application is not obvious, hence the resort to various metrics and to technique comparison to help solve the problem. However, the typical algorithm comparison relies on statistical measures, leading to global conclusions that apply on average, and that may not be appropriate at the local level for individual applications. On the other hand, an application has a lot of intrinsic information that can be represented in a graph and that can be combined with the features of a scheduling algorithm to determine its appropriateness for the application This paper presents a new method to extract and compare knowledge about scheduling algorithms, leading to a novel perspective on algorithm selection. It operates by providing insight about the algorithms, thus allowing comparisons for given application attributes and performance metrics. This is done by finding association  betw ee n  the two.  The pape r is st r u ct ured a s  follows: In Section II, we provide background about the scheduling problem; in section III, we present our methodology to extract knowledge about an algorithm Section IV presents experimental results for two scheduling techniques that we tested and Section V concludes this work 2  B ACKGROUND  Given a set of N tasks comprising an application, the allocation and scheduling of each task on an executive unit within a given deployment architecture starts by first creating a high level abstraction of the problem with a DAG Task-scheduling problem A directed acyclic graph, or DAG, is formally defined as a couple G = <V, E>, where V represents a set of nodes and E a set of forward-oriented edges between the nodes Each node is indivisible \(atomic\be duplicated and implemented in software or hardware. Moreover, each node and each edge have performance constraints imposed upon them by the designer. The DAG can be implemented with a homogeneous or heterogeneous architecture distributed or not.  The architecture can satisfy constraints such as number of processors, power consumption and speed.  The problem is to allocate and schedule the different nodes and their interactions to meet the imposed constraints. Since the scheduling problem is NP-Complete except for a few basic cases, the standards methods for addressing it use estimated values for the computation and communication costs and focus on specific architectures to reduce complexity   At least four classes of algorithms have been proposed for the scheduling problem: List, clustering, duplicated and random. The list methods include PETS \(Performance effective task scheduling\, HEFT \(Heterogeneous earliest finish time\ \(Critical path on a processor  and are based on timing levels and prioritized nodes; each node has a priority and the algorithm schedules nodes starting with the one having the highest priority. The CASS Clustering and scheduling system\ method [1 rs  nodes into groups, tapping the advantage of using more processors to reduce schedule length. The duplicated class 978-1-61284-137-3/11/$26.00 ©2011 IEEE 65 


  method CPFD \(Critical path fast duplicated allow s node cloning; the resulting node multiplicity copies methods for reducing communication delays. The random methods mostly use genetic or simulated annealing [8, 12   t h e y ca n b e ver y fa st  an d a r e si m p le t o  i m plem en t All the previous techniques make assumptions about the architecture to use and are most efficient for specific ones. As a result, the final value of their solutions depends on which graph and performance estimators are used which architecture is targeted, etc Performance Metrics and estimators Many performance metrics have been proposed for algorithm comparison. The metrics can be divided in two groups. The first one evaluates the algorithm by itself. The normalized schedule length \(NSL\scheduling length ratio \(SLR\and the speedup ratio [14 r e  e x am pl e s  of such metrics. NSL defines the system length as the longer time between the first and last tasks; SLR represents the ratio between the length and the critical path, defined as the longest path in a DAG including all arcs costs and computing node costs; the speedup ratio for a given application graph is the execution time ratio between a sequential execution and parallel execution. The second group of metrics serves to compare algorithms. The average percentage degradation \(APD\number of best solution \(NB typica l exa mples   APD eval ua t e s how bad an algorithm will perform with respect to another and NB is the number of times that an algorithm will perform better than another. These metrics are effective for global algorithm performance evaluation, but we can also derive useful information locally from the DAG profile The profile of a DAG can depend on its width, depth, and the communication-to-computation ratio \(CCR\ned as the total communication cost of each arc over the total computation cost of each node. This information provides insight and profile an algorithm to extract its strength and weakness 3  P ROPOSED METHODOLOGY   u t o m a te d fram ewor k t h a t  helps the designer select a scheduling algorithm from a set of alternatives. It relies on creating a repository of uniformly formatted, validated knowledge available from different sources. The sources may come from papers simulations, analyses. The framework uses a software pattern analogy to represent an algorithm, and includes information about its strengths and weaknesses in its defining record. In this paper, we propose to use association rule learning to extract this information, thus giving valuable insights about scheduling techniques to the designer.  This knowledge is for the expert users who create and know about scheduling algorithm and architecture  Association Rule mining We first define the concepts that will be used in this work Association rule: Given a set of binomial attributes A={a 1  i=1,2É,N that specify a pattern, an association rule is an implication x    y where  x and y are disjoint subsets of A. In other words, the presences of certain attributes in the pattern are concomitant with those of others, in our case sought strength or weakness indicators Confidence: Given a set of patterns P={p 1 p 2 p M  each characterized by a specific attribute set A i the confidence level of a given association  rule is the relative measure of how often the consequent of the rule is true when the antecedent is true        x Support y x Support y x Confidence     Support x relative frequency of x within P. Thus confidence x    y an be viewed as the estimate of the conditional probability of y given x There exist frequent and rare association rules [16  d e pen di n g of t h e le ve l  of a  minimal support Lift: The lift of an association rule compares its observed relative frequency to that expected in the hypothesis of independent antecedent and consequent terms. It is defined by       y Support y x Confidence y x Lift     A lift value that is greater than one indicates that patterns where x is present tend to include y more often than those where x is absent. Thus the lift of an association rule is a measure of its çinterestingness We propose to use association rule learning to extract the strengths and weaknesses of an algorithm.  Figure 1 summarizes the methodology There are five steps to follow in the discovery of association rules for a scheduling algorithm 1  Create a database of applications composed of randomly produced DAGs [9 t h a t t r i but e  definitions as propose n d re l e vant performance metrics that are computed after applying the scheduling algorithm to each DAG. The resulting information \(DAG attributes, plus metrics\h application is then stored as a numerical record 2  For each DAG attribute, create a binomial sequence that represents the presence or absence of given values or ranges thereof, so as to create a binary representation of the DAG signatureês inclusion of those properties. This is accomplished as follows: The DAG attributes are viewed as taken from a set of sets R i each one representing possible values Ö singletons or ranges Ö for one attribute. More precisely, if the DAG signature is E= \(N, C    D\here N is the number of nodes in the DAG, C the communicationto-computation ratio \(CCR  the parallelism ratio   the range of computation of each node, and D the DAG density, then, with reference to R= \(R N R C R   R  R D which provides the domain of each of the DAGês attributes, a binary representation of E can be created which signals the presence or absence of a related subrange of R i see example below 3  Repeat step 2 for the metrics computed in step 1 to convert them in the form of binary positive \(pattern negative \(anti-pattern\outcomes 4  Get the association rules from the binary sequences 66 


  obtained during steps 2 and 3. For this, we use the Rapid Miner tool [1  w h ich use s the fp-gr ow th algorithm [1 t o fi nd t h e rele va n t i t em se ts i n the d a ta and then creates the association rules \(we used 75 confidence for the bloc elements to do the data mining\ The fp-growth algorithm is a fast learner that finds the sought itemsets in a dataset by using a highly compressed representation of the data, so that it can fit into main memory even for large databases. It starts its search with a high support value and iteratively decreases it by 20% in trying to find enough itemsets about 100 or more, for rule generation by Rapid Miner 5  After the data mining, a classifier sorts the obtained association rules into types based on rule conclusion When the conclusion is one or more metrics in relation to the quality of interest, the performance of the algorithm is evaluated; else, or if it only relates to ranges for the elements of E, it serves to increase knowledge about the characteristics of a set of applications An example of use of the methodology follows Consider a DAG profile E = {20, 0.12, 0.56, 0.25, 0.13 and a metric m with value 2.66. Assume also that we have R N 20,40,80} \(where the numbers stand for either ranges or enumerations\, R C 0.5,1,2}, R  0.5,1,2 R  0.1,0.5,1} and R D 0.25,0.5,0.75,1} for E and R m 1, 2, 3} for m. Then, Eês elements have corresponding binomial representations Nê=100, Cê=100  010  010, and Dê=0010, and m is encoded as m=001. Thus the binary representation of the attribute list will be 100 100 010 010 0010 001, and the different bits can be labeled N1,N2,N3, C1, C2, C3  1  2  3  1  2  3, D1,D2,D3,D4,M1,M2,M3 with respect to the different attributes they represent. E then has attributes N1, C1  2  2, D3 and M3 in the example With the described methodology, the expert can consider a group of applications and use the obtained association rules to modify the parameters of a particular scheduling algorithm or create a new one. In the next section, we focus on a rule with a metric as conclusion  Figure 1: Knowledge discovery 4  E XPERIMENTAL R ESULTS  We demonstrate here how association rules learning can be performed on two scheduling algorithms, HEFT and CPOP, and allow comparing them. From the algorithms perspective, the rule premises represent a DAG \(application characteristic\nd the conclusions a metric. We used the speedup metric for the experiments, but other metrics could have been used as well \(e.g. APD, SLR, contention, power etc\ We set a speedup factor for three processors to be 20% or more to bear interest for us. Let M={SUP, !SUP be the corresponding logical values, where !SUP is the negation of SUP. We want to know when one of the algorithms leads to a speedup value greater or equal to 20 and when the value is below 20%.  Table 1 shows some of the association rules that were extracted from the created test database when using HEFT \(There were other rule types extracted from the database, but we concentrate on rules with a metric conclusion in this paper Table 1: Some HEFT rules extracted using the fp-growth algorithm Rule Premises Conclusion Support Conf. Lift 1  2  3, C1 SUP 0.063 0.974 1.542 2  2, C1 SUP 0.159 0.964 1.526 3  2  2, C1 SUP 0.067 0.957 1.515 4  2, C3 SUP 0.143 0.955 1.515 5  2  2, C3 SUP 0.062 0.953 1.509 6 C1 SUP 0.175 0.799 1.264 7  1,C4 !SUP 0.042 0.974 2.644 8  1 !SUP 0.177 0.961 2.609 9  2  1 !SUP 0.077 0.960 2.607 10  3  1 !SUP 0.067 0.958 2.607 11 N1  1 !SUP 0.043 0.876 2.380 As Table 1 shows, the lift value is greater than one and confidence is high. Rules 1 to 6 show where the algorithm is strong with respect to SUP and 7 to 11 where it is weak. In particular, we notice that HEFT performs poorly when  the parallelism factor, is within the range  1  1 signals an application with low parallelism, e.g. 0 to 0.5\ Table 1 shows that, in this case \(rule 8\ of the applications in the database fail to show a 20 % speedup when scheduled with HEFT. For the DAG profile example given in the previous section, we see that E is subject to rules 2, 3, 6 and, hence, we can expect at more than 95 confidence that HEFT will schedule it with a speedup of 20%. Indeed, the speedup is this case was 2.66 Rule 6 also shows that HEFT meets the SUP requirement 79.9% of the time when C1 alone is present This percentage jumps to 96.4% when both C1 and  2 are present \(Rule 2  2 stands for a parallelism factor between 0.5 and 1.0 Table 2: Some CPOP rules extracted using the F-grow algorithm  Rule Premises Conclusion Support Conf. Lift 1  2  3, C1 SUP 0.055 0.858 1.856 2  2, C1 SUP 0.136 0.825 1.784 3  2  3, C3 SUP 0.046 0.811 1.755 4  2  2, C1 SUP 0.057 0.804 1.740 5  2, C3 SUP 0.117 0.785 1.699 6  1 !SUP 0.182 0.989 1.838 7  1  1 !SUP 0.079 0.988 1.836 8  1  2 !SUP 0.069 0.988 1.836 9 N1  1 !SUP 0.047 0.964 1.792 67 


  Table 2 shows the association rules extracted when using CPOP. As with Table 1, we can derive conclusions from them. For one thing, the confidence values for the algorithm strength are lower than in table 1.  Similarly to HEFT, CPOP is not good for applications where    1 with 98.8% confidence\t confidence is 85.8 and it occurs for the premise  2  3, C1 In our result, we use a speedup threshold of 20% for the speedup metric, we could have also used fail, pass good, and excellent as a range of values. The ensuing premises and conclusions would have changed accordingly Table 3  confidence comparison of HEFT and CPOP algorithms Rule   Rule HEFT Rule CPOP Premises Conc  conf 1 1 1  2  3, C1 SUP 11.6 2 2 2  2, C1 SUP 13.9 3 3 4  2  2, C1 SUP 15.3 4 4 5  2,C3 SUP 17.0 5 8 6  1 !SUP -2.8 Table 3 illustrates how we can use itemsets to compare scheduling algorithms using the same application database The rules labeled  represent identical association rules for two algorithms. The first four  rules represent strength and the fifth one weakness.  With  1 as a premise, HEFT is only 2.8% better than CPOP, but with  2, C3 the difference increases to 17%. It is also important to note that CPOP is a critical path scheduler. Thus, similar to the APD and NB metrics, our approach can evaluate the global degradation or the number of best solutions of two algorithms, but it also provides more insight into them, and we can use any premises and conclusions that are deemed useful 5  C OMPLEXITY OF THE METHODOLOGY  The method requires information about the results of each scheduling algorithm as extracted by a benchmark tool in step 1 of figure 1. To extract the association rules, there exist several algorithms. Some of them use candidate generation [19 but it is v e ry costly for lar ge data sets and can be exponential. In our experiments, over fifty thousand DAG profiles and associated metrics were generated, each one with 26 bit-long binomial representation. This led to over 1.5 million possibilities that were investigated Fortunately, Rapid Miner with fp-growth generated the rules in less than 15 s. In addition, fp-growth is efficient at extracting frequent itemsets without candidate generation by direct mining of the dataset. This allows the designer to extract a rule on demand. For example, if an association rule with the premise  3 C1 is desired, the tools can extract this particular rule The main limitations of the algorithm are those of fp-growth for extracting rules in a fixed number of iterations and the application database size, Also the data mining is not guaranteed to generate all the rules 6  C ONCLUSION  We proposed to use data mining to extract knowledge about application data sets and evaluate the impact of a scheduling algorithm on them. Our approach does not use statistics as others do, but analyses and extracts knowledge from the raw data instead. We have shown that the extracted association rules can be useful in many ways to evaluate the application data and the performance of a scheduling algorithm. We also introduced the concept of a  rule to compare algorithms.  Our goal was to spot the strengths and weaknesses of each algorithm and provide this information to the expert.  In this regard, the introduced methodology is flexible and efficiently exploits rules to increase knowledge about scheduling algorithms R EFERENCES  1  X. Tang, K.L. Li, and D. Padua. Communication contention in apn list scheduling algorithm. Science in China Series F: Information Sciences, vol. 52 \(1\, pp. 59Ö69, 2009 2  S. Jin, G. Schiavone, and D. Turgut. A performance study of multi processor task scheduling algorithms. Journal of Supercomputing vol. 43, pp. 77Ö97, 2008 3  R. Hwang, M. Gen, and H. Katayama. A comparison of multiprocessor task scheduling algorithms with communication costs Computers and Operations Research, vol. 35\(3\, pp. 976-993, 2008 4  E. Ilavarasan, P. Thambidurai, and R. Mahilmannan. Performance effective task scheduling algorithm for heterogeneous computing system. In Proc. 4th International Symposium on Parallel and Distributed Computing ISPDC 2005, pages 28Ö38, 4Ö6 July 2005 5  B. Demiroz and H.R. Topcuoglu. Static task scheduling with a unified objective on time and resource domains. The Computer Journal,  vol. 49\(6\, pp. 731-743, 2006 6  R. Sakellariou and H. Zhao. A hybrid heuristic for DAG scheduling on heterogeneous systems. In Proc.18th International Parallel and Distributed Processing Symposium,  vol. 2, page 111b, April 2004 7  R. Agrawal; T. Imielinski; A. Swami Mining Association Rules  Between Sets of Items in Large Databases", SIGMOD Conference pp 207-216, 1993 8  Yi-Wen Zhong, Jian-Gang Yang, and Heng-Nian Qi. A hybrid genetic algorithm for tasks scheduling in heterogeneous computing systems. In Proc. of 3 rd International Conference on Machine Learning and Cybernetics, vol. 4, pp. 2463Ö2468, August 2004 9  H. Topcuoglu, S. Hariri, and Min-You Wu. Performance-effective and low-complexity task scheduling for heterogeneous computing IEEE Transactions on Parallel and Distributed Systems, vol. 13\(3 pp. 260Ö274, 2002   J.C. Liou and M.A. Palis. An efficient task clustering heuristic for scheduling dags on multiprocessors. In proc. Workshop on Resource Management, Symposium on Parallel and Distributed Processing. pp 152-156, 1996   Y.K. Ahmad, I. And Kwok. A new approach to scheduling parallel programs using task duplication. In proc. International Conference on Parallel Processing, vol. 2, pp. 47-51, 1994   S. Gupta, V. Kumar, and G. Agarwal. Task Scheduling in Multiprocessor System using Genetic Algorithm. In proc. 2 nd International Conf. on Machine Learning and Computing, pp. 267Ö271, 2010   M. Houshmand, E. Soleymanpour, H. Salami, M. Amerian, and H Deldari. Efficient scheduling of task graphs to multiprocessors using a combination of modified simulated annealing and list based scheduling. In proc. 3 rd Int. Symposium on Intelligent Information Technology and Security Informatics, pp. 350-354, 2010   Y.K. Kwok and I. Ahmad. Benchmarking the task graph scheduling algorithms. In proc. 12 th Intêl Parallel Processing Symposium on Parallel and Distributed Processing, pp. 531Ö537, 1998   M. Dubois and M. Boukadoum. Toward an automated framework for task scheduling. 22 nd International conference on microelectronics Cairo \(Egypt\, 2010   Romero, C., Romero, J.R., Luna, J.M. and Ventura, S. Mining Rare Association Rules from e-Learning Data. Proceedings of 3 rd  Educational Data Mining conference. pp  171Ñ179, 2010   Mierswa, Ingo and Wurst, Michael and Klinkenberg, Ralf and Scholz, Martin and Euler, Timm:   YALE: Rapid Prototyping for Complex Data Mining Tasks, in Proc.12th ACM SIGKDD International Conf. on Knowledge Discovery and Data Mining KDD-06\, 2006   Han, J., Pei, J., Yin, Y. and Mao, R. Mining frequent patterns without candidate generation: A frequent-pattern tree approach  Data mining and knowledge discovery. Vol.8 \(1\. pp. 53-87, 2004   Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules in large databases. Proceedings of the 20th International Conference on Very Large Data Bases, VLDB, pages 487-499, Santiago, Chile, September 1994  68 


                                                                                                                                                                                                                                                                                                     1194 


and worst case under different parameter MinSup The axes and curves have the same meaning as described above. If we set Z\006\007  1 we can detect 90% of total faults by examining only 16 event handlers out of the total 176 event handlers in the best case, but in the worst case, we have to examine 89 event handlers to detect the same percentage of faults. Table IV shows the same meaning statistics as Table III but to TerpSpreadSheet From this table we can come to a conclusion that to TerpSpreadSheet our method performs very well in both the best case and worst case except that Z\006\007$%& equals to 1 Attention should be paid that although we may have to check same number of event handler to detect 90% of total faults both in the best case and worst case under some parameter MinSup but the two figures clearly indicate that we have to check fewer event handlers to detect a lower percentage of total faults in the best case. We should also not neglect that besides MinSup  equals to 1, the other 7 curves overlap each other highly both in the best case and worst case  Figure 5. Data from TerpSpreadSheet best case  Figure 6. Data from TerpSpreadSheet worst case  3  TerpWord Fig. 7 and Fig. 8 respectively show the result of our method on TerpWord in the best case and worst case under different parameter MinSup If we set MinSup 1 we can detect 90% of total faults by examining only 7 event handlers out of the total 617 event handlers in the best case but in the worst case, we have to examine 150 event handlers to detect the same percentage of faults. Table V shows the same meaning statistics as Table III but to TerpWord  Judging from this table, we can conclude that to TerpWord  our method performs very well in both the best case and worst case except that MinSup equals to 1, 7 or 8.Simliarly although we may have to check roughly same number of event handler to detect 90% of total faults both in the best case and worst case under some parameter MinSup on the whole, our method performs better in the best case. Also besides MinSup equals to 1 or 8, the other 6 curves coincide with each other highly both in the best case and worst case TABLE IV. N UMBER OF EVENT H ANDLER EXAMINED TO REVEAL 90 OF THE 139 FAULTY VERSIONS OF T ERP S PREAD S HEET  Case  Z\006\007  1  2  3  4  5  6  7  8  Best 16  30  28  28  27  26  26  26  Wor st  89  32  29  29  28  27  27  27   Figure 7. Data from TerpWord best case  Figure 8. Data from TerpWord worst case 0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 Number of faulty versions of event handler that need to be examined MS-1 MS-2 MS 3 MS-4 MS-5 MS-6 MS-7 MS-8 0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 Number of faulty versions of event handler that need to be examined MS-1 MS-2 MS-3 MS-4 MS-5 MS-6 MS-7 MS-8 0 50 100 150 200 250 0 20 40 60 80 100 Number of faulty versions of event handler that need to be examined MS-1 MS-2 MS-3 MS-4 MS-5 MS-6 MS 7 MS-8 0 50 100 150 200 250 0 20 40 60 80 100 Number of faulty versions of event handler that need to be examined MS-1 MS-2 MS 3 MS-4 MS-5 MS-6 MS-7 MS-8 
330 


TABLE V. N UMBER OF EV ENT H ANDLER EX AMINED TO REVEAL 90 OF THE 224 FAULT VERSIONS OF T ERP W ORD  Case  Z\006\007  1 2  3  4  5  6  7 8 Best  7 40  40  40  42  41  57  57 Wo r st  150  40  40  40  43  42  617  617   4  TerpPresent Fig. 9 and Fig. 10 respectively show the result of our method on TerpPresent in the best case and worst case under different parameter MinSup If we set MinSup 1, we can detect 90% of total faults by examining only 2 event handlers \(these two event handlers contain most of the faults for TerpPresent in the best case, but in the worst case, we have to examine 34 event handlers to detect the same percentage of faults. Judging from Table VI, we can reach a decision that to TerpPresent our method performs very well in both the best case and worst case except that MinSup equals to 1 or 8. Equally, we should take care that the other 6 curves overlap each other highly besides MinSup equals to 1 or 2 in the best case, meanwhile another 5 curves coincide with each other highly besides MinSup  equals to 1, 2 or 8 in the worst case From the above results we can say that our proposed method performs well for GUI software. After applying N gram analysis to the event sequences of all test cases, we can rank the event whose corresponding event handler contains faults high. Making use of the result, programmer can check a few event handlers to detect a majority of the total faults At present we have no common rule for the value selection of MinSup but we can observe from this result that if we give MinSup a too low value, then we may take into consideration some irrelevant N grams, in this condition we will get a pretty good result in the best case yet a relatively poor result in the worst case. On the contrary, if the value of MinSup is too high, some very important N grams will be abandoned by us, this will cause our fault localization technique to lose efficacy. Considering all these factors, we can give MinSup a median value when putting our method into practice, in such a case, we can expect to get a good result both in the best case and worst case. Our preliminary result indicates 3 is a candidate value for MinSup  TABLE VI. N UMBER OF EVENT HANDLER EXAMIN ED TO REVEAL 90 OF THE 139 FAULT VERSIONS OF T ERP P RESENT  Case  Z\006\007  1  2  3  4  5  6  7  8 Best  2  8  3  3  3  3  3  3 Wo r st  34  9  4  4  4  4  4  322  D  Threats to Validity There may be several threats to the validity of this paper that include, but are not limited to, the following. The results of the study, presented above, should be interpreted keeping in mind the following threats to validity  Figure 9. Data from TerpPresent best case   Figure 10. Data from TerpPresent worst case  First of all, although we can sort the events in order of probability of their corresponding event handler containing the fault successfully using N gram analysis, but to certain event of GUI software, the eventês event handler contains much source code and consequently there exists many faults in the event handler. Until now we can only identify this kind of event handler has high level of suspicion but can not identify which specific part of it is doubtful. Maybe we can combine with various traditional software fault localization techniques to further analyze this kind of event handler Another important threat to validity concerns the complexity of a few event handlers. Some event handlers may contain nested function call. Although when establishing the one-to-one relationship between event handler and fault, we take one thing with another to the best of our ability, but owing to the complication of GUI software we canêt guarantee the fullness between event handler and fault, i.e., there may exist few event handlers that also contain certain fault but the relationship is neglected by us this also can to some degree affect the accuracy of our result But the relationship between event handler and fault we use in our study exists in no doubt, if we can find the complete 0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 Number of faulty versions of event handler that need to be examined MS-1 MS-2 MS-3 MS-4 MS-5 MS-6 MS-7 MS-8 0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 Number of faulty versions of event handler that need to be examined MS-1 MS-2 MS-3 MS-4 MS-5 MS-6 MS-7 MS-8 
331 


relationship between event handler and fault, we can anticipate a better numeral result As described above, our downloaded test cases are generated according to GUIês graph model EIG and are used for smoke testing, so the length of each test case is limited For one thing, our method makes use of the information of the whole test case suite, the experimental results may not be valid under random smaller test case suite, i.e., they may not be statistically repeatable. Moreover, restricted by the limited length of each test case, we canêt systemically observe the effect of C DEF by giving it a variation range although we have full reason a choice of 3 for it should be reasonable  V  R ELATED W ORKS  In this section, we briefly review previous work related to GUI software fault localization To the best of our knowledge, this is the first work that focuses on GUI software fault localization. However, many fault localization techniques based on the contrast between failing and passing cases run-time information has been applied to conventional software. In [1   2   the autho r s  make use of various heuristics and apply them to program traces to rank statements in order of their relative suspiciousness. In [3   s t a tis ti c a ll y b a s e d tec h niq u e s r e l y o n  the instrumentations and evaluations of predicates in programs to produce a ranking of suspicious predicates. In 12  t h e au t h o r s m o d e l s o f t w a r e  e x e c u t io n t r ac es as  networks and two centrality measures are adopted to calculate the suspiciousness of each statement. Especially in 4  the m e tho d  N gram analysis is used to analyze the exact execution sequence of the program to rank the executable statements of software by level of suspicion Our proposed GUI software fault localization technique based on some important achievements in GUI testing several researchers and practitioners have discussed the following concepts and definitions that are relevant to its specific parts. In [5  Mem o n  b u ild s up  a g ood b a s i s  f o r th e  future research of GUI software by giving a reasonable and effective GUI Representation, including the representation of GUIês state, GUI events, GUI test cases, etc., also, the definition of EFG is brought up  us ing  s p e c if ic gr a p h traversal algorithms, an improvement for EFG, namely EIG is employed to generate smoke test cases. In [6  the autho r s  give a relatively formal definition of event handler   VI  C ONCLUSIONS AND F UTURE W ORKS  This paper presents a GUI software fault localization technique by analyzing the event sequences of faulty versions to sort the events in order of probability of their corresponding event handler containing the fault. Our proposed technique has been evaluated with respect to four subject programs under different parameters and results clearly demonstrate its effectiveness if we take appropriate parameter GUI software fault localization is a totally new research issue, how to carry out effective fault localization in connection with GUI softwareês characteristic is a challenging problem. This research has presented several exciting opportunities for future work. To begin with, we can combine GUI software fault localization with traditional software fault localization, we can first successfully sort the event using our proposed method, then regarding the event whose corresponding event handler has much source code we can use traditional software fault localization technique to further analyze the event handler to get a more detailed result Moreover, a process of using certain random strategy to select some test cases from the whole test case suite and then applying our method on the relatively smaller test case suite can be implemented, the mean result of this repetitive process can be used to more comprehensively evaluate the feasibility of our proposed method. We are also working on applying our proposed technique on GUI applications with different kinds of characteristics to further demonstrate its effectiveness and commonality, such as programs with complex underlying business logic and a fairly simple GUI even other event-driven software \(EDS\, including Web applications, device drivers and embedded software  R EFERENCES  1  J. A. Jones and M. J. Harrold, çEmpirical evaluation of the Tarantula automatic fault-localization technique,é in Proc. 20th IEEE/ACM Int Conf. Automated Softw. Eng., Long Beach, CA, pp. 273Ö282, Nov 2005 2  W. E. Wong, V. Debroy, and B. Choi, çA family of code coveragebased heuristics for effective fault localization,é J. Syst. Softw., vol 83, no. 2, pp. 188Ö208, Feb. 2010 3  C. Liu, L. Fei, X. Yan, J. Han, and S. P. Midkiff, çStatistical debugging:a hypothesis testing-based approach,é IEEE Trans. Softw Eng., vol. 32 000\003 no. 10, pp. 831Ö848, Oct. 2006 4  S. Nessa, M. Abedin, W. E. Wong, L. Khan, Y. Qi, çSoftware fault location using N-gram analysis,é in Lecture Notes in Computer Science including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics, \(2008\, vol. 5258 LNCS, pp 548-559 5  Atif M. Memon, A Comprehensive Framework for Testing Graphical User Interface, Ph.D. Dissertation, University of Pittsburgh, USA 2001 6  Zhao L and Cai K. Y., çE vent Handler-Based Coverage for GUI  Testing,é in Proceedings of the tenth International Conference on Quality Software \(QSIC 2010\, pp.326-331, 14-15 July 2010 7  Atif M. Memon,   http://www.cs.umd.edu/~atif/TerpOfficeWeb TerpOfficeV3.0/index.html 8  A. M. Memon and Q. Xie, çStudying the fault-detection effectiveness of GUI test cases for rapidly evolving software,é IEEE Trans. Softw Eng., vol. 31, no. 10, pp. 884Ö896, 2005 9  Han, J., Kamber, M.: Data Mining: Concepts and Techniques Morgan Kaufmann Publishers, San Francisco \(2001 10  Atif M Memon http://www.cs.umd.edu/~atif/Benchmarks UMD2005b.html 11  Cai K. Y., Zhao L., Hu H., and Jiang C.H.,çOn the Test Case Definition for GUI Testing,é in Proceedings of the Fifth International Conference on Quality Software \(QSIC 2005\, pp.19-28 Melbourne,Australia, September 2005 12  Zhu L.Z., Yin B.B., Cai K. Y., çSoftware Fault Localization Based On Centrality Measures,é compsacw, pp.37-42, 2011 IEEE 35th Annual Computer Software and Applications Conference Workshops, 2011    
332 


3 Check the user is registered or not, if YES then 4       whether the user is first time visitor, if YES then 5  return W\(Pi 6    else 7      calculate FW\(Pi 4 8 SET   W\(Pi Pi 9 return W\(Pi 2010 5th International Conference on Industrial and Information Systems, ICIIS 2010, Jul 29 - Aug 01, 2010, India 197 spent on a page is inversely proportional to the degree of information useful to user. The formula to calculate the weight based on time spent on each page is given in \(6  TW?u? = T??? ????? ?? ? ???????/P??? S??? ????????U?T??? ????? ?? ? ???????/P S??? ????                  \(6  The Frequency and Time spent based page Weight FTPW spent on a particular page, as biasing factors. Frequency and Time spent based page Weight can be calculated by the formula given in \(7  W\(u u u 7  Where FW\(u which can be computed from \(4 u page based on time spent on a page which can be computed from equation \(6 u based on frequency and time spent both The Pseudo code of FPW algorithm given as follows            


   V. PERFORMANCE EVALUATIONS The performance of the proposed approach in terms of various parameters is discussed in this section A.  Data Set We have evaluated the performance of proposed algorithms by using the synthetic data set for different users to calculate the weight of each web page. Consider the structure of a web site consisting of four web pages namely A, B, C and D and their respective interconnection shown by direct edges as shown below in Figure 1       Figure 1: Web Site Structure The following factors are considered for performance evaluation Outbound links which shows the out degree of nodes for the graph of Figure1 Page Rank \(PR which can be calculated on the basis of the equation \(3 Number of visit shows the visiting frequency of each web page by user, it can be decided on the basis data received from web log Page Size defines the size of page on the basis of information content of the web page, which can be measured in bytes Time spent describes the resumption of a particular web page by user in seconds  The above parameters used in proposed experimental set up for the Web Path Traversal, by considering the graph mentioned in Figure1 B. Evaluation Method The attributes considering for the training data set for each user of a web site is represented in terms of time spent on each page and frequency. From these data sets we are 


calculating the weight for each web page for respective web site. The proposed approach uses Visiting Frequency and Time Spent on a Web page as two parameters to measure the weight of each web page To estimate the performance of the proposed two algorithms i.e. FPW and FTPW, discussed in section IV based on the above parameters involved in estimation C. Experimental Results The performance of the proposed approach can be evaluated by comparing the performance of FPW and FTPW algorithms which differ in number of parameters considered for experimentation. The comparison is made by taking the attribute like Visiting Frequency in FPW, and further Visiting Frequency and Time spent on a web page are clubbed together in FTPW as another attribute. The experimental setup uses five users and weights are plotted against various parameters Figure 2 shows the plot of Visiting Frequency v/s Weight of a web page              Figure 2: Plot between frequency and weight Algorithm: FTPW Input: Web traversal path database Output: Weight for each page 1 Calculate PageRank for each page \(PRi 3 2 Initially Set W\(Pi 3 Check the user is registered or not, if YES then 4       whether the user is first time visitor, if YES then 5  return W\(Pi 6    else 7      calculate FW\(Pi 4 


8            calculate TW\(Pi 6 9   SET   W\(Pi Pi Pi 10   return W\(Pi A C B    D 0 0.05 0.1 0.15 0.2 0 5 10 15 20 w ei gh t Visiting Frequency User1 User2 User3 user4 User5 2010 5th International Conference on Industrial and Information Systems, ICIIS 2010, Jul 29 - Aug 01, 2010, India 198  Using the weights calculated in Figure the higher weight for more frequently visited information in FPW algorithm the different the scenario described in Figure 1 for all th plotted in Figure 3            


   Figure 3: Recommendation for Web Path Trave Algorithm  The weight assigned to various pages Visiting Frequency and Time spent on web Figure 4                  Figure 4: Plot between \(frequency + time sp  The Figure 5 gives the details of recomm Traversal for different users by FTPW a weights calculated by considering two para and time spent in Figure 4 which indicates for more frequently visited pages and in term on web pages  0 0.02 0.04 0.06 0.08 0.1 0.12 


0.14 0.16 0.18 W ei gh t Web Pages Us Us Us Us Us 0 0.2 0.4 0.6 0.8 1 1.2 1.4 W ei gh t FTPW \(Frequecy  and 2 which indicate pages. Using this traversal paths in e users have been rsal based on FPW by combining the page is plotted in ent ended Web Path lgorithm and use meters frequency the higher weight s more time spent    


         Figure 5: Recommendation W Algo Figure 6 shows the relative on the synthetic data sets, in w more efficient than FPW algor performance of proposed a increase the complexity of alg and provide better Web Path Tr    Figure 6: Relative Access  The proposed FTPW algorithm parameters which otherwise ar FPW algorithm. A matrix dep comparison between above two parameters are performance ce show that the performance o increase the number of param FPW algorithm to FTPW algori The experimental results d better and provides a methodo optimized Web path traversal past navigation behavior by c page 0 1 2 3 4 5 1 2A 


cc es si bi lit y Ti m e fo r M or e re qu ir ed In fo rm at io n er1\(A->C->D->B er2\(D->B->A->C er3\(D->B->C->A er4\(C->B->A->D er5\(B->A->D->C Time User1 User2 User3 User4 User5 0 0.2 0.4 0.6 0.8 1 1.2 


W ei gh t Web Pages eb Path Traversal based on FTPW rithm  execution for FPW and FTPW hich we can see that FTPW is ithm. Hence, it is clear that the lgorithm increases when we orithm in terms of parameters aversal in less time  ibility time for FPW and FTPW consists of clubbing of various e not available in first proposed icted in the Figure 7 describes proposed algorithms. Here the ntric and a comparison results f the system improves as we eters i.e. when we move from thm rawn for FTPW algorithms are logy for effective, efficient and for various users based on their omputing weight for each web 3 4 5 User FTPW FPW User1\(C->B->A->D User2\(A->B->C->D User3\(B->C->A->D User4\(D->A->C->B User5\(B->D->A->C 2010 5th International Conference on Industrial and Information Systems, ICIIS 2010, Jul 29 - Aug 01, 2010, India 199 Figure 6: Comparison Matrix VI.  CONCLUSION & FUTURE WORK 


 Web Usage Mining have been used in improving Web site design and marketing decision support, user profiling, and Web server system performance. Web page prediction technique is a very important role in web technologies. This paper proposes efficient algorithms for web path recommendation based on Weighted Association Rule. Two factors frequency and time spent were used to decide the web path traversal. The experimental results show that in the proposed approach when we increase the number of parameters for finding the web path the accuracy of the system is enhanced drastically and FTPW produces more accurate results than those achieved by FPW In the future, we shall improve the Web Path Traversal by considering the parameter Data Transfer Rate to provide the accurate Web Path traversal REFERENCES 1] M. S. Chen, X. M. Huang and I. Y. Lin, Capturing User Access Patterns in the Web for Data Mining, Proceedings of the IEEE International Conference on Tools with Artificial Intelligence, pp. 345348, 1999 2]  R. Cooley, B. Mobasher, and J. Srivastava, Web Mining: Information and Pattern Discovery on the World Wide Web, Proceedings of the 9th IEEE International Conference on Tools with Artificial Intelligence, pp 558-567, 1997 3]  B. Mobasher,N. Jain,E. Han et al, Web mining: pattern discovery from World Wide Web transactions, Tech Rep: TR96-050, 1996 4]  C. Shahabi, A. Zarkesh, J. Abidi, V. Shah, Knowledge discovery from user's Web-page navigation,  in Proceedings of the 7th IEEE International Workshop on Research Issues in Data Engineering, 1997 5]  Yue-Shi Lee, Show-Jane Yen, Ghi-Hua Tu and Min-Chi Hsieh, Web Usage Mining: Integrating Path Traversal Patterns and Association Rules, Proceedings of International Conference on Informatics Cybernetics, and Systems \(ICICS'2003 6]  Yue-Shi Lee, Show-Jane Yen, Ghi-Hua Tu and Min-Chi Hsieh, Mining Traveling and Purchasing Behaviors of Customers in Electronic Commerce Environment, Proceedings of IEEE International Conference on e-Technology, e-Commerce and e-Service \(EEE'2004 pp. 227-230, 2004 7]  J. Srivastava, et al. Web Usage Mining: Discovery and Applications of Usage Patterns from Web Data. SIGKDD Explorations, pp. 12-23 2000 


8]  Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertextual Web search engine. Proceedings of the seventh international conference on World Wide Web 7: pp. 107-117, 1998 9]  J. Pei, J. Han, B. Mortazavi-Asl and H.Zhu, Mining Access Patterns Efficiently from Web Logs, Proceedings of the Pacific-Asia Conference on Knowledge Discovery and Data Mining, pp. 396-407 2000 10]  C. H. Cai, A. W. C. Fu, C.H. Cheng and W. W. Kwong, Mining Association Rules with Weighted Items, In Database Engineering and Applications Symposium, Proceedings IDEAS'98, pp. 68  77, 1998 11]  F. Tao, F. Murtagh and M. Farid, Weighted Association Rule Mining using Weighted Support and Significance Framework, In Proceedings of the 9th SIGKDD conference, 2003 12]  Show-Jane Yen, An Efficient Approach for Analyzing User Behaviors in a Web-Based Training Environment, International Journal of Distance Education Technologies, Vol. 1, No. 4, pp.55-71, 2003 13]  Show-Jane Yen, Yue-Shi Lee and Chung-Wen Cho, Efficient Approach for the Maintenance of Path Traversal Patterns, In Proceedings of IEEE International Conference on e-Technology, eCommerce and e-Service \(EEE 14]  M. Spiliopoulou and L. C. Faulstich, Wum: A web utilization miner EDBT Workshop WebDB98, Springer Verlag, 1996 15]  M. S. Chen, J. S. Park and P. S. Yu, Efficient data mining for path traversal patterns,  IEEE Transactions on Knowledge and Data Engineering, pp. 209-221, 1998 16]  H. Yao,H. J. Hamilton, and C. J. Butz, A Foundational Approach to Mining Itemset Utilities from Databases, Proceedings of the 4th SIAM International Conference on Data Mining, Florida, USA, 2004 17]  Z. Chen, R. H. Fowler and A. Wai-Chee Fu, Linear Time Algorithm for Finding Maximal Forward References, Proceedings of International Conference on Information Technology. Computers and Communications  \(ITCC'2003 18]  T. Jing, Wan-Li Zou and Bang-Zuo Zhang, An Efficient Web Traversal Pattern Mining algorithm Based On Suffix Array, Proceedings of the 3rd International Conference on Machine Learning and Cybernetics , pp 1535-1539, 2004 19]  Show-Jane Yen, Yue-Shi Lee and Min-Chi Hsieh, An efficient incremental algorithm for mining Web traversal patterns, Proceedings of the 2005 IEEE International Conference on e-Business Engineering ICEBE05 20]  L. Zhou, Y. Liu, J. Wang and Y. Shi, Utility-based Web Path  Traversal Pattern Mining, Seventh  IEEE International Conference on Data 


Mining Workshops, pp. 373-378, 2007 21]  C. F. Ahmed, S. K. Tanbeer, Byeong-Soo Jeong and Young-Koo Lee Efficient mining of utility-based web path traversal patterns, 11th International Conference on Advanced Communication Technology ICACT09 22]   http://en.wikipedia.org/wiki/PageRank 23] en.wikipedia.org/wiki/Association_rule_mining  Attributes? FPW Algorithm FTPW Algorithm Recognition of User behavior Visiting Frequency Page Rank Time Spent on Web page Page Size Accessibility of required information in less time Improving Web navigation and system design of Web applications  Enhancing server performance 2010 5th International Conference on Industrial and Information Systems, ICIIS 2010, Jul 29 - Aug 01, 2010, India 200 


Enhanced Inte r transaction Association Rules Enhanced Inter transaction Association Rules Enhanced Inter transaction Association Rules Extension of association rules Conditional relationships at multiple different time steps e.g itemset a\(0 0 1 2 You sense Higher data rate than normal AND You see New Route g enerated AND 1 minute a g o you detected checksum gg error packets AND 2 minutes ago your encountered wrong checksum   Implies High Chance of Intrusion Enhanced Rules and Confidence Associative IDS for NextGen Frameworks Dr S Dua LA Tech 21 


Complex Spatio temporal Association Complex Spatio temporal Association Rules Further extension of inter transaction association rules Describe event durations e.g itemset a\(0,X j,Y k,Z Eg  You sense high data rates for X seconds AND new route generated j minutes ago task completed in Y AND new route generated j minutes ago task completed in Y seconds AND checksum error packets received k minutes ago for Z seconds High Chance of Intrusion With highest confidence level in association rules  association rules  Associative IDS for NextGen Frameworks Dr S Dua LA Tech 22 


DMITAR Al ith ARD DMITAR Al gor ith m  ARD Problem Domain Problem Statement and Challenges Aiti Miig bd IDS A ssoc i a ti ve Mi n i n g b ase d IDS  Introduction to data mining Association rule in data mining DMITAR Algorithm  ARD New research Associative IDS for NextGen Frameworks Dr S Dua LA Tech 23 


DMITAR Algorithm DMITAR Difference Matrix Based Inter Transaction Association Rule Miner developed in DMRL Uses vertical data format Differences of the transaction IDs are used to generate extended itemsets Windowless mechanism Associative IDS for NextGen Frameworks Dr S Dua LA Tech 24 


Deep into the Mechanism The DMITAR algorithm is based on lhilii comp l ex mat h emat i ca l assoc i at i ve formulation and proofs Four major parts Four major parts Frequent 1 itemset generation Frequent 2 itemset generation Frequent k itemset generation k>2 Spatio temporal rule formation Associative IDS for NextGen Frameworks Dr S Dua LA Tech 25 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


