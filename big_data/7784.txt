Parallel Mining of Maximal Frequent Itemsets from Databases Soon M Chung  and Congnan Luo Dept of Computer Science and Engineering Wright State University Dayton Ohio 45435 USA Abstract In this paper we propose a parallel algorithm for mining maximal frequent itemsets from databases A frequent itemset is maximal if none of its supersets is frequent The new parallel algorithm is named Parallel Max-Miner PMM and it is a parallel version of the sequential Max-Miner algorithm 3 Most of existing mining algorithms discover the frequent k itemsets on the k th pass over the databases and then generate the candidate  
k  1 for the next pass Compared to those level-wise algorithms PMM looks ahead at each pass and prunes more candidate itemsets by checking the frequences of their supersets We implemented PMM on a cluster of workstations and evaluated its performance for various cases PMM demonstrated better performance than other sequential and parallel algorithms and its performance is quite scalable even when there are large maximal frequent itemsets i.e long patterns in databases Key words Parallel data mining maximal frequent itemsets association rules scalability 1 Introduction Let I   i 1 i 2 i 
m  be a set of items Let D be a set of transactions where each transaction T contains a set of items An association rule is an implication of the form X  Y where X I  Y I and X  Y    The association rule X  Y holds in the database D with conìdence c if c  of transactions in D that contain X also contain Y  The association rule X  Y has support s if s  of transactions in D contain 
X  Y  Mining association rules is to nd all association rules that have support and conìdence greater than or equal to the user-speciìed minimum support called minsup  and minimum conìdence called minconf  respectively  T he rst step in the d isco v e ry of association rules is to nd each set of items called itemset hathaveco-occurrencerateabovethemini This research was supported in part by Ohio Board of Regents LexisNexis NCR and Wright Brothers Institute WBI mum support An itemset with at least the minimum support is called a 
frequent itemset  The size of an itemset represents the number of items contained in the itemset and an itemset containing k items will be called a k itemset The second step of forming the association rules from the frequent itemsets is straightforward as described in 1 F o r e v e r y fr equen t itemset f  nd all non-empty subsets of f  For every such subset a  generate a rule of the form a   f  a fthe ratio of support f  to support a satleast minconf  In mining association rules the most timeconsuming job is nding all frequent itemsets from a 
large database with respect to a given minimum support Many sequential and parallel algorithms have been proposed to solve this problem The most common sequential algorithms are Apriori 1 an d i t s v ari ations Apriori-like algorithms employ a strict bottomup breadth-ìrst search and enumerate every frequent itemset 3  T hey r equir e m ultiple pa sses o v e r t he database In the rst pass the occurrences of individual items are counted and frequent 1-itemsets are determined Then the frequent 1-items are used to generate the potentially frequent 2-itemsets called candidate 2-itemsets In the second pass we count the occurrences of the candidate 2-itemsets so that we can determine the frequent 2-itemsets Frequent 2 
itemsets are used to generate the candidate 3-itemsets andsoon Thisprocessisrepeateduntilthereisno new candidate itemset generated In Apriori if F k  1 denotes the set of frequent  k  1 discovered in the  k  1 pass over the database then the set of candidate k itemsets for the next pass denoted by C k  is obtained by the natural join of F k  1 and F k  1 on the rst k  2 items For example if F 2 includes  1 2 
 and  1 3  then  1 2 3  is a candidate 3-itemset However a candidate k itemset is pruned if any of its  k  1\-subsets is not frequent Thus for  1 2 3  to remain as a candidate 3-itemset  2 3  also should be a frequent 2-itemset This subset-infrequency based pruning step prevents many candidate k itemsets from being counted in each Proceedings of the 15th IEEE International Conference on Tools with Artificial Intelligence \(ICTAIê03 1082-3409/03 $17.00 © 2003 IEEE 


pass k  Due to the large size of the database to be mined parallel data mining is a very promising direction and a few parallel algorithms were proposed for mining association rules The most well-known one is Count Distribution CD algorithm 2 whic h i s a pa r a llel version of the Apriori algorithm The database is partitioned and distributed over multiple processing nodes initially At each pass k  every node collects local counts of the same set of candidate k itemsets Then these counts are merged between processing nodes so that each node can identify the frequent k itemsets and generate the candidate  k  1 for the next pass as in Apriori To merge the local support counts of the candidate itemsets synchronization between nodes is required at every pass and maintaining the same set of candidate itemsets in all the nodes is redundant In Apriori-like algorithms if there is a frequent itemset with length l  then they will generate and count its 2 l subsets This exponential complexity makes Apriori-like algorithms just suit for mining small frequent itemsets i.e short patterns 3 T o address this problem algorithms that can eciently mine the maximal frequent itemsets MFI were proposed The basic idea is that if we nd a large frequent itemset i.e long pattern early we can avoid counting all its subsets because they are all frequent So these algorithms always look ahead and try to nd large frequent itemsets early Once we nd all the maximal frequent itemsets all frequent itemsets can be obtained from them In this paper we propose a new parallel algorithm named Parallel Max-Miner PMM for mining maximal frequent itemsets PMM is a parallel version of the sequential Max-Miner algorithm 3 Ho w e v e r  it requires multiple passes over the database like the Count Distribution algorithm In each pass all the nodes have the same set of candidate groups to be counted and after each pass the count information is exchanged between nodes so that every node holds the same global count information and then generates the same candidate groups for the next pass PMM uses this bottom-up search but it looks ahead at each pass and prunes more candidate itemsets by checking the frequences of their supersets Moreover PMM does not require the transfer of transactions between nodes 2 Max-Miner Algorithm Unlike Apriori-like algorithms the Max-Miner algorithm 3 e xtr a cts o nly t he ma xima l fr equen t itemsets from which all the frequent itemsets can be obtained Max-Miner always attempts to look ahead in order to identify large frequent itemsets early so that all subsets of these discovered frequent itemsets can be pruned from the search space This method is called superset-frequency based pruning  Max-miner used the set-enumeration tree to represent the search space How to construct the set enumeration tree is illustrated in Figure 1 for four items 1 2 3 and 4 The set-enumeration tree lists all combinations of the four items from level 0 to level 4  Level   0 Level   1 Level   2 Level   3 Level   4   012  015   1 2   012  015  4 3 3,4 1,2 012\015  1,4 1,3 2,3 2,4 2,3,4 1,3,4 1,2,3 1,2,4 1,2,3,4 015\012  012    012  015  Figure 1 Search space of Max-Miner Each node in the tree is called a candidate group and a candidate group g consists of two components which are actually two itemsets The rst itemset is called the head of the group and denoted by h  g  The second itemset is called the tail of the group and denoted by t  g  t  g  is an ordered set and contains all the items not in h  g  but can potentially appear in any subnode derived from node g  For example the node i.e candidate group g 1 enumerating item 1 which is the leftmost node at level 1 has two components h  g 1   1  and t  g 1   2  3  4   The main procedure of Max-Miner can be explained as follows From the root of the tree at level 0 we count the support of 1-itemsets Only the 1-itemsets which are frequent can be enumerated at level 1 In this example 4 nodes are generated at level 1 if 1 2 3 and 4 are all frequent 1-itemsets For the node g 1  which corresponds to item 1 at level 1 we need to count the support of  h  g 1   t  g 1     1  2  3  4  If the support of  h  g 1   t  g 1   is equal to or greater than minsup  then we do not need to expand the tree from the node g 1 anymore Similarly for the node g 2  which corresponds to item 2 at level 1 the support of  h  g 2   t  g 2     2  3  4  is counted If  h  g 2   t  g 2   is frequent then the subnodes of node g 2 are not generated at level 2 At any node g if  h  g   t  g   is not frequent for each item i in t  g  we check if  h  g   i  is frequent Proceedings of the 15th IEEE International Conference on Tools with Artificial Intelligence \(ICTAIê03 1082-3409/03 $17.00 © 2003 IEEE 


If  h  g   i  is frequent a corresponding subnode is generated The head of the subnode is  h  g   i  and its tail contains all the item j  such that  h  g   j  is frequent and j comes after i in t  g  For example if  1  2    1  3  and  1  4  are frequent when  1  2  3  4  is not frequent then we generate three subnodes of node g 1  They are enumerated at level 2 as  h  g    1  2   t  g    3  4    h  g    1  3   t  g    4   a n d  h  g    1  4   t  g     This pro cess c on tin u es level by level until the whole tree is completed Here we can notice that for a candidate group node g ifanitemappearslastinthetailof g in ordering it will appear in most osprings of the node g 3 For example item 4 is the last item in the tail of the root node in Figure 1 and appears in either the head or tail of every node at level 1 and below On the other hand item 2 appears in a much smaller number of nodes Thus to discover the long patterns maximal frequent itemsets early we better order the subnodes of each node in ascending order of their support At level 1 of the set-enumeration tree Max-Miner orders the frequent 1-itemsets in ascending order of their support And at lower levels it orders the tail items of each node g in ascending order of support  h  g   i   for i  t  g  In the above example if support  1  4    support  1  2    support  1  3  whenwegenerate the subnodes of node g 1  then the items are reordered in its tail t  g 1 s  4  2  3   and the corresponding subnodes are generated in that order as  h  g    1  4  t  g    2  3    h  g    1  2  t  g    3   a n d  h  g    1  3  t  g     This heur istic reordering strategy increases the eectiveness of the superset-frequency based pruning In t h ey also prop osed ho w t o e st imat e a lo w e rbound of the support of each itemset using the support of some of its subsets By discovering the maximal frequent itemsets early Max-Miner can reduce the number of passes over the database As a result it can reduce the total execution time considerably compared to other level-wise algorithms 3 Parallel Max-Miner PMM Algorithm We present the PMM algorithm in this section It is implemented on a shared-nothing multiprocessor system where each node has private memory and a private disk All nodes communicate with each other by passing messages through a communication network The database is evenly divided into N partitions  D 0 D 1 D 2 D N  1   one for each of the N nodes  P 0 P 1 P 2 P N  1  involved in the parallel mining i.e each node has the same number of transactions allocated PMM requires multiple passes over the database For each pass k  all the nodes have exactly the same set of candidate groups which is denoted by C k and each node counts the occurrencies of the candidate groups in the local database independently At the end of each pass all nodes exchange the count information so that they can generate the same set of candidate groups C k 1 for the next pass After the rst pass each node determines which items are frequent and in the second pass each node counts the occurrencies of all the pairs of frequent 1-itemsets using a twodimensional data array to maintain the counts of the candidate 2-itemsets appearing in the local database Here we do not count the occurrences of the large itemset  h  g   t  g   of each candidate group g because those large itemsets turn out to be infrequent most of the time at this stage as reported in  After exchanging and merging of the local counts of the candidate 2-itemsets every node can identify the same set of frequent 2-itemsets Then each node generates the same set of candidate groups for the following third pass The procedure of generating the candidate groups for the k th pass k  3 is exactly same as that of sequential Max-Miner algorithm For example if  1  2    1  3  and  1  4  are frequent 2itemsets then we generate candidate groups whose heads are  1  2  3    1  2  4  and  1  3  4   respectively In general each candidate group generated for the k th pass has k items in its head This process of counting candidate groups exchanging and merging count information and generating candidate groups for the next pass is repeated until there is no more candidate group for the next pass PMM algorithm is simple and ecient and it has lower communication overhead because it does not require the transferring of transactions between nodes during the processing Like Count Distribution PMM requires the synchronization between nodes to exchange the count information after each pass However the number of passes required in PMM is smaller than that of Count Distribution because the supersetfrequency based pruning is performed in each pass by looking ahead to nd maximal frequent itemsets 3.1 Cube-based Communication between Processors To perform the communication between nodes eciently we impose a logical binary n cube structure on the processing nodes Then the nodes can exchange and merge the local count information through increasing dimensional links between them 4 In the n cube there are 2 n nodes and each node has n bit binary address Also each node has n neighbor nodes Proceedings of the 15th IEEE International Conference on Tools with Artificial Intelligence \(ICTAIê03 1082-3409/03 $17.00 © 2003 IEEE 


which are directly linked to that node through dierent dimensional links For example there are 8 nodes in a 3-cube structure and node 000 2 is directly connected to 001 2  010 2 and 100 2 through a 1stdimensional link a 2nd-dimensional link and a 3rddimensional link respectively Thus in the n cube all the nodes can exchange and merge their local counts in n steps through each of the n dierent dimensional links 3.2 Pseudo-code of PMM As we assume a homogeneous distributed computing environment where all the nodes are the same we just give the pseudo-code of the PMM algorithm running on a node P i   Pass 1  P i counts the occurrencies of items in D i  n log 2 N  N nodes are used for mining  for  j 1 j  n  j  P i exchange and merge the local counts of items with a neighbor node through a j dimensional link P i determines F 1  F 1 is the set of frequent 1-itemsets   Pass 2  P i generates C 2 by pairing the members of F 1  P i counts the occurrences of the C 2 members in D i  for  j 1 j  n  j  P i exchange and merge the local counts of C 2 members with a neighbor node through a j dimensional link P i determines F 2  F 2 is the set of frequent 2-itemsets  P i generates C 3  all nodes have same C 3   C 3 includes all candidate groups generated based on F 2 for pass 3   Pass k for k  3  C k includes all candidate groups generated based on F k  1 for pass k  while  C k      P i scans D i to count the candidate groups in C k   counting all  head  i  for i  tail and  head  tail  for each candidate group in C k  for  j 1 j  n  j  P i exchange and merge the local counts of C k members with a neighbor node through a j dimensional link P i identiìes frequent itemsets P i inserts frequent itemsets into GM  and keeps only maximal frequent itemsets in GM  P i generates C k 1 using F k  superset-frequency based pruning is applied  k    GM is the set of all maximal frequent itemsets  4 Performance Evaluation Our test platform is an 8-node Linux cluster system where nodes are connected by a Fast Ethernet switch Each node has a 800 Mhz Pentium processor 512 MB memory and a 40 GB disk drive The processes are communicating using the MPI Message Passing Interface The databases used in our experiments are synthetic sales transaction databases generated as in 1 All parameters used for generating databases are described in Table 1 For all databases c  0.5 m  0.5 v  0.1  L   2000 and NI  1000 Table 2 lists all databases used in our performance evaluation experiments The size of each database is about 360 MB When running the parallel algorithm on a database we need to partition it into local databases To balance the size of the local databases each transaction is randomly allocated to a node In order to compare the performance of PMM and Count Distribution we also implemented Count Distribution on the same platform Table 1 Synthetic database parameters  D  Number of transactions in the database  T  Average size of the transactions  I  Average size of the maximal potentially frequent itemsets  L  Number of maximal potentially frequent itemsets NI Number of items c Correlation level m Mean of the corruption level v Variance of the corruption level Table 2 Databases Name  T   I   D  T10 I02 D7852K 10 2 7852K T20 I04 D4288K 20 4 4288K T25 I06 D3504K 25 6 3504K T30 I08 D2954K 30 8 2954K T40 I10 D2256K 40 10 2256K 4.1 Improvement of PMM over Count Distribution We ran both PMM and Count Distribution on different synthetic databases with dierent minsup values Table 3 shows the speedup of PMM over Count Distribution CD If we deìne T CD and T PMM as the execution times of CD and PMM respectively then the speedup of PMM over CD is T CD T PMM In Table 3 the speedup of PMM is shown for dierent databases listed in the rst column and for dierent values of minsup listed in the rst row In these experiments all 8 nodes in our cluster system were used Proceedings of the 15th IEEE International Conference on Tools with Artificial Intelligence \(ICTAIê03 1082-3409/03 $17.00 © 2003 IEEE 


Table 3 Speedup of PMM over CD 8-node case Database 1 0  75 0  5 0  25 0  15 0  1 T10 I02 D7852K 1.01 1.02 1.01 1.07 1.08 1.08 T20 I04 D4288K 1.04 1.02 1.04 1.30 1.64 1.66 T25 I06 D3504K 1.06 1.07 1.45 2.46 2.98 4.19 T30 I08 D2954K 1.05 1.97 2.79 5.82 16.80 21.34 T40 I10 D2256K 1.40 1.70 2.83 9.38 20.10 29.76 When minsup is high PMM is comparable to or a little bit slower than Count Distribution We also ran Apriori and Max-Miner for these cases and found that Max-Miner doesnêt show much improvement over Apriori either That is because the high minsup limits the number of frequent itemsets and the length of those frequent itemsets Thus the eect of look-ahead technique used by Max-Miner is not clearly shown and naturally PMM has the same result As minsup decreases PMM begins to show more and more improvement in our tests As shown in Table 3 when the value of parameter  I  of the database is large such as 8 or 10 even if the minsup is as high as 0.5 PMM is faster than Count Distribution with a speedup above 2.5 It is because a large  I  value results in larger frequent itemsets i.e long patterns which beneìts PMM If minsup is less than 0.25 PMM outperforms Count Distribution considerably 4.2 Synchronization Requirement of PMM and Count Distribution We compared the number of synchronizations needed between processing nodes in PMM and Count Distribution Table 4 shows the comparison results Here we deìne S PMM and S CD as the number of synchronizations needed in PMM and CD respectively The rst row of the table lists various values of minsup and the rst column lists the names of databases The values in each entry of the table represents S PMM  S CD  Table 4 Comparison of synchronization requirement Database 1 0  75 0  5 0  25 0  15 0  1 T10 I02 D7852K 2:3 3:4 5:5 5:9 6:9 5:9 T20 I04 D4288K 4:6 6:6 8:9 10:10 10:11 9:11 T25 I06 D3504K 7:8 7:8 11:11 11:15 10:15 12:16 T30 I08 D2954K 6:6 12:12 13:15 13:16 17:18 17:18 T40 I10 D2256K 11:12 9:13 14:15 16:17 17:18 18:19 As we can see in most cases PMM requires less number of synchronizations than Count Distribution but the dierence is small Our experiments showed that MaxMiner does not reduce the number of passes over the database much compared to Apriori As a parallel version of MaxMiner PMM does not reduce the number of passes much either 4.3 Communication Requirement of PMM and Count Distribution Like Count Distribution all nodes running PMM have the same set of candidates in each pass So each node sends and receives the same amount of count information for the candidates The dierence between two algorithms is the meaning of candidates which determines how much count information must be exchanged between processing nodes during the mining In Count Distribution its candidates are the potential frequent itemsets ge nerated as in Apriori On the other hand in PMM candidates are the candidate groups deìned exactly as in Max-Miner where each candidate group contains a head itemset and a tail itemset For each candidate group we can say there are two kinds of candidates potential frequent itemsets each of which contains the head and one of the tail items and a potential maximal frequent itemset which is the union of the head and the tail Thus to exchange the count information for each candidate group processing nodes need to exchange multiple integers for the two kinds of candidates It may sound that we need more communication between processing nodes in PMM However in reality PMMês look-ahead technique can reduce the communication requirement considerably because if one potential maximal frequent itemset is found frequent early then many candidates can be removed due to the superset-frequency based pruning So compared with Count Distribution which does not use the superset-frequency based pruning the total amount of count information exchanged in PMM is smaller in most cases We implemented two versions of Count Distribution one is using the n cube communication and the other is using the all-to-all communication We compared the average amount of data each node communicateswithotherswhenweexecutedPMMandCount Distribution on the T30 I08 D2954K database with various values of minsup  and the results are shown in Figure 2 Compared with Count Distribution using the allto-all communication scheme PMM shows a big improvement in communication for all cases shown in Figure 2 When both algorithms use the same n cube communication scheme the communication overhead of PMM is still a little lower than that of Count Distribution 4.4 Sensitivity Analysis of PMM In this section we evaluate the characteristics of the PMM algorithm in terms of speedup and sizeup All tests were performed with a minsup of 0.25 Proceedings of the 15th IEEE International Conference on Tools with Artificial Intelligence \(ICTAIê03 1082-3409/03 $17.00 © 2003 IEEE 


                             0 50 100 150 200 250 300 350 400 450 500 1 0.75 0.5 0.4 0.3 0.25 0.2 0.15 0.1 Minimum Support Data Communication \(MB   PMM  Count Distribution\(n-Cube  Count Distribution\(all-to-all Figure 2 Comparison of communication requirement 4.4.1 Speedup We evaluated the speedup of PMM as the number of nodes increases while the database size remains the same For the databases listed in Table 2 we kept the same database size of 360 MB but the database was partitioned into 2 4 and 8 parts when the number of nodes were 2 4 and 8 respectively Figure 3 shows the execution time of PMM on the 2-node 4-node and 8-node systems To demonstrate the speedup we also ran the sequential Max-Miner for each database on a single node As the number of nodes is doubled the execution time of PMM decreases by about 40 to 50 In PMM all the nodes generate and count the same set of candidates in each pass no matter how many nodes are used Thus PMM shows an almost linear speedup                            0 2000 4000 6000 8000 10000 12000 14000 16000 0246810 Number of Nodes Execution Time \(sec    T10_I02_D7852K   T20_I04_D4288K   T25_I06_D3504K   T30_I08_D2954K   T40_I10_D2256K Figure 3 Speedup of PMM 4.4.2 Sizeup For the sizeup test we xed the system to the 8-node conìguration and distributed each database listed in Table 2 to the 8 nodes Then we increased the local database size at each node from 45 MB to 215 MB by duplicating the initial database partition allocated to the node Thus the data distribution characteristics remain the same as the local database size is increased The results shown in Figure 4 indicates that PMM has a very good sizeup property Since increasing the size of local database doesnêt aect the local mining result of PMM at each node the total execution time increased just due to more disk I/O and computation cost which scaled almost linearly with sizeup                                 0 2000 4000 6000 8000 10000 12000 0 45 90 135 180 225 270 Amount of Data per Node \(MB Execution Time \(sec    T10_I02_D7852K   T20_I04_D4288K   T25_I06_D3504K   T30_I08_D2954K   T40_I10_D2256K Figure 4 Sizeup of PMM 5 Conclusions In this paper we proposed a parallel maximal frequent itemset mining algorithm named Parallel MaxMiner PMM for shared-nothing multiprocessor systems PMM is based on the sequential Max-Miner algorithm and avoids enumerating all potential frequent itemsets So PMM has much lower computation cost than Count Distribution which is a parallel version the Apriori algorithm A cube-based communication scheme is employed by PMM for ecient communication between processing nodes PMM also has very good speedup and sizeup properties as evidenced by the corresponding test results References  R  A gra w al an d R  S rik a n t   F a st A l gorit h m s f or Mining Association Rules Proc of the 20th VLDB Conf  1994 pp 487Ö499  R  A gra w al an d J  C  S h a fer P arallel M in in g o f A ssociation Rules IEEE Trans on Knowledge and Data Engineering  Vol 8 No 6 1996 pp 962Ö969  R  J  B a y ard o   Ecien t M in in g L on g P at t e rn s f rom Databases Proc.oftheACMSIGMODIntêlConf on Management of Data  1998 pp 85Ö93  S  M  C h u n g an d J  Y an g A P a rallel D ist r ib u tive Join Algorithm for Cube-Connected Multiprocessors IEEE Trans on Parallel and Distributed Systems  Vol 7 No 2 1996 pp 127Ö137 Proceedings of the 15th IEEE International Conference on Tools with Artificial Intelligence \(ICTAIê03 1082-3409/03 $17.00 © 2003 IEEE 


Manager manages the agreed proposals and provides necessary information for the process of derived data generation The Template Generators generate templates based on the proposals A template is similar to a multi-resolution cache object 10 and a n X SL stylesheet 13   I t c ontains information for the Data Generators to generate different 221views\222 based on different p ermissions to the levels of behavior logs For example a simple ACL Template Generator can be implemented by attaching ACL s Access Control Lists to raw data on which derived data depend In addition an ACL Filter can be developed to 036lter out unauthorized components from the template produced by the ACL Template Generator To illustrate suppose a derived data 000 002 004 006 002 t 013 depends on the raw data 002 r 006 002 020 and 002 021 where 002 004 023 002 r 025 002 020  and 002 t 023 030 031 002 033  When a template about the pair is generated the ACL Template Generator attaches the ACLs of 002 r and 002 020 to 002 004  and the ACL of 002 021 to 002 t  If a requester wishes to obtain the pair but he has no permission to view 002 r then 002 004 will be 036ltered out and only 002 t is provided For some applications rather than 036ltering out entirely a component of a derived data a more sophisticated template may be developed to leave the component but removing the effect of the raw data on which the component depends but a requester has no permission to view it This will be left to our future work 7 Future Work Other than a concrete implementation of DDS there are many interesting things left to be done st we observe that this paper focuses on user behaviors in using services in cyberspace Similar methods may be used to develop a framework for studying user behaviors in more general applications Secondly current design in DDS does not consider storage limitation The retention of outdated logs and their destruction time may be considered to save storage space Third XQuery is u sed in DDS for derived data de\036nition However XQuery provides only simple aggregate functions such as sum count min max etc More complicated data mining functions such as 036nding association rules or sequence patterns may be included Finally in case a user wishes to manually modify the generated derived data should such modi\036cation be allowed If so should requesters be aware of such modi\036cation and how References  G  A dom a v icius a nd A  T u zhilin U s i ng data m i ning m e thods to b u ild customer pro\036 les Computer  34\(2 Feb 2001  R  M  A rlein B J a i M J a k obs s on F  Monros e and M  K  R eiter Privacy\227preserving global customization In Proc ACM EC  pp 176\226184 2000  A  L  B er ger a nd V  O  M ittal O celot a s y s t em for s um m a rizing web pages In Proc ACM SIGIR  pp 144\226151 2000  S  B oag D Cham berlin M  F  F ernandez D Flores cu J  R obie J Simeon and M Stefanescu XQuery 1.0 An XML Query Language In W3C Working Draft  2002  D  B rickle y a nd R Guha R DF V o cab ulary D es cription L anguage 1.0 RDF Schema In W3C Working Draft  2003  S  C Cha On the Realization of a Universal Pro\036 le System in Cyberspace  PhD thesis National Taiwan University Taipei Taiwan 2003  S  C Cha Y  J  J oung and Y  E  L u e Building uni v e rs al pro\036 le systems over a peer-to-peer network In Proc WIAPP  pp 142\226151 2003  S  C Cha W  Y a ng and Y  Z  J oung T h e i nte g ration o f w eb bro w s ing behavior In Proc 4th Taiwan Conference on Doctoral Consortium in Information Management  pp 7\22612 Shanghai China Apr 2002  L  C hen a nd K Sycara W e bMate A p ers onal a gent for b ro ws ing and searching In Proc Agents  pages pp 132\226139 1998  J  H  Chim  R  W  L au a nd A  S i  M ultir e s o lution cache m anagement in digital virtual library In Advances in Digital Libraries Conference  pp 66\22675 Apr 1998  I Cingil S upporting g lobal u s e r p ro\036 l es through trus ted a uthorities  In ACM SIGMOD Record  volume 31 pp 11\22617 2002  I Cingil A Dogac and A  A zgin A broader a pproach to pers onalization CACM  43\(8 Aug 2000  J  Clark XSL T rans form ations XSL T   V ers i on 1 0 W 3C Recom mendation http://www.w3.org/TR/xslt  Nov 19 1999  L  Cranor  M  L angheinrich M  M ar chiori M Presler-Marshall and J Reagle Platform for Privacy Preference P3P In W3C Recommendations  2002  A Deuts c h M Fernandez D Florescu A Levy and D Suciu XML-QL A Query Language for XML In Submission to the World Wide Web Consortium  1998  D oubleClick I nc h ttp://w w w  doubleclick com    D  C F a lls ide X M L S chem a P ar t 0  P r i m e r  I n W3C Recommendation  2001  X Fu J  B udzik a nd K J  Ham m ond Mining navigation history for recommendation In H Liebermann editor in Proc IUI  pp 106\226 112 Jan 9\22612 2000  T  J o achim s  D  F r eitag and T  M  M itchell W e b w atcher  A tour guide for the world wide web In IJCAI 1  pp 770\226777 1997  T  C K i nnear and J  R  T aylor  Marketing Research\227an Applied Approach  McGRAW-HILL Inc 4th edition 1991  Y  L a brou and T  F inin Y ahoo as an ontology Us ing y ahoo categories to describe documents In Proc CIKM  pp 180\226187 Nov 2\2266 2000  O L a ssila a n d R  R  S wic k  R e s ourc e De sc ription F ra m e w o rk RDF Model and Syntax Speci\036 cation In W3C Recommendation  1999  W  L itw in and A  A bdellatif A n o v e rvie w o f t he m u lti-databas e manipulation language MDSL Proc IEEE  75\(5 pp 621\226631 May 1987  A L uotonen T h e c om m o n l og\036 le form at 1995  Micros oft Micros oft P as s port T echnical W h ite P a per  Mar  2001  C Ne well A O L quietly launches m agic carpet In eWeek.com  2002  B P a dm anabhan Z  Z h eng and S  O  K im brough Pers onalization from incomplete data what you don\222t know can hurt In Knowledge Discovery and Data Mining  pp 154\226163 2001  M Perk o w itz and O  E tzioni A dapti v e W eb s ites  CACM  43\(8 Aug 2000  G Salton Automatic text processing The transformation analysis and retrieval of information by computer  Addison-Wesley Reading MA USA 1989  A P  Sheth a nd J  A L a rs on Fede rated database systems for managing distributed heterogeneous and autonomous databases ACM Computing Surveys  22\(3 Sept 1990  G L  Soml o a nd A E  Ho we  I nc remental clustering for pro\036 le maintenance in informatio n gathering web agents In Proc AAMAS  pp 262\226269 May 2001  F  v a n H arm e len J  Hendler  I  H orrocks  D L  McGuinnes s  P  F  Patel-Schneider and L A Stein OWL Web Ontology Language In W3C Working Draft  2003  S W e ibel J  K unze C L a goze and M  W olf RFC 2413 Dublin Core metadata for resource discovery 1998  H Z h a Generic s um m a rization a nd k e yphras e e xtraction u s i ng m u tual reinforcement principle and sentence clustering In Proc ACM SIGIR  pp 113\226120 2002 Proceedings of the 2005 Symposium on Applications and the Internet \(SAINT\22205 0-7695-2262-9/05 $ 20.00 IEEE 


  0 1000 2000 3000 4000 0.00 0.50 1.00 1.50 2.00 Support No. of Patterns Checked Apriori BDFS\(b Figure 12 Number of patterns checked by Apriori and BDFS\(b\or T10I28D100K with varying supports   0 10 20 30 40 50 050100150 No. of Transactions \(x 1000 Time \(s b=100K b=10K b=1K  Figure 13 Time scalability of BDFS\(b\with increasing no. of transactions for T5I4D1K,10K,100K and b=1K,10K and 100K, support 0.5  0 0.2 0.4 0.6 0.8 1 0 200 400 600 800 1000 1200 No. of Items Time \(s b=100K b=10K b=1K  Figure 14 Scalability Test of BDFS\(b\with number of items for T5I4, support 0.5 It will provide decision makers with more accuracy and reduced time lag and help in real-time decision-making In this paper, we have proposed an algorithm BDFS\(b\which is a brute force version of the Block Depth First Search\(BDFS w e  h a v e co m p ared the performance of BDFS\(b\ith FP-Growth and Apriori and shown that it does significantly better than both Moreover, by adjusting its block size properly BDFS\(b\as the extra ability to run with limited available memory, which often becomes a point of concern in other algorithms. We have then shown that while running under real-time constraints it outputs large chunks of frequent patterns with fractional execution times. We have made detailed performance evaluation based on empirical analysis using several commonly used synthetic datasets and one real-life dataset          93 94 95 96 97 98 99 10 0 0 20 4060 80100 Tim e Patterns  Figure 15 Time-patterns % of BDFS\(b\or 0.05 support of  T6I5D10K 50.00 60.00 70.00 80.00 90.00 100.00 0.00 20.00 40.00 60.00 80.00 100.00 time pattern 0.03% supp 0.04% supp 0.05% supp 0.06% supp 0.07% supp 0.08% supp 0.09% supp 0.1% supp  Figure 16 Time-patterns % of BDFS\(b\or varying support for T6I5D10K Thus, we have demonstrated that real-time frequent pattern mining can be done successfully using BDFS\(b This algorithm may be modified for matching the memory and time constraints dynamically. It may be worthwhile to generalize this algorithm for other application domains having specialties such as sequential or temporal data This proposed brute force version of BDFS\(b expected to handle well problems of moderate to large size. For very large databases it may be beneficial to design domain specific heuristics for enhanced efficiency We believe this study will encourage use of AI search techniques in real-time frequent pattern mining 0-7695-2268-8/05/$20.00 \(C Proceedings of the 38th Hawaii International Conference on System Sciences - 2005 8 


  0 50 100 150 200 250 N o  o f P a tte r n s  F[4 F[5 F[6 F  7 16 45 75 95 Frequent Pattern Length Time 15.67 25.60 45.44 65.28 75.20 85.12 95.04 100.00  Figure 17 Real-time output of frequent patterns of T6I5D10K, support 0.05%, b =1K 0 50 100 150 200 250 No. of Patterns F F  4 F  5 F  6 F  7 8.97 25.00 50.64 76.28 100.00 Frequent Pattern Length Time 8.97 12.18 25.00 37.82 50.64 63.46 76.28 89.10 100.00  Figure 18 Real-time output of frequent patterns of T6I5D10K support 0.05%, b =10K 0 50 100 150 200 250 No. of Patterns F[3 F[4 F[5 F[6 F[7 8.64 24.07 54.94 85.80 Frequent Pattern Length Time 8.64 17.90 24.07 39.51 54.94 70.37 85.80 100.00  Figure 19 Real-time output of frequent patterns of T6I5D10K support 0.05%, b=100K      Tim e 1.72 4.18 14.01 23.83 38.58 48.41 63.15 97.55 100.00     350    350    350    350    350    350    350    350 F  350     2319    2319    2319    2319    2319    2319    2319    2319    2319     612    1028    1542    1488    1783    1935    2078    2695    3049     488    607    953    944    1070    1163    1250    1456    1790     299    301    347    331    363    376    390    445 F  508     134    134    134    134    134    136    136    137 F  141     1    31    31    31    31    31    31    31 F  31   F F  8 3 F  8 3 F  8] : 3 F  8] : 3 F  8 3 F  8 3 F  8 3  Bor der Sets BS[4 524 BS[4 879 BS[4 2469 BS[4 2446 BS[4 2890 BS[4 3159 BS[4 3676 BS[4 4820 BS[4 8127  BS[5 285 BS[5 310 BS[5 625 BS[5 625 BS[5 697 BS[5 747 BS[5 771 BS[5 927 BS[5 1865  BS[6 90 BS[6   90 BS[6 106 BS[6 101 BS[6 110 BS[6 118 BS[6 118 BS[6 136 BS[6 170  BS[7 21 BS[7 21 BS[7 21 BS[7 21 BS[7 21 BS[7 21 BS[7 21 BS[7 21 BS[7 21  Can dida te Sets C   17180 C   14570 C   10755 C   11698 C   9632 C   8504 C   7345   Figure 20 Frequent pattern output along with border sets and candidates patterns of BDFS\(b\or BMS-Web View-1 with support 0.08%. N= 497, T=2.5, D=59602, b = 497 0.00 20.00 40.00 60.00 80.00 100.00 0 20406080100 Time Patterns Output patterns including F\(1\ & F\(2 Output patterns excluding F\(1\ & F\(2  Figure 21 Time Ö patterns% comparison of BDFS\(b\or BMS-WebView-1 with support 0.08% and b = 497    0-7695-2268-8/05/$20.00 \(C Proceedings of the 38th Hawaii International Conference on System Sciences - 2005 9 


 7. Reference  1 B. G o e t ha ls  M em or y Is s u e s  in Fre q ue nt P a tte r n  Mining," in Proceedings of SAC'04 Nicosia, Cyprus ACM, 2004 2 B. G o e t ha ls  S urv e y on Fre que nt P a tte rn Mi ning   vol. 2004, 2003 3 A  Ma ha nti, S. G hos h  a nd A  K   P a l A H i g h  Performance Limited-Memory Admissible and Real Time Search Algorithm for Networks," University of Maryland at College Park, MD 20742, Maryland College Park, Computer Science Technical Report Series CS-TR-2858 UMIACS-TR-92-34, March 1992 1992  M  L  G o n zales Un earth BI in Real-tim e vo l  2004: Teradata, 2004 5 G a rtn er T h e RealT i m e En terp rise v o l. 2 0 0 4   2004 6 K  R  B hote  B ey ond C u s t o m e r Sa tisf a c tion to  Customer Loyalty," in Proceedings of the American Management Association New York, NY, 1996  S  L a n g en f e ld  CRM an d t h e Cu sto m er Driven  Demand Chain," vol. 2004, 2004 8 W  L i n, "A ssoc ia tion Rule  Min i n g f o r Colla b o ra tiv e  Recommender Systems," in Computer Science  Worcester Polytechnique Institute www.wpi.edu/Pubs/ETD/Available/etd-0515100145926 unrestricted/wlin.pdf, 2000, pp. 64 9 J  Sc ha f e r, J   K ons ta n, a nd J  Rie d l E le c t ronic  Commerce Recommender Applications Journal of Data Mining and Knowledge Discovery vol. 5, pp 115-152, 2001 1 R L a w r en ce  G   A l masi V  Ko tlyar M  V i vero s an d S. Duri, "Personalization of Supermarket Product Recommendations Journal of Data Mining and Knowledge Discovery vol. 5, pp. 11-32, 2001 1 Y. D. S h en  Q. Ya n g Z Zh a n g an d H L u  M i n in g  the Customer's Up-To-Moment Preferences for Ecommerce Recommendation," in Proceedings of the Advances in Knowledge Discovery and Data Mining 7th Pacific-Asia Conference, PAKDD 2003, Seoul Korea, April 30 - May 2 vol. 2637 / 2003 Lecture Notes in Computer Science K.-Y. Whang, J. Jeon, K Shim, and J. Srivastava, Eds. Heidelberg: SpringerVerlag, 2003, pp. 166-177 12 R. Ka la k o ta J. Sta lla e r t, a n d A  C. W h inston   Implementing Real time Supply Chain Optimization Systems," presented at Supply Chain Management Hong Kong, 1995 1 J D y ch e  Real T i me o r Righ t T i m e E x p l ain i n g T h e Real Time Enterprise," vol. 2004: CRM Guru, 2003 1 Op en S e rviceIn c  Real-T i m e En terp rise Risk an d Vulnerability Management," vol. 2004: Open Service Incorporation, 2004 15  Se e B ey ond R e a l T i m e Stoc k Ma na g e m e nt a n d  VMI," vol. 2004, 2004 16 W   L e e  S. J  Stolf o, P  K  Cha n E. Es k i n, W  Fa n, M  Miller, S. Hershkop, and J. Zhang, "Real time data mining-based intrusion detection," presented at DARPA Information Survivability Conference Exposition II, Anaheim, CA , USA, 2001 17 P. Ha je k  I. Ha v e l, a nd M. Chy til T he  G uha Me thod  of Automatic Hypotheses Generation Computing  vol. 1, pp. 293-308, 1966 18 R. Ag a r wa l, T  I m ie lin sk i, a nd A  S w a m i M ining  Association Rules Between Sets of Items in Large Datasets," in Proceedings of the ACM SIGMOD Conference on Management of Data  Washington,D.C.: ACM, 1993, pp. 207-216 1 J Hip p  U G u n t zer an d  G  Na kh aeizad e h   Algorithms for Association Rule Mining -- A general Survey and Comparision," in Proceedings of ACM SIGKDD vol. 2: ACM, 2000, pp. 58-64 20 R. Ag ra w a l a nd R. Srik a n t M ining Se que n tia l  Patterns," in Proceedings of the 11th IEEE International Conference on Data Engineering  Taipei, Taiwan: IEEE, 1995 2 A  S a vaser e E  O m i eci n s ki  an d S  B Navat h e   A n  Efficient Algorithm for Mining Association Rules in Large Databases," in Proceedings of the 21nd International Conference on Very Large Databases  Zurich, Swizerland, 1995, pp. 432-444 22  H  T o iv one n S a m pling L a r g e D a ta ba se s  f o r Association Rules," in Proceedings of the 22nd International Conference on Very Large Databases  Mumbai, India, 1996, pp. 134-145 23  C. H i dbe r O nline A s s o c i a tion Rule Mini ng  in Proceedings ACM SIGMOD International Conference on Management of Data Philadephia Pennsylvania: ACM, 1999, pp. 145-156 2 M  J Z aki  S cal ab l e A l go ri t h ms f o r A sso ci at i o n  Mining IEEE Transactions on Knowledge and Data Engineering vol. 12, pp. 372-390, 2000 25  J  H a n, J   P e i, a nd Y  Y i n M i n i n g Fre que nt P a tte rns  Without Candidate Generation," in Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data Dallas,TX: ACM, 2000, pp. 112 26  J  H Su a nd W  Y  L i n CBW  A n Eff i c i e n t Algorithm for Frequent Itmeset Mining," in Proceedings of the 37th Hawaii International Conference on System Sciences Hawaii: IEEE, 2004 27  R  K oha v i a nd F P r ov os t A pplic a t i ons of D a ta  Mining to Electronic Commerce Journal of Data Mining and Knowledge Discovery vol. 5, pp. 5-10 2001 28  M. S Ra hm a n N  L  Ma rti n  a nd S   P a ul D a t a  Mining, Group Memory, Group Decision Making: A Theoretical Framework," presented at Ninth Americas Conference on Information Systems, 2003 29  Deshpande, M. and G. Karypis Item Based Top-N Recommendation Algorithms 2003     0-7695-2268-8/05/$20.00 \(C Proceedings of the 38th Hawaii International Conference on System Sciences - 2005 10 


xt a ppl i c a t i on oc t e t s t re a m 22832 xt t e xt  pl a i n 17993 


the initial global candidate set would be similar to the set of global MFIs. As a result, during the global mining phase the communication and synchronization overhead is low  0 2 4 6 8 1 0 Number of Nodes Figure 5. Speedup of DMM 4.4.2 Sizeup For the sizeup test, we fixed the system to the 8-node con figuration, and distributed each database listed in Table 2 to the 8 nodes. Then, we increased the local database sire at each node from 45 MB to 215 MB by duplicating the initial database partition allocated to the node. Thus, the data distribution characteristics remained the same as the local database size was increased. This is different from the speedup test, where the database repartitioning was per formed when the number of nodes was increased. The per formance of DMM is affected by the database repartitioning to some extent, although it is usually very small. During the sizeup test, the local mining result of DMM is not changed at all at each node The results shown in Figure 6 indicate that DMM has a very good sizeup property. Since increasing the size of local database did not affect the local mining result of DMM at each node, the total execution time increased just due to more disk U 0  and computation cost which scaled almost linearly with sizeup 5 Conclusions In this paper, we proposed a new parallel maximal fre quent itemset \(MFI Max-Miner \(DMM tems. DMM is a parallel version of Max-Miner, and it re quires low synchronization and communication overhead compared to other parallel algorithms. In DMM, Max Miner is applied on each database partition during the lo 0 45 90 135 180 225 270 Amwnt of Data per Node \(ME Figure 6. Sizeup of DMM cal mining phase. Only one synchronization is needed at thc end of this phase to construct thc initial global candi date set. In the global mining phase, a top-down search is performed on the candidate set, and a prefix tree is used to count the candidates with different length efficiently. Usu ally, just a few passes are needed to find all global maximal frequent itemsets. Thus, DMM largely reduces the number of synchronizations required between processing nodes Compared with Count Distribution, DMM shows a great improvement when some frequent itemscts are large \(i.e long patterns employed by DMM for efficient communication between nodes; and global support estimation, subset-infrequency based pruning, and superset-frequency based pruning are used to reduce the size of global candidate set. DMM has very good speedup and sizeup properties References I ]  R. Agrawal and R. Srikant  FdSt Algorithms for Mining As sociation Rules  Pmc. o f f h e  ZOrh VLDB Conf, 1994, pp 487499 2] R. Agrawal and I. C. Shafer  Parallel Mining of Association Rules  IEEE Trans. on Knowledge and Dura Engineering Vol. 8, No. 6, 1996, pp. 962-969 3] R. I. Bayardo  Efficient Mining Long Patlems from Databases  Proc. ofrhe ACM SIGMOD Inf  l Conf on Man ogemenr ofDara, 1998, pp. 85-91 4] S.  M. Chung and J. Yang  A Parallel Distributive Join Al gorithm for Cube-Connected Multiprocessors  IEEE Trans on Parallel and Disrribured Systems, Vol. 7, No. 2, 1996, pp 127-137 51 M. Snir, S. Otto. S. Huss-Lederman, D. Walker, and J. Don gana, MPI: The Complete Reference, The MIT Press, 1996 


gana, MPI: The Complete Reference, The MIT Press, 1996 6] R. Rymon  Search through Systematic Set Enumeralion   Pmc. of3rd Inr  l Con$ on Principles of Knowledge Repre sentation and Reasoning, 1992, pp. 539-550 507 pre></body></html 


sketch-index in answering aggregate queries. Then Section 5.2 studies the effect of approximating spatiotemporal data, while Section 5.3 presents preliminary results for mining association rules 5.1 Performance of sketch-indexes Due to the lack of real spatio-temporal datasets we generate synthetic data in a way similar to [SJLL00 TPS03] aiming at simulation of air traffic. We first adopt a real spatial dataset [Tiger] that contains 10k 2D points representing locations in the Long Beach county \(the data space is normalized to unit length on each dimension These points serve as the  airbases  At the initial timestamp 0, we generate 100k air planes, such that each plane \(i uniformly generated in [200,300], \(ii, iii destination that are two random different airbases, and iv  the velocity direction is determined by the orientation of the line segment connecting its source and destination airbases move continually according to their velocities. Once a plane reaches its destination, it flies towards another randomly selected also uniform in [0.02, 0.04 reports to its nearest airbase, or specifically, the database consists of tuples in the form &lt;time t, airbase b, plane p passenger # a&gt;, specifying that plane p with a passengers is closest to base b at time t A spatio-temporal count/sum query has two parameters the length qrlen of its query \(square number qtlen of timestamps covered by its interval. The actual extent of the window \(interval uniformly in the data space \(history, i.e., timestamps 0,100 air planes that report to airbases in qr during qt, while a sum query returns the sum of these planes  passengers. A workload consists of 100 queries with the same parameters qrlen and qtlen The disk page size is set to 1k in all cases \(the relatively small page size simulates situations where the database is much more voluminous specialized method for distinct spatio-temporal aggregation, we compare the sketch-index to the following relational approach that can be implemented in a DBMS. Specifically, we index the 4-tuple table lt;t,b,p,a&gt; using a B-tree on the time t column. Given a count query \(with window qr and interval qt SELECT distinct p FROM &lt;t,b,p,a&gt WHERE t?qt &amp; b contained in qr The performance of each method is measured as the average number of page accesses \(per query processing a workload. For the sketch-index, we also report the average \(relative Specifically, let acti and esti be the actual and estimated results of the i-th query in the workload; then the error equals \(1/100 set the number of bits in each sketch to 24, and vary the number of sketches The first experiment evaluates the space consumption Figure 5.1 shows the sketch index size as a function of the number of sketches used \(count- and sum-indexes have the same results more sketches are included, but is usually considerably smaller than the database size \(e.g., for 16 signatures, the size is only 40% the database size 0 20 40 60 80 


80 100 120 140 160 8 16 32 number of sketches size \(mega bytes database size Figure 5.1: Size comparison Next we demonstrate the superiority of the proposed sketch-pruning query algorithm, with respect to the na  ve one that applies only spatio-temporal predicates. Figure 5.2a illustrates the costs of both algorithms for countworkloads with qtlen=10 and various qrlen \(the index used in this case has 16 sketches also illustrate the performance of the relational method which, however, is clearly incomparable \(for qrlen?0.1, it is worse by an order of magnitude we omit this technique Sketch-pruning always outperforms na  ve \(e.g., eventually two times faster for qrlen=0.25 increases with qrlen, since queries returning larger results tend to set bits in the result sketch more quickly, thus enhancing the power of Heuristics 3.1 and 3.2. In Figure 5.2b, we compare the two methods by fixing qrlen to 0.15 and varying qtlen. Similar to the findings of [PTKZ02]4 both algorithms demonstrate  step-wise  growths in their costs, while sketch-pruning is again significantly faster The experiments with sum-workloads lead to the same observations, and therefore we evaluate sketch-indexes using sketch-pruning in the rest of the experiments 4 As explained in [PTKZ02], query processing accesses at most two paths from the root to the leaf level of each B-tree regardless the length of the query interval Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE sketch-pruning naive relational 0 100 200 300 400 500 600 700 800 900 0.05 0.1 0.15 0.2 0.25 number of disk accesses query rectangle length 300 0 100 200 400 500 600 1 5 10 15 20 number of disk accesses query interval length a qtlen=10 b qrlen=0.15 Figure 5.2: Superiority of sketch-pruning \(count As discussed in Section 2, a large number of sketches reduces the variance in the resulting estimate. To verify this, Figure 5.3a plots the count-workload error of indexes 


using 8-, 16-, and 32- sketches, as a function of qrlen qtlen=10 error \(below 10 it increases slowly with qrlen used, however, the error rate is much higher \(up to 30 and has serious fluctuation, indicating the prediction is not robust. The performance of 16-sketch is in between these two extremes, or specifically, its accuracy is reasonably high \(average error around 15 much less fluctuation than 8-sketch 32-sketch 16-sketch 8-sketch relative error 0 5 10 15 20 25 30 35 0.05 0.1 0.15 0.2 0.25 query rectangle length relative error 0 5 10 15 20 25 30 35 1 5 10 15 20 query interval length a qtlen=10, count b qrlen=0.15, count relative error query rectangle length 0 5 10 15 20 25 0.05 0.1 0.15 0.2 0.25 relative error query interval length 0 5 10 15 20 25 30 1 5 10 15 20 c qtlen=10, sum d qrlen=0.15, sum Figure 5.3: Accuracy of the approximate results The same phenomena are confirmed in Figures 5.3b where we fix qrlen to 0.15 and vary qtlen 5.3d \(results for sum-workloads number of sketches improves the estimation accuracy, it also leads to higher space requirements \(as shown in Figure 5.1 Figures 5.4a and 5.4b show the number of disk accesses for the settings of Figures 5.3a and 5.3b. All indexes have almost the same behavior, while the 32-sketch is clearly more expensive than the other two indexes. The interesting observation is that 8- and 16-sketches have 


interesting observation is that 8- and 16-sketches have almost the same overhead due to the similar heights of their B-trees. Since the diagrams for sum-workloads illustrate \(almost avoid redundancy 32-sketch 16-sketch 8-sketch number of disk accesses query rectangle length 0 50 100 150 200 250 300 350 400 0.05 0.1 0.15 0.2 0.25 number of disk accesses query interval length 0 50 100 150 200 250 300 350 1 5 10 15 20 a qtlen=10 b qrlen=0.15 Figure 5.4: Costs of indexes with various signatures Summary: The sketch index constitutes an effective method for approximate spatio-temporal \(distinct aggregate processing. Particularly, the best tradeoff between space, query time, and estimation accuracy obtained by 16 sketches, which leads to size around 40 the database, fast response time \(an order of magnitude faster than the relational method average relative error 5.2 Approximating spatio-temporal data We proceed to study the efficiency of using sketches to approximate spatio-temporal data \(proposed in Section 4.1 as in the last section, except that at each timestamp all airplanes report their locations to a central server \(instead of their respective nearest bases maintains a table in the form &lt;time t, plane p, x, y&gt;, where x,y with parameters qrlen and qtlen distinct planes satisfying the spatial and temporal conditions. For comparison, we index the table using a 3D R*-tree on the columns time, x, and y. Given a query, this tree facilitates the retrieval of all qualifying tuples, after which a post-processing step is performed to obtain the Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE number of distinct planes \(in the sequel, we refer to this method as 3DR method introduces a regular res  res grid of the data space, where the resolution res is a parameter. We adopt 16 sketches because, as mentioned earlier, this number gives the best overall performance Figure 5.5 compares the sizes of the resulting sketch indexes \(obtained with resolutions res=25, 50, 100 the database size. In all cases, we achieve high compression rate \(e.g., the rate is 25% for res=25 evaluate the query efficiency, we first set the resolution to the median value 50, and use the sketch index to answer workloads with various qrlen \(qtlen=10 


workloads with various qrlen \(qtlen=10 size \(mega bytes database size 0 20 40 60 80 100 120 140 160 25 50 100 resolution Figure 5.5: Size reduction Figure 5.6a shows the query costs \(together with the error in each case method. The sketch index is faster than 3DR by an order of magnitude \(note that the vertical axis is in logarithmic scale around 15% error observations using workloads with different qtlen Finally, we examine the effect of resolution res using a workload with qrlen=0.15 and qtlen=10. As shown in Figure 5.6c, larger res incurs higher query overhead, but improves the estimation accuracy Summary: The proposed sketch method can be used to efficiently approximate spatio-temporal data for aggregate processing. It consumes significantly smaller space, and answers a query almost in real-time with low error 3D Rsketch number of disk accesses query rectangle length 1 10 100 1k 10k 0.05 0.1 0.15 0.2 0.25 16 14% 15 15% 13 relative error number of disk accesses query interval length 1 10 100 1k 10k 1 5 10 15 20 16 15% 15% 12% 11 relative error a qtlen=10, res=25 b qrlen=0.15, res=25 0 500 1000 1500 2000 2500 25 50 100 number of disk accesses resolution 20% 15% 14 relative error c qrlen=0.15, qtlen=10 


c qrlen=0.15, qtlen=10 Figure 5.6: Query efficiency \(costs and error 5.3 Mining association rules To evaluate the proposed algorithm for mining spatiotemporal association rules, we first artificially formulate 1000 association rules in the form \(r1,T,90 with 90% confidence i randomly picked from 10k ones, \(ii in at most one rule, and \(iii Then, at each of the following 100 timestamps, we assign 100k objects to the 10k regions following these rules. We execute our algorithms \(using 16 sketches these rules, and measure \(i  correct  rules divided by the total number of discovered rules, and \(ii successfully mined Figures 5.7a and 5.7b illustrate the precision and recall as a function of T respectively. Our algorithm has good precision \(close to 90 majority of the rules discovered are correct. The recall however, is relatively low for short T, but gradually increases \(90% for T=25 evaluated in the previous sections, the estimation error decreases as the query result becomes larger \(i.e., the case for higher T 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 precision HT 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 recall HT a b Figure 5.7: Efficiency of the mining algorithm Summary: The preliminary results justify the usefulness of our mining algorithm, whose efficiency improves as T increases Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE 6. Conclusions While efficient aggregation is the objective of most spatio-temporal applications in practice, the existing solutions either incur prohibitive space consumption and query time, or are not able to return useful aggregate results due to the distinct counting problem. In this paper we propose the sketch index that integrates traditional approximate counting techniques with spatio-temporal indexes. Sketch indexes use a highly optimized query algorithm resulting in both smaller database size and faster query time. Our experiments show that while a sketch index consumes only a fraction of the space required for a conventional database, it can process 


required for a conventional database, it can process queries an order of magnitude faster with average relative error less than 15 While we chose to use FM sketches, our methodology can leverage any sketches allowing union operations Comparing the efficiency of different sketches constitutes a direction for future work, as well as further investigation of more sophisticated algorithms for mining association rules. For example, heuristics similar to those used for searching sketch indexes may be applied to improve the brute-force implementation ACKNOWLEDGEMENTS Yufei Tao and Dimitris Papadias were supported by grant HKUST 6197/02E from Hong Kong RGC. George Kollios, Jeffrey Considine and were Feifei Li supported by NSF CAREER IIS-0133825 and NSF IIS-0308213 grants References BKSS90] Beckmann, N., Kriegel, H., Schneider, R Seeger, B. The R*-tree: An Efficient and Robust Access Method for Points and Rectangles. SIGMOD, 1990 CDD+01] Chaudhuri, S., Das, G., Datar, M., Motwani R., Narasayya, V. Overcoming Limitations of Sampling for Aggregation Queries. ICDE 2001 CLKB04] Jeffrey Considine, Feifei Li, George Kollios John Byers. Approximate aggregation techniques for sensor databases. ICDE, 2004 CR94] Chen, C., Roussopoulos, N. Adaptive Selectivity Estimation Using Query Feedback. SIGMOD, 1994 FM85] Flajolet, P., Martin, G. Probabilistic Counting Algorithms for Data Base Applications JCSS, 32\(2 G84] Guttman, A. R-Trees: A Dynamic Index Structure for Spatial Searching. SIGMOD 1984 GAA03] Govindarajan, S., Agarwal, P., Arge, L. CRBTree: An Efficient Indexing Scheme for Range Aggregate Queries. ICDT, 2003 GGR03] Ganguly, S., Garofalakis, M., Rastogi, R Processing Set Expressions Over Continuous Update Streams. SIGMOD, 2003 HHW97] Hellerstein, J., Haas, P., Wang, H. Online Aggregation. SIGMOD, 1997 JL99] Jurgens, M., Lenz, H. PISA: Performance Models for Index Structures with and without Aggregated Data. SSDBM, 1999 LM01] Lazaridis, I., Mehrotra, S. Progressive Approximate Aggregate Queries with a Multi-Resolution Tree Structure. SIGMOD 2001 PGF02] Palmer, C., Gibbons, P., Faloutsos, C. ANF A Fast and Scalable Tool for Data Mining in Massive Graphs. SIGKDD, 2002 PKZT01] Papadias,  D., Kalnis, P.,  Zhang, J., Tao, Y Efficient OLAP Operations in Spatial Data Warehouses. SSTD, 2001 PTKZ02] Papadias, D., Tao, Y., Kalnis, P., Zhang, J Indexing Spatio-Temporal Data Warehouses ICDE, 2002 SJLL00] Saltenis, S., Jensen, C., Leutenegger, S Lopez, M.A. Indexing the Positions of Continuously Moving Objects. SIGMOD 2000 SRF87] Sellis, T., Roussopoulos, N., Faloutsos, C The R+-tree: A Dynamic Index for MultiDimensional Objects. VLDB, 1987 TGIK02] Thaper, N., Guha, S., Indyk, P., Koudas, N Dynamic Multidimensional Histograms 


SIGMOD, 2002 Tiger] www.census.gov/geo/www/tiger TPS03] Tao, Y., Papadias, D., Sun, J. The TPR*Tree: An Optimized Spatio-Temporal Access Method for Predictive Queries. VLDB, 2003 TPZ02] Tao, Y., Papadias, D., Zhang, J. Aggregate Processing of Planar Points. EDBT, 2002 TSP03] Tao, Y., Sun, J., Papadias, D. Analysis of Predictive Spatio-Temporal Queries. TODS 28\(4 ZMT+01] Zhang, D., Markowetz, A., Tsotras, V Gunopulos, D., Seeger, B. Efficient Computation of Temporal Aggregates with Range Predicates. PODS, 2001 ZTG02] Zhang, D., Tsotras, V., Gunopulos, D Efficient Aggregation over Objects with Extent PODS, 2002 Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE pre></body></html 


