A Lazy Approach to Pruning Classification Rules EIena Baralis Politecnico di Torino Dipartimento di Automatica e Informatica baralis@polito.it Abstract Associative classifcation is a promising technique for the generalion of highly precise classifers Prpvious works propose several clever rechniques ro prune rhe huge ser of generared rules with rhe rwofold aim of selecring a small ser of high quality rules. and reducing the chance of eve In rhis papez we argue rhar pruning should be reduced ro 
a minimum and rhar rhe ovailability of a large rule base may improve the precision of rke classifer wirhour affecting its performance In L:\222 Live and Let Live\a new algorirhm for associarive classifcarion o lazy pruning technique irer arively discards all rules rhar only yield wrong case classi ficarions Classifcarion is performed in rwo steps Inirially rules which have already correcrly classified at leasr one train ing case, sorred by confidence are considered Ifthe case is srill 
unclassijed rhe remaining rules \(unused during rhe rroining phasej are considered again sorred by confidence Exrensive experimenrs on 26 darabases fmm rhe UCI machine learning darabase reposirory show thar L7 im proves rhe classifcarion precision with respecr ro previous approaches ring 1 Introduction An important class of data mining problems is repre sented by association rule discovery Z Association rules describe the co-occurrence among data items in a large amount of collected data Recently association rules have been also considered a valuable 
tool for classification pur poses Classification rule mining is the discovery of a small set of rules in the training database to form a model of the data the classifier The classifier is then used to classify appro priately new data for which the class label is unknown 9 Association rules are usually extracted without previous definition of the set of labels that belong to the domain of 0-7695-1754-4102 17.00 0 2002 IEEE 35 Paolo Garza Politecnico di Torino Dipartimento di Automatica e Informatica 
gaaa@polito.it the head of the rules, while for classification rules this do main corresponds to the set of class labels Differently from decision trees association rules consider the simultaneous correspondence of values of different attributes hence a lowing to achieve better accuracy in general as shown by a number of experiments 3,6,7 1 I Recent approaches to associative classification e.g CAEP 3 CMAR 6 CBA 7 and ADT Ill extract a small set of high quality association rules with specified 
thresholds for support and confidence Since association mining may yield a huge number of classification rules, the rule base obtained by association mining is then pruned to reduce its size Several different methods have been pro posed for performing pruning all achieving a good accu racy However we argue that most pruning techniques may go too far by discarding also useful knowledge to gether with low quality rules Hence, we propose a novel two steps classification technique in which only a very re duced amount of pruning is performed by eliminating only 223harmful\224 rules i.e rules that only produce wrong classifi 
cation results in the training data The contribution of this paper is as follows First we customized a good association rule extraction algorithm 61 to extract rules with variable suppon threshold in order to generate a similar number of rules for all classes, including those that may have a small number of cases Second, we designed a lazy pruning technique that only discards 223harm fur rules from the rule base Third we developed a two steps classification approach in which 223first\224 class rules i.e rules used in the classification of training cases are considered first and second class rules i.e rules not used 
during the training phase are used next for the same task of classification. Second class rules are considered only when a test case cannot be classified by means of first class rules In the following Section, we show by means of a moti vating example how previous approaches fail the classifica tion of a new test case because of excessive pruning Sec tion 2 discusses the probIem of associative classification In Section 3 we present the generation of the L3 classifier by means of multiple support thresholds and lazy pruning 


Housing Finance Social Health Class h:c fc s:np he:r recommended h:c fc s:sp he:r recommended h:lc fic s:np hem not recommended Figure 1 Example training data set Rule Conf Supp fc  rec 100.00 66.67 h:c  rec 100.00 66.67 hex i rec 100.M 66.67  Attribute I Domain 11 Attribute I Domain I I Housine I convenient 11 Social I nonorob I 1ess.conv slightly-prob inconvenient recornmended notIec less-conv Figure 2 Domains of attributes Section 4 describes how L classifies test data by means of its two levels Section 5 provides an extensive suite of ex periments to validate the L approach. Finally in Section 6 we discuss the main differences between our approach and previous work on associative classification, and Section 7 draws conclusions 1.1 Motivating Example Current approaches to associative classification albeit to a different extent. select association rules for the classifier by matching them with training set cases Once one e.g CBA 7 or more rules \(e.g CMAR 6 cover a case the case is discarded Hence all other rules that were covering only the same case will not he included in the classifier Consider the example training dataset in Fig I which describes the characteristics of two classes of nurses rec ommended and not recommended Values of attributes have been abbreviated for the sake of rule representation The actual domains of attributes are given in Fig 2 Given this training data the extraction of all rules with out using suppon and confidence constraints yields 38 rules In Fig 3 are reponed some of the extracted rules sorted by descending confidence descending suppon and increasing length We now apply the pruning techniques proposed in pre vious approaches to derive the classifier rules A first prun ing technique, applied by most previous approaches e.g CMAR 6 CBA 7 and ADT 111 uses general rules to prune more specific rules with lower confidence The rules remaining after this pruning are 9 CMAR next applies a pruning technique based on x coefficient Let all rules he positively correlated and not pruned by this technique Finally the last pruning technique adopted by CBA and CMAR is the dumbuse coverage tech nique In panicular CBA extracts the minimal number of he:nr  not rec 100.00 33.33 he:r.s:np  rec 100.00 33.33 he:r.s:sp  rec 100.00 33.33 s:np  rec 50.00 33.33 Figure 3 Some of the example generated rules    Rule Conf Supp fx  rec 100.00 66.67 tic  not rec 100.00 33.33 Figure 4 Example CBA classifier rules necessary to cover each case in the training data Af ter this pruning, the rules in Fig 4 compose the final CBA classifier Differently from CBA. CMAR selects at least 6 rules be fore discarding any training case This allows the gener ation of a richer classifier containing more rules which will be able to cover a wider spectrum of test data By using 6  2 adequate for such a small training dataset CMAR yields a classifier composed by the rules in Fig 5 Now consider the following new test tuple h:lc Elc s:sp he:r Both CBA and CMAR classifiers are not able to classify this tuple because of an excessive pruning Indeed e.g rule he  1 s  sp  mc and several others which would cover the case has been discarded although with high confidence IM Excessive pruning affects albeit to a different extent all previous proposals of associative classifiers The database coverage technique used by CMAR is a first step towards a reduction of the negative effect of excessive pruning but still useful knowledge is lost We argue that rules may he useful to cover new cases even if they have not been used to cover training data and should not he discarded However Suppan IS only 33.33 low with resprcr CO other rules so a simple threshold on suppon may have discarded chis rule as well Rule Conf Supp fc  rec I00.0OW 66.67  h:c  rec 100.00 66.67 fic  not rec 100.00 33.336 he:nr not rec 100.00 33.33 Figure 5 Example CMAR classifier 36 


since the quality of these rules has not been verified dur ing the training phase \(they have not been used these rules should be treated differently from the high quality rules se lected during the training phase To this end we propose a two steps classification process in which the second step considers rules usually pruned by the other algorithms 2 Problem Statement The database is represented as a relation R whose schema is given by k distinct attributes A1  Ab and a class attribute 6 The attributes may have either a cate gorical or a continuous domain For categorical attributes all values in the domain are mapped to consecutive posi tive integers In the case of continuous attributes, the value range is discretized into intervals and the intervals are also mapped into consecutive positive integers2 In this way, all attributes are treated uniformly Each tuple in R can be described as a collection of pairs attribute integer value plus a class label a value belong ing to the domain of class attribute C Each pair attribute integer value will be called item in the reminder of the pa per A training case is a tuple in relation R where the class label is known, while a test case is a tuple in R where the class label is unknown A classifier is a function from A   A to C that al lows the assignment of a class label to a test case Given a collection of training cases the classification task is the generation of a classifier able to predict the class label for test cases with high accuracy Association rules extraction is a recently proposed ap proach for the classification task Association rules are rules in the form X  Y When using them for classification purposes X is a set of items while Y is a class label A cased is said to match a collection of items X when X C d The quality of an association rule is measured by two pa rameters, its support given by the number of cases match ing X U Y over the number of cases in the database and its confidence given by the the number of cases matching X U Y over the number of cases matching X Hence, the classification task can be reduced lo the generation of the most appropriate set of association rules for the classifier Our approach to such task is described in the next Section 3 Mining classification rules In our approach. abundance of classification rules is im portant to allow a wider selection of rules when classifica tion is performed. Hence, our aim is to extract a large num ber of rules by settine rather low suuwrt and confidence   Thhe pmblcm of di~~rrti~ation has ken widely dealt with in the ma chine lcaming community we e.g 141 and will not k discussed funher in this papr thresholds Ideally setting the support threshold close to zero could be considered, together with a limitation on the number of generated rules by screening rules on the con fidence value An attempt in this direction has been pro posed in IO where classification rules are extracted only with a confidence threshold but it is unclear how well the approach scales on training cases characterized by a large number of items Currently the most efficient approach to the extraction of classification rules is proposed in CMAR 6 which is based on the well-known  algorithm To per form classification rule extraction. we use a variation of the rule extraction pan of the CMAR algorithm with a variable support threshold that allows us to treat uniformly classes characterized by an uneven number of training cases In the remainder of this Section we discuss the use of multiple support thresholds for rule extraction and the lazy pruning technique applied by L 3.1 Multiple support thresholds Most algorithms for the extraction of classification rules set only one fixed threshold nrinrup for the minimum sup port value As already stated in 8 this is not the best choice when the training cases are unevenly distributed among classes In SI the extraction algorithm uses adiffer ent support threshold for each class ci given by minsup  mansup freq\(c where frey\(c is the frequencyofclass e However the number of extracted association rules de pends also significantly on the characteristics of the data distribution in class ci Hence, this definition of minsup does not guarantee that a sufficient number of rules for classes having low frequency will be generated We use an iterative approach in which the appropri ate support threshold minsup for each class is selected by analyzing the result of the previous extraction cycle As an initial value for support analogously to 181 we set minsup  minsupr freq\(c Afterrhis first step, since the availability for all classes of some rules with high conti dence is important, we check the following two conditions for each class q a an excessively low number of high confidence rules has been extracted for class ci with respect to the average number of rules less than 15 or b\class c is characterized by a significantly smaller number of rules with respect to the average number of rules in classes \(less than 20 If any of the above conditions is true minsup is further lowered as minsuppl  0.9  minsupik where k is the iteration number We have found after several experiments that the constant value 0.9 allowed us IO grad ually decrease the support threshold Furthermore only a few iterations were necessary before both above conditions became false The effect of variable support thresholds is further discussed in Section 5 37 


3.2 Lazy Pruning Although the number of association rules generated by rule extraction can be huge we argue that most rules can provide useful knowledge for the classification of test cases Hence in L lazy pruning is proposed to limit the number of pruned rules to a minimum as described below This technique yields good precision results only if it is coupled with the novel classification technique described in Sec tion 4 as shown by the experiments presented in Section 5 Before performing lazy pruning a global order is im posed on the rule base similar to 6 71 Let rl and r2 be two rules Then TI precedes rz denoted as TI  TZ if I conf\(rl  conf\(r2 or 2 conf\(r1  conf\(r2 and sup\(r1  sup\(r2 or 3 conf\(r1  conf\(r2 and sup\(rl  sup\(r2 and len\(rl  len\(rz or 4 if conf\(rl  conf\(r2 and sup\(r1  sup\(r2 and len\(r  le and lex\(rl  lex\(rz where len\(r denotes the number of items in the body of r and lex\(r denotes the position of T in the lexicographic order on items The only significant difference with respect to previous work is that in point 3 we son rules in decreasing length order while previous approaches prefer short NkS over long rules The reason for this choice is to give a higher rank in the ordering to more specific rules rules with a larger number of items in the body over generic rules which may lead to misclassification Note that since shorter rules are not pruned they can be considered anyway when specific rules are not applicable The idea behind lazy pruning is to discard from the clas sifier only the rules that do not correctly classify any train ing case i.e., the rules that only negatively contribute to the classification of training cases To this end after rule sort ing we cover the training cases to detect harmful rules Lines 1-24 of the pseudocde in Figure 6 show OUT ap proach For each training case we assign it to the first rule in the sort order that covers it lines 2-17 and we check if the assigned class label was correct or wrong lines 9 IO The case is then discarded line 16 After all cases have been considered the pruning step discards rules that only classified training cases wrongly \(lines 18-23\Train ing cases classified by discarded rules enter again the as signment loop lines 1-24 and the cycle is repeated until no training case is covered by discarded rules After having discarded harmful rules the remaining rules are divided in two groups lines 25-30 used rules which have already correctly classified at least one training case spare rules which have not been used during the training phase but may become useful later 3N0~c that the case io discarded also if it is not covered by any mlr Procedure generateClassifier\(ru1 es,duta 1 while datu not empty  2 3 covered=false 4 5 6 7 8 9 if d.class==r.class right IO else r.wrong II covered=tNe 13 NR 14 r=next rule from rules 16 delete d from datu 17  18 for each T in rules  19 20 delete r from rules 21 data data U r.dataClassified 23  24  25 for each T in rules  26 if r.right>O 27 28 else 29 30  for each d in data  NR  number of rules r  first rule of rules while covered==false and NR>O  if r covers d  r.dataClassified  r.dataClassified U d 12  15  if r.wrong>O and r.right==O  22  usedRules  usedRules U r spareRules  spureRules U r Figure 6 L7 classifier generation Used rules are generated with a database coverage tech nique similar to other approaches 6.7 and provide a high level model of each class Spare rules instead allow us to increase the precision of the classifier by capturing spe cial cases which are not covered by used rules see Sec tion 1.1 Both groups of rules are used to create the clas sifier In particular used rules are assigned to the first level of the classifier while spare rules yield the second level Rules in each level are ordered following the global order described above Our pruning technique is very simple compared with previous proposals \(see e.g 16.7 I I but it is very effective as shown by the experiments in Sec tion 5 It eliminates from the rule base only those rules which yield negative effects on the classification of train ing cases As discussed in Section 5 we observe that such a simple pruning is not adequate if the classifier is to be generated only with used rules Hence more tolerance in pruning is effective only if coupled with more tolerance in the generation of the classifier 38 


We finally note that pruning of more specific classifica tion rules as suggested in 6 71 even with a lower confi dence is not performed Indeed if the iterative rule deletion loop eliminates a general rule. the specific rule may remain and if it does not generate erroneous classifications be as signed to spare rules and become useful for test cases 4 Classification In this Section we describe the classification of test cases by means of the two levels L\222 classifier When a test case is to be classified rules of level I of the classifier are consid ered If no rule in this level matches the test case then rules in level II are considered If again no rule matches the test case the case is assigned to the default class Differently from previously proposed associative classi fiers L:\222 usually contains a large number of rules In par ticular level I of the classifier containing rules used in the training phase to cover some training case is characterized by a small number of rules analogously to previous ap proaches As shown in Section 5 this level performs the 223heavy duty\224 classification of most test cases and can pro vide a general model of each class By contrast level 11 of the classifier may contain a large number of rules which are seldom used These rules allow the classification of some more cases which cannot be covered by rules in the first level Hence the few used rules in the second level allow a significant increase in classification precision as shown in Section 5 Since level I usually contained about 102-103 rules it can easily fit in main memory Thus the main classifi cation task can be performed efficiently Level 11 in our experiments, included around lo5 rules Rules were orga nized in a compact list, sorted as described in Section 3.2 Hence level 11 of L could be loaded in main memory as well as discussed in more detail in Section 5 Of course if the number of rules in the second level further increases e.g because the support threshold is further lowered to capture more rules with high confidence\efficient access may become difficult We are currently considering a more compact representation of rules, based on the relationship between a generic rule and its specializations We finally note that similarly to 7 I I we only con sider the first rule matching the test case to classify it, but a further increase in precision may be obtained by performing classification with multiple rules as described in 61 In deed, this technique is orthogonal to our approach and can be separately applied to each L3 level 5 Experimental Results In this Section. we describe the experiments to measure accuracy classification efficiency and memory consump tion for L\222 We have performed a large set of experiments using 26 data sets downloaded from UCI Machine Learn ing Repository I  We compared L3 with the classification algorithms CBA 7 CMAR 6 and C4.5 9 The exper iments show that L3 outperforms all previous approaches by achieving a larger average accuracy 0.63 over the best previous, i.e CMAR and improving accuracy on 14 data sets over 26 The confidence constraint has not been enforced i.e minconf=O The value of the lowest rninsup column 5 and the average value of mznsupi column 6 for each data set are shown in Table 1 We have adopted the same tech nique used by CBA to discretize continuous attributes A IO fold cross validation test has been used to compute the accuracy of the classifier All the experiments have been performed on a ImMhz Pentium 111 PC with 1.SG main memory running RedHat Linux 7.2 Table I compares the accuracy of L3 with the accuracy of C4.5 CBA and CMAR obtained using standard values for all the parameters In parlicular the columns of Table I are I name of data set 2 number of attributes 3 num ber of classes, \(4\number of cases \(records 7 accuracy of C4.5 8 accuracy of CBA 9 accuracy of CMAR and 10 accuracy of L\222 fi\222 has best average accuracy 0.63 with respect to CMAR and best accuracy on 14 of the 26 UCI data sets over 50 of the data sets The improvement in accuracy may be due to several factors: \(a\the use of multiple support thresholds b the lazy pruning technique, and c the use of rules included in the second level of the classifier It has not been necessary to adopt multiple support thresholds in most of the 14 cases in which L3 achieves better accuracy than previous approaches Hence, multiple support thresholds only have a limited effect on the average accuracy value To observe the effect of rules in level II of L\222 we com pare the accuracy obtained by only using rules in level I of LJ with the accuracy obtained by using both levels The result of the experiment is reported in Table I The related columns of Table 1 are 1 1 number of rules in the first level of L 12 number of rules in the second level 13\accuracy of L3 using only rules in the first level 14\difference between L3 with both levels column IO and L3 with only first level \(column 13 By considering only rules in the first level L3 achieves best accuracy on only 6 of the UCI data sets and an aver age accuracy somewhat lower than CBA and CMAR In particular the average accuracy is about 1 less in the worst case i.e with respect to CMAR  Hence the first level already captures most fundamental characteristics of each class thus providing a model of reasonable qual ity However this experiment also shows the importance of rules stored in the second level Indeed lazy pruning 39 


I Average I I I I 0.94 I 1.04 I 83.34 I 84.69 I 85.22 I 85.85 I I I 84.18 I 1.67 I Table 1 Comparison of L accuracy with respect lo C4.5, CBA CMAR by itself considering only level I rules\yields a medium quality classifier, while the joint effect with the presence of level 11 rules allows a significant gain in average accuracy 1.67 This result shows that the small number of rules used for covering training data \(rules in level I albeit very effective is not sufficient. A significant improvement in ac curacy is obtained by exploiting the knowledge contained in rules which have not been used for covering training cases i.e level II rules We observe that usually the number of test cases correctly classified by the second level is rather small but sufficient to obtain a better accuracy for L For example for data set labor level II correctly classifies just 3 more test cases but since the data set is composed by 57 cases, this allows a 5.26 in accuracy In the case of data set glass the improvement obtained by the second level is more evident Indeed 23 more cases are correctly classified over a total of 214 cases 10.75 in accuracy To verify if lazy pruning affected classification accuracy we also ran tests \(not reported here\using two level classi fication without performing any pruning of rules For these experiments. each training case is considered only once for matching \(lines 2-17 in Fig 6 Rules that matched at least one training case \(disregarding correctness of the match are assigned to the first level while the second level contains all remaining rules These tests resulted in lower average pre cision 1.44 with respect to the value reported for L:l in Table I showing that also two level classification by itself is not an effective classification technique Since the number of rules in the two levels of L is sig nificantly different see Table I we analyzed the perfor mance of L3 during the classification of test data The re sults are reported in column 5 of Table 2 which shows the CPU time required by the classification of the whole data set as a test set using L3 We have also make a test using only the first level but we have not reported those times because they are very closed to the ones showed in Table 2 A larger time is necessary for the generation of the classifier with respect to previous approaches, owing to the lower \(and variable support threshold for assvciation rule extraction However we observe that the generation of the 40 


classifier is a task that takes place very rarely as opposed to the classification of new cases Thus a longer generation time may be acceptable in order to increase significantly the precision of the classification step Table 2 also shows the number ofcases classified by rules of the first level col umn 3 rightly classified  wrongly classified and by rules of the second level  unclassified data column 4 The number of cases in each data set is reported in column 2 We observe that a vast majority of cases is classified by rules ofthe first level, while only a minimal number of accesses to the second level is required Furthermore rules of sec ond level usually correctly classified each considered case Only in very few cases rules of the second level are not able to classify a case or wrongly classify it Hence, although the number of rules in the second level can be quite large accessing it is a rare event that does not significantly affect performance while it significantly improves accuracy To 0btain:efficent classification it is important that rules in both levels can be stored in memory Hence we ana lyzed main memory usage of L for the storage of the rule base during the classification phase Table 2 reports the re sults The columns of table 2 are 6 number of rules in the first level of L 7 number of rules in the second level of LT3 8 memory used to store the rules of the first level in Kbyte 9 memory used to store the rules of the sec ond level in Kbyte As expected. the memory required for storing first level rules is negligible, while the second level rules need significantly more memory especially for some data sets \(up to SOMbyte Hence on currently available machines \(512Mbyte main memory is a standard today\we are able to store second level rules in main memory The ability of storing second level rules is closely related to the value of the support threshold Since we believe that second level rules have to be stored in main memory, we selected support threshold values that did not generate an excessive number of rules such that main memory storage becomes impossible For the presence of the second level L3 required more memory of CBA and CMAR, and so L memory pre formance is:worse than the previous ones 6 Previous Related Work Associative classification has been first proposed in CBA 7 CBA based on the Apriori association rules mining al gorithm 2 extracts a limited number of association rules ma 8oooO A weakness of this approach is the reduced number of rules extracted by means of Apriori because long and interesting rules are usually not generated thus losing some relevant knowledge. Furthermore as discussed in Section 1.1 after sorting on descending confidence a pruning technique is applied and only the minimal num ber of rules necessary to cover training data is used to cre ate the classifier A new version of the algorithm has been presented SI in which the use of multiple supports is pro posed to increase accuracy in presence of uneven class dis tributions, together with a combination of C4.5 and Naive Bayes classifiers to be used when CBA rules wrongly clas sify training cases While using multiple supporl thresh olds permits to overcome to some extent the limitations due to the use of the Apriori algorithm it does not address the overpruning problem described in Section I I ADT Ill is a different classification algorithm based on association rules combined with decision tree pruning techniques All rules with a confidence greater or equal to a given threshold are extracted and more specific rules are pruned A decision tree is created based on the remaining association rules on which classical decision tree pruning techniques are applied Analogously to other algorithms the classifier is composed by a mall number of rules and prone to the overpruning problem CMAR[6 is the latest classification algorithm based on association rules proposed in literature CMAR proposes a suite of different pruning techniques pruning Tspecialis tic rules use of the xz coefficient and database coverage As already noted the database coverage technique is more tolerant than the coverage technique adopted by CBA, and allows more but not all rules to cover the same training case Again useful rules may be pruned thus yielding the same overpruning problem A further technique applied in CMAR to increase average accuracy is the classification of test cases by means of more than one rule This technique is independent of the adopted pruning techniques and is likely to be profitable in our setting as well Hence, we are considering it as a further improvement of our classifier 7 Conclusions In this paper we have described L a novel approach to classification by means of association rules While pre vious approaches suggested various techniques to generate a classifier containing a very limited number of rules we believe that it is important to exploit all the knowledge that can be extracted from the training cases. Hence, we propose a lazy pruning technique which only discards harmful rules, coupled with a two levels classification approach in which rules normally discarded in previous approaches are included in the second level of the classifier and used only when first level rules are not able to classify a test case We observe that even in presence of a large number of rules into the second level of L classifier the interpretabil ity of techniques based on classification rules is not lost In deed used rules \(rules in the first level of the classifier\cap ture recurring properties of the data, thus providing a high level model of each class, composed of just a few rules Furthermore we note that overfitting is avoided by the second level of the classifier Indeed, as shown in Section 5 41 


Table 2 Classification time and memory usage of L this level forces a priority ordering among rules that avoids considering a medium quality rule belonging to the sec ond level when a better rule is available for classification in the first level Hence. considering second level rules can never cause a reduction in precision since cases classified by these rules would he otherwise unclassified or assigned to the default class The lazy pruning approach allowed us to significantly increase average classification precision over previous ap proaches In particular the experiments show that the pres ence of the second level provides a major enhancement in the classification quality without reducing the efficiency of the classification activity References I Illp;//wvwl,icr.uci.edu ml~~m/MLRepo.~iru~.hrml UCI Machine Leornin Reposirory Unrversiw of CaliJomio Imine 121 R Agrawal and R Srikant Fast algorithm for mining asso ciation rules In VLDB'Y4 Sonriujio Chile Sept 1994 3 G Dong X Zhang L Wong and 1 1.i CAEP Classifi cation by aggregating emerging piter In lnlemnrionrrl Conference on Discovery Science To!qo Japan Dec 1999 141 U Fayyad and K lrani Multi-interval dircrriization of continuos-valued attributes for classification learning In IJ CAl'Y3 1993 151 J Han J Pei and Y Yin Mining frequent patterns with out candidate generation In SIGMOD'W Dollns 7X May 2ooO 6 W Li J Han and J Pei CMAR Accurate and efficient classification based on multiple class-association ruler In ICDM'OI Son Jose CA November 2001 71 B Liu W Hsu and Y Ma Integrating classification and association rule mining In KDD'Y8 New York NY August 1998 8 B Liu Y Ma and K Wong Improving an association rule based classifier In PKDD'oO Lyon France Sept 2wO 191 J Ouinlan C4.5 ru~rarn forclossificarion learninz Mor   gan Kaufmann 1992 IO K Wang Y He D W Cheung and F Y L Chin Mining confident rules without SUDDO reauirement In CIKM'OI  Allanro GA November 2001 I I K Wang S Zhou and Y He Growing decision trees on 42 


a Computing view b query graph t f ivot method Figure 5 pivot method query graphs re\337ecting the produced columns of the select clause contributes with a value of pc 004 8 to the overall cost resulting in 14 004 pc 6 004 pt  5.2 Comparing different Methods The cost of computing the nearest neighbour using the t method may be split into the computation of the cost for the view de\336nition to perform the t of the prototype information and the core select statement using the t view As already mentioned the pt 212 1 ary join of the prototype table is easily optimized by the existence of an index of the prototype ID In this case each join partner has the size of 1 004 1 d  yielding pt 004 1 d  as the overall cost computing the view without any return operator The query itself joins the view with the PConformations table Reading the result of the view corresponds to one single tuple with pt 004 1  d  columns The access of the PConformations table has the size of pc 004 1  d   The outer query adds to the overall costs with reading the result of the inner query  pc 004 1  d  004  pt 1  and accessing the Prototypes table  pt 004 1  d   The return operator 336nally consumes a data stream of cardinality pc with d 3 attributes The cost of discretization using the self-join method can be computed in three steps The 336rst step considers the computation of the view P rototypeAssignment which requires the access to the two tables P roteinConf ormation  pc 004 1  d   and P rototypes  pt 004 1  d   The resulting data stream which has to be read for further processing yields due to the semantics of the Cartesian product to  pc 004 pt  004 1  d 2  The second step addresses the inner query consuming exactly the size of the view and producing a data stream of pc 004 2 for only two columns The outer query reads the result of the inner query and the result of the view   pc 004 pt  004 3  d   At last the return operator has costs the size of the join operator yielding pc 004  d 3  In a similar y the cost for the minpos  method can be computed Instead of the join with the P rototypeAssigment view in the outer query the P roteinConf ormation table with a cost of pc 004  d 1 a Computing view b query graph PrototypeAssignment of self join method Figure 6 self-join method query graphs Figure 7 cost reduction scenario is referenced Additionally the view must be executed only once further reducing the cution costs Table 1 summarizes the partial and total cost for all different methods computing the prototype for each amino acid residue Since this tabular and formular-based representation does not give any hint about the best strategy 336gure 7 gives a scenario with four different dimensions i.e d 1  4  7  10  and 5000 amino acid residues The scenario shows the resulting performance gain of the pivot method compared to minpos  and selfjoin method with 25 100 1000 and 2500 prototypes Within the proposed cost model the t method yields a slightly lower total cost than the minpos  method because the pt 212 1 ary selfjoin is considered extremely cheap r due to the dependency of the total cost from the number of prototypes the t method can not be considered a feasible solution for real applications with a reasonable high number of prototypes Compared to the self-join method the minpos  method yields a substantial cost reduction 6 Summary and Conclusion This paper introduces the problem of 336nding frequent substructures in protein data sets The analysis process is split into two parts The 336rst step consists in 336nding the Proceedings of the 15th International Conference on Sci entific and Statistical Database Management \(SSDBM\22203 1099-3371/03 $17.00 \251 2003 IEEE  


 t method view execution pt 001 1  d   f database operations inner query pt 001 1  d  pc 001 1  d   pt 1 joins outer query pc 001 1  d  001  pt 1 pt 001 1  d  pc 001  d 3 total cost pt 001 3  5 d  pc 001 5  3 d  pc 001 pt 001 1  d  self-join method view execution 2 001  pc 001 1  d  pt 001 1  d   f database operations inner query  pc 001 pt  001 3  d  1 join outer query pc 001 2 pc 001 pt  001 3  d  pc 001 3  d  2 cross product total cost pt 001 2  2 d  pc 001 7  3 d  pc 001 pt 001 6  2 d  1 group by minpos method view execution pc 001 1  d  pt 001 1  d   f database operations inner query  pc 001 pt  001 3  d  1 join outer query pc 001 2 pc 001 1  d  pc 001 3  d  1 cross product total cost pt 001 1  d  pc 001 7  3 d   pc 001 pt  001 3  d  1 group by Table 1 Cost comparison of the different methods nearest prototype in the multi-dimensional dihedral angle space To accomplish this task the data sets are brought into a relational schema and a method is proposed to compute the minimal distance considering the wrap-around effect in the angle space Three different methods to 336nd an associated prototype inside the database systems are compared A minimal SQL extension  minpos  maxpos  function results in much more ef\336cient query execution plans The second step of generating frequent item sets to detect frequent substructures within the amino acid sequences requires substantial SQL extension A ew operator as a new member of the OLAP grouping function operators is introduced This operator is a generic tool and may be ploited by a huge set of data mining applications To summarize a database system used to ef\336ciently analyse huge data volumes requires additional support from the technology  The required extension range from minimal UDFs like our proposed minpos  maxpos functions to more complex operators like our proposed grouping combinations operator f and only if the database community provides this kind of functionality the acceptance of database systems in the biotechnology community will increase in the near future References  R Agra w al T  Imielinski and A N Sw ami Mining association rules between sets of items in large databases In Proceedings of the International Conference on Management of Data  pages 207\320216 ACM Press 1993  S Bohl M Dink elack er  J  Griese and S Schrader  Highly adaptable amino acid side chain rotamer library in pdb coordinates In Workshop in Computational Biology at the Plant Biochemistry Department of the Albert-Ludwigs-Universitt Freiburg Germany  2002  M Bo wer  F  Cohen and R Dunbrack Homology modeling with a backbone-dependent rotamer library J Mol Biol  267:1268\3201282 1997  R Chandrasekaran and G Ramachandran Studies on the conformation of amino acids xi analysis of the observed side group conformations in proteins Int J Pept Prot Res  2:223\320233 1970  R Dunbrack and F  Cohen Bayesian statistical analysis of protein side-chain rotamer preferences Protein Sci  6:1661\320 1681 1997  J Gray  A  Bosw orth A Layman and H Pirahesh Data cube A relational aggregation operator generalizing groupby cross-tab and sub-total In Proceedings of the Twelfth International Conference on Data Engineering  pages 152\320 159 IEEE Computer Society 1996  A Hinneb ur g M Fischer  and F  Bahner  Finding frequent substructures in 3d-protein databases In Workshop on Bioinformatics at the 19th International Conference on Data Engineering  IEEE Computer Society 2003  M James and A Sielecki Structure and re\336nement of penicillo-pepsin at 1.8 a resolution J Mol Biol  125:299\320 361 1983  J K usze wski A Gronenborn and G Clore Impro ving the quality of nmr and crystallographic protein structures by means of conformational database potential derived from structure databases Protein Sci  5:1067\3201080 1996  S C Lo v ell J M W ord J S Richardson and D C Richardson The penultimate rotamer library Proteins Struct Funct Genet  40:389\320408 2000  M MCGre gor  S  Islam and M Sternber g Analysis of the relationship between side-chain conformation and secondary structure in globular proteins J Mol Biol  198:295\320310 1987  R Srikant and R Agra w al Mining generalized association rules In VLDB\32595 Proceedings of 21th International Conference on Very Large Data Bases Switzerland  pages 407\320 419 Morgan Kaufmann 1995  M J Zaki Ef 336cient enumeration of frequent sequences In Proceedings of the 1998 ACM CIKM International Conference on Information and Knowledge Management Bethesda Maryland USA November 3-7 1998  pages 68\32075 ACM 1998  M Zhang B Kao C L Y ip and D  W L Cheung Ffs an i/o-ef\336cient algorithm for mining frequent sequences In Knowledge Discovery and Data Mining PAKDD 2001 5th Paci\336c-Asia Conference Hong Kong China April 16-18 2001 Proceedings  volume 2035 of Lecture Notes in Computer Science  pages 294\320305 Springer 2001 Proceedings of the 15th International Conference on Sci entific and Statistical Database Management \(SSDBM\22203 1099-3371/03 $17.00 \251 2003 IEEE  


OM OM 006 OD8 01 012 014 016 018 02 022 False alarm demity Figure 9 Percentage of tracks lost within 200 seconds using three-scan assignment with PD  0.9 TI  O.ls Figure 11 T2  1.9s and T  Is ij  20 and 0  0.1 24 1 22  20  E fls 0  8l 16 0 n 14  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 T1/12 PD Average track life of three-scan assignment with PD varying TI  0-ls T2  1.9s T  Is X  0.02 ij LO and   0.1 mareuvenng index Figure 12 Percentage of lost tracks of 4-D assipment in 200 seconds with maneuvering index varying X  0.01 Ti  0.1 T2  1.9s and T  IS PD  0.98 Figure 10 Percentage of lost tracks of 4-D assignment in 200 SeoDllCls with TI and T2 varying PD  0.98 X  0.02 q 20 and 0  0.1 4-1607 


Figure 13 Average gate size for Kalman filter Figure is relative as compared to nq and curves are parametrized by ij/r with unit-time between each pair of samples 1.2 Iy I 1.1 0.5 I A CRLB for he unifm samiina I  0.4 0.35 d 3 03 i7 3 0.25 0 0.M 0.04 0.06 008 0.1 0.12 0.14 0.16 0.18 0.2 False A!am DemW V I    Figure 14 CramerRao Lower Boundfor Mean Square Error of uniform and nonuniform sampling schemes with Ti  O.ls T2  1.9s T  IS PD  0.9 ij  5 and U  0.25 1 unifon sampling r-ls ked i non-uniform sampling loge inlewi I ti non-uniform sampling shod interva I 0.9 0.8 I Figure 15 MSE comparison of three-scan assignment with Ti and T2 varying I'D  1 X  0.01 ij  20 and U  0.1 4-1608 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


