Using Formal Concept Analysis to Establish Model Dependencies  Igor Ivkovic and Kostas Kontogiannis Dept of Electrical and Computer Engineering University of Waterloo Waterloo ON N2L3G1 Canada  iivkovic kostas  swen.uwaterloo.ca Abstract Software models evolve at different levels of abstraction from the requirements speciìcation to development of the source code The models underlying this process are related and their elements are usually mutually dependent To pre 
serve consistency and enable synchronization when models are altered due to evolution the underlying model dependencies need to be established and maintained As there is a potentially large number of such relations this process should be automated for suitable scenarios This paper introduces a tractable approach to automating identiìcation and encoding of model dependencies that can be used for model synchronization The approach rst uses association rules to map types between models and different levels of 
abstraction It then makes use of formal concept analysis FCA on attributes of extracted models to identify clusters of model elements Keywords software evolution software modelling model synchronization model dependencies formal concept analysis 1 Introduction As software evolves its main constituents from more abstract ones such as business workîows to very speciìc ones such as source code also evolve The problem with this behavior is in its complexity that is concurrently and 
systematically updating all of the software models as some of them change This complexity is inherent and twofold First models from different stages of development are created by different stakeholders  e.g  source code is created by developers architectural design documents are created by software architects with differing rationale in mind Second the models that are created can differ greatly they 1 This work is funded in part by the IBM Canada Ltd Laboratory Center for Advanced Studies CAS in Toronto can be at signiìcantly different levels of abstraction and 
they can exhibit varied levels of expressiveness and semantics We have previously proposed and discussed this problem in a form of a framework for incremental model synchronization titled mSynTra 6 15 Our frame w o rk follo ws the Model-Driven Software E volution MDSE paradigm that is based on the theory of Model Driven Architecture MDA 8 W ithin this frame w o rk each change in s o ftware is made on a model at a particular level of abstraction within an iterative and incremental lifecycle such as the Rational Uniìed Process RUP 4 T o res t ore cons i s t e nc y 
between models after changes are made transformations applied to one model are traced and recorded Using identiìed model dependencies th e transformation trace is then translated and applied to all affected models Model dependencies play a crucial role here as they indicate what models and what model elements are effected and might need to be changed to reîect a traced transformation The synchronization process is in the end validated and terminated based on predeìned equivalence relations In this paper we aim to further extend the mSynTra framework and deìne a basis for systematic identiìcation and establishment of dependencies among related models We propose the use of formal concept analysis FCA 3 
as an apparatus for identiìcation of clusters of objects that share common attributes We view models and model elements as top-level objects that po ssess certain attributes As an attribute logic for mapping attributes from different context  i.e  domain models we introduce an idea of attribute association rules  These rules are deìned at two levels at the domain model level where mappings between types are deìned and at the concrete model level where mappings between attributes and related annotations are deìned To identify the semantic and abstraction-level differ 
ences we also discuss the process of model extraction where related domain models are rst reìned to enable a more direct mapping between their types These altered domain models are then used as a basis to extract intermediate Proceedings of the International Conference on Information Technology: Coding and Computing \(ITCCê05 0-7695-2315-3/05 $ 20.00 IEEE 


models that are less semantically different and that are more suited for establishment of model dependencies As an example of applicability and validity of our approach we present a case study of establishing dependencies between Business Process Models BPMs  i.e business workîows that include processes tasks decisions etc  and the underlying source code  i.e JavaandEnterprise Java Beans EJB 19 that is us ed to enact represented functionality Due to signiìcant semantic and abstraction-level differences between these two domains it is necessary to both simplify the representation of the business workîows and abstract the code representation in terms of related business functionality For both of these processes we have selected XML as a unifying language for expression and transformation The rest of the paper is organized as follows Section 2 shows how our research relates to previously published results in the area of software reuse and hierarchical data management Section 3 introduces our framework for identiìcation and establishment of model dependencies using FCA including the discussion of our approach to model extraction Section 4 presents a case study that demonstrates the use of this framework in practice Finally Section 5 presents the conclusions and directions for future research 2 Related Research Intheareaofsoftwarereuse,Engels et al  in  h a ve d i scussed the transformation of Uniìed Modelling Language UML Class Diagrams and UML Collaboration Diagrams 11 i nt o J a v a code The y h a v e s ho wn ho w t o deal wi t h both the structural and behavioral  e.g  ow mapping problems between UML and Java using a pattern-based transformation algorithm The pattern used is an instance of a metamodel from which one can identify parts of the source diagram that is to be transformed The pattern-based approaches depend on a predeìned set of patterns that is not trivial to extract and that has to be updated as new patterns are introduced Rich and Willis have used subgraphs to recognize program design 12 w hi l e S p anoudaki s a nd Constantopoulos have used a distance metric as a similarity measure to evaluate reuse potential of chosen artifacts 14 In the area of hierarchical data management Gianolli and Mylopoulos describe a semantic approach where XML data stores are mapped using a common DTD schema 13 while Faid et al  discuss how to use formal concept analysis to discover concepts and rules from structured complex objects 2  I n our approach we cons i d er s e mant i c mappi ngs of types from complex hierarchical data structures using intermediate models but we also address the problem of inference of relations between individual model elements based on the mappings of related attributes 3 A Framework for Establishing Model Dependencies using FCA In this section we introduce our framework for establishing model dependencies using the theory of formal concept analysis FCA As a part of the framework we describe an approach to model extraction that is used to bridge the semantic and abstraction-level differences between related models We also describe several attribute transformation techniques for establishing associations between attributes that belong to different domain models The process follows the following steps 1 Deìning metamodels and domain models 2 Extracting intermediate models 3 Deìning association rules and establishing model dependencies based on these rules and 4 Validating established dependencies In Figure 1 we illustrate the usage of established model dependencies within the context of the mSynTra framework Speciìcally let M and G be two models that need to be synchronized Also let M and G be derived from domain models DM M and DM G  respectively The set of association rules A R is then established between the properties and predicates of DM M and DM G andA R is used to derive a set of model dependencies D for M and G Using dependency tuples m j g i  from D the elements g i of G that are affected by changes of related elements m j of M can be identiìed and marked for if possible automatic updating  e.g  a direct change of a particular attribute or highlighting so they can later be reviewed and modiìed by model maintainers Furthermore starting with M and G are that are initially synchronized let M be altered through a sequence of model transformations T m into M   Each transformation t from T m is viewed as as a basic graph transformation  e.g insertion deletion or modiìcation and is applied on properties and predicated deìned at the domain model level Through established model dependencies D and association rules A R  we can identify elements of G that are affected by changes applied to M and also interpret T m into corresponding transformation sequence T g ToestablishthatM  and G  are synchronized an equivalence relation R is deìned Generation of the sequence T g and discussion of the equivalence relation R are beyond the scope of this paper 3.1 Deìning Domain Models For two models or sets of models that are to be synchronized it is rst necessary to abstract and represent model Proceedings of the International Conference on Information Technology: Coding and Computing \(ITCCê05 0-7695-2315-3/05 $ 20.00 IEEE 


 Model M M dm instance Model G G dm Domain Model M dm Domain Model G dm instance Association Rules A r Synchronization Rules S r output preserves defines Model M M dm transformed Transformation Sequence T m Transformation Sequence T g Model G G dm Equivalence Relation R validates validates Dependencies D establishes provides elements provides elements input transformed trace trace Association Rules Metamodel defines defines Figure 1 mSynTra Framework syntactics and semantics in a format of corresponding domain models or metamodels Generally domain models are already available as part of the design documentations but in some cases they have to be extracted using a suitable domain-analysis technique such as Feature-Oriented Analysis Technique FODA 7 The resulting domain models should be accurate representations of the type and content represented in their respective models  e.g  software design information based on UML syntax and should be stored in a format that can be easily accessed and manipulated such as MOF and XML 16 3.2 Extracting Intermediate Models The relations between models are expressed either at the metamodel or domain model level where the higher-level relations are used to infer the more speciìc ones or at the concrete model level Establishing relations at the domain model level includes identiìcation and encoding of properties and features that are mutually dependent These relations are then used as a basis to establish speciìc relations among models or among model elements The relations at the metamodel level can be also included but are not discussed here as we are only considering MOF-compliant metamodels For models that are based on different domain models establishing meaningful relations between model elements includes identifying differences in model semantics and underlying abstraction levels We propose to overcome this problem by generating int ermediate models which are more closely related and for which the process of establishing model dependencies is simpliìed We base our approach on a technique for model-driven business process recovery 18 which deals with s ynchronization of business workîows and the enacting source code and we extend this technique so that it can be used within the mSynTra framework Hence we interpret the process of model extraction as follows 1 For two models M and G and their respective domain models DM M and DM G  analyze the domain models and recognize compatible structural properties  e.g  compatible events metaclasses data types 2 Reìne DM M and DM G into DM M  and DM G  by omitting grouping or breaking up incompatible properties  e.g  metaclasses that cannot be mapped and 3 Use DM M  and DM G  as schemas to extract M  and G  from M and G respectively 3.3 Applying FCA to Extract Model Dependencies Depending on the type of information  e.g  structural behavioral temporal that is stored in the two models that have to be synchronized the amount of data available and the complexity of the mapping can differ signiìcantly Moreover depending on the information type the level of precision of the established relations also differs For example mapping behavioral and structural properties together would make more data available for the mapping but would Proceedings of the International Conference on Information Technology: Coding and Computing \(ITCCê05 0-7695-2315-3/05 $ 20.00 IEEE 


increase the complexity of the mapping process when compared to mapping only structural properties In this paper we focus on mapping a particular set of structural and behavioral properties based on our case study of mapping business workîows to underlying source code Within mSynTra our conceptual view of all MOFcompliant models 10 u s e d duri ng t h e cours e o f s oft w are evolution is as directed labelled attributed graphs In this view all graph properties are expressed in terms of labels and attribute and value pairs for individual nodes and edges All models also comply with their respective domain models and metamodels and their element types conform to corresponding domain types and metaclasses respectively We also presume that models at a particular level of abstraction  e.g  design architecture representing a software from a particular domain  e.g  accounting database management would be based on stable feature and domain models and would have convergent vocabularies and ontologies Hence within our approach to identiìcation of model dependencies we would identify individual relations starting at the level of matching domain model types and then proceed to match individual elements based on mappings of related vocabularies and ontologies If a dependency for a particular element cannot be found we would prompt for userês input and if no input was provided leave the dependency as undetermined As an apparatus for identifying and establishing model dependencies we apply the theory of formal concept analysis as deìned in 3 From this theory a formal context K  O A I consists of two sets O and A and a relation I between O and A The elements of the set O are called the objects and the elements of the set A are called the attributes of the context To express that object o from O is in a relation I with an attribute a from A we write this as oIa and read it as the object o has the attribute a Hence the relation I is also called the incidence relation of the context K We utilize this deìnition and interpret domain models as contexts DM  O DM A DM I DM  that consist of a set of metaclasses/types O DM  a set of related attributes A DM  and a set of relations I DM that deìne associations between the types and the attributes We also interpret models as contexts M  O M A M I M  that consist of a set of model elements O M  a set of attribute values A M and a set of relations I M that represent the mapping of types and attributes of model elements instantiated from respective domain models to corresponding values in A M We also deìne domain model DM M as a metacontext for a model/context M that is instantiated from DM M Asweare limiting our approach to binary contexts only we map naryto-binary relations through combinatorial scaling of format o   o v 1  o v 1 v 2   o v 1 v 2   v n  where o is an object and v 1 v 2  are n possible values Context Concept Lattice Association Rules  a1 a2 a3 a4 O1 x x x O2 x x O3 x O4 x x a2 a4 a1 a3 O3 O4 O1 O2 1 <2> a4 => a2 2 <1> a3 => a1 3 <1> a1 a2 => a4 Figure 2 FCA Example To deìne a concept we again follow the FCA theory Hence it holds that for a set O 1  O of objects let us deìne A 1   a  A  oIa for all o  O 1   as the set of attributes common to the objects in O 1 Also,forasetA 2  Aof attributes let us deìne O 2   o  O  oIa for all a  A 2   as the set of objects which have all the attributes in A 2  A formal concept of the context O A I is a pair O 1  A 2  with O 1  O A 2  A O 1 O 2 and A 2 A 1 whereO 1 is the extent and A 2 is the intent of the concept O 1 A 2  An implication between attributes in A is a pair of subsets of the attribute set A denoted A i  A j  which corresponds to informal statement that every object with the attributes in A i also has the attributes in A j  Figure 2 shows a context of four distinct object O and four distinct attributes A represented as a matrix where the relation o 1 Ia 1 implies that the matrix value of cell 1 1 equals true By examining the matrix the underlying association rules can be derived  e.g a4  a2 implies that all objects that have attribute a4 also have attribute a2 The related concept lattice can be inferred from the attribute associations as discussed in 3  We refer to the rules for deìning associations between attributes of different contexts as attribute association rules  To deìne our dependencies between models more formally let M and G be two models/contexts at different levels of abstraction with corresponding domain models/metacontexts DM M and DM G LetO M and O G be the objects stemming from these models and let A M and A G be the attributes contained within those models Let also Proceedings of the International Conference on Information Technology: Coding and Computing \(ITCCê05 0-7695-2315-3/05 $ 20.00 IEEE 


A MG   a M a G  a M  A M a G  A G where a M  a G  be a set of related attributes from A M and A G The attribute logic for deìning attribute associations a M  a G of attributes from different contexts is deìned as a set A R of attribute association rules For nonempty sets O M   O M and O G   O G of objects let A MG    m  A MG  g 1 Im g 2 Im for all g 1  O M  and for all g 2  O G    as the set of attributes common to the objects in O M  and O G   Also for a nonempty set A MG   A MG letB MG    g 1  O M g 2  O G  g 1 Im g 2 Im for all m  A MG    as the set of objects which have all the attributes in A MG   An inter-context concept of two contexts M and G is a set O M  O G  A MG   with O M   O M O G   O G A MG   A MG andB MG O M   O G   Finally a model dependency is an element d of a set of dependencies D of inter-context concepts for models M and G that have associated attributes Establishment of model dependencies therefore involves 1 Identifying a set of related A MG attributes from A M and A G  2 Selecting and encoding corresponding attribute association rules A R  3 Extracting inter-context concepts using attribute association rules as relations between attributes and 4 Reìning the extracted concepts by excluding the ones that are irrelevant  e.g  include only the objects from one context or that are redundant  e.g  equivalent association results based on different sets of attributes Attribute Association Rules Using an assumption that individual contexts based on particular domains would exhibit stable properties  e.g  feature maps ontologies the attribute association rules can be viewed as functions that map recognized properties between attribute with the goal of determining if they are related The rules can be classiìed as follows Hierarchical Association Models are viewed as parts of broader model hierarchies and corresponding feature maps are extracted Based on the feature mapping the two types of associations are recognized  Direct where a model implements a particular feature or  Implied where a model implements a subfeature of a higher-level feature Type-Based Association Types deìned in domain models are matched based on structura l compatibility and the association rules are created following these mapping Additional rules are added for recognized compatible data types Spatial Association Parts of the ow in behavioral models are represented as attributes  e.g elementAprecedes element B and the associations are established between the attributes using type-based associations for resolving element or data type mappings Text-Based Association Attributes are viewed as strings of text The potential syntactic differences are augmented through techniques such as  thesaurus replacements  recognizing different synonyms  stemming  reducing each word to its root  e.g  allocation allocated allocating to allocat  abbreviation expansion  expanding recognized abbreviations  stop-word elimination  eliminating words with no semantic meaning  word-matrix matching  recognizing clusters of attributes that share s emantically-relevant words and  nGram matching  substring matching irrelevant of position based on substrings of n size The given rules are not domain speciìc nor do they exhibit properties that are related to a particular domain Creating rules that are domain speciìc is implausible with regards to their updating and expansion as it requires extensive domain analysis that might have to be repeated every time the underlying contexts are changed The island-based approach can be used where those objects that do not belong to any particular cluster can be recognized based on the clustering of their neighbors For example in behavioral models for two ows  a  a   a  nd  b  b   b   where tuples  a  b nd a   b   were directly matched infer the  a   b   matching The conîict resolution for objects that belong to more than one cluster is achieved through scoring where the number of cluster matches for any two objects represents the score and the match is selected through the process of maximizing that score We also note that matches are tuples and not only pairs so that the relations between objects can be one-to-one one-to-many and many-to-many Hence where there is more than one instance of a match with the maximum score established dependency tuple would contain all of the related objects from all of the instances that exhibit the maximum score Proceedings of the International Conference on Information Technology: Coding and Computing \(ITCCê05 0-7695-2315-3/05 $ 20.00 IEEE 


3.4 Validation of Established Dependencies Validation of established dependencies can be automated if there exists a set of previously encoded dependencies in a suitable format su ch as XML or semi-automated if no encoded dependencies exist and if developers and domain experts need to be queried for feedback If the precision and recall levels obtained through validation are not satisfactory several categories of changes can be applied to improve the overall results such as 1 Including or excluding particular attribute association rules 2 Changing implied attribute mappings and object granularity levels and 3 Modifying threshold levels and other input parameters 4 Case Study Establishing Dependencies between BPMs and Java Source Code In our case study we are dealing with an industrial size system that consists of a hierarchy of business process models  i.e  business workîows as in Figure 3 and is enacted through a corresponding set of Java and EJB source code Through domain analysis and discussion with developers we found out that business workîows were in some cases created based on the information ow from the source code but were in most cases built independently We also found that design documentation specifying the mapping between the two was incomplete and out of date and that the precise dependencies between the two sets were implied and only known by developers and architects who worked on the system  Figure 3 Business Workîow Example Our goal in this case study was to assist in the process of systematically propagating change in both directions between business workîows and the source code However before that could be accomplished we rst needed to identify and establish dependencies among those models that were related In the following text we reîect on this experience of establishing model dependencies by rst describing the process of recovering intermediate models that bridge the gap between the business workîows and the underlying source code We then discuss how we use those intermediate models to extract conceptual information and infer potential dependencies represented as clusters of related attributes Finally we show statistics demonstrating the accuracy of our approach 4.1 Recovering Intermediate Models This part of the process had two parts simplifying and annotating the BPMs and abstracting and annotating the source code models The simpliìcation of the BPMs included the following 1 Analysis of the business workîows represented in XML with identiìcation of a simpliìed domain model of objects and attributes that have meaningful representation in the source code 2 Augmentation of the domain model with the annotation attributes for those elements for which additional information is available  e.g  notes explaining the meaning and the usage context of particular workîow elements and 3 Automatic extraction of the XML les based on the annotated domain model from the original BPM XML les The abstraction of the source code included the following 1 Analysis of the source code les and extraction of a suitable domain model of source code elements that have meaningful representation in the business workîows 2 Augmentation of the domain model with the annotation attributes for those elements for which additional information is available  e.g  top-level comments comments proceeding methods or individual statements and 3 Automatic extraction of the XML les based on the annotated domain model from the source code les 4.2 Extracting Dependencies using FCA Based on the reìned domain models we then proceeded to apply the FCA to identify and establish model dependencies The rst step in this process was creation of attribute Proceedings of the International Conference on Information Technology: Coding and Computing \(ITCCê05 0-7695-2315-3/05 $ 20.00 IEEE 


association rules and deìnition of corresponding FCA clusters On the BPM side at the domain model level in addition to type-based association rules we have also made use of the fact that business workîows are part of an overall workîow hierarchy to create hierarchical association rules On the source code side we could not establish hierarchical relations and at the domain model level we have only established type-based associations At the model level we have identiìed compatible attributes and annotations and have deìned a suitable set of spatial and text-based associations through experimentation on a selected through domain analysis a set of workîows and related source code  comparing Business Flow Source Code Objects Workflow process A set of corresponding Java classes Attributes Process name process notes Class name, top-level comments Objects Elements of the workflow process Elements of the source code flow Attributes Element name, type description, data input and output, spatial info Element name, type comments, method params, spatial info Objects Unmatched elements of the workflow process Unmatched elements of the source code flow Attributes Spatial information Spatial information Properties Hierarchical and typebased associations Type-based associations Objects A set of workflow processes A set of Java source code files Level 1 Level 2 Level 3 Level 0 Figure 4 Attribute and Property Mappings Figure 4 shows our mapping of the BPM and the source code attributes and properties The results of the matching process are stored in XML using a corresponding DTD Figure 5 illustrates a successful mapping between a source code ow at the bottom and the workîow model at the top along with sample association rules represented in OCL 17 4.3 Validating Extracted Dependencies The extracted dependencies could not be validated automatically as the previously established relations were not accurate nor complete Therefore we have included feedback from system developers and architects as part of our validation and have performed several iterations of feedback solicitation and reìnement We have measured recall as the presence of the correct match in a set of top results of our matching process and have not enforced the conîict resolution rules based on the feedback received from the developers The initial approximated recall levels that were obtained show a gradation of results that stem from almost perfect scores for our training data set which represents 0.15 of the complete data set 0.8 for the average case which represents approximat ely 0.60 of the complete data set to approximately 60 in the worst case which represents 0.25 of the complete data set with the overall recall average at 0.78 The differences in scores depend on several factors including  Experienced attrition of information when clustering less BPM-speciìc source code elements  The lack of conformance of workîows to source code ows in certain subsets  The fact that certain workîow and source code subsets were not synchronized etc  5 Conclusions and Future Research In this paper we have presented a framework for establishing model dependencies using formal concept analysis We have described the steps in this approach including creation of domain models extraction of intermediate models application of the formal concept analysis to extract model dependencies and validation of the established dependencies We have also discussed in some detail attribute association rules that can be used to infer relations between attributes from different contexts Finally we have demonstrated the usage of our approach by applying it to an industrial case study of synchronization of business process models with the enacting Java source code In future research we intend to further investigate the suitability of this approach by applying it to different case studies that relate to different stages of the software development lifecycle For example we intend to speciìcally investigate the synchronization of UML design diagrams with as-designed or as-implemented software architectures in one direction or with the underlying source code in another 6 Acknowledgements This work is performed in collaboration with the IBM Canada Ltd Laboratory Center for Advanced Studies CAS in Toronto We would especially like to acknowledge Tack Tong Ross McKegney and Terry Lau for their invaluable support on this project References  G  E ngel s R  Huecki ng S  S a uer  and A W agner  U ml collaboration diagrams and their transformation to java In Proceedings of the Second International Conference on The Uniìed Modeling Language UML  Fort Collins CO Oct 1999 Proceedings of the International Conference on Information Technology: Coding and Computing \(ITCCê05 0-7695-2315-3/05 $ 20.00 IEEE 


Yes  startUse findStale OrderItems verifyStale OrderItems Stop No hasMore Elements bATP Enabled Yes getInventory Status BO Deallocate ExistingInventoryCmd Deallocate ExpectedInventoryCmd Yes No Task Decision Process Termination Legend Legend Task Decision Data Element Process Termination Loop Reference Workflow Model Source Code Model Sample OCL Rules 1. NGramMatching.ApplyRule\(Workflow->ElementName SourceCode->ElementName, 3, 0.6 2. NGramMatching.ApplyRule\(Workflow->ElementDescription SourceCode->ElementComments, 3, 0.4 Figure 5 Mapping Workîows to Source Code Example 2 M  F ai d R  Mi ssaoui  a nd R  Godi n Kno wl edge di sco v e r y in complex objects Computational Intelligence  15\(1 Jan 1999 3 B  G an ter an d R  W ille Formal Concept Analysis Mathematical Foundations  Springer-Verlag 1999 4 I B M  R at i onal uni  e d p r o cess  r up  O nl i n e b y I B M C o r poration 2004 http://www.ibm.com/software/awdtools/rup 5 I  I vk o v i c and K K ont ogi anni s Model synchr oni zat i o n as a problem of maximizing model dependencies In Proceedings of the 19th Annual ACM Conference on ObjectOriented Programming Systems Languages and Applications OOPSLA 2004  Vancouver BC Oct 2004 6 I  I vk o v i c and K K ont ogi anni s T r aci ng e v ol ut i o n c hanges through model synchronization In Proceedings of the 20th IEEE International Conference on Software Maintenance ICSM 2004  Chicago IL Sep 2004  L  K ean F eat ure-ori ent ed domai n a nal ysi s S oft w are t echnology review Software Engineering Institute Carnegie Mellon University Pittsburgh PA 1997 8 A K l e p p e  J W a r m e r  a n dW B a s t  The Model Driven Architecture Practice and Promise  Addison-Wesley 2003  O MG Model dri v en archi t ect ure a t echni cal perspective Object Management Groupês OMGês Architecture Board ORMSC Document ORMSC/01-07-01 Object Management Group Jul 2001 10 OM G M e ta o b j ect f acility mo f sp eciìcatio n v ersio n 1 4  Technical report Object Management Group OMG Apr 2002 http://www.omg.org/docs/formal/02-04-03.pdf  OMG Uni ed modelling language uml speci cation Technical report Object Management Group Mar 2003 http://www.omg.org/docs/formal/03-03-01.pdf  C Rich and L  M W illis Recognizing a p rogr am s design A graph-parsing approach IEEE Software  Jan 1990  P  R odr i guezGi anol l i and J Myl opoul os A semant i c approach to xml-based data integration In Proceedings of the 20th International Conference on Conceptual Modeling  2001  G S p anoudaki s a nd P  C onst ant opoul os Measur i n g s i m i l a r ity between software artifacts In Proceedings of 6th International Conference on Software Engineering and Knowledge Engineering SEKE 94  Jurmala Latvia Jun 1994  T  T ong R  McK e gne y  T  L a u K K ont ogi anni s I  I v k o v i c  P Liew Y Zou Q Zhang and M Hung Model synchronization for efìcient software application maintenance In Proceedings of the 12th International Workshop on Program Comprehension IWPC 2004  Bari Italy Jun 2004  W 3 C  E x t e nsi bl e mar kup l a nguage  x ml  s peci  cat i on Technical report W3C XML Core Working Group Oct 2000  J W a rmer and A Kl eppe The Object Constraint Language Precise Modeling with UML  Addison-Wesley 1998  Y  Z ou T  L a u K K ont ogi anni s T  T ong and R  McK e gne y  Model driven business process recovery In Proceedings of the 11th IEEE Working Conference on Reverse Engineering WCRE 2004  Amsterdam The Netherlands Nov 2004  S u n m i c rosyst ems Onl i n e b y S un Mi crosyst ems Inc No v 2003 http://java.sun.com/products/ejb/docs.html Proceedings of the International Conference on Information Technology: Coding and Computing \(ITCCê05 0-7695-2315-3/05 $ 20.00 IEEE 


In the example we imagine that a customer visiting the mall is engaged in several associations simultaneously The 223well-come\224 association with the mall may be activated occasionally throughout the visit according to the needs of customer or mall As an example the customer may need to find a toilet by assistance from the mall At the same time the customer may be engaged in associations with one or more shops as well as with friends and family also visiting the mall Individual associations support these engagements hut must be potentially active whenever needed Interleaved execution of the life cycles of such examples of associations ensure their coexistence and hence the service of the customer 5.6 Distributed and Centralized Sequencing Figure 18 Distributed and centralized sequencing As illustrated by Figure 18 left the sequencing rules for an association described so far are distributed at each role-the form is role-centric description. In Figure 18 right centralized description of sequencing rules is illustratec&the form is association-centric The association as such has centralized information senders and receivers and sequencing rules Mixtures are possibl+the choice depends among others on the actual application 6 Conclusion We briefly summarize the identified characteristics of OUT notion of associations Finally we enumerate a number of identified challenges and problems to he addressed We characterize OUT notion of association by Abstraction Associations are abstractions over collaboration Objectification Associations support integrated objectification of collaboration and role aspects Loose Coupling: Properties of associations supported by event-based infomation sending and receiving at the programming level Various challenges exist for our notion of associations Associations of associations Coupling such associations through the roles of the association \(as illustrated in Figure 19 Creation and deletion of association instances Introduction of diagrams and notation for respectively at the modeling and programming level for creation and deletion of instances Dynamic enter and leave of association Support of dynamic entry and exit of an object in an association Inheritance of sequencing rules Investigation of similarities with inheritance anomaly\222 IS Association from an object to itself only Clarification of similarities with subjectivity 7 13 androles 12 IO Access from RI of properties of C1 Exploration of possible restrictions on access Description of sequencing rules including centralized sequencing Exploration of further possibilities Figure 19. Associations of roles in\associations Inheritance anomaly refers to the serious difficulty in combining inheritance and concurrency in a simple and satisfactoly way within concurrent object-oriented languages The problem is caused by synchronization constraints imposed on the acceptance of a message by an object Synchronization code is often hard to inherit and tends to require extensive redefinitions 4112 


Acknowledgments This research was supported in part by the Danish National Center for IT Research Center for Pervasive Computing Project No 212 Flexible Inter Processing FLIP and the A P Meller and Chastine Mc-Kinney Meller Foundation We thank the SWEAT group at the Maersk Mc-Kinney Moller Institute for collaboration and contributions to this article References l K.Amold JGasling The JAVA Programming Language. Addison Wesley 1999 2 K.Beck W.Cunningham A Laboratory For Teaching Object-Oriented Thinking Proceedings of the Object-Oriented Systems Languages and Applications Conference OOPSLA'89 1989  G.Boocb J.Rumbaugb LJacobson The Unified Modeling Language User Guide Addison Wesley, 1998 4 J.Bur!&ardt H.Henn S.Hepper TScbaeck K.RhdtorfF Pervasive Computing Technology and Architecture of Mobile Internet Applications Addison Wesley 2001 5 A.Goldberg D.Robson Smalltalk-80  The language and its implementation Addison Wesley 1983  K.Hallenborg B.B.Kristensen Pervasive Computing Mapping Tang0 Model onto Jini Technology Proceedings of the 6th World Multiconference on Systemics Cybernetics and Informatics SCI 2002 Orlando Florida 2002 7 W.Harrison H.Ossher Subject-Oriented Programming A Critique of Pure Objects Proceedings of the Object-Oriented Programming Systems Languages and Applications Conference OOPSLA'93 1993 SI B.B.Kristensen. Transverse Activities Abstractions in Object-Oriented Programming Proceedings of International Symposium on Object Technologies for Advanced Sofhvare ISOTAS93\1993  B.B.Kristensen Complex Associations Abstractions in Object-Oriented Modeling Proceedings of Conference on Object-Oriented Programming Systems Languages and Applications OOPSLA'94 1994 IO B.B.Kristensen Object-Oriented Modeling with Roles Proceedings of the 2nd Intemational Conference on Object-Oriented Information Systems OOIS'95 1995  111 B.B.Kristensen D.C.M.May Activities Abstractions for Collective Behavior Proceedings of the European Conference on Object-Oriented Programming ECOOP'96 1996  B.B.Kristensen K.0sterbye Roles Conceptual Abstraction Theory  Practical Language Issues Special Issue of Theory and Practice of Object Systems \(TAPOS on Subjectivity in Object-Oriented Systems, 1996  131 B.B.Kristensen Subjective Behavior International loumal of Computer Systems Science and Engineering Volume 16 Number 1 13-24\January 2001 14 B.B.Kristensen Associative Modeling and Programming Proceedings of the 8th International Conference on Object-Oriented Information Systems 00Is'2002 2002  S.Matsuoka A.Yonezawa Analysis of Inheritance Anomaly in Object-Oriented Concurrent Languages In G Agha P Wegner and A Yonezawa editors, Research Directions in Object-Based Concurrency MIT Press 1993  D.C-M.May B.B.Kristensen P.Nowack Tango Modelig In Style Proceedings of the Second International Conference on Generative Systems in the Electronic Arts Second Iteratio-Emergence Melboume, Australia 2001  J.Rumbaugh Relations as Semantic Constructs in an Object-Oriented Language Proceedings of the Object Oriented Systems Languages and Applications Conference OOPSLA'87 1987 IS J.Rnmbaugh M.Blaba W.Premerlani F.Eddy W.Lorensen Object-Oriented Modeling and Design Prentice Hall 1991 19 Mark Weiser The Computer for the 21st Centn www.ubiq.com/hypertexffweiser/SciAmDraft3 4113 


two rooms being occupied at once, although compared to data outside the range of these graphics, the activity level is significantly higher between the relevant entrance events. The system thus reports visiting periods with relatively low confidences to reflect the level of uncertainty Several parameters within the visitor detection process can be changed, and by comparing data from further actual visits to detected visits, optimal values for each of our parameters can be reached So far however, too few actual visits have been identified with which to confidently determine adjustments that would increase accuracy in the majority of cases Aside from the activities already discussed, the recently extended sensor network has also provided the opportunity to infer further activities relating to food preparation and piped water usage, plus watching television and using the phone. However detailed validation information has only very recently become available, and the time-consuming examination and comparisons with sensor reported and inferred results continues along with development of their inference methods. However current observations indicate that phone usage is detected very accurately and television viewing is largely inferred correctly, although as these sensors record interaction with the devices, this is as expected Example Television Watching The major assumption made about watching television is that the client would be seemingly inactive for several minutes at a time, hence this is basically an adaptation from the inactivity detection Known Start Time Known End Time Inferred Start Time Inferred End Time 20/08/05, 17:15:33 17:23:40 20/08/05, 17:15:19 18:26:14 20/08/05, 17:24:58 18:26:18 20/08/05, 18:37:05 00:09:45  N/A N/A 20/08/05, 21:01:30 21:07:52  20/08/05, 21:01:45 21:08:42 20/08/05, 21:19:09 21:32:14  20/08/05, 21:14:20 21:32:03 20/08/05, 22:05:35 22:36:20  20/08/05, 22:08:12 22:33:19 The first watching period has had two separate periods inferred, as between these periods the client is very active presumably cleaning up after finishing the snack eaten during the first period The second short viewing period was not detected because the client was constantly active after a phone call and then left the room. The third period was detected very accurately, and the fourth period is only ìinaccurateî in starting time because the client was reading a newspaper for five minutes whilst having the television on. The fifth inferred period is a slightly smaller than the actual period due to the high activity whilst the client settles down and later begins to leave the room Note that errors with determining television viewing occur because we have no knowledge as to whether or not the television is actually on nor can we tell if the client is actually looking at the television or just has it on in the background. We do have several methods for easily increasing the accuracy of these specific results, but more validation data needs to be examined before we can be justified in applying these methods to all the data CONCLUSIONS Telecare systems aim to carry out intelligent analyses of a personís wellbeing using data about their daily activities. This is a very challenging task for a number of reasons. First of all, the sensors were designed to be completely non-intrusive which excludes the use of any form of tagging for personal identification. This means that the decision reached by any analysis algorithm will always contain a degree of ambiguity in terms of the identity of the person performing an activity, and in terms of the number of people in the home at any one period of time The second issue is related to the fact that people tend to change their behaviour and perform unpredictable actions that cannot be learnt Combined with the type of sensors that can be used this means that the ability of the data analysis engine to make decisions has a significant error rate. For example, consider the bed sensors, which cannot distinguish between a person lying in bed from a heavy object placed on the bed. The fact is that no matter how many sensors are installed, the decision making process will always be associated with some uncertainty. This is why all the data is processed using the techniques of fuzzy systems 21 


which exploit the tolerance for imprecision and uncertainty The third factor is related to evidence collection for the purpose of learning and validation of the analysis results. In this project, simple diary style records are collected by occa sionally telephoning the client about their day over a period of several weeks, supplemented by a week of video footage where clients are filmed doing non-private activities in limited areas of their home. The problem with the diary records is that they introduce an element of intrusion that can become a nuisance for the clients after a period of time and leads to them not treating it seriously enough to record their activities correctly. As for the video evidence, cameras were not installed in bedrooms or bathrooms for privacy reasons and thus they provide an incomplete picture of the activities In spite of limited sensor information, we have shown that it is possible to answer the majority of questions regarding a personís behaviour and wellbeing. Periods of sleep and where the client leaves the home are easily monitored, allowing for effective normality modelling and long-term trend analyses of these periods Although many other important events such as preparing meals, preparing to go out, and hosting visitors have proved more difficult to accurately infer, the measures produced are still worthy of tracking over time. If for instance there is a decrease in the amount and confidence of visits detected, it is likely that visitor numbers have indeed dropped, and the rough data has thus served its desired purpose. Eliminating all uncertainties is clearly impossible with the sensors employed in this study, and it is likely that no matter what sensors are deployed in a fully non-invasive system, this problem will remain to a certain degree. The challenge therefore remains of how to extract the maximum amount of knowledge using a system that imposes a minimum level of intrusion REFERENCES 1 Website http://www.americantelecare.com 2 S. P. Nelwan et al, 2002, ìUbiquitous mobile access to real-time patient monitoring dataî. Computers in Cardiology vol. 29, pp. 557-560 Sept. 2002 3 K. Z. Haigh and H. A. Yanco, 2002 Automation as caregiver: A survey of issues and technologiesî. AAAI 02 Workshop ìAutomation as Caregiverî , pp 39-53 July 2002 4 P. Garner et al, 1997, ìThe application of telepresence in medicineî. Journal of Telemedicine and Telecare, vol. 15, No. 4 pp.181-187, Oct. 1997 5 N. Barnes et al, 1998, ìLifestyle Monitoring technology for supported independence Computing & Control Engineering Journal vol.9, No.4, pp. 169-174, Aug. 1998 6 D. Nauck and B. Majeed, 2004, ìAutomatic Intelligent Data Analysis in Sensor Networks for iSpacesî, BT Technology Journal, vol. 22, No.3, July 2004, pp 216224 7 D. Rose, B. Egan and P. Yung, 2003 Modelling of PIR data from a Telecare trialî. BT Technology Journal, vol. 21, No 2, April 2003, pp 101-111 8 S. Brown, N. Hine, A Sixsmith and P Garner, 2004, ìCare in the Communityî, BT Technology Journal, vol. 22, No.3, July 2004, pp 56-64 9 I. Neild, D. Heatley, R. Kalawsky and P Bowman, 2004, ìSensor Networks for Continuous Health Monitoringî, BT Technology Journal, vol. 22, No.3, July 2004, pp130-139 10 M. Berthold and D. Hand, 1999, ìIntelligent Data Analysisî, Springer, 1999 11 L. Zadeh, 1994, ìSoft computing and fuzzy logicî, IEEE Software, vol. 11, No. 6 November 1994, pp 48-56 12 R. Kruse, J. Gebhardt and F. Klawonn F 1994, ìFoundations of Fuzzy Systems Wiley, Chichester,1994 22 


1 client using UIR 100 clients using UIR 200 clients using UIR Figure 5. The percentage of reduced uplink requests scheme and the UIR scheme. Generally speaking, using our CMIP scheme, the percentage of reduced uplink requests increases as the cache size increases. After the cache size reaches 120, the percentage no longer changes. This can be explained as follows. As the cache size increases, more and more important data items having high access probability can be stored in the cache. When the cache size is still not large enough, the cache hit ratio will increase sharply when cache size increases. As a result, the rate of reduced uplink requests is higher than the arrival rate of new requests Thus, the percentage of reduced uplink requests has a trend of increasing. When the cache size is big enough to hold all the items within our prefetch sets, the rate of reduced uplink requests is no long signi?cant to the arrival rate of new requests. Hence, the percentage of reduced uplink requests no longer changes after reaching a certain cache size From Figure 5, we also notice that although the trend of the percentage of reduced uplink requests is increasing there are some ups and downs. For example, when the number of clients is 200, the percentage of reduced uplink requests reaches the peak when the cache size is about 50 As the cache size continues increasing, the percentage begins to decrease a little bit, and then it increases again. This can be explained as follows. As the cache size increases more queries can be served within the cache. So, the number of reduced uplink requests keeps increasing. Depending on the access patterns of the clients, the number of reduced uplink requests may increase at a rate higher or lower than the increase of the number of requests. If the rate is higher, the percentage of reduced uplink requests will increase. If the rate is lower, the percentage will decrease instead. Thus, there are some ups and downs in the percentage of reduced uplink requests, although the trend is increasing Figure 5 also shows that our CMIP scheme outperforms the UIR scheme with various cache sizes and various number of mobile clients. The percentage of reduced uplinks using the CMIP scheme is twice as much as that of usProceedings of the 2004 IEEE International Conference on Mobile Data Management \(MDM  04 0-7695-2070-7/04 $20.00  2004 IEEE ing the UIR scheme. In term of percentage of reduced uplink request, even the worst case of the CMIP scheme is better than the best case of the UIR scheme. For instance the CMIP scheme has the worse performance when there is one mobile client and the percentage of reduced uplinks is about 6% \(7.5% at best achieves its best performance when there are 200 mobile clients with about 3% of reduced uplinks. This is because the CMIP scheme can better predict the future access of the clients and prefetch them in advance than the UIR scheme Hence, the CMIP scheme can reduce the uplink requests at a percentage much higher than the UIR scheme 3.2.3. The Percentage of Additional Traf?c. The percentage of additional traf?c is de?ned as the ratio of the number of prefetches from the broadcast channel to the number of requests. As we know, downloading a data item from the channel also consumes a lot of system resources such as the bandwidth and the power. So the prefetch scheme should not do aggressive prefetching; otherwise it will consume too much system resources. This is especially important in mobile environments, where the system resource is very limited One argument about this metric is that the percentage of additional traf?c depends on the number of requests that are quite application-dependent and it does not make much sense to compare on this metric. We argue that this met 


sense to compare on this metric. We argue that this metric, when combined with other metric, such as the cache hit ratio, can well describe a prefetch scheme  s ef?ciency and predictability. A good prefetch scheme should be able to improve the cache hit ratio without incurring too much additional traf?c \(or prefetches pares the two schemes in term of the percentage of additional traf?c Figure 6\(a more additional traf?c to the system. For example, the percentage of additional traf?c for UIR scheme is up to 20 when there are 200 clients. This is because the UIR scheme is an aggressive prefetch scheme. Whenever a data item within the cache has been updated and is broadcasted on the channel, the client will download it and update the cache But for the CMIP scheme, the percentage of additional traf?c to the system is negligible, as shown in Figure 6\(b example, the percentage is lower than 0.5% when cache size becomes larger than 100. Why the percentage of additional traf?c is so small is due to the characteristic of our CMIP scheme. Using the CMIP scheme, only those data items which are within our prefetch sets are prefetched The data items within prefetch sets are got from association rules with a high con?dence and support. So the set of data items to be prefetch is small and the number of prefetches is also small. This explains why the percentage of additional traf?c is negligible Figure 6\(c percentage of additional traf?c when there are 200 mobile clients. From 6\(c curs only a fraction of 20 of the percentage of additional traf?c incurred by the UIR scheme. By far, we can say that our CMIP scheme is much better than the UIR scheme and the NOPRE scheme in terms of increased cache hit ratio reduced uplink requests and negligible additional traf?c 4. Related Work In the literature, prefetch technique is widely employed to reduce the access latency in WWW environments [17 16, 8, 12, 9]. [17] presents a predictive prefetching scheme for the World Wide Web in which the server tells the clients which ?les are likely to be requested by the user, and the clients decide whether to prefetch these ?les or not based on local considerations \(such as the contents of the local cache  posed. This scheme predicts the ?les  future access probabilities based on the access history and the network condition. The scheme allows the prefetching of a ?le only if the access probability of the ?le is greater than a function of the system bandwidth, delay and retrieval time. In [9], Cohen and Kaplan investigate three other types of prefetching in web: pre-resolving host-names \(pre-performing DNS lookup prefetching TCP connections prior to issuance of HTTP request sending a  dummy  HTTP HEAD request to Web servers  velops a new method for prefetching Web pages into the client cache. Clients send reference information to the Web server, which aggregates the reference information in nearreal-time and then disperses the aggregated information to all clients, piggybacked on GET responses. The information indicates how often hyperlink URLs embedded in pages have been previously accessed relative to the embedding page. Based on the knowledge about which hyperlinks are generally popular, clients initiate prefetching of the hyperlinks and their embedded images according to any algorithm they prefer Most of these work were not designed for mobile environments and did not consider the constraints of mobile environments. Recently, several prefetch schemes have been proposed as a client-side technique to reduce the access latency in mobile environments [1, 13, 6, 19]. In [1], a simple prefetching heuristic, called PT, computes the value of 


ple prefetching heuristic, called PT, computes the value of a page by taking the product of the probability \(P cessing of the page with the time \(T fore that page appears on the broadcast again. PT ?nds the page in the cache with the lowest pt value and replaces it with the current broadcast page if the latter has a higher pt value. However, this time-based prefetch scheme is expensive to implement since it computes the pt for each item in Proceedings of the 2004 IEEE International Conference on Mobile Data Management \(MDM  04 0-7695-2070-7/04 $20.00  2004 IEEE 0 5 10 15 20 0  50  100  150  200  250  300 Th e pe rc en ta ge o f a dd iti on al tr af fic Cache size 1 client using UIR 100 clients using UIR 200 clients using UIR 0 0.5 1 1.5 2 2.5 3 3.5 4 0  50  100  150  200  250  300 Th e pe rc en ta ge o f a dd iti on al tr af fic Cache size 1 client using CMIP 100 clients using CMIP 200 clients using CMIP a b 0 5 10 15 


20 0  50  100  150  200  250  300 Pe rc en ta ge o f a dd iti on al tr af fic Cache size 200 clients using CMIP 200 clients using UIR c Figure 6. The percentage of additional traf?c the cache at every clock tick. A similar scheme has been proposed in [13], which uses fv, a function of the access rate of the data item only, to evaluate the value of each data item i that becomes available to the client on the channel If there exists a data item j in the client  s cache such that fv\(i j replaced with i A prefetch scheme based on the cache locality, called UIR scheme, was proposed in [7]. It assumes that a client has a large chance to access the invalidated cache items in the near future. It proposes to prefetch these data items if it is possible to increase the cache hit ratio. In [6], Cao improves the UIR scheme by reducing some unnecessary prefetches based on the prefetch access ratio \(PAR scheme, the client records how many times a cached data item has been accessed and prefetched, respectively. It then calculates the PAR, which is the number of prefetches divided by the number of accesses, for each data item. If the PAR is less than one, it means that the data item has been accessed a number of times and hence the prefetching is useful. The clients can mark data items as non-prefetching when PAR &gt; b, where b is a system tuning factor. The scheme proposes to change the value of b dynamically according to power consumption. This can make the prefetch scheme adaptable, but no clear methodology as to how and when b should be changed. Yin et al. [19] proposed a power-aware prefetch scheme, called value-based adaptive prefetch \(VAP the number of prefetches based on the current energy level to prolong the system running time. The VAP scheme de?nes a value function which can optimize the prefetch cost to achieve better performance These existing schemes have ignored the following characteristics of a mobile environment: \(1 query some data items frequently, \(2 during a period of time are related to each other, \(3 miss is not a isolated events; a cache miss is often followed by a series of cache misses, \(4 eral requests in one uplink request consumes little additional bandwidth but reduces the number of future uplink requests. In this paper, we addressed these issues using a cache-miss-initiated prefetch scheme, which is based on association rule mining technique. Association rule mining is a widely used technique in ?nding the relationships among data items. The problem of ?nding association rules among items is clearly de?ned by Agrawal et al. in [5]. However in the mobile environment, one cannot apply the existing association rule mining algorithm [4] directly because it is too complex and expensive to use This makes our algorithm different from that of [4] in 


This makes our algorithm different from that of [4] in twofold. First, we are interested in rules with only one data item in the antecedent and several data items in the consequent. Our motivation is to prefetch several data items which are highly related to the cache-miss data item within Proceedings of the 2004 IEEE International Conference on Mobile Data Management \(MDM  04 0-7695-2070-7/04 $20.00  2004 IEEE the cache-miss initiated uplink request. We want to generate rules where the antecedent is one data item, but the cache-missed data item and the consequent is a series of data items, which are highly related to the antecedent. If we have such rules, we can easily ?nd the data items which should also be piggybacked in the uplink request. Second in mobile environment, the client  s computation and power resources are limited. Thus, the rule-mining process should not be too complex and resource expensive. It should not take a long time to mine the rules. It should not have high computation overhead. However, most of the association rule mining algorithms [4, 5] have high computation requirements to generate such rules 5. Conclusions Client-side prefetching technique can be used to improve system performance in mobile environments. However, prefetching also consumes a large amount of system resources such as computation power and energy. Thus, it is very important to only prefetch the right data. In this paper, we proposed a cache-miss-initiated prefetch \(CMIP scheme to help the mobile clients prefetch the right data The CMIP scheme relies on two prefetch sets: the alwaysprefetch set and the miss-prefetch set. Novel association rule based algorithms were proposed to construct these prefetch sets. When a cache miss happens, instead of sending an uplink request to only ask for the cache-missed data item, the client requests several items, which are within the miss-prefetch set, to reduce future cache misses. Detailed experimental results veri?ed that the CMIP scheme can greatly improve the system performance in terms of increased cache hit ratio, reduced uplink requests and negligible additional traf?c References 1] S. Acharya, M. Franklin, and S. Zdonik. Prefetching From a Broadcast Disk. Proc. Int  l Conf. on Data Eng., pages 276  285, Feb. 1996 2] S. Acharya, M. Franklin, and S. Zdonik. Balancing Push and Pull for Data Broadcast. Proc. ACM SIGMOD, pages 183  194, May 1997 3] S. Acharya, R. Alonso, M. Franklin, and S. Zdonik. Broadcast disks: Data Management for Asymmetric Communication Environments. Proc. ACM SIGMOD, pages 199  210 May 1995 4] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In J. B. Bocca, M. Jarke, and C. Zaniolo editors, Proc. 20th Int. Conf. Very Large Data Bases, VLDB pages 487  499. Morgan Kaufmann, 12  15 1994 5] R. Agrawal, Tomasz Imielinski, and Arun Swami. Mining Association Rules Between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207  216, Washington, D.C May 1993 6] G. Cao. Proactive Power-Aware Cache Management for Mobile Computing Systems. IEEE Transactions on Computers, 51\(6  621, June 2002 7] G. Cao. A Scalable Low-Latency Cache Invalidation Strategy for Mobile Environments. IEEE Transactions on Knowledge and Data Engineering, 15\(5 ber/October 2003 \(A preliminary version appeared in ACM MobiCom  00 8] K. Chinen and S. Yamaguchi. An Interactive Prefetching Proxy Server for Improvement of WWW Latency. In Proc INET 97, June 1997 9] E. Cohen and H. Kaplan. Prefetching the means for docu 


9] E. Cohen and H. Kaplan. Prefetching the means for document transfer: A new approach for reducing web latency. In Proceedings of IEEE INFOCOM, pages 854  863, 2000 10] R. Cooley, B. Mobasher, and J. Srivastava. Data preparation for mining world wide web browsing patterns. Knowledge and Information Systems, 1\(1  32, 1999 11] C. R. Cunha, Azer Bestavros, and Mark E. Crovella. Characteristics of WWW Client Based Traces. Technical Report TR-95-010, Boston University, CS Dept, Boston, MA 02215, July 1995 12] D. Duchamp. Prefetching hyperlinks. In USENIX Symposium on Internet Technologies and Systems \(USITS  99 1999 13] V. Grassi. Prefetching Policies for Energy Saving and Latency Reduction in a Wireless Broadcast Data Delivery System. In ACM MSWIM 2000, Boston MA, 2000 14] S. Hameed and N. Vaidya. Ef?cient Algorithms for Scheduling Data Broadcast. ACM/Baltzer Wireless Networks \(WINET  193, May 1999 15] Q. Hu and D. Lee. Cache Algorithms based on Adaptive Invalidation Reports for Mobile Environments. Cluster Computing, pages 39  48, Feb. 1998 16] Z. Jiang and L. Kleinrock. An Adaptive Network Prefetch Scheme. IEEE Journal on Selected Areas in Communications, 16\(3  11, April 1998 17] V. Padmanabhan and J. Mogul. Using Predictive Prefetching to Improve World Wide Web Latency. Computer Communication Review, pages 22  36, July 1996 18] N. Vaidya and S. Hameed. Scheduling Data Broadcast in Asymmetric Communication Environments. ACM/Baltzer Wireless Networks \(WINET  182, May 1999 19] L. Yin, G. Cao, C. Das, and A. Ashraf. Power-Aware Prefetch in Mobile Environments. IEEE International Conference on Distributed Computing Systems \(ICDCS 2002 Proceedings of the 2004 IEEE International Conference on Mobile Data Management \(MDM  04 0-7695-2070-7/04 $20.00  2004 IEEE pre></body></html 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207ñ216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intíl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intíl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





