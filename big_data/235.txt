An Efficient Clustering Algorithm for Mining Fuzzy Quantitative Association Rules Been-Chian Chien Zin-Long Lin Tzung-Pei Hong Institute of Information Engineering I-Shou University I-Shou University I-Shou University Kaohsiung, Taiwan R 0 C cbc@isu.edu.tw zlong@kids.csie.isu.edu.tw tphong@isu.edu.tw Dept of Information Engineering Kaohsiung, Taiwan R 0 C Dept of Information Management Kaohsiung Taiwan R 0 C Abstract Mining association rules on categorical data has been discussed widely of late years It is a relatively difficult problem in discovery of association rules from numerical data since the reasonable intervals for 
unknown numerical attributes or quantitative data may not be discriminated easily In this paper we propose an efficient hierarchical clustering algorithm based on variation of density to solve the problem of interval partition We define two main characteristics of clustering numerical data relative inter-connectivity and relative closeness By giving a proper parameter a to determine the importance between relative closeness and relative inter-connectivity the proposed approach can generate a reasonable interval automatically for the user The experimental results show that the proposed clustering algorithm can behave a good performance on both of clustering results and speed 1 
Introduction Data mining is a key step on the processes of knowledge discovery. One of the most important topics of data mining is to find association rules from a transaction database efficiently  11 The mining of itemized association rules also referred to be Boolean association rules has been studied extensively of late years A general association rule can be view as being defined on attributes of a relation and has the expression of form XsY where X and 
Y are conjunctions of a set of conditions C The condition C for a categorical attribute is A  v where v is the item value of the attribute A An association rule holds when the rule satisfies user-specified minimum support and confidence The support of an association rule X5Y is defined as the ratio of the number of tuples satisfying the condition XAY and the total number of tupes in the database. The confidence is 
the ratio of the number of tuples satisfying XAY and the number of tuples satisfying X While an attribute is numerical type the condition C for a quantitative attribute is denoted as v E f,k u,J where Ck u,k is an interval of numerical attribute A and v is a value in the domain of attribute A The mining process in quantitative attributes is more difficult than in categorical attributes relatively because intervals involved in quantitative 
association rules may not be concise and meaningful enough for users In order to discover association rules with quantitative attributes, an efficient method to find meaningful intervals is necessary Many researchers propose different approaches to solve the problem of interval partition Srikant er al  151 use equi-depth partitioning to mine quantitative rules which separates intervals by their relative ordering and quantities equally. However they do not consider the relative distance between values and density of an interval Miller and Yang I31 apply Birch clustering  181 to identify intervals and proposed a 
distance-based association rule to improve the semantics of intervals At the same time ARCS proposed by Lent er al present a geometric-based algorithm BitOP for performing the clustering in numerical attributes They show that clustering is a possible solution to figure out the meaningful regions and support the discovery of association rules Although clustering provides a useful technique to discriminate dense space it is not feasible for all clustering algorithms Cheng et al 4 suppose that most of the clustering algorithms such as DBSCAN SI BIRCH 
18 CURE 6 and STING 16 do not satisfy the requirement of identifjing clusters embedded in subspace of multi-attributes Thus they develop an entropy-based subspace clustering method called ENCLUS to handle the mining of subspace in numerical attributes Another possible solution is fuzzy set theory In contrast with quantitative clustering fuzzy linguistic based approaches focus on the qualitative filtering Yager  181 introduces hzzy linguistic summaries to 0-7803-7078-3/0v$10.00 C IEEE Page 1306 


provide summarization on different attributes Hirota and Pedrycz 7 7 I41 propose a context-sensitive fuzzy clustering method based on Fuzzy C-means to construct rule-based models However the context sensitive FCM method cannot deal with the data consisting of both numerical and categorical attributes For solving the qualitative knowledge discovery Au and Chan 2 employ fuzzy linguistic terms to relational databases with numerical and categorical attributes, then proposed the F-APACS method 3 to discovery fuzzy association rules. Hong et al 9 give definitions of the support and confidence for fuzzy membership grade and design a data mining approach based on fuzzy sets to find association rules with linguistic terms of human knowledge However, the specified fuzzy linguistic terms in fuzzy association rules can be given only when we have understood the properties of attributes In real life contents of attributes may be unknown and meaningful intervals are not concise and crisp enough usually In this paper we propose an automatic clustering method for discovering hzzy quantitative association rules The proposed clustering method is based on the variation of densities For two neighbor clusters CI and C we merge them into a single cluster C\222 if the density of C\222 is close to the densities of original cluster C and C2 A discriminate hnction and a hierarchical algorithm are developed to process all clusters recursively. At last we use a determined function to get the best clusters. For one dimensional quantitative attribute a cluster indicates an interval The membership grade of fuzzy interval then can be obtained by the approach of fuzzy C-mean and fuzzy quantitative association rules can be generated We also show an experimental result with about 8,000 numerical data The clustering results is close to the visual partitions of human and behave an efficient performance in speed This paper is organized as follows In Section 2 the definitions of fuzzy quantitative association rules are given. Then, we introduce the idea and have a formal description for the proposed clustering method in Section 3 The experimental results of the proposed clustering method with different parameters are shown in Section 4 Finally Section 5 concludes a summary and gives some directions of future research 2 Fuzzy Quantitative Association Rules We first give formal definitions for fuzzy quantitative association rules. Let R  A A   A be a set of attributes in a relational database, where IR  m is the number of attributes in R Let t[R stand for the tuples in R and lt[R]I  n be the number of tuples in the database R Assume that A is an attribute with quantitative domain D I i 5 m Let 12  Ilk u,kl be the kth interval of the attribute A where Ilk E DA u,k E DAr and k I q An item value of attribute A is denoted as t[A Let C denote the condition t[A,]e I  C and Cy be the conjunctions of conditions CA I i I m The sets ofXand Y represent the sets of attributes in the conjunctions of conditions CX and Cy respectively The number of tuples in the database satisfying the conditions C and Cy are respectively denoted as IC 1 and Cy I We define a quantitative association rule over a set of crisp intervals as follows Definition 1  151 A quantitative association rule is an implication of the form Cxa Cy where XcR YcR andXn Y 0 A rule C 3 Cy holds with confidence c and support s if ICx~Cy  lCxf 2 c and lCx~CyI  n 2 s In Definition 1 the conditions in association rules are based on crisp interval data For unknown numerical values intervals may not be clear enough in general Therefore, we define the concept of fuzzy intervals as follows Assume that a set of fuzzy intervals sets denoted as A AI2    AIK is given for the attribute A The membership function is pA#k for the kth fuzzy interval in attribute A such that pA,k DA  0,1 The fuzzy interval AIk k  1,2  K are defined as If pA~~k\(t[A the item r[A belongs to the fuzzy interval AIk certainly If t\(f[A it is no doubt that the item r[A are not in the fuzzy interval A,k Let I and Iy be the conjunctions of fuzzy intervals in the attributes sets of X and Y Based on hzzy interval we define fuzzy quantitative association rules over a set of fuzzy intervals as follows Definition 2 A hzzy quantitative association rule is an implication of form 1 ly where XcR YcR and XnY=0 A rule I 3 Iy holds with confidence c and supports if 0-7803-7078-3/0U$l0~00 C IEEE Page 1307 


3 The Clustering Method In the section we present the proposed automatic clustering method formally The symbols used in the algorithm are defined in Section 3.1 The clustering algorithm is described in Section 3.2 We also give a membership grade generating algorithm based on the clustering results and fuzzy C-mean to generate the membership functions of clusters rapidly 3.1 Notations Since we only consider the algorithm on a single attribute here each interval on an attribute can be viewed as a cluster. The symbols used in our proposed method are defined in the following n  the total number of tuples in original database r  the initial number of intervals in an attribute N  the number of intervals C  the ith interval of an attribute h  the number of data contained in the ith intervals 1  the length of the ith intervals  the centroid of ith interval 4 the difference between the density of interval C and the density of the interval which is merged by two neighbor intervals C and C,+I a  the distance of the centroids in interval C moved after the merging of two neighbor intervals C and C I RI  Relative Inter-Connectivity The degree of relative inter-connectivity between two neighbor intervals C and Cf+l RC Relative Closeness The degree of relative closeness between two neighbor intervals C and Clcl 5 El the cost function for evaluating if C and C,+I should be merged a  0 L E  RC x RI 6 di the set of all N intervals after k merging processes P 7 3.2 the connectivity variance of d the h is the number of data in interval C after k merging processes 7 the closeness variance of  the hr is the number of data in initial interval C The Algorithm After the definitions of all notations we make a preprocess in the first step for the original numerical data in the algorithm The preprocessing is to quantize the original data using an acceptable minimum interval The minimum interval can be set by users Once upon the minimum interval was determined we can partition the domain of the attribute into r intervals initially Then we compute the cost hnctions for each two neighbor intervals and merge the intervals with minimum E Of course, the number of intervals will be decreasing and the length for merged intervals will be increasing after each merging process Next we just 0-7803-7078-3/0V$10.00 C IEEE Page 1308 


only re-compute the cost function of the merged intervals in the previous merging process and merge the merge the intervals with minimum E again Such merging processes will be repeated until only one interval remains When the merging processing stops we find the value k with min p   The set of d is the best partition The detailed algorithm is shown in the following Input one dimension data set t[A rand a Output the interval set d Step 1 Quantize original data set t[A and compute h Nck=r k0 b CO CI C2    C,,,'-I Step 2 Compute E Os i 5 NCk  I Step 3 Compute and vk Step 4 Find C and C,+I with min\(E,lO I I NCk-1 merge C and Cl Nlh"=N,k 1 Step 5 Re-compute E,-l and &.re-numbering uk to be I Step 6 If Nlk  1 goto Step 7 Otherwise k=k+l goto Step 3 Step 7 Find min\(e output uk The parameters cand 4 are used to determine the best interval partitions. The parameter f is used to estimate the inter-connectivity among intervals The parameter qk is used to estimate the closeness within a interval While the summation of the two parameters has the smallest value we claim that there is the best interval partition at this time Users usually need to give the number of partitions in the traditional cluster method It is difficult for users to assign the number of clusters when the characteristics of data are unknown In our method the best number of clusters can be determined automatically 3.3 Membership Functions Generating The algorithm proposed in Section 3.1 and 3.2 also supports improvement of rapid membership fimction generating Here we propose a efficient algorithm for membership function generation based on fuzzy C mean Instead of using the initial random data to be the centroids of the clusters we use the centroids obtained from our proposed algorithm. The procedure of membership function generating is described as follows Step I  Initializing the C points by y the centroids of the intervals in set uk Step 2 Compute p,\(q the membership grades of Go belonging to C,k 0  j  r 1 0 5 i  N,.k 1  Step 3 Compute Cost if Cosr is smaller than a user specified value E then the algorithm halts otherwise, goes to Step 4 Step 4 Compute y and go to Step 2 We have successfully partitioned a numerical attribute into the best intervals and produced their membership values By means of the discovered intervals and the fuzzy membership generating function, we can depict the fuzzy intervals as defined in Section 2 The fuzzy quantitative association rules in a database thus are able to be discovered through the given support s and confidence c The detailed mining algorithm is similar to the traditional itemized data mining except the definitions of support and confidence Of course there exists another improvement to find the rules but it is out of the issue we discussed in this paper 4 Experimental Results We implement the proposed algorithm and make some experiments Only one data set is shown here due to the limitation of paper length The number of data is about 8,000 The domain of each data is an integer between 0 and 64000 We first quantize all data into r initial intervals The different r had done by setting to be 128, 256 512 and 1024. Here, only the results of r  256 are presented The histogram of original data after quantization is shown in Figure 1 The different a values produce distinct clustering results as shown in Figure 2 to Figure 5 For our visual judgement there is the best result when a  2.5 But for the data set with different distribution, the value of a will not have the same value for obtaining the best result 5 Conclusions The data mining process from numerical attributes is more difficult for analysts than itemized data in general 0-7803-7078-3/0U$l0.00 C IEEE Page 1309 


The main obstacle is the lack of meaningful property in data and the vagueness of intervals The contribution of this paper is to proposed an automatic interval partition method and an efficient fuzzy membership function generator By the proposed method we can find the fizzy quantitative association rules effectively Our future work is to extended the proposed method to the clustering of multi-dimensional features. Also, we try to find a more efficient mining algorithm for fuzzy quantitative association rules  References R Agrawal T lmielinski and A Swami Mining Association Rules between Sets of Items in Large Databases in Proc of the ACM SIGMOD Int'l Con on Management of Data Washington D C May 1993 pp 207-216 W H Au and K C C Chan An Effective Algorithm for Discovering Fuzzy Rules in Relational Databases in Proc the IEEE Int'l Con on Furzy Systems 1998 pp 1314-1319 K.C.C Chan and W.H Au Mining Fuzzy Association Rules in Proc of the 6th ACM Int I Con on Information and Knowledge Management Las Vegas Nevada Nov 1997 pp C H Cheng A W Fu, and Y Zhang Entropy based Subspace Clustering for Mining Numerical Data in Proc of Int'l Conf on Knowledge Discovery and Data Mining San Diego CA M Ester H Kriegel J Sander and X Xu A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise in Proc of the 2nd Int'l Conf on Knowledge Discovery in Databases Menlo Park CA 1996 S Guha R Rastogi and K Shim CURE An Eflicient Clustering Algorithm for Large Databases in Proc of the ACM SIGMOD Int'l Con on Management of Data Seattle WA K Hirota and W Pedrycz Linguistic Data Mining and Fuzzy Modeling lEEE International conference on Fuzzy systems Vol K Hirota and W Pedrycz Fuzzy Computing for Data Mining Proceedings of the IEEE Vol 87 T P Hong C S Kuo and S C Chi A Data Mining Algorithm for Transaction Data with 209-2 15 USA 1999 pp 84-93 pp 226-23 1 1998 pp.73-84 2 1996 pp 1488- 1496 NO 9 pp 1575-1600, 1999 Quantitative Values Intelfgent Data Analysis Vol 3 No 5 pp 363-376 1999 IO G Karypis E H Han and V Kumar Chameleon Hierarchical Clustering Using Dynamic Modeling IEEE Computer Vol 32  1 I L H Lee and L K Hyung An Extension of Association Rules Using Fuzzy Sets in Proc of the Int'l Con IFSA  1991 I21 B Lent A Swami and J Widom Clustering Association Rules in Proc of IEEE International Conference on Data Engineering I31 R J Miller and Y Yang Association Rules over Interval Data in Proc of the ACM SIGMOD Int'l Con on Management of Data 14 W Pedrycz Fuzzy Set Technology in Knowledge Discovery Fuzzy Sets and Systems 151 R Srikant, and R Agrawal Mining Quantitative Association Rules in Large Relational Tables in Proc of the ACM SIGMOD Int'I Con on Management of Data Montreal Canada 1996 161 Wei Wang Jiong Yang and Richard Munts STING A Statistical Information Grid Approach to Spatial Data Mining in Proc of the 23rd con on Very Large Data Bases Athens Greece 1997 pp 186 195 I71 X Xu M Ester, H. P. Kriegel and J Sander A Distribution-Based Clustering Algorithm for Mining in Large Spatial Databases in Proc of the IEEE Int'l ConJ on Data Engineering 1998 I81 R R Yager Fuzzy Summaries in Database Mining in Proc of the 11th Con on Artijicial Intelligence for Application Los Angeles CA I91 T Zhang R Ramakrishnan and M Livny BIRCH An Efficient Data Clustering Method for Very Large Databases in Proc of the ACM SIGMOD Int'l Cont on Management of Data Montreal, Canada 1996 pp 103-1 14 NO 8 pp 68-75 1999 1997 pp 220-23 1 AZ, USA 1997 pp 452-46 1 98 pp.219-290, 1998 pp 1-12 1995 pp 265-269 Figure 1 Test data r  256 0-7803-7@78-3/0V$l0~00 C JEEJL Page 1310 


Figure 2 Partition rcsult 1 for r  256 CL  0 fVC  49  Figure 3 Partition result 2 for r  256 U 0.5 Art  25 Figure 4 Partition result 3 for r  256 a  1.5 N  22   Figure 5 Partition result 4 for r  256 a  2.5 ff  18 0-7803-7078-3/0U$l0.00 C IEEE Page 1311 


Trace Figure 5 Accuracy 1 2 3 4 Accuracy Excess Hypotheses I  I ID3 I Ibhys ID3 I Ibhys ID3 I Ibhys Time 450 400 250 200 150 100 300 350 t   t  t  Figure 4 Computing time in minutes versus trace length Table in figure 5 is a direct comparison of the respec tive accuracies of IBHYS and ID3 These tests were per formed on a trace of 1000 actions with 1 x I 2 see SUBMIT in section 3 These 1000 actions were split in a training set and a test set The leftmost column lists the size of the training sets used, and column 1 lists the number of training examples repetitive sequences the algorithms have found in the training sets Column 2 shows the predic tive accuracy on new examples It shows that IBHYS had correctly predicted a repetitive sequence in 52.58 of the case versus 46.13 for ID3 Column 3 lists the 223excess rate\224 that is the number of time the algorithms have pre dicted erroneous repetitive sequences whereas no predic tion were expected This excess rate value is very impor tant Hight values means that the agent constantly bothers the user with useless suggestions IBHYS and ID3 have almost the same excess rate Finally column 4 lists the number of hypotheses the algorithms have learnt. Note that both IBHYS and ID3 have an average decision time of 10 milliseconds Due to the fact that the Apprentice and the Assistant have been implemented in Smalltalk 4.0 and that few pro grammers still use this environment we could not find pro grammers to intensively test our agents Of course, we are working to adapt our agents to the newest version of the Smalltalk environment However we can give examples of the habits learnt during the experiments reported here fig ure 6 Habit 1 means that the user systematically moves and resizes a debugger he has opened after an error habit 2 shows that the user systematically removes system com ments of new methods 5 Conclusion We have proposed a new approach, called IBMYS and an incremental algorithm with low computing time, for induc tive concept learning particularly suited for learning inter face agents This approach lets each training example build a set of hypotheses that locally approximate the global tar get function, limiting the hypothesis search to a small por tion of the hypothesis space Because training examples can choose among several description languages to form an hypothesis, and different description languages to form different hypotheses, it allows to handle simultaneously hy potheses described in different languages We presented an application of this approach to learn user\222s habits of inter active programming environments and propose an original assistance to programmers based on two software agents the Apprentice and the Assistant We showed with exper imental results on real data, that IBHYS outperforms ID3 both in computing time and predictive accuracy IBNYS seems a promising approach for data-mining Further studies will be conducted to evaluate our IB HYS approach with respect to standard Irvine collection machine-learning datasets In the context of the Appren tice and the Assistant, an important limitation of IBHYS is that it bounds in advance the length of the description and therefore, the length of the situation patterns searched We are investigating to bypass this limitation We are currently working to adapt the Apprentice and 206 


1 Situation pattern Repetitive sequence ActionErreur\(ni1  ActionMenu\(aDebugger,move ActionMenu\(aDebugger,debug ActionMenu\(aDebugger,resize Figure 6 Example of user\222s habits 2 the Assistant to the newest version of the Smalltalk envi  1 11 P Maes. Agents that reduce work and information overload ActionSelect\(aBrowser message selector and argument names ActionMenu\(aBrowser,cut 223comment stating purpose of message\224 I temporary variable names I statements ronment We hope that they will be soon available to full time programmers for intensive tests Communications of the ACM Special Issue on Intelligent Agents 37\(7 July 1994 1121 P Maes Social interface agents Acquiring comoetence by Acknowledgements We would like to thank Christophe Fiorio for his Algo rithm LaTeX style arld his help in the preparation of the final manuscript of this paper References l R Armstrong D Freitag T Joachims and T Mitchell Webwatcher A learning apprentice for the world wide web In AAAI Spring Symposium on Information Gather ing 1995 2 A Caglayan M Snorrason J Jacoby J Mazzu and R. Jones. Lessons from open sesame!, a user interface leam ing agent In Proceedings of PAAM96 pages 61-74 Apr 1996 3 A Cypher EAGER: Programming repetitive tasks by ex ample In Proceedings of ACM CHI\22291 Programming by Demonstration pages 33-39 1991 4 R 0 Duda and P E Hart Pattern Classification and Scene Analysis John Wiley and Sons New York 1973 5 0 Gascuel and G Caraux Distribution-free performance bounds with the resubstitution error estimate Pattern Recognition Letters 13:757-764 1992 6 J Hertz A Krogh and R G Palmer An Introduction to the Theory of Neural Computation Lecture Notes Volume I Addison Wesley 1991 7 J H. Holland Adaptation in natural artijicial systems Uni versity of Michigan Press Ann Arbor, 1975 8 R C Holte and C Drummond A learning apprentice for browsing In 0 Etzioni, editor Software Agents  Spring Symposium AAA1 Press Mar 1994 9 R M Karp R E Miller, and A L Rosenberg Rapid iden tification of repeated patterns in strings trees and arrays In 4th Annual ACM Symposium on Theory of computing pages 125-136 Denver, Colorado 1-3 May 1972  101 Y Lashkari, M. Metral, and P Maes. Collaborative interface agents In Proceedings ofAAAI\22294 pages 444-449 1994  learning from users and othkr agents In 6 Etzioni, editor Software Agents  Spring Symposium pages 71-78 AAAI Press Mar 1994 13 P Maes and R Kozierok Learning interface agents In Proceedings of the 1 I th National Conference on Artificial Intelligence pages 459-464 Menlo Park CA USA July 1993. AAAI Press I41 T Mitchell R Caruana D Freitag J McDermott and D Zabowski. Experience with a learning personal assistant Communications of the ACM Special Issue on Intelligent Agents 37\(7 July 1994 15 T M Mitchell Version Spaces An Approach to Concept Learning PhD thesis Electrical Engineering Dept Stan ford University, 1979 16 J. Orwant Heterogenous learning in the doppelganger user modeling system User Modeling and User-Adapted Inter action 4\(2 1995 17 T R Payne and P Edwards Interface agents that learn an investigation of leaming issues in a mail agent interface Applied Artificial Intelligence 11 1-32 1997 18 W Pohl Learning about the user  user modeling and ma chine learning In V M J Herrmann, editor ICML\22296 Work shop Machine Learning meets Human-Computer Interac tion pages 29-40 1996 I91 M J D Powell Radial basis functions for multivariable interpolation A review In Algorithms for Approximation pages 143-167 Oxford 1987. Clarendon Press 20 J R. Quinlan Induction of decision trees Machine Learn ing 1\(1 1986 21 C Rich and R C Waters The programmer\222s apprentice Computer pages 11-25 Nov 1988 22 J C Schlimmer and L A Hermens. Software agents: Com pleting patterns and constructing user interfaces Journ of AI Research 1:61-89 Nov 1993 23 R. Waters The programmer\222s apprentice: Knowledge-based program editing IEEE Trans Software Engineering SE 8\(1 Jan 1982 24 S Wu and U Manber Fast text searching allowing errors Communications of the ACM 35\(10 Oct 1992 25 K Yoshida and H Motoda Automated user modeling for intelligent interface Int J of Human Computer Interaction 3\(8 1996 207 


I Plenary Panel Session J Future Directions in Database Research  456 Chair Surajit Chaudhuri Microsoft Corporation Panelists Hector Garcia-Molina Stanford University Hank Korth, Bell Laboratories Guy Lohman IBM Almaden Research Center David Lomet Microsoft Research David Maier Oregon Graduate Institute I Session 14 Query Processing in Spatial Databases I Chair Sharma Chakravarthy University of Florida Processing Incremental Multidimensional Range Queries in a Direct Manipulation Visual Query Environment  458 High Dimensional Similarity Joins Algorithms and Performance Evaluation  466 S Hibino and E Rundensteiner N Koudas and K.C Sevcik Y Theodoridis E Stefanakis and T Sellis Cost Models for Join Queries in Spatial Databases  476 Mining Association Rules Anti-Skew Algorithms  486 J.-L Lin and M.H Dunham Mining for Strong Negative Associations in a Large Database of Customer Transactions  494 A Savasere E Omiecinski and S Navathe Mining Optimized Association Rules with Categorical and Numeric Attributes  503 R Rastogi and K Shim Chair: Anoop Singhal AT&T Laboratories S Venkataraman J.F Naughton and M Livny Remote Load-Sensitive Caching for Multi-Server Database Systems  514 DB-MAN A Distributed Database System Based on Database Migration in ATM Networks  522 T Hara K Harumoto M Tsukamoto and S Nishio S Banerjee and P.K Chrysanthis Network Latency Optimizations in Distributed Database Systems  532 I Session 17 Visualization of Multimedia Data I Chair Tiziana Catarci, Universita di Roma 223La Sapienza\224 W Chang D Murthy A Zhang and T.F Syeda-Mahmood Global Integration of Visual Databases  542 X 


The Alps at Your Fingertips Virtual Reality and Geoinformation Systeps  550 R Pajarola l Ohler P Stucki K Szabo and P Widmayer C Baral G. Gonzalez and T.C Son Design and Implementation of Display Specifications for Multimedia Answers  558 1 Session 18 Management of Objects I Chair: Arbee Chen National Tsing Hua University P Boncz A.N Wilschut, and M.L. Kersten C Zou B Salzberg, and R Ladin 0 Wolfson S Chamberlain S Dao L Jiang, and G. Mendei Flattening an Object Algebra to Provide Performance  568 Back to the Future Dynamic Hierarchical Clustering  578 Cost and Imprecision in Modeling the Position of Moving Objects  588 ROL A Prototype for Deductive and Object-Oriented Databases  598 A Graphical Editor for the Conceptual Design of Business Rules  599 The Active HYpermedia Delivery System AHYDS using the M Liu W Yu M Guo and R Shan P Lang W Obermair W Kraus and T Thalhammer PHASME Application-Oriented DBMS  600 F Andres and K. Ono S Chakravarthy and R Le S Mudumbai K Shah A Sheth K Parasuraman and C Bertram ECA Rule Support for Distributed Heterogeneous Environments  601 ZEBRA Image Access System  602 Author Index  603 xi 


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


