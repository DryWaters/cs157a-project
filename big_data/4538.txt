Numeric Missing Value’s Hot Deck Imputation Based on Cloud Model and Association Rules  Wang Zhao-hong Department of Computer Science and Technology Weifang University Weifang, China wangzhhwfxy@163.com   Abstract Filling missing value is main task of data-processing, at present Hot Deck Imputation is preferred. Defining the similar standard of Hot Deck Imputation objectively becomes an important prerequisite. The Cloud model combines ambiguity and randomness organically to fit the real world data objectively first get the cloud models which present the raw no missing value then to discrete the numeric value and do the association rules mining in the discrete value to get the knowledge base, filling the missing value with the value which generated by the cloud model from the knowledge base. The meth od considered the original data’s distribution as a whole and to improve its precision with association rules from the raw data for each record, it simulates the humans’ behavior; this method has smaller absolute mean difference than other methods Keywords- Missing value; Cloud model; Association rules; Hot Deck Imputation I  I NTRODUCTION  Data in the database that contains noise, the missing data and ambiguity will affect the validity of data mining knowledge discovery model should have a certain degree of fault tolerance, it is urgent to find a mechanism to handle missing value efficiently and accurately. Dealing with incomplete data sets methods can be divided into the following three categories: remove the tuples, data filling, and do the data mining with the missing data A Delete the tuples Delete the tuples with missing attribute values to get the information completeness by re ducing the historical data amount, discarded a lot of hidden in formation in these objects When missing data is a larger proportion of the data particularly there is non-ran dom distribution of missing data this approach may lead to deviation, which leads to wrong   B Data Filling This method is used to fill the missing values with a certain value. Usually based on statistical theory, according to the decision table the remaining values’ distribution to fill a missing value, there are several commonly used filling method  3 in data m i ning 1 filling manually The user fill the missing data, this method is timeconsuming. It is not feasible with large-scale data or a huge number of missing values 2 Treating Missing Attribute values as Special values Deal with the missing value as a speci indicate that the method may lead to serious data deviation Generally these kinds of methods are not recommended 3 Mean/Mode Complete If the missing value is numeric, to fill the missing value with the average of all the other non missing value; if the missing value is not numeric, to fill the missing value with the most likely value in all the other tuples without missing value Reference [1 thinks in rea l  applications, generally people have very little prior knowledge w ith the owned data, so this approach is obviously not satisfactory 4 Hot deck imputation To fill the missing values with its most similar values in all the data set. The concept of this method is very simple, and to estimate the missing value using relationship between the data of a tuple. At present   ink th at th e ho t d eck method is superior to other method There are several other filling methods such as EM Expectation maximization\\(Multiple Imputation and so on, these method are relatively complex, and the EM method convergence rate is slow, time complexity is large From the use of the frequency and extent of research now, generall filling method is the most commonly used method to deal with missing values. Its main disadvantage is difficult to define similar standards, there are more subjective factors Cloud model can transform qualitative and quantitative each other, considering the original data distribution in the database, using the cloud model to discrete non-missing values of database, then use the association rules to generate knowledge base with non-missing value tuples, when fill the missing value, first search  knowledge base to find associated knowledge, then use asso ciation rules to find the corresponding numerical cloud mode l, to fill missing value with the cloud generator, the model itself has the universal nature of social phenomena and nature, the filled data has the same distribution with the orig inal data, the association rules This work was suppo rted by the Scientific and technological development projects of Shandong Province \(Grant Nos 2008GG30001030 ral Science Fund Project of Weifang University \(Grant Nos. 2008Z08 
2010 Second International Workshop on Education Technology and Computer Science 978-0-7695-3987-4/10 $26.00 © 2010 IEEE DOI 10.1109/ETCS.2010.299 238 


 limit makes filled value of a r ecord relevant to the other attribute information of itself, so the similarity of Hot deck imputation are defined objectively II C LOUD MODEL  Cloud model is a qualitative and quantitative conversion model; it combines ambiguity and randomness organically Set U is a mathematical domain U={x}, T is the language value associated with the U 002\035 T x\a stable tendency random number which expressed the elements x subordination of T concept, subordination’s distribution in the domain is known as the cloud Fig 1 presents a cloud model; cloud mathematical expected curve is its subordination curves from the view of fuzzy set theory. However, "thickness" of the curve is uneven waist is the most scattered, the top and bottom of the curve are convergent, cloud's "thick" refl ects the subordination degree randomness, near to or away from the concept center have smaller subordination randomness, while concept center have the largest subordination randomness, which is consistent with people's subjective feelings  Figure 1 The digital features of the normal cloud The digital features of the no rmal cloud characterized by three values with the expectation Ex, entropy En, excess see Fig 1\value of the concept domain, is the most representative qualitative value of the concept it should be 100% belongs to the concept entropy En: is a qualitative measur e of the concept’s ambiguity reflecting the accepted range values of the concept domain hyper entropy He: can be descri bed as entropy En of entropy reflecting the degree of dispersi on of the cloud droplets. The normal cloud is the most important cloud model, because various branches of the social and natural sciences have proved the normal distribution’s univer sality. The equation of normal cloud curve 000\003 e 2En 2 Ex  002\035 2   MEC A 000\020 000\020 000 000P 000\003 000\013\000\024\000\014\000\003 000\003 000D\000\003\000\003\000\016\000\003\000E\000\003\000\003\000 \000\003\000F\000\021\000\003 000\013\000\024\000\014\000\003 000\013\000\024\000\014\000\003 Expectation curve is a normal curve, for a qualitative knowledge, the elements outside the Ex 000r 3En in its corresponding cloud model all ca n be ignored, because it has been proved that approximately 99.74% elements of model fall into the range of Ex 000r 3En by the mathematical characteristics of normal distribution. Extendi ng the normal cloud model to get trapezoidal cloud model, trapezoidal cloud model can be expressed by six values, they are the expected number: Ex 1 and Ex 2 entropy: En1 and En 2 hyper entropy: He 1 and He 2  Trapezoidal cloud curve equations are determined by the expectation and the Entropy are following 000\003 000\003\000\003\000\003\000\003\000\003\000\003\000\003 000\013\000\014 000\013\000\014 000\013\000\014 A A A 2 x Ex1 2 13 1 1 2En1 Ex1<=x<=Ex2 2 x Ex2 2 2232 2En2 MEC x MEC x  1 MEC x e e Ex En x Ex E x x Ex En 000\020 000\020 000\020\000\037\000 \000\037\000 000\020 000\020 000\037\000 000\037\000 000\016 000 000 000\003\000\003\000\003\000\003\000\003\000\003 000\013\000\025\000\014 000\003 Clearly, the left and the right half-cloud expectation curve is a normal curve It can complete the qualitativ e and quantitative transform more accurately, if there is a range belongs to the concept totally, then it can be expressed by the upper edge of Trapezoid if only one value belongs to th e concept totally then the upper edge of Trapezoid degenerate to a point, trapezoidal cloud model also degenerated into the normal cloud model. He1 and He2 can have different values, and thus the concept of the border on behalf of different fuzzy situation, when the He1 and He2 all degenerate to 0, trapezoidal cloud model expressed a concept with accurate border su bordination, when one of the He1 or He2 degenerate to 0, which expressed a concept with one r accurate border subord ination and one vague border subordination, so trapezoidal cloud model has a better generality III GENERATED NUMERICAL CHARACTERISTICS OF CLOUDS WITH EXISTING NON EMPTY ORIGINAL DATA  A Cloud Transform 000\003 Any function can be decomposed into cloud-based superposition with allowed er ror range, which is Cloud Transform. The equation is     1 x f x g j m j j c 000 000 000 000\003\000\003      0  1 000H 000\037 000\020 000\037 000 000 x f x g j j m j c 000\003\000\003\000\003\000\003\000\003\000\003\000\003\000\003 000\013\000\026\000\014 000\003 g\(x\ibution function f j x\oud-based expectations function c j coefficients m: the number of superimposed cloud 0010 user-defined maximum error From the concept of clouds: in the domain the element’s subordination to the concept has statistical and random properties. In addition, th e high-frequency elements contributions to the concept are higher than the low-frequency elements. That is the reason to use probability density function of data distribution to get the concept set, so the concept division algorithms can be done According to the definition of cloud transform, the quantitative attribute’s domain di viding into m concepts can evolve to a problem to get answers from the formula 
239 


000 000\016 000 000 m j i j j j j j j i j He He En En Ex Ex f x g c 1  2  1  2  1  2  1    000H I.e. to get Ex1 j Ex2 j En1 j En2 j He1 j He2 j and c j for each cloud concept the quantitative attribute domain is divided into a number of concepts by using cloud model, the data in each concept aggregate, and the data between different concepts separated B The original non-empty data discrete algorithm Counting non-missing values of the original data to get the histogram of data distribution, lo cal peak of the histogram is that the data aggregation part, taking it as a concept center is reasonable, the higher the peak, indicating more data convergence there, deal with it with priority. The discrete grouping algorithm is Algorithm 1: Discrete algorithm Input: the domain of property that need the concept division, the property values attributes i corresponds to them the overall error threshold 0010 and the peak height error threshold 0010 y the length error 0010 x between trapezoidal top edge and the minimum value Output: m concepts and the corresponding digital features of attribute i 1\ each possible values x of attribute i and get the actual data distribution function g \(x 2 3 000 002 g’\(x\=g\(x 4\max\(g’\(x 0010  5\ Ex j Find_Ex\(g’\(x 6\  Ex1 j search1\(g’\(x 0010 y  0010 x  7\  Ex2 j search2\(g’\(x 0010 y  0010 x  8\  En1 j Find_En\(c j Ex1 j  0010  9\  En2 j Find_En\(c j Ex2 j  0010  10\   gj\(x j Cloud\(Ex 1 En1 j Ex2 j En2 j  11\   g’\(x\g’\(x\g j x 12\   j=j+1 13\ endwhile 14\ for j=0 to m-1 do 15\ Clouds\(Ex1 j Ex2 j En1 j En2 j He1 j He2 j  He\(g j x\, g’\(x\Cloud\(Ex1 j Ex2 j En1 j En2 j  16 In Step 1, using statistical methods to get the actual data distribution function g \(x Step 2, 3 does variable initialization; Step 4 the division of the process is ended, if the error limit less than a given error, Step 5 Search for the peak value of c j of property in the data distribution function g\(x and its corresponding value x is defined as the cloud model center \(expectation\6, 7 search approximate horizon line near the peak \(within the error limit threshold 0010 y width is greater than the minimum width of the threshold value 0010 x  where were identified as uniformly distributed, the two endpoints of the line are recorded as the trapezoidal top edge endpoints Ex1 j Ex2 j otherwise get the trapezoidal top edge endpoints are equal to the peak point value Ex1 j Ex2 j Ex j  trapezoidal cloud degenerated into the normal cloud Trapezoidal cloud height coeffi cient is the function value of the Ex1 j or Ex2 j The step 8, 9 calculate cloud model entropy En1 j and En2 j to fit g \(x\for the half-liter cloud with Ex1 j  half-falling cloud with Ex2 j to get En1 j searching left area of the cloud model with Ex1 j to get En2 j searching right area of the cloud model with Ex2 j the entropy value increase from 0 step from the smaller va lue until the threshold 0010 is greater than the difference between the half-normal cloud value and distribution histogram value; th e step 10 calculate distribution function of the corresponding Tr apezoidal; step 11 use the original data minus the known distribution function of trapezoidal cloud model data distribution to get the new data distribution function g x\epeat step 4 to 12 until the peak value is less than the error threshold. Step 14, 15, 16 determine half hyper entropy of all cloud model with the residuals of distribution histogram Through the above processing, the non-missing numeric values are divided into several cloud models, the non-empty numerical attribute can be discrete based on their subordination to the cloud mode l. The data after discretization can be easily carried out Boolean association rule mining IV A SSOCIATION RULE MINING  Association rule mining refers mainly to get the knowledge such as "customers bought tea 000\021 also purchased the coffee," which meet the minimum support and the minimum confidence. At present association rules can be divided into two types: Boolean asso ciation rules and quantitative association rules, and most of the research are focused on the Boolean association rules research First, give the formal desc ription of association rules mining: Suppose I = \(i 1 i 2 i m of m different items, T = \(t 1 t 2 t n  j is a group of items of I set, t j 001\031 I. Each transaction with a unique identifier T ID linked. If X is a subset of I with X 001\031 t j we say that a transaction contains X. An association rule is a "X 000 Y implication, in which X 001\031 I, Y 001\031 I, and X 001 Y 001 Definition 1: If the ratio of transaction T contains X Y is 001 Sup, the association rules X 000 Y in T has a support degree Sup   100 n y support\(x Sup 000u 000 000\033  000\013\000\027\000\014  Support \(X Y\nds for the number which support 001 X Y transactions, n stands for the total number of 001 transactions Definition 2: If the ratio tran saction T contains X also contains Y is Conf, then th e confidence level of association rules X 000 Y in T is Conf 000\003\000\003\000\003\000\003\000\003\000\003\000\003\000\003\000\003\000\003\000\003\000\003  100 support\(x y support\(x Conf 000u 000 000\033 000\003\000\003\000\003\000\003\000\003\000\003\000\003\000\003\000\003\000\003\000\003\000\003\000\003 000\013\000\030\000\014 000\003 Association rules are called the strong association rules if they meet the minimum support and minimum confidence Discrete grouping algorithm di vided non-empty original quantitative attribute into multiple concepts, if one attribute value is mapped to a different concept, mapped to the concept whose subordination value is the greatest. A quantity data corresponding to the original mea ning is to buy not to buy tea through the concept discrete al gorithm is now extended to buy very much more tea, buy a lot of tea buy a few tea. The question naturally converted to Boolean association rule 
240 


mining, using the famous Apriori algorithm which generated frequent item sets and FP-Growth algorithm which does not produce the frequent item set to generate association rules database, which contains knowledge like <tea more 000 coffee, little> knowledge V G ENERATE THE MISSING VALUE WITH CLOUD MODEL AND ASSOCIATION RULES  Use three digits values of cloud model as a basis to fill the cloud droplets, the cloud droplets and the original data which generated three digits values of the cloud model are in the same distribution, characteristics the same data group, with these data to fill the missing values. This method combined organically the fuzzy nature and the randomness, which has a better exactness matching the human society behavioral characteristics. The accuracy of each record is limited by association rules, such as <tea, more 000 coffee, very little that is, tea cloud model if it is "more", then the missing value to fill the coffee just use the "fe w". Generate missing value to fill the value is to make the greates t possible reduction of the raw data Algorithm 2: The missing value reduction algorithm Input: association rule knowledge base, and the original database with a quantitative missing value Output: the database with no quantitative missing value 1\ocate the records with missing value 2\nd association rule "A 000 B" according to record’s nonmissing attribute A in the Knowledge Base 3\Get cloud model digital features \(Ex, En, He\from B 4\Generate the normal random number En' with En as expectation, He as mean square error 5\Generate the normal random number x with Ex as expectation, En' as mean square error to fill the missing value 6\required number of missing values produced VI E XPERIMENTAL A NALYSIS  Take two typical supermarket food rice and bean as example, the majority retail sales data scattered between 0.5-5kg, Extract 6000 transaction records as experiment data, set empty value in 600 records use random method respectively \(1\manually; \(2\ing Attribute values as Special values; \(3\an/Mode Complete; \(4\Hot deck imputati on: is given in this article based on cloud model and association rules. Calculate Absolute mean difference between the fill value and the original value of various methods, the smaller absolute mean difference, the higher the method accuracy. The Data are showed in table 1 TABLE I E XPERIMENTAL DATA ANALYSIS   Method Method 1 Method 2 Method 3 Method 4 Absolute mean difference 0.463 1.391 0.562 0.486 Seen from the experimental results, filling manually method’s overall error is not large, because the people know actual fact well, but it cost too much time. Method 2 and method 3 change the raw data distribution, they are meaningfulness. Methods 4 take into account both the distribution of the raw data, but also each record data correlation, so the method has high accuracy, to improve its accuracy further, reduce discrete particle size of the original non-empty data, and lower the minimum support for association rules and, but this will take more time, the minimum support degree and the minimum confidence degree shouldn’t too small, if so, the association rules does not make sense, the minimum support degree and the minimum confidence degree are given by the experts in the field VII C ONCLUSION  Filling an empty value based on cloud models and association rules first consider the distribution of the original data, use the cloud model to discrete the original non-missing data, the do the data association rules mining with the Boolean data and get knowledge base. Use the knowledge in knowledge base and the numerical cloud model features to fill the empty values, the missing value substitute have the same characteristics with the cloud model that generated the concept of the original data. The individual differences reflected the double features in ambiguity and randomness; it simulates the real-world general similarities and slight differences of the same group. For each record, use association rules in the knowledge base to enhance the substitute precision of each record further, the accuracy of the method is higher than others R EFERENCES   M.l. Brown and J.F Kros, Industr ial Management and Data Systems, vol 103, 2003,  pp.611-621  X.L Huang, A pseudo-nearest-n eighbor approach for missing data recovery on Gaussian random data sets 9 Pattern Recognition Letters 9  vol. 23, 2002, pp.1613-1622 9   J.W  Gr zy m a laBu sseand M   Fu, A co m p ar ison of sever a l appr oaches to  missing attribute values in data mining. In: Proc of the 2nd Int’ Conf on Rough Sets and Current Trends in Computing. Berlin: Springer-Verlag 2000, pp. 378-385  R. K. Pearson, The Problem of Disguised Missing Data ACM SIGKDD Explorations Newsletter, Vol. 8, Issue 1, June 2006 9   P. Jonsson and W  Claes An evaluation o f k-n e arest neighbour imputation using like 000U t data, Proceedings-10th international symposium on software metrics, 2004  J. Yu, General C-m eans clustering m odel. IEEE Tranctions on Pattern Analysis and Machine Intelligence, 2005, vol. 27l, pp. 1197-1211  S.C. Zhang Z.X. Qin, S.L. Sh engand C.L Ling, “missing is useful missing values in cost-sensitive decision trees. IEEE Transactions on Knowledge and Data Engineeri ng, 2005, Vol.17 pp.1012-1016 9   J. Han, and M  Kam b er Data M i ni ng Concepts and Techniques, 2nd ed Morgan Kaufmann Publishers: 2006, pp.98-112 9   Li De-y i and Liu Chang-y u Discussion of the u n iversal nature on the normal cloud model. China Engineeri ng Science, 2004,vol. 6,  pp. 28-33  M e h m ed  Kantar d z ic, Data m i ning Concepts M odels, M e thods and Algorithms, Beijing: Tsinghua University Press, 2003 9   J Han, J Pei Y Yin Mining Fr equent Patter ns without Candidat e  Generation A Frequent Pattren Tree Approach, data Mining and knowledge discovery, 2004, vol. 8,  pp.53-87 9  
241 


 GAR H  S  s l1  s l 2   c l  Input: H   a hierarchy concept for every attribute S  U  A  V an information system, where U are objects A are attributes, and V are their values s l1   threshold for a minimum Right Support at level l s l2   threshold for a minimum Left Support at level l c l   threshold for a minimum Confidence at level l  Step 1 Find the frequent generalized closed factor-sets: the sets of factors that have minimum left and right supports at highest level Step 1.1 Find the domain of each concept at level l and its support count above the predefined s l1 or s l2  AV  Get all distinct attribute-concept-value pairs for all attribute-concept-value pair  a,v  do begin HD  A, CV, L, D\:=Get domain of each concept value CV – concept value, L – Level value, 1, 2, 3,…., D – domain While L 007 1 do For each concept value If Card\(HD  min \(s l1 s l2    qualified attribute-value pairs  a, v  distinctValueQueue  006  a,v   end if end for end while end for Step 1.2 Find the frequent generalized closed factor-set Step 1.2.1 1-factor-sets N:= Get all elements at level  1 from distinctValueQueue M:= Get all elements at level  1 from distinctValueQueue while subLevelDistinctValueQueue changed do  for each attribute-value pair  a n v n  do For each attribute-value pair  a m v m  do If  a n  a m  AtomicFactor  a l  v n 005 v m   If  supL  AtomicFactor  012 s l1 supR  AtomicFactor  012 s l2  atomicFactorQueue 006  AtomicFactor  subLevelDistinctValueQueue 006 subLevelDistinctValue  end if end if end for N:= Get all elements from subLevelDistinctValueQueue M:= Get all elements from subLevelDistinctValueQueue end for end while  Step 1.2.2 2-factor-sets F:= Get all atomic factors  from atomicFactorQueue every factor denotes as f=\(a, v 005 v n:= F.size         // the total number of primitive factors in F for  i:=0; i++; i>n  do for  j  i+1  j++; j>n  do if  f i a 007 f j a  newCandidate  f i 015 f j  if \( supL  newCandidate  s 11  supR  newCandidate  s 12  add this new factor-set into the frequent factor-set queue and mark its length with 2 frequentFactorQueue 006  newCandidate    end if end if end for end for Step 1.2.3 Find the frequent k factor-sets, where k 2.  This step is iteratively generating new k factor-sets using the frequent \(k1\ctor-sets found in the previous iteration until there are no new frequent factor-sets generated P:=Get all frequent  k 1  factor-sets from frequentFactorQueue n:= the total number of \(k-1\-factor-sets  in P For  i:=0; i++; i>n  do If the first \(k-2\ factors are identical in f i and f j newCandidate  f i 015 f j  If \( supL  newCandidate  s 1 supR  newCandidate  s 2  If all subsets of newCandidate 023 frequentFactorQueue add this new factor-set into the frequent factor-set queue and mark its length with k frequentFactorQueue 006  newCandidate  end if end if end if end for Step 1.2.4 F max get all frequent factor-sets f with max Length from frequentFactorQueue maxLength:= the length of a factor-set in F max CompactFactorQueue  006 F k for  i   maxLength-1  i- -, i  1  do F i get all frequent factor-sets f’ with length i from frequentFactorQueue for each f’ in F i do if f  supL > max  010 f.supL in F i+1  if f  supR > max  010 f.supR in F i+1   CompactFactorQueue  006 f  end if end if end for end for Step 2. Generate strong Rules having the confidence above c  For each frequent factor-set f in frequentFactorQueue do n := f.length //the length of the frequent factor-set EQ for elementQueque EQ:= Parse the factor-set into factors and store each factor with level of 999 for the length of  the premise i from 1 to n-1 get next available premise for all factors in EQ Consequence  f- premise.factor   end for ruleCandidate  premise 000 Consequence  if \(conf\(ruleCandidate  012 c add rule the output and place a positive mark results 006 ruleCandidate mark EQ.level with level i //place a Positive mark end if end for end for print all Reclassification rules from result Figure 2. Pseudo-code of algorithm GAR 
2008 
2020 


5. Generalized Reclassification Rules Authors in [15  n tro d u ce h i erarch y  tax o n o m y  into association rule mining to identify more interesting rules and their results show promise. In this paper, we also studied generalization in mining, but used a different mining paradigm, actionable rule mining.  We developed a similar approach, called Generalized Actionable Rules \(GAR\, in order to find a concise set of reclassification rules.  The algorithm GAR is a breadth-first approach to mining a set of all frequent generalized closed factor-sets within concept hierarchies from top to lower level abstraction and then constructs reclassification rules straightforward by partitioning the valid factor-sets into a IF-THEN representation A generalized closed factor-set 000 G in S is defined as 000 G  a 1  013 1 005 014 1  015  a 2  013 2 005\014 2  015  015  a p  013 p 005\014 p  iff 000 G 023 000 and none of its factors is the ancestor factor of others A frequent generalized closed factor-set 022 G is defined as 022 G  a 1  013 1 005 014 1  015  a 2  013 2 005\014 2  015  015  a p  013 p 005\014 p  is a frequent generalized closed factor-set at level l iff 022 G 023 000 G  supL 022 G  012 s l1 and supR 022 G  012 s l2 where s l1 and s l2 are the user specified minimum left support  and minimum right support thresholds at level l respectively If a factor occurs rarely, its descendants will be even less frequent. For finding generalized actionable rules, different minimum left support right support, and confidence thresholds can be specified at different levels.   By doing this, if users want to find actionable rules at relatively lower levels of abstraction, the minimum left and right supports can be set relatively low without compromise, generating many valid uninteresting rules at higher or intermediate levels A generalized reclassification rule 000U G in S is defined as a statement 000U G  000 000 000<\000\017\000\003\000\017 where 000  000<\000\017\000\003 003\022 G  000 007 000<\000\017  000 016 000<\000\017\000\003  017 and no factor-set in 000 is an ancestor of any factor-set in 000 in the concept hierarchy Algorithm GAR consists of two main steps: \(1 generate all frequent generalized closed factor-sets, \(2 generate strong generalized reclassification rules Below is the short description of the algorithm and the pseudo-code of GAR is presented in Figure 2 5.1 Generate all Frequent Generalized Closed Factor-Sets  This step computes the frequent factor-sets in the data set through several iterations and the breadth-first top-downward approach is utilized. In each iteration we generate new candidates from frequent generalized closed factor-sets found in the previous iteration; the left support and right support for each candidate is then computed and tested against the user-specified thresholds for that level.  It examines the highest top level of abstraction to generate frequent closed factorsets and then progresses deeper into their frequent descendants at lower concept levels. The lower level concepts will be evaluated only when its ancestor has large left and right supports at the corresponding upper level.  End users have the power to gradually decrease the minimum left and right support thresholds at lower levels of abstraction in order to discover rules at different levels 5.2 Generate Generalized Rules  A reclassification rule is extracted by partitioning the factor-set W into two non-empty subsets X and Y and represented as X 000 Y where X, Y 023 W  X, Y 024 F S and X  W Y A level-wise approach is utilized for generating rules, where each level corresponds to the number of factors belonging to the rule premise The rule-premise is extended one factor at a time.  It starts with one factor on the rule-premise, such as X If the rule confidence is above the predetermined threshold, it is called a strong rule and marked with a positive mark  The principle of minimum description length is also adopted to identify the general rules.  Therefore, in the next iteration, it generates new candidates from only unmarked rules found in the previous iteration by moving one of the consequence-type factors to the rule premise, now X is a 2-factor-set.  Repeat the previous step until there is only one flexible factor on the rule consequent, such as Y By doing this, the resulting set of rules is comprised of those with the shortest length of premise, not all the lengths 
2009 
2021 


6.   Conclusion This paper addressed the problem of discovering generalized actionable rules in a dataset.   We investigated the properties of Reclassification Rules presented the notions of generalized closed factor-sets generalized reclassification rules, and introduced algorithm GAR This new algorithm is based on the concept of frequent generalized closed factor-sets to generate a very concise set of rules. The proposed framework provides more generalized knowledge from data than previous existing methods by incorporating concept hierarchy into mining procedure.  In addition the quality of the extracted rules in terms of their interestingness and understandability is improved Therefore, it would be easier for the user to comprehend the rule result in a timely manner.   In the future, we intend to apply this approach to a large variety of databases and application domains 7. References 1 A grawal R   Imiel i n s ki  T an d  S w ami, A M i n i n g  association rules between sets of items in large databases. In Proceedings of the ACM SIGMOD International Conference on the Management of Data. P. Buneman and S. Jajodia, Eds ACM Press, Washington DC, 1993, pp. 207–216   B e ne dit t o  M  E M  D  a nd Ba r r o s L  N d Us ing  Concept Hierarchies in Knowledge Discovery. A.L.C Bazzan and S. Labidi \(Eds.\: SBIA 2004, LNAI 3171, pp 255–265, 2004. Springer-Verlag Berlin Heidelberg 2004   G r z y m a l a B us se   J  A ne w v e r s i on of the r u le induc t i on system LERS. In: Fundamenta Informaticae, 31\(1\, 1997, pp 27-39  Ha n  J  Ca i  Y   and Ce r c one  N  Da t a D ri v e n D i s c ov e r y  of Quantitative Rules in Relational Databases. In: IEEE Trans. Knowledge and Data Eng., vol. 5, pp. 29-40, 1993   Ha n J   a nd Fu Y   D i s c ov e r y  of  m u lt ipl e l e v e l  association rules from large databases. In Proc. of the 21st Int'l Conference on Very Large Databases Zurich Switzerland, September 1995  H e  Z  Xu, X  a nd De ng  S  M i ning  Cl us te r D e f i ning  Actionable Rules. In: Proceedings of NDBC’04, 2 004   H e Z X u X  D e ng S a nd M a R  M i ning A c t i on Rules From Scratch. Expert Systems with Applications. Vol 29, No. 3, 691--699 \(2005   I m S and R a 001\036 Z.W.: Action Rule Extraction from a Decision Table: ARED.  In: 17th International Symposium on Methodologies for Intelligent Systems \(ISMIS\, pp 160—168. Springer,  Toronto, Canada \(2008 9 i n g  C  X Che n T  Y a ng Q a nd Che n   J  M i ni ng  Optimal Actions for Intelligent CRM. In: 2002 IEEE International Conference on Data Mining, pp.767--770 IEEE Computer Society, Maebashi City, Japan \(2002 9 i u B   H s u  W   a nd M a  Y    I d e n ti f y ing  Nona c t i ona ble  Association Rules. In: Proceedings of KDD 2001, pp. 329-334. San Francisco, CA, USA \(2001  a s quie r N    B a st i d e Y T a oui l R a nd L a k h a l   L    Discovering Frequent Closed Itemsets for Association Rules In: the 7th international conference on database theory ICDT’99\ pp 398--416, Jerusalem, Israel \(1999 11 P a wlak Z   In f o r m at io n S y st em s   Th eo ret i ca l Foundations. Information Systems Journal. Vol. 6, pp. 205-218 \(1981  i a t e s k y S ha pi ro G   a nd M a t h eu s C   J    T h e  interestingness of deviations. In: Proceedings of AAA Workshop on Knowledge Discovery in Database, pp. 25--36 AAAI Press, Menlo Park, CA \(1994  n la n J   R  C4.5: program for machine learning  Morgan Kaufmann, 1992  m a k r i s hns n  S  a nd R a k e s h  A    M i ni ng G e ne ra l i z e d  Association Rules. In: Proceedings of ICDM’03, pp 685688. IEEE Computer Society, Florida, USA \(2003  a 001\036 Z.W. and Tsay, L.-S.: Discovering Extended Action-Rules \(System DEAR\. In: Proceedings of the IIS'2003 Symposium, Advances in Soft Computing, pp. 293300. Springer, Zakopane, Poland \(2003  001\036 Z.W. and Wieczorkowska, A.: Action Rules: How to Increase Profit of a Company. In: Principles of Data Mining and Knowledge Discovery, Proceedings of PKDD'00, LNAI, pp. 587-592. Springer, Lyon, France 2000  T s a y L  S a nd Ra 001\036 Z.W.: Action Rules Discovery System DEAR2, Method and Experiments. Journal of Experimental and Theoretical Artificial Intelligence, Vol. 17 No. 1-2, pp. 119-128. Taylor and Francis \(2005  s a y L  S a nd Ra 001\036 Z.W.: E-Action Rules. In: Lin T.Y., Xie, Y., Wasilewska, A., and Liau, C.-J. \(eds Foundations of Data Mining, Studies in Computational Intelligence, pp. 261--272. Springer, Berlin / Heidelberg 2007  s ay   L  S a nd Ra 001\036 Z.W.: Discovering the Concise Set of Actionable Patterns. In: 17th International Symposium on Methodologies for Intelligent Systems \(ISMIS\ LNAI, Vol 4994, pp. 169--178. Springer \(2008 20  Tsay  L  S   R as  Z  W   an d Im, S   Re class i ficat io n  Rules. In: IEEE/ICDM Workshop on Foundations of Data 
2010 
2022 


Mining \(FDM 2008\, pp. 619--627. IEEE Computer Society Pisa, Italy \(2008  T s a y L  S   a nd I m   S    M i ni ng NonRe dunda nt Reclassification Rules.  In: Proceedings of the Twenty Second International Conference on Industrial, Engineering Other Applications of Applied Intelligent Systems IEA/AIE'09\, Tainan, Taiwan, June 24-27, 2009, 806-815  W ong R  C  W a nd Fu A   W  C    I S M   It e m  Se le c t ion for Marketing with Cross-selling Considerations. In the Eighth Pacific-Asia Conference on Knowledge Discovery and Data Mining \(PAKDD  pp. 431--440. Sydney, Australia 2004  ng Q Yi n  J   L i n  C  X   a nd C h e n  T     Postprocessing Decision Trees to Extract Actionable Knowledge. In Proceedings of ICDM’03, pp 685-688 IEEE Computer Society, Florida, USA \(2003  Z h a n g   H  Zha o Y  C a o, L  a nd Zha n g   C    C o m b i n e d  Association Rule Mining. In: Advances in Knowledge Discovery and Data Mining, Proceedings of the PAKDD Conference, Lecture Notes in Computer Science, 5012, pp 1069-1074. Springer, Antwerp, Belgium \(2008  n g  T  Ra m a k r i s hna n, R a nd L i v n y  M   B I R C H  An efficient data clustering method for very large databases In: Proceedings of ACM SIGMOD Conference, Montreal Canada, pp. 103–114 
2011 
2023 


