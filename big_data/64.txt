A Border-Based Approach for Hiding Sensitive Frequent Itemsets Xingzhi Sun School of ITEE The University of Queensland Australia sun@itee.uq.edu.au Philip S Yu IBM Watson Research Center Hawthorne NY USA psyu@us.ibm.com Abstract Sharing data among organizations often leads to mutual beneìt Recent technology in data mining has enabled efìcient extraction of knowledge from large databases This however increases risks of disclosing the sensitive knowledge when the database is released to other parties To 
address this privacy issue one may sanitize the original database so that the sensitive knowledge is hidden The challenge is to minimize the side effect on the quality of the sanitized database so that non-sensitive knowledge can still be mined In this paper we study such a problem in the context of hiding sensitive frequent itemsets by judiciously modifying the transactions in the database To preserve the non-sensitive frequent itemsets we propose a border-based approach to efìciently evaluate the impact of any modiìcation to the database during the hiding process The quality of database can be well maintained by greedily selecting the 
modiìcations with minimal side effect Experiments results are also reported to show the effectiveness of the proposed approach 1 Introduction Information sharing may require an organization to release its data to public or to allow another party to access it However some sensitive information which is secret to the organization needs to be hidden before the data is released Data mining technology enables people to efìciently extract unknown knowledge from a large amount of data This however extends the sensitive information 
from sensitive raw data e.g individual identiìer income and type of disease identiìer to the sensitive knowledge e.g sales pattern of particular product In the problem of hiding sensitive knowledge has been considered as an important issue of privacy preserving data mining To preserve data privacy in terms of knowledge one can modify the original database in some way so that the sensitive knowledge is excluded from the mining result In this paper we study the problem of hiding frequent itemsets from a transaction database The motivating examples of this research problem have been discussed in 3 
Here we give another real scenario In Australia the supermarket COLES and K-MART share the same customer bonus card by which the customer id can be identiìed during the transactions To facilitate business cooperation note that the products sold in these two supermarkets are not much overlapped two supermarkets may integrate their transaction datasets and analyze the inter-associations between their products However before releasing the dataset to the other party each supermarket wants to hide sensitive frequent itemsets/association rules of its own products Generally to hide the sensitive frequent itemsets the 
original database D needs to be modiìed into D   called result database  Considering the goal of information sharing releasing a garbage database is meaningless Therefore during the process of hiding sensitive frequent itemsets the quality of the database needs to be preserved so that the impact on the non-sensitive frequent itemsets is minimized Previous work 10 2 4 6 8 9 on hiding sensiti v e frequent itemsets has applied different heuristics to preserve the quality of the database However during the hiding process none of these studies really evaluates the impact 
of each modiìcation on the database In this paper we use the border  of the non-sensiti v e frequent itemsets to track the impact of altering transactions During the hiding process instead of considering every non-sensitive frequent itemset we focus on preserving the quality of the border which can well reîect the quality of the result database According to this idea a border-based approach is proposed to efìciently evaluate the impact of any modiìcation to the database during the hiding process The quality of database can be well maintained by greedily selecting the modiìcations with minimal side effect 
The contribution of this paper is as follows First we adopt the well-known border concept to the problem of preserving non-sensitive frequent itemsets Furthermore during the hiding process while previous work follows heuristics rather than really evaluating the impact of each Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


 Tid Items 1 abcde 2 acd 3 abdf g 4 bcde 5 abd 6 bcdf h 7 abcg 8 acde 9 acdh a Database Frequent itemset  Support abd 3 acd 4 bcd 3 cde 3 ab 4 ac 5 ad 6 bc 4 bd 5 cd 6 ce 3 de 3 a 7 b 6 c 7 d 8 e 3 b All frequent itemsets Expected Frequent itemsets on D acd cde ab ac ad bd cd ce de a b c d e c Non-sensitive frequent itemsets Figure 1 An example change our approach ensures that any modiìcation on the database is controlled according to the impact on the result database An efìcient algorithm is devised to identify the candidate change that has minimal impact on the border Speciìcally for each sensitive frequent itemset X  the algorithm determines the optimal deletion candidate  T,x   i.e the item x and the transaction T from which item x is deleted For each item x  X  we provide an efìcient way to estimate the upper and lower bounds on the impact to the border of deleting x  With these bounds the item which may potentially minimize the border impact can be straightforwardly selected We then determine from which transactions to delete that item The rest of this paper is organized as follows In Section 2 we give the background of hiding sensitive frequent itemsets and formulate our research problem A border-based approach is proposed in Section 3 Section 4 gives algorithms to hide sensitive itemsets The experiment results are shown in Section 5 Section 6 reviews the related work Finally we conclude this paper in Section 7 2 Hiding Sensitive Frequent Itemsets The work in this paper is based on the concept of frequent itemset which is deìned as below Let I   i 1 i m  be a set of items An itemset is a subset of I A transaction T is a pair  tid X   where tid is a unique identiìer of a transaction and X is an itemset A transaction  tid X  is said to contain an itemset Y iff X  Y A database D is a set of transactions Given a database D  the support of an itemset X  denoted as Supp  X   is the number of transactions in D that contain X  Foragiven threshold   X is said to be  frequent if Supp  X     Suppose that L be the complete set of  frequent itemsets in D and  L  L be the set of frequent itemsets that need to be hidden In the process of transforming D to D   we have the following considerations 1 Any X  L is not a  frequent itemset in D  To ensure that during the hiding process for any sensitive frequent itemset X  we delete an item x  X from a transaction that contains X to the decrease of Supp  X   2 Suppose that L  be the set of  frequent itemsets in D  and L r be the set of non-sensitive frequent itemset s determined by L and  L  we try to minimize  L r  L    i.e to avoid over-hiding non-sensitive frequent itemsets Note that according to the Apriori property L r can be computed by removing each sensitive frequent itemset and its supper-itemset from L  3 We try to maintain the relative frequency of nonsensitive frequent itemsets in L r  Let us consider the following example Suppose that we want to hide a set  L of sensitive itemsets in a given database D under the threshold 200  Let abc and bcd be two nonsensitive frequent itemsets with Supp  abc   500 and Supp  bcd   400  Assume that there are two result databases D  and D  corresponding to different hiding processes  In D  Supp   abc   330 and Supp   bcd   390  while in D  Supp   abc   400 and Supp   bcd   320  Apparently the quality of result database D  is better than D  as the relative frequency of itemsets is maintained much better To give more intuitive explanation of our hiding problem we give the following example A transaction database D is given in Figure 1\(a Let the support threshold  be 3 Figure 1\(b shows all  frequent itemsets with their support in D  Among those frequent itemsets abd bcd and bc are sensitive itemsets non-sensitive frequent itemsets are shown in Figure 1\(c The question is how to transform D into the result database D  in a sensible way such that the sensitive frequent itemsets become infrequent in D  and the quality of D  in terms of consideration 2 and 3 is maintained 3 A Border-Based Approach In this section we propose a border-based approach to address the hiding problem The key idea is that we use the Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


border of non-sensitive frequent itemsets  L r  to track the impact on the result database during the hiding process and maintain the quality of the result database by selecting the modiìcation with minimal impact at each step  Figure 2 Lattice and border 1      Figure 3 Lattice and border 2 The concept of border is initially introduced in and it is well applied in the research of maintaining the frequent itemsets e.g in F o r the completeness of the presentation we review the concept of border here Formally given a set of itemsets U the upper border respectively lower border f U denoted as Bd   U  respectively Bd   U   is a subset of U such that 1 Bd   U  respectively Bd   U   is an antichain collection of sets 1 and 2  X  U there exists at least one itemset Y  Bd   U  respectively Y  Bd   U   holding X  Y respectively X  Y  An itemset in the upper border or lower border is called a border element  In the running example we have Bd   L   abd acd bcd cde  and Bd    L   abd bc   The itemset bc is a border element of Bd    L   Note that due to the Apriori property we only need to hide the lower border Bd    L   Figure 2 gives a graphic representation of Bd   L  in the itemset lattice Figure 3 shows Bd   L r  in our example note that the sensitive frequent itemsets are circled Now let us examine why the border of L r is helpful to maintain the quality of the result database i.e taking care of the consideration 2 and 3 First according to the Apriority property concentrating on the border Bd   L r  during the hiding process is effective in avoiding the overhiding non-sensitive frequent itemset Secondly keeping the relative frequency among border elements is helpful to maintain the relative frequency in L r  It is because that the  1 A collection S of sets is an antichain if for any X Y  S both X  Y and Y  X hold support of border elements is relatively low and the relative frequency among them is sensitive to the sanitization Intuitively focusing on the most sensitive part of the frequent itemsets can effectively avoid the signiìcant change on the relative frequency In addition because of the Apriori property the support of the border could also reîect the support of the other frequent itemsets to some degree 3.1 Hiding One Itemset with Minimal Impact on Border For brevity we use border Bd  and Bd  to denote Bd   L r  and Bd    L  in the rest of this paper In this section we analyze the problem of hiding a given itemset with a minimal effect on Bd   Given a frequent itemset X  let  X  be the set of transactions that contain X  The set C of hiding candidates of itemset X is deìned as   T,x   T   X   x  X   Note that once a hiding candidate  T 0 x 0  is deleted i.e x 0 is deleted from transaction T 0  the new set of C  hiding candidate is C   T,x   T  T 0   The basic idea of our border-based approach is as follows Each border element B in Bd  is assigned a weight showing its vulnerability of being affected by item deletion The weight of B is dynamically computed based on its current support during the hiding process When we try to reduce Supp  X  of a sensitive frequent itemset X  for each hiding candidate c  we can calculate its impact on the border as the sum of weights of the border elements that will be affected by deleting c  Each time the candidate item with a minimal impact on the border Bd  is deleted until Supp  X  drops to   1  We examine the weight of a border element rst To take care of the consideration 2 and 3 stated in Section 2 we give the following deìnition for the weight of a border element The larger the weight of a border element B is the more vulnerable B is to further change therefore the lower priority of having B affected Deìnition 3.1 Given a database D and a border element B  Bd   let Supp  B  be the support of B in D In addition let  D be the database during the process of transformation and  Supp  B  be the support of B in  D note that at the beginning of transformation  D  D and  Supp  B  Supp  B   The weight of border element B is deìned as w  B     Supp  B    Supp  B  Supp  B      Supp  B    1      Supp  B   0 012  Supp  B  012  From the above deìnition we can see the following points 1 For a border element B when the current support of B   Supp  B   is greater than the threshold  w  B  is no more than 1 When  Supp  B  equals to   w  B  is Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


assigned a large integer   where      Bd    The intuition behind this is if the border element B is about to be infrequent a large value is assigned to w  B   indicating low priority of being affected If B is already overhidden   Supp  B    B should also be avoided for further change In that case w  B  is decided by  and the amount of  Supp  B  less than  2 If  Supp  B   1  with the decrease of  Supp  B  w  B  increases under the rate of 1  Supp  B     This reîects the consideration of checking the risk of destroying the border element and maintaining the relative frequency among itemsets on the border For example consider the case for two border elements B 1 and B 2  where Supp  B 1 0  Supp  B 2 5 and  10  When we need to hide a sensitive itemset by affecting the support of B 1 and B 2  we can see that B 1 has higher priority of being affected until its support is down to 26  Then B 2 starts to decrease its support by 1 followed by another 4 changes on Supp  B 1  if necessary  3 After  Supp  B  drops below the threshold the weight is increased at rate 1 with the decrease of  Supp  B  note that this rate is no less than the rate of 1 Supp  B     Considering two border elements with their current support below the threshold they will have the same increase rate on their weights i.e their support difference in original database D is ignored The reason is that we want to keep the support of every disappeared border element close to the threshold X  Bd  T 1 T 2 T 3 T 4 T 5 T 6 T 7 T 8 T 9 abd 1 1 1 bc 1 1 1 1 w  B i  B i  Bd  1 ab 1 1 1 1 1 2 bd 1 1 1 1 1 1 acd 1 1 1 1  cde 1 1 1 Figure 4 Demo In our running example the weight of each border element in database D is shown in Figure 4 with value on the left-most column After deìning the weight of the border element we next discuss the impact on the border caused by deleting a hiding candidate Hiding a frequent itemset may not potentially affect all border elements Apparently only the border element that has intersection with the sensitive frequent itemset may be affected during the hiding process Given a sensitive frequent itemset X and a border Bd   we deìne the potentially affected border of X  denoted as Bd   X  as the set of border elements of Bd   which may potentially be affected by hiding X  Formally we have Bd   X   B i  B i  Bd   B i  X 015     Clearly for evaluating the impact of hiding X on Bd   only Bd   X needs to be considered For a hiding candidate u of sensitive frequent itemset X  we can determine the set S u of border elements that will be affected by deleting u note that S u is a subset of Bd   X  The impact of deleting u on the border should be the sum of the weights of border elements in S u  Formally let Bd   X be  B 1 B n  and a lexicographical order can be imposed among B 1  and B n  Given a hiding candidate u of sensitive frequent itemset X we have a relevance bit vector b 1 b 2  b n such that b i 1 if u is a hiding candidate of B i i.e deleting u will decrease Supp  B i   otherwise b i 0  The relevance bit vector of u shows which border element B i will be affected if deleting u  In our running example for sensitive itemset abd  Bd   abd   ab bd acd cde   The relevance bit vector of hiding candidate  T 1 a  and  T 3 b  are 1010 and 1100 respectively Deìnition 3.2 Given a hiding candidate u of a sensitive itemset X  w  B i  for each B i  Bd   X  and the relevance bit vector b 1 b 2  b n of u the impact of u on Bd   denoted as I  u   is deìned as I  u   b i  w  B i   The value of I  u  is the sum of the weights of border elements that will be affected by deleting u  In our running example if we rst delete  T 1 a  to reduce Supp  abd   the impact I  T 1 a  is computed as w  ab  w  acd  1=2  According to Deìnition 3.2 at each step we can compute the impact for each hiding candidate and select the one with minimal impact to delete Remember that we have    Bd    So when the border Bd  is intact for any change that will break the border i.e affect the border element with weight   the impact of it must be greater the impact of the change that will not break Bd   In other words our strategy guaranties to select the change that will not break the border as long as such possible change exists 3.2 The Order of Hiding Itemsets in Bd  In this part we discuss the appropriate order of hiding frequent itemsets in Bd   The reason why we should consider the order of itemsets in Bd  is as follows If there exists two itemsets X Y  Bd   where Bd   X  Bd   Y 015   Hiding X may change the weight of border element B  Bd   X  Bd   Y  therefore affect the process of hiding Y  In general different orders may lead to different results Let us consider the following example Suppose that abcd and de are two sensitive itemsets and bcd  Bd   bcd is directly related to hiding abcd and indirectly related to de because abcd is a super-itemset of bcd but de is not If Supp  abcd  and Supp  bcd  are close hiding abcd may have the risk of over-hiding bcd  Note that hiding de may also affect the support of bcd  However this may be regarded as a side effect on maintaining bcd because they are less correlated To keep the border element bcd frequent Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


we want to maintain that every decrease of its support contributes to the hiding of abcd  In this case we need to consider abcd rst to avoid any side effect on the vulnerable border element bcd In general the longer border element is vulnerable to be over-hidden In this case for any two sensitive frequent itemsets we consider the longer sensitive itemset rst For two sensitive itemsets with the same length we hide the one with less support rst for the same reason 4 Algorithm In this section we give the border-based algorithms to efìciently hide sensitive frequent itemsets Figure 5 shows the main algorithm of hiding sensitive frequent itemsets which is a summary of the approach we described before The key step is to efìciently nd a hiding candidate with minimal impact on the border After selecting a candidate we need to update the hiding candidate set and the weights of the border elements respectively Note that for each sensitive frequent itemset we update the database once after selecting all hiding candidates to be deleted Main Algorithm Input A database D  the set L of  frequent itemset in D and the set of sensitive itemsets  L Output D  so that the quality is maintained Method Compute Bd  and Bd   Sort itemsets in Bd  in descending order of length and ascending order of support for each X  Bd  do Compute Bd   X and w  B j  where B j  Bd   X  Initialize C  C is the set of hiding candidates of X   for  i 0 i<Supp  X    1 i  do  Candidate selection algorithm Find u i  T i x i  such that I  u i  Min  I  u   u  C   Update C  C   T,x   T  T i   Update w  B j  where B j  Bd   X  Update database D  Output D   D  Figure 5 Main algorithm 4.1 Candidate Selection Algorithm The candidate selection algorithm gives the core of the border-based approach which is to efìciently nd the hiding candidate with minimal impact on the border Remember that at each step the search space of hiding candidates is   T,x   T    X   x  X   where   X  is the set of transactions in the current database  D that contains X For a large database and a long sensitive itemset X  it is costly to evaluate the impact of every hiding candidate in  C In this section we propose some heuristics to speed up this search To efìciently select one hiding candidate with minimal impact on the border our strategy is to quickly select the item x rst and then decide a transaction T    X   To nd an item x that may bring the minimal impact we rst estimate the possible impact of deleting a hiding candidate with item x  Based on the estimation we select an item with possible minimal impact on the border In general for a sensitive frequent itemset X when deleting the hiding candidate with x  X we can nd some border elements that must be affected and some border elements that could be affected  According to this observation for any item x  X we can use an interval to estimate the possible impact to the border of deleting the hiding candidate with x  Formally given a sensitive itemset X  the affected border Bd   X can be partitioned into direct border and indirect border  denoted as Bd   1 X and Bd   2 X respectively such that  Y  Bd   1 X Y  X For example given a sensitive itemset abd with its potentially affected border  ab bd acd cde    ab bd  is its direct border and  acd cde  is its indirect border Let u  T,x  be a hiding candidate of a sensitive itemset X  For any direct border element Y  Bd   1 X and x  Y  deleting u must decrease the support of border element Y  For an indirect border element Z  Bd   2 X and x  Z  the support of Z decreases iff T  X  Z Given a sensitive frequent itemset X and an item x  X  we can use an interval i  x  I l I r   called impact interval  to represent possible range of the impact of changing a hiding candidate with item x i  x  can be interpreted as changing a hiding candidate with item x will cause at least I l impact on the border for sure and with the risk of I r impact in the worst case At the rst iteration for any x  X i  x  I l   w  Y i  where Y i  Bd   1 X  x  Y i and i  x  I r   w  Z i  where Z i  Bd   x  Z i  The left bound I l is the sum of the weights of all relevant direct border The right bound I r is the sum of the weights of all relevant border element of Bd   Having known the impact interval of each item to show the priority of being changed we deìne partial order  on items based on the following principle for any two items x 1 x 2  X if i  x 1  I r    i  x 2  I r    x 1  x 2  on the contrary if i  x 2  I r    i  x 1  I r    x 2  x 1  otherwise x 1  x 2  i  x 1  I l i  x 2  I l or i  x 1  I l  i  x 2  I l  i  x 1  I r 012 i  x 2  I r The intuition behind this ordering is for any two items if we know one has risk of damaging the border but the other does not we will change the no risk one to guarantee that the border is intact Otherwise we always select the item with less possible impact on the border Let us consider our running example The impact interval for item a b and d are initialized as 1  2   3  2  3 2 012  and  1 2    3 2 012  respecProceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


tively We have the order a  b  d In general based on the order   we can select an item x  Now we show how to determine the transaction T After nding an item x  we calculate the impact of deleting x for each transaction and nd the one with the minimal impact To hide a sensitive frequent itemset X we use a bit map representation to reduce the size of  X   We maintain   X   bit vectors each of which corresponds to a transaction T   X   The length of each bit vector is  Bd   X   For a bit vector of the transaction T   X   b  i 1 iff T contains B i  where B i  Bd   X  In Figure 4 we can see bit vectors for T 1 T 3 T 5 are 1111  1100  1100  Given a hiding candidate u  T,x  of sensitive frequent itemset X and the bit vector b  1 b   Bd   X  of T the relevance bit vector b 1 b  Bd   X  of u can be computed as b i  b  i   x  B i  for i 1  and  Bd   X   In our example Bd   abd   ab bd acd cde  and the relevance bit vector of  T 1 a  is computed as 1010  For each transaction T   X   the impact of deleting  T,x  can be computed by the formula given in Deìnition 3.2 After scanning the bit map once in a worst case we can nd the hiding candidate with minimal impact on the border Note that once a hiding candidate is selected and deleted for any affected border element Y  its weight needs to be updated from w  Y  to w   Y   Accordingly the impact interval i  x  of each item x are also updated to i   x   5 Experiment Results In this section we evaluate the effectiveness and the efìciency of our border-based approach by comparing it with a heuristic-based approach which is referred as Algorithm 2.b in The heuristic in Algorithm 2.b for selecting a hiding candidate is straightforward Given a sensitive frequent itemset for all the transactions containing this itemset Algorithm 2.b rst identiìes the transaction with the shortest length In such a transaction the candidate item with the maximal support value is deleted to decrease the support of the sensitive itemset This approach hides the frequent sensitive itemsets efìciently and meanwhile demonstrates good effectiveness on minimizing the side effect on the result database In the rest part of this section we denote it as the heuristic approach  We evaluate our border-based approach on three synthetic datasets which are created by IBM synthetic data generator The characteristics of datasets are gi v e n i n Table 1 Table 1 Characteristics of datasets  Dataset  T   I   L   D  N Size in Megabytes T10I6L1.5K 10 6 1.5K 100K 1K 5.8 T10I6L1K 10 6 1K 100K 1K 5.8 T20I8L2K 20 8 2K 100K 1K 10.44 As our goal is to maintain the quality of the result dataset D   the quality of the result dataset D  could be measured as Q   L    L r   Apparently the percentage of over-hidden non-sensitive frequent itemsets is 1  Q  For each given dataset we evaluate the quality Q of the result dataset under different sets of sensitive frequent itemsets Now we rst deìne three characteristics of Bd  the lower border of the set  L of sensitive frequent itemsets as below 1 Number of sensitive itemsets in Bd   denoted as  Bd    2 Average support difference formally deìned as Avg Difsup    Supp  X i    1  Bd    where X i  Bd  and  is the support threshold  3 Average length of itemsets in Bd   deìned as Avg len   len  X i   Bd    where X i  Bd  and len  X i  returns the length of X i  For example if Bd  is  a 10 bc 8 def 6  and the support threshold  is 5  we have  Bd   3  Avg Difsup 4  and Avg len 2  In our experiments for each dataset we evaluate the quality Q of result dataset in terms of the size of the lower border   Bd    and the average support difference  Avg Difsup f Bd   Figure 6 gives the complete results of effectiveness evaluation by comparing with the heuristic approach Figure c show the impact of  Bd   on the quality of the result dataset Let us take Figure 6\(a as an example The corresponding experiments are performed on dataset T10I6L1.5K with the percentage of support threshold i.e  100 K  always set as 0.6 We intentionally create multiple sets of sensitive frequent itemsets such that in each set the average length is 3 and the average support difference is controlled within the range from 10 to 12 In this case we can evaluate how  Bd   impacts the quality Q  This result shows that the quality of the result dataset is well maintained with over 98 of non-sensitive frequent itemsets preserved In general the quality of the result dataset decreases with the increase of  Bd    In Figure f the impact of Avg Difsup on Q is shown on the condition that  Bd   and Avg len are constant in each dataset It is also clear that the quality Q degrades with the rise of Avg Difsup the reason is that the increase on Avg Difsup requires to delete more hiding candidates thus leads to more impact on Q   Based on the experiment results we can see that our approach outperforms the heuristic approach in terms of the quality of the result dataset i.e protecting more nonsensitive frequent itemsets from being over-hidden In all these gures the maximum improvement by the borderbased approach is around 5 i.e up to an additional 5 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


 a T10I6L1.5K  100 K  0  6 Avg len 3 Avg Difsup  1012 b T10I6L1K  100 K  0  8 Avg len 3 Avg Difsup  4952 c T20I8L2K  100 K  0  8 Avg len 5 Avg Difsup  2024 d T10I6L1.5K  100 K  0  6 Avg len 3    Bd    4 e T10I6L1K  100 K  0  8 Avg len 3    Bd    5 f T20I8L2K  100 K  0  8 Avg len 4    Bd    5 Figure 6 Effectiveness evaluation of the non-sensitive frequent items could be oven-hidden by the heuristic approach Note that  L r  is often a large number it is roughly ranged from 1000 to 2000 in our experiments So little difference in the percentage indicates more signiìcant difference in the actual number Also the over-hidden non-sensitive frequent itemsets are close to the border which often carry more signiìcant information than the itemsets at the lower level of the lattice Figure 7 shows the efìciency of the border-based approach on the basis of the dataset T20I8L2K Particularly Figure 7\(a and Figure 7\(b depict the performance of hiding frequent itemsets in terms of  Bd   and Avg Difsup respectively From both gures we can see that while the heuristic approach takes less time than our border-based approach their performance curves are very close This is because the most time-consuming step of hiding sensitive frequent itemsets lies in the dataset scan Both approaches require the same number of dataset scan i.e  Bd    Although our border-based approach is more complex in the step of selecting hiding candidates the heuristics introduced in Section 4.1 offers an innovative algorithm which effectively reduces the computational cost Further we examine the scalability of our approach Figure 7\(c shows the response time of hiding one sensitive itemset against the number of transactions in the dataset We can see that our approach is linearly scalable 6 Related Work Privacy preserving data mining has become a popular research direction recently The problem of hiding frequent itemsets or association rules was rstly studied in by Atallah et al In this w ork nding the optimal sanitization solution to hide sensitive frequent itemsets was proved as a NP-hard problem Also a heuristic-based solution was proposed to exclude sensitive frequent itemsets by deleting items from the transactions in the database The subsequent work e xtended the sanitization of sensiti v e frequent itemsets to the sanitization of association rules The work prevented association rules from being discovered by either hiding corresponding frequent itemsets or reducing their conìdence below the threshold The work provided some heuristics to select the items to be deleted with the consideration of minimizing the side effect under the assumption that sensitive frequent itemsets were disjoint The later work further discussed the problem of hiding association rules by changing items to unknown instead of deleting them Also substantial work 6 8 9 has been done in this area by Oliveira and Zaiane Generally their work focused on designing a variety of heuristics to minimize the side effect of hiding sensitive frequent itemsets Particularly in  the Item Grouping Algorithm IGA grouped sensiti v e association rules in clusters of rules sharing the same itemsets The shared items were removed to reduce the impact Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


 a T20I8L2K  100 K  0  8 Avg len 5 Avg Difsup  2024 b T20I8L2K  100 K  0  8 Avg len 4    Bd    5 c T20I8L2K D400K   D  0  8    Bd    1 Figure 7 Efìciency evaluation on the result database In a sliding windo w w as applied to scan a group of transactions at a time and sanitized the sensitive rules presented in such transactions In recent work the y considered the attacks against sensitive knowledge and proposed a Downright Sanitizing Algorithm DSA to hide sensitive rules while blocking inference channels by selectively sanitizing their supersets and subsets at the same time In summary the challenge of hiding sensitive itemsets or association rules is to minimize the side effect on the result database In previous work a variety of approaches have been proposed based on different heuristics However during the hiding process none of them really evaluates the impact of each modiìcation on the database 7 Conclusions In this paper we have studied the problem of hiding sensitive frequent itemsets with a focus on maintaining the quality of the result database The originality and contributions of our work include the following aspects 1 We considered the quality not only based on the number of non-sensitive frequent itemsets preserved in the result database but also in terms of their relative frequency 2 Most importantly to minimize the side effect on the result database we provided the rst efforts to evaluate the impact of any modiìcation to the database during the hiding process Thus the quality of database can be well maintained by controlling modiìcations according to the impact on the result database 3 We applied the border concept in the context of frequent itemset hiding problem A border-based approach was proposed to efìciently select the modiìcation with minimal side effect 4 We studied the performance of the proposed approach and the results were superior to the previous work in effectiveness at the expense of a small degradation in efìciency References  R Agra w a l and R Srikant F ast algorithms for mining association rules In Proc of the 20th VLDB  pages 487Ö499 1994  M Atallah E Bertino A Elmagarmid M Ibrahim and V Verykios Disclosure limitation of sensitive rules In Proc of KDEXê99  pages 45Ö52 1999  C Clifton and D Marks Security and pri v a c y implications of data mining In Workshop on Data Mining and Knowledge Discovery  pages 15Ö19 Montreal Canada 1996  E Dasseni V  S V erykios A K Elmagarmid and E Bertino Hiding association rules by using conìdence and support In Proc of the 4th Information Hiding Worshop  pages 369Ö383 2001  H Mannila and H T o i v onen Le v e l wise search and bor ders of theories in knowledge discovery Data Mining and Knowledge Discovery  1\(3 1997  S Oli v eira and O Zaiane Pri v a c y preserving frequent itemset mining In Proc ICDM Workshop on Privacy Security and Data Mining  pages 43Ö54 2002  S Oli v eira and O Zaiane Algorithms for balancing pri v a c y and knowledge discovery in association rule mining In 7th Proc of the IDEAS  pages 54Ö63 2003  S Oli v eira and O Zaiane Protecting sensiti v e kno wledge by data sanitization In Proc of the 3rd ICDM  pages 613 616 2003  S Oli v eira O Zaiane and Y  Saygin Secure association rule sharing In Proc of the 8th PAKDD  pages 74Ö85 2004  Y  Saygin V  S V erykios and C Clifton Using unkno wns to prevent discovery of association rules ACM SIGMOD Record  30:45Ö54 2001  A V eloso W  Meira Jr  M de Carv alho B Possas S Parthasarathy and M J Zaki Mining frequent itemsets in evolving databases In Proc of the 2nd SDM  2002  V  S V erykios E Bertino I N F o vino L P  Pro v enza Y Saygin and Y Theodoridis State-of-the-art in privacy preserving data mining ACM SIGMOD Record  33:50Ö57 2004  V  S V erykios A K Elmagarmid E Bertino Y  Saygin and E Dasseni Association rule hiding TKDE  16:434 447 2004 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


and continuous attributes as so to cover more important features for analysis, compared with traditional association rules The example database and data set The supermarket basket database contains three tables illustrated in Fig. 7 a customers, such as name, sex, age, postcode, etc b panel data, which is randomly generated by data generator according to several certain shopping behaviours of customers. It includes UPC \(universal product code number of items purchased, total price, etc c information published in the website [12], such as energy, fat content, sugar content of each product in per 100 gram Fig. 7: Supermarket basket database example B 1 The parameters and examples To mining fuzzy association rules using the effective reduced database algorithm from prepared data set, we set up maximum probability threshold G as 0.15, which drops those entries whose joint probability is less than 15% of the maximum probability of each reduced database table, and credibility threshold H as 0.9 Nutrient contents not items We use SQL to select an customer shopping history to form 2 data sets, which contains his total spending or the amount of items purchased in each day separately and 10 kinds of nutrient contents of products purchased in that shopping in total, to discover fuzzy association rules In the first data set, we discover 9 fuzzy association rules and one of them is shown below IF \(sum_of_energy is high sum_of_protein is high sum_of_carbohydrate is high sum_of_vitamins is medium sum_of_fat is high sum_of_sodium is high sum_of_sugar is medium sum_of_fibre is high sum_of_starch is high sum_of_iron is medium THEN \(total_spending is high In the second data set, we discover 8 fuzzy association rules, and one of them is shown below The 2005 IEEE International Conference on Fuzzy Systems783 IF \(sum_of_energy is high sum_of_protein is high sum_of_carbohydrate is high sum_of_vitamins is medium sum_of_fat is high sum_of_sodium is high sum_of_sugar is medium sum_of_fibre is medium sum_of_starch is high sum_of_iron is medium THEN \(amount_of_items_purchased is big We can use this method to analyse the favourite poroduct or behaviour of a certain customer or a certain group of customers. The new approach of using nutrient contents instead of items in the relations gives much less complexity of association rules, especially to analyse hundreds or thousands of items in the supermarket 2 V Both discrete and continuous attributes We give another simple example below that includes both discrete and continuous attributes in the fuzzy association rule which is discovered from the data set containing customer  s sex, age and the sum of sugar content of products he/she purchased each time IF \(sex is male age is old sum_of_sugar is medium As it can be seen, if we use crisp sets as special fuzzy sets in fuzzy association rules, we still have the traditional association rules Combining discrete attributes such as customer details \(sex postcode, city, etc attributes such as nutrient contents, spending, price, stock, cost and customer  s age in the association rules is very helpful in 


and customer  s age in the association rules is very helpful in various kinds of business analysis. It provides alternative new approach to find some good relations for analyser CONCLUSION AND FUTURE WORK Compared with traditional association rules, fuzzy association rules provide good linguistic explanation, and can deal with both discrete attributes and continuous attributes. It provides an alternative new approach in the applications of association rules, which not only can be used to reduce the complexity of association rules, but also to cover more important attributes in the rules In order to solve complexity problem of mining fuzzy association rules, we discover those rules by using the effective reduced database algorithm \(ERDB The reduced database table \(RDBT database algorithm are good methods to build a transparent and knowledge based fuzzy model with less space and time complexity. Afterwards it can be used to discover various kinds of fuzzy models, such as fuzzy association rules, fuzzy decision tree, fuzzy Bayesian net, etc In the end, we only keep those fuzzy association rules whose credibility is not less than credibility threshold H, which are discovered from the final reduced database table It is helpful to either use feature selection before running the ERDB algorithm or combine it with the ERDB algorithm which can drop redundant and unimportant attributes to reduce the dimension of the final reduced database table. This improvement is our future work ACKNOWLEDGMENT This work is supported by my supervisor Jim F. Baldwin The author thanks him for proposing the reduced database table \(RDBT ERDB REFERENCES 1] R. Agrawal, T. Imielinski, A. Swarmi  Mining Association Rules between Sets of Items in Large Databases  in proceedings of the ACMSIGMOD 1993 International Conference on Managementof Data Washington D. C., U.S.A., pp. 207-216, 1993 2] Chan Man Kuok, Ada Fu, and Man Hon Wong  Mining Fuzzy Association Rules in Databases  SIGMOD Record, pp. 41-46, Vol 27 No. 1, 1998 3] Jim Baldwin, http://www.enm.bris.ac.uk/teaching/enjfb/emat31600 4] Lotfi A. Zadeh  Fuzzy Logic = Computing with Words  IEEE Transaction on Fuzzy Systems, Vol 4, No. 2, May 1996 5] J. F. Baldwin, T. P. Martin and B. W. Pilsworth  Fril  Fuzzy and Evidential Reasoning in Artificial Intelligence  Research Studies Press Ltd. \(John Wiley 6] E.H. Ruspini  A new approach to clustering  Information and Control vol 15, 1969, p. 22-32 7] J. F. Baldwin, Dong \(Walter  Simple Fuzzy Logic Rules based on Fuzzy Decision Tree for Classification and Prediction Problem   Intelligent Information Processing II, Published by Springer, ISBN 0387-23151-X \(HC 8] R. Agrawal, R. Srikant  Fast algorithms for mining association rules   in proceedings of the 20th VLDB Conference, pages 487--499 Santiago, Chile, 1994 9]   Jim Baldwin, Sachin Karale  New concepts for fuzzy partitioning defuzzification and derivation of probabilistic fuzzy decision trees   North American Fuzzy Information Processing Society \(NAFIPS 10]   J. F. Baldwin, E. Di Tomaso  Bayesian networks for continuous values and uncertainty in the learning process  the EUSFLAT Conference Zittau, Germany, September 2003 11] XLMiner, http://www.resample.com/xlminer/help 12] Sainsbury  s online, http://www.sainsburys.co.uk/healthyeating The 2005 IEEE International Conference on Fuzzy Systems784 pre></body></html 


accurate estimates for distinct value queries and event reports This algorithm outperforms other estimators that are based on uniform sampling[8 9 e v e n when using much less sample space We also provide comparison with our Implication Lossy Counting ILC algorithm described in Section 5.1 which is based in the Lossy Counting algorithm introduced in For this series of experiments we used a real dataset of eight dimensions which was given to us by an OLAP company whose name we cannot disclose due to our agreement The cardinalities of the dimensions are presented in Table 3 The parameters of the algorithms are presented in Table 5 For NIPS/CI we used 64 concurrent bitmaps with a fringe size of four thus requiring memory enough to hold  2 4  1   64  K  1920 itemsets We expect that the averagingî\([5 o v e r these man y bitmaps will result in an error less than 10 We used the exact same sample space for DS The bound parameter t for DS was set to  1920  50  following the suggestion in F o r ILC we used an approximation parameter   0  01 which increases the memory requirements of ILC relative to those of NIPS/CI or DS On the average ILC used more than twice the memory that NIPS/CI and DS used For example for the experiment in Figure 7\(B it used more than 8,000 entries We evaluate the results of the algorithms with respect to the number of tuples the cardinality of the participating dimensions and the implication conditions To simulate a real data stream scenario we tracked the conditional implication counts of A  B  E  F and the unconditional B  E using the aforementioned algorithms The rst workload corresponds to quite large compound cardinality while the second to very moderate cardinalities Table 4 presents the actual aggregates for various instances of the stream for   5 and  1  60 We believe that most workloads fall somewhere in the middle with respect to the complexity of the wanted implications and the size of the returned counts Figure 7\(A depicts the relative error as the stream evolves for workload A using the algorithms DS NIPS/CI and ILC for different implication parameters In Figure a we show the results for minimum support   5 and  1  60 or  1  80 The different  1 are encoded in the parentheses next to identication of the algorithm in the legend of the graph In Figure b we increased the minimum support to   50 We observe that the behavior of DS varies widely while NIPS/CI remains always below the expected 10 error DS actually keeps a sample of the distinct elements seen so far and tries to scale the implication count that holds for that sample to the whole set of distinct elements In most cases the data in the sample is not representative of the implication The situation for DS is exacerbated when the minimum support increases where quite a lot of samples do not participate in the count making the scaling even more errorprone Algorithm ILC in all cases returned very erroneous results although it used much more space than NIPS/CI and DS since it tries to store not the implication counts but the actual implicated itemsets In these workload the implicated itemsets overwhelm its available memory which is actually larger than the amount given to NIPS/CI and DS In gure 7\(B we present the results of the algorithms for workload B The situation is still in favor of NIPS/CI whose relative error remains always close to the expected 10 unlike DS who returns highly skewed errors even though the domain cardinalities are much smaller and therefore keeps in the sample space much more data As expected from the analysis the error guarantees of NIPS/CI are virtually unaffected by changes in the cardinalities or the number of tuples seen so far in the stream ILC returns very erroneous results although now the cardinalities and the implicated items are much smaller compared to those of workload A The reason is not only because it keeps too much information in memory i.e all the implicated itemsets while both NIPS/CI and DS only hold a mantissa for the count but also because the constraint    rel is broken as the number of tuples increases 7 Related Work There are unique challenges in query processing for the data stream model Most challenges are the result of the streams being potentially unbounded in size Therefore the amount of the storage required in order to get an exact size may also grow out of bounds Another equally important issue is the timely query response required although the volumes of data the need to be processed is continually augmented at a very high rate Essentially the amount of computation per data item received should not add a lot of latency to each item Otherwise any such algorithm wont be able to keep up with the data stream In many cases accessing secondary storage such as disks is not even an option In  there is a discussion of what queries can be answered e xactly using bounded memory and queries that must be approximated unless disk access is allowed Sketching techniques\([14 ha v e been introduced to build summaries of data in order to estimate the number F 0 of distinct elements in a dataset In three algorithms that      approximate the F 0 are described with various space and time requirements Distinct is dri v e n by hashing functions similar to those studied in 14 and provides highly accurate results for distinct value queries compared to those taken by uniform sampling by using only a fraction of their sample size In the algorithms Stick y Sampling and Lossy Counting are introduced that estimate frequency counts with application to association rules and iceberg cubes In a framework for performing set expression on continuously updated streams based on sketching techniques is presented In a general framework over multiple granularities is presented for both range-temporal and spatio-temporal aggregations In a framework for identifying hierarchical heavy hitters i.e hierarchical objects like network addresses whose prexes denes a hierarchy with a frequency above a given threshold is described The effect of impications between columns has been emphasized in the system that identies correlated pairs of columns and soft-dependencies and has been proved very useful in query optimization 8 Conclusions We have presented a generalized and parameterized framework that can accurately and efciently estimate implication counts and can be applied to many scenarios To the best of our knowledge this is the rst practical and truly scalable approach to the problem Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


 Dimension Cardinality A 1557 B 2669 C 2 D 2 E 3363 F 131 G 660 H 693 Table 3 Cardinalities Workload A Workload B Tuples A  B  E  G E  B 134,576 608 50 672,771 12,787 125 1,344,591 34,816 152 2,690,181 84,190 165 4,035,475 132,161 182 5,381,203 187,584 188 Table 4 Impl counts w.r.t tuples NIPS/CI bitmaps 64 NIPS/CI K 2 DS sample size 1920 DS bound t 39 ILC  0.01 Table 5 Algorithm Parameters           0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILC \(.8             0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILS \(.8           0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6 ILC \(.8             0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILC \(.8   a   5 b   50 a   5 b   50 A Workload A B Workload B Figure 7 Relative Error vs stream size of online estimation within small errors of complex implication and non-implication counts between attributes of a data stream under severe memory and processing constraints and even in the presence of noise We prove that the complement problem of estimating non-implication counts can be      approximated when the size of the fringe zone is xed appropriately We demonstrate that existing algorithms for estimating frequent itemsets or sampling cannot be applied to the problem since they lose the cumulative effect of small implications In addition through an extensive set of experiments on both synthetic and real data we have shown that NIPS/CI always remains very close to the actual implication count capturing even very small implications whose total contribution is signicant References  R Agra w al A Evmie vski and R Srikant Information sharing across private databases In ACMÖSIGMOD  2003  N Alon Y  Matias and M Sze gedy  The space comple xity of approximating the frequency moments Journal of Computer and System Sciences  58:137 147 1999  A Arasu B Babcock S Bab u J McAlister  and J W idom Characterizing memory requirement for queries over continuous data streams In Proceedings of the Twenty-ìrst ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems  2002  B Babcock S Bab u M Datar  R  Motw ani and J W idom Models and Issues in Data Stream Systems In Proceedings of the Twenty-ìrst ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems  2002  Z Bar Y ossef T  Jayram R K umar  D  S i v akumar  and L T r e visan Counting Distinct Elements in a Data Stream In RANDOM  pages 1ñ10 2002  A Belussi and C F aloutsos Estimating the selecti vity of spatial queries using the correlation fractal dimension In VLDBê95 Proceedings of 21th International Conference on Very Large Data Bases September 11-15 1995 Zurich Switzerland  pages 299ñ310 1995  J Bunge and M Fitzpatrick Estimating the number of species A r e vie w  Journal of American Statistical Association  88:364ñ373 1993  M Charikar  S  Chaudhuri R Motw ani and V  Narasayya T o w ards estimation error guaranties for distinct values In ACMÖPODS  pages 268ñ279 2000  S Chaudhuri R Motw ani and V  Narasayya Random sampling for histogram construction How much is enough In ACMÖSIGMOD  pages 436 447 1998  G Cormode F  K orn S Muthukrishnan and D Sri v astana Finding hierar chical heavy hitters in data streams In VLDB  2003  M Datar  A  Gionis P  Indyk and R Motw ani Maintaing stream statistics over sliding windows In Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms  2002  A Deshpande M Garof alakis and R Rastogi Independence is Good Dependency-Based Histogram Synopses for High-Dimensional Data In ACM SIGMOD  2001  C Estan S Sa v age and G V a r ghese Automatically inferring patterns of resource consumption in network trafc In ACMÖSIGCOMM  2003  P  Flajolet and N Martin Probabilistic Counting Algorithms for Data Base Applications Journal of Computer and System Sciences  pages 182ñ209 1985  N Friedman L Getoor  D  K oller  and A Pfef fer  Learning probabilistic relational models In IJCAI  pages 1300ñ1309 1999  S Ganguly  M  Garof alakis and R Rastogi Processing set e xpressions o v e r continuous update streams In ACM SIGMOD  pages 265ñ276 2003  P  Gibbons Distinct sampling for highly-accurate answers to distinct v alues queries and event reports In VLDB  pages 541ñ550 2001  P  J Haas J F  Naughton S Seshadri and L Stok es Sampling-based estimation of the number of distinct values of an attribute In VLDB  1995  I Ilyas V Markl P  Haas P  Bro wn and A Aboulnaga CORDS Automatic Discovery of Correlations and Soft Functional Dependencies In ACMÖSIGMOD  2004  J Jung B Krishnamurthy  and M Rabino vich Flash cro wds and denial of service attacks Characterization and implications for cdns and web sites In ACMÖWWW  2002  J Ki vinen and H Mannila Approximate dependenc y inference from relations Theoritical Computer Science  149:129ñ149 1995  G Manku and R Motw ani Approximate frequenc y counts o v e r data streams In VLDB  2002  V  Poosala P  J Haas Y  E Ioannidis and E J Shekita Impro v e d histograms for selectivity estimation of range predicates In ACMÖSIGMOD  pages 294ñ305 1996  S Stolfo W  Lee P  Chan W  F an and E Eskin Data mining-based intrusion detectors An overview of the columbia ids project ACMÖSIGMOD Record  30\(4 2001  H W ang D Zhang and K G Shin Detecting SYN Flooding Attacks In INFOCOM 2002  2002  K Whang B V ander Zander  and H T aylor  A Linear Time Probabilistic Counting Algorithm for Database Applications ACM Transactions on Database Systems  pages 209ñ229 1990  D Zhang D Gunopulos V  Tsotras and B  See ger  T emporal and spatiotemporal aggregations over data streams using multiple time granularities Information Systems  28\(1-2 2003 The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofﬁcial policies either expressed or implied of the Army Research Laboratory or the U S Government Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


Figure 3a and 3b, is negative since CBA and PART achieved a higher classification rate against this particular dataset A comparison of the knowledge representation produced by our method, PART and CBA has been conducted to evaluate the effectiveness of the set of rules derived. Figure 4 represents the classifiers generated form the hyperheuristic datasets. Analysis of the rules sets indicated that MMAC derives a few more rules than PART and CBA for the majority of the datasets. In particular, the proposed method produced more rules than PART and CBA on 8 and 7 datasets, respectively. A possible reason for extracting more rules is based on the recursive learning phase that MMAC employs to discover more hidden information that most of the associative classification techniques discard, since they only extract the highest confidence rule for each frequent item that survives MinConf Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 6. Conclusions A new approach for multi-class, and multi-label classification has been proposed that has many distinguishing features over traditional and associative classification methods in that it \(1 that contain rules with multiple labels, \(2 evaluation measures for evaluating accuracy rate, \(3 employs a new method of discovering the rules that require only one scan over the training data, \(4 introduces a ranking technique which prunes redundant rules, and ensures only high effective ones are used for classification, and \(5 discovery and rules generation in one phase to conserve less storage and runtime. Performance studies on 19 datasets from Weka data collection and 9 hyperheuristic scheduling runs indicated that our proposed approach is effective, consistent and has a higher classification rate than the-state-of-the-art decision tree rule \(PART and RIPPER algorithms. In further work, we anticipate extending the method to treat continuous data and creating a hyperheuristic approach to learn  on the fly   which low-level heuristic method is the most effective References 1] R. Agrawal, T. Amielinski and A. Swami. Mining association rule between sets of items in large databases In Proceeding of the 1993 ACM SIGMOD International Conference on Management of Data, Washington, DC May 26-28 1993, pp. 207-216 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rule. In Proceeding of the 20th International Conference on Very Large Data Bases, 1994, pp. 487   499 3] M. Boutell, X. Shen, J. Luo and C. Brown. Multi-label semantic scene classification. Technical report 813 Department of Computer Science, University of Rochester Rochester , NY 14627 &amp; Electronic Imaging Products R &amp D, Eastern Kodak Company, September 2003 4] A. Clare and R.D. King. Knowledge discovery in multilabel phenotype data. In L. De Raedt and A. Siebes editors, PKDD01, volume 2168 of Lecture Notes in Artificial Intelligence, Springer - Verlag, 2001,  pp. 42-53 5] P. Cowling and K. Chakhlevitch. Hyperheuristics for Managing a Large Collection of Low Level Heuristics to Schedule Personnel. In Proceeding of 2003 IEEE conference on Evolutionary Computation, Canberra Australia, 8-12 Dec 2003 6] R. Duda, P. Hart, and D. Strok. Pattern classification Wiley, 2001 7] E. Frank and I. Witten. Generating accurate rule sets without global optimisation. In Shavlik, J., ed., Machine Learning: In Proceedings of the Fifteenth International 


Learning: In Proceedings of the Fifteenth International Conference, Madison, Wisconsin. Morgan Kaufmann Publishers, San Francisco, CA, pp. 144-151 8] J. Furnkranz. Separate-and-conquer rule learning Technical Report TR-96-25, Austrian Research Institute for Artificial Intelligence, Vienna, 1996 9] W. Li, J. Han and J. Pei. CMAR: Accurate and efficient classification based on multiple class association rule. In ICDM  01, San Jose, CA, Nov. 2001, pp. 369-376 10 ] T. Joachims. Text categorisation with Support Vector Machines: Learning with many relevant features. In Proceeding Tenth European Conference on Machine Learning, 1998,  pp. 137-142 11] T. S. Lim, W. Y. Loh and Y. S. Shih. A comparison of prediction accuracy, complexity and training time of thirtythree old and new classification algorithms. Machine Learning, 39, 2000 12] B. Liu, W. Hsu and Y. Ma. Integrating Classification and association rule mining. In KDD  98,  New York, NY, Aug 1998 13] J.R. Quinlan. C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann, San Francisco, 1993 14] J.R. Quinlan. Generating production rules from decision trees. In Proceeding of the 10th International Joint Conferences on Artificial Intelligence,  Morgan Kaufmann San Francisco, 1987, pp. 304-307 15] R. Schapire and Y. Singer, "BoosTexter: A boosting-based system for text categorization," Machine Learning, vol. 39 no. 2/3, 2000, pp. 135-168 16] F. Thabtah, P. Cowling and Y. Peng. Comparison of Classification techniques for a personnel scheduling problem. In Proceeding of the 2004 International Business Information Management Conference, Amman, July 2004 17]Y. Yang. An evaluation of statistical approaches to text categorisation. Technical Report CMU-CS-97-127 Carnegie Mellon University, April 1997 18] X. Yin and J. Han. CPAR: Classification based on predictive association rule. In  SDM  2003, San Francisco CA, May 2003 19]CBA:http://www.comp.nus.edu.sg/~dm2/ p_download.html 20] Weka: Data Mining Software in Java http://www.cs.waikato.ac.nz/ml/weka 21] M. J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. New algorithms for fast discovery of association rules. In Proceedings of the 3rd KDD Conference, Aug. 1997 pp.283-286 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 R=1000 1e-4 0.2 High 1.153e3 1.154e3 3.180e3 2.698e3 45 45.0 597 0.214 0.9900 0.0071 1.0000 0.0162 2e-4 2e-4 Low 0.4017 0.4024 109.291 71.994 4 4.0 39.7 0.996 0.9960 0.0075 0.9995 0.0012 2e-3 1.0 1e-3 Medium Medium 0.4128 0.4129 17.488 19.216 6 6.0 29.6 0.992 0.9938 0.0018 0.9987 0.0034 R=0.1 0.01 4e-3 High 0.4612 0.4722 4.1029 5.410e3 12 12.0 28.0 0.992 0.9902 0.0045 1.0000 0.0132 0.04 1e-8 Low 0.0250 0.0272 253.917 155.263 3 3.0 38.0 1.002 0.9991 0.0001 0.9994 0.0008 1e-3 1e4 1e-7 High Medium 0.0338 0.0290 17.303 12.837 3 3.0 14.5 0.332 0.9900 0.0058 0.9951 0.0045 R=1e-5 1e-2 1e-6 High 0.0918 0.0557 1.6071 1.225e5 8 8.0 9.9 0.992 0.9906 0.0034 0.9994 0.0142 1e-3 TABLE II SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=100, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 2e-4 Low 0.0408 0.0407 2.9255 2.0060 5 5.0 30.3 0.996 0.9933 0.0016 0.9998 0.0020 0.02 1.0 1e-3 Medium Medium 0.0446 0.0446 0.5143 333.138 10 10.0 27.8 0.996 0.9900 0.0042 0.9995 0.0093 R=0.01 0.1 4e-3 High 0.0763 0.1062 0.1580 3.402e3 84 84.0 90.9 0.793 0.9900 0.0099 1.0000 0.0334 0.4 TABLE III SIMULATION RESULTS FOR: TRACKING REGIME PD=0.9, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.166e3 1.167e3 1.073e4 3.357e4 11 12.2 257 0.542 0.9900 0.0089 1.000 0.0073 5e-5 1e-4 0.1 Low Medium 1.167e3 1.167e3 5.509e3 1.481e4 21 23.2 309 0.858 0.9900 0.0067 1.0000 0.0052 R=1000 1e-4 0.2 High 1.168e3 1.170e3 3.180e3 6.979e3 46 50.6 475 0.798 0.9900 0.0089 1.000 0.0022 2e-4 3] T. Fortmann, Y. Bar-Shalom, Y. Scheffe, and S. B. Gelfand  Detection Thresholds for Tracking in Clutter- A Connection Between Estimation and Signal Processing  IEEE Trans Auto. Ctrl., Mar 1985 4] S. B. Gelfand, T. Fortmann, and Y. Bar-Shalom  Adaptive Threshold Detection Optimization for Tracking in Clutter  IEEE Trans. Aero. &amp; Elec. Sys., April 1996 5] Ch. M. Gadzhiev  Testing the Covariance Matrix of a Renovating Sequence Under Operating Control of the Kalman Filter  IEEE Auto. &amp; Remote Ctrl., July 1996 6] L. C. Ludeman, Random Processes: Filtering, Estimation and Detection, Wiley, 2003 7] L. Y. Pao and W. Khawsuk  Determining Track Loss Without Truth Information for Distributed Target Tracking Applications  Proc. Amer. Ctrl. Conf., June 2000 8] L. Y. Pao and R. M. Powers  A Comparison of Several Different Approaches for Target Tracking in Clutter  Proc Amer. Ctrl. Conf., June 2003 9] X. R. Li and Y. Bar-Shalom  Stability Evaluation and Track Life of the PDAF Tracking in Clutter  IEEE Trans. Auto Ctrl., May 1991 10] X. R. Li and Y. Bar-Shalom  Performance Prediction of 


10] X. R. Li and Y. Bar-Shalom  Performance Prediction of Tracking in Clutter with Nearest Neighbor Filters  SPIE Signal and Data Processing of Small Targets, July 1994 11] X. R. Li and Y. Bar-Shalom  Detection Threshold Selection for Tracking Performance Optimization  IEEE Trans. on Aero. &amp; Elect. Sys., July 1994 12] D. Salmond  Mixture Reduction Algorithms for Target Tracking in Clutter  SPIE Signal and Data Processing of Small Targets, Oct. 1990 13] L. Trailovic and L. Y. Pao  Position Error Modeling Using Gaussian Mixture Distributions with Application to Comparison of Tracking Algorithms  Proc. Amer. Ctrl. Conf., June 2003 4323 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207ñ216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intíl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intíl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





