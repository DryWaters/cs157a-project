SPAAT-A Modern Tree Based Approach for sequential pattern mining with Minimum support Ashwin C S                                         Rishigesh.M                                  Shyam Shankar T M  Sri Sai Ram Engineering College              MNM Jain Engineering College              MNM Jain Engineering College Information Technology                                 Computer Science                                       Computer Science Chennai, INDIA                                         Chennai, INDIA                                         Chennai, INDIA tocsashwin@gmail.com                          rishigesh_gr8@yahoo.com                             shyam2332@gmail.com 
 A bstract\227 Though we have seen many data-mining methods there are always various discrepancies and we have proposed a frequent pattern mining is an important data-mining for finding the correlations among items Since the frequencies for various items are always varied specifying a single minimum support cannot exactly discover interesting patterns In order to overcome these discrepancies we propose an apriori based method to include the concept of multiple minimum supports MMS in short on association rule mining It allows user to specify MMS to reflect the different natures of items Since the mining of sequential pattern may face the same problem we extend the traditional definition of sequential patterns to include 
the concept of MMS in this study For efficiently discovering sequential patterns with MMS we develop a data structure named PLMS-tree to store all necessary information from database After that a pattern growth method named MSCPgrowth is developed to discover all sequential patterns with MMS from PLMS-tree This paper can be used in evolving scenarios and sure to create a revolution in its own field  Keywords: Sequential pattern, multiple minimum supports  pattern growth  their unique natures and generate different association rules depending on which items are in the patterns  Recently there have been some extended studies on MMS  1, 5, 7-9   T h e s e  r e s e a r c h e s  s h o w  t h a t  M M S  c a n  b o t h  e n h a n c e  t h e  
efficiency of the mining process and generate more interesting patterns However most of previous studies deal with the  rare item problem on association rule mining but rarely on sequential pattern mining In this study we first extend the definitions of sequential pattern with consideration to MMS, which allows users to specify multiple item supports to reflect different frequencies of items Items with low frequencies will be specified with lower minimum supports and patterns involving these items can be easily retrieved for further decision support  The major problem on frequent pattern mining with MMS  is that the  downward closure property  no longer holds in the mining process which means that a super-pattern of an 
infrequent pattern might be a frequent pattern. To effectively I  I NTRODUCTION  reduce the search space in level-wise methods, Liu et al. [5  proposes the sorted closure property where all items in the Sequential pattern mining is one of the most important  research issues in data mining which was first introduced by Agrawal and Srikant and c a n be descri bed  as follow s w e  a r e  given a set of data-sequences as the input data Each data sequence contains an ordered list of transactions \(itemsets\Given a user-specified minimum support threshold minsup sequential pattern mining finds all subsequences with  frequencies larger than minsup in a sequence database  Some previous studies on association analysis [1, 5-7   indicate that using only minsup as single frequency threshold does 
not deliberate on the nature of items in real-life applications That is single  minsup  implicitly assumes that all items in the database have similar frequency However some items may appear very frequently in the database while others rarely appear Under such circumstance, if we set the value of minsup too high we will not find those rules involving rare items in the database On the other hand, if we set that value too low, it will generate a huge amount of meaningless patterns. Therefore, Liu et al. [5  f i r s t  address this dilemma \(also known as the  rare item problem  and propose the concept of multiple minimum supports MMS in short to redefine the problem of association rule mining The method allows users to specify different  minsup s for different items \(MIS values\ to reflect dataset are sorted in ascending order by their MIS values. Keep this 
ordering in subsequent operations and let  k patterns generated from k 1\-pattern share the same prefix, i.e. the smallest minsup  among all items in k patterns, the complete set of frequent patterns with MMS can be easily discovered. The sorted closure property however, is invalid in sequential pattern mining since the order of events in a data-sequence cannot be altered Therefore to discover complete set of sequential patterns with MMS is not straightforward. Base on the new definitions of sequential pattern with MMS, we first propose an extended version of the PLWAPtree structure  n a m e d  Preorder Linked Multiple Supports tree  PLMS-tree to compress and store all necessary information from sequence databases. After that, we extend PLWAP-Mine to develop an efficient algorithm named 
 Multiple Supports Candidate Pattern growth  MSCP-growth for discovering complete set of sequential patterns with MMS We extend PLWAP-tree and PLWAP-Mine because they have been proven to be a compact data structure and a highly efficient algorithm on sequential pattern mining  The structure of this paper is as follows. In Section 2, we  first review the concept of PLWAP-tree, which is considered as the basis of our approach Section 3 introduces the definition of sequential pattern mining with MMS. Our data structure 978-1-4244-9825-3/11/$26.00 \2512011 IEEE 177 


    PLMS-tree and the mining algorithm MSCP-growth will be  III  P ROBLEM DEFINITION presented in Section 4, and we have a conclusion in Section 5 In this section, we formally give definitions used in the  discovery of sequential pattern with MMS. Let I denote the set II  R ELATED WORK  of items in the database, and a subset of I is called an itemset Preorder Linked WAP tree \(PLWAP-tree\ algorithm is  proposed by Ezeife et al  E xt e nde d fro m  W A P-t r e e   3 PLWAP-tree enhances its performance by avoiding reconstruction the intermediate tree The PLWAP-tree stores the sequence databases into a preorder linked tree, and each node in the PLWAP-tree has a unique binary code to indicate its position The position code makes this tree be able to directly recognize the ancestors and descendants of a node by comparing the assigned position codes between nodes and avoid re-constructing the intermediate trees during the mining process In order to makes it easier to explain how to implement the position codes the PLWAP-tree uses an equivalent binary tree to represent the original tree Given a node  n   n s first child node in the original tree is  n s left child node in equivalent binary tree and  n s first sibling node in the original tree is  n s right child node in equivalent tree Fig 1 is an example on describing how to represent WAP-tree as an equivalent binary tree and how to assign the position code on each node  The rule of assigning the position code is as follows, given  an equivalent binary tree, and nodes in the tree including a  node n 1 a left child of n 1 named n 2 and a right child node of n  1 named n Given n s position code is a  n s position code  3 1 2 will be assigned with an additional 1 in the bottom of a and  becomes a 1 n 3 s position code will be assigned with an  additional 1 in the bottom of a and becomes a 0". For  example, the leftmost child of root node in tree is B, node B will be the root node of equivalent binary tree. The leftmost child node of B is D, and the first sibling node of B is C, so the equivalent binary tree is represented as Fig. 1 and the position codes of B, D and C are "1", "11" and "10", respectively. The other nodes are assigned in the same way          Figure 1. Identify the decendants by position code   Fig 1 also explains how to use position codes on identifing relationships between ancestors and descendants of a given node Given a node  n  with positon code  a  suppose we assighs an additional 1 in the bottom of a and becomes a 1", all nodes with the same prefix  a 1 are descendants of  n  For example the position code of node B is "1". We assigns an additional 1 in the end of B's postion code, i.e. "11", and each node with prefix "11" is the descendant of node B A customer's data-sequence is an ordered list of items with time stamps. Therefore, a sequence, say  can b ep sented  e r re as a e a s a and t j stands for the time when a j occurs, 1 j n and t j 1 t j for 2 j n If several items occur at the same time in the  1  t 1  a 2  t 2  a 3  t 3    a n  t n wher j i n item  sequence, they are ordered alphabetically  Definition 1 Given an item I q  i 1  i 2  we say  set  i m itemset I q occurs in  if integers 1 k 1  k 2    k m n exist such that i 1   a k 1  i 2  a k 2  i m  a km and t k 1  t k 2  t km We refer to k 1 and t k 1  jointly as the position and the time that I q  occurs in  respectively  Definition 2 Let   I 1  I 2  I s  I q   I for 1 q s\ be  a sequence of itemset. Assu e t each I q in  occurs in    m tha urs   t I1  t I2  t Is where t Iq 1 q s is the time, at which I q  Then we say sequence  occ in or is a subsequence of  if  occurs in    Definition 3 A sequence database S is formed by a set of  records sid  s where sid is the identifier of this data sequence and s is a data-sequence. For a given sequence  the  support count of sequence  in S are defined as follows  supp    sid  s  sid  s    S    is a subsequence in s   The following definitions are related to the concept of  MMS In this model the definition of the minimum support is changed. Each item in the database can have its minsup which is expressed in terms of  minimum item support  MIS In other words, users can specify different MIS values for different items  Definition 4 Let MI i denote the MIS value of item i  i  S  Iq itemset I q  i 1 i 2  i m 1 k m denoted as MIS I q is equal   I Given an itemset  i 1  i 2    i m the MIS value of  to  min[ MIS i 1 MIS i 2   MIS i m    D finition 5 Given a sequence   I 1 I 2  I s  I q   I for 1  e q s the minim um support threshold of  denoted as  MIS  is equal to  min[ MIS I 1 MIS I 2   MIS I s    Definition 6 Given a sequence database S and a seque e    nc we call  is frequent in S or  is a sequential pattern in S if supp    MIS    By providing different minimum item support thresholds  for different items the user can effectively express different support requirements for different data-sequences MMS allow us to have higher minimum supports for sequences involving highly frequent items and also allow us to have lower minimum supports for sequences involving rare items In summary the problem we addressed can be stated as follows. Given a sequence database S and a set of MIS values for all items in S we discover all sequential patterns that satisfy MIS       178 


IV  T HE P ROPOSED ALGORITHM  will use an example to explain the PLMS-tree constructing In this section, we first proposed a PLWAP-tree-like  algorithm called PLMS-tree to store all necessary information from sequence database After that we than propose a new pattern growth algorithm named MSCP-growth to generate the complete set of sequential patterns with MMS based on the PLMStree   A. PLMS-tree construction  The PLMS-tree is an extension of PLWAP-tree. But there  are still some differences between these two tree structures. The first is that PLWAP-tree contains only items with support steps as follows           Figure 2. Example tree no less than minsup while PLMS-tree retain items with support  Input : sequence database S and the MIS vale of each item in I  no less than MIS I i.e. the smallest MIS value among all items  Output : PLMS-tree T header table HT  It is because items with support no less than MIS I have  M thod : tree_construction S MIS value of each item i in I  e 1  possibilities to be part of sequential patterns. The second is that in PLMS-tree we need to add some additional information into the tree structure for discovering complete set of sequential patterns This part will be discussed later  Similar to PLWAP-tree, PLMS-tree is composed of a  header table and tree nodes The header table stored related information of items in the sequence database where  iname records item's name  mis  records the MIS value of this item and nlink records the link to the first node in the tree node shares the same item-name with this item in prefix order. Items in the header table are sorted in increasing order based on its MIS value  The tree node stores the information including iname  iflag   icount  mps  pcode and the related node links plink  clink  slink and nlink The iflag and mps are two new fields. The iflag is used to identify the last item of a transaction. Since we consider 2 3 4  5  6 7 8 9  10 11 12 13 14 15  16 17 Initial PLMS-tree T create root  Initial header table HT ba ed on MIS value for each item i in I  s For each sequence s in S  Call ins rt_tree s, T insert s into the tree  e End loop  For each node n in T  If with supp  n.iname MIS I  then  Call prune i , T prune items that cant reach the threshold  End if  End loop  For each node n in T  Call merge T merge odes and give position code to each node  n If n is a leaf node of T  Call hres_set T set the threshold to each node  t End if  End loop  Call bulidLinkage\(root of T build linkage with the same iname  Figure 3. Algorithm 2 - Tree construction   In the following example, node n 1  iname 1  icount 1  pcode 1  multiple items in an event, the mining process has to  distinguish whether the nodes with item-name i and their suffix are in the same itemset. The mps is the minimum possible support which is equal to the minimum MIS value among this node and all nodes in its suffix tree. This value is used to be part of threshold in our candidate pattern growth algorithm to reduce the number of candidate patterns. The iname records which item this node represents. The icount records the number of sequences represented by the portion of denotes n 1 s item name, item count and position code  respectively The path from  n 1  to its descendent  n r  is denoted as  iname 1  icount 1  pcode 1     iname r  icount r  pcode r Given  a sequence database and assume the MIS value of each item is  defined as follows a 70 b 80 c 40 d 65 e 30 f 40 Based on the MIS values above, we can sort each sequence shown in table I the path reaching this node. The pcode is position code used to TABLE I S ORTED SEQUENCE DATABASE identify the ancestor and descendent of this node. The related node links, including plink  clink  slink and nlink are linked to its parent node, first child node, first sibling node and the next node sharing the same item name, respectively  We use an simple tree with three sequences ab  a  abc   and ac to explain our tree in Fig. 2 first. Node a 3\ is not the SID  10  20 30 40 50 Unsorted se uence  q  ac  ab  bc    ac  ab  c    a  e  c  a  b   ac  f  ab  d   a  e  c  d  b  Sorted sequence   ca  ab  cb    ca  ab  c    a  e  c  a  b   ca  f  ab  d   a  e  c  d  b  last item of an itemset, it's iflag is false. Node \(_ c 1\ is the last item of an itemset, it's iflag is true. The mps value of node a 3\ is 30 because the minimum MIS value in it's suffix tree is 30 The represent tree is used to describe the sequence database and the data structure tree shows the related node links between nodes, in the following content we will use the represent to describe our example  Based on the definitions of PLMS-tree, we proposed a  PLMS-tree construction algorithm. As shown in Fig. 3, and we At the first step, we create a root node of tree T labeled as  NULL!. A header table HT records all items' names and their MIS values. The first scan of database S inserts the sequences s into T by turns We use the first two sequences to explain the insertion process as follows  1. The first sequence s 10  ca  ab  cb Since the child  node of root is NULL, we create new nodes as Fig. 4\(a\. Noted  that each sequences in the S is a combination of itemsets, we mark the last item i of each itemset as _ i to distinguish among      179 


itemsets, and the items in the same itemset are sorted by their MIS value in increasing order  2. For the second sequence s 20 it shares the same prefix   ca  ab  c with the s 10 But our study wants to discuss about the situation of itemset, the last item c in s 20 is the last item of one itemset, and the same item in s 10 is not, the last item c in s 20  will be considered as a different item with item c in s 10 The icount in branch c 1 _ a 1  a 1   _ b 1\are increased by 1 because they shares the same prefix  but the next item of s 20 i.e. \(c\is not the same node with s 10   Item c in s 20 cannot share node with the item c in s 10 it will be  a new child node \(_ c 1\ of \(_ b 2\ as Fig. 4\(b  The remaining sequences will be inserted in the same way  the insert algorithm is shown at Fig. 5 and the result after step 1 is shown at Fig. 6\(a the node links related to \(_ f 1\ will be modified to keep the data structure right at the same time. Several links related to this node needed to be modified, including node a 2\'s slink is linked to _ f 1\ and node a 1\'s plink is linked to \(_ f 1\, Node a 2\'s slink  will be linked to \(_ f 1\'s child node a 1\ and node a 1\parent link will linked to \(_ f 1\'s parent \(_ a 3\. The complete pruning algorithm is shown in Fig. 7     Figure 6\(a  Figure 6\(b  Figure 6. PLMS-tree - Step 2 - pruning    Input : node n with supp n  iname MIS I   M thod : prune e  e 1  Modify the link linked to e s child node and sibling node 2  Remove e  Figure 4\(a  Figure 4 \(b  Figure 7. Method: prune - remove infrequent items    Input : sequence s PLMS-tree T  Figure 4. Tree construction steps  Since we prune some nodes from the tree, there might exist nodes sharing the same item name with their sibling node. The third step merges nodes with their sibling node if they share the Method : Insert_tree T  s point to root  1 2 3 4  5 6 7 8   9  10 11 12 13 14  15 16  17  18 19  20 For each itemset t in s  Sort all items in t based on their MIS values in increasing order  If item i is the last item of t then  lastit m = TRUE //distinguish between itemset  e End if  For each item i in t  If there exists a child node e which e  iname  i  iname and the  same lastitem value then  e  icount   Point to e  Else  Create a new node e with e.iname   iname  i Set the re ted information about e  la Point to e  End if  End loop  If i appears in s the first time  supp i count item i s support   End if  End loop  Figure 5. Method: insert tree - insert S into tree same item name and assign position code to the nodes which  do not need to be merged. Follow the example in Fig. 6, we find that two child nodes of \(_ a 3\, i.e a 2\ and a 1\share the same iname  a Moreover, both of them are not the last item of an itemset, so they will be merged together and the merging step is shown in Fig 8\(b\At the same time, we modify the related node link to keep the data structure right. The node \(_ b 1\'s plink will be changed to the merged node a 3\ and node \(_ b 2\'s slink is changed to \(_ b 1\because they have the same parent node. The following two nodes \(_ b 2\ and \(_ b 1\ will do so in the same way as Fig. 8\(c  At the second step, since all items in S are inserted into T it possibly has items with support less than MIS I i.e. these  items are impossible to be part of sequential patterns. Follow  Figure 8\(a  Figure 8 \(b  Figure 8 \(c  the example above, the supports of each item is as follows a 100 b 80 c 100 d 40 e 40 f 20%. Since the MIS I  is 30%, all nodes with iname  f will be removed because supp  f  is less than MIS I The pruning step is shown in Fig. 6. In this case, node \(_ f 1\is going to be removed and         Figure 8. PLMS-tree - Step 3 - merging steps   The position code is assigned at the same time when we merge nodes by traversing T from root, if a given node n does 180 


not need to be merged, it will be assigned with a position code Since the leftmost node of root in T is c 3\we use c 3\ to be the root of equivalent binary tree, which means it is assigned with position code 1. As we motioned in section 2, its leftmost child _ a 3\is assigned with an additional 1 in the bottom and patterns with MMS. The complete pseudo code of MSCP growth is shown in Fig. 13. Due to the limit of space, we briefly explain the main procedure of MSCP-growth below  Input : node n  Method : buildLinkage n  If n NULL then  becomes 11, the first sibling node \(_ c 1\is assigned with an  addition 0 in the bottom and becomes 10. The other nodes in T are assigned with unique position code in the same way. The result of PLMS-tree with position code is shown in Fig. 12, too and the merging and assigning algorithm shows in Fig. 9  Input : Tree T  Method : merge T  1 2 3 4  5  6  7 8 If HT  n  iname   nlink  NULL then  H  n.iname   nlink  n   T Else  link he last node linked by HT  n  iname   nlink s nlink to n  t End if  buildLinkage n  clink  buildLinkage n  slink  1 2 3 4 5  6  7  8 9  10  11  12 Form the root of T trace e ch node e in T  a If e s slink NULL then  For each sibling nodes of e  If this node is appeared the first time then   as ign position code  s Else  mer e to the node with same node name  g End if  End loop  Else  assign position code  End if 9 End if  Figure 11. Method: thres_set - set mps to each node Figure 9. Method: merge - merge tree and assign position code   The fourth step is to set the mps value of each node e in T This value is used to record the minimal possible support in this node's suffix tree and it will play an important rule in our mining process. The mps is equal to the minimum MIS value between e  and all nodes in e s suffix trees. We trace form the leaf nodes in T  to assign their mps value. In this case, the mps of node _ b 1:1111111\ is 80% and its parent node c 1:11111\ is 40%, so the mps value of c 1:11111\ does not to be modified, but  c 1:11111\'s parent node, \(_ b 3:1111\'s mps       Figure 12. Complete PLMS-tree with part of node link   Input : Root set RS root  Output : the complete set of sequential patterns  M thod : MSCP-growth RS   MIS  e 1    If RS is empt y then return  value is 80%, so the mps value of \(_ b 3:1111\ needs to be modified to 40% which is equal to it's child's mps value. The other nodes in T are assigned with mps vales in the same way. The result of PLMS-tree with mps value is shown in Fig. 12 and the mps setting algorithm is shown in Fig. 10  Input : leaf node of T  M thod : thres_reset T  e 1   2 3 4 5 6 7 8  9 For each item i with supp  i MI S in HT  I For each node n belongs to i in T  If n is a descendent node of RS and n is not recounted then  icount  n  i count  Join n s child into ne root set nRS  w If n.mps  mps  hen  t mps  n.mps  End if 2 3  4  5 If e  plink  mps  e  mps  en  th e  link  mps  e  mps  p Else  brea   k End if  Figure 10. Method thres_reset T    The final step is building nlink between header table and 10 11 12 13 14 15 16  17  18 the nodes, so we can trace all nodes with the same iname  19  End loop End if  End loop  If icount MIS    ount  mps then  ic Append i to  as    If icount MIS   then  Join  to sequential patterns set  End if  MSCP-growth nRS   MIS    End if through header table. In this example, the nlink of item e in the  header table is linked to \(_ e 1:11111001\ first, and the nlink of node \(_ e 1:11111001\in linked to next node with the same item name \(_ e 2:101\ in prefix order. The other nodes are also linked together by the same way. The algorithm of building nlink shows in Fig 11 and the complete PLMS-tree with part of node link value shows in Fig. 12   B. MSCP-growth  After the PLMS-tree is constructed, we develop the MSCP  growth algorithm to generate a complete set of sequential Figure 13. Algorithm 3: MSCP-growth - generate sequential patterns set   In line 2 through line 11, firstly, the algorithm counts each item i s support in  s conditional PLMS-tree \(the suffix tree of root set RS In PLMS-tree, the nodes with the same iname in the same path cannot be counted more than once since each item in the same sequence can be counted once only. Here, the binary code can be used to quickly identify the relationships between any two nodes in PLMS-tree. While cumulating icount for node n in line 4 through line 6, the algorithm records n s child node in a new root set nRS The nRS will be used to    181 


locate   s conditional PLMS-tree in the mining of sequence     s super-patterns. In line 7 through 9, we examine the mps of each node n  and record their minimum mps This value will be used in next step  In line 12 through line 18, secondly, we generate candidate  patterns and output sequential patterns according to each item's icount  Three conditions may occur here The first is that the icount satisfies MIS   This sequence is directly appended to icount 40% which reaches the MIS bde 30%, pattern bde   can be joined into the sequential patterns set. We can see that an infrequent pattern's super set become frequent pattern in this case that is why we need to consider about the  mps  value The other sequential patterns which are started from other items are generated in the same way Finally we can have the complete sequential patterns with MMS by MSCP-growth algorithm the sequential patterns set. The second is that the icount doesn't  V  C ONCLUTION satisfy MIS   but satisfies mps That is, the sequence is currently not a sequential pattern but its super-pattern i.e appending suffix in the sequence has possibility to be Therefore both two conditions have to keep growing and recursively call procedure MSCP-growth for further examining their super-patterns. The third is that the count of sequence doesn't satisfy both MIS   and mps It means that any super- pattern of the sequence   will never be a sequential pattern The procedure will terminate while the root set becomes empty  We use the constructed PLMS-tree in Fig.12 to explain the  mining process in detail At first the  RS  is root and we trace all nodes belong to the first item e in HT According to the nlink of  item e from HT we find \(_ e 1:11111001 n 1 at first, the  position code shows that n 1 is a descent node of RS and n 1 is not recounted in the same branch  n 1  icount  will be add into icount  since n 1 does not have any child node, no node will be  joined into nRS According to the nlink in \(_ e 1:11111001\, the  next node \(_ e 2:101 n 2 will be found n 2 is a descent node of  root and n 2 is not recounted n 2  icount will be added into icount  n 2  also has a child node \(_ c 2:1011\, that can be joined into the  nRS to be a new root set if we need to growth this pattern. And  the next node \(_ e 1:10111011\ is recounted so the node count of this node will not be added into icount Since the icount of In this study, we first address the rare item problem in the  mining of sequential patterns To generate more interesting patterns we extend the concept of MMS on sequential pattern mining and propose an algorithm named MSCP growth to discover complete set of sequential pattern with MMS In experimental study we will compare three well-known traditional methods including GSP PrefixSpan and PLWAP tree with MSCP-growth using several synthetic datasets as well as real-life datasets. Some research directions are briefly  discussed First users may have to tune items supports many times while discovering useful patterns Instead of restarting the whole mining process it is valuable if a support tuning mechanism can be developed Second the concept of MMS can be further applied to constraint-based sequential pattern mining such as time constraints or duration constraints    R EFERENCES item  e  is 60 which reaches the candidate growth condition we mentioned before, we use the nRS and e as a candidate pattern to generate new patterns Pattern  e  also reaches the threshold of sequential pattern i.e MIS e  and can be joined into the sequential patterns set. The second step uses the node in the nRS  _ c 2:1011\, as the new root set of conditional tree to growth new pattern\(s According to  HT  the  icount  of first item e  rooted by _ c 2:1011 is 20 which does not reach the candidate growth condition The  icount  of second item  c  in  HT rooted by _ c 2:1011 is 40 which reaches the condition to growth new candidate patterns so we use the child nodes of _ c 2:1011 _ a 1:10111\ and \(_ b 1:10111\, as nRS to generate new candidate patterns. This pattern also larger than MIS ec which means it can be joined into the sequential patterns set. The complete sequential patterns set with beginning item  e  can be generated by repeating these steps  The reason we use the mps value and candidate growth will  show in following example. We trace the item b now, the icount  of item b is 100% which reaches the condition to generate new candidate and the threshold to be joined into the sequential patterns set. As the tracing step goes, we have the icount of item d 60 in the suffix tree of pattern  b  it reaches the  mps  value=30%, which is the second condition to candidate growth we mentioned before, and it means that this pattern still has possibility to be part of sequential pattern Noted that it doesn't reach MIS bd 65 which means it is not a sequential pattern In the next candidate growth step, we have item e with 1     2    3     4     5      6     7     8      9   Y.-H. Hu and Y.-L. Chen, "Mining association rules with multipl  e minimum supports: a new mining algorithm and a support tuning  mechanism Decision Support Systems vol. 42, is. 1, pp.1-24, 2006  R. Agrawal and R. Srikant, "Mining sequential patterns ata  D Engineering \(ICDE'95 Taipe i, Taiwan, Mar, 1995, pp.3-14  J. Pei, J. Han, B. Mortazavi-Asl, and H. Zhu, "Mining access patterns  efficiently from web logs  Lecture notes in computer science  pp.396 407, 2000  C. Ezeife and Y. Lu, "Mining web log sequential patterns with position  coded pre-order linked wap-tree  Data Mining and Knowledge Discovery vol. 10, no. 1, pp.5-38, 2005  B. Liu, W. Hsu, and Y. Ma, "Mining association rules with multiple  minimum supports  Proceedings of the fifth ACM SIGKDD international conference  San Diego CA USA August 15-18 1999 p.341  K. Wang, Y. He, and J. Han, "Mining frequent itemsets using support  constraints  Proceedings of the 26 th  international conference on VLDB  2000, Cairo, Egypt, pp.43-52  Y. Lee, T. Hong, and W. Lin, "Mining fuzzy association rules with  multiple minimum supports using maximum constraints Lecture notes in computer science vol. 3014, pp.1283-1290, 2004  M. Tseng and W. Lin, "Mining generalized association rules with  multiple minimum supports  International Conference on Data Warehousing and Knowledge Discovery DaWaK'01  Munich Germany, 2001, pp.11-20  Y. Lee, T. Hong, and W. Lin, "Mining association rules with multiple  minimum supports using maximum constraints  International Journal of Approximate Reasoning vol. 40, pp.44-54, 2005      182 


s2,c2 from Xl sl,c1  s,C association rule X3 s,c  B. Post Mining o/Non-redundant Association Rules /or Sensor Data Estiamtion In this subsection, we describe our proposed technique for post mining of non-redundant association rules for sensor data estimation purpose. This algorithm is developed based on non-redundant informative association rules which means all rules cannot be derived from other rules and the left hand side and right hand side of the selected rules contain the input itemset. There are some research works that can generate the closed itemsets and corresponding association rules using generated closed itemsets [3 , 18 Based on our previous works [8] we assume to have the non-redundant association rules ready and the criteria of selecting the rules from non-redundant rules further reduce the number of rules to the minimum In this algorithm, dn.vm is treated as one item in association rules. Dn represents the sensor which is indexed as n. Vm represents the value of the sensor n is read as m. In the following figure, Xinput is the itemset represent the current round of sensor readings, it can be represented by a set of dn.vm pairs. Xl and X2 are the itemsets in left hand side and right hand side of association rules. Z represents all items in X2/Xinput. Index\(z z and value of sensor value pair z. Confidence{Xl=>X represents the confidence of the rule Xl=>X2 Support\(Xl=>X2 C represents the confidence of association rule, dn V represents the identifier and value pair of the sensor dn Xestimate represents the returned estimation itemset which contains the senor identifiers with missing values in the current round of readings of stream data and their corresponding estimated values. Sspecify represents the user specified support, and Cspecify represents the user-specified confidence The algorithm is described as below. The input of the procedure Estimate includes the current round of sensor reading, current association rule confidence, user-specified minimum support and user-specified minimum confidence 


In line 4, all association rules that satisfy with the conditions of minimum support and confidence are selected The confidence of the select association rules are calculated in line 5. The condition of line 6 filters out the rules whose left hand side itemset Xl does not contain the itemset Xinput in this case, sensor identifier is filtered based on the enriched contextual information. The condition of line 7 filters out the rules whose right hand side itemset X2 does not contain new estimated items. Each new item was processed in line 8 and 9 so that missing sensor values are V5-104 2010 2nd International Conference on Education Technology and Computer \(ICETC modified and updated. At the end of this procedure, an iterative call of the same procedure keep on searching and processing for the new association rules that are able to make contribution on the sensor value estimation Input: \( 1 contains missing values 2 3 Output: Xestimste: a set containing the senor ids with missing values in the current round of sensor readings and their corresponding estimated values Method 1 Xestirnste = <\\l 2 Cinput=l 3 Procedure Estimate\(XinpUb Cinpub Sspecify, Cspecify 4 for all \(rule: X,=>X2 and Support\(X,=>X2 Confidence\(X,=>X2 5 C = Conjidence\(X,=>X2 6 if \(X,EAinput 7 if\(X2\\Xinput*<\\l 8 for all \(ZEX2\\Xinput and zE Xestimste 9 Xestimste = Xestimste U z 10 n = index\(z 1 1  d., v= dn v +C*value\(z 12 end for 13 end if 14 Estimate\(X2, C, Sspecify, Cspecify 15 end if 16 end for 17 end procedure 


Figure 1. The post mining of non-redundant association rule for sensor data estimation IV. EXPERIMENTAL STUDY The performance of our proposed approach is studied by means of simulation. Several different simulation experiments are conducted in order to evaluate the proposed technique and compare it with the Average Window Size A WS linear trend approach, and with the WARM approach, the state-of-the-art data estimation algorithm in sensor databases using 2-frequent itemsets based association mining [6]. We compared the estimation accuracy, running time and memory space usage when applying different methods to the application dataset The dataset was collected in year 2000 at various locations throughout the city of Austin, Texas. The data represents the current location, the time interval, and the number of vehicles detected during this interval. All sensor nodes report to a single server. The sensors are deployed on city streets, collect and store the number of the vehicles detected for a given time interval. The vehicle counts taken as sensor readings that are used as input for our simulation experiments are traffic data provided by [2 A. Performance Study of Estimation Accuracy The evaluation of the estimation accuracy of the missing values is done by using the average Root Mean Square Error RMSE I'\(xa, -Xe 1 ___ l __ lI_?''? ____ _ numStates # estimations where Xa; and Xe; are the actual value and the estimated value, respectively; #estimations is the number of estimations performed in a simulation run; and numStates is the number of subsets, in which the actual readings are distributed The expression u  xa, -Xe estimations error and is an estimate of the standard deviation under the assumption that the errors in the estimated values \(i.e. Xai Xei 


see the smaller the RMSE, the better the estimation accuracy From Figure 2, we can see that the proposed technique gives the best average estimation result of the above approaches regarding the accuracy, followed by the WARM approach. The linear interpolation, A WS, and linear trend approaches perform no better than WARM and the proposed approaches. From Figure 2, we can also see that the proposed technique gives the best estimation result on the maximum estimation accuracy, which is the root square error for the maximum difference between the estimated and accurate values 0.6 0.5 0.4 w n 0.3 cz 0.2  Average 0.1  Maximum 0 l' ?? 0" </.,oQ Figure 2. Perfonnance study of average and maximum estimation accuracy for traffic monitoring application B. Performance Study of Running Time Figure 3 illustrates the running time in seconds of A WS linear interpolation, linear trend, WARM and proposed approaches. The experimental results show that in terms of running time, the WARM and proposed approaches are outperformed by A WS, linear interpolation and linear trend V5-105 2010 2nd International Conference on Education Technology and Computer \(ICETC approaches. The proposed approach is faster than the WARM technique 0.03 v 0.025 0.02 E U.uJ c 0.01 0.005 0 a  r 


r r r  9  Figure 3. Performance study of running time for traffic monitoring application in seconds C. Performance Study of Memory Usage Figure 4 illustrates the memory usage of A WS, linear interpolation, linear trend, WARM and proposed approaches in MB. The experimental results show that in terms of memory space, the WARM approach is outperformed by all the other four approaches. The results of the simulation experiments show that for 108 sensors the needed memory space using WARM is much higher than that using proposed approach. This is because the closed lattice data structure uses less memory space than the cube data structures, and it only stores the condensed closed itemsets information Figure 4. Performance study of memory usage for traffic monitoring application in MB V. CONCLUSTIONS Sensor stream applications are becoming very common with the advances in technologies for sensor devices. In this paper we propose a method to post mine non-redundant association rules, and used the result to produce missing data estimation in sensor network applications. The objective is to further reduce the resulting rules from non redundant association rule mining based on the users request, and apply the retrieved meaningful information to perform missing data estimation in wireless sensor networks We have evaluated the proposed technique with real data from a wireless sensor network of a traffic monitoring site Our proposed method is able to estimate missing sensor value with both time and space efficiency, and greatly improves the estimation accuracy. Our performance study 


shows that proposed post mining of association rule mining technique for missing sensor data estimation is an area worth to explore REFERENCES 1] Agrawal, R., & Imielinski, T., & Swami, A., "Mining association rules between sets of items in massive databases", International Conference on Management of Data, 1993 2] Austin, F. I., "Austin Freeway ITS Data Archive", Retrieved January 2003 from http://austindata.tamu.eduidefauIt.asp 3] Bastide, Y., & Pasquier, N., & Taouil, R, & Stumme, G., & Lakhal L., "Mining minimal non-redundant association rules using frequent closed itemsets", First International Conference on Computational Logic, 2000 4] Cool, A. L., "A review of methods for dealing with missing data The Annual Meeting of the Southwest Educational Research Association, 2000 5] Deshpande, A., & Guestrin C., & Madden, S., "Using probabilistic models for data management in acquisitional environments", The Conference on Innovative Data Systems Research, 2005 6] Halatchev, M., & Gruenwald, L., "Estimating missing values in related sensor data streams", International Conference on Management of Data, 2005 7] Iannacchione, V. G., "Weighted sequential hot deck imputation macros", Proceedings of the SAS Users Group International Conference, 1982 8] Nan Jiang, "Discovering Association Rules in Data Streams Based On Closed Pattern Mining", SIGMOD Ph.D. Workshop on Innovative Database Research, 2007 9] Li, Y., & Liu, Z. T., & Chen, L., & Cheng, W., & Xie, C.H Extracting minimal non-redundant association rules from QCIL The 4th International Conference on Computer and Information Technology, 2004 10] Little, R 1. A., & Rubin, D. B., "Statistical analysis with missing data", New York: John Wiley and Sons, 1987 II] McLachlan, G., & Thriyambakam, K., "The EM algorithm and extensions", New York: John Wiley & Sons, 1997 12] Mitchell, T., "Machine Learning", McGraw Hill, 1997 13] Papadimitriou, S., & Sun, 1., & Faloutsos, C., "Streaming pattern discovery in multiple time-series", The International Conference on Very Large Databases, 2005 14] Rubin, D., "Multiple imputations for nonresponce in surveys", New York: John Wiley & Sons, 1987 


15] Shafer, 1., "Model-Based Imputations of Census Short-Form Items In Proceedings of the Annual Research Conference, 1995 16] Taouil, R., & Pasquier, N., & Bastide, Y., & Lakhal, L., "Mining bases for association rules using closed sets", International Conference on Data Engineering, 2000 17] Wilkinson & The AP A Task Force on Statistical Inference, 1999 18] Zaki, M. 1., Hsiao, C. 1., "Efficient algorithms for mining closed itemsets and their lattice structure", IEEE Transactions on Knowledge and Data Engineering, 2005 V5-106 


General Chair f!!\f  Organizing Chairs  f!!\f  f$% \f!!\f  Organizing Co-chairs f    f  f\f   f\f\f   f*!\f!\f.\f  f f  Program Committee Chairs  f\f\f   f!!\f  Publication Chair 0   


200 250 300  The size of dataset/10,000 R es po ns e tim e S    a 0 50 100 150 200  The size of dataset/10,000 R es po ns e tim e S    b 0 10 20 30 40 50 


60  The size of dataset/30,000 R es po ns e tim e S    c Fig. 9 The scalability of our algorithm compared with FP-growth  Paper [12] proposed a way to reduce times of scanning transaction database to reduce the cost of I/O IV. CONCLUSIONS AND FUTURE WORK This paper first discusses the theory of foundations and association rules and presents an association rules mining algorithm, namely, FP-growth algorithm. And then we propose an improved algorithm IFP-growth based on many association rules mining algorithms. At last we implement the algorithm we propose and compare it with algorithm FPgrowth algorithm. The experimental evaluation demonstrates its scalability is much better than algorithm FP-growth 177 Now, lets forecast something we want to do someday Firstly, we would parallelize our algorithm, because data mining needs massive computation, and a parallelable environment could high improve the performance of the algorithm; Secondly, we would apply our algorithm on much more datasets and study the run performance; At last, we would study the performance when the algorithm deal with other kinds of association rules  REFERENCES 1] S. Sumathi and S. N. Sivanandam. Introduction to Data Mining and its Applications, Springer, 2006 2] V. J. Hodge, J. Austin, A survey of outlier detection 


methodologies, Artificial Intelligence Review, 2004, 22 85-126 3] Han, J. and M. Kamber. Data Mining: Concepts and Techniques. Morgan Kaufmann, San. Francisco, 2000 4] Jianchao Han, Mohsen Beheshti. Discovering Both Positive and Negative Fuzzy Association Rules in Large Transaction Databases, Journal of Advanced Computational Intelligence and Intelligent Informatics 2006, 10\(3 5] Jiuyong Li, Hong Shen, Rodney Topor. Mining Informative Rule Set for Prediction. Journal of Intelligent Information Systems, 2004, 22\(2 6] Jianchao Han, and Mohsen Beheshti. Discovering Both Positive and Negative Fuzzy Association Rules in Large Transaction Databases. Journal of Advanced Computational Intelligence, 2006, 10\(3 7] Doug Burdick, Manuel Calimlim, Jason Flannick Johannes Gehrke, Tomi Yiu. MAFIA: A Maximal Frequent Itemset Algorithm. IEEE Transactions on Knowledge and Data Engineering, 2005, 17\(11 1504 8] Assaf Schuster, Ran Wolff, Dan Trock. A highperformance distributed algorithm for mining association rules. Knowledge and Information Systems, 2005, 7\(4 458-475 9] Mohammed J. Zaki. Mining Non-Redundant Association Rules. 2004, 9\(3 10] J.Han, J.Pei, Y.Yin, Mining frequent patterns without candidate generation, Proceedings ACM SIGMOD 2000 Dallas, TX, May 2000: 1-12 11] P.Viola, M.Jones. Rapid Object Detection Using A Boosted Cascade of Simple Features. Proc. IEEE Conf. on Computer Vision and Pattern Recognition, 2001 12] Anthony K. H. Tung, Hongjun Lu, Jiawei Han, Ling FengJan. Efficient Mining of Intertransaction Association Rules. 2003, 154\(1 178 


For each vertex b in g form j forests body\(a, g, i s.t. bodyAnt\(a, g, i a, g, i with itemsets Ant\(b b and each subset of itemsets Ant\(b b in P\(a, g, j Assign to each leaf l of trees bodyAnt\(a, g, i bodyCons\(a, g, i a fresh variable Vm,M, m, M = size\(itemset\(l Assign to each leaf l of tree headAnt\(a, g, j the variable assigned to itemset l in some leaf of some tree bodyCons\(a, g, i TABLE II.  EXPERIMENTAL DATA Conf. #rules #pruned #dftrs PtC 0.5 6604 2985 1114 0.6 2697 2081 25 0.75 1867 1606 10 0.8 1266 1176 0 0.95 892 866 1 0.98 705 699 1 DSP 0.5 2473 1168 268 0.6 1696 869 64 0.75 1509 844 89 0.8 1290 1030 29 0.95 1032 889 15 0.98 759 723 1 Arry 0.5 770 492 82 0.6 520 353 60 0.75 472 327 39 0.8 408 287 22 0.95 361 255 25 0.98 314 243 30  Our induction algorithm has been launched for each combination of thresholds. Our scheme eliminates all redundant rules in the sense of [25, 31], i.e. those association rules that are not in the covers. All the meta-rule deductive schemes implicitly included in [25] and [31] are induced by our method. The percentage of pruning, thus, outperforms [25 


The results produced for k=3, support 0.25 and confidences between 0.7 and 0.99 are shown in Fig. 3, in terms of pruning percentage \(vertical axis when applied to low confidences \(from 0.7 to 0.9 The percentage of pruning achieved diminishes as the confidence is superior to 0.9. Nevertheless, the pruning is effective with confidence of 0.99 in the majority of cases Pruning at Support = 0.25 0,00 5,00 10,00 15,00 20,00 25,00 30,00 35,00 40,00 45,00 50,00 0,7 0,8 0,9 0,95 0,99 Confidence P ru n in g L e v e l Case 1 Case 2 Case 3  Figure 3.  Pruning experiences at support 0.25  V. DISCUSSION AND CHALLENGES It is important to discuss the technique presented here with focus on the purpose the technique pursues:  to produce semantic recommendation The reader should have noticed that the algorithm presented 


relies strongly on "choice". For instance, the algorithm chooses ears in the graph to form an order for elimination, and the choice is arbitrary. This strategy is essential to maintain low complexity \(polynomial practical. Nevertheless, a warned reader may conclude that this arbitrary choice implies that there are many compactions to produce and therefore the approach as a whole does not show to produce an optimal solution. And the reader is right in this conclusion. Since the goal is compaction, the search for an optimal solution can be bypassed provided a substantial level of pruning is achieved To complete the whole view, we describe how web service descriptions are complemented with the association rules as recommendations. In effect, under our scheme, the document describing the web service is augmented with a set of OWL/RDF/S triples that only incorporate the non-pruned rules with the format of Example 1, that is, the set ARmin of the compaction program obtained by our algorithm, together with the thresholds applied to the mining process and a registered URI of a registered description service. The assumptions and defeaters are not added to the web service description. If the associations encoded in the triples are not sufficient for the client \(a search engine, for instance widening of the response to the description service identified by the given URI, and then the assumptions and defeaters are produced. The reasoning task required for deriving all the implicitly published rules is client responsibility Notice that, under this scheme, the actual rules that appear as members of the set initial ARmin set are irrelevant; the only important issue is the size of the set The developed scheme also supports an extension of the algorithm that admits the assignment of priorities to rules and to itemsets, in order to allow the user to produce a more controlled program as output. Nonetheless, the importance of the extension has not been already tested, and therefore it is beyond the subject of the present paper It would be also interesting to design a scheme that supports queries where the client provides an itemset class and values for support and confidence and the engine produces a maximal class of inferred associated itemsets as a response. This scheme is also under development, so we have not discussed this aspect here 


VI. CONCLUSION In this paper, we have presented a defeasible logic framework for managing associations that helps in reducing the number of rules found in a set of discovered associations. We have presented an induction algorithm for inducing programs in our logic, made of assumption schemas, a reduced set of association rules and a set of counter-arguments to conclusions called defeaters, guaranteeing that every pruned rule can be effectively inferred from the output. Our approach outperform those of [17], because all reduction compactions presented there can be expressed and induced in our framework, and several other patterns, particular to the given datasets, can also be found. In addition, since a set of definite clauses can be obtained from the induced programs, the knowledge obtained can be modularly inserted in a richer inference engine Abduction can be also attempted, asking for justifications that explain the presence of certain association in the dataset The framework presented can be extended in several ways Admitting defeaters to appear in the head of assumption, to define user interest Admitting arithmetic expressions within assumptions for adjustment in pruning Admitting set formation patterns as itemset constants Extending the scope, to cover temporal association rules REFERENCES 1]  R. Agrawal, and R. Srikant: Fast algorithms for mining association rules In Proc. Intl Conf. Very Large Databases. \(1994 2]  A. V. Aho, J. E. Hopcroft, J. Ullman. The design and analysis of computer algorithms, Addison-Wesley, 1974 3]  G. Antoniou, D. Billington, G. Governatori, M. J. Maher, A. Rock: A Family of Defeasible Reasoning Logics and its Implementation. ECAI 2000: 459-463 4]  G. Antoniou, D. Billington, G. Governatori, M. J. Maher: Representation results for defeasible logic. ACM Trans. Comput. Log. 2\(2 2001 5]  A. Basel, A. Mahafzah, M. Al-Badarneh: A new sampling technique for association rule mining, Journal of Information Science, Vol. 35, No. 3 358-376 \(2009 6]  R. Bayardo and R. Agrawal: Mining the Most Interesting Rules. In Proc of the Fifth ACMSIGKDD Intl Conf. on Knowledge Discovery and Data Mining, 145-154, \(1999 


7]  R. Bayardo, R. Agrawal, and D. Gunopulos: Constraint-based Rule Mining in Large, Dense Databases. Data Mining and Knowledge Discovery Journal, Vol. 4, Num-bers 2/3, 217-240. \(2000 8]  A. Berrado, G. Runger: Using metarules to organize and group discovered association rules. Data Mining and Knowledge Discovery Vol 14, Issue 3. \(2007 9]  S. Brin, R. Motwani, J. Ullman, and S. Tsur: Dynamic itemset counting and implication rules for market basket analysis. In Proc. ACMSIGMOD Intl Conf. Management of Data. \(1997 10] L. Cristofor and D.Simovici: Generating an nformative Cover for Association Rules. In ICDM 2002, Maebashi City, Japan. \(2002 11] Y. Fu and J. Han: Meta-rule Guided Mining of association rules in relational databases. In Proc. Intl Workshop on Knowledge Discovery and Deductive and Object-Oriented Databases. \(1995 12] B. Goethals, E. Hoekx, J. Van den Bussche: Mining tree queries in a graph. KDD: 61-69. \(2005 13] G. Governatori, D. H. Pham, S. Raboczi, A. Newman and S. Takur: On Extending RuleML for Modal Defeasible Logic. RuleML, LNCS 5321 89-103. \(2008  14] G. Governatori and A. Stranieri. Towards the application of association rules for defeasible rules discovery In Legal Knowledge and Information Systems, JURIX, IOS Press, 63-75. \(2001 15] J. Han, J. Pei and Y. Yin: Mining frequent patterns without candidate generation. In Proc. ACM-SIGMOD Intl Conf. Management of Data 2000 16] C. Hbert, B. Crmilleux: Optimized Rule Mining Through a Unified Framework for Interestingness Measures. DaWaK: LNCS 4081, 238247. \(2006 17] E. Hoekx, J. Van den Bussche: Mining for Tree-Query Associations in a Graph. ICDM 2006: 254-264 18] R. Huebner: Diversity-Based Interestingness Measures For Association Rule Mining. Proceedings of ASBBS Volume 16 Number 1, \(2009 19] B. Johnston, Guido Governatori: An algorithm for the induction of defeasible logic theories from databases. Proceedings of the 14th Australasian Database Conference, 75-83. \(2003 20] P. Kazienko: Mining Indirect Association Rules For Web Recommendation. Int. J. Appl. Math. Comput. Sci., Vol. 19, No. 1, 165 186. \(2009 21] M. Klemettinen, H. Mannila, P. Ronkainen, H. Toivonen, and A Verkamo: Finding interesting rules from large sets of discovered association rules. In Proc. 3rd Intl Conf. on Information and Knowledge 


Management. \(1994 22] M. J. Maher, A. Rock, G. Antoniou, D. Billington, T. Miller: Efficient Defeasible Reasoning Systems. International Journal on Artificial Intelligence Tools 10\(4 2001 23] C. Marinica, F. Guillet, and H. Briand: Post-Processing of Discovered Association Rules Using Ontologies. The Second International Workshop on Domain Driven Data Mining, Pisa, Italy \(2008 24] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal: Closed sets based discovery of small covers for association rules. In Proc. BDA'99 Conference, 361-381 \(1999 25] N. Pasquier, R. Taouil, I. Bastide, G. Stume, and  L. Lakhal: Generating a Condensed Representation for Association Rules. In Journal of Intelligent Information Systems, 24:1, 29-60 \(2005 26] P. Pothipruk, G. Governatori: ALE Defeasible Description Logic Australian Conference on Artificial Intelligence.  110-119 \(2006 27] J. Sandvig, B. Mobasher Robustness of collaborative recommendation based on association rule mining, Proceedings of the ACM Conference on Recommender Systems \(2007 28] W. Shen, K. Ong, B. Mitbander, and C. Zaniolo: Metaqueries for data mining. In Fayaad, U. et al. Eds. Advances in Knowledge Discovery and Data Mining. \(1996 29] I. Song, G. Governatori: Nested Rules in Defeasible Logic. RuleML LNCS 3791, 204-208 \(2005 30] H. Toivonen, M. Klemettinen, P. Ronkainer, K. Hatonen, and H Mannila: Pruning and grouping discovered association rules. In ECML Workshop on Statistics, Machine Learning and KDD. \(1995 31] M. Zaki: Generating Non-Redundant Association Rules. In Proc. of the Sixth ACMSIGKDD Intl Conf. on Knowledge Discovery and Data Mining, 34-43, \(2000 32] w3c. OWL Ontology Web Language Reference. In http://www.w3.org/TR/2004/REC-owl-ref-20040210 33] w3c. RDF/XML Syntax Specification. In: http://www.w3.org/TR/rdfsyntax-grammar 34] w3c. RDF Schema. In: http://www.w3.org/TR/rdf-schema      


 8   2  3\f            8  D    F  \b 1 8 & #J      b 1  1  4    2  


4 1    9  E 1  2 4 1    9 1   4      8 2  8 1  D 1        1 1  b 


     b b b b b  K            8          2 D 9   F  \b 1 8 ,+J  9 


     b 1     1 2  9 1  12 L 1   9  8       1  2      2   


     b b b b b  K            2  0 \b f  b\f      9       


  8 2   E 1   1     M13 31L 1    b  8E 1   1 #3\b?### 1  1     E 1   1 \b?###3        


1   1   b 1  2 2 18 2     8              1    2 \b 1    2  


    2          2   1 L 2 1   1   L 2 2    2 1  2        


    8  2H D \b A             2  2H D \b A 2 \f 3%\f  f   4%\f f !  , \f\b  C    2    2 


 6    3 1      253 6   1 L 2    6   1         f\b3\f       


               1     1     8 2    E       2  1   


     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


