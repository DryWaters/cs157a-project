Partitioning large data to scale up lattice-based algorithm Huaiguo Fu Engelbert Mephu Nguifo CRIL-CNRS FRE2499 Universit e dêArtois Ruedelêuniversit e SP 16 62307 Lens cedex France  fu,mephu  cril.univ-artois.fr Abstract Concept lattice is an effective tool and platform for data analysis and knowledge discovery such as classiìcation or association rules mining The lattice algorithm to build formal concepts and concept lattice plays an essential role in the application of concept lattice We propose a new efìcient scalable lattice-based algorithm ScalingNextClosure 
to decompose the search space of any huge data in some partitions and then generate independently concepts or closed itemsets in each partition The experimental results show the efìciency of this algorithm 1 Introduction Concept lattice structure 6 8 h as sh o w n t o b e a n effective tool for data analysis and knowledge discovery It has been applied to machine learning data mining and information retrieval etc Concep t lattice can derive conceptual structures from data It studies how objects can be hierarchically grouped together according to their common attributes It can generate formal concepts from the data to 
reveal the relations between objects and attributes So concept lattice is a natural framework for data mining Its characteristics are very suitable for data mining For example a closed itemset or the intent of a formal concept is a maximal itemset for association rules 11 So the problem of nding frequent itemsets from data for association rules can be reduced to nding frequent closed itemsets with closed itemset lattice or concept lattice The lattice algorithm to build formal concepts and concept lattice plays an essential role in the application of con 
cept lattice Several algorithms were proposed to generate concepts or concept lattices of a data context for example Bordat Ganter Ne xtC los ure algorithm 5  C h ein 3 Norri s  9   G odi n  7  and Nouri ne 10  et c Experi mental comparisons of performance of existing algorithms show that NextClosure algorithm is the best for large and dense data 8 4 Bu t t h e p r o b l em is th at it still tak es v e ry h i g h expensive time cost to deal with huge data So in this paper we propose a new efìcient lattice-based algorithm Scal 
ingNextClosure that decomposes the search space of any huge data in some partitions and then generates independently concepts or closed itemsets in each partition The new algorithm is a kind of decomposition algorithm of concept lattice All existing decomposition algorithms for generating concept lattices use an approach of context decomposition that are different from ours Our new algorithm uses a new method to decompose the search space It can freely decompose the search space in any set of partitions if there are concepts in each partition and then gener 
ate them independently in each partition So this algorithm can be used to analyze huge data and to generate formal concepts Moreover for this algorithm each partition only shares the same source data data context with other partitions ScalingNextClosure algorithm shows good scalability and can be easily used for parallel distributed network and partial computing The experimental comparison with NextClosure algorithm shows that our algorithm only sequential computing is better for large data Our algorithm succeeds in computing some large data in worst case that are impossible to be 
computed with another algorithm The rest of this paper is organized as follows The basic notion of concept lattice will be described in the next section The new algorithm ScalingNextClosure will be presented in section 3 In section 4 the performance of the new algorithm will be shown The paper ends with a short conclusion in section 5 2 Concept lattice The theoretical foundation of concept lattice relies on the mathematical lattice theory 1 6 Concept lattice is used to represent the order relation of concepts Concept lattice de 
scribes the character of the set pair intent and extent of concept Deìnition 2.1 Data context is deìned by a triple         where  and M are two sets and R is a relaProceedings of the 15th IEEE International Conference on Tools with Artificial Intelligence \(ICTAIê03 1082-3409/03 $17.00 © 2003 IEEE 


tion between  and   The elements of G are called objects or transactions while the elements of M are called attributes or items Deìnition 2.2 Given a subset    of objects from a data context         we deìne an operator that produces the set   of their common attributes for every set    of objects to know which attributes from M are common to all these objects          for all      Dually we deìne   for subset of attributes       denotes the set consisting of those objects in  that have all the attributes from            for all      These two operators are called the Galois connection for         These operators are used to determine a formal concept So if  is an attribute subset then   is an object subset and then      is an attribute subset We have          Correspondingly for object subset   we have          Thus we deìne two closure operators as     for set  and     for set   Deìnition 2.3 A formal concept of the data context       is a pair     with            and      A is called extent B is called intent Deìnition 2.4 If        and        are concepts       or        then we say that there is a hierarchical order between        and         All concepts with the hierarchical order of concepts form a complete lattice called concept lattice  3 ScalingNextClosure algorithm Analyzing all concepts of a data context and their search space we deìne the Ordered data context and analyze the property of the ordered data context in order to decompose the search space Deìnition 3.1 A data context is called ordered data context if the attributes are ordered by number of objects of each attribute from the smallest to the biggest one In other words the smallest attribute is in rst column and the biggest one is in the last column and the attributes with the same objects are merged as one attribute Proposition 3.1 An ordered data context has the same concept lattice as the data context Proof  By the deìnition of ordered data context G M and R arenêt changed so the data context does not change by the preprocessing step There is a unique concept lattice for any given context[6 Itês the precondition of ScalingNextClosure algorithm to transform a data context to the ordered data context It is easy to generate the ordered data context We propose a new method to divide the search space of concepts into partitions For each partition we can nd all concepts in it This is the principle of our ScalingNextClosure algorithm In following section we will present the main idea of ScalingNextClosure algorithm and explain why and how we can divide the search space into partitions and then generate concepts in each partition 3.1 The search space for concepts Intent and extent of a concept are bijection so we can only study the search space of intent or extent of concepts instead of search space for concepts In fact any concept intent is a attribute subset of   so all subsets of  are elements of the search space So the size of search space for enumeration of all concept intents is    This search space can be considered as the fold of some attribute subsets of   For example we consider that the search space is formed by 012 where 012  is all subsets of  015    015     015     015     015   that include 015   Each 012 is a search sub-space The whole search space will be decomposed according to the situations of such 012 Herewe deìne Folding set and Folding search sub-space in order to decompose the search space Deìnition 3.2 The attribute set  of a data context        is  015    015    015     015     015    an attribute 015     the set    is called folding set of 015   where      015     for all 015          In other words the folding set of 015  is the set of  015   015   015    015    For example the folding set of 015  is   For attribute 015     its folding set is  015    015    015    Deìnition 3.3 An attribute joins respectively with all subsets of its folding set to generate the new attribute subsets these new attribute subsets form a search sub-space of concepts that is called folding search sub-space of an attribute\(F3S  For example F3S of 015    is  015       015     015   F3S of 015  is all subsets of  015   015     015     015   that include 015   Proposition 3.2 For a data context          015     the number of attributes of M is   if the number of objects of attribute 015  is   then the folding search sub-space  for concepts  of 015  is the minimum of   and      Proof   015     The folding set of 015  is  015    015      015     015     015   ithas  012  attributes So the size of Proceedings of the 15th IEEE International Conference on Tools with Artificial Intelligence \(ICTAIê03 1082-3409/03 $17.00 © 2003 IEEE 


subset of the folding set of   is        can be assembled with     attribute subsets to form new attribute subsets On the other hand if the number of objects of attribute   is n we have   object subsets corresponding to   Forany concept itês a bijection between concept and corresponding object set So there are at most   concepts that include attribute    Thus the folding search sub-space of   is the minimum of   and      According to this proposition we order the data context with the number of objects of each attribute In practice this arrangement can remarkably reduce the search space for real data In the deìnition of ordered data context we need to merge the attributes with exactly the same objects as one attribute The important reason for this is that we need to completely ensure that there are concepts in the folding search sub-space of an attribute Itês one important precondition of the following proposition Proposition 3.3 For an ordered data context it exists concepts in the folding search sub-space of an attribute Proof  For an ordered data context              and         wehave            so      is in the folding search sub-space of attribute   otherwise                    This property of ordered data context allows us to nd partitions that include some search sub-space 3.2 A scalable algorithm ScalingNextClosure We propose a new algorithm ScalingNextClosure which decomposes the search space and builds all concepts of each search sub-space For each search sub-space we use the same method NextClosure algorithm to generate the concepts So we can generate all concepts of each search subspace in parallel as the search sub-spaces are independent ScalingNextClosure algorithm has two steps determining the partitions see Algorithm 1 and generating all concepts of each partition see Algorithm 2 For the rst step of the algorithm determining the partitions we can decide the size of partition by a parameter  of our algorithm according to the size of data and our needs For the real data we can give a value of         is used to determine the position of the beginning and the end of each partition We choose some attributes of ordered data context to form an order set   If the number of the elements of  is 012 wehave                     we denote           is the search space from attribute    to attribute     for ordered data context From            is the rst subset of           we generate the next concepts until        so we can nd all concepts between           Algorithm 1 The rst step of ScalingNextClosure algorithm determining the partitions 1 input a parameter      2 generate the ordered data context saving the order of attributes for ordered data context in an array 3 output the order of attributes of the ordered data context 4   cardinal of the attribute set of the ordered data context 5 015   6    7 while  015   do  determining partition  8   9    015 10 output   11 015  015  015    12 end while 13 012    012 is the number of the partitions All concepts non-empty of data context are included in                    We use all   to form the partitions          and      where     012 Here   means the position of an attribute of the ordered data context and we use it to represent the attribute    of the ordered data context    doesnêt represent the attribute of data context When we search the concepts  isnêt considered For each partition we compute the next concepts from      to        There is no relation between each partition The partitions only share the same source data We can deal with any partition independently So we can apply this algorithm for parallel distributed and network computing We use the principle of NextClosure algorithm to generate concepts in each partition The principle of NextClosure algorithm uses the characteristic vector which represents arbitrary subsets  of   to enumerate all conAlgorithm 2 The ScalingNextClosure algorithm to nd all concepts in each partition 1 input the order of attributes of the ordered data context 2 input   and    input the partition 3  012     4   012     5    6 while 012   do 7  012 generate the next closure of  for the ordered data context 8 if     when searching the next closure then 9     10 end if 11 end while Proceedings of the 15th IEEE International Conference on Tools with Artificial Intelligence \(ICTAIê03 1082-3409/03 $17.00 © 2003 IEEE 


cepts for data context         Attribute subset                          is the closure operator The lectically smallest attribute subset is    The NextClosure algorithm proved that if we know an arbitrary attribute subset   the next concept the smallest one of all concepts that is larger than   with respect to the lexicographical order is     where  is deìned by                           and        being the largest element of  with     by lexicographical order In other words for        from the largest element to smaller one of     we calculate      until we nd the rst time     then     is the next concept Here we show an example of using ScalingNextClosure algorithm to nd all concepts First we need not to generate a data le for ordered data context the order of attributes is only stored in the main memory The ordered attribute set of the ordered data context for this example is                  And then we give a value of the parameter to determine the partitions for example       We use ScalingNextClosure algorithm to get 4 partitions                     and    In the end we nd all concept intents in each partition Ordered attributes                                                            012      012      012      012     The partitions and their search space                                                                                                                                                                                3.3 ScalingNextClosure for worst case data For the worst case a different technique can be used to generate partitions in order to avoid a great unbalance of partitions in terms of number of concepts The worst case appears when the sizes of  and  are equal to  and each attribute is veriìed by  012  different objects each object possesses  012  different attributes We can redecompose the search sub-space of an attribute into partitions so that itês easy to deal with for each partition according to the number of concepts per partition The aim is to decrease the complexity of each partition 4 Experimental results We have implemented our algorithm in Java Preliminary results of our implementation on a PIII450 computer with 128Mo RAM show that our algorithm has efìcient performance It can deal with huge data and the total time of computing of all partitions for large data is lower than that of the NextClosure algorithm We have tested our algorithm with the datasets of the UCI repository The comparison with NextClosure algorithm shows that our algorithm only sequential computing is better for large and dense data The experimental results show that the algorithm h as high performance for very large data The total time cost of all partitions is remarkably lower than that of NextClosure algorithm For example see Figure 1 for the large data of UCI agaricus with 8124 objects and 124 attributes NextClosureês time cost is 60 times higher than that of ScalingNextClosure Some data have a large amount of attributes and itês very hard to deal with them with NextClosure but ScalingNextClosure is very efìcient in this case                                                 The 11 data contexts     1 soybean-small 47,79 2 SPECT 187,23 3 are2 1066,32 4 audiology\(26,110 5 breast 699,110 6 lung-cancer 32,228 7 promoters 106,228 8 soybeanlarge 307,133 9 dermatology 366,130 10 nursery 12960,31 11 agaricus 8124,124 Figure 1 Performance of comparison for Next Closure and ScalingNextClosure algorithms on UCI data The time cost\(in milliseconds is represented by LN\(Time For our experiments we have used ScalingNextClosure algorithm to generate various different partitions for some Proceedings of the 15th IEEE International Conference on Tools with Artificial Intelligence \(ICTAIê03 1082-3409/03 $17.00 © 2003 IEEE 


datasets For example we can build different partitions according to the size of data Figure 2 shows the results of comparisons with different values of parameter  Given a bigger value we can build more partitions Varying different values of  can affect the result of our algorithm as it is the case with Agaricus dataset How many partitions and what partition should we create for the best performance We will try to nd an answer to this question in our future work  Figure 2 Performance of comparison on different values of parameter  with 4 test data We have tested our algorithm in the worst case and it succeeds in computing some large data that were impossible to be computed with other algorithms For example the worst case with 30 attributes is very hard to compute with other algorithms 4  Usin g S calin g N e x tClo su r e alg o r ith m  we have generated all concepts for worst case data sets with 24 25 30 35 and 50 attributes 5 Conclusion As an effective tool of data analysis and knowledge discovery concept lattice has been applied for classiìcation and association rules mining etc Itês the essential task to generate concepts and concept lattice in the application of concept lattice The existing lattice algorithms are difìcult to deal with large data The search space of concepts is very large for large data Itês a hard problem to determine all the concepts of large data We study the search space of concepts in order to partition the search space to scale up lattice algorithm In this paper a new efìcient scalable lattice-based algorithm ScalingNextClosure is proposed for creating the partitions of the search space and building concepts in each partition ScalingNextClosure is different from other existing decomposition algorithms that generate concept lattice using the approach of context decomposition 12 which is based on an incremental approach The experimental results show that ScalingNextClosure algorithm is very suitable and scalable to deal with large data For the ongoing research we will parallelize ScalingNextClosure in order to improve its performance Futhermore we will extend our method to classiìcation and association rules mining Acknowledgements We are grateful to anonymous reviewers for helpful comments and to Corine Zimny for english proofreading This research beneìts from the support of the region Nord/Pas de calais References 1 G  B i r khof f  Lattice Theory  American Mathematical Society Providence RI 3rd edition 1967 2 J  B o r d a t Calcu l p ratiq u e d u treillis d e g a lo is d  u n e co rrespondance Math ematiques Informatiques et Sciences Humaines  24\(94\:31Ö47 1986  M Chei n Al gori t h me de recherche des sous mat ri ces premi eres dêune matrice Bull.Math.R.S  13 1969 4 H  F u a n d E M e p h u Ng u i fo  H o w well g o lattice alg o rith ms on currently used machine learning testbeds ICFCA 2003 First International Conference on Formal Concept Analysis  2003  B Gant er  T w o basi c a l gori t h ms i n concept anal ysi s T echnical report Darmstadt University 1984 6 B  G an ter an d R  W ille Formal Concept Analysis Mathematical Foundations  Springer 1999  R  G odi n G Mi neau and et al  M  ethodes de classiìcation conceptuelle bas es sur les treillis de galois et application Revue dêintelligence artiìcielle  pages 105Ö137 1995 8 E  M ephu Ngui f o  M  L i qui er e and V  Duquenne Journal of Experimental and Theoretical Artiìcial Intelligence JETAI Special Issue on Concept Lattice-based theory methods and tools for Knowledge Discovery in Databases Taylor and Francis 2002  E  Norri s An al gori t h m f or comput i n g t he maxi mal rect angles in a binary relation Revue Roumaine Math Pures et Appl  XXIII\(2  1978  L  Nour i n e a nd O R a ynaud A f ast al gor i t h m f or b u i l d i n g lattices Information Processing Letters  71:199Ö204 1999  N Pasqui er  Y  B ast i d e R  T a oui l  and L  L akhal  E f  ci ent mining of association rules using closed itemsets lattices Journal of Information Systems  24\(1\:25Ö46 1999  P  V a l t c he v a nd R  Mi ssaoui  B ui l d i n g g al oi s  concept  l at tices from parts Generalizing the incremental approach In In Proceedings of the ICCS 2001 Harry Delugach Gerd Stumme eds LNCS 2120  pages 290Ö303 Springer Verlag 2001 Proceedings of the 15th IEEE International Conference on Tools with Artificial Intelligence \(ICTAIê03 1082-3409/03 $17.00 © 2003 IEEE 


Generalisation The attribute value has heen replaced by the higher level concept that retains the same set of characteristics hut possibly with larger support Contradiction The attribute value is in contradiction with another potentially pre-defined as a belief For example auser may he allocated a parlicularcomputer but the profile indicates the use of a different one The recognition of the occurrences of these differences may be automated Some may be combined repetition or discarded as unimportant generalisation or trivial beliefs Others may require inspection by investigators to decide if they are worth following up. Algorithm 2 Irem$er itemxet fame describes the calculation of a metric that indicates the closeness or similarity of two k-itemsets by comparing their elements It employs the attr function defined prior to presenting Algorithm 1 and assumes that the itemsets to be compared are represented by bits from the hit-vector U format defined there Because of the consecutiveness re quirement it follows that the hits at position o in a k-itemset he in three distinct relationships 1 They may belong to different attnbutes A  A 2 They may belong to the same attribute A and he the same attribute value or concept or have a child-parent relationship 3 They may belong to the same attribute A but he dif ferent valueslconcepts with no relationship Algorithm 2 IS2IS-dist Inputs K-itemsets I  b     bi and Ij  g  q an m x m concept relationship bit-matrix C attribute function attr\(b Outputs Distance d E 0     k  11 1 Initialised  0 2 Foro  1 to k 2.1 If attr\(bp  by set d  k  1 and stop 2.2 If bp  b A b not in child-parent relationship with by increment d It can be seen that distance d of Algorithm 2 can he less than the length of the itemset k only if the same attribute valuelconcept is found duplicated \(i.e. equals or is in a con cept relationship with at least once in the two itemsets he ing compared It also follows that the total number of such duplicates found and d equals k with d  0 only if the re spective elements of the two itemsets are the same or are in a concept relationship Thus the metric is a non-negative in Figure 2 Example time slice of past and cur rent user login information as obtained by ex ecuting the UNIX last command hold nil values for attributes not originally in the itemset then comparing this itemset with the data record the same way as comparing two I-itemsets The difference between Algorithm 2 and this modified version is that d is not in cremented for attributes where the itemset holds a nil value This limits d to a maximum value of k 5 Data Experiments and Results To evaluate the profiling methodology proposed in this paper, a number of experiments have been performed Both Algorithms 1 and 2 have been implemented as well as IS2DAT-dist As input log files captured by executing the UNIX last command were used which searches the wtmp system log file and lists past and current user login informa tion for a computer An example output from executing the last command is shown in Figure 2 Note that the data used in our experiments are actual log data recorded by a UNIX-based computer set up as a server with remote login access However in order to preserve anonymity the data attribute name instances have been modified Furthermore there was no implication of inappropriate behaviour in the data set Of several columns of information generated six at tributes were copied or composed into a table containing formatted input. Some filtering was performed at this stage to remove incomplete current and non-user e.g shut down logins The table, using additional higher level con cepts from attribute hierarchies was then mined to produce a profile containing association rules. Intra-profile and data to-profile contrasting was then performed The distance metric of IS2IS-dist and IS2DAT-dist was employed to produce reports for both contrasting methods 5.1 Intra-Profile Experiments Intra-profile contrasts were calculated only for itemsets teger 2 E 0    k 11 from which only values 0 d  k    of the same length For example in one test from about 2000 original data records approximately 2200 itemsets are or interest A similar algorithm can be devised to calculate the distance hetween a k-itemset and a data record IS2DAT with than element were produced Intra-profile con dist This can he achieved by expanding the itemset trasting produced roughly 43000 distances that were less This algorithm is not presented due to its similarity to AlgonUlm 2 than the lengths of the itemsets being compared Although 16 


this is a far smaller number than what it potentially could have been it is still more than what can be perused manu ally To rcduce this set further, additional strategies need to be devised One option is to prioritise attributes That ih if difference is measured only in a particular attribute that may not be carrying imponant information \(such as day of the week then pairs exhibiting distance only in such at tributes may be dropped Similarly a strategy may be em played to drop contrasts that are too high That is the distance metric for a particular itemset length may be re garded as high even though it satisfies the initial constraint of being less than the length This may for example render all distances produced for 2-itemsets unnecessary Finally focusing techniques may be provided to filter the distances for certain attributes or attribute values One of the more interesting contrasts produced by IS2IS-dist during testing was the I-distance pair io  User  pedru A Origin  viiunli 11  User  pedro A Origin  adeluide which indicates that the same user has been logging in from two very different geographic locations Further inspection of this contrast revealed that the user in question left his place of work in Adelaide for another in Miami while still regularly accessing his old Adelaide account 5.2 Data-to-Profile Experiments The filtering requirement to reduce the set of distances to manageable proportions becomes even more evident with data-to-profile contrasts Without pre-processing each itemset needs to be compared to every data record po tentially producing a much larger result set than for intra profile contrasts This is partly due to the fact that a number of records are not included in the profile due to unsatisfac tory support Each of these records could produce small dis tances to itemsets similar to it that made it into the profile As in the case of intra-profile contrasts measures can be taken to reduce the final result set In addition to the strate gies outlined in Section 5.1 duplicate records may be re moved by post-processing the results Also data-to-protile distances may be zero if a particular data record was one of those used to generate the itemset it is being compared to These distances should also be pruned from the results Figure 3 shows some of the distances from a test calcu lated for a particular itemset of length 5 top row Non zero distances up to a maximum value of 2 were allowed in order to list contrasts where difference is present in not too many attributes Duplicates were removed and as men tioned some attributes were sanitised to remove contiden tial information from the data The itemset contains gen eralised concepts for both the User and Origin attributes while Durarion is represented by concepts categorising a Figure 3 Example data-to-profile distances from ISZDAT-dist for a sample profile element and a collection of data records, ordered by User for readability potentially large number of discrete values From the def inition of the metric valueslconcepts in the same hierar chy have a distance of zero, which explains the diversity of rows of \(non-generalised values in the data having similar distances For readability we give here some of the concept relationships from the otherwise rather large hierarchies that exist for User and Origin mar,milo,pedro stuort c lecturer cs.x?/u.edu.au 188.191.47 c cs.xyu.edu.au C zyu.edu.au c adelaide  tnt2.tow.net.au 198.twun0103.twn.net.ou C ISP.adelaide c adelaide Using this information the first data row with d  1 shows that user Clyde is not a lecturer whilst for lecturers viuz and pedru who log onto university computers we can ob serve that the same wtmp login information is valid for sev eral weekdays other than Monday Figure 3 as is contains superfluous information De pending on the support used in mining the profile some or most of the data records contribute to itemsets gener ated by the algorithm Comparing a k-itemset to data that contributes to another k-itemset is a repetition of compar ing an itemset with another Crosschecking a data record against every other k-itemset prior to calculating a distance would, however, be even less cost-effective Instead a strat egy of producing distances in a matrix form for k-itemsets k  2   1 then discarding rows with at least one zero in it would be a better solution Alternatively a separate algorithm may parse the data set to locate individual occur rences of records that do not contribute to any itemset of a given length and then run the contrasting algorithm against this filtered data set only This is indeed the requirement proposed in Section 4.2 for data-to-profile contrasting 17 


6 Conclusions and Future Directions The initial implementation of the profiling analysis pro cess described in this paper has resulted in promising results capable of identifying irregularities in computer logs that can serve as useful evidence in computer crime investiga tions Protile analysis, however forms only a pan of the in vestigative process and relies heavily on expert knowledge It is therefore best perceived as a component in a larger col lection of tools designed to aid the forensic investigator The profiling tool presented in this paper presents further opportunities for enhancement One such area is the han dling of multiple log information in a single process Multi dimensional mining may offer a solution for this problem with some interesting work already found in the literature 16 201 Alternatively it may be possible to 223flatten\224 sev eral logs into a sequence of 223events\224 for which more tradi tional sequential mining techniques can be applied Further improvements may be achieved by replacing the mining 81 gorithm used in protiling One obvious candidate is the attribute-oriented induction technique  141 This technique compacts a collection of records into a generalised relation or a conjunction of generalised records where individual at tribute values are replaced by higher level concepts by as cending concept hierarchies One of the advantages of this technique is that the final rule set incorporates information about every record in the original data set Further work is also to be carried out in the intelligent presentation of results notably in the provision of appropriate visual inter pretation of the profiles and its potential contrasts. Contrast measures currently used are itemset-specific Deriving dis tance measures for rules such as the value distance metric VDM 221 may yield better results in identifying discrep ancies Some of the better known data mining interesting ness measures 12 or variations of may also be adopted for this purpose References I G Adomavicius and A Tuzhilin Expert-driven valida tion of rule-based user models in personalization applica tions Data Mining and Knowledge Discovery 5\(1/2 58,2001 121 G Adomavicius and A Tuzhilin Using data mining meth ads to build customer profiles Computer 34\(2 2001 3 C Aggamal Z Sun and P Yu Online algorithms for find ing profile association rules In Proceedings of the ACM Internatinno1 Conference on Informorion and nowledge Management CIKM-98 Bethesda MD USA 1998  R Agrawal T Imielinski and A Swami Mining associ ations between sets of items in massive databases In Pro ceedings ofthe ACM SIGMOD hi Conference on Manage ment ofDara Washington, DC USA May 1993 SI R Agrawal H Mannila R Srikant H Toivonen and A Verkamo Advances in Knowledge Discovery and Data Mining chanter Fast discoverv of association rules AAA1 224 Press 1996 161 E Casev Dipifal Evidence and Computer Crime Academic  Press 2w0.\221 171 P K Chan A non-invasive learninx amroach to building   224 web user profiles In Proceedings of the Workshop on Web Usage Analysis and User Profiling WEBKDD\22299 1999 8 0 de Vel A Anderson M Corney and G Mohay Mining e-mail content for author identification forensics SIGMOD Record 30\(4 2001 191 M Ester H.-P Krieeel J Sander and X Xu A densitv  based algorithm for discovering clustc~s in large spatial databases with noise In Proceedings of the Second Ini Con ference on Knowledge Discovery ond Dam Mining 1996 IO T Fawcett and F Provost Adaptive fraud detection Data Mining and Knowledge Discovery 1\(3 1997 1111 J Han and Y Fu Discovery of multiple-level assmiation rules from large databases In Proceedings of2lst VLDB Conference September 1995 I21 R 1 Hilderman and H I Hamilton Knowledge discovery and interestingness measures A survey Technical Repon CS-99-04 Dept of Computer Science University of Regina 1999 I31 M Hirsh, C Basu. and B Davidson Learning to personal ize Communicarions ofthe ACM 43\(8 2ooO I41 H 1 Y Cai and N Cercone Knowledge discovery in databases an attribute-oriented approach In Proceedings ofl8th hr Conference on Very Large Databases 1992 I51 1 Konstan B Miller D Malte J Herlocker L Gordon and 1 Riedl Grouplens Applying collaborative filtering to usenet news Communications ofthe ACM 40\(3 1997 Beyond intra-transaction assmiation analysis mining multi-dimensional inter transaction rules ACM Transactions on Information Sw I61 H Lu L Feng and J Han terns 18\(4 2000 1171 B Mobasher H Dai T Luo Y Sun and J Wiltshire  Discovery of aggregate usage profiles for web personaliz tion In Proceedings ofthe Workhop on Web Mining for E-Commerce WEBKDD\222OO August 2000 IS A Nanopoulos D Katsaros and Y Manolapoulos Ef fective prediction of web-user accesses a data mining ap proach In Proceedings of the Workshop on Mining Logdata Accross All Customer Touchpoints WEBKDD\222OI 2001 I91 S Nesbitt and 0 de Vel A collaborative filtering agent system for dynamic virtual communities on the web In Proceedings of the Conference on Learninp and Discoven CONALD98 June 1998 1201 T Oates and P R Cohen Searchine for structure in multiole I streams of data In Proceedings of the Thirteenth Interno rional Conference on Machine Learning 1996 ZI R Srikant and R Agrawal Mining generalized assmiation rules In Proceedings ofZlsr VLDB Conference 1995 221 C Stanfill and D Waltz Toward memory-based reasoning Communications of the ACM 29\(12 1986 23 P N Tan and V Kumar Mining indirect assmiations in web data In Proceedings oflhe Workshop on Mining Logdata Accross All Customer Touchpoints WEBKDD\222OI 2001 18 


Category Manual Automatic No of associations 63 30 No of rules 330 44 Max association size 6 4 Avg support 0.45 0.43 Avg rule con\256dence 0.80 0.82 Table 1 Manual versus automatic image content mining 4.2 Quality of results We should mention that there were no false association rules It did not happen that an object was incorrectly identi\256ed and then a rule was generated with the incorrect identi\256er In general when we found a match between two objects they were the same shape All the incorrect matches are 256ltered out by the support parameter and then the association rules are generated for objects correctly ideinti\256ed Also some redundant matches happened b ecause of the blobs that represented several shapes but these matches are 256ltered out by the rule support In Table 1 we present a summary of our experimental results with 100 hundred images We compare the results obtained by manually identifying objects in each image and then generating association rules from such identi\256ers Manual Column against the results obtained by our current implementation Automatic Column Ideally our image mining algorithm should produce the same results as the manual process So the table gives a standpoint to assess the quality of our experimental results For these 100 images unwanted matches either incorrect or involving many objects happened in at most 4 images and therefore their support was well below the minimum support frequency which was at 30 These experiments were run using the same parameters for object identi\256cation as in our small example with 10 images The parameters for object identi\256cation had the following values We set color standard deviation to 0.5 contrast standard deviation to 0.5 and anisotropy also to 0.5 The similarity threshold as needed by the similarity function was set to 0.6 We tuned these parameters after several experiments These parameters maximized the number of associations and decreased the errors in unwanted matches The association rule program was set to look for rules with a 30 support and 70 con\256dence The background represents an object itself Since association rules with the background were not interesting for our purposes it was eliminated from consideration by the object identi\256cation step It is important to note that this is done after objects have been identi\256ed We tuned the object identi\256cation step to 256nd similar objects changing values for several parameters in the following manner The most important features used from each object were color and contrast We allowed some variance for color 0.5 and the maximum allowed variance for contrast 0.5 The anisotropy helped eliminate matches involving several geometric shapes We ignored shape b ecause objects could be partially hidden and rotated Position was considered unimportant because objects could be anywhere in each image Anisotropy and polarity were i gnored because almost all our shapes had uniform texture Area was given no weight because objects could be overlapping and thus their area diminished this can be useful to make perfect matches when objects are apart from each other A few rules had high support One problem that arose during our experiments was that the same shape could have two different blob descriptors and these blob descriptors could not be matched with two other descriptors for the same shape in another image This caused two problems First a rule could be repeated because it related the same shapes Second a rule did not have enough support and/or con\256dence and therefore was discarded So the rules found were correct and in many cases had an actual higher support and also higher con\256dence To our surprise in some cases there were no object matches because an object was very close to another one or was located in a corner of the image When two or more objects were overlapping or very close they were identi\256ed as a single object This changed the features stored in the blob The problem was due to the ellipsoidal shape of the blobs and the fact that when a geometric shape was located in a corner thta changed its anysotropy and polarity descriptors Given a blob for an object very close to one corner means determining an adequate radius for the blob i.e ellipse Regular shapes such as the triangle square and hexagon were easily matched across images This is a direct consequence of the circular blob representation produced when the image is segmented In this case neither position nor rotation affect the mining process at all It was surprising that in some cases there were no matches for the circle in these cases it was in a corner or some other shape was very close or overlapping Another important aspect about shape is that we do not use it as a parameter to mine images but shape plays an important role during the segmentation step So shape does affect the image mining results quality The rectangle and the ellipse are the next shapes that are easily matched even though we did not use the shape feature The most complicated shape was the L In this case a number of factors affected matches When this shape was overlapped with other shapes a few matches were found b ecause a big blob was generated Also orientation changed dominant 


ofimages 50 100 150 200 1 feature 50292 80777 127038 185080 2 obj identif 210 338 547 856 3 aux image 3847 6911 10756 13732 4 assoc rules 6 3 6 4 Table 2 Measured times in seconds for each Image Mining step with different image set sizes colors and contrast When the L was close to another shape its colors were merged making it dissimilar to other L shaped objects This suggests that irregular shapes in general make image mining dif\256cult We worked with color images but it is also possible to use black and white images Color and texture were important in mining the geometric shapes we created However we ignored shape as mentioned above Shape may be more important for black and white images but more accurate shape descriptors are needed than those provided by the blobs 4.3 Performance evaluation We ran our experiments on a Sun Multiprocessor forge.cc.gatech.edu computer with 4 processors each running at 100 MHz and 128 MB of RAM The image mining program was written in Matlab and C The 256rst three steps are performed in Matlab The feature extraction process is done in Matlab by the software we obtained from UCB Object identi\256cation and record creation were also done in Matlab by a program developed by us An html page is created in Matlab to interpret results The association rules were obtained by a program written in C In this section we examine the performance of the various components of the image mining process as shown in Table 2 for several image set sizes These times were obtained by averaging the ellapsed times of executing the image mining program 256ve times 4.4 Running time analysis Feature extraction although linear in the number of images is slow and there are several reasons for this If image size increases performance should degrade considerably since feature extraction is quadratic in image size Nevertheless this step is done only once and does not have to be repeated to run the image mining algorithm several times Object identi\256cation is fast This is because the algorithm only compares unmatched objects and the number of objects per image is bounded For our experimental results time for this step scales up well Auxiliary image creation is relatively slow but its time grows linearly since it is done on a per image basis The time it takes to 256nd rules is the lowest among all steps If the image mining program is run several times over the same image set only the times for the second and the fourth step should be considered since image features already exist and auxiliary images have already been created 5 Application Image mining could have an application with real images The current implementation could be used with a set of images having the following characteristics 017 Homogeneous The images should have the same type of image content For instance the program can give useless results if some images are landscapes other images contain only people and the remaining images have only cars 017 Simple image content If the images are complex they will produce blobs dif\256cult to match Also the association rules obtained will be harder to interpret A high number of colors blurred boundaries between objects large number of objects signi\256cant difference in object size make the image mining process more prone to errors 017 A few objects per image If the number of objects per image is greater than 10 then our current implementation would not give accurate results since Blobworld in most cases generates at most 12 blobs per image 017 New information The image itself should should give information not already known If all the information about the image is contained in associated alphanumeric data then that data could be mined directly 6 Future Work Results obtained so far look promising but we need to improve several aspects in our research effort We are currently working on the following tasks We also need to analyze images with repeated geometric shapes If we want to obtain simple association rules this can make our program more general This can be done without further modi\256cation to what is working However if we want to mine for more speci\256c rules then we would need to modify our algorithm For instance we could try to 


produce rules like the following if there are two rectangles and one square then we are likely to 256nd three triangles The issues are the combinatorial growth of all the possibilities to mine and also a more complex type of condition We will also study more deeply the problem of mining images with more complex shapes such as the irregular one similar to the letter L We need a systematic approach to determine an optimal similarity threshold or at least a close one A very high threshold means only perfect matches are accepted On the other hand a very low similarity threshold may mean any object is similar to any other object Finding the right similarity threshold for each image type l ooks like an interesting problem Right now it is provided by the user but it can be changed to be tuned by the algorithm itself Also there are many ways to tune the eleven parameters to match blobs and the optimal tuning may be speci\256c to image type There also exists the possibility of using other segmentation algorithms that could perform faster or better feature extraction It is important to note that these algorithms should give a means to compare segmented regions and provide suitable parameters to perform object matching in order to be useful for image mining From our experimental results it is clear that this step is a bottleneck for the overall performance of image mining We can change the object identi\256cation algorithms to generate overlapping object associations using more features Our algorithm currently generates partititons of objects that is if one object is considered similar To another one the latter one will not be compared again By generating overlapping associations we can 256nd even more rules For instance a red rectangular object may be considered similar to another rectangular object and at the same time be similar to another red object Mining by position is also possible for instance two objects in a certain position may imply another object to be in some other position Since the software we are using for feature extraction produces eleven parameters to describe blobs we have 2 11 possibilites to match objects 7 Conclusions We presented a new algorithm to perform data mining on images and an initial experimental and performance study The positive points about our algorithm to 256nd association rules in images and its implementation include the following It does not use domain knowledge it is reasonably fast it does not produce meaningless or false rules it is automated for the most part The negative points include some valid rules are discarded because of low s upport there are repeated rules because of different object id's unwanted matches because of blobs representing several objects slow feature extraction step a careful tuning of several parameters is needed it does not work well with complex images We studied this problem in the context of data mining for databases Our image mining algorithm has 4 major steps feature extraction object identi\256cation auxiliary image creation and identi\256ed object mining The slowest part of image mining is the feature extraction step which is really a part of the process of storing images in a CBIR system and is done only once The next slowest operation is creating the auxiliary blob images which is also done once Object identi\256cation and association rule 256nding are fairly fast and scale up well with image set size We also presented several improvements to our initial approach of image mining Our experimental results are promising and show some potential for future study Rules referring to speci\256c objects are obtained regardless of object position object orientation and even object shape when one object is partially hidden Image mining is feasible to obtain simple rules from not complex images with a few simple objects Nevertheless it requires human intervention and some domain knowledge to obtain better results Images contain a great deal of information and thus the amount of knowledge that we can extract from them is enormous This work is an attempt to combine association rules with automatically identi\256ed objects obtained from a matching process on segmented images Although our experimental results are far from perfect we show that it is better to discover some reliable knowledge automatically than not discovering any new knowledge at all Acknowledgments We thank Chad Carson from the University of California at Berkeley for helping us setup the Blobworld system We also thank Sham Navathe and Norberto Ezquerra for their comments to improve the presentation of this paper References 1 R  A g r a w a l  T  I m i e lin s k i a n d A  S w a m i  M in in g a s s o ciation rules between sets of items in large databases In Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data  pages 207\261 216 Washington DC May 26-28 1993  R  A gra w a l a n d R  S ri ka nt  F a s t a l gori t h m s for m i n i n g association rules in large databases In Proceedings of the 20th International Conference on Very Large Data Bases  Santiago Chile August 29-September 1 1994  S  B e l ongi e  C Ca rs on H  G r e e n s p a n  a nd J  Ma lik Recognition of images in large databases using a learning framework Technical Report TR 97-939 U.C Berkeley CS Division 1997 


 C  C a r s on S  Be l ongi e  H  G r e e n s p a n  a nd J  Ma l i k  Region-based image querying In IEEE Workshop on Content-Based Access of Image and Video Libraries  1997 5 G  D u n n a n d B  S  E v e r itt An Introduction to Mathematical Taxonomy  Cambridge University Press New York 1982  U  F a yya d  D  H a u s s l e r  a nd P  S t orol t z  M i n i n g s c i e n ti\256c data Communications of the ACM  39\(11\51\26157 November 1996  U  F a yya d G  P i a t e t s k y-S h a p i r o a n d P  S m y t h  T he kdd process for extracting useful knowledge from volumes of data Communications of the ACM  39\(11\:27\261 34 November 1996 8 D  F o r s y t h J M a l i k  M F l e c k H G r e e n s p a n  T L e ung S Belongie C Carson and C Bregler Finding pictures of objects in large collections of images Technical report U.C Berkeley CS Division 1997  W  J  F ra wl e y  G  P i a t e t s k y S ha pi ro a nd C J  Ma t h e u s  Knowledge Discovery in Databases  chapter Knowledge Discovery in Databases An Overview pages 1 261 27 MIT Press 1991  V  G udi v a da a n d V  R a gha v a n Cont e n t ba s e d i m age retrieval systems IEEE Computer  28\(9\18\26122 September 1995 11 R  H a n s o n  J  S t u t z an d P  C h ees eman  B ay es i a n c l a s si\256cation theory Technical Report FIA-90-12-7-01 Arti\256cial Intelligence Research Branch NASA Ames Research Center Moffet Field CA 94035 1990  M H o l s he i m e r a n d A  S i e be s  D a t a m i ni ng T h e search for knowledge in databases Technical Report CS-R9406 CWI Amsterdam The Netherlands 1993  M H out s m a a nd A  S w a m i  S e t ori e nt e d m i ni ng of association rules Technical Report RJ 9567 IBM October 1993  C O r done z a nd E  O m i e c i ns ki  I m a ge m i ni ng A new approach for data mining Technical Report GITCC-98-12 Georgia Institute of Technology College of Computing 1998  J  R Q u i n l a n Induc t i o n o f d e c i s i on t r e e s  Machine Learning  1\(1\81\261106 1986  A  S a v a s e re  E  O m i e c i ns ki  a nd S  N a v a t h e  A n e f 256 cient algorithm for mining association rules In Proceedings of the VLDB Conference  pages 432 261 444 Zurich Switzerland September 1995  O  R Z a i a ne  J  H a n  Z  N  L i  J  Y  Chi a ng a n d S Chee Multimedia-miner A system prototype for multimedia data mining In Proc 1998 ACM-SIGMOD Conf on Management of Data  June 1998 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


