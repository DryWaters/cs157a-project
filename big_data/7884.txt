784 IEEE SIGNAL PROCESSING LETTERS VOL 24 NO 6 JUNE 2017 Iterative Soft/Hard Thresholding With Homotopy Continuation for Sparse Recovery Yuling Jiao Bangti Jin and Xiliang Lu Abstract In this note we analyze an iterative soft/hard thresholding algorithm with homotopy continuation for recovering a sparse signal x  from noisy data of a noise level 002  Under suitable regularity and sparsity conditions we design a path along which the algorithm can 036nd a solution x 002  which admits a sharp reconstruction error 003 x 002  x  003 002 004  O  002  with an iteration complexity O  ln 003 ln 004 np  where n and p are problem dimensionality and 003 005 0  1 controls the length of the path Numerical examples are given to illustrate its performance Index Terms Continuation convergence iterative soft/hard thresholding IST/IHT solution path I I NTRODUCTION S PARSE recovery has attracted much attention in machine learning signal processing statistics and inverse problems over the last decade Often the problem is formulated as y 002 x   002 1 where x  002 R p is the unknown sparse signal y 002 R n is the data with the noise 002 002 R n of level 003  003 002 003  and the matrix 002 002 R n  p with p 004 n has normalized columns  004 i   i.e 003 004 i 003 1  i 1 p The desired sparsity structure can be enforced by either the 005 0 or 005 1 penalty i.e min x 002 R p 1 2 003 002 x  y 003 2  002 003 x 003 t t 002 0  1  2 where 002  0 is the regularization parameter Among the existing algorithms for minimizing 2 iterative soft/hard thresholding IST/IHT algorithms  and their accelerated extension   6 are e xtremely popular  T hese Manuscript received March 1 2017 revised April 9 2017 accepted April 10 2017 Date of publication April 12 2017 date of current version April 24 2017 The work of Y Jiao was partially supported by the National Science Foundation of China NSFC 11501579 and the National Science Foundation of Hubei Province 2016CFB486 The work of B Jin was supported by the Engineering and Physical Sciences Research Council under Grant EP/M025160/1 The work of X Lu was supported by the NSFC 11471253 and 91630313 The associate editor coordinating the review of this manuscript and approving it for publication was Prof Marco Felipe Duarte Corresponding author Xiliang Lu Y Jiao is with the School of Statistics and Mathematics and Big Data Institute Zhongnan University of Economics and Law Wuhan 430063 China e-mail yulingjiaomath@whu.edu.cn B Jin is with the Department of Computer Science University College London London WC1E 6BT U.K e-mail bangti.jin@gmail.com b.jin@ucl.ac.uk X Lu is with the School of Mathematics and Statistics and the Hubei Key Laboratory of Computational Science Wuhan University Wuhan 430072 China e-mail xllv.math@whu.edu.cn Color versions of one or more of the 036gures in this letter are available online at http://ieeexplore.ieee.org Digital Object Identi\036er 10.1109/LSP.2017.2693406 algorithms are of the form x k 1  T 006 k 002 002 x k  006 k 002 t  y  002 x k  003 3 where 006 k is the stepsize and T 002 is a softor hard-thresholding operator de\036ned componentwise by T 002  t  004 max  t  002   t   IST 007  t   005 2 002   t   IHT 4 where 007  t  is the characteristic function Their convergence was analyzed in many works mostly under the condition 006 k  2  003 002 003 2  This condition ensures a asymptotically contractive thresholding and thus the desired convergence  Meanwhile it was observed that the continuation along 002 can greatly speed up the algorithms  N onetheless as pointed out by Tropp and Wright  the design o f a rob u st practical and theoretically effective continuation algorithm remains an interesting open question  There were several works aiming at 036lling this gap In the works  13 a proximal-gradient method with continuation for the 005 1 problem was analyzed with linear search under the sparse restricted eigenvalue/restricted strong convexity condition Recently a Newton-type method with continuation was studied for 005 1 and 005 0 problems   15 I n t his w ork we present a uni\036ed approach to analyze IST/IHT with continuation and a 036xed stepsize 006 1  denoted by iterative soft/hard-thresholding with continuation ISTC/IHTC The challenge in the analysis is the lack of monotonicity of function values due to the choice 006 1  The overall procedure is given in Algorithm 1 Here 002 0 is an initial guess of 002  supposedly large 010 002 0  1 is the decreasing factor for 002  and K max is the maximum number of inner iterations for a 036xed 002  The choice of the 036nal 002 006 is given in 5 below Distinctly the inner iteration does not need to be solved exactly actually one inner iteration suf\036ces the desired accuracy of the 036nal solution x 006  cf Theorem 2 and there is no need to perform stepsize selection In Theorem 2 we prove that under suitable mutual coherence MC condition on the matrix 002 cf Assumption 2.1 and Remark 2.2 ISTC/IHTC always converges II C ONVERGENCE ANALYSIS The starting point of our analysis is the next lemma Lemma 1 For any x y 002 R  there holds  T 002  x  y   x 007 004  y   002 IST  y   005 2 002 IHT  1070-9908  2017 IEEE Personal use is permitted but republication/redistribution requires IEEE permission See http://www.ieee.org/publications standards/publications/rights/index.html for more information 


JIAO et al  ITERATIVE SOFT/HARD THRESHOLDING WITH HOMOTOPY CONTINUATION FOR SPARSE RECOVERY 785 Algorithm 1 Iterative Soft/Hard-Thresholding With Continuation 1 Input 002 002 R n  p  y  002 0  010 002 0  1  002 006  K max 002 N  x  002 0   2 for 005 1  2   do 3 Let 002 005  010 002 005  1  x 0  x  002 005  1   4 If 002 005  002 006  stop and output x 006  x 0  5 for k 0  1   K max  1 do 6 x k 1  T 002 005  x k 002 t  y  002 x k   7 end for 8 Set x  002 005  x K max 9 end for Proof By the de\036nition of the operator T 002  cf 4  T 002  x  y   x 007 T 002  x  y    x  y     y  007 004  y   002 IST  y   005 2 002 IHT which completes the proof of the lemma 002 Let the true signal x  be s sparse with a support A   i.e s  A    and I  the complement of A   Recall also that the MC  of the matrix 002 is de\036ned by   max i 010  j 011 004 i 004 j 012  Assumption 2.1 The MC  of 002 satis\036es s  1  2  The proper choice of the regularization parameter 002 is essential for successful sparse recovery It is well known that under Assumption 2.1 the choice 002  O  003  for the 005 1 penalty and 002  O  003 2  for the 005 0 penalty ensures 003 x  x  003 005 013  O  003    T hus w e consider the f ollo wing apriori choice 002 006  005 006 006 007 006 006 010 C 1 003 with C 1  1 1  2 s  for ISTC C 0 003 2  with C 0  1 2\(1  2 s  2  for IHTC  5 In practice one may consider a posteriori choice rules  Now we can state the global convergence of Algorithm 1 Theorem 2 Let Assumption 2.1 hold and 002 006 be chosen by 5 Suppose that 002 0 is large K max 002 N  and 010 002 005 006 006 007 006 006 010 2 s 1  1 C 1   1  for ISTC 011\012 2 s 1  1  2 C 0  1  2 013 2  1 013  for IHTC  Then Algorithm 1 is well de\036ned and the solution x 006 satis\036es 1 supp x 006  014A   2 there holds the error estimate 003 x 006  x  003 005 013 007 014  C 1  1 003  s   for ISTC 002 015 2 C 0  1 003 003  s   for IHTC  Furthermore if min i 002A   x  i  is large enough then supp x 006  A   Proof We only prove the assertion for ISTC since that for IHTC is similar The choice of C 1 in 5 implies C 1  1 and 2 s 1  1 C 1  1  and thus the choice of 010 makes sense First we consider the inner loop at lines 5–7 of Algorithm 1 and omit the index 005 for notational simplicity Let E k  003 x k  x  003 005 013  and 011  1  1 C 1 s  Consider one IST iteration from x k to x k 1  The key step to the convergence proof is the following implication with A k  supp x k  A k 014A  and E k 007 011 002 015A k 1 014A  and E k 1 007 011\010 002 016 002 017 002 006  6 Now we show this claim It follows from 1 and 003 002 i 003 1 the following componentwise expression for the update x k 1 i  T 002  x k i 002 t i  y  002 x k   T 002 016 x  i 002 t i 002 A  020A k  i   x   x k  A  020A k  i   002  017  By the hypothesis in 6 A k 014A   E k 007 011 002  002 017 002 006  and 5 we deduce that for any i 002I   x  i 002 t i 002 A  020A k  i   x   x k  A  020A k  i   002   007 002 t i 002 A   x   x k  A     002 t i 002  007 sE k  003 007 012 1 C 1  s\011 013 002  002 by the de\036nition of 011  and the second inequality follows from 15 Lemma Hence  x k 1 i 007 T 002  sE k  003   0  which implies directly A k 1 014A   Meanwhile under 6 and 5 for any i 002A   by Lemma 1 we deduce  x k 1 i  x  i 007 002   002 t i 002 A   i   x   x k  A   i     002 t i 002  007 002    s  1 E k  003 007 002  s\011 002  1 C 1 002  012 1 1 C 1  011µs 013 002 2 002 007 011\010 002  Thus we have E k 1 007 011\010 002  i.e the claim 6 holds Next we prove the following assertion by mathematical induction for all 005 with 002 005 017 002 006  there holds supp x  002 005  014A   003 x  002 005   x  003 005 013 007 011\010 002 005  7 Since 002 0 is large it satis\036es 7 Now assume 7 holds for 002 005  1  i.e supp x  002 005  1  014A  and 003 x  002 005  1   x  003 005 013 007 011\010 002 005  1  When Algorithm 1 runs lines 3–7 for 002 005  since x 0  x  002 005  1   then we have A 0 014A  and E 0 007 011 002 005  From 6 we obtain that for all k 017 1  A k 014A  and E k 007 011\010 002 005  In particular if we choose k  K max  then 7 holds for 002 005  When Algorithm 1 terminates for some 002 005  002 006  then 002 005  1 017 002 006 and x 006  x  002 005  1  From\(7\,wehave supp x 006 014A  and 003 x 006  x  003 005 013 007 011 002 006  C 1  1 003  s   Likewise if min i 002A   x i    C 1  1 003  s   property ii implies supp x 006  A   Last we brie\037y discuss IHTC For the choice C 0 in 5 010 002  2 s 1  1  2 C 0  1  2  2  1 makes sense With 011  1  1  2 C 0  1  2 s  a similar argument yields A k 014A  and E k 007 011 005 2 002 015A k 1 014A  and E k 1 007 011 015 2 010 002  The rest follows like before and thus it is omitted 002 Remark 2.1 The proof works for any choice K max 017 1 including K max 1  In practice we 036x it at K max 5 This together with Theorem 2 allows estimating the complexity of Algorithm 1 At each iteration one needs to compute matrixvector product 002 x and 002 t y  and for each 002  the number of iterations is bounded by K max  The overall cost depends on the decreasing factor 010 by O  ln 002 006 ln 010 np  O  ln 003 ln 010 np   


786 IEEE SIGNAL PROCESSING LETTERS VOL 24 NO 6 JUNE 2017 Remark 2.2 Conditions similar to Assumption 2.1 have been widely used in the literature for analyzing Orthogonal Matching Pursuit OMP   19  20 with 2 s  1  007 1  and for bounding the estimation error of Lasso  22 with 7 s  1 and 4 s 007 1  Thus Assumption 2.1 is fairly standard Examples of matrices with small MC  include that formed by equiangular tight frame and random sub-Gaussian matrices  Furthermore w e note t hat other similar conditions e g restricted eigenvalue condition and RIP conditions were also used to derive error bounds of the type 003 x  x  003 2  O  003  for proximal-gradient homotopy PGH algorithms  13 and Greedy methods e.g Compressive Sampling Matching Pursuit CoSaMP NIHT  25 and CGIHT  26 III N UMERICAL R ESULTS AND D ISCUSSIONS Now we present numerical examples to show the convergence and the performance of Algorithm 1 First we give implementation details e.g data generation parameter setting for the algorithm Then our method is compared with several state-ofthe-art algorithms in terms of reconstruction error and recovery ability via phase transition A Implementation Details Following the s ignals x  are chosen as s sparse with a dynamic range DR  max  x  i   x  i 010 0   min  x  i   x  i 010 0   The matrix 002 002 R n  p is chosen to be either random Gaussian matrix or random Bernoulli matrix or the product of a partial fast Fourier transform FFT matrix and inverse Haar wavelet transform Under proper conditions such matrices satisfy Assumption 2.1 The noise 002 has entries following independent and identically distributed N 0 012 2   We 036x the algorithm parameters as follows 002 0  003 002 t y 003 013 and 002 0  003 002 t y 003 2 013  2 for ISTC and IHTC respectively   decreasing f actor 010 0  8  Since the optimal 002 006 depends on the noise level 003  which is often unknown in practice we prede\036ne a path 003  002 005  N 005 0 with 002 005  002 0 010 005 and N  100  Then we run Algorithm 1 on the path 003 and select the optimal 002 006 by the Bayesian information criterion A ll the computations were performed on an eight-core desktop with 3.40 GHz and 12-GB RAM using MATLAB 2014a The MATLAB package ISHTC for reproducing all the numerical results can be found at http://www0.cs.ucl.ac.uk/staff/b.jin/companioncode.html First we illustrate Theorem 2 by examining the in\037uence of sparsity level s  coherence   and noise level 012 on IHTC recovery on three settings  n  500 p  1000 and DR  100 1 random Gaussian 002  012  1e-2 s  10  10  100 2 random Gaussian 002  s  50 012  1e-4,1e-3,1e-2,1e-1,1 3 002 is random Gaussian with correlation where the parameter 013 controls the coherence  see 27 Sec for details In general a larger parameter 013 gives a larger  a typical example   0.19 for 013  0   0.33 for 013  0.15   0.56 for 013  0.3 and   0.74 for 013  0.5 We choose 013  0  0.05  1 s  10 012  1e-3 The results in Fig 1 are computed from 100 independent realizations It is observed that when the sparsity level s and noise level 012 and incoherence 013 are small IHTC recovers the exact support with high probability as implied by Theorem 2 B Comparison of ISTC With 005 1 Solvers Now we compare ISTC with four state-of-the-art 005 1 solvers GPSR  http://www lx.it.pt/mtf/GPSR SpaRSA 9 http://www.lx.it.pt/mtf/SpaRSA PGH method https www.microsoft.com/en-us/download/details.aspx?id  52421 Fig 1 Exact support recovery probability versus s  012 and 013 a\s\(b 012 c 013  TABLE I N UMERICAL R ESULTS CPU TIME AND E RRORS  WITH R ANDOM B ERNOULLI 002  OF S IZE p  10 000 18 000 n  021 p 4 022  s  021 n 40 022  WITH DR  100 AND 012  5 E 2 p method time s nMV Re 005 2 Ab 005 013 ISTC 1.0 58 4.21e-3 2.66e-1 PGH 1.7 419 4.14e-3 2.66e-1 10 000 SpaRSA 3.4 302 4.13e-3 2.63e-1 GPSR 3.0 256 4.25e-3 2.71e-1 FISTA 5.3 505 4.30e-3 2.65e-1 ISTC 3.3 58 4.34e-3 2.88e-1 PGH 5.6 443 4.25e-3 2.85e-1 18 000 SpaRSA 11.4 309 4.25e-3 2.84e-1 GPSR 9.5 258 4.36e-3 2.91e-1 FISTA 17.2 506 4.40e-3 2.74e-1         Fig 2 Empirical phase transition curves for ISTC PGH SpaRSA and GPSR with 014  s/n and 015  n/p  and FISTA  implemented as https://web iem.technion.ac.il images/user-\036les/becka/papers/wavelet_FISTA.zip 1 The numerical results CPU time number of matrix-vector multiplications nMV relative 005 2 error Re 005 2  and absolute 005 013 error Ab 005 013  are computed from ten i ndependent realizations of random Bernoulli sensing matrices with different parameter tuples  n p s DR 012   as shown in Table I It is observed that ISTC yields reconstructions that are comparable with that by other methods but at least two to three times faster Furthermore it scales well with the problem size p  Next we compare the empirical performance of ISTC with other methods by their phase transition curves in the 014 015 plane with 014  s/n and 015  n/p  When computing the curves we 036x the dimension p  1000 partition the range  015 014    0.1  1  2 into a 30  30 equally spaced grid and run 100 independent simulations at each grid point The s sparse signal x  002 R p matrix 002 002 R n  p  and data y 002 R n are generated as 28 Fig Fig 2 plots the logistic regression curves identifying the 90  success rate for the algorithms IHTC exhibits similar phase transition behavior as other methods 1 All the codes were last accessed on February 23 2017 


JIAO et al  ITERATIVE SOFT/HARD THRESHOLDING WITH HOMOTOPY CONTINUATION FOR SPARSE RECOVERY 787 Fig 3 Reconstructed signals and their PSNR values TABLE II 1-D S IGNAL method CPU time PSNR IHTC 0.41 51 OMP 1.20 49 NIHT 0.96 46 CoSaMP 0.49 26 CGIHT 0.98 49 Left 1-D signal with n  665 p  1024 s  247 and 012  1e-4 Right 2-D image with n  34 489 p  262 144 s  7926 and 012  3e-2 C Comparison of IHTC With Greedy Solvers Now we compare IHTC with four state-of-the-art greedy methods for the 005 0 problem to recover 1-D signal and benchmark MRI image These methods include OMP  https://sparselab.stanford.edu/SparseLab_\036les/Download_\036les SparseLab21-Core.zip normalized IHT NIHT  http www.gaga4cs.org CoSaMP  http://mda v  ece.g a tech edu/software/SSCoSaMP-1.0.zip and conjugate gradient IHT CGIHT  http://www g ag a4cs.or g  The underlying 1-D signal and 2-D MRI image are compressible under a wavelet basis Thus the data can be chosen as the wavelet coef\036cients sampled by the product of a partial FFT matrix and inverse Haar wavelet transform For the 1-D signal the matrix 002 is of size 665  1024 and consists of applying a partial FFT and an inverse two-level Harr wavelet transform The signal under wavelet transform has 247 nonzeros and 012  1e-4 The results are shown in Fig 3 and Table II The reconstruction by IHTC is visually more appealing than that of the others cf Fig 3 The results by AIHT and CoSaMP suffer from pronounced oscillations This is further con\036rmed by the peak signal-to-noise ratio PSNR value de\036ned by PSNR  10  log V 2 MSE  where V is the maximum absolute value of the true signal and MSE is the mean squared error of the reconstruction Table II also presents the CPU time of the 1-D example which shows clearly that IHTC is the fastest one For the 2-D MRI image the matrix 002 amounts to a partial FFT and an inverse wavelet transform and it has a size 34 489  262 144  The image under eight-level Haar wavelet transformation has 7926 nonzero entries and 012  3e-2 The numerical results are shown in Fig 4 and Table III All 005 0 methods produce comparable results but the IHTC is fastest Next we compare the empirical sparse recovery performance of IHTC with these greedy methods by means of phase transition curves in the 014\015 plane with 014  s/n and 015  n/p  When computing the curves we 036x the dimension p  1000 Fig 4 Reconstructed MRI images and their PSNR values TABLE III 2-D IMAGE method CPU time PSNR IHTC 6.1 28 OMP 932 28 NIHT 9.4 27 CoSaMP 14.3 26 CGIHT 7.9 27         Fig 5 Empirical phase transition curves of IHTC OMP CoSaMP NIHT and CGIHT with 014  s/n and 015  n/p  partition the range  015 014  002  0.1  1  2 into a 90  90 uniform grid and run 100 independent simulations at each grid point Like before the s sparse signal x  002 R p matrix 002 002 R n  p and data y 002 R n are generated as 28 Fig F ig 5 plots t he logistic regression curves identifying the 90  success rate for the algorithms IHTC exhibits comparable phase transition phenomenon with other greedy methods whereas CoSaMP performs slightly worse than others IV C ONCLUSION In this paper we analyze an IST/IHT algorithm with homotopy continuation for sparse recovery from noisy data Under standard regularity condition and sparsity assumptions sharp reconstruction errors can be obtained with an iteration complexity O  ln 003 ln 010 np   Numerical results indicated its competitiveness with state-of-the-art sparse recovery algorithms The results can be extended to other penalties e.g Minimax Concave Penalty MCP  or Smoothly C lipped Absolute De viation Penalty SCAD A CKNOWLEDGMENT The authors would like to thank anonymous referees for their helpful comments 


788 IEEE SIGNAL PROCESSING LETTERS VOL 24 NO 6 JUNE 2017 R EFERENCES  I  D aubechies M  D efrise a nd C De Mol An iterati v e thresholding algorithm for linear inverse problems with a sparsity constraint Commun Pure Appl Math  vol 57 no 11 pp 1413–1457 2004  P  C ombettes and V  W ajs  Signal r eco v e ry by proximal f orw a rdbackward splitting Multiscale Model Simul  vol 4 no 4 pp 1168 1200 2005  T  B lumensath and M  E  D a v ies  Iterati v e thresholding for s parse a pproximations J Fourier Anal Appl  vol 14 nos 5/6 pp 629–654 2008  H  A ttouch J Bolte and B  F  S v aiter  Con v er gence o f d escent m ethods for semi-algebraic and tame problems Proximal algorithms forward backward splitting and regularized Gauss–Seidel methods Math Program  vol 137 nos 1/2 pp 91–129 2013  A  B eck and M  T eboulle A f a st iterati v e shrinkage-thresholding algorithm for linear inverse problems SIAM J Imag Sci  vol 2 no 1 pp 183–202 2009  S  B eck er J  B obin and E  C and  es NESTA A fast and accurate 036rstorder method for sparse recovery SIAM J Imag Sci  vol 4 no 1 pp 1–39 2011  E  H ale W  Y i n and Y  Z hang Fix ed-point continuation for 005 1 minimization Methodology and convergence SIAM J Optim  vol 19 no 3 pp 1107–1130 2008  M  F igueiredo R No w a k and S  Wright  Gradient p rojection f or sparse reconstruction Application to compressed sensing and other inverse problems IEEE J Sel Topics Signal Process  vol 1 no 4 pp 586–597 Dec 2007  S  Wright R  N o w ak a nd M Figueiredo Sparse r econstruction b y separable approximation IEEE Trans Signal Process  vol 57 no 7 pp 2479–2493 Jul 2009  D A Lorenz  Constructing test i nstances for b asis pursuit denoising  IEEE Trans Signal Process  vol 61 no 5 pp 1210–1214 Mar 2013  J T ropp and S  Wright  Computational methods for s parse s olution o f linear inverse problems Proc IEEE  vol 98 no 6 pp 948–958 Jun 2010  L Xiao and T  Z hang A proximal-gradient homotop y method for t he sparse least-squares problem SIAM J Optim  vol 23 no 2 pp 1062 1091 2013  A Aga w al S Ne gahban and M  J  W ainwright F ast global con v er gence of gradient methods for high-dimensional statistical recovery Ann Statist  vol 40 no 5 pp 2452–2482 2012  Q F a n Y  Jiao a nd X Lu  A p rimal dual acti v e set algorithm w ith continuation for compressed sensing IEEE Trans Signal Process  vol 62 no 23 pp 6276–6285 Dec 2014  Y  Jiao B  J in a nd X Lu  A p rimal dual acti v e set w ith continuation algorithm for the 005 0 regularized optimization problem Appl Comput Harmon Anal  vol 39 no 3 pp 400–426 2015  D L Donoho and X  H uo Uncertainty principles and i deal atomic d ecomposition IEEE Trans Inf Theory  vol 47 no 7 pp 2845–2862 Nov 2001  D L Donoho M Elad a nd V  N T e mlyak o v Stable reco v e ry of sparse overcomplete representations in the presence of noise IEEE Trans Inf Theory  vol 52 no 1 pp 6–18 Jan 2006  K Ito a nd B Jin Inverse Problems Tikhonov Theory and Algorithms  Hackensack NJ USA World Scienti\036c 2015  J A T r opp and A  C  G ilbert Signal r eco v e ry from r andom measurements via orthogonal matching pursuit IEEE Trans Inf Theory  vol 53 no 12 pp 4655–4666 Dec 2007  T  T  Cai a nd L W a ng Orthogonal matching pursuit for s parse s ignal recovery with noise IEEE Trans Inf Theory  vol 57 no 7 pp 4680 4688 Jul 2011  K Lounici Sup-norm c on v e r g ence rate and s ign c oncentration p roperty of Lasso and Dantzig estimators Electron J Statist  vol 2 pp 90–102 2008  T  Zhang Some s harp performance bounds for least s quares re g ression with l1 regularization Ann Statist  vol 37 no 5A pp 2109–2144 2009  S F oucart a nd H Rauhut A Mathematical Introduction to Compressive Sensing  Basel Switzerland Birkh  auser 2013  D Needell and J  A  T ropp CoSaMP Iterati v e signal r eco v e ry from i ncomplete and inaccurate samples Appl Comput Harmonic Anal  vol 26 no 3 pp 301–321 2009  T  Blumensath and M  E  D a v ies  Normalized iterati v e hard thresholding Guaranteed stability and performance IEEE J Sel Topics Signal Proc  vol 4 no 2 pp 298–309 Apr 2010  J D Blanchard J T a nner  and K  W ei CGIHT  C onjugate gradient iter ative hard thresholding for compressed sensing and matrix completion Inf Inference  vol 4 no 4 pp 289–327 2015  Y  Jiao B  J in a nd X Lu  A p rimal dual acti v e set algorithm f or a class of nonconvex sparsity optimization preprint arXiv:1310.1147 2013  D L Donoho and Y  T saig  F a st solution o f 005 1 norm minimization problems when the solution may be sparse IEEE Trans Inf Theory  vol 54 no 11 pp 4789–4812 Nov 2008  C.-H Z hang Nearly unbiased v a riable selection under m inimax conca v e penalty Ann Statist  vol 38 no 2 pp 894–942 2010  J F a n a nd R Li  V a riable selection v ia nonconca v e p enalized lik elihood and its oracle properties J Amer Statist Assoc  vol 96 no 456 pp 1348–1360 2001 


                 E Real-time Monitoring with Data Quality Monitor            F Test for ROI Data Reduction Scheme                                             et al  arXiv:1011.0352    et al  J Instrum    et al   IEEE Trans Nucl Sci    et al   IEEE Trans Nucl Sci    et al   Phys Procedia    et al   IEEE Trans Nucl Sci     J Instrum    et al   arXiv:1406.4028   et al  IEEE Trans Nucl Sci    J Instrum   et al   IEEE Trans Nucl Sci     IEEE Trans Nucl Sci    et al   Nucl Instrum Meth Phys Res Sec A   


2332-7790 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/TBDATA.2017.2703830, I\EEE Transactions on Big Data XX 7 the This send data minimize other  input egate gest Ag egate on exas Oakland model execute ech input exas the limited input esented ogram to e\002ne contexts Scenarios of CloudFinder interface  code to CloudFinder and individual nonmultiple a cloud bioinformatics ogram and  ith CloudFinder along the using ces CloudFinder would This cher for physically Using CloudFinder would much  physics comes Using CloudFinder can updated them eserved would a A for CloudFinder the should for such cher than distribution compliset take experiment conjuncand be trivial CloudFinder of a input CloudFinder distribution observe to continues all Amazon CloudFinder cloud ocessor performance CloudFinder well the of ogram 002le and CloudFinder This the addition CloudFinder the workonment CloudFinder place onment of CloudFinder lives and as also ovides chers E D A S of CloudFinder a the Hi HiBench various workloads T o test our non-iterative PageRegates HiBench by of clusters 48 The PageRank benchusing the 


mance distribution T ogether  these benchmar ks pr ovide of topologies Hadoop locations districometically would worker between the a e compute ound ed of time the may contention of applications performing federation ences esult of ces hmark benchA&M 25GB e system data between the these egate execution other exas seceason the size to network over om smaller data is little Adding execuin available gest the The 2 egate performance ogram the sending data in time data ge seconds of the 2332-7790 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/TBDATA.2017.2703830, I\EEE Transactions on Big Data XX 8 


Time take data the map3 4 1 the RAM CPU the execution but performs machine on workload performance the e machines hmark benchand exas PageRank University using e system e esults an es other egate time the system outthe workers evious by to been the case execution easing in Observations in tests change our some the Even executions on can and egate the mappings erasort ovement 4  test 2332-7790 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/TBDATA.2017.2703830, I\EEE Transactions on Big Data XX 9 


mance come between is can the PageRank operations execution es other shows types given workload tests maesult variabillocation and an 002cult in CloudFinder that near on federation C D F E W K lusion implemented CloudFinder that workloads version be federation federations execution e e times Second CloudFinder of federations ork computing doat two 2332-7790 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/TBDATA.2017.2703830, I\EEE Transactions on Big Data XX 10 


992\226 2012  P  Buncic 223CernVM Minimal Maintenance Appr oach to the V ir tualization,\224 2011  BoincVM https://code.google.com/p/boincvm  V  Cunsolo S Distefano A Pulia\002to and M Scarpa 223Applying in 2010  S Distefano and A Pulia\002to 223Cloud@Home T owar d a V olunteer Cloud,\224 2012  M Fazio A Pulia\002to and S Distefano 223Managing V olunteer Cloud,\224 2013  A Alwabel R J W alters and G B W ills 223T owar ds a V olunteer in 2013  A E S Ahmed A K Alsammak and E Algizawy  223A New used ces,\224 76 2013  GENI https://www geni.net  Apache 223Hadoop,\224 http://hadoop.apache.or g  J Dean and S Ghemawat 223MapReduce Simplied Data Pr ocessing in 2004  K S Shams M W  Powell T  M Cr ockett J S Norris R Rossi and Framein  606\226611  J Hellerstein 223Science in the Cloud,\224 in  2012  A Rezgui and S Rezgui 223A Stochastic Appr oach for V irtual in 2014  A Rezgui G Quezada M M Ra\002que and Z Malik 223A CapacUsing in 2014  B Rochwer ger  D Br eitg and D Hadas J Cacer es A Galis MonCloud in  2010  A Celesti F  T usa M V illari and A Pulia\002to 223Impr oving V irtual in  2010  227\227 223Thr ee-Phase Cr oss-Cloud Federation Model The Cloud in 2010  D Bermbach T  Kurze and S T ai 223Cloud Federation Ef fects of Cost,\224 in 2013  T  Kurze M Klems D Bermbach A Lenk S T ai and M Kunze in  2011  G Fox and D Gannon 223Pr ogramming Paradigm s for T echnical in 2012  D Ar dagna E D Nitto P  Mohagheghi S Mosser  C Ballagny  Petcu model-driven multiple in 50\22656  M Miglierina G P  Gibilisco D Ar dagna and E D Nitto 223Model in 2013  M Miglierina M Balduini N S Hoonejani E D Nitto and Multiin 2013  B Rochwer ger  D Br eitgand A Epstein D Hadas I Loy  K Na Enough,\224 3 2011  B Rochwer ger  D Br eitgand E L Levy  A Galis K Nagin I M Benand Computing,\224 2009  IBM 223Reservoir 224 http://www r esear ch.ibm.com/haifa/pr ojects systech  D V illegas N Bobr of f I Roder o J Delgado Y  Liu A De\223Cloud Model,\224 2012  G Kecskemeti A Kertesz A C Mar osi and P  Kacsuk 223Inter oper in 18\22635  M Makkes C Ngo Y  Demchenko R Strijkers R Meijer  and Multiin 2013  R Buyya R Ranjan and R Calheir os 223InterCloud UtilityScalin C.Berlin 13\22631 Engineering IC2E and Sciences on AFIN Inter\(CCGRID of INTERNET Phys Paradigms Computer 2332-7790 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/TBDATA.2017.2703830, I\EEE Transactions on Big Data Computational IJCSE ofessional Cloud orkshop in MISE for ocessing Applications on COMPUTING ICSE MISE  S Caton and O Rana 223T owar ds Autonomic Management for ces,\224 Interirtualization COMPUTING Journal Development orkshop ence USA IEEE Boston ency Experience OSDI Grid CCGrid on Reasoning XX 11 scalabile may access perspective other e ce security eats because allocate ces data this originating at om ces up its shrinks  ent in CloudFinder we of execution workload A S a Consortium R S LNCS Theory Practice 


2011  J Br ober g R Buyya and Z T ari 223MetaCDN Harnessing Storage Delivery,\224 2009  Emulab http://www emulab.net  Planetlab http://www planetlab.or g  SuraGrid http://www suragrid.or g  Futur eGrid https://portal.futur egrid.or g  Open Science Grid http://www opensciencegrid.or g  V  D Cunsolo S Distefan o and A Pulia\002to 223CloudHome on T op VOIR,\224 2012  S Malik and F  Huet 223V irtual Cloud Rent Out the Rented in 2011  GENI 223GENI Rack Speci\002cation,\224 http://gr oups.geni.net/geni wiki/GeniRacks  227\227 223GENI Rack Pr ojects,\224 her ehttp://gr oups.geni.net/geni wiki/GENIRacksHome  Chamelon https://www chameleoncloud.or g  CloudLab http://www cloudlab.us  HiBench https://github.com/intelhadoop/HiBench  J Buell 223A Benchmarking Case Study of V irtualized Hadoop Per e 2011  IBM 223T eraSort Benchmark,\224 http://www01.ibm.com/support knowledgecenter/SSGSMK educe integration/map educe terasort example.dita ICET XX 12 CLOUD pp 2010  S Malik F  Huet and D Car omel 223Coope rative Cloud ComputCloud,\224 in and UAE and Applications 2332-7790 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/TBDATA.2017.2703830, I\EEE Transactions on Big Data  F  Bermbach M Klems S T ai and M Menzel 223MetaStorage A Consistency-Latency in Sciences Engineering proScience he Big Comrecomcoauthored erences  ifunded committees IEEE LCN in Institute pursusame on  comput comwhere REsearch lie uted inteis these deal au of He and CM essor tHe in receivVirginia include data ation OutPhD ICWS cited top  SerEngineer Medjaheds utomo in and echxComessor Inxinclude comor al apcontrol 


                                


                                 


                            


                           


                 





This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination EEE   19 S Gong Cheng om Xidian  in 2007 and the M.S. and technical  3   He is currently an Associate Professor with Northwestern Polytechnical University. His main research interests are computer vision and pat tern recognition ei Han ently techni ch The ersity cher at the Uni His omputer vision, multi and brain imaging analysis. He es such as IEEE T C t T IONS  ON P A t t T ERN  A A YSIS  AND M CHINE  I I N t T ELLIGENCE AMI I I N t T ER NA t T IONAL J OURNAL  OF  C C O m p MP U t T ER V ISION V T C t T IONS  ON  I I m M GE P SSING  TIP C C ONFERENCE  ON  C C O m p MP U t T ER V ISION  AND P A t t T ERN  R R OGNI t T ION VPR I I N t T ERNA t T IONAL  C C ONFERENCE  ON  C C O m p MP U t T ER V ISION V I I N t T ERNA t T IONAL J OIN t T  C C ONFER ENCE  ON  A A R t T IFICIAL  I I N t T ELLIGENCE IJCAI Prof. Han is an Associate Editor of the I E E E IEEE T RANSAC t T IONS  ON  H H U m M AN M ACHINE  S S YS t T E m M S  Neurocomputing   Processing and Machine Vision and Applications  u ently f  tor ests include emote sensing om e eas  international journal, including Neurocomputing Elsevier Cognitive  Computation Springer International Journal of Image and Graphics  World of Scientific 


