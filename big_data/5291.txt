Parallel Hierarchical Afìnity Propagation with MapReduce Dillon Mark Rose 
Computer Science Department Florida Institute of Technology Melbourne Florida Email drose2010@my.ìt.edu Computer Science Department George Mason University Fairfax Virginia Email jrouly@gmu.edu Mathematical Sciences Department Florida Institute of Technology Melbourne Florida Email rhaber2010@my.ìt.edu Electrical Engineering Department Florida Institute of Technology Melbourne Florida Email nmijatov2005@my.ìt.edu 
 Jean Michel Rouly  Rana Haber  Nenad Mijatovic  Adrian M Peter 
002     002    
Engineering Systems Department Florida Institute of Technology Melbourne Florida Email apeter@ìt.edu 
Abstract 
 
The accelerated evolution and explosion of the Internet and social media is generating voluminous quantities of data on zettabyte scales Paramount amongst the desires to manipulate and extract actionable intelligence from vast big data volumes is the need for scalable performance-conscious analytics algorithms To directly address this need we propose a novel MapReduce implementation of the exemplar-based clustering algorithm known as Afìnity Propagation Our parallelization strategy extends to the multilevel Hierarchical Afìnity Propagation algorithm and enables tiered aggregation of unstructured data with minimal free parameters in principle requiring only 
a similarity measure between data points We detail the linear run-time complexity of our approach overcoming the limiting quadratic complexity of the original algorithm Experimental validation of our clustering methodology on a variety of synthetic and real data sets  
I I NTRODUCTION In 2010 big data was growing at 2.5 quintillion bytes per day This overwhelming volume velocity and variety of data can be attributed to the ubiquitously spread sensors perpetual 
e.g Index Terms 
images and point data demonstrates our competitiveness against other state-of-the-art MapReduce clustering techniques MapReduce Cluster Afìnity Propagation Hierarchical Afìnity Propagation Hadoop 
streams of user-generated content on the web and increased usage of social media platformsÑTwitter alone produces 12 terabytes of tweets every day The sustained nancial health of the worldês leading corporations is intimately tied to their ability to sift correlate and ascertain actionable intelligence from big data in a timely manner These immense computational requirements have created a heavy demand for advanced analytics methodologies which leverage the latest in distributed fault-tolerant parallel computing architectures Among a variety of choices MapReduce has emerged as one of the leading parallelization strategies with its adoption rapidly increasing due to the availability of robust open source distributions such as Apache Hadoop In the present w ork 
we develop a novel MapReduce implementation of a fairly recent clustering approach and demonstrate its f a v orable performance for big data analytics Clustering techniques are at the heart of many analytics solutions They provide an unsupervised solution to aggregate similar data patterns which is key to discovering meaningful insights and latent trends This becomes even more necessary but exponentially more difìcult for the big data scenario Many clustering solutions rely on user input specifying the number of cluster centers e.g K-Means clustering or Gaussian Mixture Models and biasedly group the data into these desired number of categories Frey et al introduced an exemplar-based clustering approach called Afìnity Propagation AP As an exemplar-based clustering 
approach the technique does not seek to nd a mean for each cluster center instead certain representative data points are selected as the exemplars of the clustered subgroups The technique is built on a message passing framework where data points talk to each other to determine the most likely exemplar and automatically determine the clusters 
there is no need to specify the number of clusters a priori The sole input is the pairwise similarities between all data points under consideration for clusteringÑmaking it ideally suited for a variety of data types categorical numerical textual etc A recent extension of the AP clustering algorithm is Hierarchical Afìnity Propagation HAP which groups 
i.e 
and stacks data in a tiered manner HAP only requires the number of hierarchy levels as input and the communication between data points occurs both within a single layer and up and down the hierarchy To date AP and HAP have been mainly relegated to smaller manageable quantities of data due to the prohibitive quadratic run time complexity Our investigations will demonstrate an effective parallelization strategy for HAP using the MapReduce framework for the rst time enabling applications of these powerful techniques on big data problems First introduced by Google the MapReduce frame w ork is a programming paradigm designed to facilitate the distribution of computations on a cluster of computers The ability 
to distribute processes in a manner that takes the computations to the data is key when mitigating the computational cost of working with extremely large data sets The parallel programming model depends on a mapper phase that uses keyvalue identiìers for the distribution of data and subsequent independent executions to generate intermediate results These are then gathered by a reducing phase to produce the nal output key-value pairing This simple yet widely applicable parallelization philosophy has empowered many to take machine learning algorithms previously demonstrated only on 
2013 IEEE International Conference on Cloud Computing Technology and Science 978-0-7695-5095-4/13 $31.00 © 2013 IEEE DOI 10.1109/CloudCom.2013.97 13 


002 003 004 005 006 005 
ij jj ij jj jj ij l ij l ij l ij l ij l i ks.t.k j l ik l ik l<L ij l j l j l jj ks.t.k  i,j l kj l<L jj l j l j ks.t.k j l kj 
1   
002 003  002 
rst ever A Relevant Work e.g 
    002 003 004ä\002 005  005   005   
toy data and scale them to enterprise-level processing In this same vein we adopted the most popular open source implementation of the MapReduce programming model Apache Hadoop to develop the parallelized extension of HAP which we refer to as MapReduce Hierarchical Afìnity Propagation MR-HAP This allows efìcient fault-tolerant clustering of big data and more importantly improves the run time complexity to potentially linear time given enough machines To handle the explosion of available data there is now a vast amount of research in computational frameworks to efìciently manage and analyze these massive information quantities Here we focus on the MapReduce framework 9 10  for f aster and more ef cient data mining co v ering the most relevant to our approach Among the state of the art MapReduce clustering algorithms is Hierarchical Virtual K-means HVKM which was implemented by Nair et al HVKM uses cloud computing to handle large data sets while supporting top to bottom hierarchies or a bottom to top approach Since it derives its roots from K-means HVKM requires one to specify the number of clusters Our MR-HAP implementation does not require presetting the number of required clusters it instead organically and objectively discovers the data partitions In Wu et al the authors propose to parallelize AP on the MapReduce framework to cluster large scale E-learning resources The parallelization happens on the individual message level of the AP algorithm We perform a similar parallelization but signiìcantly go beyond and allow for hierarchical clustering which enables a deeper understanding of the dataês semantic relationships In addition our development is designed to work on a variety of data sources thus our experiments will showcase results on multiple data modalities including images and numerical data as shown in IV The rest of this paper is organized as follows In the next section II we detail the non-parallel HAP algorithm III discusses the MapReduce paradigm and the implementation details for these algorithms The experimental validations provided in IV demonstrate our favorable performance against another clustering algorithm which is readily available in the open source project Apache Mahout Finally  we conclude with a summary of our efforts and future recommendations II H IERARCHICAL A FFINITY P ROPAGATION AP is a clustering algorithm introduced by Frey et motivated by the simple fact that given pairwise similarities between all input data one would like to partition the set to maximize the similarity between every data point and its clusterês exemplar Recall that an exemplar is an actual data point that has been selected as the cluster center As we will brieîy discuss these ideas can be represented as an algorithm in a message passing framework In the landscape of clustering methodologies which includes such staples as K-means K-medoids and Gaussian Mixture Models 5 predominantly all methods require the user to input the desired number of cluster centers AP avoids this artiìcial segmentation by allowing the data points to communicate amongst themselves and organically give rise to a partitioning of the data In many applications an exemplar-based clustering technique gives each cluster a more representative and meaningful prototype for the center versus a fabricated mean HAP introduced by e xtends AP to allo w tiered cluster ing of the data The algorithm starts by assuming that all data points are potential exemplars Each data point is viewed as a node in a network connected to other nodes by arcs such that the weight of the arcs describes how similar the data point with index is to the data point with index  HAP takes as input this similarity matrix where the entries are the negative real valued weights of the arcs Having the similarity matrix as the main input versus the data patterns themselves provides an additional layer of abstractionÑone that allows seamless application of the same clustering algorithm regardless of the data modality  text images general features etc The similarity can be designed to be a true metric on the feature space of interest or a more general non-metric The negative of the squared Euclidean distance if often used as a metric for the similarities The diagonal values of the similarity matrix  are referred to as the preferences which specify how much a data point j wants to be an exemplar Since the similarity matrix entries are all negative values  implies data point has high preference of being an exemplar and implies it has very low preference In some cases as in 3 16 the preference values are set using some prior knowledge for example uniformly setting them to the average of the maximum and minimum values of  or by setting them to random negative constants Through empirical veriìcation we experienced better performance with randomizing the preferences and adopt this approach for most of our experiments Once the similarity matrix is provided to the HAP algorithm the network of nodes data points recursively transmits two kinds of intra-level messages between each node until a good set of exemplars is chosen The rst message is known as the responsibility message and the second as the availability message The responsibility messages  are sent at level from data point to data point portraying how suitable node thinks node is to be its exemplar Similarly availability messages  are sent at level from data point to  indicating how available is to be an exemplar for data point  The responsibility and availability update equations are given in Eq 1 and Eq 2 respectively 1 2 3 where is the number of levels deìned by the user and 
s i j s s s j s s 002 l i j i j 003 l j i j i 002 s 004  003 s 003 c 005 002 002 003 c 005 002 L 
0 0 min max   min 0    max 0   max 0 
14 


Algorithm 1 Input Initialize for do for do Optional end for end for for do end for 
l i l j l j l jj ks.t.k j l kj l i k l ik l ik l i j l ij l ij l ij l ij l ij js.t.j i l ij l ij l i j l ij l ij l ij l l ij l l j l j l j l ij l j th th th th 
1  1 1  2 
2 3 4 5 Update 6 Update 7 Update 10 11 12 Update 
005 
002  002 
006   006 005 005   002 007 007 007 
1 0 1    max\(0  max    max      max    1 arg max  0 0  0 0 0 1 1 1       
l  L j 006  l 004 005 c 004 c 002 002 005 003 s c 003 002 s s s 007 003 002 007 l l l e 003 002 006 003 002 004 005 c e iter l 002 002 003 003 004 005 c s l e S 003 002 c 004 005 S 003 002 c S 003 002 N N L LN c 004 005 N L LN S 003 002 i j l c 004 005 i j i l 010 i l 010 003 002  011 i l j l 010 j l 010 011 j l 004 c 002 005 003 
node-based format exemplar-based format 
 Eq 3 is the self-availability equation which reîects the accumulated positive evidence that can be an exemplar The self-responsibility messages are updated the same way as the responsibility messages To avoid numerical oscillation the responsibility and availability messages are dampened by at every level  HAP also introduces two inter-level messages These messages are denoted by in Eq 4 which receives messages from the lower level and in Eq 5 which receives messages from the upper level At every level the cluster preference is updated using Eq 6 4 5 6 A variety of strategies can be employed to update the similarity matrix to vary level-wise We have achieved good results by simply taking into consideration the cluster relationship of the previous level 7 where is a constant value within This updates the relation between data points in level by negatively increasing the similarity between points that belong to different clusters in level and enforces the similarity between points that fall under the same cluster in level  After all messages have been sent and received the cluster assignments are chosen at every level based on the maximum sum of the availability and responsibility messages as in Eq 8 These cluster assignments can be used to extract the list of exemplars 8 These net message exchanges seek to maximize the cost of correctly labeling a point as an exemplar and gathering its representative members a cluster In Algorithm 1 we detail the pseudo-code implementation of HAP Given this description of HAP we now proceed to discuss MapReduce and our novel parallelization strategy III M AP R EDUCE H IERARCHICAL A FFINITY P ROPAGATION The MapReduce programming model is an abstract programming paradigm independent of any language that allows the processing workload of the implemented algorithm to be balanced over separate nodes within a computer cluster Our overarching MapReduce approach for HAP was motivated by viewing the major update equations for HAP see Algorithm 1 as tensorial mathematical constructs One can simply view these tensorial constructs as two or three dimensional matrices The HAP algorithm can be parallelized because all the updates to the various tensors require only a subset of the information provided Therefore the updates can be split up Hierarchical Afìnity Propagation 1 Similarity S Levels L Iterations and      Iterations Levels eq 1  Dampen eq.2&3 Dampen   eq 4 5&6 8 Update eq 7 9 Levels 8 13 into different jobs and each job will receive the subset of data needed to evaluate the update To achieve a balance between computational partitioning and efìcient formatting for data representation on the Hadoop Distributed Filesystem HDFS all the data is constructed as three dimensional tensors In support of the fault tolerance aspect of MapReduce it is important to retain a copy at all times of the      and tensors Recall    and refer to the Similarity Availability Responsibility and Cluster Preferences respectively To this end even those tensors not required by a job must be passed directly through to the next job For the   and tensors the dimensions represent the nodes the exemplars and the levels Since there are nodes possible exemplars and levels these tensors contain values For the   and tensors the rst two dimensions represent the index and level and the depth dimension has length one Since there are indices and levels these tensors contain values In the sequel for the   and tensors the node dimension will be iterated by  the exemplar dimension will be iterated by  and the level dimension iterated by  As for the   and tensors the index dimensions will be iterated by both and  With these underlying structures data must be deconstructed and represented as key,value pairs for use in the MapReduce framework There are two formats for storing the information node-based and exemplar-based formatting In the  the keys are string tuples  where represents the node represents the level and represents the tensor  The values represented by  are the vectors for the node of the matrix on the level of the tensor In the  the keys are string tuples  where represents the exemplar represents the level and represents the tensor The values represented by  are the vectors for the exemplar of the matrix on the level of the tensor With the data thus represented MapReduce jobs must be constructed to manipulate the information using the given HAP equations In our parallelization scheme MR-HAP is broken down into three separate MapReduce jobs The rst job handles updating   and  The second job handles updating and  These 
15 


2 
kLN M 
2 
Fig 1 Parallelization Scheme rst two jobs loop for a set number of iterations At the end of the iterations the nal job extracts the cluster assignments on each level Due to dependencies set out in the equations the 
002 004 c 004 c 002 004 c j l 010 i i l j 003 005 i l 010 j j l i j l 010 i i l j O kLN k L N S L N N M LN O O kN M LN M LN M LN LN 003 002 S 004 005 c 
1 Updating   and  2 Updating and  3 Extracting Cluster Assignments A Runtime Complexity i.e e.g 
update must occur rst Therefore and are not updated during the rst iteration In all other iterations they occur before the Responsibility update At the start of each iteration the data will be in exemplar-based format After the rst job the data will have switched to node-based format The second job converts the data back to exemplar-based format to begin a new iteration or to be used as input to the nal job See Fig 1 for a visual representation of the parallelization scheme The gure represents what happens to the data during either of the rst two jobs The tensors have been stacked to show how the indices line up The yellow strips on the left represent information being passed to one mapper one strip per mapper The focus of each mapper is on providing the reducers with the necessary information The subsequent focus of each reducer is on performing the tensor updates as deìned in the HAP equations As the data comes out of the job the switch between exemplar-based and node-based formats can be easily seen The output is now ready for use by the next job which will follow a similar ow The following sections will provide in-depth explanations of each MapReduce job This job takes as input the exemplar-based representation of the data and outputs the node-based representation of the data with updated values In the rst iteration and are not updated due to previously mentioned dependencies In this MapReduce job the mapper deconstructs the exemplar-based vectors into node-based values for the reducer to reconstruct node-based vectors Each mapper receives a key describing a unique combination and a value with the corresponding vector The indices of the vector represent the nodes thus the mapper iterates over the vector with  Each reducer receives a key describing a unique combination and a list of values which will be used to reconstruct the 6 node-based vectors the 2 node-based vectors from the level below and the 2 special diagonal vectors The indices of the constructed vector represent the exemplars so the reducer iterates over the vector with  This job takes as input the nodebased representation of the data and outputs the exemplarbased representation of the data with updated values In this MapReduce job the mapper deconstructs the node-based vectors into exemplar-based values for the reducer to reconstruct exemplar-based vectors Each mapper receives a key describing a unique combination and a value with the corresponding vector The mapper iterates over the vector with  Each reducer receives a key describing a unique combination and a list of values which will be used to reconstruct the 6 exemplar-based vectors and the 2 node-based vectors from the level above The indices of the constructed vector represent the nodes so the reducer iterates over the vector with  This job takes as input the exemplar-based representation of the data and outputs the cluster assignments In this MapReduce job the mapper deconstructs the exemplar-based vectors into node-based values for the reducer to reconstruct node-based vectors Each mapper receives a key describing a unique combination and a value with the corresponding vector The mapper iterates over the vector with  Since this is the last step only the required information has to pass to the reducer and the other information can be neglected Each reducer receives a key describing a unique combination and a list of values which will be used to reconstruct the 2 node-based vectors and the 2 special diagonal vectors The reducer iterates over the vector with  A standard sequential HAP implementation must necessarily have a runtime complexity of where represents the number of algorithmic iterations run either as a hard limit or until convergence is reached represents the number of output levels requested and represents the cardinality of the input data set such that the size of is  The runtime complexity is a direct result of iterating over all three dimensions of the tensors for each iteration By implementing the algorithms in the MapReduce framework we are able to achieve superior runtime complexity Under MapReduce the MR-HAP runtime complexity reduces to a linear relationship with the data assuming the total number of Virtual Machines VMs on the cluster  scales to  as  In MR-HAP can only scale up to a maximum of because is limited to the number of tasks that can be evaluated at the same time In this case it is limited to the minimum of the mapper tasks and the reducer tasks where the constant factor six represents the number of tensor identiìcations introduced into the algorithm namely  and  IV E XPERIMENTAL R ESULTS To demonstrate the effectiveness and adaptability of the proposed approach we executed validation experiments on several data sets with a variety of modalities imagery and synthesized numerical point data Where applicable we compared our performance to a popular MapReduce hierarchical clustering algorithm currently available in the Mahout library At its core their hierarchical clustering is based on a level-wise K-means clustering approach thus we refer to it 
                    6 
  007 
16 


6 
purity extrinsic cluster quality metric A Image Segmentation B Scalability and Comparison to HK-Means 
103 103 120 100  10  0 5 3 3 
a Original Mandrill b 15 Exemplars c 7 Exemplars d 6 Exemplars Fig 2 Mandrill 103x103 See text for discussion as Hierarchical K-Means HK-Means With K-means as the foundation HK-Means requires the number of cluster centers as input Since our method does not explicitly impose this requirement we adopted the initialization method of running Canopy clustering also available in Mahout to discover the natural number of centers We then use these cluster centers to seed HK-Means In order to truly gain an objective understanding of MR-HAP performance versus HK-Means we use the to assess their respective aggregation capabilities Hierarchical Afìnity Propagation performs very well in image segmentation tasks as shown in Fig 2  Fig 3 The Mandrill image Fig 2 is of size  which provided 10,609 pixels data points to cluster Similarly the Buttons image Fig 3 is of size  resulting in a data set of 12,000 pixels The similarity input was computed using the negative Euclidean distance between all pixels treating RGB intensities as vectors The diagonal or preference entries were selected as random numbers within  As for the other parameters we set the iterations to 30 and the dampening factor to  To generate the clustered images we recolor all pixels within a cluster with the color of the selected exemplar The number of hierarchy levels for the Mandrill data set was set to  The top right image is the lowest level where the pixels were grouped into 15 clusters The bottom left image is the second level where the pixels were grouped into 7 clusters Finally the bottom right image is the highest level where the pixels were grouped into 6 clusters From these images we can still see the mandrillês shape and most of its colors but at the highest level it appears fuzzier This is because the members of the same clusters were given the color of the exemplar For the Buttons image the number of levels was set to  The top right image is the lowest level where the pixels were grouped into 154 clusters The bottom left image is the second level where the pixels were grouped into 25 clusters Finally the bottom right image is the highest level where the pixels were grouped into 11 clusters The highest level of the hierarchy appears fuzzier than the original image due to similar colors clustering underneath a single exemplar a Original Buttons b 154 Exemplars c 25 Exemplars d 11 Exemplars Fig 3 Buttons 120x100 See text for discussion In order to test the scalability of the MR-HAP algorithm with respect to speed we use the data set Aggregation which is a shape set composed of 788 two-dimensional points The purpose of these tests was to observe trends in algorithm runtime as cluster computing power increased as well as to determine the beneìts of running in a distributed environment as compared to an undistributed environment a single-machine Hadoop cluster Hadoop clusters were provided using Amazon Elastic MapReduce EMR to dynamically create clusters of standard Amazon Elastic Compute Cloud EC2 instances Cluster computing power was scaled both by increasing the number of VMs within a cluster and by provisioning more powerful VMs The two VM instance types used are 1 the m1.small which has 1.7 GB of memory and is considered to have 1 EC2 Compute Unit ECU with 160 GB of instance storage and a 32-bit architecture and 2 the m1.xlarge which has 15 GB of memory 8 ECU 1,690 GB of instance storage and a 64-bit architecture The single-machine Hadoop cluster utilized to simulate an undistributed environment has 8 GB of memory 8 ECU 40 GB of machine storage and a 64-bit architecture For comparison to another state-of-the-art MapReduce clustering methodology our MR-HAP algorithm was benchmarked against HK-Means Due to its inherently parallel design MR-HAP immediately begins to beneìt from being placed in a distributed environment Represented by a solid blue line in Fig 4 MR-HAP runtime decreases by 64 from 320 minutes to 115 minutes when cluster computational power is increased by just 4 additional ECU MR-HAP eventually reaches the threshold of a linear relationship with the size of the input data at a runtime of around 20 minutes which is a 94 decrease from the single ECU cluster Furthermore at its best MR-HAP performs 66 faster in a distributed environment than the undistributed environment which is represented by the blue dotted line in Fig 4 In contrast the Mahout HK-Means algorithm used in this experimentation indicated by the solid green line in Fig 4 is not parallelized to the extent of MR-HAP Each single iteration of K-Means is structured under Mahout to distribute over a Hadoop cluster but the hierarchical Top Down structure requires iterative 
   
 006  L L 
17 


Fig 4 Time vs Number of EC2 CPUs Our MR-HAP better utilizes available compute resources to signiìcantly improve runtime executions of K-Means for each level This lack of an overall parallelization scheme results in reduced performance at scale than MR-HAP HK-Means runtime initially increases by 8.5 when ECU is increased from 1 to 10 due to Hadoop cluster overhead including network latency and I/O time However at 10 ECU HK-Means overcomes this overhead and begins to beneìt from the MapReduce parallelization scheme This results in an eventual 16 runtime decrease between 1 and 80 ECU at which point HK-Means eventually reaches a linear relationship with the data at a runtime around 225 minutes Unlike MR-HAP HK-Means never surpasses its undistributed runtime threshold of 146 minutes indicated by the green dotted line in Fig 4 Finally at its best HK-Means runs 90 slower than MR-HAP requiring 226 minutes of execution compared to MR-HAPês 23 minutes With signiìcantly faster runtimes MR-HAP still posts purity levels competitive with HK-Means shown in Fig 5 This combination of speed and high performance is ideal for processing big data in a largescale cloud computing environment V C ONCLUSION The need for efìcient and high performing data analysis frameworks remain paramount to the big data community The AP clustering algorithm is rapidly becoming a favorite amongst data scientists due to its high quality grouping capabilities while requiring minimal user speciìed parameters Recently a multilayer structured version of the AP algorithm HAP was introduced to automatically extract tiered aggregations inherent in many data sets HAP is modeled as a message-based network that allows communication between nodes and between levels in the hierarchy and mitigates many of the biases that arise in techniques that require one to input the number of clusters In the present work we have developed the rst ever extension MR-HAP to address the big data problemÑdemonstrating an efìcient parallel implementation using MapReduce that directly improves the runtime complexity from quadratic to linear The novel tensor-based partitioning scheme allows for parallel message updates and utilizes a consistent data representation that is leveraged by map and reduce tasks Our approach seamlessly allows us to cluster a variety of data modalities which we experimentally showcased on data sets ranging from synthetic numerical points to imagery Our analysis and computational perforFig 5 Purity levels of MR-HAP vs HK-Means MR-HAP posts results highly competitive with HK-Means mance is competitive with the state-of-the-art in MapReduce clustering techniques R EFERENCES  S Humbeto v  Data-intensi v e computing with MapReduce and Hadoop  in  2012 pp 1Ö5  T  A S F oundation Hadoop 1.1.2 documentation  The Apache Software Foundation 03 2013 A v ailable http://hadoop.apache.org/docs/r1.1.2  I E Gi v oni C Chung and B J Fre y  Hierarchical af nity propagation  vol abs/1202.3722 2012  J A Hartigan and M A W ong  Algorithm AS 136 A k-means clustering algorithm  vol 28 pp 100Ö108 1978  G McLachlan and D Peel  John Wiley  Sons Inc 2000  B J Fre y and D Dueck Clustering by passing messages between data points  vol 315 2007  J Dean and S Ghema w at MapReduce simpliìed data processing on large clusters in  USENIX Association 2004 pp 10Ö10  C.-T  Chu S K Kim Y A Lin Y  Y u G Bradski A Y  Ng and K Olukotun MapReduce for Machine Learning on multicore in  2007 pp 281Ö288  C Ze wen and Z Y ao P arallel te xt clustering based on mapreduce  in  2012 pp 226Ö229  H W ang Y  Shen L W ang K Zhufeng W  W ang and C Cheng Large-scale multimedia data mining using MapReduce framework in  2012 pp 287Ö292  R M Este v es T  J Hack er  and C Rong Cluster analysis for the cloud Parallel competitive tness and parallel k-means for large dataset analysis in  2012 pp 177Ö184  T  Nair and K Madhuri Data mining using hierarchical virtual k-means approach integrating data fragments in cloud computing environment in  2011 pp 230Ö234  F  W u W  W ang H Zhang and Y  Zhuang  IGI Global 2010 ch The Clustering of Large Scale E-Learning Resources pp 94Ö104  T  A S F oundation  Apache Mahout Scalable machine learning and data mining The Apache Software Foundation 2013 Available http://mahout.apache.org  L Kaufman and P  Rousseeuw  Clustering by means of medoids   vol 87-3 1987  J Xiao J W ang P  T an and L Quan Joint af nity propagation for multiple view segmentation in  2007 pp 1Ö7  H Lu K N Plataniotis and A N V enetsanopoulos  A surv e y of multilinear subspace learning for tensor data  vol 44 no 7 pp 1540Ö1551 Jul 2011  N Sahoo J Callan R Krishnan G Duncan and R P adman Incremental hierarchical clustering of text documents in  ACM 2006 pp 357Ö366  A Gionis H Mannila and P  Tsaparas Clustering aggre gation   vol 1 Mar 2007 
AICT CoRR Applied Statistics Finite Mixture Model Science OSDI NIPS CGC IEEE CloudCom IEEE CloudCom IEEE CCIS Handbook of Research on Hybrid Learning Model Advanced Tools Technologies and Applications Reports of the Faculty of Mathematics and Informatics Delft University Technology IEEE Computer Vision Pattern Recognition CIKM ACM TKDD 
18 


          
Manager Node Job 4 Job 1 Job 3 Job 2 Queue Manager Node Node Node Job Simulator Predictor Task Job Simulator Predictor Task Time Simulation Selection Hadoop simulation interval t2 state capture sim start t2+x fineätune sim end t1 state capture sim start t1+x fineätune sim end Dynamic Scheduling Data Prefetching Online Prediction Online Prediction Figure 10 Use Cases of Online Prediction Framework same data block from disks To get the best performance and efìciency scheduler should work together with both caching and prefetching We will use an example to illustrate how caching and prefetching can improve pe rformance and efìciency of Hadoop Suppose 3 jobs as shown in Table II are submitted to a MapReduce system The nish column shows estimated nish time of each job in the baseline MapReduce system With these numbers in a baseline system the 3 jobs account for 340 clusteräseconds in a baseline MapReduce system With caching enabled only j ob 3 can beneìt from caching because it accesses data 
d d d d d d 
1 1 1 1 1 1 20 
which was also accessed by job 1 Job 1 and job 2 access data for the rst time so these data cannot be cached and cannot be sped up Assume job 3 get a speed up of 50 and it runs for 50 seconds Therefore The 3 jobs account for 290 clusteräseconds in total with caching enabled If we can predict task locations and prefetch data blocks for all tasks we can preload data into RAM for each job and we can speed up each job by 50 regardless of whether data has ever been accessed before The 3 jobs account for 170 clusteräseconds in total with prefetching enbaled In terms of performance Prefetching outperforms baseline by 100 and caching by 70 Prefetching can improve performance of the system but it imposes more load on underlining disk resource Because we prefetch data again for job 3 is read twice  once for job 1 and once for job 3 Total data read for prefetching is 3 GB the same as baseline In comparison with caching enabled we do not need to read data again for job 3 because is already cached in RAM Total d ata read for caching is 2 GB a 50 improvement over baseline and prefetching Job Start Finish Accessing data 1 0 100 d1 1GB 2 10 150 d2 1GB 3 200 300 d1 1GB Table II J OBS IN A M AP R EDUCE SYSTEM  Job Baseline w caching w prefetching w both 1 100 100 50 50 1GB 1GB 1GB 1GB 2 140 140 70 70 1GB 1GB 1GB 1GB 3 100 50 50 50 1GB 0GB 1GB 0GB Overall 340 290 170 170 3GB 2GB 3GB 2GB Table III P ERFORMANCE AND RESOURCE CONSUMPTION OF THE JOBS  In fact caching and prefetching can work together and make the system optimal Consider the same example with caching and prefetching both enabled Consider with prefetching enabled and we k eep data prefetched and pro cessed in RAM as we would do with caching enabled All jobs can be sped up by 50 and total CPU used is 170 clusteräseconds For data read from disks we donêt need to read again for job 3 so total data read from disks is 2 GB Overall Table III summarizes the pros and cons of caching and prefetching The default scheduler in MapReduce JobQueue TaskScheduler is a rstäcomeäìrstäserve FCFS scheduler Under JobQueueTaskScheduler all jobs are sorted by submission order into a queue and the scheduler always picks new tasks from the rst job in the queue until the rst job nishes and the second job is promoted to be the new rst job A major drawback of FCFS is that subsequent jobs must wait until preceding jobs nish If the rst job in the queue is a large job subsequent small jobs must wait for a long period before they are executed A new class of schedulers including Quincy 13 and D el ay S c hedul i n g 9 tries to solve the problem of long delays for small jobs in FCFS Multiple jobs are allowed to run concurrently and share the resource of a cluster in term of task slots fairly so small jobs are not blocked by longärunning large jobs Wang et al 11 ha v e s h o w n d if ferent w o rkloads p erform differently under different scheduler Hence the scheduling strategy should be determined at runtime based on the properties of the jobs currently running in the cluster and the ones waiting in the queue Our online prediction framework can solve exactly that problem by predicting the execution time of the current workload under different scheduler policies in faster virtual time In Section III we have shown that running the simula tion every seconds is sufìciently frequent to be accurate as running time of most tasks is in the minutes Thus the system can provide feedback in time to make the next real scheduling decision Of course this technique should not be 
B Dynamic Scheduling 
305 
305 
305 
305 


taken to an extreme where the system is forced to switch back and forth between two schedulers that produce very similar results for the current workload To prevent this and any overhead associated w ith that scenario the system should change to a different scheduler only if that results in signiìcant performance gain V R ELATED W ORKS shares its goal of integrated simulation with another MapReduce simulator Mumak 14  b ut di f f ers from it in that Mumak is designed to run ofîine driven by a trace whereas our runs online along with the real and is driven by the live workload on the cluster In  we must predict execution time of a new task based on historical Another difference is that runs periodically and each time it runs we must take snapshots of  task scheduler and worker nodes and replicate them into  Several recent works 12 7  8 are b as ed on pre deìned performance models within each MapReduce task We adopt a simple linear model in this paper Since MapRe duce jobs are a collection of a large number of smaller tasks simple linear model is accura te enough for a computation framework like Hadoop There has been other extensive previous research in simulation of MapReduce workloads and setups 14   15  12    16  17    18  is unique in its goal of predicting a live MapReduce task Compared to all other MapReduce simu lators our prediction framework is arguably more realistic easier to verify and evaluate and can directly beneìt system performance Our framework predicts what is about to happen in the current system in the near future and therefore predictions can be veriìed and evaluated The results from the prediction can be readily used to improve performance of the live system In contrast other simulators either try to match what has already happened in the past or simulate a particular cluster environment ofîine VI C ONCLUSION In this paper we have described a simulationäbased online prediction framework for Hadoop We design and employ our simulator to predict nearäfuture system behavior based on the current state of the Hadoop scheduler The information can then be incor porated into the scheduler to better allocate jobs to nodes and achieve overall higher performance We evaluate the proposed simulation frame work using TeraGen TeraSort grep and wordcount We nd that for studied applications 95 of the predicted task execution times are within 10 of the actual values and 80 of predicted task start times in a 30äsecond window are within seconds of the actual start times In our future work we plan to leverage the prediction framework to implement prefetching for MapReduce to improve latency of initial I/O and a dynamic multiästrategy scheduler that can switch between multiple scheduling strategies based on current workload A CKNOWLEDGMENT This work was sponsored in part by the NSF under Grant No CNSä1016408 CNSä1016793 CCFä0746832 and CNS 1016198 R EFERENCES 1 A pache S of t w ar e F oundat i on   A p ache H adoop  F eb  2011  A v a ilable h ttp://hadoop.apache.org  J  D ean and S  G hema w at MapReduce Simpliìed Data Processing on Large Clusters in 
Job Simulator Job Simulator JobTracker Job Simulator Job Simulator JobTracker Job Simulator Job Simulator 
2 
 2004 pp 137Ö150 3 B al deschw i e l e r  E r i c   H o r t onw or ks Mani f e st o   O n l i n e  Available http://hortonworks.com/blog/ourämanifesto 4 D  B or t h akur  S  R ash R  S c hmi d t  A  A i yer  J G r ay  J S Sarma K Muthukkaruppan N Spiegelberg H Kuang K Ranganathan D Molkov and A Menon Apache hadoop goes realtime a t Facebook in  Jun 2011 p 1071  A mazon Amazon E lastic MapReduce  E M R   Online Available http://aws.amazon.com/elasticmapreduce 6 H  M ont i  A  R  B u t t  and S  S  V azhkudai  C A T C H  A Cloudäbased Adaptive Data Transfer Service for HPC in  2011 7 H  H er odot ou H  L i m G  L uo N  B o r i so v  L  D ong F  B  Cetin and S Babu Starìsh A Selfätuning System for Big Data Analytics in  2011 pp 261Ö272 8 H  H er odot ou H adoop P e r f o r mance Model s   D u k e U n i v er  sity Tech Rep CSä2011ä05 Feb 2011  M  Z aharia D  B orthakur  J  S en S arma K E l melee gy  S Shenker and I Stoica Del ay scheduling a simple tech nique for achieving locality and fairness in cluster schedul ing in  2010 pp 265Ö278  G  A n ant h anar ayanan A  G hodsi  A  W a ng D  B o r t hakur  S Kandula S Shenker and I Stoica Pacman coordinated memory caching for parallel jobs in  2012 pp 20Ö20  G  W a ng A  R  B u t t  H  Mont i  and K  G upt a T o w ar ds Synthesizing Realistic Workload Traces for Studying the Hadoop Ecosystem in  2011  G  W a ng A  R  B u t t  P  Pande y  and K  G upt a A si mul a t i o n approach to evaluating desig n decisions in mapreduce setups in  2009  M I s ar d V  P r abhakar a n J C u r r e y  U  W i eder  K  T alw a r  and A Goldberg Quincy fair scheduling for distributed computing clusters in  2009 pp 261Ö276  A  C  Mur t hy   Mumak M ap R educe S i m ul at or   MAPREDUCEä728 Apache JIRA 2009 Onlin  A v a i l a bl e http://issues.apache.org/jira/browse/MAPREDUCEä728  A  V e r ma L  C h er kaso v a  a nd R  H  C ampbel l  P l a y I t Again SimMR in  IEEE Sep 2011 pp 253Ö261  F  T e ng L  Y u a nd F  Magoul  es SimMapReduce A Sim ulator for Modeling MapReduce Framework in  IEEE Jun 2011 pp 277Ö282  Y  L i u M L i  N  K  A l h am and S  H ammoud H S i m A MapReduce simulator in enabling Cloud Computing  May 2011  S  H ammoud MR S i m  A di scr e t e e v ent b ased MapR educe simulator in  IEEE A ug 2010 pp 2993Ö2997 
OSDI SIGMOD Proc IPDPS CIDR Proc EuroSys NSDI MASCOTS MASCOTS Proc sosp 2011 IEEE International Conference on Cluster Computing 2011 Fifth FTRA International Conferen ce on Multimedia and Ubiquitous Engineering Future Generation Computer Systems 2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery 
306 
306 
306 
306 


state of innovation stakeholder  node PQ It  s a balanced node Based on this, we could calculate the  node PQ Calculation process is: set different inn ovation stakeholders state i U  and j U Value of ij 000T can be get from  innovation time difference. Innovation stakeholdersí social effect and industrial effect can be obtained upon ij B  and ij G set according to relation between innovation stakeholders  Model 4.1 points out  that the value of Gij  directly affects social benefits and sector benefits. Large Gij  can lead to increasing benefits of the entire industry and the entire social growth Bij reflects big organizationís impact on businesses. Only strengthening the inter agent association within big organization and enhancing the str ategic partnership between enterprises can jointly promote the development of the entire industry, and bring more social benefits, so that each agent can be improved   5 Summary This paper puts forward the concept of the big organization based on the CSM t heory. It introduces the basic implication of the big organization and theoretical framework of the big organization including: the big organization's perspective  overall perspective, dynamic perspective, and new resource perspective; the big organizat ionís sense  the purpose of the organizational structure is innovation, organizational activities around the flow of information, breaking the traditional organizational structure, encouraging self run structure, and blurring organizational boundaries; the big organizationís platform  the platform ecosystem of the big organization ; the big organizationís operation mode  borderless learning mode, and cluster effect; the big organizationís theory  active management theory  leading consumers, and culture  entropy reduction theory  negative culture entropy and humanistic ecology theory  inspiring humanity, and circuit theory  a virtuous circle, and collaborative innovation theory  collaborative innovation stakeholder. This paper also discusses culture entropy reduction theory of the big organization  negative culture entropy, and coordinated innovation theory  innovation stakeholders collaboration. Culture entropy change model and collaborative in novation model are constructed   The research has just begun for the big organization. It also needs further improvement but remains the trend of the times   Reference  1  Gordon Pellegrinetti, Joseph Bentsman. Nonlinear Control Oriented Boiler Modeling A Benchmark Problem for Controller De sign [J  I E E E tr a n s a c tio n s o n c o n tr o l s y s te m s te c h n o lo g y 2 0 1 0  4 1\57 65  2  Klaus Kruger, Rudiger Franke, Manfred Rode Optimization of boiler start up using a nonlinear 457 


boiler model and hard constraints [J  E n e r gy 201 1 29   22 39 2251  3  K.L.Lo, Y.Rathamarit  State estimation of a boiler model using the unscented Kalman filter [J  I E T  Gener. Transm. Distrib.2008 2 6\917 931  4  Un Chul Moon, Kwang. Y.Lee. Step resonse model development for dynamic matrix control of a drum type boiler turbine system [J IE E E  T ra nsactions on Energy Conversion.2009 24 2\:423 431  5  Hacene Habbi, Mimoun Zelmat, Belkacem Ould Bouamama. A dynamic fuzzy model for a drum boiler turbine system [J  A u to m a tic a 2 0 0 9 39:1213 1219  6  Beaudreau B C. Identity, entropy and culture J   J o ur na l  o f  economic psychology, 2006, 27\(2 205 223  7  YANG M, CHEN L. Information Technique and the Entropy of Culture J  A cad e m i c E x ch a n g e  2006, 7: 048  8  ZHANG Zhi feng. Research on entropy change model for enterprise system based on dissipative structure J  Ind ustrial  Engineering and  Management 2007, 12\(1\ :15 19  9  LI Zhi qiang, LIU Chun mei Research on the Entropy Change Model for Entrepreneurs' Creative Behavior System Based on Dissipative Structure J  C h i n a S of t S c i e n c e  2009   8  1 62 166   458 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





