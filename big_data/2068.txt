Association Analysis with Interval Valued Fuzzy Sets and Body of Evidence Ping Chen Andre de Korvin and Chenyi Hu Department of Computer and Mathematical Sciences University of Houston-Downtown Houston Texas 77002 Abstract  Association analysis is proven to be one of the most useful techniques in data mining to ana lyze large datasets In this paper we apply associa tion analysis to fuzzy datasets and show how it works in decision making process We define three funda mental concepts 
belief compatibility and plausibil ity for association analysis in terms of fuzzy logic We further extend our study on association analysis with interval valued fuzzy sets and body of evidence Applications with sample data are discussed I Introduction In this section we briefly introduce data mining ass ciation analysis in data mining and fuzzy sets A The need for data mining The fast growing tremendous amount of data collected and stored in large databases has exceeded human abil ity for comprehension As a result data 
collected in large databases often become 223data tomb\222\222-data archives that are seldom revisited On the other hand, important de cisions are often made not based on the information-rich data but rather on a decision maker\222s intuition simply because the decision maker may not have the tools to extract the valuable knowledge embedded in the vast amounts of data 5 The widening gap between data and information calls for a systematic development of tools that will turn data tombs into gold nuggets of knowledge Data mining has attracted a great deal of attention recently Data mining 
tools that perform data analysis may uncover important data patterns contributing greatly to business strategies knowledge bases and scientific and medical research B Association analysis Association analysis is very useful for data analysis It can generate association rules showing the items which frequently appear together For example, in a medical database if most patients who had cough and fever and headache actually suffer flu Then one may deduce the rule 223if a patient has cough and fever and headache then hehasfluverylikely\224 Let S={sl,sz,...,s beaset of attributes\(symptoms diseases and T 
be a database such that t c S for every t E T Let X and Y be two distinct subsets of S If it is true for most t E T if t contains X then t contains Y as well then we may draw an association rule Z as X  Y from the database T There are two parameters associated with a rule sup port and confidence To define these parameters we use T and 
T to denote the largest subsets of T that con tain X and both X and Y respectively It is obvious T 2 T C T The support of the rule X f Y is the ratio of the cardinality of T to the cardinality of T i e  IT\222\222 The confidence of the ruie X  Y is the ratio of IT1 221 ITS I ITJ\222 the cardinality of T to the 
cardinality of T  In this paper ow main concern is support C Mining association rules Usually mining of association rules includes two steps 5 Find all frequent itemsets All frequent itemsets should appear at least as frequently as minimum support count Generate strong association rules from the frequent sets strong association rules should satisfy mini mum support and confidence threshold The dominant part of an association rules discovery algorithm is to find the subset T of 
T with strong sup port Once T is deemed to have strong support it is an easy task to decide which item in the itemset can be the consequent by using the confidence threshold which have strong support In this paper our concern is mainly in the first step We use Apriori algorithm proposed in l to find the fre quent large itemsets and then we show how to use these frequent itemsets in a fuzzy logic-based decision-making system 0-7803-7280-8/02/$10.00 02002 lEEE 518 


D Fuzzy sets Our study in this paper is focused on fuzzy datasets This is because of that a patient may have 223some fever\224 vs 224high fever\224 or 223mild cough\224 vs 223severe cough\224 in reality To describe such 223fuzzy\224 properties we need to use fuzzy logic Consider a set M we say that A is a fuzzy subset of M if A is a function from M into 0 11 The value of A at an element z of X is the membership of 2 in A The operations of union intersection and complement can he extended to fuzzy subset by AV B mm IA\(z z AA B z min A\(z z and By Cardinality of A denoted by IAl we mean r A\(z A z 1  A\(z 2 11 Association analysis with fuzzy set A Motivation Information gained from data mining is often not ready to he used by domain experts For example association analysis often yields thousands of rules which are still hard to analyze Therefore we need to integrate data mining tools with other analysis techniques to convert the information to knowledge which can be used more easily by human beings On the other hand in the field of decision making, huge amount of data often has to he processed first and due to the lack of efficient analysis tools often we have to use the estimates from domain ex pert or portion of the data instead of results from a thor ough analysis of available data As we how one goal of data mining is to deal with large datasets efficiently and effectively Therefore integrating data mining tech niques with decision making systems will yield a better analysis system In solving real world problems accurate data may not exist and accurate computation may be impossible or not necessary With uncertainty and ambiguity however people need to make unambiguous high-quality decisions Fuzzy logic bridges the gap between ambiguous process of decision making and explicit demonstration of how a decision is made or possible refinement and justification Fuzzy logic control system has been successfully used in consumer appliances industrial process control and au tomotive applications just to cite a few examples 4 6 The integration of data mining fuzzy logic and deci sion making techniques can benefit all these three areas We will show how to perform association analysis on a fuzzy set and transform the association rules further into decisions in the following sections of this paper 171 B Deriving association rule in a fuzzy dataset In a traditional transaction set we are sure that one transaction contains a specific item or not But in some cases this 223containment\224 relation can he better described using fuzzy values Now we discuss how to generate as sociation rules on a fuzzy set using Apriori algorithm We will define associations by the use of three functions compatibility belief and plausibility by extending their traditional definitions to our present setting B.1 Finding large itemsets Let tk be a fuzzy data item and tk  5 Pjk/Sj for k  j 1  N We define support\(sj  r N h where 0 5 I Pjk I 1 and N is the size of dataset We also define fuzzy large itemsets of cardinality one by A:\222  sjlsupport\(sj 2 7 where T is some rectified threshold for 223large\224 we define the support of 2-itemset as The fuzzy large itemsets of cardinality two can he de fined as A  sj,si sj,si 2 T 2 Similarly  sj,~r,~k j,~i,sk 2 7 4 In general we can define the support and large m itemset A!\224 as well for n 5 N B.2 Compatibility belief and plausibility After the definition of fuzzy large itemsets we now try to perform association analysis Let d be a possible fuzzy consequent i.e d  a;/S where SI Sz   S are sets of items in S as defined in the previous section and 0 5 ai 5 1 indicates to what extent the set of Si implies the presence of d Our objective is to decide any possible relationships between d and a given fuzzy set of items P  C,fl;/S In our sample application we are interested in 223large itemsets\224 S1,Sz _ S i We now define compatibility of d with P as 0-7803-7280-8/02/$10.00 02,2002 IEEE 519 


Comp\(d,F u  supd\(z 5 where the sup is taken over 2 such that P\(z  U Thus Comp\(d F is a fuzzy subset of 0 11 For the traditional setting of compatibility, readers may refer 7 For P E U.A<\224 we define a mass on P as   I m\(P  Let I\(S C d   we can define the belief of d where T E A c suppow 222 1dAS.l ldl 1-1 is a consequent of P by Bel\(d  XI c d S where the sum is over all S E U,A i.e over all large sets Let J\(S;#d  max ab the max is taken over sk E S we can also define the plausibility as Pls\(d  c J\(S,#d S where the sum is over all Si E U,A For the traditional settings and use of belief and plau sibility, readers may refer 9 lo Note the fact that I\(S C d 5 J\(S;#d The operator I plays the role of the 223weakest implication\224 of S  d and J plays the role of largest intersection i.e possibility of S and d C A sample application In this sample application we assume that a dataset con sists of symptoms of different patients when a disease is prevailing we could have thousands of patients in a data base The symptom could happen in different diseases Each of the diseases is represented as a fuzzy set of symp toms We perform the association analysis to determine what symptoms and their combinations may occur most frequently which can give us some hints of what diseases are prevailing for the group of patients C.1 Data We denote the symptom set as S  A,B,C,D,E and a disease as d Here is a sample symptom set for four patients The number between 0 and 1 attached with each symptoms reflects the degree of presence for that symptom Table 1 0-7803-7280-8/02/$10.00 02,2002 IEEE C.2 Finding large itemsets From data in Table 1 we try to derive the large item sets as we described in the previous section We as sume that large support should be no less that 0.38 i.e T  0.38 Then the supports for all 1-itemsets are supportA  0.38 supportB  0.48 supportC  0.50 supportD  0.23 and supportE  0.48 Hence the large 1-itemset is Ai1  A B C E Since D is excluded we only consider 2-itemset AB AC AE BC BE CE The supports for these 2 itemsets are supportAB  0.10 supportAC  0.35 supportAE  0.18 supportBC  0.23 supportBE  0.40 and supportCE  0.28 Hence the large itemset is Ai\222  BE Since Ai2\has only one member Ai3 A and Ai5 are empty C.3 Compatibility belief and plausibility Provided that the symptoms for a patient is P  0.38/B  0.38/C  0.32/BE We try to find the com patibility he may suffer the disease d  0.4/A  0.8/C  0.5/E Based on the given data, the possible U\222S in 5 over which Comp\(d,P  0 are 0.38 and 0.32 Thus Comp\(d,P 0.38  supd\(z where z E B,C so Comp\(d P 0.38  0.8 Comp\(d P 0.32  0 because d  0 on BE We also calculate the belief of our fuzzy decision First we use the support from last section to compute the mass for each large itemset m\(A  0.17 m\(B  0.21 m\(C  0.22 m\(E  0.21 and m\(BE  0.19 We also have I\(A c d  0.4 I\(B c d  0 I\(C c d  0.8 I\(BE c d  0.25 and I\(E c d  0.5 So Bel\(d  0.4 For the plausibility we have J\(A#d  0.4 J B#d  0 J\(C#d  0.8 J\(E#d  0.5 and J BE#d  0.5 So Pls\(d  0.46 In this sample application we have found the compat ibility belief and plausibility in association analysis with fuzzy set Compatibility gives us an estimation of how much some disease is compatible with symptoms present to some degree in a population The belief and plau sibility functions are traditionally used to measure the amount of uncertainty present 111 Association analysis with interval valued fuzzy set A Motivation In Table 1 we assumed that the expert was able to pin point exactly the amount of presence of symptoms on patients Typically experts feel more comfortable to de scribe the presence of symptoms by a range i.e an in 520 


terval More realistically the symptom table should look like Table 2 In this section we try to define the concepts of com patibility belief and plausibility for interval valued fuzzy sets We still focus on 223large itemsets\224 B Deriving association rule in interval valued fuzzy B.1 Finding large itemset We now view data as a fuzzy set where the val ues of the membership functions are intervals tk  c@Jk,F3k]/sJ Then we can define 223the lower support\224 and 223the upper support\224 LSupport\(s  1 1 y k usupport\(s  yxFJ k Thus we can de fine the 1-itemset support as dataset If the threshold for large itemset is T we can define the concept of 223large itemset to some degree\224 hy look ing at the suhsetbood of support\(sj in 511  T i.e w\([l\222ul 222 222 where UJ is the width function of an interval i.e w\([l,u U  1 UJ\([44 We now take two parameters  the threshold T the degree to which we would like the support to exceed T and we call the degree e The support of Zitemset can be similarly defined as Proceeding as the above we can define A$:i and etc B.2 Compatibility belief and plausibility Let d  E,]/S be a possible fuzzy consequent where SI Sz 222   S are 223large itemsets to degree e\224 as we defined and 0 I 5 Ei 5 1 Let P he a set in U,A i.e P  n Si Our objective is to define the compatibility belief and plausibility between d and P We now try to perform association analysis To de he 223interval-valued\224 compatibility a natural extension of 8 is i i We define the mass of P as m\(P  221,\224\222I CUT C1T  where T E U,A$\223 and support\(T  T,uT In the standard situation CsEonap m\(S should 223add up to l\224 in the classical case we let go of this condition By analogy with the previous section we can define the quantities of I\(S C d and J\(S;#d when the support of Si is an interval and d is also interval valued I\(S Cd    c G 13 sresi SkE.7 Note I\(S c d 223is less\224 than J\(S,#d in the sense that max{I\(S C d Si#d  J\(Si#d Also note that J and I play the role of 223largest intersection\224 and 223fraction of S in d\224 since Si      l/sk    and k,Ekk A 1  h and I is With the I and J defined as the above we can extend the belief and plausibility defined in the previous section in terms of interval valued functions as Bel\(d  xI\(S c d S 15 Id A S.1 where I\(d c S   and the sum is over all Si E u,A!\223 i.e over all large itemsets Id1 1 suPPort\(sj,sl  Ejk A&,Fjb Apik 10 b We can also define the plausibility as The large 2-itemset to degree e can be defined as Pls\(d  CJ\(Si#d S 16 2 e where J\(S,#d  maxcuk the max is taken over sk E Si the sum is over all S E U,A!n w\(support\(sj SI n z 2 T 11 0-7803-7280-81026 10.00 02002 IEEE 52 1 


C A sample application C.1 Finding large itemsets In this sample application we use data in Table 2 to derive the large itemsets We assume that large support should be no less that 0.38 i.e T  0.38 and e 2 0.15 supportA  30,.40 supportB  40,.53 supportC  40,.50 supportD  l.20 231 and supportE  I.40 581 By equation 9 we have the large 1-itemset A$!i  A,B C E Since D is excluded we only consider AB AC AE BC BE CE among all 2-itemsets From 10 and ll we have the large 2-itemset AZi  BE Since A$*\222 bas only one member A$:L and beyond are empty C.2 Compatibility belief and plausibility Provided that the symptoms for a patient is P  0.2,0.38]/B  0.30,0.35]/E  0.3,0.4]/BE We try to find the compatibility he may suffer a disease d  0.5,0.6]/B  0.8,0.9]/E  0.3,0.4]/F From the given data a possible U\222S over which Comp\(d,F  0 are  0.3,0.35 and 0.3,0.4 Thus Comp d,F 0.2,0.38  0.5,0.6 Comp d,F 0.3,0.35  I.8 9 and Comp d,F 3 411  0 To find the belief and plausibility of our fuzzy decision we need to calculate the mass for each of large itemsets first From the definition of mass we have m\(A  12,.22 m\(B  16,.29 m\(C  16,.27 m\(E  1.16 311 and m\(BE  14,.26 We also have I\(A c d  0 I\(B C d  5 6 I\(C C d  0 I\(BE C d  0 and I\(E c d  8,.9 So Bel\(d  21,.45 For the plausibility we have J\(A#d  0 J B#d  5 SI J\(C#d  0 J\(E#d  1.8 9 and J BE#d  0 So Pls\(d  21 451 In this sample application the Bel\(d and Pls\(d have the same value hut in generally they will yield different results The supports for all 1-itemsets are IV Association Analysis Using Body of Evidence A Motivation One problem in the application of fuzzy logic to decision making systems is the determination of relevant mem bership functions Sometimes experts can not agree on which value or interval should be assigned It makes more sense to ask each expert to give a lower and upper bound on the membership values This naturally leads to the concept of 223body of evidence\224 Early research on body of evidence can be found in SI A body of evidence is a function from the input space into a finite set of in tervals I with probability p pi denotes the fraction of experts voting for I to be the membership interval Let us use Bi to denote the body of evidence that represents symptom s present in patient i in our sample applica tion Then B is a collection of intervals a:i b with associated probability p where rp  1 r With this in mind we can have a symptom set of our sample application as follows Table 3 One option to analyze this case is to reduce the body of evidence to a single interval hy taking the 223average\224 interval We take a different approach here B Deriving association rule in body of evidence B.l Finding large itemset Corresponding to B6 we have membership inter vals a b with associated probability p If T is a threshold for large itemsets we denote the degree for which the interval a b3 exceeds T as g\(s i T T We now define the extent to which 223B exceeds e\224 by the  fuzzy set cp\222Jg\(s i T 7 where E\222 specifies over g\(s i T T 2 e Now extending the concept of how large a 1-itemset is from the previous section we define L\(sk e r as 17 Note that 0 I L\(sk,e I 1 A natural definition on how large of a 2-itemset is L\(\(sk,sd,e  L\(sk,e nL\(sl,e 18 Proceeding inductively in the same way we can define how large an n-itemset of SI s2   sn is First we need a third parameter \(in additional to T and e named 7 The fuzzy set of large item sets of symptoms can then be defined by L  L\(X e 19 X where E is over L\(X e 2 7 and X is a subset of S 0-7803-7280-8/02/$10.00 02002 IEEE 522 


Thus the universe of discourse of C are subsets of S un der consideration 223larger than 11\224 Thus from a dataset of interval valued body of evidence we can determine the large m-itemset A B.2 Compatibility belief and plausibility Let d  Si\(D be a possible fuzzy consequent which means the set of S indicates the presence of d with body of evidence Da In the spirit of previous sections we would lie to define Comp\(d,C u for 0 5 U 5 1 as Si Comp\(d,C u  sup d\(Si 20 S,/C\(S+\224 informally Thus the compatibility of d with C has L\(S d E sets of symptoms as support and Comp\(d L e  sup Dc The last step is to define \223supcEH{&}\224 where H is an arbitrary set If we assume that all the probabilities involved are in dependent then sup  can be defined as a body of evidence as IUce~a  UcE~b>C with associated probb C/C\(C,e S e Tc I hility nccHprC We now consider how to define functions in the mesent setting somewhat similar to plausibility and belief Our first step is to define a mass for any set X in the support of L m\(X  the sum is over all sets C in the support of L Note Ex m\(X  1 Now we need define J\(z#d the extent to which 223 the set of X intersects disease d\224 If d was a fuzzy set the 223largest intersection\224 of the set X with d would be supzEx d\(x In our setting the value of d at Si is the body of evidence S,\(Ds If d was a regular set the plau sibility of d would be defined by Pls\(d  m\(X In our setting we would like to count 223large itemsets in tersecting 8\222 It is therefore natural to define CC L\(C e XndZ0 The term up~,,~+~{S~\(Ds is a body of evi dence with intervals a bk with probability pk the sup of a family of body of evidence has been de fined earlier in this section Thus the intervals akm\(X btm\(X define Pls\(d as a body of k XEC I evidence and associated probability pk in the present setting as Similar considerations lead to the definition of belief V Conclusion and future work In this paper we have defined 223large itemsets\224 in asso ciation analysis for each of the following cases in increas ing complexity The compatibility is a fuzzy subset of OJ and the belief and plausibility are scalars The compatibility is defined on intervals and is in terval valued The belief and plausibility are defined on fuzzy sets and are interval valued The compatibility of two fuzzy sets is a body of evi dence as are the belief and plausibility We also generate functions somewhat similar to com patibility belief and plausibility for each of the above cases These functions are central in studying uncer tainty in data and in making inference when information is vague and/or inaccurate In future we would like to analyze the interplay among belief plausibility and compatibility along the lines sim ilar to what is described in 7 Also we will look at interplay between body of evidence and the above func tions along lines similar to 9 Finally we will look at rules involving fuzzy sets of type I1 as described in 3 and analyze the possible relations of how these rules are fired with the functions introduced in this paper References R Agrawal H Mannila R Srikant H Toivonen and A.Inkeri Fast Discovery of Assmiation Rules In Advances in Knowledge Discovery and Data Mining, AAA1 press 1996 D Barbara P Chen Self-similar Mining of Assmiation Rules over Intemol Data Technical report ISE Dept George Ma son Univenity 2001 A de Korvin C Hu 0 Sirisaengtaksin On Firing Rules of zy Sets of Qpe II Int J of Applied Math Vol 3\(2 pp 151-159 2000 D Dubois H Prade nUw Sets in Appmzineote Reasoning Part I Inference with Possibility Distributions Fuzzy Sets and System 40 pp 143-202 1991 J Han M Kamher Doto Mining Concepts and Techniques Morgan Kaufmann Publishers 2001 K Omer and A de Korvin Utilizing Fuzzy Lqic in Decision Malnng Application of Fuzzy Sets and the Theory of Evidence to Accounting 11 pp 3-14 JAI Press 1998 W Pedrycz and F Gomide An Introduction to Fuzzy Sets Analysis and Design MIT Press 1998 L M Rocka Intend Based Evidence Sets Proc of 1995 NAFIPS/IFIS Joint Conference IEEE Press 1995 G Shafer Mathematical Theory of Evidence Princeton Uni versity Press, Princeton N.J 1976 1101 T Strat Decision Analysis Using Belief Functions Inter national Journal of Approximatic Reawning 4 pp 391-417 1990 0-7803-7280-8/02/$10.00 02,2002 IEEE 523 


Acknowledgments The Pacific Northwest National Laboratory is operated for the U.S. Department of Energy by Battelle Memorial Institute under contract DE-AC06-76RLO 1830 This research has been supported by a Laboratory Directed Research and Development grant funded by the U.S. Department of Energy for the Pacific Northwest National Laboratory. We wish to thank Dan Adams George Chin, Kris Cook, Sharon Eaton, Beth Hetzler Wanda Mar, Dennis McQuerry, Ted Tanasse, and Paul Whitney who provided assistance of many forms throughout this research References 1  Rakesh Agrawal and Ramakrishnan Srikant. Mining Sequential Patterns. In Proceedings of the International Conference on Data Engineering ICDE Taipei, Taiwan, March 1995 2  Barry G. Becker. Volume Rendering for Relational Data. In John Dill and Nahum Gershon, editors Proceedings Information Visualization \22297 pages 87 226 90, Los Alamitos, CA, Oct 20 226 21, 1997 IEEE CS Press 3  Barry G. Becker. Visualizing Decision Table Classifiers. In Graham Wills and John Dills, editors Proceedings of Information Visualization 222 98  pages 102-105, Los Alamitos, CA, Oct 19 226 20 1998. IEEE CS Press 4  A. Bookstein, S.T. Klein, and T. Raita. Clumping Properties or Content-Bearing Words Journal of the American Society for Information Science  49\(2\102 226 114, 1998 5  d Elke A Rundensteiner. Hierarchical Parallel Coordinates for Exploration of Large Datasets. In David Ebert Markus Gross, and Bernd Hamann, editors Proceedings IEEE Visualization 222 99 pages 43 226 50 New York, NY, Oct 24 226 Oct 29, 1999. ACM Press 6  Beth Hetzler, Paul Whitney, Lou Martucci, and Jim Thomas. Multi-faceted Insight through Interoperable Visual Information Analysis Paradigms In Graham Wills and John Dill, editors Proceedings Information Visualization 222 98 pages 137 226 144, Los Alamitos, CA, Oct 19-20, 1998. IEEE CS Press 7  Alfred Inselberg and Bernard Dimsdale. Parallel Coordinates: A Tool for Visualizing MultiDimensional Geometry. In Arie Kaufman, editor Proceedings IEEE Visualization 222 90 pages 361 226 375, Los Alamitos, CA, Oct 1990. IEEE Computer Society Press 8  Nancy E. Miller, Pak Chung Wong, Mary Brewster, and Harlan Foote. TOPIC ISLANDS 231 A Wavelet-Based Text Visualization System. In David Ebert, Hans Hagan, and Holly Rushmeier editors Proceedings IEEE Visualization 222 98  pages 189 226 196, New York, NY, Oct 18 226 23 1998. ACM Press 9  Ramakrishnan Srikant and Rakesh Agrawal. Mining Sequential Patterns: Generalizations and Performance Improvements. In Proceedings the Fifth International Conference on Extending Database Technology \(EDBT Avignon, France, March 1996 10  Jim Thomas, Kris Cook, Vern Crow, Beth Hetzler Richard May, Dennis McQuerry, Renie McVeety Nancy Miller, Grant Nakamura, Lucy Nowell Paul Whitney, and Pak Chung Wong. Human Computer Interaction with Global Information Spaces 226  Proceedings British Computer Society Conference Bradford UK, April 1999 Springer Verlag 11  Christopher Westphal and Teresa Blaxton Data tions 226 Methods and Tools for Solving Real-Word Problems New York, 1998. John Wiley and Sons 12  James A. Wise, James J. Thomas, Kelly Pennock David Lantrip, Marc Pottier, Anne Schur, and Vern Crow. Visualizing the Non-visual: Spatial Analysis and Interaction with Information from Text Documents. In Nahum Gershon and Steve Eick, editors Proceedings IEEE Information Visualization 222 95 pages 51 226 58, Los Alamitos CA, Oct 20 226 21, 1995. IEEE CS Press 13  Pak Chung Wong. Visual Data Mining 226 Guest Editor 222 s Introduction IEEE Computer Graphics and Applications Vol 19, No 5, Los Alamitos CA, 1999. IEEE CS Press 14  Pak Chung Wong, Paul Whitney, and Jim Thomas Visualizing Association Rules for Text Mining. In Graham Wills and Daniel Keim, editors Proceedings of IEEE Information Visualization 222 99  Los Alamitos, CA, 1999. IEEE CS Press e   0-76950 9 00 $10.00 @ 2000 IEEE 


Hei kki Mannila Biographical Summary Heikki Mannila is a professor of computer science at the University of Helsinki where he also obtained his Ph.D in 1985 Since then he has been an associate professor at the Universities of Tampere and Helsinki a visiting professor at the Technical University of Vienna, and a guest researcher at the Max Planck Institut fur Informatik in Saarbrucken He has also worked at the National Public Health Institution in Helsinki as well as being an industry consultant His research interests include data mining machine learning database design and text databases He is the co-author of the book Design of Relational Databases Addison Wesley\and he has been the author of numerous articles on algorithms, databases machine learning and data mining He is one of the editors-in-chief of the new scientific journal \223Data Mining and Knowledge Discovery.\224 9 


Figure 7 Correlations between query con straints and new indexed constraint the index attribute may have different ranges We iden tify all possible constraint introduction solutions as follows Algorithm For each constraint on the index attribute CO.Roj Step 1 find the least expensive association from a query constraint to i.e find the incoming link C~.R  CO.Roj with the fewest exceptions For exam ple if the cardinalities of El     E5 are nl   n5 and n4 5 nz _ n3 then the least expensive association for CO.R is c6.b  CO.R~,\(E  E4 Step 2 filter out all the exceptions that do not satisfy at least one of the query constraints C1.Rl    C12 We do not need to test the exceptions for the antecedent of the corresponding rule since they satisfy it by definition The constraint introduction solutions identified in our example are the following  Introduced Constraints Exceptions ele t El Cz.n,\(e    C12.nl2\(e el  E4 C1.h e    C5.nS e C~.R e     C12.nll e ele  E5 C1.nl e     C5.nS e C~.R e    C12.n12 e CO.Rol CO.RO2 CO.RO3 5 Optimizing OQL queries In the previous sections a series of algorithms were given to find a collection of constraint elimination or con straint introduction solutions In this section we show how the original query is transformed to its optimized form using the optimal solution. Consider the OQL query select x fromExtentX as x where C~.R and   and Cn 5.1 Heuristic H1 Let  Cil   Ci,},Ei  be the maintained con straints and the exceptions of a solution i Only the main tained constraints of the optimization solution should be tested on the objects of the whole extent however all the constraints should be tested on the exception cases and the objects that satisfy the maintained constraints but not the ones omitted should be removed from the result Hence the original query should be converted to the following one select x from Extent2 as x where Cil and   and Ci except Ei If we assume that tests on the query constraints take roughly the same time, the optimal solution is the one with fewest exceptions Ei 5.2 Heuristic H2 Let  Co.Roi Ei  be the index constraint and the corre sponding exceptions of a constraint introduction solution Instead of testing the query constraints C1.~l    C,.R on the entire extent Extent X we apply them only on the re sults of the subquery select x from Extent2 as x 40 where CO.R Since Co.Roi is a constraint on a cluster index attribute, the select operation is expected to be quite fast The original query is transformed to its more efficient form select x from 90 as x where Cl and    and C union Ei The exceptions Ei are merged to the result because they satisfy the query constraints but not the new index con straint Co.Roi Since the union operation is relatively cheap the optimal solution is the one that introduces the index con straint with the highest selectivity 6 Discussion We now look at two different scenarios and estimate the extent to which heuristics H1 and H2 speed up query exe cution The Jirst scenario concerns frequently executed queries Assume that the association rules which are used by algo rithm 3 are not modified We may optimize a query once at compilation time then execute its optimized form It is worth optimizing provided that the execution time of the optimized query is less than the execution time of the orig inal query For heuristic H1 this happens only if the time 132 


saved by omitting some constraints is greater than the time needed to remove the exceptions from the result except operation The more the eliminated constraints and the fewer the exceptions the better the optimization As one of the referees pointed out, the time saved by the elimina tion of constraints is CPU-related Since query execution is dominated by data access time this optimization is not expected to alter performance significantly It would help only in contexts rich in associations with few exceptions in which users express many constraints in their queries Heuristic H2 is expected to bring more significant ben efits Firstly, this optimization involves a union operation which is much cheaper than the except operation used in H1 Secondly instead of retrieving all the objects of an extent from the database we need only look at the subset retrieved through an indexed constraint Hence we save a considerable amount of data access time, spending a negli gible amount of CPU time in evaluating the additional con straint The second scenario concerns queries which are exe cuted only once In this case the time required for opti mization is significant. This time depends on the algorithm that finds associations between relaxed constraints and tight constraints \(see section 3 since this is the most expensive step in the optimization process This algorithm finds paths in a directed graph, and combines the exceptions associated with each edge of the path to derive the total exceptions for the path. Therefore its complexity is a function of i the av erage number of exceptions in the existing association rules and ii the number of different constraints found in the an tecedents and the consequents of the rules We have already implemented the algorithms for apply ing H1 the next step is to implement the corresponding al gorithms for H2 This should not be difficult since the main algorithm  finding associations between rule constraints  is common to the two heuristics We intend to set up an experimental model in order to evaluate H1 and H2 in the scenarios discussed above 7 Conclusion The use of association rules for query optimization is rel evant to both relational and object-oriented database sys tems There has been a lot of research on generating asso ciation rules and maintaining them in the presence of up dates Research has also focused on finding heuristics that take advantage of rules in order to optimize a query Most of this work 2 31 has considered integrity rules rather than association rules with exceptions Semantic optimiza tion heuristics were also applied without considering indi rect associations In this paper we implement algorithms that apply two optimization heuristics presented by Siegel et al taking account of both exceptions and indirect asso ciations We show how to use these heuristics to optimize an OQL query The complexity of the optimization process is closely related to the complexity of the constraint graph which represents the set of association rules in the data It also depends on the number of exceptions associated with each rule We have designed an experimental framework to evaluate the two optimization techniques both in the con text of queries repeated frequently over a period of time, and in the context of ad-hoc queries executed once only The re sults of this experimental work will be presented in a later paper 8 Acknowledgements We are grateful to the anonymous referees who read the paper carefully and critically and made many helpful suggestions Agathoniki Trigoni is supported by a scholar ship from the Greek Scholarships Foundation and is deeply obliged to the National Bank of Greece References I R Agrawal T Imielinski and A Swami Mining asso ciation rules between sets of items in large databases In Proceedings of the 1993 ACM SIGMOD Intl Conference on Management oj data pages 207-2 16 1993 2 U Chakravarthy J Grant and J Minker Logic-based ap proach to semantic query optimization ACM Transactions on Database Systems 15\(2 162-207 1990 3 J Grant, J.Gryz J Minker and L Raschid. Semantic query optimization for object databases In ICDE pages 444-453 1997 4 R Miller and Y Yang Association rules over interval data In ACM SIGMOD 1997 5 J Park An effective hash-based algorithm for mining asso ciation rules In ACM SIGMOD pages 175-186 1995 6 M Siegel E Sciore and S Salveter A method for auto matic rule derivation to support semantic query optimiza tion ACM Transactions on Database Systems 17\(4 600 December 1992 7 R Srikant and R Agrawal Mining quantitative association rules in large relational tables In ACM SIGMOD Intl Con ference on Management ojdata pages 1-12 1996 8 D Tsur J Ullman S Abiteboul C Clifton R Motwani S Nestorov and A Rosenthal Query flocks a general ization of association-rule mining In ACM SIGMOD Intl Conference on Management of data pages 1-12 1998 Intelligent query answering in deductive and object-oriented databases In Fourth ACM Intl Conference on Information and Knowledge Management pages 244 251 1994 IO S Yoon 1 Song and E Park Semantic query processing in object-oriented databases using deductive approach In Intl Conference on information and knowledge management pages 150-157 1995 9 S Yoon 133 


