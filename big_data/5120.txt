2009 2 nd Conference on Data Mining and Optimization 27-28 October 2009, Selangor, Malaysia   978-1-4244-4944-6/09/$25.00 ©2009 IEEE   84   Capturing Uncertainty in Associative Classification Model  Yun-Huoy Choo 1 Azuraliza Abu Bakar 2 Azah Kamilah Muda 1  1 Faculty of Information and Communication Technology  Universiti Teknikal Malaysia Melaka \(UTeM 2 Faculty of Information Science and Technology,  Uni versiti Kebangsaan Malaysia 43600 Bangi, Selangor, Malaysia   Abstract This paper aims to propose a weighted linguistic associative classification model for uncertainty da ta analysis using rough membership function Transformation of quantitative association rules into linguistic repr esentation can be achieved in discretizing the numerical interval into rough interval described with respective rough membership  values Transformation of linguistic information system is s uggested prior to the frequent pattern discovery Neither pr uning of association rules nor classifier modelling is neede d The rough membership values of the each linguistic frequent i tem are composited to form the weighted associative classif ication rule Simulated results on Iris Plant dataset were shown in the paper The future work of the research will focus on implem enting the model with more experimental dataset  Keywords— associative classification, rough set theor y, rough membership function, uncertainty  I  INTRODUCTION Human tend to employ natural language when describi ng a certain situation when communicating with others, w hile the same situation is not easily explainable when using  numerical values. Quantitative rules that convey in numbers are sometimes less practical in conversation Thus  describing the interval data in natural language ha s lead to the emergence of the usage of linguistic terms in a ssociation rule which are later known as linguistic associatio n rule Linguistic association rules mining is derived from  the original concept of association rules mining 1   T he term  Linguistic association rule was coined in order to replace the numerical values with linguistic terms in quant itative association rules mining With the introduction of associative classification task in 2   l i n g u i s t i c  association rule mining can be further explored and transformed  into associative classification rule mining Besides fuz zy set theory, rough sets suggested by Zdzislaw Pawlak is another technique in soft computing that is able to deal wi th ambiguous boundary data sets 3   R o u g h  s e t s  t h e o r y  uses the upper and lower approximation as well as the ro ugh membership function to determine the items in a set  which can be obtained through self discovery in learning data. They are similar but different from the uncertainty hand ling proposed by fuzzy theory.  Hence, it has gained the interest of researchers in the development of rough sets tec hniques as a complement to fuzzy techniques Currently most researchers focus only on the conce pt of approximation. Hence, this paper attempts to invest igate the opportunity and strength of rough sets theory in li nguistic association rules mining from the perspective of ro ugh membership function and embed the proposed model in to associative classification task The expected outpu t of the model is the weighted associative classification mo del for uncertainty data analysis The organization of the paper starts with the brief explanation on the theoretica l concepts of associative classification in section II-A and s ection II-B capturing uncertainty in linguistic association rul e in section II-C the related rough set concepts in section III  the proposed associative classification model and the d etailed discussion on every phases in section IV before dis cussion on the simulation results in section V and lastly t he conclusion and future work of this research in sect ion VI  II  A SSOCIATIVE C LASSIFICATION  The task of associative classification \(AC\ was bor n from combination of two frequently used data mining task s i.e the association rules discovery and the classificat ion task. Its main purpose is to model the classifier for predict ion akin to the goal of classification 4   A C  h a s  b e e n  i n i t i a t ed to complement whatever inadequacies in both the associ ation rules mining and classification tasks 5-8   T h e  t a sk of AC starts from the frequent itemsets discovery, associ ation rules generation pruning and rules selection to the regu lar classification with AC rules A  Association Rules Mining Association rule mining was first proposed by Agraw al Imielinski and Swami 9   I t  i s  a  t e c h n i q u e  t o  e x t ract patterns from huge data sets to obtain useful infor mation generally from the transaction data Earliest examp le of association rule mining is presented by the market basket analysis which forms the foundation of today’s ass ociation rule mining Later the emergence of the categorica l and quantitative association rules mining has widened t he 


 978-1-4244-4944-6/09/$25.00 ©2009 IEEE   85  discussion of association rules mining The convent ional quantitative association rules mining with A priori algorithm proposed by Agrawal and Srikant 10   I t  h a s  b e c o m e  the foundation of many new techniques in the later work  of association rules mining. The A priori algorithm consists of two major tasks i.e the frequent pattern discover y to scan for frequent occurring itemsets that satisfied the predefined support and confidence thresholds and the associati on rules generation B  Classification Classification is one of the most important tasks i n data mining The classification task models a classifier  from the data records with the intention of categorizing ob jects into different decision classes as precisely as possible  according to the values of each attributes in the dataset. Cl assification is commonly carried out in two phases, i.e. the tra ining and the testing. The training phase aims to generate cl assification patterns where each of the rules links a decision c lass while the testing phase is to verify the generated classi fication rules on the unseen data record. The higher the acc uracy of generated classification rules set on the test data  the better the classifier is C  Capturing Uncertainty with Linguistic Association Mining Linguistic association mining describes data attrib utes in a human understandable language. It conveys the mes sage in linguistic terms instead of numerical values in qua ntitative attributes Linguistic association mining task is g enerally being divided into two phases. The first phase is t o discretize the continuous numeric data and represent those int ervals with a suitable linguistic term [11  b e f o r e  p r o c e e d ing to the second phase of frequent pattern mining Partitioni ng the numeric values into interval sets will cause crisp boundary problems This may create some contradiction and overlapping when these intervals are being presente d in linguistics term Hence there is a need for a soft  boundary instead of crisp boundary. Since the partitions are divided by linguistic terms, it is not easy to differentiate t he boundary of the set 12   T h e r e  a r e  s e v e r a l  t e c h n i q u e s  i n  m i n i n g quantitative and categorical attributes in associat ion rules Some of the attributes may not be suitable to be de scribed by intervals. To make them more recognizable to humans it is advisable to use the natural language terms Howeve r this introduces some other problems The linguistic term s used may not be sufficient to embody the actual distribu tion of data 12   P r o b l e m s  h a p p e n  d u e  t o  t h e  d i f f i c u l t y  o f  distinguishing the exact boundary of a particular i nterval set Thus, concept of uncertainty is used to soften the boundary of interval sets in data analysis [13   Data discretization has arisen since the earlier wo rk on quantitative association rule mining while the lin guistic terms representation is always being related to Fuz zy sets theory. Soft partitioning technique and some soft c omputing related techniques were introduced to create vague interval sets to handle this problem Although typical analy sis such as binning 14 15   s t a t i s t i c a l   1 6   a n d  e v o l u t i o n ary techniques 17  h a v e  c o n t r i b u t e d  t o  a  c e r t a i n  e x t e n t in dealing with quantitative attribute partitioning, i t seems that soft computing methods especially involving fuzzy t heory or its hybrids techniques with A priori  algorithm and genetic algorithm are more promising techniques in managing  soft partition problems. Fuzzy algorithm has been improv ed from time to time to more efficiently and accurately min e the linguistic terms used in association rules mining Since then fuzzy sets theory has become the more prominent tec hnique in dealing with linguistic representation on soft b oundary interval On the other hand, rough sets theory introduced by Pawlak also deals with vagueness and uncertainty With emb edded foundation of classical theory, rough sets theory h as its own perspective towards vagueness which is similar to f uzzy set theory 18   A c c o r d i n g  t o  P a w l a k   i t  i s  a  u n i q u e  implementation of Frege’s idea of vagueness The ro ugh membership function is a special case of fuzzy memb ership function 19   H e n c e   i t  i s  e x p e c t e d  t h a t  r o u g h  s e t s theory can be used to describe linguistic knowledge as an alternative to fuzzy set theory in a different view  of vagueness and uncertainty III  R OUGH S ET T HEORY  Rough sets theory was introduced by Pawlak 5  t o  d eal with vagueness and uncertainty According to 20  a n information system can be presented as a pair   A U S   or a function V A U f     where U  is the nonempty universe object set A  an nonempty attribute set and V a  value set of attribute a such that V a U a    for every A a  A decision system is any information system of the form       d A U S  where A d   is the decision attribute. The elements of A are called conditional attributes The universe U  can be partitioned into disjoint subsets known as equivalence classes which are discernible among one another All the elements in one particular sub set are considered equal or indiscernible based on the sele cted attribute subset. This gives the definition of indi scernibility Let A U A  be an information system and each subset of attributes A B   defines an equivalence relation called an indiscernibility relation as in \(1        2      j x a i x a B a U j x i x B IND      A   1  The core of rough sets approach is the idea of indiscernibility between objects. Thus, the equalit y operator is needed to define every attribute domain This ma kes the method suitable to apply on any problem with discre te or categorical values.  Therefore, continuous or real values have to be combined or discretised in the pre-processing  step in order to take on a suitable level of granularity. D iscretisation 


 978-1-4244-4944-6/09/$25.00 ©2009 IEEE   86  on quantitative values will reduce and optimize the  determining input before the process of rules gener ation  Rough membership function is another uncertainty representation in rough sets theory Let A B   a set of attributes, the rough membership function  1  0    U B X m is defined as in \(2                  x B X x B x B X   m         2 Equation 2 is an unbiased estimation of condition al probability    Pr B x X x   the probability of object x  belongs to set X in terms of B Similar to fuzzy membership value, the operation of union and intersection of t wo RMVs 19  i s  d e f i n e d  i n   3   a n d   4   a s  f o l l o w s   3 4 Reference 21  s h o w s  t h e  f u r t h e r  d e t a i l s  i n  t h e  n o t ation of rough membership function Rough membership derived  from accuracy of the rules will be used as the roug h accuracy value to generate membership reference in transform ing numerical values into linguistic variable Instead of getting the membership value from the expert, our method is able to auto generate membership references through learnin g from data Similar to fuzzy membership values the gener ated rough membership reference will be applied in trans forming the instances value into rough approximation region  in linguistic rules mining  According to 20   d e f a u l t  r u l e s  g e n e r a t i o n  f r a m e w ork proposed by Mollestad to discover more general rule s to capture useful knowledge in incomplete dataset The  rules generated have two advantages compared to the deterministic rules First they are usually simple r in structure because they cover larger classes defined  over a few condition attributes Secondly they are in man y cases proven to be better when handling unseen cases. The default rules are generated through creating the indetermin acy in a decision system The generation of indeterminacy is  done through selection of projections over the condition attributes allowing certain attributes to be excluded from consideration The removal of the information in te rms of condition attributes from a given decision system a llows the generation of rules from the resulting decision tab les The underlying idea is to search for default rules by r emoving reducts from the original system The set of full r educts computed from the decision system determines the se t of attributes to be retained in the projection Refer to 20  f o r  further information on default generation framework  IV  T HE P ROPOSED M ODEL  The proposed associative classification model emplo ys the similar architecture to common AC technique. A new phase of generating rough membership value is embedded pr ior to the two major steps of original associative classif ication model to capture and define uncertainty in dicretiz ed attribute Some modifications have been made to the  AC phases to facilitate rough associative classificati on rules generation Fig 1 shows the proposed associative classification model The proposed rough associativ e classifier model comprises of three major phases se rving different functionalities as below A  Generating Rough Membership Value \(RMV This phase serves the purpose of inducing rough intervals with the corresponding rough membership v alues for linguistic representation and transforms the li nguistic decision system It is an automated membership gene ration process similar to the automated fuzzy membership f unction in fuzzy system Once the rough intervals had been determined from the training data they can be used  for association rules mining process in any relevant te sting data Basically, there were two major steps involved in t he rough variables generation i.e the discretisation and t he rough intervals induction The Boolean reasoning suggested as the discretisati on method in rough classification is implemented as a necessary step in rough interval generation 22   R o u g h  i n t e r vals induction was the second step in generating rough v ariables The purpose of rough interval generation is to obta in the rough membership values RMV corresponding to each  discretised interval 23   R M V  c a n  b e  d e r i v e d  f r o m  two sources either the right hand side accuracy of the  rough classification rules 23  o r  t h e  p r o b a b i l i t y  o f  o c c urrence of the respective intervals in the data 21   I n  t h e  p roposed model, default rules generation framework is used t o obtain the rough classification rules and the accuracy val ues. RMV indicates the probability of an object being classi fied in the set of a decision class 19 21 24 25   I t  i s  u s e d as the numerical weightage in AC RMV for all output class  of every rough interval is determined according to the formula defined in \(5\ as follows  repeat for every 0 < i  x and 0 < j  y 5  Let x denotes the number of rough interval and i is the i th rough interval. Card i is the total instance in the i th rough interval Card  O j  is the total instance from i th rough interval that concludes to decision class O j while j is the j th decision class and y is the total decision class  


 978-1-4244-4944-6/09/$25.00 ©2009 IEEE   87   Fig. 1: The proposed Rough AC model  B  Association Mining The association mining is the second phase in the proposed model Modification on the conventional association rule mining has been made in the propos ed model Instead of generating association rules from  the frequent itemset the proposed model starts with transforming the decision system into linguistic in formation system \(IS\. The next step is to scan for frequent occurring linguistic itemsets that satisfy the user-defined s upport thresholds The linguistic IS transformation serves a two-fold purpose i.e replaces the decision classes in the original decision system with linguistic terms and transform the deci sion table into a linguistic IS table. This is an important st ep before the frequent pattern mining in order to generate lingui stic frequent pattern instead of quantitative frequent p attern. The linguistic terms carry different RMV weightage towa rds the discretised RMR intervals The decision classes wil l be replaced by user-defined linguistic terms Replacin g the decision classes with predefined linguistic terms w ill  change the decision classes into different rough linguisti c sets [19   User-defined linguistic terms were used in the expe riments Therefore this process requires the intervention of  expert knowledge Rough membership function helps in captu ring the uncertainty boundary of the rough linguistic se t, but user understanding of the dataset used is still necessar y in order to determine suitable linguistic terms to be replaced Imprecise terms chosen may result in deviation of the meaning  of the rules However this would not affect the process o f rules generation and the performance of the association r ules Choosing suitable linguistic terms with expert know ledge is not the focus of this paper. The second part of the linguistic decision system transformation is to convert the or iginal decision system into a linguistic decision system Similar to fuzzy decision system proposed in [26   t h e  a t t r i b u tes in the linguistic decision system are the linguistic terms  chosen previously while the attributes values are the RMV attained from respective rough intervals. All the instances that fall in the same rough interval will be assigned the same R MV Iteration of the algorithm finds different large it emsets frequent combination of items that appears togethe r The frequent pattern discovery algorithm 9  i s  r e f e r r e d in this phase It starts with first scan pass the dataset t o determine the occurrences to produce the large 1-itemsets Th is is denoted as L 1  L 1 is then used to find L 2 which is the large 2itemsets in the 2-pass scanning The process is car ried on until no more frequent k itemsets can be found Since the associative mining process stops at frequent patter n discovery only the measurement of minimum support is needed in this phase The higher the minimum suppor t thresholds the lesser the frequent pattern  discove red in the experiments but it can increase the accuracy since only strong frequent itemsets passed the threshold const raints [27 28    C  Weighted Linguistic Associative Classification Rule s Generation This is the last major phase in the proposed AC mod el Some modification has been made in this phase compa red to the conventional concept of AC model This phase ai ms to generate weighted associative classification rules from the linguistic frequent pattern in the previous phase Neither pruning of association rules nor classifier modelli ng is needed. The RMV values of the each linguistic frequ ent item are composited to form the weighted associative classification rule. Refer to section II V  S IMULATION R ESULTS  The proposed model was simulated and discussed here  The simulation results shown in this section is bas ed on the Iris Plant \(IP\ dataset from UCI repositories [29  The rough interval of IP dataset and their respective RMV is shown in Fig 2 Three decision classes were replaced with l inguistic terms describing the size of the Iris plant i.e s hort Irissetosa\, medium \(Iris-versicolor\ and long \(Iris-vi rginica   Fig. 2: Example of rough intervals with respective RMV  The original decision system was transformed into linguistic information system Fig 3 is an example  of linguistic data table for IP dataset. The RMV was u sed as the linguistic attributes value in the new IS. No decis ion class is Generating RMV Association Mining Discretisation Induce Rough Intervals Linguistic IS Transformation  Frequent Pattern Discovery  Weighted Linguistic Associative Classification Rules Generation  


 978-1-4244-4944-6/09/$25.00 ©2009 IEEE   88  needed as each linguistic attribute itself embodies  a rough decision class set The associative relation betwee n these rough decision class set need to be identified   Fig. 3:  Example of linguistic data table The frequent pattern mining was done on the new linguistic information system to get the large item set An example of the associative frequent pattern generat ed is shown in table I Each of the generated items conta ins multiple relations between the rough decision class es All the relations were composited into one AC rule  TABLE I  A N E XAMPLE OF L INGUISTIC F REQUENT P ATTERN FOR IP  D ATASET  No  Linguistic Frequent Pattern 1 PL.Medium^PW.Medium^PW.Short  For example, the linguistic frequent L 3 itemset consists of three rough decision classes The rough intervals a nd their respective RMVs generated in the first phase of the proposed model were referred. The composition of RMVs is sho wn as follows               Thus, weighted linguistic AC rules generated from linguistic frequent L 3 itemset is as shown in table II TABLE II  T HE W EIGHTED L INGUISTIC A SSOCITIVE C LASSIFICATION R ULES  No  Linguistic AC Rules Rule Weightage  1 IF PL.Medium^PW. Medium^PW.Short   => Iris-setosa  0.51 2 IF PL.Medium^PW. Medium^PW.Short  => Irisversicolor 1 3 IF PL.Medium^PW. Medium^PW.Short => Iris-virginic a  0  VI  C ONCLUSION AND F UTURE W ORK  This paper has proposed an associative classificati on model for uncertainty data analysis using rough mem bership function Rough membership value is suggested as th e weightage for capturing uncertainty in linguistic representation of discretized numerical intervals With the new phase of generating rough membership values embedded prior to the two major steps of original a ssociative classification model the proposed model is able to  handle continuous real-value attributes to generate weight ed linguistic AC rules instead of crisp quantitative A C rules The linguistic terms carry different RMV weightage  The weighted attribute sets will then go through the pr oposed task of associative classification including lingui stic IS transformation frequent pattern discovery weighte d associative classification rule generation The gen erated weighted linguistic associative classification rule s set can be used to perform classification operation on respect ive unseen data records. The future work of this research is t o verify the proposed model with real dataset The results will be reported in near future   A CKNOWLEDGMENT  The work described in this paper was partially supp orted by the Faculty of Information and Communication Technology, Universiti Teknikal Malaysia Melaka \(UT eM grant no PJP/2008/FTMK\(15\-S494 We also thank the  anonymous reviewers for their helpful comments R EFERENCES  1  R   S r i k a n t  a n d  R   A g r a w a l    M i n i n g  Q u a n t i t a t i v e  Association Rules in Large Relational Tables in Proc  1996  ACM SIGMOD International Conference on Management of Data pp. 1-12 2  B   L i u   W   H s u   a n d  Y   M a    I n t e g r a t i n g  c l a s s i f ication and association rule mining," in Proc  1998 KDD-98 pp 3  Z   P a w l a k    R o u g h  S e t s  a n d  D a t a  A n a l y s i s    i n  Proc  1996 Asian Fuzzy Systems Symposium Soft Computing in Intelli gent Systems and Information Processing pp. 1-6 4  F   T h a b t a h    A  R e v i e w  o f  A s s o c i a t i v e  C l a s s i f i c a tion Mining The Knowledge Engineering Review vol. 22:1, pp. 37-65, 2007 5  A   V e l o s o  a n d  W   M e i r a  J r    R u l e  g e n e r a t i o n  a n d  rule selection techniques for cost-sensitive associative classific ation in Proc  2005 pp. 295–309 Instances Linguistic terms RMV 


 978-1-4244-4944-6/09/$25.00 ©2009 IEEE   89  6  W   L i   J   H a n   a n d  J   P e i    C M A R   A c c u r a t e  a n d  Efficient Classification Based on Multiple Class-Association Rules," 2001 7  D   M a l l e t t    A  P a p e r  R e v i e w  o n  I n t e g r a t i n g  C l a s sification and Association Rule Mining," 2003 8  D   J a n s s e n s   G   W e t s   T   B r i j s   a n d  K   V a n h o o f   Integrating Classification and Association Rules by proposing a daptations to the CBA Algorithm in Proc  2003 Proceedings of the 10th International Conference on Recent Advances in Reta iling and Services Science pp 9  R   A g r a w a l   T   I m i e l i n s k i   a n d  A   S w a m i    M i n i n g Association Rules Between Sets of Items in Large Databases in Proc  1993 ACM SIGMOD Conference pp. 1-10 10  Q   A   A l R a d a i d e h   M   N   S u l a i m a n   M   H   S e l a m at, and H. Ibrahim Approximate reduct computation by rough sets based  attribute weighting in Proc  2005 IEEE International Conference on Granular Computing pp. 383 - 386 11  H   I s h i b u c h i   T   N a k a s h i m a   a n d  T   Y a m a m o t o    Fuzzy Association Rules for Handling Continuous Attributes in Proc  ISIE 2001 International Symposium on Industrial Electronics pp. 118-121 12  J   L u   B   X u   L   X u   D   K a n g   H   C h e n   a n d  H   Yang Mining Association Rules with Linguistic Terms in Proc  2003 The 15th IEEE International Conference on Tools with Artific ial Intelligence ICTAI’03 pp. 129-133 13  H   I s h i b u c h i   T   Y a m a m o t o   a n d  T   N a k a s h i m a    Fuzzy Data Mining Effect of Fuzzy Discretization in Proc  2001 IEEE 2001 International Conference on Data Mining pp. 241-248 14  R   J   M i l l e r  a n d  Y   Y a n g    A s s o c i a t i o n  R u l e s  O ver Interval Data," in Proc  1997 The 1997 ACM SIGMOD international conference o n Management of data pp. 452-461 15  B   P  s s a s   W   M   J r    M   C a r v a l h o   a n d  R   R e s e nde Using Quantitative Information for Efficient Association Rules Generation in ACM SIGMOD Record vol. 29, 2000, pp. 19-25 16  Y   A u m a n n  a n d  Y   L i n d e l l    A  S t a t i s t i c a l  T h e o r y for Quantitative Association Rules Journal of Intelligent Information Systems  vol 20, pp. 255-283, 2003 17  J   M a t r a   J   L   A l v a r e z   a n d  J   C   R i q u e l m e    An Evolutionary Algorithm to Discover Numeric Association Rules i n Proc  2002 The 2002 ACM symposium on Applied computing pp. 590-594 18  Z   P a w l a k    S o m e  I s s u e s  o n  R o u g h  S e t s    i n  Transactions on Rough Sets vol. LNCS 3100 Lecture Notes in Computer Sciences J. F. P. e al., Ed.: Springer-Verlag Berlin Heidelberg, 2004 pp. 1-58 19  Y   Y a o    A  C o m p a r a t i v e  S t u d y  o f  F u z z y  S e t s  a n d  Rough Sets Journal of Information Sciences vol. 109, pp. 227-242, 1998 20  A   A b u  B a k a r    P r o p o s i t i o n a l  S a t i s f i a b i l i t y  M e thod In Rough Classification Modeling For Data Mining," vol. PhD Universiti Putra Malaysia, 2002 21  L   P o l k o w s k i    R o u g h  S e t  T h e o r y   A n  I n t r o d u c t i on in Rough Sets Mathematical Foundations  Advances in Soft Computing  PhysicaVerlag, Heidelberg, 2002, pp. 5-6 22  Z   P a w l a k  a n d  A   S k o w r o n    R o u g h  s e t s  a n d  B o o l ean reasoning International Journal of Information Sciences  vol 177 pp 41-73 2007 23  A   O h r n    T e c h n i c a l  r e f e r e n c e  m a n u a l    v o l   P h D Trondheim Norway Norwegian University of Science and Technol ogy 2001 pp. 16-18 24  Z   P a w l a k  a n d  A   S k o w r o n    R u d i m e n t s  o f  R o u g h  Sets Journal of Information Sciences vol. 177, pp. 3-27, 2007 25  Y   Y a o    T w o  V i e w s  o f  t h e  T h e o r y  o f  R o u g h  S e t s  in Finite Universes International Journal of Approximate Reasoning vol. 15 pp. 291-317, 1996 26  T  P   H o n g   K  Y   L i n   a n d  S  L   W a n g    F u z z y  Data Mining for Interesting Generalized Association Rules Fuzzy Sets and Systems  vol. 138, pp. 255-269, 2003 27  T  P   H o n g   C  S   K u o   a n d  S  L   W a n g    A  f u z z y AprioriTid mining algorithm with reduced computational time Applied Soft Computing vol. 5, pp. 1-10, 2004 28  R   S   G y  rödi A Comparative Study of Iterative Algorithms in Association Rules Mining Studies in Informatics and Control vol 12, pp. 205-213, 2003 29  A   A s u n c i o n  a n d  D   J   N e w m a n    U C I  M a c h i n e  L e a rning Repository University of California School of In formation and Computer Science, 2007    


numerical attribute e  g the attribute Age which is generally in the range 0-100, can be divided into six sharp partitions namely 0-15, 16-30, 31-45, 46-60, 61-75, 76 and above. In case of FACISME, fuzzy sets are used instead of sharp partitions, in order to convert numerical attributes to fuzzy partitions. The attribute Age can be divided into three fuzzy partitions, namely Young, Medium-aged, and Old, with each value of age belonging to each of the three fuzzy partitions with some membership value 265 Thus, the number of sharp partitions that need to be used to handle a numerical attribute is far greater than the number of fuzzy partitions required to do the same. Moreover, this leads to better understanding by the user as there are few well-defined fuzzy partitions with linguistic names/meanings like Young and Old, as opposed to sharp partitions which are less intuitive and to which a user cannot relate to immediately For the current analysis we have used two fuzzy partitions for all numerical attributes throughout all the datasets. As described in section 5.C, the time complexity of FACISME is quite high, because of which the time required to train a fuzzy classifier using FACISME is also high. Hence, we have used two fuzzy partitions so as to limit the training time. But, even with just two fuzzy partitions we have achieved quite high accuracies, as described above. More importantly, even with this, we achieved similar \(sometimes better results\s other state-of-the art classifiers used in this experimental analysis. The reason for these high accuracies is that FACISME is based on the theoretically sound and well-established maximum entropy framework  VII. CONCLUSIONS In this paper we have proposed a new classifier based on the paradigms of association rules mining and fuzzy logic  Though recent classifiers i nvolving association rules have shown their techniques to be accurate, their approaches to build a classifier either manifest unobserved statistical relationships in the dataset, or have no statistical basis at all FACISME uses the well-known maximum entropy principle and iterative scaling to build a statistical and robust theoretical model for classification Fuzzy Logic gives FACISME the capability to deal accurately and efficiently  with any type of datasets and domains, and in any kind of environments, which is not necessarily true for traditional crisp classifiers. Thus, using maximum entropy and fuzzy logic, FACISME provides very  good  accuracy and can work with all types of data irrespective of size and type of attributes \226 numerical or binary  REFERENCES 1  Agrawal, R. and Srikant, R.: Fast algorithms for mining association rules. In Proceedings of the 20 th International Conference on Very Large Data Bases, pp. 487-499. Morgan Kaufmann, Santiago, Chile 1994 2  Clark, P. and Niblett, T.: Bayesian network classifiers. Machine Learning, 29, 131-163 \(1997 3  Meretakis, D. and Wuthrich, B.: Extending naive-bayes classifiers using long itemsets. In Proceedings of International Conference on Knowledge Discovery and Data Mining, pp. 165-174. ACM, New York, NY \(1999 4  Zadeh, L.A.: Fuzzy sets. Inf. Control, 8, 338\226358 \(1965 5  Chen G., Yan P., Kerre E.E.: Computationally Efficient Mining for Fuzzy Implication-Based Association Rules in Quantitative Databases. International Journal of General Systems, 33, 163-182 2004 6  H\374llermeier, E.: Fuzzy methods in machine learning and data mining Status and prospects. Fuzzy Sets and Systems. 156, 387-406 \(2005 7  Mangalampalli, A., Pudi, V.: Fuzzy Logic-based Pre-processing for Fuzzy Association Rule Mining. Technical Report IIIT/TR/2008/127 International Institute of Information Technology \(2008 8  Mangalampalli, A., Pudi, V.: Fuzzy Association Rule Mining Algorithm for Fast and Efficient Performance on Very Large Datasets In Proceedings of IEEE International Conference on Fuzzy Systems pp. 1163-1168. IEEE Computational Intelligence Society, Piscataway NJ \(2009 9  Good, I.: Maximum entropy for hypothesis formulation, especially for multidimensional contingency tables. Annals of Mathematical Statistics, 34, 911-934 \(1963   Verlinde, H., De Cock, M., Boute, R.: Fuzzy Versus Quantitative Association Rules: A Fair Data-Driven Comparison. IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics 36, 679-683 \(2006   Dunn, J.C.: A Fuzzy Relative of the ISODATA Process and its Use in Detecting Compact, Well Separated Clusters. J. Cyber., 3, 32-57 1974   De Cock, M., Cornelis, C., and Kerre, E.E.: Elicitation of fuzzy association rules from positive and negative examples. Fuzzy Sets and Systems, 149, 73\22685 \(2005   Dubois, D., H\374llermeier, E., Prade, H.: A systematic approach to the assessment of fuzzy association rules. Data Min. Knowl. Discov., 13 167-192 \(2006   Darroch, J.N. and Ratcliff, D.: Generalized iterative scaling for loglinear models. The Annals of Mathematical Statistics, 43, 1470\2261480 1972   Zaki, M.J.: Generating non-redundant association rules. In Proceedings of the 6 th International Conference on Knowledge Discovery and Data Mining, pp. 34-43. ACM, New York, NY \(2000   Thonangi, R. and Pudi, V.: ACME: An Associative Classifier Based on Maximum Entropy Principle. In Proceedings of the 16 th  International Conference Algorithmic Learning Theory, pp. 122-134 Springer, Singapore \(2005   Liu, B., Hsu, W., and Ma, Y:. Integrating classification and association rule mining. In Proceedings of the 4 th International Conference on Knowledge Discovery and Data Mining, pp. 80-86 AAAI Press, New York, NY \(1998   Li, W., Han, J., and Pei, J.: CMAR: Accurate and efficient classification based on multiple class-association rules. In Proceedings of IEEE International Conference on Data Mining, pp. 369-376. IEEE Computer Society, San Jose, CA \(2001   Yin, X. and Han, J.: CPAR: Classification based on Predictive Association Rules.  In Proceedings of the 3 rd SIAM International Conference on Data Mining, pp. 331-335. SIAM, San Francisco, CA 2003   Beeferman, D., Berger, A., and Lafferty, J.: Statistical models for text segmentation. Machine Learning. 34, \(1-3\, 177-210 \(1999   Thabtah, F.A.: A review of associative classification mining Knowledge Engineering Review, 22, 37-65 \(2007   Quinlan, J.R.: C4.5: Programs for Machine Learning. Morgan Kaufmann, San Francisco \(1993   Cohen, W.: Fast Effective rule induction. In Proceedings of the 12 th  International Conference on Machine Learning, pp. 115-123. Morgan Kaufmann, Tahoe City, CA \(1995   H\374hn, J. C., and H\374llermeier, E.: FR3: A Fuzzy Rule Learner for Inducing Reliable Classifiers. IEEE Transactions on Fuzzy Systems 17, 138-149 \(2009   H\374hn, J. C., and H\374llermeier, E  FURIA: an algorithm for unordered fuzzy rule induction. Data Mining Knowledge Discovery, 19, 293-319 2009   Gonzalez, A. and Perez, R.: Slave: A genetic learning system based on an iterative approach. IEEE Transactions on Fuzzy Systems, 7, 176191 \(1999   Gonzalez, A. and Perez, R.: Selection of relevant features in a fuzzy genetic learning algorithm. IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics, 31, 417\226425 \(2001  


  Fig. 4. Experimental results on iris dataset     Fig. 5. Experimental results on breast \(breast cancer\ataset     Fig. 6. Experimental results on pima dataset  93 93.5 94 94.5 95 95.5 FACISME CPAR CMAR CBA C4.5 Ripper FURIA FR3 SLAVE 94 94.5 95 95.5 96 96.5 FACISME CPAR CMAR CBA C4.5 Ripper 71 72 73 74 75 76 FACISME CPAR CMAR CBA C4.5 Ripper FURIA SLAVE 


Application of Chaotic Particle Swarm Optimization Algorithm in Chinese Documents Classification 763 Dekun Tan Qualitative Simulation Based on Ranked Hyperreals 767 Shusaku Tsumoto Association Action Rules and Action Paths Triggered by Meta-actions 772 Angelina A. Tzacheva and Zbigniew W. Ras Research and Prediction on Nonlinear Network Flow of Mobile Short Message Based on Neural Network 777 Nianhong Wan, Jiyi Wang, and Xuerong Wang Pattern Matching with Flexible Wildcards and Recurring Characters 782 Haiping Wang, Fei Xie, Xuegang Hu, Peipei Li, and Xindong Wu Supplier Selection Based on Rough Sets and Analytic Hierarchy Process 787 Lei Wang, Jun Ye, and Tianrui Li The Covering Upper Approximation by Subcovering 791 Shiping Wang, William Zhu, and Peiyong Zhu Stochastic Synchronization of Non-identical Genetic Networks with Time Delay 794 Zhengxia Wang and Guodong Liu An Extensible Workflow Modeling Model Based on Ontology 798 Zhenwu Wang Interval Type-2 Fuzzy PI Controllers: Why They are More Robust 802 Dongrui Wu and Woei Wan Tan Improved K-Modes Clustering Method Based on Chi-square Statistics 808 Runxiu Wu Decision Rule Acquisition Algorithm Based on Association-Characteristic Information Granular Computing 812 JianFeng Xu, Lan Liu, GuangZuo Zheng, and Yao Zhang Constructing a Fast Algorithm for Multi-label Classification with Support Vector Data Description 817 Jianhua Xu Knowledge Operations in Neighborhood System 822 Xibei Yang and Tsau Young Lin An Evaluation Method Based on Combinatorial Judgement Matrix 826 Jun Ye and Lei Wang Generating Algorithm of Approximate Decision Rules and its Applications 830 Wang Yun and Wu-Zhi Qiang Parameter Selection of Support Vector Regression Based on Particle Swarm Optimization 834 Hu Zhang, Min Wang, and Xin-han Huang T-type Pseudo-BCI Algebras and T-type Pseudo-BCI Filters 839 Xiaohong Zhang, Yinfeng Lu, and Xiaoyan Mao A Vehicle License Plate Recognition Method Based on Neural Network 845 Xing-Wang Zhang, Xian-gui Liu, and Jia Zhao Author Index 849 
xiii 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





