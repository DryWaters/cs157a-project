267 Comparing Evolutionary Optimization with Ant Colony Optimization of Drug Design Interval Rules with and without Pre-Initialization Jurgen Paetz Department of Chemical and Pharmaceutical Sciences Marie-Curie-Stral3e 9-11 60439 Frankfurt am Main Germany paetz@bioinformatik.uni-frankfurt.de AbstractMany applications deal with knowledge in the form of if-then rules In numerical data spaces the condition part of such rules is often based on intervals where the values of single variables are allowed to be within the ranges of the intervals The interval rules can be 
interpreted geometrically as hyper rectangles They can be derived heuristically by adaptive learning In a previous approach we used cuts of membership functions of neuro-fuzzy rules for the pre-initialization of interval rules that were reduced in their dimensions As we showed before such rules can be optimized by evolutionary or by ant colony algorithms to a problem-specific criterion Here we demonstrate how interval rules in the chemical area of virtual screening can be optimized to characterize molecules as novel drugs Mainly a 
comparison between evolutionary and ant colony optimization is given with and without using the neuro-fuzzy pre-initialization of interval borders The results show that pre-initialization is more useful for the evolutionary optimization paradigm 1 Introduction Knowledge in the format of if-then rules is useful in many application areas Many learning approaches are known decision tree rules Mitchell97 association rules Agrawal94 generalization rules PaetzO2 fuzzy rules SpiegelO2 or neuro-fuzzy rules Mitra2000 For example not new but more and more emergent application areas 
are bioinformatics Selbig99 cheminformatics DeLis2004 or medicine Barro2002 beside others In the chemical area of virtual screening LyneO2 BohmOO rules are needed that restrict the number of all available molecules to a clearly smaller number of bioactive molecules The filtered molecules are potential drug candidates Since it is not possible to test all the available molecules on their activity in a laboratory the virtual approach helps decreasing costs by preselection of the most promising molecules Since the dimensionality of the molecules property vectors is 
high a selection of the most important features is desired We used a neuro-fuzzy system FDDA FuzzyDynamic Decay Adjustment Huber95 Paetz2001 to learn heuristic interval rules as the cuts of the corresponding membership functions This approach gave us hyper rectangles of a clearly lower dimensionality Since these rules are adapted heuristically they need not to be optimal Our optimality critrion is the enrichment factor which is the number of bioactive molecules divided by their a priori probability An evolutionary approach EVO Paetz04 as well as 
an ant colony optimization ACO approach PaetzO5 was able to increase the enrichment factors of the rules For a comparison we have to deal with statistical meaningful individuals or ants An analysis about the number of statistical meaningful ants is given Our evolutionary optimization approach mutates and recombines interval borders considering single dimensions We used a strategy where children and parents may survive The ACO optimizes the path length between two edges of a formal graph by natural ants DorigoO4 Every time an ant is 
walking on a sub-path the pheromone concentration is increased Over time subpathes with higher pheromone concentrations become more attractive for following ants In our ACO rule approach for interval rule optimization we used an underlying Markov graph where every direction 261 of a dimension was coded as a layer of states Since we wanted to obtain a precision of two decimals we designed the same number of 101 states 0.00 0.01 0.99 1.00 per layer which was also comfortable to implement However the optimization 
strategies might find the optimal rules without pre-initialization of the interval borders of the rules To demonstrate how the optimization benefits by their pre-initialization we compare the resulting enrichment factors of the best rules with EVO and ACO with and without pre-initialization by classifying the bioactivity of molecules with respect to twelve different drug targets In the next section the chemical data and the corresponding learning task are described The usage of the neuro-fuzzy system for initialization of interval rules 0-7803-9363-5/05/$20.00 2512005 IEEE 267 


268 is presented in Section 3 The optimization algorithms EVO and ACO for interval rule optimization are explained in the Sections 4 and 5 The comparative results and necessary statistical considerations are given in Section 6 followed by a conclusion in Section 7 2 The Chemical Data In life science virtual screening defines the process of searching for a small number of molecules with desired properties out of a magnitude of molecules The properties e.g topological information charges are coded in descriptor vectors TodesciniOO One important task is to find bioactive molecules so called ligands that bind strongly to drug targets Drug targets are larger molecules that are part of disease processes The bioactive ligands inhibit the functioning of the drug targets in the processes For our work we used descriptor vectors for each molecule that codes the two-dimensional information of the molecular graph Schneider99 Five different properties of an atom or atom group are coded d hydrogen-bond donors a hydrogen-bond acceptors p positively charged atoms/groups n negatively charged atoms/groups and 1 lipophilic atoms/groups 1 Since two atoms are connected by a number of single bonds the pairs ddO daO dpO dnO dlO aaO apO anO alO ppO pnO plO nnO nlO 110 dd9 da9 dp9 dn9 d19 aa9 ap9 an9 a19 pp9 pn9 p19 nn9 n19 119 can be considered when using bond lengths between 0 and 9 The dimensionality of the descriptor vectors is 150 The molecular database contains 4705 molecules with their activity class labels for different drug targets SchneiderP03 The drug targets are listed in Table I TABLE I DIMENSION OF THE DRUG TARGET DATASETS AFTER DIMENSION REDUCTION DIM AND DIMENSIONALITY OF THE RULE WITH THE HIGHEST EF ACE  ANGIOTENSIN CONVERTING ENZYME COX2 PEROXISOME PROLIFERATORACTIVATED RECEPTOR SEC  SECRETASE THROM  THROMBIN THE A PRIORI IS NOTED IN THE SECOND COLUMN Drug A priori Dim of Dim of target percentage dataset support rule ACE 0.94 23 7 COX2 2.00 20 4 CRF 1.34 18 4 DPP IV 0.53 24 5 GPCR 34.90 19 8 HIV 1.23 23 5 HOR 4.48 24 5 MMP 1.64 21 4 NK 2.51 19 12 PPAR 0.74 13 9 SEC 0.94 20 9 THROM 4.00 17 7 3 Pre-Initialization of Interval Rules and the Learning Task In the last section we derived a high-dimensional representation for molecules The general tasks considering molecule data are classification of the whole data identification of relevant features and rule generation With respect to rule generation especially the best rules are interesting The FDDA type neuro-fuzzy network Huber95 Silipo99 Paetz2001 is capable of performing all the tasks in a heuristic manner By considering the cuts of the fuzzy membership functions that are hyper trapezoids interval rules R are obtained with the format if ANDi1 var1 in ai b1 then class c 1 If ai is 00 or b1 is 00 then the interval border is not relevant for the interval rule in Formula 1 If both borders are infinite then the dimension is not relevant for the rule Fortunately the interval rules have usually a much lower number of relevant interval borders than 2n where n is the dimensionality of the data Since a small number of interval borders is efficient for a prospective virtual screening in the application phase we generally used the reduced feature set with m  n features as the starting point for optimization of the interval rules The search space for optimization is then composed of 2m  2n interval borders only The pre-initialization with the FDDA neuro-fuzzy system is done by training the m-dimensional data after feature reduction The network topolgy of the FDDA is a common two-layer feedforward network architecture The input is fully connected to the first layer neurons This layer is composed of neurons with trapezoidal membership functions as activation functions The neurons belong to exactly one class here bioactive or inactive All neurons of each class are connected to a summation neuron in the second layer During training the interval borders of the upper and lower rectangles core and support rectangles are adapted geometrically by expansion core rectangle and shrinking support rectangle procedures Starting with only one neuron further neurons are inserted during training whenever no already inserted neuron of the same class as the actual training data sample covers the data by its support rectangle In the following support rules of the format in Formula 1 are considered We used three measures for rating the performance of an interval rule R Paetz2005\(2 The frequency freq\(R is defined in Formula 2 as the number of samples sr,c that are elements of the support hyper rectangle divided by the number s of all the samples in the whole dataset 268 PERCENTAGE OF THE DATA  CYCLOOXYGENASE 2 CRF  CORTICOTROPIN RELEASING FACTOR ANTAGONISTS DPP  DIPEPTIDYLPEPTIDASE GPCR  G PROTEIN COUPLED RECEPTOR HIV  HUMAN IMMUNODEFIENCY VIRUS PROTEASE HOR  HORMONE RECEPTORS MMP  MATRIX METALLOPROTEINASE NK  NEUROKININ RECEPTOR PPAR  


269 freq\(R  Srec  S 2 The confidence conf\(R of rules for class bioactive is defined in Formula 3 as the number of samples Srec active of class bioactive that are elements of the hyper rectangle divided by the number of samples in it conf\(R  Srec active  Srec 3 Multiplied by hundred the indices could be interpreted as a percentage The measures were calculated on the test data not on the training data The third measure considers the a priori probability pactive of the bioactive samples in the dataset The enrichment factor ef\(R is defined as the confidence of R for class active divided by Pactive ef\(R  conf\(R  pactive To get an idea of the rules we give the rule for the ACE dataset that has the best ef under the constraint of a minimum frequency of 0.008 0.8 in the training data More details can be found in Paetz2005\(2 The rule R is given by if dd  0.00 and ppO  0.50 and nnO 2 0.00 and pnl  0.25 and da3  0.38 and nn7 2 0.00 and n18  0.36 then class bioactive with freq\(R  1.1 conf\(R  26.7 and ef\(R  28 In this case the rule R uses only seven features each one with a single interval border so that seven directions need to be optimized with EVO or ACO 4 The Evolutionary Algorithm for Optimization of Interval Rules Evolutionary algorithms were used for optimization of the fuzzy terms in fuzzy rules or for membership function tuning For an overview see Cordon2004 In our approach for optimizing the interval rules every finite interval border is coded in an m-dimensional array that represents one individual cf Algorithm 1 Mutation is done by adding values of normal distributions N\(0,\(Y for varying single borders Recombination is done by either calculating the mean of two corresponding borders of two individuals or by chosing one of the two values Paetz2004 Algorithm 1 EVO begin initialize individuals select the 50 fittest ones as starting population while not maximum number of generations reached do calculate mutated individuals calculate recombined individuals calculate fitness for all individuals select fittest individuals for the next generation parents and children may survive end We chose the following parameter settings for the experiments number of individuals 20 dim of best rule survival rate 0.5 mutation rate 0.5 survived individuals considered recombination rate 0.5 survived individuals considered maximum number of epochs 15 break when S.D of efs is lower than X  0.05 for GPCR X  0.01 because ef is lower due to higher a priori probability in this case cf Table I The fitness calculation is based on the ef of a rule individual R To avoid too small rules the fitness on the training data is set to 1 if the frequency of R is too low Here the training frequency threshold is 0 0.008 0.8 fef freq\(R 60 5 fitness\(R  i_1 freq\(R  0 In Section 6 we give the results of two initialization modes with and without pre-initialization of the interval borders by the neuro-fuzzy system In the next section we describe the idea of how to apply the ACO metaheuristic to interval rule optimization 5 The ACO Approach for Optimization of Interval Rules Ant colony optimization ACO was introduced in 1996 Dorigo96 DorigoO4 Only a few approaches were concerned with optimization of fuzzy rules with ACO Parpinelli2002 GaleaO4 SchockaertO4 Our aim is not the optimization of the fuzzy terms but the optimization of the neuro-fuzzy interval rules by ACO ACO-IntRule PaetzO5 Figure 1 shows the construction graph that corresponds to the ACO Every layer s1 i  1  n in Fig 1 represents a positive or a negative direction in one dimension We assumed two decimals precision so that we modeled 101 states for every layer I 0.00 0.01  0.99 1.00 For example the number of the different values in the relevant dimensions are 2 2 2 4 61 5 38 for the ACE dataset 269 


270 Fig 1 The construction graph for ACO-IntRule Again Formula 5 was used for fitness calculation of single ants with a support threshold on training data 0 0.8 The pheromone-update was done in the usual way cf Formula 6  new=\(1 p THE NEURO-FUZZY ALGORITHM WITH 235 2 TEST DATA ENTRIES FOR GPCR DATA WITH ONE DECIMAL PRECISION THE FREQUENCY ON THE TEST DATA IS GIVEN IN BRACKETS  NO RESULT WITH FREQ 2 0.5 WAS OBTAINED Drug target ACE COX2 CRF DPP IV GPCR HIV HOR MMP NK PPAR SEC THROM ef freq EVO wo 41 1.1 28 0.8 40 0.6 40 0.8 2.3 1.8 22 0.6 22 1.2 51 0.8 21 0.6 26 0.9 23 0.4 22 0.8 ef freq EVO w 44 0.9 31 0.7 32 0.6 40 0.8 2.8 1.3 49 1.0 22 1.2 51 0.8 21 1.1 34 0.5 40 0.7 21 0.7 w or wo w w wo w/wo w w W/wo w/wo w/wo w w w/wo In eleven of twelve cases the ef was equal to or higher with pre-initialization than without considering EVO Only in the CRF case EVO performed better without preinitialization It seems that in this case a not optimal local maximum was obtained with pre-initialization It is remarkable that the ef of the interval rules could not be increased in five cases with pre-initialization To compare the results of the ACO an investigation of the number of statistical meaningful ants was necessary Using EVO all individuals converged to the same local optimum and had no test frequency below the threshold Using 500 ants not all ants achieved a test frequency equal to or above the threshold although a too low training frequency was penalized Thus for every dataset we considered a plot of the test ef values of the 500 ants The resulting curve is depicted for the ACE dataset in 270 Tl  p fitness11 6 Algorithm 2 presents the main steps of ACO-IntRule Paetz2005 Algorithm 2 ACO-IntRule begin Init parameter settings antcycle  1 while not maximum number of ant cycles do for all ants Ri do calculate fitness\(Ri on training data end  for all for all layers do update pheromones C y new calculate transition probabilities pvf new end  for all increment antcycle end while calculate fitness\(R on test data and select fittest rule Parameter settings are number of ants is 500 initialization of pheromones with enrichment factor of the best rule initialization of transition probabilities with identical values pre-initialization of ant distribution with a normal distribution of values in one direction considering the interval borders or with a uniform distribution in the case without preinitialization with respect to the interval borders maximum number of ant cycles set to 30 evaporation factor p  0.3 More details can be found in Paetz2005 Especially a higher number of ants is needed compared to the number of individuals and a lower number of ants led to less performant results 6 Comparative Results With the parameter settings for EVO Section 4 and ACO-IntRule Section 5 we performed experiments For training testing the same 50 data were used as for training testing the neuro-fuzzy system in every of the four cases 1 EVO without pre-initialization 2 EVO with pre-initialization 3 ACO-IntRule without pre-initialization and 4 ACO-IntRule with pre-initialization The minimum training frequency 0.8 and the minimum test frequency 0.5 of one rule were used to assure a general statistical meaning of the rules These thresholds were determined heuristically In Table II the results of the EVO are presented In the right column we mark a result as better with w or without wo preinitialization if the difference is greater than one GPCR greater than 0.1 TABLE II COMPARISON OF THE ENRICHMENT FACTORS EF RESULTS WITH W AND WITHOUT WO PRE-INITIALIZATION EVOLUTIONARY OPTIMIZATION EVO USING 


0 100 200 300 wo w/wo In most cases ACO performed equally well with and without pre-initialization There are three cases where ACO with pre-initialization performed better We conclude that in the ACO case pre-initialization can be useful but it is more important when using EVO cf Table II TABLE V NUMBER OF BEST RULES WITH AND WITHOUT PRE-INITIALIZATION pre-initialization EVO ACO mode w 1+1+1 1+0.5 0.5+0.5 0.5+0.5 0.25+0.25+0.25 0.25+0.25+0.25  4.75  3.25 wo 0.5+0.25 1+0.5 0.25+0.25 0.5+0.25  1.25 0.25+0.25  2.75 271 1I I 271 Fig 2 and for the SEC dataset in Fig 3 without preinitialization In the latter case more ants are statistical meaningful  wo wo w/wo PRE-INITIALIZATION Drug IS ISI target wo w ACE 70 89 COX2 372 362 CRF 317 327 DPP IV 265 290 GPCR 378 350 HIV 270 269 HOR 443 451 MMP 145 159 NK 484 470 PPAR 494 492 SEC 455 422 THROM 410 409 Table IV shows the results considering only the statistical meaningful ants The best efs together with the test frequencies are listed 80r 7060 30 FREQUENCY ON THE TEST DATA IS GIVEN IN BRACKETS Drug target ACE COX2 CRF DPP IV GPCR HIV HOR MMP NK PPAR SEC THROM ef freq ACO wo 38 1.3 33 0.8 44 0.5 58 0.6 1.8 0.6 34 0.5 22 1.1 51 0.8 16 1.2 24 1.2 49 0.6 22 0.8 ef freq ACO w 43 1.1 38 0.7 44 0.5 58 0.6 1.5 0.6 34 0.5 22 0.9 51 0.8 12 1.1 32 0.7 43 0.6 22 0.8 w or wo w w w/wo w w/wo w/wo wo w PRECISION THE 120j  1-100F 80 460 40 20 0 100 200 300 400 Ant number l1 500 Fig 2 Enrichment factors ef on test data for the 500 ants sorted by their training ef ACE data without pre-initialization TABLE III STATISTICAL MEANINGFUL ANTS WITH AND WITHOUT 400 500 Ant Number Fig 3 Enrichment factors ef on test data for the 500 ants sorted by their training ef SEC data without pre-initialization The ants were sorted decending by their training fitness The test ef gets smaller as long as the training frequency threshold criterion was fulfilled With too small frequencies the ef increases clearly Thus only the ants are called statistical meaningful if they are in the set S composed of the first ants where the ef does not increase clearly In Figure 2 seventy of the 500 ants were statistical meaningful Out of the set S the ant with maximum ef was determined only considering the ants with test frequency above 0.5 as for the individuals in EVO We have not yet considered this rigorous scheme in PaetzO5 In Table III the number of statistical meaningful ants is given for all datasets with and without initialization With pre-initialization in most of the cases a similar number of statistical meaningful ants was obtained In some cases the numbers are different but not significantly higher for one initialization mode TABLE IV COMPARISON OF THE ENRICHMENT FACTORS EF RESULTS WITH AND WITHOUT PRE-INITIALIZATION ANT COLONY OPTIMIZATION ACO USING THE NEURO-FUZZY ALGORITHM WITH 2352 TEST DATA ENTRIES FOR GPCR DATA WITH ONE DECIMAL 


272 To compare the number of best rules found by ES or ACO with or vithout pre-initialization the point for one best rule was divided by the number of equal results cf Table V For example if ACO performs equally with and without pre-initialization both modes got 0.5 points EVO in mode w performed best and generated in three cases the best rule solely EVO in mode wo achieved the best results only together with other modes Using ACO pre-initialization was not that important as it was for EVO although a slight performance gain was achieved with it Without pre-initialization ACO is a better choice than EVO which is not the case when using preinitialization In this case EVO performed better Standard distance measures for CATS2D descriptor vectors were used to calculate enrichment factors in virtual screening Fechner2003 These ef values and the improvement factors of our optimized approaches are given in Table VI EVO without pre-initialization performed worst on average TABLE VI IMPROVEMENT FACTORS COMPARED TO EF VALUES OF STANDARD SIMILARITY MEASURES  GPCR WAS NOT CONSIDERED FOR MEAN AND STD CALCULATION BECAUSE IT WAS NOT AVAILABLE WITH A DECIMAL PRECISION IN THE ORIGINAL CASE Drug ef EVO EVO ACO ACO target wo w wo w ACE 23 1.8 1.9 1.7 1.9 COX2 13 2.2 2.4 2.5 2.9 CRF 12 3.3 2.7 3.7 3.7 DPP IV 14 2.9 2.9 4.1 4.1 GPCR 2 1.2 1.4 0.9 0.8 HIV 17 1.3 2.9 2.0 2.0 HOR 9 2.4 2.4 2.4 2.4 MMP 8 6.4 6.4 6.4 6.4 NK 10 2.1 2.1 1.6 1.2 PPAR 11 2.4 3.1 2.2 2.9 SEC 8 2.9 5.0 6.1 5.4 THROM 11 2.0 1.9 2.0 2.2 mean 2.7 3.1 3.2 3.2 std 1.3 1.4 1.7 1.6 The algorithms were implemented in Matlab EVO took several hours of computing time ACO was six times slower The runtime could be speed up to a factor of order 0\(101 by using compiled program versions 7 Conclusion Interval rules can be applied efficiently in virtual screening Earlier we proposed the generation of interval rules by cutting rectangulars from trapezoid membership functions The shapes of the membership functions were adapted by neuro-fuzzy learning These interval rules were not optimal with respect to the enrichment factors that indicates the number of bioactive molecules in the rule compared to their a priori probability Thus we designed an evolutionary approach and an ant colony approach to optimize the enrichment factors Since the two optimization schemes are quite different we gave a comparison of the results that we have achieved While EVO uses individuals that are mutated and recombined probabilities for the pathes of the ants need to be calculated in the ACO case Both optimization schemes depend on parameter settings For ACO we needed to calculate the number of statistical meaningful ants after training It cannot be fully excluded that other settings would lead to other results but general trends were outlined The pre-initialization of the interval borders increased performance in the EVO case clearly so that it can be recommended In the ACO case preinitialization led only to slightly better results Overall if no pre-initialization procedure is available ACO can be applied directly Otherwise EVO with pre-initialization performed best on average The results are plausible due to EVO's property to get stuck in a local optimum EVO is not based on the idea of discrete states so that mutation and recombination can be applied in a continuous manner Since ACO-IntRule was designed with discrete states the next step is to develop a variant with individual numbers of states per layer to generate a more suitable construction graph with an adapted number of state nodes per layer Acknowledgments The modlab team members are thanked for their general support on the biochemical knowledge Bibliography Mitchell T.M 1997 Machine Learning McGrawHill Agrawal R and Skrikant R 1994 Fast Algorithms for Mining Association Rules In Proc of the 20th Int Conf on Very Large Databases VLDB Santiago de Chile Chile pp 487-499 Paetz J 2002 Intersection Based Generalization Rules for the Analysis of Symbolic Septic Shock Patient Data In Proc of the 2nd IEEE Int Conf on Data Mining ICDM Maebashi City Japan pp 673-676 Spiegel D and Sudkamp T 2002 Tuning Membership Functions in Local Evolutionary Learning of Fuzzy Rule Bases In Proc of the 21st Int Conf of the North American Fuzzy Information Processing Soc NAFIPS New Orleans MS USA pp 475-480 Mitra S and Hayashi Y 2000 Neuro-Fuzzy Rule Generation Survey in Soft Computing Framework IEEE Transactions on Neural Networks vol 11\(3 pp 748-768 272 


273 Selbig J Mevissen T and Lengauer T 1999 Decision Tree-based Formation of Concensus Protein Secondary Structure Prediction Bioinformatics vol 15\(12 pp 1039-1046 DeLisle R.K and Dixon S.L 2004 Induction of Decision Trees via Evolutionary Programming J Chem Inf Comput Sci vol 44\(3 pp 862-870 Barro S and Marin R 2002 Fuzzy Logic in Medicine Physica-Verlag Lyne P.D 2002 Structure-based Virtual Screening an Overview Drug Discovery Today vol 7\(20 pp 10471055 Bohm H.-J and Schneider G 2000 Virtual Screening for Bioactive Molecules Wiley VCH Todeschini R and Consonni V 2000 Handbook of Molecular Descriptors Wiley VCH Schneider G Neidhart W Giller T and Schmid G 1999 Scaffold Hopping by Topological Pharmacophore Search a Contribution to Virtual Screening Angewandte Chemie International Edition vol 38\(19 pp 2894-2895 Schneider P and Schneider G 2003 Collection of Bioactive Reference Compounds for Focused Library Design QSAR  Combinatorial Science vol 22 pp 713-718 Huber K.-P and Berthold M.R 1995 Building Precise Classifiers with Automatic Rule Extraction In Proc of the IEEE Int Conf on Neural Networks ICNN Perth Western Australia pp 1263-1268 Paetz J Metric Rule Generation with Septic Shock Patient Data 2001 In Proc of the 1st Int Conf on Data Mining ICDM San Jose CA USA pp 637-638 Silipo R and Berthold M.R 1999 Discriminative Power of Input Features in a Fuzzy Model In Proc of the 3rd Int Symp on Intelligent Data Analysis IDA Amsterdam The Netherlands pp 87-98 Paetz J Towards Ant Colony Optimization of NeuroFuzzy Interval Rules 2005 In Proc of the 24th Int Conf of the North American Fuzzy Information Processing Society NAFIPS Ann Arbor MI USA accepted for publication Dorigo M and Stutzle T 2004 Ant Colony Optimization Bradford Book Paetz J A Neuro-Fuzzy Approach to Virtual Screening in Molecular Bioinformatics 2005 Fuzzy Sets and Systems vol 152\(1 pp 67-82 in print Cordon O Gomide F Herrera Hoffmann F and Magdalena L 2004 Ten Years of Genetic Fuzzy Systems Current Framework and New Trends Fuzzy Sets and Systems vol 141 1 pp 5-31 Dorigo M Maniezzo V and Colorni A 1996 Optimization by a Colony of Cooperating Agents IEEE Transactions on Systems Man and Cybernetics Part B vol 26\(1 pp 1-13 Parpinelli R.S and Lopes H.S and Freitas A.A 2002 Data Mining with an Ant Colony Optimization Algorithm IEEE Transactions on Evolutionary Computation vol 6\(4 pp 321-332 Galea M and Shen Q 2004 Fuzzy Rules from AntInspired Computation Proc of the IEEE Int Conf on Fuzzy Systems FUZZ-IEEE Budapest Hungary 16911696 Schokaert S de Cock M Cornelis C and Kerre E.E Fuzzy Ant Based Clustering 2004 Proc of the 4th Int Workshop on Ant Colony Optimization and Swarm Intelligence ANTS Brussels Belgium 342-349 Fechner U Franke L Renner L Schneider P and Schneider G 2003 Comparison of Correlation Vector Methods for Ligand-based Similarity Searching Journal of Computer Aided Molecular Design vol 17 pp 687698 Paetz J Evolutionary Optimization of Interval Rules for Drug Design 2004 In Proc of the 1st IEEE Symp on Computational Intelligence in Bioinformatics and Computational Biology CIBCB La Jolla CA USA pp 238-243 273 


camp?on?the strongest?cell nR=1 Greedy Alg \(nR=1 b Figure 3: Performance of macroscopic scheduler versus transmit power at nR = 1, 2, 4. Path loss exponent = 3.5. ?L = ?4dB users to one base station even if the user is closer to another base station \(unless the user is very far away from the base station interference cancellation could be effective only for users associated with the base station. This is illustrated in figure 4 Figure 3\(b utility function at nR = 1, 2, 4. Similarly, significant gains are observed at all nR 6 Conclusions In this paper, we consider a generic multi-cell system with K client users \(each with single antenna each with nR antennas and multiuser detector time, a user could be associated with one base station only. Encoding and decoding terminates at the base station level. The problem is to find the optimal dynamic user association with the nB base Figure 4: Illustration of the advantage to associate users with one base station. It is better to associate mobile 1, 2, 3 to base station A \(even if mobile 3 is closer to base station B mobile 4 should be associated with base station B because it is very close to the base station stations such that the system performance is optimized. We proposed an analytical framework for the above macroscopic scheduling problem and introduce a greedy sub-optimal solution. We show that when multi-user detection is incorporated in base stations, the conventional camp-on-the-strongest-cell approach is far from optimal. For example, we illustrated that there are 4 time of throughput gain between the proposed greedy algorithm and the conventional algorithm at various nR. This is because with multi-user detection it is advantageous to associate users to a common base station due to interference cancellation unless the user is very far away from the base station. We found that the scheduling gain increases with decreasing path loss exponent because of the increase in the effective overlapping area between cells References 1] S. Verdu  The Capacity Region of the Symbol-Asynchronous Gaussian Multiple-Access Channel  IEEE Transactions on Information Theory., pp. 733  751, July 1989 2] G. J. Foschini and M. J. Gans  On the limits of wireless communications in a fading environment  Wireless Personal Communications., pp. 315  335, Nov 1998 3] J. M. Torrance and L. Hanzo  Latency Considerations for Adaptive Modulation in an Interference-Free Slow Rayleigh Fading Channel  Proc. VTC  97, Phoenix, USA, June 1997 4] T. M. Cover and J. A. Thomas, Elements of Information Theory John Wiley and Sons, second ed., 1991 5] T. S. Rappaport, Wireless Communications - Principles and Practice. Prentice Hall, second ed., 2002 WCNC 2004 / IEEE Communications Society 564 0-7803-8344-3/04/$20.00  2004 IEEE pre></body></html 


CBA and Neural Networks at 1% confidence level while they are all significantly better than C4.5 decision tree. Taking the interpretability of classification model into account, these two adapted CBA algorithm seem to be appropriate choices for credit scoring because they generated much more compact decision lists \(less sequential rules original CBA. They therefore favour the well-known Occam  s Razor theory and are more suitable for decision makers to understand. A deeper insight into the rules structures shows that original CBA and adapted CBA 1 both focus on generating classification rules that predict good clients \(with bad clients as the default class implication, numerous rules with high confidence but low support have lower ranks than they are in original CBA. These rules are finally discarded since they are not fired by any training samples, which are matched by these rules with higher intensity of implications thus making the decision lists generated by adapted CBA 1 more compact. Adapted CBA 2 mainly mines these classification rules for bad clients \(with good clients as the default class compact rule sets Original CBA Adapted CBA1 Adapted CBA2 C45 NN dataset accuracy no. of rules accuracy num. of rules accuracy no. of rules accuracy accuracy 1 Austr 85.65% 130.5 86.52% 26.4 86.96% 12.4 86.52% 85.07 2 Germ 73.30% 134 74.40% 56.5 73.20% 19.7 72.40% 74.90 3 Bene 72.92% 393 72.30% 186 73.51% 51 70.21% 72.63 Table 2. Experiment results Proceedings of the 2005 IEEE International Conference on e-Business Engineering \(ICEBE  05 0-7695-2430-3/05 $20.00  2005 IEEE In addition, decision makers in financial institution certainly pay more attentions to those rules that predict bad clients, which will be extraordinary costly if they are regarded as good ones 5. Conclusion Intensity of implication is proposed in the beginning as an interestingness measure for association rules Another novel interestingness measure called dilated chi-square is designed by us to reveal the statistical interdependence between the antecedents and consequents of association rules We then adapt CBA algorithm, which can be used to build classifiers based on class association rules, by coupling it with intensity of implication and dilated chi-square respectively. More concretely, Intensity of implication \(or dilated chi-square primary criterion to rank class association rules at the first step of the database coverage pruning procedure in CBA algorithm. Experiments on three credit scoring datasets proved that these two adapted algorithms compared with original CBA, classical C4.5 decision tree and neural network, achieve satisfactory performance and generates classifiers much more compact than CBA 6. Acknowledgement The work was partly supported by National Natural Science Foundation of China \(70231010/70321001 7. References 1] Wang, K. and S. Zhou, Growing decision trees on support-less association rules. in KDD'00. 2000. Boston,MA 2] Liu, B., W. Hsu, and Y. Ma, Integrating Classification and Association Rule Mining. in the 4th International Conference on Discovery and Data Mining. 1998. New 


Conference on Discovery and Data Mining. 1998. New York,U.S 3] Dong, G., et al, CAEP:Classification by aggregating emerging patterns. in 2nd International Conference on Discovery Science,\(DS'99 Artificial Intelligence. 1999. Tokyo,Japan: Springer-Verlag 4] Liu, W., J. Han, and J. Pei, CMAR: Accurate and efficient classification based on multiple class-association rules. in ICDM'01. 2001. San Jose, CA 5] Yin, X. and J. Han, CPAR:Classification based on predictive association rules. in 2003 SIAM International Conference on Data Mining \(SDM'03 Fransisco,CA 6] Agrawal, R. and R. Srikant, Fast algorithm for mining association rules. in the 20th International Conference on Very Large Data Bases. 1994. Santiago,Chile 7] Agrawal, R., T. Imielinski, and A. Swami, Mining association rules between sets of items in large databases. in the ACM SIGMOID Conference on Management of Data 1993. Washington,D.C 8] Guillaume, S., F. Guillet, and J. Philippe, Improving the discovery of association rules with intensity of implication Principles of Data Mining and Knowledge Discovery, 1998 1510: p. 318-327 9] Janssens, D., et al, Adapting the CBA-algorithm by means of intensity of implication. in the First International Conference on Fuzzy Information Processing Theories and Applications. 2003. Beijing, China 10] Gras, G. and A. Lahrer, L'implication statistique: une nouvelle methode d'analysis de donnees. Mathematiques Informatique et Sciences Humaines n 20, 1993 11] Suzuki, E. and Y. Kodratoff, Discovery of  surprising exception rules based on intensity of implication. in PKDD'98. 1998. Berlin: Springer 12] Mills, F., Statistical Methods. 1955: Pitman 13] Lan, Y., et al, Dilated Chi-square: A novel interestingness measure to build accurate and compact decision list. in International conference on intelligent information processing. 2004. Beijing,China 14] Quinlan, J.R., C4.5 programs for machine learning 1993: Morgan Kaufmann 15] Witten, I.H. and E. Frank, Data Mining: practical machine learning tools and techniques with Java implementations. 2000: Morgan Kaufmann, San Francisco 16] Fayyad, U.M. and K.B. Irani, Multi-interval discretization of continuous valued attributes for classification learning. in the Thirteenth International Joint Conference on Artificial Intelligence \(IJCAI Chambery,France: Morgan Kaufmann 17] Blake, C.L. and C.J. Merz, UCI repository of machine learning databases http://www.ics.uci.edu/~mlearn/mlrepository.htm]. 1998 Irvine,CA:University of California, Dept. of Information and Computor Science 18] Dietterich, T.G., Approximate statistical tests for comparing supervised classification learning algorithms Neural Computation, 1998. 10\(7 Proceedings of the 2005 IEEE International Conference on e-Business Engineering \(ICEBE  05 0-7695-2430-3/05 $20.00  2005 IEEE pre></body></html 


absolute values. The results can vary on other computers. But it can be guaranteed that performance ratio of the algorithms will remain the same After making the comparisons with sample data, we came to the conclusion that PD algorithm performs significantly better than the other two especially with larger datasets. PD outperforms DCP and PIP regarding running time. On the other hand, since PD reduces the dataset, mining time does not necessary increase as the number of transactions increases and experiments reveals that PD has better scalability than DCP and PIP. So, PD has the ability to handle the large data mine in practical field like market basket analysis and medical report documents mining 5. References 1] R. Agrawal and R. Srikant, "Fast algoritlnns for mining association rules", VLDB'94, pp. 487-499 2] R. J. Bayardo, "Efficiently mining long patterns from databases", SIGMOD'98, pp.85-93 3] J. Pei, J. Han, and R. Mao, "CLOSET: An Efficient Algorithm for Mining Frequent Closed Itemsets \(PDF Proc. 2000 ACM-SIGMOD International Workshop on Data Mining and Knowledge Discovery, Dallas, TX, May 2000 4] Qinghua Zou, Henry Chiu, Wesley Chu, David Johnson, "Using Pattern Decomposition\( PD Finding All Frequent Patterns in Large Datasets", Computer Science Department University of California - Los Angeles 5] J. Han, J. Pei, and Y. Yin, "Mining Frequent Patterns without Candidate Generation \(PDF  SIGMOD International Con! on Management of Data SIGMOD'OOj, Dallas, TX, May 2000 6] S. Orlando, P. Palmerini, and R. Perego, "The DCP algoritlnn for Frequent Set Counting", Technical Report CS2001-7, Dip. di Informatica, Universita di Venezia 2001.Available at http://www.dsi.unive.itl?orlando/TR017.pdf 7] MD. Mamun-Or-Rashid, MD.Rezaul Karim, "Predictive item pruning FP-tree algoritlnn", The Dhaka University  Journal of Science, VOL. 52, NO. 1, October,2003, pp. 3946 8] Park, J. S., Chen, M.-S., and Yu, P. S, "An Effective Hash Based Algoritlnn for Mining Association Rules", Proc ofthe 1995 ACM-SIGMOD Con! on Management of Data 175-186 9] Brin, S., Motwani, R., Ullman, J., and Tsur, S, "Dynamic Itemset Counting and Implication Rules for Market Basket Data", In Proc. of the 1997 ACM-SIGMOD Conf On Management of Data, 255-264 10] Zaki, M. J., Parthasarathy, S., Ogihara, M., and Li, W New Algoritlnns for Fast Discovery of Association Rules In Proc. of the Third Int'l Con! on Knowledge Discovery in Databases and Data Mining, 283-286 11] Lin, D.-I and Kedem, Z. M., "Pincer-Search: A New Algoritlnn for Discovering the Maximum Frequent Set", In Proc. of the Sixth European Conf on Extending DatabaseTechnology, 1998 12] R. Ramakrishnan, Database Management Systems University of Wisconsin, Madison, WI, USA; International Edition 1998 pre></body></html 


tors such as union, di?erence and intersection are de?ned for pairs of classes of the same pattern type Renaming. Similarly to the relational context, we consider a renaming operator ? that takes a class and a renaming function and changes the names of the pattern attributes according to the speci?ed function Projection. The projection operator allows one to reduce the structure and the measures of the input patterns by projecting out some components. The new expression is obtained by projecting the formula de?ning the expression over the remaining attributes [12 Note that no projection is de?ned over the data source since in this case the structure and the measures would have to be recomputed Let c be a class of pattern type pt. Let ls be a non empty list of attributes appearing in pt.Structure and lm a list of attributes appearing in pt.Measure. Then the projection operator is de?ned as follows ls,lm c id s m f p ? c, p = \(pid, s, d,m, f In the previous de?nition, id ing new pids for patterns, ?mlm\(m projection of the measure component and ?sls\(s ned as follows: \(i s usual relational projection; \(ii sls\(s and removing the rest from set elements. The last component ?ls?lm\(f computed in certain cases, when the theory over which the formula is constructed admits projection. This happens for example for the polynomial constraint theory 12 Selection. The selection operator allows one to select the patterns belonging to one class that satisfy a certain predicate, involving any possible pattern component, chosen among the ones presented in Section 5.1.1 Let c be a class of pattern type pt. Let pr be a predicate. Then, the selection operator is de?ned as follows pr\(c p Join. The join operation provides a way to combine patterns belonging to two di?erent classes according to a join predicate and a composition function speci?ed by the user Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE Let c1 and c2 be two classes over two pattern types pt1 and pt2. A join predicate F is any predicate de?ned over a component of patterns in c1 and a component of patterns in c2. A composition function c pattern types pt1 and pt2 is a 4-tuple of functions c cStructureSchema, cDataSchema, cMeasureSchema, cFormula one for each pattern component. For example, function cStructureSchema takes as input two structure values of the right type and returns a new structure value, for a possible new pattern type, generated by the join. Functions for the other pattern components are similarly de?ned. Given two patterns p1 = \(pid1, s1, d1,m1, f1 p2 = \(pid2, s2, d2,m2, f2 p1, p2 ned as the pattern p with the following components Structure : cStructureSchema\(s1, s2 Data : cDataSchema\(d1, d2 Measure : cMeasureSchema\(m1,m2 Formula : cformula\(f1, f2 The join of c1 and c2 with respect to the join predicate F and the composition function c, denoted by c 1   F  c  c 2   i s  n o w  d e  n e d  a s  f o l l o w s    F  c  c 2     c  p 1   p 2   p 1    c 1  p 2    c 2  F   p 1   p 2     t r u e   5.1.3. Cross-over database operators OCD Drill-Through. The drill-through operator allows one to 


Drill-Through. The drill-through operator allows one to navigate from the pattern layer to the raw data layer Thus it takes as input a pattern class and it returns a raw data set. More formally, let c be a class of pattern type pt and let d be an instance of the data schema ds of pt. Then, the drill-through operator is denoted by c c Data-covering. Given a pattern p and a dataset D sometimes it is important to determine whether the pattern represents it or not. In other words, we wish to determine the subset S of D represented by p \(p can also be selected by some query the formula as a query on the dataset. Let p be a pattern, possibly selected by using query language operators, and D a dataset with schema \(a1, ..., an ible with the source schema of p. The data-covering operator, denoted by ?d\(p,D responding to all tuples in D represented by p. More formally d\(p,D t.a1, ..., t.an In the previous expression, t.ai denotes a speci?c component of tuple t belonging to D and p.formula\(t.a1, ..., t.an instantiated by replacing each variable corresponding to a pattern data component with values of the considered tuple t Note that, since the drill-though operator uses the intermediate mapping and the data covering operator uses the formula, the covering ?\(p,D D = ?\(p not be equal to D. This is due to the approximating nature of the pattern formula 5.1.4. Cross-over pattern base operators OCP Pattern-covering. Sometimes it can be useful to have an operator that, given a class of patterns and a dataset, returns all patterns in the class representing that dataset \(a sort of inverse data-covering operation Let c be a pattern class and D a dataset with schema a1, ..., an pattern type. The pattern-covering operator, denoted as ?p\(c,D all patterns in c representing D. More formally p\(c,D t.a1, ..., t.an true Note that: ?p\(c,D p,D 6. Related Work Although signi?cant e?ort has been invested in extending database models to deal with patterns, no coherent approach has been proposed and convincingly implemented for a generic model There exist several standardization e?orts for modeling patterns, like the Predictive Model Markup Language \(PMML  eling approach, the ISO SQL/MM standard [2], which is SQL-based, and the Common Warehouse Model CWM  ing e?ort. Also, the Java Data Mining API \(JDMAPI 3] addresses the need for a language-based management of patterns. Although these approaches try to represent a wide range of data mining result, the theoretical background of these frameworks is not clear. Most importantly, though, they do not provide a generic model capable of handling arbitrary cases of pattern types; on the contrary only a given list of prede?ned pattern types is supported To our knowledge, research has not dealt with the issue of pattern management per se, but, at best, with peripheral proximate problems. For example, the paper by Ganti et. al. [9] deals with the measurement 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


