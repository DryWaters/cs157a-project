html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Genetic  Algorithms Based Optimization of Membership Functions for Fuzzy Weighted Association Rules Mining Mehmet KAYA Department of Computer Engineering Firat University 23 119, Elaug, Turkey kaya@firat.edu.tr Abstract- Finding the most appropriate fuzzy sets becomes complicated when items are not considered to have equal importance and the support and confidence parameters needed in the mining process are specified as linguistic terms. Existing clustering based automated methods are not satisfactory because they do not consider the optimization of the discovered membership functions To tackle this problem, we propose Genetic Algorithms GAS the fuzzy sets to provide maximum profit based on minimum support and confidence specified as linguistic terms. This is achieved by tuning the base values of the membership functions for each quantitative attribute in a way that maximizes the number of large itemsets. To the best of our knowledge, this is the first effort in this direction Experimental results on lOOK transactions taken from the adult database of US census in year 2000 demonstrate that the proposed clustering method exhibits good performance in terms of the number of produced large itemsets and interesting association rules Index Terms: association rules, fuzzy rules, data mining linguistic terms, weighted rules 1. INTRODUCTION Data mining is the process of extracting previously unknown and potentially useful hidden predictive information from large amounts of data. Discovering association rules is one of the several mining techniques described in the literature. Associations allow capturing almost all possible rules that explain the presence of some attributes according to the presence of other attributes in the same transaction. For instance, an association rule in a supermarket basket data may be stated as  in 20% of the transactions, 75% of the people who buy butter also buy eggs in the same transaction  20% and 75 respectively, represent rule  s support and confidence which are the major factors in measuring the rignificance of an association rule. Simply, support is the percentage of transactions that contain both butter and eggs, while confidence is the ratio of the support of butter and eggs together to the support of butter alone. So, the problem can be stated as: j ind all association rules that satisfy user-specified minimum support and confidence values Recently, researchers investigated weighted mining since the degree of interest in items of a database may differ from one user to another; even the same user may not show the same degree of interest in all items. To satisfy such cases, Cai, et al [3] proposed weighted mining to reflect different importance in different attributes; a user specified numerical weight is assigned to each attribute. Weighted support and weighted 0-7803-8623-W04/$20.00 02004 IEEE Reda ALHAJJ ADSA Lab &amp; Department of Computer Science University of Calgary Calgary, Alberta, Canada alhajj @ cpsc.ucalgary.ca confidence were defined to determine interesting association rules. Yue, et al [25] extended these concepts to fuzzy item vectors. In these studies, minimum support and minimum confidence are specified as numerical values; however, linguistic values are more natural and understandable for humans [14 Although current quantitative association rules mining 


Although current quantitative association rules mining algorithms solved some of the problems introduced by quantitative attributes, they mainly introduced the sharp boundary problem, which can be handled smoothly by introducing fuzziness into the model Unlike classical set theory where membership is binary, the fuzzy set theory provides excellent means to model the  fuzzy  boundaries of linguistic terms by introducing gradual membership. +Based on this, some work has recently been done on the use of fuzzy sets in discovering association rules for quantitative attributes However, in existing approaches fuzzy sets are either supplied by an expert or determined by applying an existing known clustering algorithm. The former is not realistic, in general, because it is extremely hard for an expert to specify fuzzy sets in a dynamic environment On the other hand, approaches that applied classical clustering algorithms to decide on fuzzy sets have not produced satisfactory results. In particular, they have not considered the optimization of membership functions The number of fuzzy sets is given as a constant beforehand and membership functions are tuned in terms of this fixed value This paper contributes to the ongoing research on data mining by combining advantages of several concepts including fuzziness, association rules, weighted mining and linguistic terms for both minimum support and minimum confidence. We also propose a clustering method that employs GAS to optimize membership functions used in determining fuzzy quantitative association rules. The base values of membership functions for each quantitative attribute are tuned by GAS in order to maximize the number of large itemsets in a certain continuous interval of minimum support values Advantages and the effectiveness of the proposed method are demonstrated by testing on lOOK transactions taken from the adult database of US census in year 2000 The rest of this paper is organized as follows. Section 2 is an overview of the related work. Fuzzy weighted association rules, fuzzy item importance, fuzzy minimum support and fuzzy minimum confidence are all defined in Section 3. Our approach of utilizing GAS to optimize membership functions is described in Section 4 Experimental results are presented in Section 5. Sectian 6 is the conclusions 110 2. RELATED WORK Srikant and Agrawal [22] used equidepth partitioning to mine quantitative rules. They separate intervals by their relative ordering and quantities eqpally. Miller and Yang applied Birch clustering [19] to identify intervals and proposed a distance-based association rule to improve the semantics of intervals. Lent, et al [18 presented a geometric-based algorithm to perform clustering for numerical attributes. Finally, Guha, et al [8 proposed an efficient clustering algorithm called CURE Another trend to handle this problem is based on fuzzy theory. In contrast to quantitative clustering, fuzzy linguistic-based approaches focus on qualitative filtering For instance, Yager [24] introduced fuzzy linguistic summaries on different attributes. Hirota and Pedrycz lo, 211 proposed a context sensitive fuzzy clustering method based on fuzzy C-means to construct rule-based models. However, context-sensitive fuzzy C-means method cannot handle data consisting of both numerical and categorical attributes. To solve the qualitative knowledge discovery problem, Au and Chan [4] applied fuzzy linguistic terms to relational databases with numerical and categorical attributes. Later, they proposed the F-APACS method [2] to discover fuzzy association rules. They utilized adjacent difference analysis and fuzziness in finding the minimum support and confidence 


fuzziness in finding the minimum support and confidence values instead of having them supplied by a user Fu, et a1 [6] proposed an automated methodl to find fuzzy sets for the mining of fuzzy association rules; their method is based on CLARANS clustering algorithm [20 We developed a more efficient approach based on CURE 16]. Hong et al [12] proposed an algorithm that integrates fuzzy set concepts and Apriori mining algorithm to find interesting fuzzy association rules from given transactional data. They also proposed definitions for the support and confidence of fuzzy membership grade and designed a data mining approach based on fuzzy sets to find association rules with linguistic terms of human knowledge [13]. Ishibuchi, et a1 [15] illustrated fuzzy versions of confidence and support that can be used to evaluate each association rule. Gyenesei [9] presented two different methods for mining fuzzy quantitative association rules, namely without normalization and with normalization. The approach developed by zhang [27 extends the equidepth partitioning with fuzzy terms However, it assumes fuzzy t e r m  as predefined. Wang and Bridges [23] used GAS to tune membership functions of the fuzzy sets used in mining fuzzy association rules for intrusion detection system 3. WEIGHTED FUZZY ASSOCIATION RULES In this section, we present an overview of weighted fuzzy association rules. We introduce the degree of membership, and define fuzzy association rules. Also, we define wdghted fuzzy support and confidence Let T={tl, tz, .... l transaction t j  represents the j-th tuple in T. We use I=\( i iz ,... , im items each attribute ik may have a binary, categorical or quantitative underlying domain, denoted Di,. Besides each quantitative attribute ik  is associated with at least two fuzzy sets with a membership function per fuzzy set such that each value of it qualifies to be in one or more of the fuzzy sets specified for i k .  The degree of membership of each value of i k  in any of these fuzzy sets is directly based on the evaluation of the membership function of the particular fuzzy set with the value of i t  as input Definition 3.1 \(Membership Function 6, =\(A: ,f item i k .  Each fuzzy set fa! has a corresponding membership function, denoted P from the domain of ik into the interval [0,1], where vi, E Dit . Formally, P,; : D,, + [OJL where U,; \(v According to Definition 3.1, the obtained value P     in the interval [0,1], with the lower bound 0 strictly indicates  not a member  and the upper bound 1 indicates  total membership  All other values between 0 and 1, exclusive, specify  partial membership  degree Consider a database of transactions, its attributes, and the fuzzy sets associated with quantitative attributes interesting rules are potentially useful regularities [17 Definition 3.2 \(Fuzzy Association Rules association rule is expressed as v,. totally belongs to fuzzy set f U,, is not a member of fuzzy set f,a  o h e i i s e  v,, partially belongs to fuzzy set fi If X = { x l , x z ,  ..., x p Y=\(YI, YZ, ..., y J  is B=Ig l ,  g2, ..., gql where X and Y are disjoint sets of attributes called itemets, i.e., X U Y r l  and X n Y = &amp;  A and B contain the fuzzy sets associated with corresponding attributes in X and U, respectively, i.e.,f; is the set of fuzzy set related to attribute x, and g, is the set of fuzzy set related to attribute y,; l S S p  and 15jSq. Finally, for a rule to be interesting it should have enough support and high confidence value i.e., larger than user specified thresholds As weighted mining is concerned, items are assigned weights to reflect their importance and weighted support 


weights to reflect their importance and weighted support and confidence values are employed in deciding on interesting association rules. In general, most data mining algorithms set weights of items as numerical values, with the weight of an item varies between 0 and 1 based on users  experience or intuition. Here  0  means the corresponding attribute should be neglected, and  1   indicates the corresponding attribute is one of the most important attributes for the user So, let \( X . A is a set of attributes and A is the set of corresponding fuzzy sets. A weight 0 5  w instance \(x, a X , A By assigning weights to attributes, we employ weighted fuzzy support,and weighted fuzzy confidence in deciding on large weighted \(itemset, fuzzy-set interesting association rules, respectively Definition 3.3 \(Weighted Fuzzy Support itemset-fuzzy set pair \( X - A  z,,J-I,,e x M+/ JW  X I IPl e 111 Based on Definition 3.3, an itemset-fuzzy set pair X . A greater than or equal to the specified minimum support threshold, i.e., wsp Definition 3.4 \(Weighted Fuzzy Confidence the rule "If X is A then Y is  B ' ,  its weighted fuzzy confidence is Explicitly, each large itemset, denoted L, is used in deriving all association rules \(L-S Strong association rules are discovered by choosing from among all the generated possible association rules only those with confidence over a pre-specified minimum confidence. However, not all strong rules are interesting enough to be reported to the user. Whether a rule is interesting or not can be judged either subjectively or objectively. Ultimately, only users can judge if a given rule is interesting or not, and this judgment, being subjective, may differ from one user to another. However objective interestingness criterion, based on the statistics behind the analyzed data, can be used as one step towards the goal of weeding out uninteresting rules from presentation to the user To illustrate this, consider a rule X a Y  with 50 support and 66.7% confidence. Further, assume that the support of Y is 70%. For such case, it can be said that the rule X-Y is strong based on the supportconfidence framework. However, it is incomplete and misleading since the overall support ofk is 75%, even greater than 66.7%. In other words, this analysis leads to the following interpretation: a customer who buys Xis  less likely to buy Y than a customer about whom we have no information he truth here is that there is a negative dependence between buying X and buying Y. This negative dependence leads to not considering X-Y as strong rule As a result, there should be some filtering criteria to eliminate such rules from consideration as interesting rules. Explicitly, to help filtering out such misleading strong association rules, the interestingness of a rule XaY, denoted I\(XsY X = Y X Y to give a more precise rule characterization A rule is filtered out if its interestingness is less than 1 since the nominator is the actual likelihood of both X and Y being present together and the denominator is the likelihood of having the two attributes being independent As the above example is concerned, we can calculate the interestingness of X ~ Y  as: 1 \( ~ - r which means that this rule is not interesting enough to be reported to the user. ?his process will help in returning only rules having positive interestingness, and hence the 


only rules having positive interestingness, and hence the size of the reported result is reduced to include more precise rules 0.75x0.7 3.1. FUZZY REPRESENTATION OF ITEM IMPORTANCE MINIMUM SUPPORT AND MINIMUM CONFIDENCE The importance of an item is not only a vital measure of interestingness, but also a way to permit users to control the mining results by taking specific actions. So, it is more natural and intuitive for humans to deal with linguistic terms than discrete values. In other words, it is more flexible and more understandable for human beings to handle the measures showing the importance of an item as linguistic terms. Motivated by this, we represent weights of items using fuzzy sets Nmin. sup Fig. 1 Membership functions representing weights of an item p\(min. sup A Fig. 2 Membership functions representing minimum support Shown in Figure 1 are membership functions of the fuzzy sets used to represent the weight of a given item Membership functions have a uniform structure and the weight of an item can take 5 different linguistic terms We also used linguistic terms to express minimum support and minimum confidence, and based on the same justification raised above about utilizing linguistic terms for the importance of items. This way, instead of a sharp boundary, we achieve a boundary with continuous interval of minimum support as well as minimum confidence. Shown in Figure 2 are the membership functions used for minimum support; note that membership functions of minimum confidence have the same trend shown in Figure 2 4. EMPLOYING ENETIC ALGOR~HMS TO OPTIMIZE MEMBERSHIP FUNCTIONS he standard CA starts with an initial population of randomly or heuristically generated individuals, and advances toward better individuals by applying genetic operators modeled on the genetic processes occurring in nature. "he population undergoes evolution in a form of natural selection. During successive iterations, called generations, individuals in the population are rated for their adaptation as solutions, and on the basis of these evaluations. As a result, a new population of individuals is formed using a selection mechanism and specific genetic operators such as crossover and mutation. To form a new population, individuals are selected according to their fitness. Consequently, an evaluation or fitness function must be devised for each problem to be solved Given a particular individual, a possible solution, the fitness function accepts a decoded chromosome and produces an objective value as a measure of the performance of the input chromosome  112 considering all factors that play important role in optimizing the problem under investigation. Every new population generated in the process is evaluated with respect to the fitness function. The evaluation process is a main source to providing the mechanism for evaluating the status of each chromosome; it is an important link J between the GA and the system The CA employed in this study maximizes the number of large itemsets in a continuous interval of minimum support values. We created a continuous domain within a certain interval of minimum support and minimum confidence because we used linguistic minimum support and minimum confidence values. Figure 4 shows membership functions of the minimum support variable used in computing the fitness function. This variable has 5 uniform membership functions and its definitive 


5 uniform membership functions and its definitive interval is bounded within [0.05,0.15 With linguistic minimum support, the process of finding the set of large itemsets proceeds as illustrated next. Assume the linguistic minimum support value is given as  Low  First, this value is transformed into a fuzzy set of minimum support, namely \(0.05, 0.075, 0. l as shown in Figure 4. Second, the fuzzy weighted set of the given minimum support is calculated. Finally, the weighted support of each item or itemset is compared to the fuzzy weighted minimum support by fuzzy ranking. If the weighted support is equal to or greater than the weighted minimum support, then the corresponding itemset is considered large Fig. 3 Membership functions and base variablesof attribute ir poni  SUP Fig. 4 Membership functions of the minimum support for the fitness function 4.1. CHROMOSOME ENCODING Our target in using GAS is to cluster the values of quantitative attributes into fuzzy sets with respect to a given fitness evaluation criteria. For this purpose, each individual represents base values of membership functions of a quantitative attribute in the database. In our experiments, we used membership functions in triangular shape because it is in general the most appropriate shape To illustrate this, consider a quantitative attribute ik and assume it has 3 corresponding fuzzy sets Membership functions for, attribute ik and their base variables are shown in Figure 3. Each base variable takes finite values. For instance, the search space of base value bi lies between the minimum and maximum values of attribute ik, denoted mW4 The search intervals of all the base values and intersection point Rib of attribute ik are bf. : [min\(D D Rj, : [min\(Di b: : [min\(D b: : [ R , ,  m a W bl :[min\(D D So, based on the assumption of having 3 fuzzy sets per attribute, as it is the case with attribute ik, a chromosome consisting of the base lengths and the intersection points is represented as: b ~ b ~ R i , b i : b ~ b ~ ~ b , : R i ~ b ~ b ~  . . b ~ b ~ R i - b ~ b We use real-valued coding, where chromosomes are represented as floating point numbers and their genes are the real parameters  Ihese chromosomes form the input to the fitness function described in the next section 4.2. FITNESS EVALUATION The fitness function measures the goodness of an individual in a given population. It is one of the key issues to a successful GA; simply because the main task in a GA is to optimize a fitness function. Consequently the fitness function should be carefully set, by 4.3. SELEC~ION PROCESS During each generation, individuals with higher fitness values survive while those with lower fitness values are destroyed. In other words, individuals who are strong according to parent selection policy are candidates to form a new population. Parent selection mimics the survival of the best individuals in the given population Many selection procedures are currently in use However, Holland  s original fitness-proportionate selection is one of the simplest selection procedures [ l  I So, we used this selection policy in our experiments Let firness\(x,t r the fitness of individual x and the average fitness of the population during evolution phase t .  Then, the usage value of individual x as a parent is: rsr \(x , t xJ 1 After selecting chromosomes with respect to the evaluation function, genetic operators such as, crossover and mutation, are applied to these individuals 


and mutation, are applied to these individuals Crossover refers to information exchange between individuals in a population in order to produce new individuals. The idea behind the crossover operation utilized in our study is as follows. It takes as input 2 individuals, selects random points, and exchanges the subindividuals behind the selected points. Since the length of the chromosomes is long, the multi-point crossover strategy has been used with the crossover points determined randomly On the other hand, mutation means a random change in the information of an-individual. It is very important 113 for populations. It is an operation that defines a local or global variation in an individual. Mutation is traditionally performed in order to increase the diversity of the genetic information. Otherwise, after several generations, the diversity of the chromosomes decreases and some chunks of the chromosomes may end up being the same for all population members and the information they contain may not evolve further. A probability test determines whether a mutation will be carried out or not. ?he probability of mutation depends on the following condition average fitness of new generation &lt;average fitness of old generation Since the initial population can be a subset of all possible solutions, an important bit of each chromosome may be inverted, i.e., 0 appears as 1 or vice versa Crossover may not solve this and mutation is inevitable for the solution Finally, after generating each individual in the initial population, the executed GA includes the following steps Algorithm 4.1 \(Generating Association Rules 1 2 3 4 5 6 7 8 9 Using the given membership functions about item importance, transform each linguistic term, which reflects the importance of item i,, I l k  c m ,  into a fuzzy set wt of weights Specify population size N and generate initial chromosomes According to the current chromosome, transform the quantitative value f ,A of each item it in each transaction f j ,  1 5 j 5 n , into a fuzzy set f Calculate the fuzzy weighted support of each item fizzy set pair \( j k 9 f Compute the weighted fuzzy set of the given minimum support value as Find the large itemsets based on the weighted fuzzy set of the given minimum support value Evaluate each chromosome with respect to the already specifie.6 fitness function Perform selection, crossover and mutation If not end-test then go to Step 3 otherwise return the best chromosome WMinS=S.\(the weight of 10. Generate all possible association rules from each identified large weighted fuzzy itemset 11. From the rules generated in step 10, identify strong rules based on the specified fuzzy weighted confidence 12. From the rules identified in step 11, decide on interesting association rules by calculating the interestingness value for each strong rule Algorithm 4.1 employs CA to return interesting 


Algorithm 4.1 employs CA to return interesting association rules. The process considers fuzzy importance of items and involves fuzzy weighted support and confidence. This algorithm has been implemented and tested, the results are presented next in Section 5 5. EXPERIMENTAL RESULTS We used real-life dataset and conducted some experiments to assess the effectiveness of the CA-based fuzzy weighted mining approach presented in this paper All of the experiments were performed using a Pentium 111, 1.4GHz CPU with 512 MB of memory and running Windows 2000. As experimental data, we used lOOK transactions dataset taken from the adult data of United States census in 2000. In the experiments, we have used 6 quantitative attributes, each with three corresponding fuzzy sets. Finally, we have used three linguistic intervals for which random linguistic weights have been generated namely \(Important, Very-Important Ordinary Important Unimportant, Ordinary 0-1 and UI-0, respectively 500 2 400 300 &lt; 200 1 0  1 100 Very Low Medium High Very Low High Minimum Support Fig. 5 Number of large itemsets for linguistic terms of minimum support M i n .  cont A Fig. 6 Membership function for nunimum confidence The first experiment tests, for the above three different linguistic weight intervals, the correlation between expressing minimum support in linguistic terms and the number of large itemsets produced. The obtained results are reported in Figure 5, which shows that the number of large itemsets decreases as a function of the linguistic minimum support yI 100 d, 80 1 60 I 5 40 0 B 20 i o Very Low Medium High Very LOW High MininumConfidencc Fig. 7 Number of interesting rules for different linguistic terms of nunimumconfidence; nunimum support fixed as  nuddle   In the second experiment, the minimum support is fixed at the linguistic value  middle  and we tested, for the three linguistic weight intervals, the effect of using linguistic te rm to express minimum confidence, as shown in Figure 6, on the number of generated interesting 114 association rules. The achieved results are reported in Figure 7. The obtained results do meet our expectations i.e., more rules are generated for higher weights However, the number decreases, for all cases, as the linguistic confidence threshold increases 140 120 TI00 80 60 2 40 20 0 0 20 40 60 80 100 Number ofTransactions \(K 


Fig. 8 Runtime for GAS to find fuzzy sets for the three linguistic intervals The last experiment is dedicated to investigate the performance for the three linguistic intervals. In particular, we examined how the performance varies with the number of transactions. This is reflected in Figure 8 which shows the runtime as we increase the number of input records from 10K to 100K, for the three different cases. The results plotted in Figure 8 show that the method scales quite linearly for the census dataset used in the experiments 6. CONCLUSIONS In this paper, we proposed a clustering approach to solve the problem of interval partitioning in favor of the maximum number of large itemsets based on linguistic minimum support and confidence. The main achievement of the proposed approach is employing GAS to dynamically adjust and optimize membership functions which are essential in finding interesting weighted association rules from quantitative transactions, based on support and confidence specified as linguistic terms Compared to previous mining approaches, the proposed approach directly manages linguistic parameters, which are more natural and understandable to humans. Results of the experiments conducted on a real life census dataset demonstrated the effectiveness and applicability of the proposed approach r31 r41 r51 REFERENCES R. Agrawal, T. Imielinski and A. Swami  Mining association rules between sets of items in large databases  Proc. of ACM SIGMOD, pp.207-216, 1993 W.H. Au and K.C.C. Chan  An Effective Algorithm for Discovering Fuzzy Rules in Relational Databases  Proc C.H. Cai, et al  Mining Association Rules with Weighted Items  Proc. of IDEAS, pp.68-77, 1998 K.C.C. Chan and W.H. Au  Mining Fuzzy Association Rules  Proc. of ACM CIKM, pp.209-215, 1997 B.C. Chien, ZL. Lin and T.P. Hong  An Efficient Clustering Algorithm for Mining Fuzzy Quantitative Association Rules  IFSA World Congress and NAFIPS International Conference, Vo1.3, pp.1306-1311,2001 A.W.C. Fu, et al  Ending Fuzzy Sets for the Mining of Association Rules for Numerical Attributes  Proc. of the OfIEEE-FUZZ, pp.1314-1319,1998 115 International Symposium of Intelligent Data Engineering and Learning, pp.263-268, Oct. 1998 D.E. Goldberg, Genetic Algorithms in Search Optimization, and Machine Learning, Addison-Wesley Reading, MA, 1989 S. Guha, R. Rastogi and K. Shim  CURE: An Efficient Clustering Algorithm for Large Databases  Information Systems, Vo1.26, No.1, pp.35-58,2001 A. Gyenesei  A Fuzzy Approach for Mining Quantitative Association Rules  TUCS Technical Report No.336 2000 K. Hirota and W. Pedrycz  Linguistic Data Mining and Fuzzy Modelling  Proc. of IEEE-FUZZ, pp.1448-1496 1996 J.H. Holland, Adaptation in Natural and Artificial Systems, The MIT Press, Cambridge, MA, MIT Press edition, 1992. First edition: University of Michigan Press 1975 T.P. Hong, C.S. Kuo and S.C. Chi  A fuzzy data mining algorithm for quantitative values  Proc. of the International Conference on Knowledge-Based Intelligent Information Engineering Systems, pp.480483, 1999 T.P. Hong, C.S. Kuo and S.C. Chi  Mining Association 


Rules from Quantitative Data  Intelligent Data Analysis Vo1.3, pp.363-376, 1999 T. P. Hong, M. J. Chiang and S. L. Wang  Mining from Quantitative Data with Linguistic Minimum Supports and Confidences  Proc. of IEEE-FUZZ, pp. 494-499,2002 H. Ishibuchi, T. Nakashima and T. Yamamoto  Fuzzy Association Rules for Handling Continuous Attributes   Proc. of IEEE International Symposium on Industrial Electronics, pp. 1 1 8 - 12 1, 200 1 M. Kaya, R. Alhajj, F. Polat and A. Arslan  Efficient Automated Mining of Fuzzy Association Rules  Proc. of DEXA, 2002 C.M. Kuok, A.W. Fu and M.H. Wong  Mining fuzzy association rules in databases  SIGMOD Record, Vol. 17 No.1, pp.41-46, 1998 B. Lent, A. Swami and J. Widom  Clustering Association Rules  Proc. of IEEE ICDE!, pp.220-23 1 1997 R.J. Miller and Y. Yang  Association Rules over Interval Data  Proc. ofACM SIGMOD, pp.452-461, 1997 R. Ng and J. Han  Efficient and effective clustering methods for spatial data mining  Proc. of VLDB, 1994 W. Pedrycz  Fuzzy Sets Technology in Knowledge Discovery  Fuzzy Sets and Systems, 98, pp.279-290 1998 R. Srikant and R. Agrawal  Mining quantitative association rules in large relational tables  Proc. of ACM W. Wang and S.M. Bridges  Genetic Algorithm Optimization of Membership Functions for Mining Fuzzy Association Rules  Proc. of the International Conference on F m y  Theory &amp; Technology, pp. 13 1-134,2000 R.R. Yager  Fuzzy Summaries in Database Mining   Proc. of the Conference on Artificial Intelligence for Application, pp.265-269, 1995 S .  Yue, el al  Mining fuzzy association rules with weighted items  Proc. of IEEE SMC Conference pp.1906-1911,2000 L.A., Zadeh  Fuzzy Sets  Information and Control W. Zhang  Mining Fuzzy Quantitative Association Rules  Proc. of IEEE ICTRI, pp.99-102, 1999 SIGMOD, pp.1-12, 19 V01.8, pp.338-353, 1965 pre></body></html 


efficiency then AOFI. However utilization of fuzzy concept hierarchies provides more flexibility in reflecting expert knowledge and so allows better modeling of real-life dependencies among attribute values, which will lead to more satisfactory overall results for the induction process. The drawback of the computational cost may additionally decline when we notice that, in contrast to many other data mining algorithms, hierarchical induction algorithms need to run only once through the original \(i.e. massive dataset. We are continuing an investigation of computational costs of our approach for large datasets ACKNOWLEDGMENT Rafal Angryk would like to thank the Montana NASA EPSCoR Grant Consortium for sponsoring this research REFERENCES 1] J. Han , M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann, New York, NY 2000 2] J. Han, Y. Cai, and N. Cercone  Knowledge discovery in databases: An attribute-oriented approach  Proc. 18th Int. Conf. Ver y Large Data Bases, Vancouver, Canada, 1992, pp. 547-559 3] J. Han  Towards Efficient Induction Mechanisms in Database Systems  Theoretical Computing Science, 133, 1994, pp. 361-385 4] J. Han, Y. Fu  Discovery of Multiple-Level Association Rules from Large Databases  IEEE Trans.  on KD E, 11\(5 5] C.L. Carter, H.J. Hamilton  Efficient AttributeOriented Generalization for Knowledge Discovery from Large Databases  IEEE Trans. on KDE 10\(2 6] R.J. Hilderman, H.J. Hamilton, and N. Cercone  Data mining in large databases using domain generalization graphs  Journal of Intelligent Information Systems, 13\(3 7] C.-C. Hsu  Extending attribute-oriented induction algorithm for major values and numeric values   Expert Systems with Applications , 27, 2004, pp 187-202 8] D.H. Lee, M.H. Kim  Database summarization using fuzzy ISA hierarchies  IEEE Trans . on SMC - part B, 27\(1 9] K.-M. Lee  Mining generalized fuzzy quantitative association rules with fuzzy generalization hierarchies  20th NAFIPS Int'l Conf., Vancouver Canada, 2001, pp. 2977-2982 10] J. C. Cubero, J.M. Medina, O. Pons &amp; M.A. Vila  Data Summarization in Relational Databases through  Fuzzy Dependencies  Information Sciences, 121\(3-4 11] G. Raschia, N. Mouaddib  SAINTETIQ:a fuzzy set-based approach to database summarization   Fuzzy Sets and Systems, 129\(2 162 12] R. Angryk, F. Petry  Consistent fuzzy concept hierarchies for attribute generalization  Proceeding of the IASTED Int. Conf. on Information and Knowledge Sharing, Scottsdale AZ, USA, November 2003, pp. 158-163 13] Toxics Release Inventory \(TRI available EPA database hosted at http://www.epa.gov/tri/tridata/tri01/index.htm The 2005 IEEE International Conference on Fuzzy Systems790 pre></body></html 


the initial global candidate set would be similar to the set of global MFIs. As a result, during the global mining phase the communication and synchronization overhead is low  0 2 4 6 8 1 0 Number of Nodes Figure 5. Speedup of DMM 4.4.2 Sizeup For the sizeup test, we fixed the system to the 8-node con figuration, and distributed each database listed in Table 2 to the 8 nodes. Then, we increased the local database sire at each node from 45 MB to 215 MB by duplicating the initial database partition allocated to the node. Thus, the data distribution characteristics remained the same as the local database size was increased. This is different from the speedup test, where the database repartitioning was per formed when the number of nodes was increased. The per formance of DMM is affected by the database repartitioning to some extent, although it is usually very small. During the sizeup test, the local mining result of DMM is not changed at all at each node The results shown in Figure 6 indicate that DMM has a very good sizeup property. Since increasing the size of local database did not affect the local mining result of DMM at each node, the total execution time increased just due to more disk U 0  and computation cost which scaled almost linearly with sizeup 5 Conclusions In this paper, we proposed a new parallel maximal fre quent itemset \(MFI Max-Miner \(DMM tems. DMM is a parallel version of Max-Miner, and it re quires low synchronization and communication overhead compared to other parallel algorithms. In DMM, Max Miner is applied on each database partition during the lo 0 45 90 135 180 225 270 Amwnt of Data per Node \(ME Figure 6. Sizeup of DMM cal mining phase. Only one synchronization is needed at thc end of this phase to construct thc initial global candi date set. In the global mining phase, a top-down search is performed on the candidate set, and a prefix tree is used to count the candidates with different length efficiently. Usu ally, just a few passes are needed to find all global maximal frequent itemsets. Thus, DMM largely reduces the number of synchronizations required between processing nodes Compared with Count Distribution, DMM shows a great improvement when some frequent itemscts are large \(i.e long patterns employed by DMM for efficient communication between nodes; and global support estimation, subset-infrequency based pruning, and superset-frequency based pruning are used to reduce the size of global candidate set. DMM has very good speedup and sizeup properties References I ]  R. Agrawal and R. Srikant  FdSt Algorithms for Mining As sociation Rules  Pmc. o f f h e  ZOrh VLDB Conf, 1994, pp 487499 2] R. Agrawal and I. C. Shafer  Parallel Mining of Association Rules  IEEE Trans. on Knowledge and Dura Engineering Vol. 8, No. 6, 1996, pp. 962-969 3] R. I. Bayardo  Efficient Mining Long Patlems from Databases  Proc. ofrhe ACM SIGMOD Inf  l Conf on Man ogemenr ofDara, 1998, pp. 85-91 4] S.  M. Chung and J. Yang  A Parallel Distributive Join Al gorithm for Cube-Connected Multiprocessors  IEEE Trans on Parallel and Disrribured Systems, Vol. 7, No. 2, 1996, pp 127-137 51 M. Snir, S. Otto. S. Huss-Lederman, D. Walker, and J. Don gana, MPI: The Complete Reference, The MIT Press, 1996 


gana, MPI: The Complete Reference, The MIT Press, 1996 6] R. Rymon  Search through Systematic Set Enumeralion   Pmc. of3rd Inr  l Con$ on Principles of Knowledge Repre sentation and Reasoning, 1992, pp. 539-550 507 pre></body></html 


sketch-index in answering aggregate queries. Then Section 5.2 studies the effect of approximating spatiotemporal data, while Section 5.3 presents preliminary results for mining association rules 5.1 Performance of sketch-indexes Due to the lack of real spatio-temporal datasets we generate synthetic data in a way similar to [SJLL00 TPS03] aiming at simulation of air traffic. We first adopt a real spatial dataset [Tiger] that contains 10k 2D points representing locations in the Long Beach county \(the data space is normalized to unit length on each dimension These points serve as the  airbases  At the initial timestamp 0, we generate 100k air planes, such that each plane \(i uniformly generated in [200,300], \(ii, iii destination that are two random different airbases, and iv  the velocity direction is determined by the orientation of the line segment connecting its source and destination airbases move continually according to their velocities. Once a plane reaches its destination, it flies towards another randomly selected also uniform in [0.02, 0.04 reports to its nearest airbase, or specifically, the database consists of tuples in the form &lt;time t, airbase b, plane p passenger # a&gt;, specifying that plane p with a passengers is closest to base b at time t A spatio-temporal count/sum query has two parameters the length qrlen of its query \(square number qtlen of timestamps covered by its interval. The actual extent of the window \(interval uniformly in the data space \(history, i.e., timestamps 0,100 air planes that report to airbases in qr during qt, while a sum query returns the sum of these planes  passengers. A workload consists of 100 queries with the same parameters qrlen and qtlen The disk page size is set to 1k in all cases \(the relatively small page size simulates situations where the database is much more voluminous specialized method for distinct spatio-temporal aggregation, we compare the sketch-index to the following relational approach that can be implemented in a DBMS. Specifically, we index the 4-tuple table lt;t,b,p,a&gt; using a B-tree on the time t column. Given a count query \(with window qr and interval qt SELECT distinct p FROM &lt;t,b,p,a&gt WHERE t?qt &amp; b contained in qr The performance of each method is measured as the average number of page accesses \(per query processing a workload. For the sketch-index, we also report the average \(relative Specifically, let acti and esti be the actual and estimated results of the i-th query in the workload; then the error equals \(1/100 set the number of bits in each sketch to 24, and vary the number of sketches The first experiment evaluates the space consumption Figure 5.1 shows the sketch index size as a function of the number of sketches used \(count- and sum-indexes have the same results more sketches are included, but is usually considerably smaller than the database size \(e.g., for 16 signatures, the size is only 40% the database size 0 20 40 60 80 


80 100 120 140 160 8 16 32 number of sketches size \(mega bytes database size Figure 5.1: Size comparison Next we demonstrate the superiority of the proposed sketch-pruning query algorithm, with respect to the na  ve one that applies only spatio-temporal predicates. Figure 5.2a illustrates the costs of both algorithms for countworkloads with qtlen=10 and various qrlen \(the index used in this case has 16 sketches also illustrate the performance of the relational method which, however, is clearly incomparable \(for qrlen?0.1, it is worse by an order of magnitude we omit this technique Sketch-pruning always outperforms na  ve \(e.g., eventually two times faster for qrlen=0.25 increases with qrlen, since queries returning larger results tend to set bits in the result sketch more quickly, thus enhancing the power of Heuristics 3.1 and 3.2. In Figure 5.2b, we compare the two methods by fixing qrlen to 0.15 and varying qtlen. Similar to the findings of [PTKZ02]4 both algorithms demonstrate  step-wise  growths in their costs, while sketch-pruning is again significantly faster The experiments with sum-workloads lead to the same observations, and therefore we evaluate sketch-indexes using sketch-pruning in the rest of the experiments 4 As explained in [PTKZ02], query processing accesses at most two paths from the root to the leaf level of each B-tree regardless the length of the query interval Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE sketch-pruning naive relational 0 100 200 300 400 500 600 700 800 900 0.05 0.1 0.15 0.2 0.25 number of disk accesses query rectangle length 300 0 100 200 400 500 600 1 5 10 15 20 number of disk accesses query interval length a qtlen=10 b qrlen=0.15 Figure 5.2: Superiority of sketch-pruning \(count As discussed in Section 2, a large number of sketches reduces the variance in the resulting estimate. To verify this, Figure 5.3a plots the count-workload error of indexes 


using 8-, 16-, and 32- sketches, as a function of qrlen qtlen=10 error \(below 10 it increases slowly with qrlen used, however, the error rate is much higher \(up to 30 and has serious fluctuation, indicating the prediction is not robust. The performance of 16-sketch is in between these two extremes, or specifically, its accuracy is reasonably high \(average error around 15 much less fluctuation than 8-sketch 32-sketch 16-sketch 8-sketch relative error 0 5 10 15 20 25 30 35 0.05 0.1 0.15 0.2 0.25 query rectangle length relative error 0 5 10 15 20 25 30 35 1 5 10 15 20 query interval length a qtlen=10, count b qrlen=0.15, count relative error query rectangle length 0 5 10 15 20 25 0.05 0.1 0.15 0.2 0.25 relative error query interval length 0 5 10 15 20 25 30 1 5 10 15 20 c qtlen=10, sum d qrlen=0.15, sum Figure 5.3: Accuracy of the approximate results The same phenomena are confirmed in Figures 5.3b where we fix qrlen to 0.15 and vary qtlen 5.3d \(results for sum-workloads number of sketches improves the estimation accuracy, it also leads to higher space requirements \(as shown in Figure 5.1 Figures 5.4a and 5.4b show the number of disk accesses for the settings of Figures 5.3a and 5.3b. All indexes have almost the same behavior, while the 32-sketch is clearly more expensive than the other two indexes. The interesting observation is that 8- and 16-sketches have 


interesting observation is that 8- and 16-sketches have almost the same overhead due to the similar heights of their B-trees. Since the diagrams for sum-workloads illustrate \(almost avoid redundancy 32-sketch 16-sketch 8-sketch number of disk accesses query rectangle length 0 50 100 150 200 250 300 350 400 0.05 0.1 0.15 0.2 0.25 number of disk accesses query interval length 0 50 100 150 200 250 300 350 1 5 10 15 20 a qtlen=10 b qrlen=0.15 Figure 5.4: Costs of indexes with various signatures Summary: The sketch index constitutes an effective method for approximate spatio-temporal \(distinct aggregate processing. Particularly, the best tradeoff between space, query time, and estimation accuracy obtained by 16 sketches, which leads to size around 40 the database, fast response time \(an order of magnitude faster than the relational method average relative error 5.2 Approximating spatio-temporal data We proceed to study the efficiency of using sketches to approximate spatio-temporal data \(proposed in Section 4.1 as in the last section, except that at each timestamp all airplanes report their locations to a central server \(instead of their respective nearest bases maintains a table in the form &lt;time t, plane p, x, y&gt;, where x,y with parameters qrlen and qtlen distinct planes satisfying the spatial and temporal conditions. For comparison, we index the table using a 3D R*-tree on the columns time, x, and y. Given a query, this tree facilitates the retrieval of all qualifying tuples, after which a post-processing step is performed to obtain the Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE number of distinct planes \(in the sequel, we refer to this method as 3DR method introduces a regular res  res grid of the data space, where the resolution res is a parameter. We adopt 16 sketches because, as mentioned earlier, this number gives the best overall performance Figure 5.5 compares the sizes of the resulting sketch indexes \(obtained with resolutions res=25, 50, 100 the database size. In all cases, we achieve high compression rate \(e.g., the rate is 25% for res=25 evaluate the query efficiency, we first set the resolution to the median value 50, and use the sketch index to answer workloads with various qrlen \(qtlen=10 


workloads with various qrlen \(qtlen=10 size \(mega bytes database size 0 20 40 60 80 100 120 140 160 25 50 100 resolution Figure 5.5: Size reduction Figure 5.6a shows the query costs \(together with the error in each case method. The sketch index is faster than 3DR by an order of magnitude \(note that the vertical axis is in logarithmic scale around 15% error observations using workloads with different qtlen Finally, we examine the effect of resolution res using a workload with qrlen=0.15 and qtlen=10. As shown in Figure 5.6c, larger res incurs higher query overhead, but improves the estimation accuracy Summary: The proposed sketch method can be used to efficiently approximate spatio-temporal data for aggregate processing. It consumes significantly smaller space, and answers a query almost in real-time with low error 3D Rsketch number of disk accesses query rectangle length 1 10 100 1k 10k 0.05 0.1 0.15 0.2 0.25 16 14% 15 15% 13 relative error number of disk accesses query interval length 1 10 100 1k 10k 1 5 10 15 20 16 15% 15% 12% 11 relative error a qtlen=10, res=25 b qrlen=0.15, res=25 0 500 1000 1500 2000 2500 25 50 100 number of disk accesses resolution 20% 15% 14 relative error c qrlen=0.15, qtlen=10 


c qrlen=0.15, qtlen=10 Figure 5.6: Query efficiency \(costs and error 5.3 Mining association rules To evaluate the proposed algorithm for mining spatiotemporal association rules, we first artificially formulate 1000 association rules in the form \(r1,T,90 with 90% confidence i randomly picked from 10k ones, \(ii in at most one rule, and \(iii Then, at each of the following 100 timestamps, we assign 100k objects to the 10k regions following these rules. We execute our algorithms \(using 16 sketches these rules, and measure \(i  correct  rules divided by the total number of discovered rules, and \(ii successfully mined Figures 5.7a and 5.7b illustrate the precision and recall as a function of T respectively. Our algorithm has good precision \(close to 90 majority of the rules discovered are correct. The recall however, is relatively low for short T, but gradually increases \(90% for T=25 evaluated in the previous sections, the estimation error decreases as the query result becomes larger \(i.e., the case for higher T 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 precision HT 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 recall HT a b Figure 5.7: Efficiency of the mining algorithm Summary: The preliminary results justify the usefulness of our mining algorithm, whose efficiency improves as T increases Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE 6. Conclusions While efficient aggregation is the objective of most spatio-temporal applications in practice, the existing solutions either incur prohibitive space consumption and query time, or are not able to return useful aggregate results due to the distinct counting problem. In this paper we propose the sketch index that integrates traditional approximate counting techniques with spatio-temporal indexes. Sketch indexes use a highly optimized query algorithm resulting in both smaller database size and faster query time. Our experiments show that while a sketch index consumes only a fraction of the space required for a conventional database, it can process 


required for a conventional database, it can process queries an order of magnitude faster with average relative error less than 15 While we chose to use FM sketches, our methodology can leverage any sketches allowing union operations Comparing the efficiency of different sketches constitutes a direction for future work, as well as further investigation of more sophisticated algorithms for mining association rules. For example, heuristics similar to those used for searching sketch indexes may be applied to improve the brute-force implementation ACKNOWLEDGEMENTS Yufei Tao and Dimitris Papadias were supported by grant HKUST 6197/02E from Hong Kong RGC. George Kollios, Jeffrey Considine and were Feifei Li supported by NSF CAREER IIS-0133825 and NSF IIS-0308213 grants References BKSS90] Beckmann, N., Kriegel, H., Schneider, R Seeger, B. The R*-tree: An Efficient and Robust Access Method for Points and Rectangles. SIGMOD, 1990 CDD+01] Chaudhuri, S., Das, G., Datar, M., Motwani R., Narasayya, V. Overcoming Limitations of Sampling for Aggregation Queries. ICDE 2001 CLKB04] Jeffrey Considine, Feifei Li, George Kollios John Byers. Approximate aggregation techniques for sensor databases. ICDE, 2004 CR94] Chen, C., Roussopoulos, N. Adaptive Selectivity Estimation Using Query Feedback. SIGMOD, 1994 FM85] Flajolet, P., Martin, G. Probabilistic Counting Algorithms for Data Base Applications JCSS, 32\(2 G84] Guttman, A. R-Trees: A Dynamic Index Structure for Spatial Searching. SIGMOD 1984 GAA03] Govindarajan, S., Agarwal, P., Arge, L. CRBTree: An Efficient Indexing Scheme for Range Aggregate Queries. ICDT, 2003 GGR03] Ganguly, S., Garofalakis, M., Rastogi, R Processing Set Expressions Over Continuous Update Streams. SIGMOD, 2003 HHW97] Hellerstein, J., Haas, P., Wang, H. Online Aggregation. SIGMOD, 1997 JL99] Jurgens, M., Lenz, H. PISA: Performance Models for Index Structures with and without Aggregated Data. SSDBM, 1999 LM01] Lazaridis, I., Mehrotra, S. Progressive Approximate Aggregate Queries with a Multi-Resolution Tree Structure. SIGMOD 2001 PGF02] Palmer, C., Gibbons, P., Faloutsos, C. ANF A Fast and Scalable Tool for Data Mining in Massive Graphs. SIGKDD, 2002 PKZT01] Papadias,  D., Kalnis, P.,  Zhang, J., Tao, Y Efficient OLAP Operations in Spatial Data Warehouses. SSTD, 2001 PTKZ02] Papadias, D., Tao, Y., Kalnis, P., Zhang, J Indexing Spatio-Temporal Data Warehouses ICDE, 2002 SJLL00] Saltenis, S., Jensen, C., Leutenegger, S Lopez, M.A. Indexing the Positions of Continuously Moving Objects. SIGMOD 2000 SRF87] Sellis, T., Roussopoulos, N., Faloutsos, C The R+-tree: A Dynamic Index for MultiDimensional Objects. VLDB, 1987 TGIK02] Thaper, N., Guha, S., Indyk, P., Koudas, N Dynamic Multidimensional Histograms 


SIGMOD, 2002 Tiger] www.census.gov/geo/www/tiger TPS03] Tao, Y., Papadias, D., Sun, J. The TPR*Tree: An Optimized Spatio-Temporal Access Method for Predictive Queries. VLDB, 2003 TPZ02] Tao, Y., Papadias, D., Zhang, J. Aggregate Processing of Planar Points. EDBT, 2002 TSP03] Tao, Y., Sun, J., Papadias, D. Analysis of Predictive Spatio-Temporal Queries. TODS 28\(4 ZMT+01] Zhang, D., Markowetz, A., Tsotras, V Gunopulos, D., Seeger, B. Efficient Computation of Temporal Aggregates with Range Predicates. PODS, 2001 ZTG02] Zhang, D., Tsotras, V., Gunopulos, D Efficient Aggregation over Objects with Extent PODS, 2002 Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE pre></body></html 


