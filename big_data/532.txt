Using Memcached to Promote Read Throughput in Massive Small-File Storage System Chuncong Xu 1 Xiaomeng Huang 2 Nuo Wu 3 Pengzhi Xu 4 Guangwen Yang 5 Department of Computer Science and Technology, Tsinghua University Tsinghua National Laboratory for Information Science and Technology Beijing, China 1, 2, 3 xcc07 xpz03, wun08}@mails.tsinghua.edu.cn 4, 5 huangxiaomeng, ygw}@tsinghua.edu.cn Abstract  Because of the bottleneck of disk I/O, the distributed file system based on disk is limited in the performance on data throughput and latency. It is a big challenge 
for such a system to meet the high performance requirement of the massive small-file storage Cache has been widely used in storage system to improve the data access performance. In order to support the storage of massive small files, we have integrated memcached into our distributed file system to optimize the storage of massive small files. However eviction problem arose from LRU replacement algorithm in memcached. It means that the non-stale objects might be replaced due to large short-lived objects. Therefore, we proposed Prioritized Cache \(PC\nd Prioritized Cache Management \(PCM\ solve the problem. The cache of memcached is reorganized and classified into permanent cache 
and temporary cache. Furthermore, in order to alleviate side effects on hit rate in sequential access, temporary cache is partitioned into different parts with different priorities and managed according to the priorities. We have implemented and evaluated the integrated prototype system. The experimental results show that the improved distributed file system with distributed object cache can deliver high performance on smallfile storage. Compared with the original system, the read of small files increased by a factor of 2.65   8.05, without write performance degradation Keywords  storage, file system, prioritized cache, Carrier 
memcached, replacement policy I I NTRODUCTION CPU performance has roughly doubled every 18 months for the past decades, while disk performance has increased very little. As a result, the growing performance gap between CPU and disk makes access latency of disk the bottleneck of datacenter. Disk capacity has increased more than 10000-fold over the last 25 years and seems likely to continue increasing in the future. Unfortunately, the maximum transfer rate for large blocks has improved only 50-fold, and access latency has only improved by a factor of two. As multi-core technology develops rapidly, the gap between processor and disk is still 
widening In order to make up the deficiency of disk performance, most datacenter improve system throughput by adding storage devices and using parallel I/O to deliver high aggregated throughput. It works well for massive large files storage, but not for small-file storage. When most objects are small files, performance decline very fast because of the long seek time and rotational latency. So, the design of parallel disk I/O canêt meet the re quirement of massive small-file storage system The problem of massive small files storage is becoming increasing prominent in some fields, such as large scale web application and short message storage. In such applications, the 
typical file size ranges from KB to hundreds of KB Currently all these small files are storage in database based on disk Generally, Web 2.0 sites use relational database system for the data storage and management, from which application servers read data and pass it to browsers The size of data object in web site is small, but the total number of objects expands very fast It results in a high burden of disk I/O, slows down access rate dramatically and could even make the whole system break down. It is mainly due to the long seek time and prepare time of disk. Objects in Cell phone message storage system are short message, multimedia message and calling record. Most of them are small file and the scale of system is huge \(80TB data 
produced in every six months\. In accordance with regulations these objects must be stored for se arch and analysis. Search is a time consuming operation because of the high access latency for massive small data objects. In the current solution, data objects are translated into structured datasets and inserted into database. Further analysis and queries are carried out based on database. This method cannot support real-time service, so it only can provide a limited query result Traditionally, it is an effective way to improve the throughput and access latency using cache. Memcached is a distributed memory object cache system, which has a simple and scalable architecture. It is usually used in database systems 
and web applications. Based on Carrier, which is the distributed file system developed by Tsinghua University, we designed a scalable distributed cache system with unified namespace to boost the access performance of small files. We modified the fuse interface to make it compatible to the programming interface of libmemcached and Carrier Due to its cache block replacement policy, memcached may kick the short lived objects out of the cache system, even though they are not stale. This is well known as the eviction problem. In order to solve this problem, we classified the 
2010 Ninth International Conference on Grid and Cloud Computing 978-0-7695-4313-0/10 $26.00 © 2010 IEEE DOI 10.1109/GCC.2010.18 24 


distributed Memcached system into two categories, permanent cache and temporary cache. Furthermore, we assign different priorities to different areas of temporary cache. The permanent data and temporary data will be written into permanent cache and temporary cache respectively. We use a threshold value to represent the access frequency in temporary cache. If a low priority temporary cache object s access frequency reaches th e threshold, it will be moved into high priority temporary cache We have performed performance evaluations of our cache system thoroughly. The read performance for small files increased by 2.65  8.05 times, while the write performance remained unaffected The rest of paper is structured as follow. Section 2 introduces buffer cache and block replacement policy. Section 3 presents the design of prioritized cache, while Section 4 demonstrates system implementation in detail. Finally, we evaluate our system performance in Section 5 and summarize our contributions in Section 6 II R ELATED W ORK The cache mechanism is the us ual method to improve the system performance. In this paper, we first introduced the deficiency of buffer cache and block replacement policy, we also pointed out their advantages and disadvantages. We then introduced the architecture of the Carrier distributed file system A Buffer Cache There are three types of cache approaches mainly used in Distributed file system: independent cache, collaborative cache and multi-layer cache GPFS[1  and S p ri te  2  f i l e sy stem u s e independent cache, every node has private cache space and can reach high access rate. However, if every client has to read the data from the same server, the network load of the server could be much higher than others. Furthermore the overloaded server could become the bottleneck of the general file system In order to eliminate the bottleneck, Lustre[3  ado p t e d collaborative cache. It takes advantage of the distributed cache space in every client and makes up a collaborative cache space which is globally coherent. Multi Layer cache has more than one cache layer between storage system and the client, such as Avere[4 an d m e m c ached  A v er e u s es SSD as a ca che lay er f o r  NFS[5 and  bu il ds u p  th e dis t ri bu te d f ile sy stem base d F l a s h   Memcached is usually used to be a cache system for database management system In this paper, memcached is used to be the important cache layer of the multi-layer cache for the distributed file system Therefore, the system I/O performance can be improved by the high performance in-memory operations in memcached B Block Replacement Policy LRU is a classical and wid ely used cache replacement algorithm. It is easy to implement with little overhead. It works well in the scenarios with the high locality of data access pattern. However, it is not fit for the pattern of sequential access. If the size of the sequen tial accessed data is larger than the cache size, LRU dose not work at all The simple replacement policy of LRU does meet the requirements of the diversity of data access patterns. In distributed file systems, it is common to work with multiple access patterns, such as random and sequential. Therefore the cache space of the file system has to be designed to support multiple data access patterns. If we use the LRU policy, the highly accessed objects might be kicked out of the cache, while replaced by the lowly accessed ones LFU \(Least Frequently Used is another classical algorithm of cache replacement policy. LRU kicks the lowly accessed objects out of the cache. There is usually a counter associated with every object in the cache. We increase the value of the associated counter by one when the cache hits. If the cached object is stale, the lowly accessed objects will be replaced by the newer objects. If the value of the counter of one object is equal with another, the LRU is used. It is obvious that if the counter of cached objects reaches a high value the object will remain in the cache space even if it is stale. As a result, the efficiency of the overall cache will decline Therefore, we must take into consideration both factors of locality and access patterns when implementing ca che and replacement algorithm Other replacement algorithms, such as 2Q[6 MQ 7    ARC[8 and  L I RS  9    f o cus on th e tim e l o c ality  I n or de r  t o  achieve high performance in va rious environments, some work has been done with the consideration of both data locality and time locality, such as DULO[1 an d W O W  1 1   Besi des   ot h e r  researches on two levels of cache have been carried out to improve the performance. Based on two-level cache, we can make better predictions of the accessed data, and cut down the page fault rate. With the help of these algorithms, the hit rate can be improved, but the eviction problem is inevitable Therefore, we proposed Prioritized Cache \(PC\ and Prioritized Cache Management PCM\ to solve the problem The experimental results show that the Prioritized Cache Management can deliver high performance on small-file storage III P RIORITIZED C ACHE D ESIGN A Architecture of Carrier with Memcahced Memcached is a distributed memory object cache system In order to accelerate dynamic web application, memcached is usually used to be a cache system for database management system. It is an in-memory key-value store for small objects accessed frequently. Memcached has two significant features One is that it is a distributed cache system based memory. The other is that it can highly reduce memory fragmentation and avoid the adverse effects of system performance by using simple, high efficient memory space management algorithm Memcached uses slab-chunk memory space allocation algorithm and LRU repl acement policy in slab. The allocation algorithm can short the time of the allocation operation, reduce the consumption of the functions and lower the probability  of the memory fragment However, memcached cannot guarantee the reliability of the system and the data, because of the lack of the mechanism of error tolerance and authentication. For this reason, it is not feasible to use memcached as storage system directly. We build the data storage system by integrating memcached with carrier which can achieve high access rate of small files to benefit from the performance dominance of memcached 
25 


  Memcached  File Interface Distributed Storage System   Fuse    Read Cache  permanent cache low priority temporary cache    temporary cache high priority temporary cache The architecture of this design is showed in figure 1. The distributed file system is based on disk which provides the persistent storage The file interfaces are supported with the help of FUSE framework. In this way, it works as the local file system in the view of end users. As the read cache of distributed file system, memcached can improve the read performance on small files by caching the objects whose size is less than 1 MB Figure 1 Architecture of  Read Cache for Distributed Storage System B Eviction Problem The eviction variable in memcached is the number of old items replaced out of the cache. The old items will be replaced by news items. The variable can be used to decide whether to install more memory. There are two cases that eviction problems will occur because of the simple of LRU algorithm memcached. 1\ Memcache uses slab-chunk as the memory allocation strategy and LRU based on internal slab. If there is no remaining chunk in a slab, the stale chunk in a slab will be kicked out even if there are chunks in other slabs, 2\ The LRU algorithm might kick out of objects which are not stale Memcached uses Lazy Expiration strategy. It does not keep track of the expiration of the object in memcached but get time stamp when view records and inspection records are expired The expired data which has not been explicitly called still takes up space. Using MD5 value instead of random string as the key to reference to a fixed space, it can avoid the possible waste of space. The LRU algorithm might kick the non-stale objects out due to large short-lived objects The simple replacement strategy of LRU does not meet the needs of the diversity of data access patterns. It is very common that there are multiple data access patterns in distributed file system, like random and sequential. These patterns will affect the cache space of the system. LRU will kick the least recently used objects out of the cache. This policy does not support the sequential access pattern well. When the total size of accessed data is larger than the cache size, LRU will not work at all Using LFU will lower the efficiency of cache. LFU kicks the objects which have the lowest accessed frequency out of the cache system. There is a counter associated with each cached objects. If cache fails, the counter increases by 1 once every hit happens. The objects whose counter is lowest will be replaced If two cached objects have the same counter value, LRU will be used. When the cached objects reach a high counter value they will not be replaced even if there are stale. These objects will occupy the cache space which lead to bad cache performance. This is the disadvantages of LFU C Design of Prioritized Cache Due to the eviction problem, when using memcached to build a cache space, we classified the cache space of memcached into permanent cache space and temporary cache space taking both locality and accessing patterns into consideration. Further, we classified temporary space two categories: high priority temporary cache and low priority temporary cache. See Figure 2 for details Figure 2 Division of prioritized cache space Permanent cache is used to storage permanent objects Temporary cache is used to store temporary objects, which solves the kick out problems of permanent objects. Permanent object will be stored into perm anent cache space and temporary objects will be stored into temporary cache space. We use LRU to implement permanent cache, low priority temporary cache and high priority temporary cache. In this way, we avoid the problem that permanent objects may be kicked out due to shorted-lived objects We classify the temporary cache space into low and high temporary spaces in order to avoid the kicking out of highly accessed objects in the sequential access pattern. We use a threshold value to represent the access frequency in temporary cache. If the access frequency of a low priority temporary cache object reaches the threshold, it will be moved into high priority temporary cache. The threshold is decided by analyzing the user access behavior and can be adjusted dynamically. Only those objects which have a high access frequency can be moved into high priority cache. The sequential accessed objects can only be moved into low priority cache, which makes the objects in the high priority cache immune from sequential accessed objects IV I MPLEMENTATION Based on Carrier developed by Tsinghua University memcached and its libmemcached interface, we modified the Fuse development library and built a cache for Carrier. The 
26 


 Data Server data string  Data Server Data Server Meta Server Backup Client control string memcached dataservers metaservers  Data Server Carrier Fuse Data Server   permanent cache low priority temporary cache high priority temporary cache  cache system contains prioritized cache module, replacement algorithm and Fuse interface, etc A Carrier We designed Carrier according to multiple distributed file systems[1, 12  17  A s w e can s e e in f i g u re 3  Ca rr ie r is  composed of metaservers, dataservers and clients, which provides POSIX compatible interface. The metadata takes charge of the meta-information of all files, including namespace[18  acc ess c o n t ro l in f o rm ati on  1 9  th e m a p information between file to data chunks and the location of each chunk. Metaserver is also responsible for the system level management, including the allocation and management of chunks and the management of different replica chunks on various dataservers. The reliability of metadata is guaranteed by a hot standby metaserver which always keep consistent with the main metaserver. We use local Linux file system to storage and access chunk data on dataservers. Dataservers split file into chunks of the same size. The default size 32M, which can be set according to actual needs. The client of Carrier is the interface of the distributed file system which provides methods like open, read, write, close, list, delete, mkdir, etc. The client will communicate with both metaserver and data server, and access data stored in Carrier on behalf of user applications After requiring the metadata from metaserver, the client asks the dataservers for chunks directly, which freed the metaserver from over loaded We modified the fuse development library, which gives the user an easy way to use. It is implemented in the user space thus there is no need to modify the Linux kernel. The Carrier uses Fuse[20 d e vel o pm ent li b r a r y to m a p  r e m o te di re ct or ies t o  local directories. After that, common applications can access data in Carrier like accessing local file  transparently B Memcached Integration In order to improve the read and write performance on small files, we optimized Carrier by using memcached as the cache of Carrier Figure 3 Architecture of Carrier with Memcached Memcached do not communicate with Carrier, instead it communicate with Fuse directly, which greatly simplifies the design. In this way, we do not need to transmit control information and data between memcached and Carrier and do not need to change the Carrier's reading and writing process System determines the size of the file cache to memcached based on the size of the documents on demand We use libmemcached application program interface Interaction with the memcached is achieved by Fuse, we changed the Fuse Development Library to meet the libmemcached programming interface. Files cached in memcached are stored as key-values. The MD5 value of the full path of the file is stored as the key and the related file is stored as the value. Instead of random string, MD5 value is used as the key to reference to a fixed space. In this way, the potential space waste problem can be avoided When reading a file whose size is larger than 1MB, it will be accessed from Carrier directly. If the size is smaller than 1MB, we will search the cache space according to the MD5 value of the file path. If successful, we will fetch the file from cache space and return it to the user. If failed, we will read the file from Carrier and put it into memcached. Depending on expiration, we can decide to put the file in whether permanent cache space or temporary cache space. When writing a file into Carrier, we write it into Carrier directly. If the file size is smaller than 1MB, we carry out a search operation in the memcached The file will be marked as stale, if it can be found in memcached C Prioritized Cache We embedded a counter in the client of memcached. If the cache is hit, we increase this counter by one. If we found that the counter of the object is greater than the threshold value, we move the object from low priority temporary space into high priority temporary space We decide the threshold value based on the temporary cache hit rate and the total cache size. If the value of threshold is set too high, the number of objects moved into high priority temporary space will decrease. As a result, the efficiency of cache space declines If the valu e of threshold is set too low too many objects will be moved into high priority temporary cache, taking the place of the highly accessed data in high priority temporary space The hit rate of temporary cache is also closely related to the total cache size, the cache space division, access patterns, the size of the object, etc. We also set the threshold value according to those factors. It is reasonable to adjust the threshold value dynamically. We make our best efforts to get the optimized threshold value. In this paper, we set the threshold value based on the experiments we made on carrier and the related trace we collected D Fuse interface Memcached does not implement the persistent storage of cached objects, thus, we do not use memcache as the write cache of Carrier. We set a writing buffer of size 512KB in Fuse to improve the performance of non-sequential writing. We write data into local writing buffer of Fuse first. When the 
27 


000\023 000\024\000\023 000\025\000\023 000\026\000\023 000\027\000\023 000\030\000\023 000\031\000\023 000\032\000\023 000\033\000\023 000\024\0000 000\030\000\024\000\025\000 000\025\000\030\000\031\000 000\024\000\025\000\033\000 000\031\000\027\000 000\026\000\025\000 000 000+\000L\000W\000\003\0005\000D\000W\000H\000\013\000\010\000\014 000$\000F\000F\000H\000V\000V\000\003\000 000$\000F\000F\000H\000V\000V\000\003\000 000$\000F\000F\000H\000V\000V\000\003\000 000$\000F\000F\000H\000V\000V\000\003\000 000$\000F\000F\000H\000V\000V\000\003\000 000\023 000\030 000\024\000\023 000\024\000\030 000\025\000\023 000\025\000\030 000\026\000\023 000\026\000\030 000\027\000\023 000\027\000\030 000\024\000 000\030\000\024\000\025\0000 000\025\000\030\000\031\0000 000\024\000\025\000\033\0000 000\031\000\027\0000 000\026\000\025\0000 000\024\000\031\0000 000\033\0000 000\027\0000 000\025\0000 000 0007\000K\000U\000R\000X\000J\000K\000S\000X\000W\000\013\0000\000%\000S\000V\000\014 000&\000D\000U\000U\000L\000H\000U\000\020\0000\000H\000P\000F\000D\000F\000K\000H\000G 000&\000D\000U\000U\000L\000H\000U buffer is full or the writing operation is completed, we write the cached data into Carrier V E VALUATION We have evaluated the caching performance of the Carrier in the same cluster by using Carrier memcached, and the evaluations are based on the same hardware and software environments. For ease of descri ption, in this section we will use Carrier-Memcached instead of Carrier A Experimental Design Experimental cluster is made up of a front end and 16 nodes The front end runs as the client, the node1 and node2 run as metaservers, node3 to node12 run as dataservers, node13 to node16 run as memcached. Each node is of the same configuration, and the details are listed in Table 1 TABLE I T EST CLUSTER CONFIGURATION Type configuration H ardware Xeon 1.6GHz 4 core 4G Memory 160GB Disk O perating System Ubuntu 8.04 Server Software Platform Erlang 5.5.5 virtual machine N etwork Gbit Ethernet We evaluate the data access performance and hit rate of the system. Write and read performance of different file sizes and different number of files is calculated as Time  Access  Total  The Volume  Access  Data Speed  Access  Data 002 Read hit rate refers to the random read hit ratio of memcached, which is calculated as Files  Total  of Number Files Hit  of Number  Rate Hit  Read 002 We have established a 512MB space in the memcached cache, including a 128MB pe rmanent cache, a 256MB low priority temporary cache and a 128MB high priority temporary cache. The access frequency threshold value is set to 36. The average system throughput is ev aluated using multiple datasets with the same aggregated file size of 2GB The dataset is made up of multiple equal files. In each dataset, the size of individual file ranges from 32KB to 1GB. Both Carrier and CarrierMemcached have been evaluated based on the abovementioned experiments. Random read and write tests have been taken 10 times and the throughput is the average. The read hit rate ratio is evaluated usin g the files less than 1MB B Random read hit rate Hit rate plays an important role in improving read performance. It depends on cache capacity, cache space division, access method, object size and other related factors As a result prioritized cache, access frequency threshold value will also impact the hit rate. For the different sizes of small files ranging from 32KB to 1MB, the relationship between random read hit rate and access frequency threshold value in Carrier-Memcached is shown in Figure 4 Figure 4 Hit Rate for Carrier-Memcached With the decreasing of file size, the read hit rate declines gradually. Because the average vi sit frequency will decrease as the number of documents increasing in the same dataset. For the same access frequency threshold value, the decreasing of the average file access frequency will make the replacement of low priority temporary cache more frequently. So the read hit rate is smaller compared with large files For the dataset with the same file size, the access frequency threshold value will make the read hit rate decrease if it is set too low or too high. As shown in Figure 4, the read hit rate is the highest when the access frequency threshold is set to 36 The access frequency threshold value will be set after cache capacity, cache space division, access method and the object size and other factors determined according to tests C Random read and write throughput For the files larger than 1MB, the read performance of Carrier-Memcached and Carrier is consistent. The test results are shown in Figure 5. This is mainly because the file size in Memcached cache is no larger than 1MB. When the file is less than 1MB, read in Carrier-Memcached is faster than Carrier The test results are shown in Figure 6. As can be seen from Figure 6, the smaller the file size, the better performance Carrier-Memcached achieved compared with Carrier. When the file size is 32KB, the reading speed in Carrier-Memcached and Carrier are 14.596MB/s and 3.679MB/s respectively Figure 5 Read Throughput of Large Files 
28 


000\023 000\024\000\023 000\025\000\023 000\026\000\023 000\027\000\023 000\030\000\023 000\031\000\023 000\032\000\023 000\024\0000 000\030\000\024\000\025\000 000\025\000\030\000\031\000 000\024\000\025\000\033\000 000\031\000\027\000 000\026\000\025\000 000 0007\000K\000U\000R\000X\000J\000K\000S\000X\000W\000\013\0000\000%\000S\000V\000\014 000&\000D\000U\000U\000L\000H\000U\000\020\0000\000H\000P\000F\000D\000F\000K\000H\000G 000&\000D\000U\000U\000L\000H\000U 000\023 000\030 000\024\000\023 000\024\000\030 000\025\000\023 000\025\000\030 000\024\000 000\024\000\025\000\033\0000 000\024\000\031\0000 000\025\0000 000\025\000\030\000\031\000 000\026\000\025\000 000 0007\000K\000U\000R\000X\000J\000K\000S\000X\000W\000\013\0000\000%\000S\000V\000\014 000&\000D\000U\000U\000L\000H\000U\000\020\0000\000H\000P\000F\000D\000F\000K\000H\000G 000&\000D\000U\000U\000L\000H\000U The write performance of Carrier-Memcached and Carrier shows no difference. The results are shown in Figure 7. The performance dropped down dramatically when the file size is less than 1MB.  As we can see, the write performance of Carrier-Memcached is independent from cache Figure 6 Read Throughput of Small Files Figure 7 Write Throughput in Carrier-Memcached and Carrier VI C ONCLUSION A ND F UTURE W ORK We proposed a method of memcached buffer space management based on prioritized cache. And we integrated the memcached into the distributed cache as part of a distributed storage system, which achieved high performance for small file reading. From the experiment, the performance of small files reading for Carrier file system has been improved 2.65 to 8.05 times by using memcached as a distributed storage system cache, while the write performance is independent from cache Many work remains to be carried out in the future. We will focus on three aspects. 1\ Files larger than 1MB should be cached. One of the proposed method s is to split the file into objects with fixed size, and store them in a memcached cache space 2\The memcached can be used as a distributed storage system write cache. Therefore, we need to address data consistency problems in the memcached cache space and Carrier 3\ We need to design a common interface of memcached to make it available for other distributed file systems A CKNOWLEDGMENT The research proposed in this paper is part of work supported by ChinaGrid project of Ministry of Education of China, Natural Science Foundation of China \(60803121 60773145, 90812001, 60963005\ National High Technology Development Program of China \(2010AA012401 R EFERENCES 1 F ra n k Sc h m u c k  R oge r Ha s k in  G P FS: A Sh a r e d Di s k F i le S y st e m for Large Computing Clusters. In Proceedings of the 2002 Conference on File and Storage Technologies \(FAST\.pp: 231  244, 2002 2 J.Oute rh o u t A  Ch e r e n so n  F  D o u g l i s M  N e l s o n a n d B  W e l c h  T h e  Sprite network operating system. IEEE Computer, 21\(2\ :23-36 February 1988 3 L us tr e  htt p  w w w  l us tr e  o r g  20 0 9  4 T h e Ave r e A r c h i t e c t u r e f o r Ti er ed NA S   5 B P a w l ow ski, C  J u sz cz ak P  S t a u b a ch  C. S m it h,D  L e bel  an d D  H itz   NFS version 3: Design and implementation.In Proceedings of the Summer 1994 USENIXTechnical Conference, pages 137  151, 1994 6 T. Johnson and D. Shasha, ç2Q: A Low Overhead High Performance Buffer Management Replacement Algorithm Proc. of VLDB ê94, 1994 pp. 439-450 7 Y. Zhou, Z. Chen and K. Li. çSecond Level Buffer Cache Management IEEE Transactions on Parallel and Distributed Systems, Vol. 15, No. 7 July, 2004 8 N. Megiddo, D. Modha, çARC: A Self Tuning, Low Overhead Replacement Cacheé, Proc. of FAST ê03, March 2003 pp. 115-130 9 S. Jiang and X. Zhang, çLIRS: An Efficient Low Interreference Recency Set Replacement Policy to Improve Buffer Cache Performanceé, Proc. of SIGMETRICS ê02, June 2002  S  J i an g X. Din g F  Ch en E Tan  and X  Zh a n g 003\014 DULO: an effective buffer cache management scheme to exploit both temporal and spatial localities 003\015  Proceedings of the 4th USENIX Conference on File and Storage Technologies \(FAST 003\011 05\ San Fransisco, CA,December 2005 pp. 101-114  B  G i ll a nd D S M o dha WOW: Wise Ordering for Writes Combining Spatial and Temporal Locality in Non-Volatile Caches Proc. USENIX Conf. File and Storage Technologies \(FAST\, Dec. 2005  Ni ls Ni eu w e j a a r  Da vid K o t z  Th e Ga lle y Pa ra l l e l  F i le S y s t em  Pa ra lle l Computing. pp: 374  381, 1996 13 Jame s V  H u b e r a n d Jr  an d  Ch r i sto p h e r L  El fo rd an d D a n i e l A  Re e d  and Andrew A. Chien and David S. Blumenthal. PPFS: A High Performance Portable Parallel File System. In Proceedings of the 9th ACM International Conference on Supercomputing. pp: 385  394, 1995  St e v e n M oye r V S Su nd e r a m. PI O U S A Sc a l a b le Pa ra ll e l  I  O S y st e m  for Distributed Computing Environments. In Proceedings of the Scalable High-Performance Computing Conference. pp: 71  78, 1994 15 G  A  Gi b s o n  D  F N a g l e   K  Am i r i   A c o s t e f f e c t i v e  h i g h b a n d w i d t h  storage architecture. In Proceedings of the 8th International Conference on Architectural Support for Programming Languages and Operating Systems \(ASPLOS\ages 92  Acknowledgments 103, San Jose, CA Oct. 1998 16 X iao na n M a a n d A  L  N a r a s i m ha  Re ddy MV S S  a n A c tiv e S to r ag e  Architecture. IEEE Transactions On Parallel and Distributed Systems Vol.14. 2001 17 P r as ha nt S h e n o y P a w a n G o y a ly  H a r r ic k  M V i nz A r chi te c t u r a l  Considerations for Next Generation File Systems. In Proceedings of the Seventh ACM Multimedia Conference. 1999  Z h en g Z h a n g, Ch ri s t os K a ra m a n o li s   Des i gn i n g a  R obu s t Na m e s p a c e for  Distributed File Services. In Proceedings of the 20th Symposium on Reliable Distributed Systems. 2001 19 D a v i d A  A ppl e g ate  G r uia Cal i n e s c u y  D a v i d S  J o hns o n z  H o w a r d  Karloffx, Katrina Ligett, Jia Wangk. Compressing rectilinear pictures and minimizing access control lists. In Proceedings of the Proceedings of ACM-SIAM Symposium on Discrete Algorithms\(SODA\ 2007  F i l e s y s t em i n Us er s p ac e  ht tp   f u s e sou r c e f o r g e n e t   
29 


7  SUMMARY AND CONCLUSIONS In this paper an application of transient CFD modeling for raised floor data centers is presented. Power and CRAC airflow supply were varied with time using a general profile. Different scenarios were investigated which included varying power alone, varying CRAC airflow alone or varying both together  Case study I Interesting behavior was observed when varying power alone where it was determined that buoyancy effects were significant in the variation of inlet temperatures with time Case study II When varying CRAC airflow alone, fluctuations in temperatures occurred indicating the changing airflow pattern in the room with time Recirculation zones were seen diffusing in and out of the inlet flow to the racks with time Case study III As both rack power and CRAC airflow were varied together it is clear that the inlet temperatures vary sooner due to the air flow rather than the power This can be explained by the fact that the airflow is directly applied to the cold aisle, where the inlet temperature values are measured. In the other case, power depends on top and edge recirculation to affect the inlet temperatures and therefore has a slower effect Case study IV  V  Varying power causes no fluctuations in inlet temperatures as the airflow dynamics in the room do not change significantly The presence of a lag between CRAC airflow and power presents two situations if CRAC airflow is lagging unacceptable operating temperatures are reached On the other hand if power is lagging redundant cooling is applied  Future studies will include accounting for the thermal mass of computer racks Also applying a variable rack fan with time will further improve the practicality of the transient model compared to a real life data center facility  REFERENCES 1  A S H R A E  P u b lic a tio n   2 0 0 5   223 D a ta c o m  E q u ip m e n t P o w e r  Trends and Cooling Applications\224 2  S c h m id t  R   224 T h e r m a l P r o f ile  o f  a  H ig h  D e n s ity  D a ta  C e n te r   Methodology to Thermally Characterize a Data Center\224 ASHRAE Symposium, Nashville, TN, June, 2004, 8 pages 3  S c h m id t R    I y e n g a r  M    B e a ty  D    a n d  S h r iv a s ta v a  S    2 0 0 5   223Thermal Profile of a High Density Data Center 226 Hot Spot Heat Fluxes of 512 Watts/ft 2 224 Proceedings of the ASHRAE A nnual Meeting, Symposium DE-05-11, Denver, June 25 th 29 th   4  S c h m id t R    I y e n g a r  M    a n d  M a y h u g h  S    223 T h e r m a l P r o f ile  of World's 3rd Fastest Supercomputer IBM's ASCI Purple Cluster  Submitted for review for publication in the Annual ASHRAE Summer Conference in Montreal, Canada 5  S c h m id t R    223 E f f e c t o f  D a ta  C e n te r  C h a r a c te r is tic s  o n  D a ta  Processing Equipment Inlet Temperatures\224 Advances in Electronic Packaging Proceedings of the Pacific Rim/ASME International Electronic Packaging Technical Conference and Exhibition Interpack Vol 2 Paper IPACK2001-15870 Kauai, Hawaii, July 8-13, pp. 1097-1106 \(2001 6  M ik e  C   a n d  W e a v e r  T    223 D a ta  c e n te r  c o o lin g  226  u s in g  w e tbulb economizers\224, ASHRAE Journal, August 2008, pp. 52-58 7  J u d g e  J    P o u c h e t J    E k b o te  A    a n d  D ix it S    223 R e d u c in g  data center energy consumption\224, ASHRAE Journal, November 2008, pp. 14-26 8  M ik e  C    W e a v e r  T    D u n n a v a n t K    a n d  F is h e r  M    223 R e d u c e  data center cooling cost by 75%\224 ASHRAE Journal April 2009, pp. 34-41 9  S a lim  M    223 E n e r g y  in  d a ta  c e n te r s  b e n c h m a r k in g  a n d  lessons learned\224, ASHRAE Journal, April 2009, pp. 24-32 10  S h a r m a  R    B a s h  C    P a te l C    F r ie d r ic h  R    a n d  C h a s e  J    223Balance of power 226 dynamic thermal management of internet data centers\224 Published by IEEE Computer Society January pp. 42-49, \(2005 11  B h o p te  S    I y e n g a r  M    S a m m a k ia  B    S c h m id t R  a n d  Agonafer D 223Numerical modeling of data center clusters 226 immpact of model complexity\224 Proceedings of the International Mechanical Engineering Congress and Exposition Chicago, Illinois, November \(2006 12  B h o p te  S    S a m m a k ia  B    I y e n g a r  M    S c h m id t R  a n d  Agonafer D., \223Effect of Under-Floor Blockages on Data Center Performance\224 Proceedings of the Inter-Society Conference on Thermal and Thermomechanical Phenomena in Electronic Systems \(ITherm\, San Diego, California, June \(2006 13  B h o p te  S    S a m m a k ia  B    I y e n g a r  M    a n d  S c h m id t R    Guidelines on Managing Under Floor Blockages for Improved Data Center Performance Proceedings of the ASME International Mechanical Engineering Congress IMECE Chicago, November \(2006 14  F lo th e r m  v e r s io n  8  2  d o c u m e n ta tio n   w w w  f lo th e r m  c o m   15  G o n d ip a lli S    I b r a h im  M    B h o p te  S    S a m m a k ia  B    Murray B., Ghose K., Iyengar M., and Schmidt R., \223Numerical modeling of data center with transient boundary condition\224 Proceedings of the Intersociety Conference on Thermal Phenomena \(ITHERM\, Las Vegas, 2010 16  P e r s o n a l C o m m u n ic a tio n    I y e n g a r  M   a n d  S c h m id t R    Advanced Thermal Laboratory IBM Systems and Technology Group, Poughkeepsie, New York, 12601, USA 


follows G opt Q  argmax G Q g  G Q   9 To solve this optimization problem we rst attempt to apply a dynamic programming DP solution similar to the 0-1 knapsack problem by setting the budget b as the total capacity of the knapsack the set of  p st  paths as the set of items  p st  as the item weight and val  p st  as the item value Let PathSet be the set of  p st  paths and  PathSet  be the number of paths in PathSet  The knapsack problem can be solved in O   PathSet  b  time Unlike the knapsack problem b here is small since b is deﬁned as the size of a graph that a human user is able to visualize clearly However there are still two problems that need to be solved First we need to obtain PathSet  Second the nodes on the paths may overlap while the items in the knapsack problem do not share their weights We address each of the problems as follows 1 Path Selection Clearly we cannot use the set of all  p st  paths within a community because the number o f such paths is too large We select a set of high-value paths by the following strategy We start a breath-ﬁrst-search  BFS  from s to collect a set of  p st  paths The BFS does not visit all nodes but only explores a node j from a node i if  V st i  V st j   0 sincewe require val  p st   0  To control the number of paths for each s t pair we stop a new iteration of BFS when the number of distinct nodes on the collected paths is greater than b  Example 8 Consider the graph in Figure 4 If b 6 theset of s tot paths is selected as follows First the second iteration of BFS selects 005 s v 4 t 006  Then the third iteration of BFS selects 005 s v 1 v 4 t 006 and 005 s v 4 v 3 t 006  The fourth iteration of BFS selects 005 s v 1 v 2 v 3 t 006  005 s v 1 v 4 v 3 t 006  005 s v 5 v 6 v 7 t 006  and 005 s v 10 v 11 v 12 t 006  Now the number of distinct nodes is greater than b  thus we stop the BFS path selection and none of the other paths in Figure 4 are selected 002 The BFS strategy has the following three advantages First it controls the length of the paths because we do not prefer long paths due to the limited budget b  Second the paths collected by BFS tend to have common nodes thereby giving a higher val  p st  within a xed b  Third the BFS strategy also collects high-value paths because T  p st  decreases for each node added to the path as reﬂected by Equation 5 2 Dynamic Programming After we select the set of  p st paths PathSet  we can construct a   PathSet  b  table for the DP solution We rst sort PathSet in descending order of the values of the paths to facilitate the DP process since we prefer high-value paths to low-value ones However the fact that paths may share common nodes signiﬁcantly complicates the problem We address this problem as follows Let Tab be the DP table and p x be the x thpathin PathSet  In the 0-1 knapsack setting Tab  x y  gives the optimal solution for the sub-problem with x paths and the capacity y where Tab  x y  is given as follows Tab  x y  MAX  Tab  x  1 y   val  p x  Tab  x  1 y  p x     10 Equation 10 however has a problem If some nodes on p x also appear on some paths processed previously then it is no longer correct to use Tab  x  1 y  p x   to compute Tab  x y   Let PathSet 1   x  1 be the set of paths in PathSet that are processed by DP before we process p x Wemayhave z nodes on p x that are also on some paths in PathSet 1   x  1 where 0 010 z 010 p x   Therefore we need to check Tab  x  1 y    p x  z  for 0 010 z 010 p x  inorder to compute Tab  x y   However we cannot simply take the maximum Tab  x  1 y    p x  z  for some z sincewe need to make sure that there are indeed z nodes being repeated on the set of paths from which Tab  x  1 y    p x  z  is computed Let PathTab  x  1 y    p x  z  be the set of paths from which Tab  x  1 y    p x  z  is computed We compute Tab  x y  as follows Tab  x y  MAX  Tab  x  1 y   val  p x  MAX  Tab  x  1  y    p x  z    11 where 0 010 z 010 p x  and there are at least z nodes on p x that also appear on some paths in PathTab  x  1 y    p x  z   We give the pseudo-code of the DP algorithm in Algorithm 1 The algorithm is self-explanatory except the handling of the common nodes on different paths In Lines 8-17 we nd the number of nodes on p x that also appear on some paths in PathTab  x  1 y    p x  z  as follows Algorithm 1 IntraConnection Input PathSet  b  Output G Q  1 Sort PathSet in descending order of the path values 2 Create Tab with   PathSet  1 rows and  b 1 columns 3 Initialize the rst row Row 0 of Tab to be all 0’s 4 for each x 1   PathSet   do 5 for each y 0 b  do 6 Tab  x y  004 Tab  x  1 y   7 PathTab  x y  004 PathTab  x  1 y   8 Let p x be the x thpathin PathSet  9 for each z 0   p x   do 10 z 003 004 0  11 for each node v on p x do 12 if  PathSet  v  005 PathTab  x  1 y    p x  z  006  007  13 z 003 004 z 003 1  14 break if z 003 010 z  15 if  z 003 010 z and Tab  x y    val  p x  Tab  x  1 y    p x  z    16 Tab  x y  004  val  p x  Tab  x  1 y    p x  z    17 PathTab  x y  004  PathTab  x  1 y    p x  z  011 p x    18 Combine the paths in PathTab   PathSet  b  to give G Q  Let PathSet  v  be the set of paths in PathSet that contain a node v  For each v on p x  we intersect PathSet  v  with PathTab  x  1 y    p x  z   If the intersection is not an empty set then v is repeated on some path in PathTab  x  1 y    p x  z  Weuse z 006 to keep the total number of nodes being repeated If there are z 006 011 z nodes on p x that also appear on some paths in PathTab  x  1 y    p x  z   we update Tab  x y  according to Equation 11 
863 
863 
 


q 3 q 2 q 1 q 3 q 2 q 1 a\Graph c\aph 2 q 3 q 2 q 1 b\Graph 1 Fig 8 Answer Graph for Example 10 Finally G Q is computed by combining the set of path in PathTab   PathSet  b   We show how the algorithm works by the following example Example 9 Consider the graph in Figure 4 and two query nodes s and t Weset b 6 and after sorted PathSet   p 1 p 2 p 3 p 4 p 5 p 6 p 7  as given in Table I Table II gives the value of each Tab  x y  and the set of paths in each PathTab  x y   To save space we omit Columns 0-2 and Row 0 which are all 0’s Recall that PathTab  x y   p i p j  can be regarded as a subgraph consisting of the set of paths  p i p j  where the number of distinct nodes in the subgraph is y  Thus we can construct PathTab 3     p 1 p 3  from PathTab 2     p 1   since all the nodes on p 3  except v 1  appear already on p 1 InRow5 PathTab 4     p 1 p 2  was replaced by PathTab 5     p 1 p 3 p 4 p 5  since Tab 5   is now greater than Tab 4   ForRow6,since p 6 consists of three new nodes and hence we need to update Tab 6   from Tab 5   However  Tab 5    0  006  Tab 5    thus p 6 is not added and Row 6 remains the same as Row 5 For Row 7 there is only one new node on p 7  thus we add p 7 to PathTab 6   to give PathTab 7     p 1 p 3 p 4 p 5 p 7   which is the nal answer 002 TABLE II V ALUES OF Tab AND PathTab IN E XAMPLE 9 3 4 5 6 1 0.267  p 1  0.267  p 1  0.267  p 1  0.267  p 1  2 0.267  p 1  0.267  p 1  0.267  p 1  0.299  p 1 p 2  3 0.267  p 1  0.281  p 1 p 3  0.281  p 1 p 3  0.299  p 1 p 2  4 0.267  p 1  0.281  p 1 p 3  0.295  p 1 p 3 p 4  0.299  p 1 p 2  5 0.267  p 1  0.281  p 1 p 3  0.306  p 1 p 3 p 4 p 5  0.306  p 1 p 3 p 4 p 5  6 0.267  p 1  0.281  p 1 p 3  0.306  p 1 p 3 p 4 p 5  0.306  p 1 p 3 p 4 p 5  7 0.267  p 1  0.281  p 1 p 3  0.306  p 1 p 3 p 4 p 5  0.311  p 1 p 3 p 4 p 5 p 7  Example 9 shows how to nd the answer graph of only two query nodes The following example further illustrates the case when the number of query nodes is more than two Example 10 Figure 8\(a shows a graph with three query nodes Assume that all paths have the same value If we set b  9  we can either output Figure 8\(b or Figure 8\(c Figure 8\(b shows strong pair-wise relationship between any pair of query nodes while Figure 8\(c shows strong relationship among all query nodes and pair-wise connection is included only because the budget still allows Our algorithm gives Figure 8\(c which reveals another advantage of our method that it values strong relationship among all or the majority of query nodes higher than pair-wise relationship 002 We now analyze the time complexity To compute each Tab  x y   we need to check  p x  number of  Tab  x  1 y    p x  z   entries For each entry we need to perform z intersections of PathSet  v  and PathTab  x  1 y    p x  z   Thus we need O   p x  2  PathSet   time to compute each Tab  x y   Note that we can terminate the intersection as soon as we nd one path in both sets Thus in many cases the intersection terminates earlier In total we need to compute b  PathSet  table entries and hence the total running time is O  b  p x  2  PathSet  2  In practice this running time is very small since  p x    PathSet  and b are all small The space required for PathSet and Tab is O  b  PathSet    We also keep PathTab  x  1 y    p x  z  with each Tab  x  1 y    p x  z   but only for the  x  1 th row and x th row of Tab  Thus the extra space needed is at most 2 b  PathSet    In most cases  PathTab  x  1 y    p x  z   is much smaller than  PathSet   However there is a potential problem in the DP solution that G Q may not be connected or may not contain all query nodes since the DP only picks up paths according to their values Since we select paths for all s t pairs 003 s t 004 Q we simply make G Q connected with all query nodes by adding the corresponding path\(s with the highest value Thus this process may output slightly more than b nodes but should not affect clear visualization VI I NTER C OMMUNITY C ONNECTION In Section V we discuss the intra-community connection between a set of query nodes that are in the same community In this section we discuss the inter-community connection for a query that contains nodes from different communities We give the algorithm for computing the inter-community connection between the query nodes in Algorithm 2 Algorithm 2 InterConnection Input H and the query Q  Output The answer graph G Q  1 Let C be the set of all communities that contain at least one query node 2 Let A be the common ancestor of all communities in C  3 Let E com be the set of all community edges of each virtual community on the path from each C i 012 C to A  4 Construct a graph G com  from E com  5 for each pair of communities C i and C j in C do 6 Compute the shortest path between C i and C j in G com  7 Let P be the set of all shortest paths obtained in Steps 5-6 8 Sort P in ascending order of the path length 9 for each path 002 C i C j 003\012 P do 10 if  C i and C j are not yet connected in G Q  11 Find the actual path between C i and C j andadditto G Q  12 if All query nodes in Q are connected 13 Return G Q  We describe Algorithm 2 as follows In Section IV-C we construct the community hierarchy tree H that shows the relationship between the communities For example if two communities are siblings in H  then they are the closest to each other since they are to be joined as one single 
864 
864 
 


community as measured by m odularity Algorithm 2 utilizes this relationship between two communities C i and C j to rst nd the connection between C i and C j at the community level Lines 2-6 then we nd the actual path in G that connects the query nodes in C i to those in C j Line 11 We rst discuss how we nd the connection between C i and C j at the community level Let p  005 C i C j 006 be the simple path that connects C i and C j in H i.e the path from C i to A and that from C j to A in Lines 2-3 of Algorithm 2 Since all nodes on p  except C i and C j  are virtual communities we need to rst convert p into a path of actual communities Recall from Section IV-C that at each virtual community in H  we keep a set of community edges Let E com be the union of the set of community edges at each virtual community on p  From E com  we can construct a graph G com  Note that G com must contain at least one path of actual communities from C i to C j  because C i and C j are reachable to each other in G  Thus the path that connects C i and C j at the community level can be computed as the shortest path from C i to C j in G com  It has been shown in  t h at s hort e s t pat h i s i n adequat e in capturing the relation between query nodes We also do not use shortest path to nd the relation between query nodes within a community However here the context in which we apply shortest path is differen t because the distance between two communities in the community hierarchy tree does show how close they are to each other For this reason the weight of each community edge in G com is deﬁned as the level at which the community edge is kept in the hierarchy tree since the level of the hierarchy tree shows the time the communities are combined The higher the level the greater the edge weight the later are two communities combined and so the further away is their relationship We further illustrate the concept by the following example Example 11 Consider the graph in Figure 2 and the hierarchy tree in Figure 3 For clear illustration let us assume that we obtain the set of actual communities at Level 7 instead of Level 9 that is the set of actual communities is  C 1 C 2 C 6 C 7 C 8   The set of community edges at each virtual community is as follows we have   C 7 C 8   at Level 8   C 6 C 7    C 6 C 8  at Level 9   C 1 C 2   at Level 10 and   C 1 C 6    C 2 C 8   at Level 11 Suppose that the query nodes are in C 1 and C 7  We collect the community edges along the two paths from C 1 and C 7 to their ancestor C 5 and construct G com accordingly as shown in Figure 9 There are four simple paths connecting C 1 and C 7 but the shortest path is 005 C 1 C 6 C 7 006  It can also be seen from Figure 2 that this shortest path can indeed capture the relationship between the two communities 002 C 6 C 8 C 7 C 1 C 2 11 98 9 11 10 Fig 9 G com for Example 11 We now discuss how to nd the actual path to connect two communities C i and C j based on the discovered community path Line 11 of Algorithm 2 Let p  005 C i C x C y C j 006 be the shortest path between two communities C i and C j in G com  We utilize p to nd the actual path connecting the query nodes in C i and C j  A conceptual view of how we apply p is depicted in Figure 10 C i q i u C x v w  C y C j q j Fig 10 A Conceptual View of Inter-Community Connection Recall that each community edge is associated with a set of connecting edges Thus we start from C i and nd the highest-value path from a query node to some u where  u v  is a connecting edge We nd this highest-value path using the BFS strategy in Section V-C but we do not run dynamic programming since we only need to pick one path Then we nd the highest-value path from v to w in C x  This process goes on until we reach C j  where we connect a query node in C j with the highest-value path to a connecting edge from C y  In the same way we nd the actual path from C j to C i because the connection is directi on-aware We then select the path with higher overall value to connect the query nodes in C i and C j in the answer graph The overall connection between C i and C j is controlled by both interand intracommunity concepts As depicted in Figure 10 the quality of th e connection at the community level is ensured by the community hierarchy tree based on modularity When the connection goes inside a community we apply the concepts of information throughput of nodes and information ow of paths to compute the highest-value intra-community path Finally in Algorithm 2 we rst sort the shortest paths in Line 8 and terminate the process when all query nodes are connected in Lines 12-13 This is a greedy algorithm that computes a connected answer graph by selecting the best inter-community path at each step  Since the inter-community connection is at a much coarser l evel than the intra-community connection connecting all query nodes by the greedy strategy sufﬁces though computing the connection between all communities in C is also possible and incurs only little overhead We now analyze the complexity of the inter-community connection First nding the shortest path in G com takes O   E com  log  V com   time and  O   E com    V com   space Since the sets of community edges kept by the virtual communities are disjoint and we only require the virtual communities on the simple path from C i to C j in H   E com  is small and so is  V com   In total we need to nd   C    C  1  2 shortest paths but  C 010|Q and in general a user does not ask a query with many query nodes Computing the actual path from the shortest path p  005 C i C j 006 takes 002 C k 004 p O   C k  2  where O   C k  2  is the time for computing the voltage and information throughput of the nodes in C k  Note that the time taken for the BFS path 
865 
865 
 


selection is dominated by O   C k  2   Finally the space requirement is O  MAX  C k  2    Since the size of a community is small both the time and space are also small VII E XPERIMENTAL R ESULTS We now evaluate the performance of our algorithm for object connection discovery We run all experiments on an AMD Opteron 248 with 1GB RAM running Linux 64-bit We use the DBLP co-authorship dataset modeled as a graph The graph has approximate ly 316K nodes and 1,834K edges where a node represents an author and the edge weight is the number of papers co-authored between two authors A Performance of Community Partition We rst evaluate the performance of community partition by our greedy algorithm We test three settings of c local 10 50 and 100 and four settings of c global 10 100 1000 10000 We also compare with Newman’s algorithm 13 Figure 11\(a shows that our algorithm is about an order of magnitude faster than Newman’s when c local 10 Theresult also shows that when c local increases the efﬁciency decreases which demonstrates the effectiveness of Heuristic 1 10 100 1000 10000 10 2 10 3 10 4 c g lobal Running Time \(sec c local 10 c local 50 c local 100 Newman a Running Time 10 100 1000 10000 0 50 100 150 200 250 300 c g lobal Peak Memory Consumption \(MB c local 10 c local 50 c local 100 Newman b Memory Consumption Fig 11 Performance of Community Partition For the effect of Heuristic 2 i.e c global  the performance is the best when c global  1000 When c local 10  the running time is 682 613 570 and 667 seconds for c global  10 100 1000 and 10000 respectively The result can be explained as follows When c global is too small many items are not kept in the global max-heap and hen ce we need to rebuild the heap more often When c global is too large there are too many items in the heap and hence the update of the heap takes longer Figure 11\(b shows that the peak memory consumption of our algorithm increases when c local increases since the size of the local max-heaps increases when c local increases Increasing c global  i.e the size of the global heap from 10 to 10000 only increases the memory usage for less than 1 MB since we have only one global heap However in all cases our algorithm consumes considerably less memory than Newman’s We also record that the value of the modularity of the optimal community partition obtained by the greedy algorithm is 0.71 note that all the algorithms compute the same partition According to Newman 13 a m odul ari t y v a l u e o f g reat er t h an 0.3 indicates a signiﬁcant comm unity structure Therefore 0.71 is a very high value of modularity and indicates a highquality community partition B Semantics of Answer Graph A Case Study We conduct a case study to compare our answer graph with the center-piece subgraph  CEPS  10 Thi s s t udy ai ms to rst provide a more intuitive view on the answer graphs obtained by our algorithm and CEPS Then we perform a more systematic comparison in the following subsection We use the query  Jim Gray  Jennifer Widom  Michael I Jordan  Geoffrey E Hinton   The four scholars are from two different communities Gray and Widom are from the database community while Jordan and Hinton are from the machine learning community This is clearly captured by our answer graph as shown in Figure 1 which is displayed in Section I Figure 12 shows the answer graph of CEPS which is very similar to our answer graph The similarity is because both our algorithm and CEPS nd nodes that are closely related to the query nodes in order to connect them Thus the result shows that both algorithms are able to capture important nodes and paths related to the query nodes However our method not only nds a good connection between all query nodes but also for query nodes that are in the same context we put more emphasis on their connection than the existing methods Compare Figure 1 with Figure 12 we clearly see a stronger connection between Gray and Widom through both Ceri and Hellerstein in our answer graph than CEPS Michael I Jordan Jim Gray Alexander Aiken 1 3 Jennifer Widom Michael Stonebraker Tommi Jaakkola Lawrence K. Saul 3 Zoubin Ghahramani 9 Geoffrey E. Hinton 7 1 2 5 11 2 1 9 1 1 5 3 Joseph M Hellerstein Fig 12 The Answer Graph of CEPS Although the quality of the answer graphs is comparable our algorithm signiﬁcantly outperforms CEPS we take only 0.33 seconds to compute our answer graph while the computation of the CEPS takes 925 seconds We further compare the two methods using more systematic measures as follows C Performance of Object Connection Discovery We compare the performance of our algorithm PCquery with CEPS 10  W e s et t h e b udget t o b e t w i ce of t h e query size We also verify that the answer graphs obtained by PCquery and CEPS are of roughly the same size Other settings of CEPS are as its default We generate two types of queries in-community queries and random queries  which are abbreviated as cq and rq in the gures For in-community queries the nodes in a query are randomly selected from a randomly selected community For random queries the nodes in a query are randomly selected from the set of all nodes in the dataset We generate 100 queries for each type and test the query size from 2 nodes to 20 nodes Figure 13\(a reports the average running time of nding the connection for a query The result shows that PCquery is more 
866 
866 
 


than three orders of magnitude faster than CEPS for both query types We nd that PCquery takes more time to process an incommunity query than a random query This is because the computation of the intra-community connection by dynamic programming is more costly than that of the inter-community connection by tracing the c ommunity hierarchy tree 2 4 8 12 16 20 10 2 10 0 10 2 10 4 Quer y Size \(number of nodes Average Response Time \(sec PCquery \(cq CEPS \(cq PCquery \(rq CEPS \(rq a Average Response Time 2 4 8 12 16 20 0 100 200 300 400 500 600 700 Quer y Size \(number of nodes Peak Memory Consumption \(MB PCquery \(cq CEPS \(cq PCquery \(rq CEPS \(rq b Memory Consumption Fig 13 Efﬁciency of PCquery and CEPS Figure 13\(b reports the peak memory consumption during the entire running process The result shows that PCquery also consumes signiﬁcantly less memory than CEPS in all cases In addition to the comparison on efﬁciency we also compare the quality of the answer graphs obtained by PCquery and CEPS For the fairness of comparison We use the quality metrics proposed in CEPS 10  NRatio and ERatio which indicate the percentage of important nodes and edges that are captured by an answer graph respectively We report the result in Figure 14 2 4 8 12 16 20 0 20 40 60 80 100 Quer y Size \(number of nodes NRatio PCquery \(cq CEPS \(cq PCquery \(rq CEPS \(rq a Average NRatio 2 4 8 12 16 20 0 20 40 60 80 100 Quer y Size \(number of nodes Eratio PCquery \(cq CEPS \(cq PCquery \(rq CEPS \(rq b Average ERatio Fig 14 Quality of Answer Graph Figure 14 shows that both PCquery and CEPS obtain high-quality answer graphs Both NRatio and ERatio of our answer graphs are comparable to those of CEPS although on average those of CEPS are slightly better Considering our algorithm is three orders of magnitude faster and also consumes signiﬁcantly less memory we can conclude that our method is both efﬁcient and effective VIII C ONCLUSIONS We propose context-aware object connection discovery in a large graph We adopt a partition-and-conquer approach to achieve both high performance efﬁciency and high quality results Our method rst partitions a large graph into a set of communities The concept of community not only naturally deﬁnes the context of the nodes but also signiﬁcantly improves the efﬁciency of connection d iscovery since a community is much smaller than the original graph We compute the connection between query nodes rst at the intra-community level by maximizing the information throughput of the nodes and the information ow of the paths in the answer graph and then at the inter-community level by retaining the close relation between the commun ities as deﬁned by modularity The quality of both the intraand intercommunity connection is thus controlled by the integration of information throughput/ﬂow and modularity We verify by experiments that our community partition algorithm is efﬁcient and the set of communities obtained has high quality We also show that our method obtains comparable high-quality answers as the state-of-the-art algorithm but is more than three orders of magnitude faster and consumes signiﬁcantly less memory Acknowledgement This work is partially supported by RGC GRF under grant number CUHK419008 and HKUST617808 We thank Mr Hanghang Tong and Prof Christos Faloutsos for providing us the source code of CEPS R EFERENCES  X  Y an P  S  Y u and J  H an  Graph i nde xing bas e d o n d is crim inati v e frequent structure analysis ACM TODS  vol 30 pp 960–993 2005  J  C he ng Y  K e  W  N g a n d A  L u  F g-i nde x t o w a rds v e r i  c a t i on-fre e query processing on graph databases in SIGMOD  2007 pp 857–872  P  Z hao J  X Y u  a nd P  S Y u   Graph i nde xing T r ee  d elta   graph in VLDB  2007 pp 938–949 4 Y  K e J  Cheng and W  N g Cor r e lation s ear ch in gr aph d atabas es   in KDD  2007 pp 390–399 5 Y  K e J  Cheng and W  N g E f  cient c or r e lation s ear ch f r o m g r a ph databases To appear in TKDE  2008  A  I nokuchi T  W as hio and H  M ot oda An apriori-based algorithm for mining frequent substruc tures from graph data in PKDD  2000 pp 13–23  X  Y an and J  H an  Clos e g raph m i ni ng closed frequent graph patterns in KDD  2003 pp 286–295  J  H uan W  W a ng J  Prins  and J  Y ang Spin mining maximal frequent subgraphs from graph databases in KDD  2004 pp 581–586 9 C  F alouts o s  K  S  M c Cur l e y  a nd A  T o m k ins  F as t d is co v e r y of connection subgraphs in KDD  2004 pp 118–127  H  T ong and C  F alouts o s  Center piece s ubgraphs  p roblem de n ition and fast solutions in KDD  2006 pp 404–413  Y  K o ren S C North and C  V olins k y  Meas uring a nd e x tracting proximity in networks in KDD  2006 pp 245–255  M E  J  Ne wm an and M  G irv a n Finding and e v a luating c om m unity structure in networks Physical Review E  vol 69 p 066113 2004  M E  J  Ne wm an  F a s t algorithm f or detecting c om m unity s t ructure i n networks Physical Review E  vol 69 p 066133 2004  F  W u and B  A  H uber m an  F i nding com m unities i n linear tim e a physics approach The European Physical Journal B Condensed Matter and Complex Systems  vol 38 no 2 pp 331–338 2004  R K u m a r  P  Ragha v a n S Rajagopalan and A  T om kins   T r a w ling the web for emerging cyber-communities Comput Netw  vol 31 no 11-16 pp 1481–1493 1999  R K u m a r  P  Ragha v a n S Rajagopa lan and A Tomkins Extracting large-scale knowledge bases from the web in VLDB  1999 pp 639 650  R K u m a r  U Mahade v a n and D  S i v akum ar   A g raph-theoretic approach to extract storylines from search results in KDD  2004 pp 216–225  D Gibs on R K u m a r  and A  T om kins   Dis c o v e ring lar g e d ens e subgraphs in massive graphs in VLDB  2005 pp 721–732  Y  Douris boure F  Geraci a nd M Pe llegrini Extraction and classiﬁcation of dense communities in the web in WWW  2007 pp 461–470  A Claus e t M E  J  Ne wm an a nd C Moore Finding com m unity structure in very large networks Physical Review E  vol 70 p 066111 2004 
867 
867 
 


  13 false \(double\argets Figure 12 illustrates this situation. The conditional update correctly updates the tracker wh ich is tasked with following the target. However, the detector which is partially spatially coincident with the tracker also receives energy from the conditional update. This can lead the detector to initiate falsely\second target nearby the first target This effect can be countered a number of ways. First, we can adjust the speed at which the tracker re-centers itself The double initialization phenomenon occurs when the PDF peaks near the edge of the tracker grid. However, this method has the side effect of potentially allowing probability to fall off of the grid in low SNR environments causing track loss. Of course if the SNR is low enough or measurement outages occur tracks will be dropped. Second a guardband around the tracker that does not allow any detector sufficiently near the tracker to receive reinforcement via the conditional density can mitigate the double target problem. However, this has the side effect of preventing detection of closely spaced targets. Third increasing the spatial extent of the tracker has a similar effect as the using a guardband. It does require increased computation, but generates a better representation of the posterior There are several engineering tradeoffs. The first is that large tracker grids \(or large guard bands\ prevent falsely detecting new targets because of conditional probability spill over. However, if applied too aggressively, this will prevent correctly detecting cl osely spaced targets. Second quick tracker grid translation correctly centers the target mass, again preventing spillover into nearby detectors However, overly liberal trac ker repositioning may in fact move trackers to spurious energy locations and drop true targets off of the finite grid On Ambiguous Targets As discussed earlier, ambiguous targets will eventually move non-physically and this will cause the tracker to remove them via its natural prediction and update process Figure 13 illustrates this phenomenon. There are two real targets that create two persistent ambiguities. All four are detected and tracked automati cally. The ambiguous targets however, eventually move non-physically due to their reliance on the node bearing angles. The tracker automatically penalizes the non-physical motion and the targets\222 present hypothesis decrease quickly over time Ambiguous target removal is done automatically in the Bayesian framework as follows The PDF on target state is predicted forward in time according to the kinematic model True targets will have behavior consistent with the kinematic model \(note the kinematic model is a statistical model so it is predicting a range of possibilities for the future target state\biguous targets may behave consistently with this model for a period of time, but eventually they will appear to perform a non-physical maneuver \(these epochs typically come when the ambiguous target crosses a line of symmetry in the sensor\this point, the predicted target position will be in strong disagreement with the inco ming measurements on that target. This mismatch in predicted target position and measurements leads to a decr ease in the target present hypothesis as calculated in eq. \(4\long, only true targets remain   Figure 12 \226 Improper selection of grid resolution leads to multiple initializations on the same target. Left Measurement update of a Tracker \(red=highest likelih ood, blue=lowest\Right Measurement update of a detector which lies near the Tracker.  Since the track er size has been improperly chosen, some energy from the measurements of a single target leak s on to the detector. This can le ad to false double-initializations 


  14 8  C ONCLUSION  This paper has described a Bayesian approach to detecting and tracking multiple moving targets using acoustic data from multiple passive arrays In contrast to traditional undersea acoustic systems, which develop tracks at the single array level and require track association, our approach fuses data at the m easurement level and operates directly in the target state space We have detailed a well known nonlinear filtering approach to single target detection and tracking [1, 4 and desc ri be d our computationally efficient finite-grid approach to the required density estimation. We have furthermore extended this to the multiple target case by employing a bank of single target detector tracke rs and approximation methods that adjust for closely spaced targets. This approximate approach avoids fully treating the computationally complex joint multitarget problem Future work includes modified approaches to posterior estimation including dynamic grid extent, dynamic grid resolution, and particle filtering. It is anticipated that adaptive sampling of the posterior will lead to computational savings. Furthermore, future work includes more detailed modeling and estimation of closely spaced targets allowing a more accurate representation of the joint target density. Naively implemented, this implies exponential growth \(in the number of targets\r the probability state space being es timated. However, recent work in a related tracking domain on adaptive density factorization [5 c h a stic sa m p lin g  p article filtering    pr ovi de m e t h o d s t h at m i t i g at e t h i s com put at i o n gr owt h  when the full joint density is treated  A PPENDIX  This section discusses the details of how the single target probability density is time evolved on a discrete grid. This discussion is similar to that found elsewhere [15, 14, 13 We wish to compute the single target probability density at time      from the density at time     The relation between these two densities can be expressed using the law of total probability as                We expand     using a second order Taylor series as               where  is the vector of partial derivatives, i.e and is the matrix of second order partial derivatives Then the relation of \(23\ approximated as   Figure 13\226 Left: P h1 over time for four targets, two of which are real and two of which are ambiguous. Although the ambiguous intersections are persistent, eventually the false targets ha ve non-physical motion. The target present hypothesis quickly goes to zero for these targets and they are elimin ated. Right: the tracker estimate of target position and red circles indicating the removal point fo r the false targets 


  15             Where denotes the expectation with respect to the transition distribution    and the omitted terms involve similar terms involving and and cross terms between the and coordinates We use the nearly constant velocity \(NCV\model to specify the transition distribution    This assumption corresponds to one where the target moves at constant velocity except for random jump changes \(i.e nearly constant velocity\is is a plausible model when  is small as it is here Specifically, the NCV model assumes step changes in target velocity defined by the Ito Equations     This model implies  and likewise for  It is furthermore assumed that th e noise processes in each coordinate are independent Under this model, we can eval uate the required terms from 25\ as follows          And likewise for terms involving and Notice that all cross terms \(e.g  have expectation due to the assumption that the noise process is independent in the two coordinates This model simplifies \(25\ to         where the terms omitted are replicas involving the  coordinate Under the assumption that is small, this can be rewritten as    For implementation, this is approximated using an implicit Euler scheme wh ere      Where the indices  represent the discrete    locations where the probability mass is captured Likewise, using forward differencing      and          and similarly for the y coordinate system When substituted into \(28\is leads to a series of equations of the form                This series of equations defi ne the probability at each point at time  It can be efficiently solved via Thomas\222 algorithm \(rather than simply inverted\he matrix is tridiagonal     


  16 R EFERENCES    R o y E. Bet h el Benjam i n Shapo, C h r i st opher M   Kreucher, \223PDF Detection and Tracking\224, under review IEEE Transactions on Aerospace and Electronic Systems  2 y. E B eth e l an d G. J. Paras, \223A PDF Mu lt it arg et Tracker\224 IEEE Transactions on Aerospace and Electronic Systems vol. 30, no. 2, pp. 386-403, April 1994 3   R o y E  B e t h e l a n d G  J  P a r a s  223 A P D F M u l t i s e n s o r  Multitarget Tracker\224 IEEE Transactions on Aerospace and Electronic Systems vol. 34, no. 1, pp. 153-168 January 1998  L   D   Stone, C. A. Bar l ow, and T. L Corwin, \223Bayesian  Multiple Target Tracking\224  Boston: Artech House, 1999  l la and A. Hero, \223Multitarget Tracking using the Joint Multitarget Probability Density\224 IEEE Transactions on Aerosp ace and Electronic Systems  vol. 41, no. 4, pp. 1396-1414, October 2005  M  M o relande, C. Kreucher, K. Kastella, \223A Bay e sian  Approach to Multiple Target Detection and Tracking\224 IEEE Transactions on Signal Processing vol. 55, no. 5 pp. 1589-1604, May 2007  B  Shapo, and R  E B e t h el  223An Overvi ew of t h e Probability Density Function \(PDF\er\224 Oceans 2006 Boston, Sept. 2006  R oy L. St r e it 223M ult i s ensor M ul tit arget Int e nsit y Fil t er 224  International Conference on Information Fusion  Cologne, Germany July 2008  M  Ort on and W Fi t z geral d 223A B a y e si an approach t o  tracking multiple targets using sensor arrays and particle filters\224 IEEE Transactions on Signal Processing, vol. 50 no. 2, pages 216-223, Feb 2002  A. Doucet B Vo, C Andri e u, and M Davy 223Par t i c le filtering for multi-target tracking and sensor management\224, IEEE International Conference on Information Fusion, 2002  H Van T r ees, \223Det ecti o n Est i m a t i on, and M odul at i o n  Theory IV:  Optimum Array Processing\224  J. C St ri k w erda, Fi nit e  Di fference Sch e m e s and Partial Differential Equations, Ch apman & Hall, New York 1989   K. Kast el la and C Kreucher, \223M ult i p l e  M odel Nonl i n ear  Filtering for Low Signal Ground Target Applications\224 IEEE Transactions on Aerospace and Electronic Systems vol. 41, no. 2, April 2005, pp. 549-564  Z. Tang and \334. \326zg\374n er, \223Sensor Fu si on for Target Track Maintenance with Multiple UAVs based on Bayesian Filtering Method and Hospitability Map\224 Proceedings of the 42 nd IEEE Conference on Decision and Control pages 19-24, December 2003  K. Kast ell a 223Fi n it e di ff erence m e t hods for no nl i n ear filtering and automatic target recognition\224 MultitargetMultisensor Tracking: Applications and Advances vol III, pages 233-258, Artech House, 2000 B IOGRAPHY  Chris Kreucher received his Ph.D. in Electrical Engineering from the University of Michigan in 2005. He is currently a Senior Systems Engineer at Integrity Applications Incorporated in Ann Arbor, Michigan. From 1998 to 2007, he was a Staff Scientist at General Dynamics Advanced Information Systems' Michigan Research & Development Facility \(formerly ERIM\. His current research interests include nonlinear filtering \(specifically particle filtering Bayesian methods of multitarget tracking, self localization information theoretic sensor management, and distributed swarm management Ben Shapo earned his Ph.D. in El ectrical Engineering in 1996 from the University of Michigan.  He is currently a Senior Systems Engineer at Integrity Applications Incorporated in Ann Arbor, Michigan.  From 2003 to 2008 he was a Lead Engineer at General Dynamics, where he contributed to a number of RF and acoustics signal processing and tracking efforts.  Dr. Shapo has 12 years experience in the DoD research community in the areas of detection, tracking, and data fusion, with emphasis on highfidelity simulations and applying new methods to real data  Dr. Roy Bethel is currently employed at The MITRE Corporation in McLean, VA. He has been actively involved in development, testing, and evaluation of signal processing and detection and tracking systems. In particular, he has developed many systems that have been implemented on United States Navy airborne, surface, and submerged platforms. He is currently engaged in research and development of innovative approaches to multitarget detection and tracking  A CKNOWLEDGEMENTS  This work was partially funded by the Office of Naval Research contract N00014-08-C-0275. The authors would like to thank Dr. John Tague for his support, and Mr. Scott Spencer and Dr. Charles Choi for their assistance 


2 1 0 00                4 G ro w th 1  1 3 1 0 9 2 0 2 3 0 10  0 56                So ci ode m og ra ph ic c ha ra ct er is tic s 


s 5 A ge y ea rs   21 7 8 7 3 9 0 01 0 22  0 1 4 0 0 8              6 G en de r i s fe m al e2   0 2 4  0 0 6 0 0 2 0 00 0 0 3 0 10    


           7 C ur re nt ly n ot w or ki ng 2  0 0 5  0 0 8 0 04 0 04 0 0 1 0 16  0 16             8 C ur re nt ly in e du ca tio n2   0 6 


6 7  0 01 0 1 9 0 08  0 03 0 6 8 0 0 7 0 3 2           9 C ur re nt ly w or ki ng 2  0 2 8  0 03 0 18  0 1 1 0 0 3 0 64  0 00 0 1 4 0 8 9   


        10 E du ca tio n ac hi ev ed 3  3 5 7 1 5 2  0 04 0 02 0 2 1 0 1 2 0 16  0 02 0 1 6 0 13  0 0 6         11 D is pe ns ab le in co m e   


  21 0 9 2 72 7  0 14  0 0 1 0 09  0 08  0 2 0 0 00 0 0 4 0 18  0 1 6 0 0 1        In te rn et u sa ge                     


  12 A ct iv e in te rn et u sa ge 1  0 0 2 0 9 6 0 2 1 0 25  0 11  0 12  0 10  0 0 4 0 05  0 0 8 0 0 5 0 0 1 0 12        13 H ou rs o nl in e h ou rs 


rs   2 6 5 3 0 3  0 04 0 12  0 1 1 0 0 3 0 40  0 0 7 0 0 7 0 4 7 0 5 3 0 07  0 1 1 0 07       14 W illi ng ne ss to p ay 1  1 8 3 0 6 3  0 03 0 10 


10  0 07  0 08  0 0 2 0 0 4 0 0 1 0 01  0 00 0 0 5 0 14  0 04 0 05      G am e sp ec ifi c va ria bl es                      15 T en 


ur e w ee ks   2 8 2 3 5 2 0 2 6 0 31  0 0 9 0 01 0 12  0 0 4 0 02 0 0 9 0 0 9 0 07  0 02 0 13  0 08  0 0 4    16 C ro ss o ve r on o ffl in e 4  0 1 5 


5 1 1 1 0 1 9 0 11  0 13  0 18  0 2 0 0 1 4 0 0 7 0 14  0 1 1 0 0 4 0 08  0 15  0 0 5 0 01 0 07    17 S at is fa ct io n1   18 7 5 1 3 16  0 18  0 00 


00 0 44  0 52  0 1 4 0 0 3 0 02 0 07  0 0 9 0 1 4 0 10  0 08  0 0 6 0 09  0 0 1 0 13   18 C om m itm en t1  0 6 2 0 8 3 0 3 1 0 13  0 37  0 39  0 0 7 


7 0 0 6 0 02 0 03  0 0 4 0 1 3 0 14  0 17  0 0 5 0 09  0 07  0 19  0 58  S ou rc e O w n ca lc ul at io n N ot e N  1 3 89 o bs er va tio ns S ig ni fic an ce le ve ls 


ls  p  0 05 S D  S ta nd ar d de vi at io n 1 5 po in t L ik er t s ca le ra ng in g fro m 2 to 2  2 du m m y va ria bl e 3 o rd in al v ar ia bl e ra ng in g fro m v oc at io na l e du ca 


tio n to P h D 4 n um be r o f c on ta ct s   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


