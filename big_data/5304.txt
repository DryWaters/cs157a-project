A Framework to Customize Privacy Settings of Online Social Network Users Agrima Srivastava Department of Computer Science and Information Systems BITS Pilani Hyderabad Campus Hyderabad,India Email agrimasrivastava1@gmail.com G Geethakumari Department of Computer Science and Information Systems BITS Pilani Hyderabad Campus Hyderabad,India Email geetha@hyderabad.bits-pilani.ac.in 204Privacy is one of the most important concerns in an online social network Online social network data is big data as millions of users are a part of it The personal information of every user in an online social network is an asset that may be traded by a third party for its bene\223ts Individuals should be aware of how much of their personal information could be shared without risk Different people have different requirements to share a pro\223le item hence measuring privacy of such huge and diverse population is a challenging and complicated task in itself In this paper we have proposed a framework that ensures privacy of individuals by allowing them to measure their privacy with respect to some speci\223c people of their choice rather than measuring it with the entire population on online social networks We have suggested a method to choose the best model to 223t the real world data and to calculate the sensitivities of various pro\223le items The framework gives speci\223c labels to users that indicates their pro\223le privacy strength and enable them to customize their privacy settings so as to improve the privacy quotient The users can also act as advisers to their online friends whose privacy quotients are low and thus spread privacy awareness in social networks customized privacy;privacy strength;item response theory Abstract Keywords I I NTRODUCTION Online Social Networks OSNs have become the most popular Internet service to be used to date It helps the users to stay connected with the world The users of OSNs lik e Facebook Twitter LinkedIn MySpace etc have increased exponentially in the recent years Using OSN people can interact with each other by sharing their personal information such as photos likes dislikes interests relationship status job details current town details political views religious views etc There is an information e xplosion in the quantity and diversity of high frequency online social network data This greatest advancement came with a price of privacy With so much of personally identi\223able information PII available online privacy concerns are bound to arise Our PII is an asset to many third parties and can be mined and utilized for bene\223ts People are unintentionally and intentionally sharing their personal details without understanding the cost that they are paying for it If personal data about individuals are collected processed stored and retrieved without their consent their privacy is under threat Exposure of one\220s private details can lead to identity theft market bene\223ts and embarrassment etc OSN users are not aware of their online privacy leaks They should know what and how much information they should share such that they are not at risk Privacy greatly depends upon the context What is private to a particular individual may not be private to others To measure a person\220s intelligence we need the intelligence quotient in the similar fashion to measure the privacy of the user we need to know and understand the privacy quotient PQ There has to be a privacy measuring scale in which the individuals can be ranked according to their PQs and should know where they stand in comparison to the rest of the world Calculating the privacy quotient for the entire social network population is a Big Data problem and a dif\223cult task to undertake The privacy requirements are a function of the demography of the users as well as the social ties they have Customizing the privacy settings of users by looking at such a diversi\223ed data does not really help because different people have different requirements and understanding of privacy which fetches them different privacy quotients Instead of measuring the privacy of the whole network our framework allows the users to measure the privacy with the user pro\223les of their choice which reduces the data set greatly Calculation of privacy quotient is analogous to the standard classical test theory We have used the existing privacy measuring models to formulate a framework to measure the privacy of the users in an OSN with respect to their circle in the OSN Our framework considers naive approach one parameter constrained and unconstrained data model and two parameter data model and selects the best model out of the four adaptive test models There are v arious approaches to measure the privacy of an online social networking user Every approach deals with a different model and every model may not necessarily 223t the real world data Selecting the best model out of all is an important step We have used the concept of Akaike Information Criterion AIC Bayesian Information Criterion BIC and log likelihood to select the suitable model After calculating the privacy quotient using a suitable model we have 223xed a range to the privacy quotient using the k means clustering algorithm and have labelled a strength to each of the range Understanding text is a lot easier than 2013 IEEE Recent Advances in Intelligent Computational Systems \(RAICS 978-1-4799-2178-2/13/$31.00 ©2013 IEEE 187 204 


0.7 and hence will be able to measure the attitude of the user correctly The accuracy of the model will be more if we use larger data set consisting of as many of our friends\220 pro\223les as possible that we want to include TABLE I List of pro\223le items numbers hence by looking at the label the users will know the strength of their pro\223les in comparison with the their list of selected friends Our paper is organized as follows Part II describes the related work in the 223eld in Part III we have given the algorithm of our framework to measure the Privacy Quotient of the user\220s surroundings and know the privacy strength of the user Various groups will have its own range of privacy quotients hence in Part IV we have explained in detail the various steps enlisted in Part III by working out an example Finally at the end in Part V we have given the conclusion and discussed our future work II R ELATED W ORK Rasch introduced a psychometric model that is used to analyze the data on the basis of the respondent\220s ability and the dif\223culty of the item The mathematical theory behind the rasch model is the special case of the item response theory Hiding an item reduces its utility hence Guo et al have proposed a framework which caters the utility needs of a pro\223le item while ensuring privacy of users by mining the facebook privacy settings of users They have developed a tradeoff algorithm to help the users to know their optimal privacy settings for a particular level of privacy concern and their personalized utility preferences W iese et al ha v e carried out a study on 42 participants and calculated their frequency of collocation and communication closeness and social group and showed that self-reported closeness is the strongest indicator of willingness to share and individuals are more likely to share in scenarios with common information Liu et al have provided an intuitively and mathematically sound methodology for computing the privacy scores of users in the OSNs by making use of the Item Response Theory model They have used the two parameter logistic model to calculate the privacy scores of individuals F ang et al ha v e proposed a template for social networking privacy wizard They have used the fact that the real users conceive their privacy settings based on some rules which are implicit in nature They proved that using machine learning techniques and with a limited amount of information and knowing the user\220s preferences they could calculate the privacy settings of the user automatically Aiello et al ha v e tackled the trade-off problem between security privacy and services in distributed social networks by providing the users the possibility to tune their privacy settings through a very 224exible and 223ne-grained access control system We use a concept of Fig 1 Proposed Privacy Measuring Framework To explain our proposed solution fully we will be using the data collected from a user having about 60 users in the Friend Set and then will collect the information for the following list of 11 pro\223le items Table I shows the list of 11 pro\223le items The cronbach\220s alpha for all these 11 items is 0.7228 which is and best model selection technique for measuring the privacy strength which not only improves the target user\220s privacy but also sends an indication to the ones having a lesser privacy strength than the target user thereby increasing the privacy awareness amongst the users III T HE P ROPOSED F RAMEWORK We describe below the steps involved in the calculation of privacy strength for a user in our proposed framework  A Input to the framework Friend Set i.e the list of people in the user\220s friend list with whom they want to compare their privacy B Formation of a dichotomous response matrix C Selection of the best model that will 223t the response matrix D Calculation of sensitivity of the pro\223le items E Calculation of probability matrix F Calculation of privacy quotient G Deciding the range of privacy quotients using k means clustering algorithm H Setting up labels for each of the ranges I Output of the framework Privacy Strength of the user\220s pro\223le with respect to the Friend Set IV D ETAILED EXPLANATION OF THE FRAMEWORK Fig 1 given below gives a complete 224ow of the framework To explain our point effectively we will explain the process of measuring privacy using values from a real data set 002 Friend Set SNo Pro\223le Items 1 Contact Number 2 E mail 3 Address 4 Birthdate 5 Hometown 6 Current Town 7 Job Details 8 Relationship status 9 Interests 10 Religious Views 11 Political Views 188 


parameters ability 1 Equation 1 shows that the probability of an item being shared is a function of the model\220s item parameters and ability of the individual 002    004 j N and 1 i n then for an item i being shared by an individual j the value of    ith i.e the discrimination constant Sensitivity 002 002 003 004 002 row and column is marked as 1 otherwise is marked as 0 002 003 002 1 The one parameter logistic model is the simplest of all the item response models This has a single parameter jth means that all the items are equally discriminating The probability of a jth individual who is having an ability of SNo Pro\223le Item Discrimination Sensitivity 1 Contact Number 1 2.9446946645 2 Email 1 0.6117859481 3 Address 1 4.5480914380 4 Birthdate 1 0.0000000000 5 Home Town 1 0.3316536983 6 Current Town 1 0.0003948577 7 Job Details 1 0.7386141987 8 Relationship Status 1 2.0182711543 9 Interests 1 1.3872733265 10 Religious Views 1 2.7736557816 11 Political Views 1 3.3946345724 4 2024 0.0 0.2 0.4 0.6 0.8 1.0 Item response function Ability Probability of a correct response 003  is measured by the steepness of the ICC A high value of discrimination constant can better differentiate between the individuals having a low and high value of P 002 2 002 3 002 4 002 5 002 6 002 7 002 8 002 9 002 10  individual with an ability  Fig 2 The Item Characteristics Curve Each item characteristic curve has two properties namely  Fig 3 An Item characteristic curve for one parameter constrained logistic model 189 Prob PR 004 002 We have extracted the data sets from Facebook because Facebook provides an API for data collection The users were asked to create the Friend Set i.e a set of users with whom they want to compare their privacy Here we are working out the solution for a user having 60 friends in their Friend Set The users can have a minimum of 10 and maximum of the number of digital friends they have in their pro\223les We have assumed here that the user using this application will at least have 10 friends in their friend list If we take n dichotomous variables for N users in the Friend Set then we can generate a Nxn dichotomous response matrix If 1 The Item Response Theory IRT is widely used to measure the latent trait of an individual It gives a relationship between the person\220s response and attitude If there are j individuals and i pro\223le items and if the ability of each of the individual is 003 003 A Selecting the Friend Set  denotes the dif\223culty of an item The more is the sensitivity the less are the chances of it being shared The item discrimination 002 002 003 002 1 002 11 Table II gives us the values of sensitivity of all the 11 pro\223le items and in Fig 3 we can see that at any given instance for a person with a speci\223c ability the probability of sharing the date of birth curve number 4 is much higher than the probability of sharing the address curve no 3 In the one parameter logistic model the value of discrimination constant is constrained to 1 for all the items TABLE II Discrimination and Dif\223culty Table then P  is the probability that the j will share an item having an index i A graph with on the x axis and P  on the y axis will give a S shaped curve that is known as the Item Characteristic Curve ICC Fig 2 shows an Item Characteristic Curve 4 20 2 4 0.0 0.2 0.4 0.6 0.8 1.0 Item response function Ability Probability of a correct response  In IRT each item has a set of item parameters and each individual has their ability i.e the attitude that denotes the sensitivity of an item i Having a constant for sharing an ith pro\223le item is given by equation 2 2 where is the sensitivity of the ith pro\223le item is the discrimination constant of the ith pro\223le item which is set to 1 for constrained model and set to a 223xed value for an unconstrained model is the ability of the jth user One parameter logistic model can be categorized as constrained and unconstrained one parameter logistic model 003 003  1 1 004 f e j j j j j j i j 002 i i j i.e the sensitivity and 212 i j i B Formation of response matrix C Selecting the best model Calculation of sensitivity for constrained one parameter logistic model 1 One Parameter Logistic Item Response Theory Model 003 ij ij ij ij Sharing 004 th 


 i j i 200 5 190 002 212 003 This approach does not seem to work well in comparison with the item response theory models Complex IRT calculations can be avoided by making use of the naive approach if the number of pro\223les in the Friend Set is less than the number of pro\223le items 3 Equation 3 shows that the probability of an item being shared in a two parameter model is a function of both the item parameters      003  1 1 Here the discrimination constant 002 N 212 N j 002 i i j i i i 004 Fig 4 An Item characteristic curve for one parameter unconstrained logistic model Fig 5 An Item characteristic curve for two parameter logistic model 004 002 003 of the individual The probability of a jth individual who is having an ability of  Sensitivity of an item i is the measure of the dif\223culty or sensitiveness of an item The more an item is sensitive the least it is being shared Sensitivity Calculation of sensitivity for unconstrained one parameter logistic model 2 Two Parameter Model 3 Naive Model Approach SNo Pro\223le Item Discrimination Sensitivity 1 Contact Number 1.363163 2.323667e+00 2 Email 1.363163 4760522e-01 3 Address 1.363163 3.601252e+00 4 Birthdate 1.363163 3.453648e-06 5 Home Town 1.363163 2.573421e-01 6 Current Town 1.363163 0.000000e+00 7 Job Details 1.363163 5.753196e-01 8 Relationship Status 1.363163 1.585066e+00 9 Interests 1.363163 1.085306e+00 10 Religious Views 1.363163 2.187068e+00 11 Political Views 1.363163 2.683194e+00 SNo Pro\223le Item Discrimination Dif\223culty 1 Contact Number 0.8744772 4.012698 2 Email 0.3523724 1.223004 3 Address 0.0000000 8.815682 4 Birthdate 0.2619243 0.000000 5 Home Town 0.7471443 1.658689 6 Current Town 0.8839744 1.527580 7 Job Details 1.1031453 2.283478 8 Relationship Status 0.5572550 3.144633 9 Interests 2.6932438 2.977822 10 Religious Views 1.9696065 3.783610 11 Political Views 10.5183067 4.009395  Fig 4 is slightly steeper than Fig 3 and hence has got a better discriminating ability The rate of change of probability from going to one ability level to another ability level increases faster than with what we see in the constrained one parameter logistic model 4 20 2 4 0.0 0.2 0.4 0.6 0.8 1.0 Item response function Ability Probability of a correct response for sharing an ith pro\223le item is given by equation 4 4 where is the sensitivity of the ith pro\223le item is the discrimination constant of the ith pro\223le item is the ability of the jth user Table IV gives us the sensitivity as well as the dif\223culty of all the 11 pro\223le items Unlike the one parameter model the two parameter model has different discrimination values for each of the pro\223le items TABLE IV Discrimination and Dif\223culty Table In Fig 5 we can see that each item has its own set of discrimination and sensitivity values Political views curve no 11 has got the highest discrimination value and address curve no 3 has got the lowest of all 4 2024 0.0 0.2 0.4 0.6 0.8 1.0 Item response function Ability Probability of a correct response  of a pro\223le item can be calculated as Prob PR 004 003 002 1 002 1 002 11 002 1 002 11 002 2 002 3 002 4 002 5 002 6 002 7 002 8 002 9 002 10 002 2 002 3 002 4 002 5 002 6 002 7 002 8 002 9 002 10 ij The two parameter model calculates the probability of the user sharing an item based on the sensitivity as well as the dif\223culty f e R Sharing 002  Calculation of sensitivity for naive model  for all the pro\223le items is set to a single estimated value Table III gives us the values of sensitivity and dif\223culty of all the 11 pro\223le items Here the estimated single value i.e the discrimination constant is 1.363163 for all the pro\223le items TABLE III Discrimination and Dif\223culty Table 003 004 004 003   and the ability i.e 


 2 2 Calculation of visibility using the naive model ij ij R R R R R 002 212 200  Visibility is the popularity of an item in the network Visibility of an ith item by the user j can be calculated as 002 P AIC      212 N X n 003 7 BIC can be calculated as follows E Calculation of Privacy Quotient and deciding the ranges   D Model Selection is the summation of the number of pro\223le items shared by the user j 003 9 where  212 212 Constrained 1PL 53498.80 53570.49 26738.40 Unconstrained 1PL 53290.42 53368.63 26633.21 Unconstrained 1PL 53290.42 53368.63 26633.21 2PL 51356.44 51499.81 25656.22   8 where K is the number of parameters used in the model These values makes sense only when it is compared with the other models As it gives us the information loss hence the model having their least value of AIC and BIC is preferred over all the models In table VI we ha v e compared the constrained one parameter logistic model with the unconstrained one parameter logistic model and observed that the unconstrained one parameter logistic model gives the lowest AIC and BIC values Hence at the end of the 223rst comparison we select the unconstrained one parameter logistic model TABLE VI Comparison of constrained and unconstrained models SNo Pro\223le Item Sensitivity 1 Contact Number 7 2 E mail 2833 3 Address 95 4 Birthdate 2166 5 Hometown 25 6 Current Town 2166 7 Job Details 3 8 Relationship status 5166 9 Interests 4 10 Religious Views 6666 11 Political Views 7833 SNo Range of Privacy Quotient Percentage of users out of 60 1 0.0 3.373010 6.61 2 3.373010 8.644307 12.84 3 8.644307 13.713378 14.14 4 13.713378 19.113382 10.14 5 19.113382 25.347364 16.36 i i j i j i i j i j j  where Here in Table V though address is the most sensitive pro\223le item but no discrimination can be made between birthdate and current town In the second step we have made a comparison between the unconstrained one parameter logistic model and the two parameter model In Table VII the AIC BIC values of the two parameter model were found to be minimum TABLE VII Comparison of unconstrained and two parameter model Hence the 223nal model that could best 223t our data is the two parameter model  To decide the range of privacy quotients we have used the k means algorithm for 1000 iterations and have calculated 223ve centers sorting these centers gave us the upper limits of the 223ve ranges TABLE VIII No of users with the PQ in the given range The maximum and minimum of the privacy quotient obtained for the Friend Set of size 60 hence 60 were 29.2634 and 0 191 is the summation of the number of users who have shared an item i TABLE V Sensitivity of the pro\223le items using naive model         004 PQ Naive model is a population biased model and does not differentiate well between the sensitivities of two pro\223le items In table V birthdate and current town have the same sensitivity Hence we will go for the naive model if and only if the number of users are less than the number of pro\223le items Most of the times the Friend Set is big enough and applying item response theory models is the best choice to go for Our task is to select the best model out of constrained one parameter logistic model unconstrained one parameter logistic model and two parameter model When we use a model to calculate the results we may not get the exact value There is always a difference between the exact result and the results obtained This happens because the selected model does not 223t the data completely hence turns out to be erroneous Our aim is to select the best model out of all such that the loss of information is minimized In order to do that we should know the Information Criterion IC of the model We will utilize the Akaike Information Criterion AIC and Bayesian Information Criterion BIC for model selection AIC and BIC values gives us the loss of information AIC can be calculated as follows Selection of the best model is followed by the calculation of the privacy quotient The privacy quotient can be calculated as   i j K P N j 002 002 V 6 where Kln log likelihood log likelihood BIC   is the summation of the number of times an item is shared by all the users and Model AIC BIC log.lik Model AIC BIC log.lik is the sensitivity and is the probability of sharing an item i by an individual j with an ability of 


Understanding text is better than numbers and hence we have given labels to the range of privacy quotient Each of the ranges have been categorized with various labels as 216High PQ\216 216Good PQ\216 216Average PQ\216 216Below Average PQ\216 216Poor PQ\216 Table IX shows the mapping between the range of privacy quotients and the privacy strength TABLE IX Mapping of privacy quotient with privacy strength Privacy Security Risk and Trust PASSAT 2012 International Conference on and 2012 International Confernece on Social Computing SocialCom  vol 35 no 1 pp 75\20588 2012  F  Bak er and S.-H Kim F Labelling the privacy strength SNo Range of Privacy Quotient Percentage of users Privacy strength out of 60 1 0.0 3.373010 6.61 High PQ 2 3.373010 8.644307 12.84 Good PQ 3 8.644307 13.713378 14.14 Average PQ 4 13.713378 19.113382 10.14 Below Average PQ 5 19.113382 25.347364 16.36 Poor PQ Information Visualization 2005 INFOVIS 2005 IEEE Symposium on Computer Communications Data Mining 2009 ICDM\22009 Ninth IEEE International Conference on Proceedings of the 19th international conference on World wide web In Fig 6 we can see that most of the users are having the privacy quotient in the range of 19.113382 25.347364 which implies Poor Privacy Quotient This means that most of the users in the Friend Set of the user have a poor privacy strength Here all the percentages are out of 60 and the lower the privacy quotient the better is the privacy strength   Fig 6 A bar graph showing the percentage of users having a given privacy strength If the user has a privacy quotient below 13.713378 then his privacy settings will fall in any of the three categories i.e 216High PQ\216,\216Good PQ\216 and 216Average PQ\216 which is an acceptable privacy setting The user can now view the sensitivity of the various pro\223le items in the group which will give an indication as what are the list of items that are extensively shared and what are the list of items that are being shared the least The user can customize their privacy settings and can recheck their privacy strength As most of the users in the Friend Set have got a poor privacy quotient the user can as well send an alert to those users alerting them of their privacy leaks V C ONCLUSION AND F UTURE W ORK In this paper we have described a framework that would ensure privacy of a user by comparing with the privacy of other users in their friend list We have made the use of various models like the Naive one parameter logistic model constrained and unconstrained and two parameter model and have suggested a method to select the best 223t model out of all using the AIC and BIC values The framework utilizes the best model selected to calculate the privacy quotient and then determine the privacy strength of the user\220s pro\223le In future we will be solving the problems of privacy in unstructured data and will be working on group privacy settings to measure loss of privacy through the members of the group and hence prevent privacy leaks due to group memberships A CKNOWLEDGMENT We would like to thank KP Krishna Kumar for his help R EFERENCES  J Heer and D Bo yd 215V izster V isualizing online social netw orks 216 in UbiComp  IEEE 2005 pp 32\20539  L A Cutillo R Molv a and T  Strufe 215Safebook A pri v ac y-preserving online social network leveraging on real-life trust,\216  vol 47 no 12 pp 94\205101 2009  R Gross and A Acquisti 215Information re v elation and pri v ac y in online social networks,\216 in  ACM 2005 pp 71\20580  B Krishnamurthy and C E W ills 215Characterizing pri v ac y in online social networks,\216 in  ACM 2008 pp 37\20542  G Rasch 215Studies in mathematical psychology I probabilistic models for some intelligence and attainment tests.\216 1960  S Guo and K Chen 215Mining pri v ac y settings to 223nd optimal pri v ac yutility tradeoffs for social network services,\216 in  2011 pp 197\205206  K Liu and E T erzi 215 A frame w ork for computing the pri v ac y scores of users in online social networks,\216 in  ACM 2010 pp 351\205360  L M Aiello and G Ruf fo 215Lotusnet tunable pri v ac y for distrib uted online social network services,\216  CRC Press 2004 vol 176  M J Mazerolle 215 Appendix 1 Making sense out of akaik es information criterion aic its use and interpretation in model selection and inference from ecological data.\216 192 Item response theory Parameter estimation techniques  IEEE 2012 pp 656\205665  J W iese P  G K elle y  L F  Cranor  L Dabbish J I Hong and J Zimmerman 215Are you close with me are you nearby investigating social groups closeness and willingness to share.\216 in Communications Magazine IEEE Proceedings of the 2005 ACM workshop on Privacy in the electronic society Proceedings of the 223rst workshop on Online social networks  IEEE 2009 pp 288\205297  L F ang and K LeFe vre 215Pri v ac y wizards for social netw orking sites 216 in 


002\003\004\005\006\006 002\003\004\005\006\002 002\003\004\005\006\007 002\003\004\005\006\010 002\003\004\005\006\011 002\003\004\005\006\012 002\003\004\005\006\013 002\003\004\005\006\014 002\003\004\005\006\015 002\003\004\005\006\016 017\020\021\022\023\024\024\025\026\020 027\030\031\022\032\033\031\020\034\031\035 035\024\027\022\036\030\037\020 \024!"\034\033\026 036\024\025#\024$#\004%\020\027!\031\030\024\034#&\030'\020#\(\\003 032\033\025\020+\033\034  032\\032 006 006\003\012 002 002\003\012 007 007\003\012 010 010\003\012 011 011\003\012 012 002\010\012\014\016\002\002\002\010\002\012\002\014\002\016\007\002\007\010\007\012\007\014\007\016\010\002\010\010 037\020"\033\025\020#0\020\035\035\033\025\020\035#&"\033\034\035$\020""\0201 0\030\026\026\030\024\034\035 030'\020#\(\!2\020"\035\031\0202 032\033\025\020+\033\034,#0\020\035\035\033\025\020\035 0\020\035\035\033\025\020\035#\(\002#\\017\033\0313 032\#0\020\035\035\033\025\020\035#\(\002#\\017\033\0313 
Figure 2 Figure 3 
V E XPERIMENTAL S ETUP  We implement and run multiple graph algorithms including BC, PageRank and All-Pairs shortest path using our Pregel.NET BSP framework on the Microsoft Azure public cloud platform.  We use four real datasets   o f va r y ing siz e s th a t e x hib it sm a ll-w o r l d  pro p e r ties:  SlashDot \(SD\nd LiveJournal \(LJ\ social networks, a web graph from Google \(WG\, and a patent citation network \(CP\, whose statistics are shown in Table 1. Our implementation does not perform any aggressive memory management or compression of NET object representation of graphs; hence the sizes of graphs selected were conservative to fit available VM memory. However, these do not detract from the key benefits of the heuristics and the analysis of Pregel/BSP that we present empirically We use large Azure VM instances for all partition worker roles and small instances for web UI and manager roles since the latter just perform coordination Large VMs have 4-cores rated at 1.6GHz, 7GB RAM and 400Mbps network and cost $0.48/VM-Hour. Small ones are exactly a fourth of these specifications. These extents were chosen based on the Azure resources made available as part of a research grant, but the scalability trends can be extrapolated to larger numbers of VMs Given the time complexity of the BC algorithm, our large graph datasets can run for days or even weeks Thus, we perform a 4-hour run for each of our experiments and extrapolate these results to the entire graph VI E VALUATION OF B ASELINE AND H EURISTICS  Our Pregel.NET BSP framework can be used to implement and run a variety of graph applications on the Azure cloud. As a basic comparison, we implement PageRank, BC, and All-Pairs Shortest Path \(APSP\, and evaluated their performance with two graphs, WG and CP, using 8 worker VMs. The LJ graph would not fit within the available physical memory of the workers for BC and APSP due to the large numbers of messages that they generate during their traversal. LJ was however run for PageRank since it is a less demanding application Figure 2 shows the time taken \(log scale\ to run these applications for the graph sizes. As noted before, these times are extrapolated from running smaller sampling runs over a subset of the vertices. PageRank however was run to completion over 30 iterations. As can be seen, both BC and APSP take 4 orders of magnitude longer to complete than PageRank for the same graph sizes. BC and APSP are inherently superlinear in computational complexity compared to PageRank as they perform graph traversals rooted at every vertex while PageRank performs pairwise edge traversals from every vertex to its neighbors Further, if we consider the communication complexity of these applications using the BSP model, we notice that PageRank has a constant number of messages exchanged between workers across different supersteps Figure 3 shows a straight line at ~637,000 average messages exchanged by each of eight workers for the 30 supersteps required to complete PageRank for the WG graph, leading to a uniform performance profile and predictable resource usage. However, the messages transferred for BC and APSP over different supersteps has a triangle waveform pattern, which spikes to a peak of 4.7M and 3M messages respectively  just for a single swath of seven vertices and not for all vertices in the graph. The ramp up and down of messages over supersteps, and their repetitive nature as subsequent swaths of vertices are executed, leads to non-uniform resource usage, in particular, memory usage for buffering these messages between supersteps. The communication cost of message transfer also causes BC and APSP them to take much longer to complete than PageRank   
Total time taken \(log scale\ to perform PageRank, BC and All-Pairs Shortest Path \(APSP\ for the WG and CP graphs on 8 workers. LJ is shown for PageRank The average number of messages transferred per worker across supersteps for the WG graph for one static swath \(BC and APSP\, and for the entire graph \(PageRank 
  
 
Since BC traverses the entire graph rooted at each vertex, extrapolating results from a subset of vertices is reasonable and was empirically verified A Standard  Graph Applications using Pregel.NET 
209 


Speedup of Swath Size Heuristics vs. Baseline largest successful single swath size running on 8 workers for the BC application. Taller is better Memory usage over time for running BC on WG graph. Heuristics use a 6GB memory threshold. VMs have 7GB physical memory. Curves close to 6GB imply good memory utilization. Those near 7GB hit virtual memory   Speedup of Swath Initiation Heuristics vs Baseline Sequential Initiation. All run the BC application on 8 workers. Taller is better Message transfers over time for the various initiation heuristics for running BC on WG graph. Flatter is better 
Figure 4 Figure 5 Figur e 6  Figure 7 
B Swath Size Heuristics the more memory that is utilized \(while staying within physical memory limits the faster the completion time     
 
 
baseline sampling heuristic adaptive heuristic 
The original Pregel/BSP approach of running all vertices in parallel can be detrimental to performance because of the memory demand for buffering messages across supersteps.  In fact, in a cloud setting, spilling to virtual memory can lead workers to seem unresponsive and the cloud fabric to restart the VM, as we have observed. This is particularly a concern for applications like BC and APSP that show a non-uniform resource usage pattern. For e.g., in Fig. 15 \(top\BC and APSP for the WG graph had to be run in small swaths of around 10 vertices as otherwise they would well surpass available memory during their peaking supersteps. Our swath size and initiation heuristics attempt to break vertex computations into a smaller number of swaths that are run iteratively to better control memory demand.  The alternative would require scaling up to 1000's of VM's to accommodate the memory demand As a we manually found the largest swath size we could successfully complete the BC application using 8 workers \(7GB memory each\or our WG and CP graphs while allowing them to spill to virtual memory \(40 and 25 swathe sizes, respectively\. Next we used our proposed swath size heuristics and to pick smaller swath sizes automatically and iteratively execute each swath for the same total number of vertices as the baseline single swath \(40 and 25\n both these heuristics the target maximum VM memory utilization threshold is conservatively set to 6GB since over-estimation can be punitive.  We ran these experiments using both 8 and 4 workers Figure 4 summarizes the relative performance gain of our swath size heuristics for performing BC on WG and CP graphs compared to the baseline approach that uses 8 workers. The sampling heuristic yields a speedup of nearly 2.5-3 versus the single large swath baseline while the dynamic heuristic yields a speedup of up to 3.5.  Figure 5 illustrates the corresponding physical memory usage during these executions. The baseline spills beyond available physical memory \(flat at 7GB the dynamic heuristic stays close to the target memory threshold of 6GB while the static heuristic stays close to it, but less often so. Intuitively This allows the adaptive     
006\003\006 006\003\012 002\003\006 002\003\012 007\003\006 007\003\012 010\003\006 010\003\012 011\003\006 015#\017\024",\020"\0354#\2\026\030\034\025 3 015#\017\024",\020"\0354#/1\0332\031\030\037\020  011#\017\024",\020"\0354#/1\0332\031\030\037\020  020\0201!2#\037\035\003#\015#5\024",\020"4#\\025\026\020\022\\017\033\0313 017\020\021\022\023\024\024\025\026\020 027\030\031\022\032\033\031\020\034\031\035 002 6#\032\020"$\024"'\033\034\027\020#\024$#\026\033"\025\020\035\0314#\035\030\034\025\026\020#\035\017\033\0313#\0313\033\031 027\033\034#\035!\027\027\020\035\035$!\026\0267#\020%\020\027!\031\020#\024\034#\0313\020#\025\030\037\020\034#\025"\03323 006 002 007 010 011 012 013 014 015 006 002\006\006\006 007\006\006\006 010\006\006\006 011\006\006\006 012\006\006\006 013\006\006\006 014\006\006\006 0\020'\024"7#8\031\030\026\0309\033\031\030\024\034#\(\023 030'\020#\(\035 015#\017\024",\020"\0354#\\030\034\025\026\020\022\012\006 015#\017\024",\020"\0354#0!\026\031\0302\026\020\022\004%\031"\0332\024\026\033\031\0201 015#\017\024",\020"\0354#0!\026\031\0302\026\020\022/1\0332\031\030\037\020#.\024\034\031"\024\026 011#\017\024",\020"\0354#0!\026\031\0302\026\020\022/1\0332\031\030\037\020#.\024\034\031"\024\026 9!"\020#\036\033 025 020#:\034\035\031\033\034\027\020#0\033%#\0323 7 035\030\027\033\026#0\020'\024 7 036\030'\030\031 020!"\030\035\031\030\027 033"\025\020\031#\03237\035\030\027\033\026#0\020'\024"7#\036\030'\030\031 006\003\006 006\003\007 006\003\011 006\003\013 006\003\015 002\003\006 002\003\007 002\003\011 002\003\013 017\020\021\022\023\024\024\025\026\020 027\030\031\022\032\033\031\020\034\031\035 0201!2#\037\020"\035!\035#\\026#;\020!"\030\035\031\030\027 030\033\026 7\034\033'\030\027 031\030\027#\011 031\030\027#\012 031\030\027#\013 002#6#\032\020"$\024"'\033\034\027\020 024$#\\020<!\020\034\031\030\033\026#:\034\030\031\030\033\031\030\024\034 006\003\004\005\006\006 002\003\004\005\006\013 007\003\004\005\006\013 010\003\004\005\006\013 011\003\004\005\006\013 012\003\004\005\006\013 013\003\004\005\006\013 014\003\004\005\006\013 015\003\004\005\006\013 006 002\006\006\006 007\006\006\006 010\006\006\006 011\006\006\006 012\006\006\006 037\020"\033\025\020#0\020\035\035\033\025\020\035#\\020\034\031#\032\020"#5\024",\020 004%\020\027!\031\030\024\034#&\030'\020#\(\\020\027\024\0341\035 020\034\031\030\033\026 033\031\030\027#\013 7\034\033'\030\027 
210 


006\003\006 006\003\007 006\003\011 006\003\013 006\003\015 002\003\006 002\003\007 032\033\025\020+\033\034 017\020\021\022\023\024\024\025\026\020 017\020\021\022 023\024\024\025\026\020 032\\032#\(\017\020\021\022 023\024\024\025\026\020 032\033\025\020+\033\034,#\(\027\030\031\022 032\033\031\020\034\031\035 027\030\031\022 032\033\031\020\034\031\035 032\\032#\(\027\030\031\022 032\033\031\020\034\031\035 004%\020\027!\031\030\024\034#&\030'\020#>\024"'\033\026\0309\0201#\031\024#;\033\0353\0201#\032\033"\031\030\031\030\024\034\030\034\025 033\0353\030\034\025 0\004 020\033'\030\034\025 032\020"$\024"'\033\034\027\020#\024$#;\033\0353\030\034\025 
Figure 8 
heuristic to execute BC on just 4 workers in roughly two-thirds the time as the baseline using 8 workers providing users with cost-performance tradeoffs in a pay-as-you-go cloud environment. The automation offered by the adaptive heuristic to the end user also eliminates the guesswork of picking a static baseline or any potential non-uniformity in sampling using the sampling heuristic C  Given the need to run computation as a series of smaller \(optimally sized\at hs, is important to decide when we initiate the next swath. Our initiation heuristics attempt to overlap execution of multiple swaths to flatten the resource \(memory, network, CPU\ usage variations causes by different supersteps within a single swath. In BC and APSP, we observe a triangle waveform with a central peak; this heuristic is not relevant for applications like PageRank with uniform resource usage. Besides improving resource utilization, overlapping consecutive swath iterations also reduce the cumulative supersteps required and thus reduces the total overhead spent on synchronization between supersteps Figure 6 compares the relative performance of our initiation heuristics for the BC application normalized to a baseline approach that runs strictly  non-overlapping iterations. These run on 8 workers Figure 7 shows the corresponding messages transferred between supersteps over time, spanning swath iterations. The initiates a new swath every supersteps while the  performs initiation when it detects a peak in the number of messages exchanged. Static-Ns performance depends on the graph and the value of that is chosen.  If the average shorte st path is greater than  we will be initiating new heur istics before the previous swath has hit its peak, thereby exacerbating the resource demand. If the average shortest path length is well distributed or is \(just\ shorter than it leads to better performance. So 4 for the larger CP graph actually works best. Our dynamic he uristic eliminates this guesswork as it picks the initiation point at runtime without user input or graph preprocessing. Using this dynamic initiation heuristic we achieve up to 24 speedup vs. sequential initiation for the WG graph. The message transfer plot in Figure 7 corroborates this While sequential shows the message transfers peak and fall to zero \(thus showing more variability and poorer utilization\, Static-6 \(which is optimal but handselected\ maintains a higher message rate while dynamic is a bit more conservative, but automated VII E VALUATING I MPACT OF G RAPH P ARTITIONING ON P REGEL NET Our Pregel.NET framework is agnostic to how the graphs are partitioned and assigned to workers. The default mode performs a simple hash over the vertex ID to determine the target worker partition. Several works have shown that intelligent graph partitioning can improve the performance of distributed graph algorithms [19  26 a nd i t is relevant to examine if these benefits carry over to the Pregel/BSP model also METIS is a commonly used strategy that provides good quality in-place partitioning that minimizes edge-cuts across partition  Rec e n t w o rk o n a p pr o x im ate partitioning using a single graph scan offers an alternative for partitioning online as the graph is read from storage P a ge R a nk is o f t e n used in l iter a tur e  to validate the effectiven ess of these partitioning strategies. However, as we have seen, PageRank implemented using Pregel/BSP has a uniform message profile while BC and APSP have a triangle waveform message profile. We analyze the consequence of this on the performance gains from intelligent graph partitioning Clearly, the benefit of partitioning comes in reduced communication time since messages to remote vertices incur additional delay due to serialization and network I/O when compared to in-memory messages sent to local vertices. Since many distributed graph algorithms are dominated by communication rather than computation, partitioning can improve overall performance However, the barrier synchronization model in Pregel/BSP means that the total time spent in a superstep is determined by the slowest worker in the superstep. Hence, the balance of work amongst workers in a superstep is as import ant as the cumulative number of remote messages generated in a superstep. Since vertices communicate with their neighbors along edges in the Pregel/BSP model and partitioning seeks to collocate a majority vertex neighbors in the same partition, there may arise local maximas in specific partitions where more vertices are active during the course of execution of a graph application. This difference in workload can cause underutilization of workers that wait for over utilized workers at the superstep barrier  
Relative time taken by PageRank, APSP and BC to run on WG and CP graphs partitioned using METIS and Streaming, normalized to Hashing approach. Smaller is better  
  
sequentially Static-N heuristic Dynamic heuristic 
Swath Initiation Heuristics N N N N N  
211 


006 002\006 007\006 010\006 011\006 012\006 013\006 014\006 015\006 016\006 002\006\006 006 012\006\006 002\006\006\006 002\012\006\006 007\006\006\006 007\012\006\006 010\006\006\006 010\012\006\006 011\006\006\006 033\0353\0201 0\004 031"\020\033'\030\034\025 0#8\031\030\026\0309\033\031\030\024\034 004%\020\027!\031\030\024\034#&\030'\020#\(\035\020\027\003 024'2!\031\020\005:AB 033""\030\020"#5\033\030\031 8\031\030\026 006 006\003\012 002 002\003\012 007 007\003\012 010 010\003\012 011 011\003\012 012 0202#\014 0202#\015 031\0202#\016 031\0202#\002\006 0\020\035\035\033\025\020\035#\\020\034\031#-7#\004\033\0273#5\024",\020 0\030\026\026\030\024\034\035 017\020\021\022\023\024\024\025\026\020#;\033\0353\0201#\032\033"\031\030\031\030\024\034\030\034\025 5\006 5\002 5\007 5\010 5\011 5\012 5\013 5\014 006 006\003\012 002 002\003\012 007 007\003\012 010 010\003\012 011 011\003\012 012 0202#\014 0202#\015 0202#\016 0202#\002\006 0\020\035\035\033\025\020\035 020\034\031#-7#\004\033\0273#5\024",\020 0\030\026\026\030\024\034\035 017\020\021\022\023\024\024\025\026\020#0\004&:\#\032\033"\031\030\031\030\024\034\030\034\025 5\006 5\002 5\007 5\010 5\011 5\012 5\013 5\014 006 002\006 007\006 010\006 011\006 012\006 013\006 014\006 015\006 016\006 002\006\006 006 012\006\006 002\006\006\006 002\012\006\006 007\006\006\006 007\012\006\006 033\0353\0201 0\004 031"\020\033'\030\034\025 0#8\031\030\026\0309\033\031\030\024\034 004%\020\027!\031\030\024\034#&\030'\020#\(\035\020\027\003 024'2!\031\020\005:AB 033""\030\020"#5\033\030\031 8\031\030\026 006 006\003\012 002 002\003\012 007 007\003\012 010 010\003\012 011 011\003\012 012 0202#\016 031\0202#\002\006 031\0202#\002\002 031\0202#\002\007 0\020\035\035\033\025\020\035#\\020\034\031#-7#\004\033\0273#5\024",\020 0\030\026\026\030\024\034\035 027\030\031\022\032\033\031\020\034\031\035#;\033\0353\0201#\032\033"\031\030\031\030\024\034\030\034 025 5\006 5\002 5\007 5\010 5\011 5\012 5\013 5\014 006 006\003\012 002 002\003\012 007 007\003\012 010 010\003\012 011 011\003\012 012 031\0202#\016 031\0202#\002\006 031\0202#\002\002 031\0202#\002\007 0\020\035\035\033\025\020\035#\\020\034\031#-7#\004\033\0273#5\024",\020 0\030\026\026\030\024\034\035 027\030\031\022\032\033\031\020\034\031\035#0\004&:\#\032\033"\031\030\031\030\024\034\030\034\025 5\006 5\002 5\007 5\010 5\011 5\012 5\013 5\014 
We evaluate the impact of graph partitioning using the best-in-class METIS partitioner as well as the best heuristic \(linear-wei ghted deterministic, greedy approach partitioner from [26 an d com p a r e  them against a baseline that uses simple of vertices by their IDs. We run PageRank, BC and APSP over the WG and CP graphs on 8 workers for this evaluation. Hash, METIS, and Streaming produce 8 partitions whose percentage of remote edges are 87 18% and 35% for the WG graph and 86%, 17% and 65% for the CP graph; smaller this number, lower the edge cuts across partitions, and METIS proves a low edge cut for both graphs. Given the large sizes of the graphs, we run these experiments on the same set of vertices as our other experiments \(50 vertices for CP and 75 vertices for WG\. We report these results when using Pregel.NET without our swath heuristics however, the trends we observe are consistent even with heuristics turned on, though the absolute performance is uniformly better Figure 8 shows the relative time taken when using the METIS and streaming partitioning normalized to hashing for PageRank, BC and APSP running on WG and CP. We see that the WG graph sees a relative improvement of nearly 42-50% for METIS for the three applications, while this improvement drops to 24-35 for the streaming partitioning. When running Pregel.NET with heuristics turn ed on, we see a best case improvement of 5x in relative time taken by METIS for BC on WG compared to hashing \(graph not shown These are consistent with results reported in  However, we also see that the CP graph does not show such a marked improvement in performance due to better partitioning, despite its edge cut ratios from different partitioning being similar to WG. In fact hashing is faster than METIS and Streaming for APSP on this graph. It is worthwhile to investigate this consistent lack of improvement for the CP graph as opposed to WG. Figure 9 shows the runtime for BC broken into compute+I/O time and the synchronization barrier wait time components for the WG graph and Figure 12 does the same for CP. The plots also show the VM utilization %, calcul ated as the time spent in compute and I/O communication against the total time including barrier wait time\ on the secondary Y-axis We see that the VM utilization % for hashing is higher though the total time taken is also higher, for both WG and CP. METIS shows the inverse property, having lower utilization but also lower total time. This is explained by looking at the number of messages emitted by workers in a supe rstep for both hashing and METIS, shown in Figures 10 and 11 for WG, and in Figures 13 and 14 for CP. We expect that a hashed assignment of vertices to a partition would spread communication roughly evenly over all workers, while also increasing the number of remote communications required. The latter contributes to the increased total time while the former leads to a uniform number of    
Figure 9 Figure 10 Figure 11 Figure 12 Figure 13 Figure 14 
 taken for BC on a subset of with  shows the ratio of Compute+I/O time to total time   transferred by each worker in the peak supersteps of BC performed over using    transferred by each worker in the peak supersteps of BC performed over       taken for BC on a subset of with  shows the ratio of Compute+I/O time to total time   transferred by each worker in the peak supersteps of BC performed over using    transferred by each worker in the peak supersteps of BC performed over   
in-place streaming hashing 
Total time WG graph different partitioning Utilization Number of messages WG graph Hash partitioning Number of messages WG graph using METIS partitioning Total time CP graph different partitioning Utilization Number of messages CP graph Hash partitioning Number of messages CP graph using METIS partitioning 
212 


Bottom Top A B 
Figure 15 Figure 16 
messages seen for all workers in a superstep \(Figures 10 and 13\. When looking at the messages sent by workers in a superstep for METIS, we see that there are message load imbalances within work ers in a superstep, caused due to concentration of vertices being traversed in that superstep in certain partitions This variability is much more pronounced in CP as compared to WG \(Figures 11 and 14\ E.g. in superstep 9 for CP, twice as many messages \(4M\ are generated by a worker compared to another \(2M\.  For Pregel BSP, the time taken in a superstep is determined by the slowest worker in that superstep. Hence increase d variability in CP causes even good partitioning strategies to cause an increase in total execution time wh en using the Pregel/BSP model VIII A NALYSIS OF E LASTIC C LOUD S CALING  Cloud environments offer elasticity  the ability to scale-out or scale-in VMs on-demand and only pay for what one uses [28   On th e f l i p s i de  on e en ds u p  paying for VMs that are acquired even if they are underutilized. We have already shown the high variation in compute/memory resources used by algorithms like BC and APSP across different supersteps. While our earlier swath initiation heuristics attempt to flatten these out by overlapping swath executions, one can consider leveraging the clouds elasticity to, instead, scale up and down the concurrent workers \(and graph partitions\ allocated in each superstep The peak and trough nature of resource utilization combined with Pregel/BSPs synchronous barrier between supersteps offers a window for dynamic scaleout and in at superstep boundaries. Peak supersteps can greatly benefit from additional workers, while those same workers will contribute to added synchronization overhead for trough supersteps We offer an analysis of the potential benefits of elastic scaling by extrapolating from observed results for running BC on WG and CP graphs, using four and eight workers.  To provide a fair and focused comparison, we turned off swath heuristics in favor of fixed swath sizes and initiation intervals Figure 15 \(Bottom\ plots the speedup of BC running on eight workers when normalized to BC running on four workers, at corresponding supersteps.  The number of workers does not impact the number of supersteps We also plot the number of active vertices \(i.e. vertices still computing for a given swath\these supersteps which is a measure of how much work is required \(Fig 15 \(Top\. We find that we occasionally get superlinear speedup spikes \(i.e. >2x\ that shows a strong correlation with the peaks of active messages, for both WG and CP graphs. At other times, the sp eedup is sublinear or even a speed-down \(i.e. <1\responding to inactive vertices.  The superlinear speedup is attributable to the lower contention and reduced memory pressure for 8 workers when the active vertices peak \(similar to what we observed for the swath initiation heuristics Similarly, the below par speedup during periods of low activity is contributed by the increased overhead of barrier synchronization across 8 workers. Intuitively, by dynamically scaling up the number of workers for supersteps with peaking active vertices and scaling them down otherwise, we can leverage the superlinear speedup and get more value per worker Using a threshold of 50% active vertices as the threshold condition for between 4 and 8 workers in a superstep, we extrapolate the time per superstep and compared this to the fixed 4 and 8 worker runtimes. We also compute the best-case run time using an oracle approach to i.e. for each superstep, we pick the minimum of the 4 or 8 workers time.  Note that these projections do not yet consider the overheads of scaling, but are rather used to estimate the potential upside if we had an ideal or an automated heuristic for scaling. The total time estimates for running BC on WG and CP graphs, normalized to  
 plot shows speedup of 8 workers relative to 4 workers, for each superstep, when running BC on WG and CP graphs plot shows the number of vertices active in that superstep Estimated time for BC using elastic scaling, normalized to time taken for 4 workers. Normalized cost is shown on secondary Y axis WG graph shown on left CP graph shown on right. Smaller is better 
022\011 022\010 022\007 022\002 006 002 007 006 002 007 010 011 012 013 014 015 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 027\031\030\037\020#@\020"\031\030\027\020\035 0201!2#\024$#\015#5\024",\020"#\017\003"\003\031\003#\011#5\024",\020"\035 024"'\033\026\0309\0201#\\031\020 2 035#\032\020"#+!\034 017\020\021\022\023\024\024\025\026\020 027\030\031\022\032\033\031\020\034\031\035 017\020\021\022\023\024\024\025\026\020#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 027\030\031\022\032\033\031\020\034\031\035#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 036\030\034\020\033"#\\0201!2 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 017\020\021\022\023\024\024\025\026\020#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035\031 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 002\003\011 002\003\013 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 033\026\030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 027\030\031\022\032\033\031\020\034\031\035#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035 031 
 
dynamically scaling ideal scaling 
Our hypothesis is that an intelligent adaptive scaling of workers can achieve a similar performance as a large, fixed number of workers, but with reduced cost 
213 


Nature Nature Ecological Applications Nature ACM International Conference on Management of Data \(SIGMOD In Parallel Object-Oriented Scientic Computing \(POOSC Science Communications of the ACM ACM Workshop on Mining and Learning with Graphs Communications of the ACM HotCloud Proceedings of the 19th ACM International Symposium on High PErformance Distributed Computing HPDC Knowledge and Information Systems KAIS International Conference on Computational Science IEEE International Conference on Cloud Computing Technology and Science ACM/IEEE Conference on Advances in Social Network Analysis and Mining \(ASONAM IEEE International Parallel and Distributed Processing Symposium \(IPDPS International Conference on Distributed Computing and Networking Journal of Mathematical Sociology International Conference on Parallel Processing Communications of the ACM 
 
observed time taken using 4 workers, are plotted in Figures 16\(A\ and 16\(B We see that our dynamic scaling heuristic using the percentage of active vertices achieves nearly the same CP\ or better \(WG\ performance as a fixed 8 worker approach. Clearly there is benefit of using fewer workers for low utilization su persteps to eliminate the barrier synchronization overhead. Also, the dynamic scaling heuristic performs almost as well as the ideal scaling. Finally, when we consider the monetary cost of the proposed approaches, assuming a pro-rata normalized cost per VM-second plotted on the secondary Y axis, we see that dynamic scaling is comparable \(CP\ or cheaper \(WG\ than a 4 worker scenario while offering the performance of an 8 worker deployment IX C ONCLUSION  In conclusion, we introduce optimization and heuristics for controlling memory utilization and show they are critical to performance.  By breaking computation into swaths of vertices and using our sizing heuristics we achieve up to 3.5x speedup over the maximum swath size that does not cause the a failure.  In addition overlapping swath executions can provide a 24% gain with automated heuristics and even greater speedup when a priori knowledge of the network characteristics is applied This evaluation offers help to eScience users to make framework selection and cost-performancescalability trade-offs. Our he uristics are generalizable and can be leveraged by other BSP and distributed graph frameworks, and for graph applications beyond BC. Our work uncovered an unexpected impact of partitioning and it would be worthwhile, in future, to examine the ability to pred ict, given certain graph properties, a suitable partitioning model for Pregel/BSP It may also be useful to perform such evaluations on larger graphs and more numbers of VMs. At the same time, it is also worth considering if non-linear graph algorithms are tractable in pr actice for large graphs in a distributed environment B IBLIOGRAPHY  1  F  L i lj er os C   Ed l i n g L  A m a r a l H  S t an ley   and Y    berg The web of human sexual contacts 
vol. 411, pp. 907908, 2001   H Je o n g  S   Ma so n A  L   B a ra b s i  a nd Z   Oltva i  L e t ha l i t y  and centrality in protein networks vol. 411, pp. 41-42 2001   O. B o din and E   E s t r ada    U s i n g n e t w ork c e nt r a l i t y  m e a s ures t o  manage landscape connectivity vol 18, no. 7, pp. 1810-1825, October 2008   D. W a ts s  and S  S t r ogat z  C olle c t i v e  d y nam i cs of  s m a ll-w orl d   networks vol. 393, no. 6684, pp. 440442, June 1998   G  Ma lew i c z   M A u s t er n A   Bik  J   Dehn er t I  Hor n   N. L e i s er and G. Czajkowski, "Pregel: A system for large-scale graph processing," in 2010   D. G r egor  and A  L u m s dain e  T h e  pa r a llel  B G L  A gen e r i c  library for distributed graph computations," in 2005   B. S h a o  H. W a n g  and Y  L i T he T r init y G r aph E n g i n e    Microsoft Research, Technical Report MSR-TR-2012-30, 2012   A  F ox  C lo ud c o m putin g w h at  s  in it for m e  as  a  s c i e n tis t     vol. 331, pp. 406-407, 2011   S. G h e m a w a t  and J  De an   Map re duc e s i m p lifi e d data  processing on large clusters vol 51, no. 3, pp. 107-113, 2008   J  L i n and M. S c hat z   Des i g n  patt er n s  for eff i ci ent gr aph algorithms in MapReduce," in 2010   L   Va l i ant   A b r id g i n g m o d e l f or pa r a llel com putati o n  vol. 33, no. 8, pp. 103-111, 1990 12 a c h e  Ha ma    O n l i n e    http://hama.apache.org   13 Ap a c h e  Ha d o op    O n l i n e    http://hadoop.apache.org     M Z a h a r i a, M. Ch ow dhu ry M F r ank l in S  S h e n k e r, and I   Stoica, "Spark: Cluster Computing with Working Sets," in 2010   J  Ekana y ak e e t a l     T w i st er A  r untim e f o r it er ati v e  MapReduce," in Chicago, 2010, pp. 810-818   U. K a n g  C  T s o u rakakis   and C. F a l outs o s  Peg a s us   Minin g  Peta-scale Graphs," in 2010   M. P a c e  B S P vs  MapR e duc e    in vol. 103.2081, 2012   S. Seo  E  Yoo n, J  K i m  S  J i n  J-S. K i m   and S   Ma e n g HAMA: An Efficient matrix computation with the MapReduce framework," in 2010, pp. 721-726   S. S a l i h ogl u  and J  W i d o m  G PS A G r a ph P r oc e s s i n g Sy s t em    Stanford University, Technical Report 2011   R L i cht e n w a l t e r and N   Cha w la D is Ne t  A fr am ew ork for  distributed graph computation," in  2011   K  Maddu r i  D. E d i g er K   J i an g  D. Bad e r  and D  Cha v a r riaMiranda, "A faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets," in 2009   E  K r e p s k a, T  K i el m a nn, W  F o kkink, H   Ba l, "A  hi g h level framework for distributed processing of large-scale graphs," in 2011, pp. 155-166   L   Pa ge  S  B r in R. M o t w ani and T  W i nogr ad  T h e P a geRank citation ranking: Bringing order to the web," Stanford InfoLab Technical Report 1999-66, 1999   U  Brand  s  A f a s t er  a l gor ith m for  b e t w eenn e s s c e nt r a l i t y    vol. 25, no. 2, pp. 163-177 2001   Stan fo r d  Net w or k A na l y s is Pro j e c t  O n l in e    http://snap.stanford.edu    I  S t ant o n and G  K l i o t, "S t r e a m i n g G r aph P a rtiti o n in g  for L a rge Distributed Graphs," Microsoft Corp., Technical Report MSRTR-2011-121, 2011   G   K a ry pis and V   K um a r A fas t and hi g h qua l i t y m u l t i l evel scheme for partitioning irregular graphs," in 1995, pp. 113-122   M. A r m b r u s t e t  a l   A v i ew of  c l o u d  c o m putin g    vol. 53, no. 0001-0782, pp. 50-58 April 2010  
214 


  13  or gani c  c he m i s t r y  i n our  Sol ar  Sy s t e m       Xi a n g  L i r e c e i v e d h i s B  S   m is tr y  fr o m  th e  P e k in g  U n iv e r s ity  C h in a  in  2 0 0 3  and P h D   i n P hy s i c al  C he m i s t r y  f r om  t he  J ohns  H opk i ns  Un i v e r s i t y  i n  2 0 0 9   He  h a s  b e e n  a  R e s e a r c h  A s s o c i a t e  wi t h  a  j o i n t  a p p o i n t m e n t  a t  t h e  U n i v e r s i t y  o f  M a r y l a n d   Ba l t i m o r e  C o u n t y  a n d  N AS A G o d d a r d  S p a c e  Fl i  Ce n t e r  s i n c e  2 0 1 1   H i s  r e s e a r c h  f o c u s e s  o n  t h e  d e t e c t i o n  of  t r ac e  e l e m e nt  and as t r obi ol ogi c al l y  r e l e v ant  or gani c  mo l e c u l e s  i n  p l a n e t a r y  s y s t e ms   l i k e  M a r s   He  i s  es p eci a l l y i n t er es t ed  i n  t h e d evel o p m en t  o f  T i m e of  and I on T r ap m as s  s pe c t r om e t e r s w i t h v a r i o u s i o n i z a t i o n  ng te c h n iq u e s   Wi l l  B r i n c k e r h o f f  sp a c e  sc i e n t i st  i n  t h e  Pl a n e t a r y  En v i r o n m e n t s  La b  a t  N A S A  s  G o d d a r d  Spac e  F l i ght  C e nt e r  i n Gr e e n b e l t   M D w i t h  pr i m ar y  r e s pons i bi l i t y  f or  th e  d e v e lo p m e n t o f th e  L D TO F  m a s s  s p e c t r o  th is  p r o je c t H e  h a s  fo c u s e d  re c e n t l y  o n  t h e  d e v e l o p m e n t  o f  m i n i a t u re  l a se r d ma s s  s p e c t r o me t e r s  f o r  f u t u r e  p l a n e t a r y  mi s s i o n s  a l o n g  wi t h  b a s i c  e x p e r i m e n t a l  r e s e a r c h  i n  a s t r o b i o l o g y  a n d  p r e bi ot i c  s y nt he s i s   D r   B r i nc k e r hof f  i s  i nv ol v e d i n t he  de v e l opm e nt  of  m as s  s pe c t r om e t e r  f or  bot h t he  2011 Ma r s  S c i e n c e  L a b o r a t o r y  a n d  t h e  2 0 1 8  E x o Ma r s  mi s s i o n s   


  14   


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a key, value list using an XSTL  Queries made against this list of key, value pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


