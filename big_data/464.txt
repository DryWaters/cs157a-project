Multi-target Data Association Approach for Vehicle Tracking in Road Situation Hongshe Dang, Chongzhao Han Absfracf  Data association was an important content in Multi-target tracking Typical algorithms to deal with such problems are the joint probabilities data association filter JPDAF proposed by Bar-Shalom and his team The basis of JPDAF is the calculus of the joint probabilities hrhveen the measurements and the tracks The algorithm assigns weights for reasonable measurements and uses B weighted centroid of those measurements to update the track Based on the evidence theory 
and furzy mathematics, a new data association method behveen the tracks and the measurements is proposed in the paper The mass function is determined by using fuzzy mathematics and a belief matrix is produced according to the evidence combination rule then the decision is made by means of the maximum belief The Monte Carlo simulation results indicate that the new method has a good association capacily Compared with cheap JPDAF method the new method has improved the track accuracy Kqwords Multi-target tracking data association evidential combination fuzzy mathematics 1 
Introduction A significant problem in multiple target tracking is the measurement-to-track data association The problem was derived from Sittler's work I and there have many different algorithms Typical algorithms to deal with such problems are the probabilities data association filter PDAF in case of single target tracking and the joint probabilities data association filter JPDAF in case of multi-target tracking proposed by Bar-Shalom and his team I The basis of JPDAF is the calculus of the joint probabilities between the tracks and the measurements The algorithm assigns weights for reasonable measurements and uses a weighted centroid of 
those measurements to update the track However the calculation of joint probabilities in JPDAF seems complicated even ifthe used formulas are well established in probability theory this motivates some authors to investigate simplifications of these formulations This includes for instance Fitzgerald's ad hoc proposal 3 Manuscript received Mar 17.2003 Hongrhc Dang is with the Institute of Synthetic Automation Xi'an Jiaotong University Xi shm xi 710049 P R China c-mail:hsdana~~ohurom Choogrhao Han is n'ilh the Institute of 
Synthetic Automatioh Xi'an liaatong Univcniv Xi'an Shm xi 710049 P R China e-mail:~an~mail.gltuedu.fn 0-7803-8125-4/03/$17.00 0 2003 IEEE which seems to work well in some cases of crossing targets and the computation cost is smaller than standard JPDAF method Another weak of JPDAF is that the JPDAF method tends to result in track convergence for closely spaced targets 4 and in road situation the targets are generally close to each other Recently people working in the fusion of uncertain data have been interested in the Dempster-Shafer theor essentially 
because of two of its tools I a nice and flexible way to represent uncertainty be it total ignorance or any form of partial or total knowledge that is more general than what the probabilistic approach provides and 2 a rule to combine uncertain data called the Dempster's rule of combination that seems to provide an excellent tool for data-aggregation In order to treat with the uncertain and inaccuracy of sensor data Rombaut[S and Gruyer[6 using fuzzy logic and Dempster-Shafer theory to rebuild the environment for intelligent vehicles and to increase the reliability of environment perception 
In this paper the Dempster-Shafer theory was used in the data association process to solve the problem of correlation between the tracks known targets and the measurements new targets The Monte Carlo simulation proves that the tracking accuracy has been improved compared with the cheap JF'DAF method The proposed method can adaptive to the situation that the number of targets is uncertain which is often barging up against in reality This paper is organized as follows Section 2 introduces problem description and JPDAF method Section 3 presents the evidence combination method for data association Section 4 gives the 
realization of the proposed method Section 5 the simulation results of four targets tracking in clutters are given Section 6 presents the conclusions 2 Problem description and JF'DAF methodology Suppose the discrete state equation of each target model is described by x'\(k 1  A'\(k k d k k f  l,z...,z 1 The measurement equation is 2 Where x'\(k+l is the n-dimensional state vector at time k represents the relative position relative velocity and z\(k  H'\(k k  v'\(k 379 


relative acceleration between the targets and the self vehicle k\(k is the transition matrix Z\(k is the measurement vector at time k H'\(k is measurement gain matrix d\(k and k are process noise and measurement noise which are independent zero mean and known variance E[w'\(k w'Ci Q'\(k 3 E[v'\(k v'\(i  R'\(k 4 Where 6 is the Kronecker delta function The set of validated measurement received at time k is defined as:Z'\(k k m is the numbers of validated measurement that is there are m measurement inside the tracking gate If we use kalman filter to estimate the state of targets, then the update equation is Where kl k is the state estimation vector of target t at time k and i'\(k1k-I is the state prediction vector oftarget t v'\(k known as the combined innovation  i'\(k I k  i'\(k I k-1 v k k t  l 5 Where k is the joint probability between the measurement jand the target t k is the innovation between the measurementjand the target t v k  z k 3 k 1 k  1 j=l&.p 1=1&.,z 7 i\(k represents the kth measurement vector of target t at time k  i'\(k1k-I is the prediction measurement vector of target t W\(k is the gin matrix ofkalman filter N k  P\(k I k l KP k I k-l  R k 8 The error covariance is P\(kIk k klk-l The predictions of state vector error covariance and measurement vector are P k I k 1  Al\(k  l k 1 I k 1 k-I p 12 2 k I k-I  A\(k 02 k-1 I k-I  I3  14 2 k I k-I  A'\(k k I k-I The general approach to calculate the joint probability in JPDAF methodology is firstly determining the validated matrix according to gate techniques then calculating the conditional probability of all possible feasible events, and finally the posterior probability g\(k is produced When the number of targets is big the computation cost in making the feasible events is too big for some applications so some sub-optimize JPDAF method were introduced among them Fitzgerald 3 gave the cheap JF'DAF method and the formula of joint probability is 16 However the computation cost of the cheap JPDAF method is also too big for some applications such as target tracking in road situation in order to handle the multiple target matching problem in automotive radar tracking we tried to use evidence theory determining the relation between the tracks and the measurements by using evidence combination and compared with the cheap JPDAF method, the result is satisfied 3 The data association based on evidence theory 3.1 Evidential reasoning method Let be a finite set of mutually exclusive and exhaustive events or propositions of particular experiment called the fnme of discernment The distribution of mass is defined on the set 2 of the subsets of   And m  2  0,1 is called mass function or basic probability assignment BPA defined for every hypothesis A  the value m\(A satisfies the following conditions l P  0 2 24'4  1 As Evidential theoly proposes a combination rule, called Dempster's rule of combination which synthesizes basic probability assignments and yields a new basic probability assignment representing the fused information The combination rule is known as the orthogonal sum 380 


Let 2241 and ni2 be the basic probability assignments on the same frame of discernment for belief functions Bel and Bel respectively If focal elements of Bel are B  B and the focal elements ofBeI,areB E  the total portion of belief exactly committed to A A is not an empv set is given by the orthogonal sum 0 A  m k  Cm,\(A  BJ 1 A,nX,=4 Where k is called the normalization factor which is often interpreted as a measure of conflict between the different sources 3.2 The framework of discernment In order to use the evidence theon a reference discernment framework must be constructed firstly For example. suppose there are three known targets in front of our vehicle at time k  and a measurement was detected by a millimeter wave radar or a laser radar In order to determine the relation beween the known targets and the measurement new targets a framework of discernment 6 is built as 0  y Y  e where Y means that the new measurement is original from the known target j  that is the measurement is a measurement of target j  In order to be sure that the framework of discernment is really exhaustive a last hypothesis noted 224*\223 is added which means that there no known targets associated with the measurement, that is the measurement is original from a new target or a known target had disappeared The possible state may he 18 The definition of the mass function basic m,,,\(Y the mass function of the proposition that the measurement X is original from the known target j  m  the mass function ofthe proposition that the measurement Xi is not original from the known target j  probability assignment  the mass function of the proposition that it is ignorance whether the measurement X is original from the known target j or not mi  The mass function of the proposition that the measurement X is not original from any known target, that is the measurement has no relation with any known targets In this mass distribution first index i denotes the measurements and the second index j the known objects predictions If one index is replaced by a dot, then the mass is applied to all measurement or known objects according to the location of this dot 3.3 Evidence combination According to the Dempster\221s rule, the combination result of 17 mass sets is produced in cascade n7,..\(1  Ky,\(Y  nI,.t\(Yi   19 k=l  n 3.4 Processing of the uncertainty Because of the complexity of environment and the inaccuracy of sensors there are some confliction and ambiguity in data association such as there are one known target or one measurement is in relation with two measurements or known targets etc In order to solve the conflicts, and get a reliable decision, the decision is made as follows First two belief matrixes are got by exchange the position of the measurement and the tracks, and make a local decision according to the maximization rule And there are some decision conflicts such as the first maximization gives a decision that the measurement x is original from the known target j  but the second maximization may gives a contradictory decision Then the second decision process is introduced that if the decision from the first matrix is consistent with that of the second matrix the decision is validate otherwise the decision is ignored 381 


3.5 The mass functions to be combined are generated according to the fuuy similarity between two vectors Then take a transform to get the mass function Where the 3 represents the fiuzy similarity of two vector and d is the distance form of 3 finally the mass function is generated from d8  The formulation is as follows Generation of mass function 7 23 dt,j  2\(1 3  11 24 m1 0  1  a0 Realization of the proposed method 27 4 The realizations of data association based on D-S theory are as follows Step 1 Starting from previous estimation X\(k-llk-I and P\(k-Ilk-I for t=l to T  determine the prediction X'\(k 1 k  1 and P'\(k I k  1 using Equ.12 and Equ.13 Step 2 Calculating mass hnctions behveen the prediction vector X  Xl,x2  xi and the measurement vectorY  yl y2  y  Step 6 Updating the state and covariance by using Step 7 Repeat Steps I  6 for different time Equ.5 to Equ.11 m  1 increments 5 Simulation results The aim in this section is to test the performance of the proposed method and compared with the cheap JPDAF method The model trajectories of the four tracks[8 are shown in Fig.1 along with the noisy range and clutter measurements\(with the line represents the actual track and  represents measurements  represents clutter The state equation of the target is  where x\(k represents the state vector composed of position and velocity between the target and the self vehicle The four targets have the same state transition and measurement transition matrixes, the deviation of the measurement noise has been selected as 6  0.5m and sample intetval is T  0 Is To evaluate the performance of the proposed method two criteria were used one is the Root of Mean Square error one is the Time Mean error. generally speaking the two criteria can evaluate the tracking performance  accurately The definition of RMS error is 28 Where dk 4 k stand for the true state of target and its estimation in the running j h at time k  f is the times ofMonte Carlo running The definition of TM error is IN N x Step 3 Calculate the belief matrix M between the T  Rb[~-error\(k 2 measurement vector Yand the track vector X according to the mass functions Where N represents the sample number     100 Step 4 Calculate the belief matrix M between the track vector X and the measurement vector Y I according to the mass functions Step 5 Finally decision The correlation between the measurement vector and the track vector is similarity u,ith the correlation between the track vector and the measurement vector The matrix M and M can represent the relations between the track and the measurements First a local decision with the I  matrix Mi or M.,j was made according to the lime a maximization rule Then if the decision fm,m the first matrix is consistent with that of the second matrix the decision is the finally decision, otherwise the decision is ignored Fig.1 noisy track trajectories and clutter signal used in the simulation The monte car10 simulation has been tried for M=50 Fig2 and Fig.3 depict the RMS position error and 382 


Fig.4 and Fig3 depict the RMS velocity error T represents the TM error of target j  0 71   T2iO.21    ii  2     0 time IS Fig Position error of the proposed method r D w.2 s bme 1s Fig3. Position error ofthe cheap JPDAF method I  Fig4. Velocity error of the proposed method From the above results we know that the proposed method can get a high tracking accuracy for multiple targets tracking in road situation compared with the cheap JPDAF method The data association method using evidence theory is effective and easy to realize in cluner environment 6 Conclusion The accurate tracking of multiple vehicles is a very important consideration in the design of active safety system In this paper a data association method based on evidence theory and fuzzy logic is introduced, which uses fuzzy logic to determine the mass functions, combines the mass functions by using the Dempster-shafer's rule finally makes the decision according to the mayimization rule The proposed approach can avoid some problems encountered by other same kind of algorithms like PDAF which is not adjusted to target crossings either the JF'DAF which take into account a fixed number of targets and doesn't initialize new tracks. Monte Carlo simulations have been provided to evaluate the performance of the method and it is proved that the proposed method is simplicit feasibility and efficiency compared with the cheap PDAF method 7 References I Sittler R W An optimal data association problem in surveillance theory IEEE Trans on Military Electroni~s.l964~8\(2 125 139 121 Bar-Shalom Y Tse E Tracking in a cluttered environment with probabilistic data association Automatica.l975 I l\(9 451-460 3 Fizgerald R Development of practical PDA logic for multi-target tracking by microprocessor American control conference Seattle Washington 1986.889 897 4 Blackman S S Popoli R Design and analysis of modem tracking systems Boston Anech Hourse.1999 SI Gruyer D Berge-Cherfaoui V Multi-sensor fusion approach for driver assistance systems IEEE intemational workshop on Robot Human Interactive Communication 2001,479-485  Rombaut M Decision in multi-obstacle matching process using theory of belief AVCS'98 Amiens France JUI i-3,1998  Rombaut M, Berge-Cherfaoui V Decision making in data fusion using Demuster-Shafer's theory 3th  IFAC Symposium on Intelligent Components and Instrumentation for Control Applications France June 1997 8 Lee M S Kim Y H New data association method for automotive radar tracking IEE Proc.-Radar Sonar Navig. 2001 148\(5 296-301 lime I FigS Velocity error of the cheap PDAF method 383 


find the value w so that the split by the test Did G 5 w offers the minimum gini index A straight-forward approach is to reorder all the points by their distances to the center 6 and compute the gini in dex by scanning the ordered points. This is costly for large datasets. Another approach is to discretize the distance into intervals and for each interval we keep the counts of posi tivehegative cases whose distance to are in that interval One shortcoming of this approach is the loss of accuracy due to discretization Our approach, outlined in Algorithm 3 avoids reorder ing all the data and any loss of accuracy. This is achieved by making the followin two observations: i if point d is close to centroid  then di the coordinate on the i-th dimension must also be close to i and ii\the best splitting position should be close to the boundary of the cluster Let N be the number of dimensions with non-zero weights \(clustered dimensions\Let D be the set of points t!at are within an initial radius T  6 to 5 For any point d E D the inequality 2iji\(di  6 5 T must hold for each clustered dimension i With the ordered attribute lists it is easy to find D points that satisfy the inequality on all the N clustered dimensions Obviously D 2 D for D can contain points whose distance to p'is up to fl After sorting D by distance we compute the gini index up to radius T and we keep the points in D  D and discard D We then increase the radius T by 6 and repeat the process However we do not have to consider all the points We are computing the gini index based on the distance to the cluster centroid we found Thus we expect a good gini index near the boundaries of the cluster. According to the weighting scheme discussed in the previous subsection Gi is set to for each clustered dimension i where i is the span of the points on that dimension Thus we have Gi\(di  5 1 for any point p'that is inside the cluster In additjon to these points we consider all points that satisfy G,\(di  6 5 2 on each dimension i Thus, the maximum 5.4 SSDT Examples Let us review the two problems in Section 1 Un like the C4.5 decision tree which fails to detect pattern hot high and builds a trivial decision tree in Fig ure l\(b  the SSDT algorithm accurately captures the pat tern and constructs a compact decision tree in Figure 4\(a The second problem is introduced by datasets with biased class distribution The decision tree model shown in Fig ure 1 d used 11 tests to classify a 2-dimensional dataset shown in Figure l\(c SSDT, shown in Figure 4\(b uses only 4 tests Apparently such differences tend to be more significant if the dataset has more than 2 dimensions Algorithm 3 distance-entropy\(Dataset S Centroid  Weight 5 1 Tt6 2 N t  of dimensions with non-zero weights 3 repeat 4 5 6 7 8 end for 9 10 for each relevant _dimension i do find instances d that satisfies r  6 5 t  6  T using the ordered attribute lists dlcount t dlcount  1 check ifd satisfies all the inequalities add dto the ordered set D if dlcount  N compute gini-index for splits by distance up to r remove in tree D the branch that represents data cases within distance T top 11 TtT+6 12 until T  W 13 return I and v n D\(hot,high a\SSDT of Figure l\(a b SSDT of Figure l\(c Figure 4 SSDT for datasets in Section 1 6 Evaluations We evaluate the SSDT algorithm in various aspects We study the size of the decision tree generated by the algo rithm, the influence of the biased class distribution the ac curacy of predictions, as well as the efficiency and scalabil ity issues. The tests were performed on a 700-MHz Pentium I11 machine with 256M of memory, running Linux Synthetic Data Generation We generate synthetic data in d-dimensional spaces with two class labels, positive and negative Points have coordinates in the range of 0,1 and positive points account for p  1  10 of the total data To generate clustered points in subspaces we use a method similar to  11 The difference is that the number of positive points is controlled by the biased class ratio Our method takes 4 parameters n the number of clusters IC a Poisson parameter that determines the number of relevant dimensions in each cluster p the percentage \(biased ratio of positive points and N the total number of points First we determine the subspace for each cluster. For a given cluster i the number of relevant dimensions Si is 541 


picked from a Poisson distribution with mean k However an additional restriction 2 5 Si 5 d must be observed We generate centroid 7i for each cluster i We simply generate a uniformly distributed point in the d-dimensional space We then decide the spread \(radius of the cluster on each dimension We set ij  0.5 for irrelevant dimension j For a clustered dimension we fix a spread parameter s and choose the radius Gj E 0 s uniformly at random. For our data generation we use 3 values for s  l 2 5 We generate positive points in each cluster i in two dif ferent ways i points are distributed uniformly in the re gion; ii\for each dimension j coordinates of points on the dimension follow a normal distribution with mean cj and variance ej We determine the size of each positive cluster by Ni  pN where wi is the volume of cluster i defined as vi  n,d=1\(2  rij Finally we generate 1  p negative points The negative points either i uniformly distribute at random in the entire space or ii\form clusters in subspaces and are generated by the method described above with parameters k  0.8d and s  0.5 If a negative point is generated inside one of the positive clusters it is discarded with a probability B For our data generation we choose 6  0.5 l 0.3 0.66 Experiments Tree Size and Scalability We generate 6 clusters Table 1 of positive points out of a training set of total lOOK records and 10 attributes The total number of positive cases account for 2 of the training set The av erage radius of the cluster on each clustered dimension is 0.05 and the negative points are uniformly distributed at random. The split at the root of the decision tree, for exam ple uses the distance function defined for Cluster 6 Total 5 clusters are used at different nodes for splitting, resulting in a tree of 37 leaf nodes before pruning, while the SPRINT algorithm uses 7 1 leaf nodes before pruning 121 1 L I Table 1.5 clusters are detected by SSDT Un clustered dimensions are denoted by 222-\222 We compare the size of the decision tree in terms of number of leaf nodes generated by SPRINT and SSDT The datasets we use have 10 attributes and the 5 clusters of biased points account for 1 2 and 5 of the total data. Figure 6\(a indicates that trees built by SSDT are sig nificantly smaller, and the sizes of the trees generally do not increase as the training sets become larger Next we vary the number of clusters in the training sets and show the results in Figure 6\(b The datasets are gen erated with the same class ratio 2 The size of the tree increases significantly as there are more positive clusters in the dataset. The trees generated by SSDT are much smaller Figure 6\(a shows the scalability of SSDT as the size of the dataset increases from 0.1 to 2.5 million The dataset has 10 attributes 8 clusters with an average dimensional ity of 4 and a biased class ration of 1 The execution time increases linearly with the size of the dataset since SSDT is able to detect the clusters and the resulted decision tree has similar heights which means the number of passes through the database does not change Figure 6\(b shows the scalability of SSDT when the average dimensionality of the positive clusters is increased from 2 to 12 The dataset used in the test has 1 million records 8 clusters 1 posi tive ratio and a total of 20 attributes. It indicates that cluster dimensionality has little impact on the performance We study the impact of the number of positive clusters on the scalability In Figure 6\(c we increase clusters from 4 to 20 The dataset has 1 million records, among which 1 are positive There are 10 attributes and the clusters have an average of 5 dimensions Since the number of positive data cases is kept unchanged during the test, each cluster contains fewer records as more clusters are used The curve is steeper than in the previous cases because more scans of the dataset have to be performed In Figure 6\(d using the dataset of the same size, dimensionality and 8 positive clusters we found the performance is stable We compare the performance of SSDT with SPRINT in Figure 6\(c The datasets have 10 attributes 8 positive clusters and a class ratio of 1 In this case SSDT has an advantage over SPRINT because SSDT trees are much smaller As we increase the class ratio and the number of clusters SPRINT becomes faster than SSDT Indeed SPRINT is 20 faster than SSDT when there are 20 clusters with a 15 class ratio which means SSDT works best with biased class distributions The association rule algorithm for mining datasets with biased class distribution does not scale well. Overall SSDT is an efficient and scalable algo rithm, despite the multivariate search it performs 7 Conclusion We presented a novel decision tree algorithm The key idea is to take advantage of the subspace clusters formed by 548 


20 2w I80 160 I40  120 5 1W 2 60  Bo 40 20 I 7W Bw 500 4w H 3w 2w 1W I 0 2MM WXa w 8woo lwwo 4 8 8 10 12 I4 16 0 05 I 15 2 25 lofnmdr  of FasdlY dYI1.R of rronlr NI rnllrnS a  of leaf nodes v training set size b  of leaf nodes v  of clusters c Execution Time Figure 5 Experiments and Comparisons mi      2  8 8 1011 Ddw"ta\(.i@l-pst lol&\(lombnr dnmmyld-a tdh.fnol Wmpns a  of records b dimensionality c  of clusters d class distribution Figure 6 Scalability the data in the biased class Once these subspace clusters are efficiently detected a compact and accurate decision tree can be constructed by splitting a node based on the distance to such clusters Our multivariate decision tree algorithm has proven to be scalable and efficient Indeed it has better performance over SPRINT for very skewed distributions References I C. C Aggarwal C Procopiuc J Wolf P S Yu and J S Park Fast algorithms for projected clustering In SIGMOD 1999 2 R. Agrawal J Gehrke D Gunopulos and P Raghavan Au thomatic subspace clustering of high dimensional data for data mining applications In SIGMOD 1998 3 R Agrawal and R Srikant. Fast algorithms for mining asso ciation rules In VLDB 1994 4 J Bioch 0 van der Meer and R. Potharst Bivariate de cision trees In Principles of Data Mining and Knowledge Discoveq 1997 5 L Breiman J Friedman R Olshen and C Stone Class cation and Regression Trees Wadsworth, 1984 6 C E Brodley and P E Utgoff Multivariate versus uni variate decision trees In Technical Report COINS-CR-92 8 Dept of Computer Science University of Massachusetts 1992 7 J Gehrke V Ganti R Ramakrishnan, and W Loh Boat optimistic decision tree construction In SIGMOD 1999 8 J Gehrke R Ramakrishnan and V Ganti Rainforest A framework for fast decision tree constructionof large datasets In VLDB 1998 9 G Guiffrida W W Chu and D M Hanssens Mining clas sification rules from datasets with large number of many valued attributes In EDBT pages 335-349,2000 IO Y Ma B Liu C K Wong P S Yu and S M Lee Tar geting the right students using data mining In SIGKDD Zurich, Switzerland, August 2000 ll S K Murthy S Kasif and S Salzberg A system for in duction of oblique decision trees In Journal of ArtiJicial Intelligence Research pages 1-32 1994 12 C Shafer R Agrawal and M Mehta Sprint A scalable parallel classifier for data mining In VLDB 1996 13 P E Utgoff and C E Brodley An incremental method for finding multivariate splits for decision trees In ICML pages 14 H Wang and C Zaniolo CMP A fast decision tree classi fier using multivariate predictions In ICDE pages 449-460 2000 I51 D Wilson and T Martinez Improved heterogeneous dis tance functions In Journal of ArtiJicial Intelligence Re search pages 1-34 1997 I61 B Zadrozny and C Elkan Learning and making decisions when costs and probabilities are both unknown In Technical Report CS2001-0664 Dept of Computer Sci UCSD 2001 58-65 1990 549 


Figure 7 Correlations between query con straints and new indexed constraint the index attribute may have different ranges We iden tify all possible constraint introduction solutions as follows Algorithm For each constraint on the index attribute CO.Roj Step 1 find the least expensive association from a query constraint to i.e find the incoming link C~.R  CO.Roj with the fewest exceptions For exam ple if the cardinalities of El     E5 are nl   n5 and n4 5 nz _ n3 then the least expensive association for CO.R is c6.b  CO.R~,\(E  E4 Step 2 filter out all the exceptions that do not satisfy at least one of the query constraints C1.Rl    C12 We do not need to test the exceptions for the antecedent of the corresponding rule since they satisfy it by definition The constraint introduction solutions identified in our example are the following  Introduced Constraints Exceptions ele t El Cz.n,\(e    C12.nl2\(e el  E4 C1.h e    C5.nS e C~.R e     C12.nll e ele  E5 C1.nl e     C5.nS e C~.R e    C12.n12 e CO.Rol CO.RO2 CO.RO3 5 Optimizing OQL queries In the previous sections a series of algorithms were given to find a collection of constraint elimination or con straint introduction solutions In this section we show how the original query is transformed to its optimized form using the optimal solution. Consider the OQL query select x fromExtentX as x where C~.R and   and Cn 5.1 Heuristic H1 Let  Cil   Ci,},Ei  be the maintained con straints and the exceptions of a solution i Only the main tained constraints of the optimization solution should be tested on the objects of the whole extent however all the constraints should be tested on the exception cases and the objects that satisfy the maintained constraints but not the ones omitted should be removed from the result Hence the original query should be converted to the following one select x from Extent2 as x where Cil and   and Ci except Ei If we assume that tests on the query constraints take roughly the same time, the optimal solution is the one with fewest exceptions Ei 5.2 Heuristic H2 Let  Co.Roi Ei  be the index constraint and the corre sponding exceptions of a constraint introduction solution Instead of testing the query constraints C1.~l    C,.R on the entire extent Extent X we apply them only on the re sults of the subquery select x from Extent2 as x 40 where CO.R Since Co.Roi is a constraint on a cluster index attribute, the select operation is expected to be quite fast The original query is transformed to its more efficient form select x from 90 as x where Cl and    and C union Ei The exceptions Ei are merged to the result because they satisfy the query constraints but not the new index con straint Co.Roi Since the union operation is relatively cheap the optimal solution is the one that introduces the index con straint with the highest selectivity 6 Discussion We now look at two different scenarios and estimate the extent to which heuristics H1 and H2 speed up query exe cution The Jirst scenario concerns frequently executed queries Assume that the association rules which are used by algo rithm 3 are not modified We may optimize a query once at compilation time then execute its optimized form It is worth optimizing provided that the execution time of the optimized query is less than the execution time of the orig inal query For heuristic H1 this happens only if the time 132 


saved by omitting some constraints is greater than the time needed to remove the exceptions from the result except operation The more the eliminated constraints and the fewer the exceptions the better the optimization As one of the referees pointed out, the time saved by the elimina tion of constraints is CPU-related Since query execution is dominated by data access time this optimization is not expected to alter performance significantly It would help only in contexts rich in associations with few exceptions in which users express many constraints in their queries Heuristic H2 is expected to bring more significant ben efits Firstly, this optimization involves a union operation which is much cheaper than the except operation used in H1 Secondly instead of retrieving all the objects of an extent from the database we need only look at the subset retrieved through an indexed constraint Hence we save a considerable amount of data access time, spending a negli gible amount of CPU time in evaluating the additional con straint The second scenario concerns queries which are exe cuted only once In this case the time required for opti mization is significant. This time depends on the algorithm that finds associations between relaxed constraints and tight constraints \(see section 3 since this is the most expensive step in the optimization process This algorithm finds paths in a directed graph, and combines the exceptions associated with each edge of the path to derive the total exceptions for the path. Therefore its complexity is a function of i the av erage number of exceptions in the existing association rules and ii the number of different constraints found in the an tecedents and the consequents of the rules We have already implemented the algorithms for apply ing H1 the next step is to implement the corresponding al gorithms for H2 This should not be difficult since the main algorithm  finding associations between rule constraints  is common to the two heuristics We intend to set up an experimental model in order to evaluate H1 and H2 in the scenarios discussed above 7 Conclusion The use of association rules for query optimization is rel evant to both relational and object-oriented database sys tems There has been a lot of research on generating asso ciation rules and maintaining them in the presence of up dates Research has also focused on finding heuristics that take advantage of rules in order to optimize a query Most of this work 2 31 has considered integrity rules rather than association rules with exceptions Semantic optimiza tion heuristics were also applied without considering indi rect associations In this paper we implement algorithms that apply two optimization heuristics presented by Siegel et al taking account of both exceptions and indirect asso ciations We show how to use these heuristics to optimize an OQL query The complexity of the optimization process is closely related to the complexity of the constraint graph which represents the set of association rules in the data It also depends on the number of exceptions associated with each rule We have designed an experimental framework to evaluate the two optimization techniques both in the con text of queries repeated frequently over a period of time, and in the context of ad-hoc queries executed once only The re sults of this experimental work will be presented in a later paper 8 Acknowledgements We are grateful to the anonymous referees who read the paper carefully and critically and made many helpful suggestions Agathoniki Trigoni is supported by a scholar ship from the Greek Scholarships Foundation and is deeply obliged to the National Bank of Greece References I R Agrawal T Imielinski and A Swami Mining asso ciation rules between sets of items in large databases In Proceedings of the 1993 ACM SIGMOD Intl Conference on Management oj data pages 207-2 16 1993 2 U Chakravarthy J Grant and J Minker Logic-based ap proach to semantic query optimization ACM Transactions on Database Systems 15\(2 162-207 1990 3 J Grant, J.Gryz J Minker and L Raschid. Semantic query optimization for object databases In ICDE pages 444-453 1997 4 R Miller and Y Yang Association rules over interval data In ACM SIGMOD 1997 5 J Park An effective hash-based algorithm for mining asso ciation rules In ACM SIGMOD pages 175-186 1995 6 M Siegel E Sciore and S Salveter A method for auto matic rule derivation to support semantic query optimiza tion ACM Transactions on Database Systems 17\(4 600 December 1992 7 R Srikant and R Agrawal Mining quantitative association rules in large relational tables In ACM SIGMOD Intl Con ference on Management ojdata pages 1-12 1996 8 D Tsur J Ullman S Abiteboul C Clifton R Motwani S Nestorov and A Rosenthal Query flocks a general ization of association-rule mining In ACM SIGMOD Intl Conference on Management of data pages 1-12 1998 Intelligent query answering in deductive and object-oriented databases In Fourth ACM Intl Conference on Information and Knowledge Management pages 244 251 1994 IO S Yoon 1 Song and E Park Semantic query processing in object-oriented databases using deductive approach In Intl Conference on information and knowledge management pages 150-157 1995 9 S Yoon 133 


