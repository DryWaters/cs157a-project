MapReduce is a programming model proposed by Google to process large datasets in clusters. However, MapReduce often needs to transfer much intermediate data among nodes, which 
Smart Intermediate Data Transfer for MapReduce on Cloud Computing  Tzu-Chi Huang 1 Kuo-Chih Chu 2  Yu-Ruei Rao 3  Department of Electronic Engineering Lunghwa University of Science and Technology Taoyuan County, Taiwan 123  tzuchi@mail.lhu.edu.tw 1 kcchu@mail.lhu.edu.tw 2 hades780215@gmail.com 3    Abstract 
is harmful to performances of an application. MapReduce can be enhanced by using the proposed Smart Intermediate Data Transfer SIDT\time system to smar tly arrange intermediate data Although SIDT does not reduce intermediate data to the minimal size in comparison with ot her intermedi ate data arrangement procedures such as Huffman coding, bzip2, and gzip, MapReduce is proved to get a better performance from SIDT than from others in the experiments of this paper 
KeywordsÑSIDT; MapReduce; Intermediate Data; Cloud Computing 
 
 I 
 I NTRODUCTION 
 MapReduce [1 a p r og r a m m in g  m odel p r op os ed  by  Google to process large datasets in clusters. MapReduce has become the important technology on cloud computi  3  because it allows programmers to easily develop applications in clusters. For applications, all MapReduce needs is programmers to prepare two functions \(a.k.a. Mapper and Reducer  tha t w ill be  a u t o m a tic a lly d i s t rib ut e d  o v e r node s in clusters at runtime by the runtime system When the runtime system works for applications, it provides Mappers with input data and transfers intermediate data [1  o u tp u t te d  b y M a p p e r s to R e d u c e r s Ho w e v e r  th e  runtime system often consumes much bandwidth in 
transferring intermediate data among nodes, which is harmful to performances. Because intermediate data is just for Reducers the runtime system should have a proposal specially designed for MapReduce to reduce bandwidth consumption. Ideally, the runtime system can use such a proposal to reduce bandwidth consumption in transferring intermediate data without costing much CPU time as a tradeoff In this paper, Smart Intermediate Data Transfer \(SIDT\ is proposed to reduce bandwidth consumption in transferring intermediate data from Mappers to Reducers. SIDT smartly arranges intermediate data when the runtime system moves intermediate data from Mappers to Reducers. SIDT improves 
performances without negative impacts on application results SIDT keeps MapReduce intact to have high portability among platforms. In the experiments, SIDT outperforms the runtime systems that use Huffman coding [4  bz i p 2 [5 gz i p [6 or no  special arrangement [1  a  d e fa ul t i n t h e e x i s t i ng r unt i m e  systems\ to handle intermediate data The remaining parts of this paper are organized as follows Section 2 is the MapReduce background. Section 3 introduces SIDT. Section 4 has experiments. Section 5 concludes this paper II 
 
BACKGROUND MapReduce [1 m a in ly h a s t w o s t ag es  i  e  th e  Ma p s t ag e  
and the Reduce stage, in the progress of application execution In the Map stage, MapReduce needs Mappers to process input data and output intermediate data in a format of key and value pairs. MapReduce relies on the runtime system to save intermediate data in different intermediate files according to a hash function based on the key of intermediate data. In the Reduce stage, MapReduce uses Reducers to process intermediate data and output parts of application results Between the two stages, MapReduce relies on the runtime system to choose appropriate nodes, e.g. the idle ones, to run Reducers and transfer intermediate data from nodes on where Mappers run to nodes on where Reducers run 
Word Count [1 i s a n ap pl i c a t i o n o f t e n  use d t o e xpl a i n  MapReduce. Word Count has a Mapper to parse a document having words separated by space characters. For each word Word Count outputs a pair of çwordé and ç1é as the key and value pair in the Mapper. In the Reducer, Word Count processes intermediate data grouped into different words and sums up the values associated with the same word as the count of the word. Finally, Word Count gets outputs collected by the runtime system from all Reducers and has the counts of all words appearing in the document as the application result Although the runtime system may partition input data into 
different blocks before giving Mappers the block data and sort outputs of Reducers for an application in some MapReduce prototypes, we do not discuss the extra data process issues because they are beyond the scope of this paper. In this paper we focus on how to reduce bandwidth consumption in transferring intermediate data from Mappers to Reducers because reducing bandwidth consumption usually benefits performances and needs to be cared for nodes connected to each other with limited bandwidth in a cluster 
2013 International Conference on Cloud Computing and Big Data 978-1-4799-2829-3/13 $26.00 © 2013 IEEE DOI 10.1109/CLOUDCOM-ASIA.2013.97 9 
2013 International Conference on Cloud Computing and Big Data 978-1-4799-2830-9/14 $31.00 © 2014 IEEE DOI 10.1109/CLOUDCOM-ASIA.2013.97 9 


A Overview B SIDT Encoder C SIDT Decoder 
 
   
III SMART  INTERMEDIATE  DATA  TRANSFER  SIDT A typical runtime system handles intermediate data as the way in the upper part of Fig. 1 and uses a hash function with a key as its parameter in order to determine the intermediate file for saving the key and value pair. When the intermediate file reaches a certain size, a typical runtime system sends it to a Reducer on an idle node over networks. Generally, a typical runtime system directly saves intermediate data produced by Mappers in intermediate files without any further processing  Fig. 1, SIDT Overview In the lower part of Fig. 1, SIDT uses SIDT Encoder at one side to translate intermediate data into SIDT Metadata before sending it to nodes over networks. SIDT can save bandwidth because SIDT Metadata usually is smaller than intermediate data. At the other side, SIDT uses SIDT Decoder to restore SIDT Metadata to intermediate data before handing over it to a Reducer. SIDT uses an algorithm specifically designed for processing intermediate data without costing much CPU time Accordingly, SIDT can improve performances by reducing bandwidth consumption in networks SIDT Encoder translates intermediate data into SIDT Metadata that has a smaller size. SIDT Encoder arranges intermediate data and targets redundant keys appearing in it SIDT Encoder slightly uses CPU time to greatly shorten intermediate data size in order to reduce bandwidth consumption when intermediate data is transferred among nodes  Fig. 2, SIDT Encoder Components SIDT Encoder has four components as shown in Fig. 2, i.e Key Lookup Module, Table Update Module, Data Flush Module, and Key Lookup Table. SIDT Encoder builds Key Lookup Table that uses a different key as a record entry and a list of records having values associated with the same key SIDT Encoder uses Key Lookup Module to compare keys in intermediate data and those in Key Lookup Table. SIDT Encoder calls Table Update Module to add a new record entry and related records to Key Lookup Table for a new key and value pair found by Key Lookup Module in intermediate data SIDT Encoder uses Data Flush Module to both create SIDT Metadata according to Key Lookup Table and save it in intermediate files  Fig. 3, Translating Intermediate Data into SIDT Metadata  SIDT Encoder translates intermediate data as shown in Fig 3. SIDT Encoder updates records in Key Lookup Table for each pair of key and value found in intermediate data, and creates a record entry with the newly found key on demand Next, SIDT Encoder merges the values associated with the same key in Key Lookup Table and translates them into SIDT Metadata. If a key or value has the separate character that is designed for distinguishing a key and each of its values from SIDT Metadata, SIDT Encoder appends an extra separate character to the original one. Finally, SIDT Encoder uses a newline character to separate keys and their values as groups SIDT Decoder restores SIDT Metadata to intermediate data before handing over it to the runtime system. SIDT Decoder extracts keys and their values from SIDT Metadata and translates them back to the original format of intermediate data In other words, SIDT Decoder executes the reverse procedures in Fig. 3. After SIDT Decoder restores and hands over intermediate data to the runtime system, the runtime system is not aware of the intermediate data translation made by SIDT Encoder for transferring intermediate data over networks  Fig. 4, SIDT Decoder Components SIDT Decoder has only two components as shown in Fig 4, i.e. Group Scan Module and Data Restore Module. SIDT Decoder uses Group Scan Module to locate a key and its values in SIDT Metadata. Next, SIDT Decoder uses Data Restore Module to restore the key and its values in the format of SIDT Metadata to a series of key and value pairs in the format of intermediate data. Technically, SIDT Decoder uses Data 
10 
10 


A Configurations B Overhead Breakdown C Performance of Word Count 
 
   
Restore Module to restore intermediate data according to the reverse procedures in Fig. 3 IV EXPERIMENT  Fig. 5, Experiment Configurations We use PHP \(short for Hypertext Preprocessor\ [7 develop a runtime system named PHPMR because PHP is a widely-used general-purpose scripting language especially suited for Web development. We make PHPMR capable of: 1 reading input files from a local disk, 2\ locating an idle node to run Mappers, 3\ distributing input files over Mappers, 4 locating an idle node to run Reducers, 5\ transferring intermediate data from Mappers to Reducers, and 6\collecting outputs from Reducers as the application result. For experiments, we use PHP to develop SIDT and two applications as well to work with PHPMR at nodes. We make PHPMR call SIDT Encoder just before saving intermediate data in intermediate files. Conversely, we make PHPMR call SIDT Decoder before sending intermediate data to Reducers Besides observing performances of PHPMR with SIDT, we observe performances of PHPMR that uses Huffman coding 4  b z ip2  5  gzi p [6 or  no s p e c i a l ar ra nge m e nt  i  e   t h e native case\or handling intermediate data. To this end, we simply replace SIDT Encoder and SIDT Decoder with the encoding and decoding functions of other intermediate data arrangement procedures in different experiments. In experiments, we prepare several identical PCs connected to each other via Gigabit Ethernet. We detail their configurations in Fig. 5 We want to observe SIDT overheads. To this end, we develop a dummy application that has a Mapper and a Reducer both to do nothing more than outputting identical data when receiving input data or intermediate data. We provide the dummy application with an input file that has totally different words to occupy 80 bytes. In Fig. 6, we show the overhead of each procedure used in PHPMR with SIDT for the dummy application to process the input file. For comparison, we also show native overheads of certain procedures without SIDT in Fig. 6 We observe that the dominant factor is the native overhead of the job partition procedure but it will not affect performances as much as the procedures of reading input data and getting intermediate data because they are in proportion to the quantity of data processed by an application. We note that SIDT hardly has a negative impact on performances and furthermore may have a chance to improve performances. For example, we note that the emit function with SIDT can outperform the native emit function about 30.79% \(i.e. \(0.8670.6\/0.867\ecause SIDT can arrange intermediate data to reduce its size in order to shorten the delay of disk I/O in a node  Fig. 6, SIDT Overhead Breakdown 
 Fig. 7\(a\, Word Count Performance on Repeated Input Data with Different Mapper Numbers In the following subsection, we observe performances of two typical applications when different intermediate data arrangement procedures are taken. For comparison, we also observe the case that takes no special arrangement for handling intermediate data and refer to it as the native case. We test an application with different numbers of Mappers and Reducers and limit each PC in the cluster to running either a Mapper or a Reducer, in order to clearly observe the performance impacts of Mapper and Reducer numbers and bandwidth consumption In order words, we test an application with one Reducer and 
11 
11 


multiple Mappers, and then we test the application with one Mapper and multiple Reducers At runtime, we provide an application with input data from a PC as the role of the Master without using any distributed file system. We respectively input repeated data and non-repeated data. We let PHPMR automatically and arbitrarily distribute Mappers and Reducers of an application to PCs in a cluster over networks. Finally, we let PHPMR collect outputs of Reducers from all PCs in the cluster. We observe time and intermediate data that an application costs in the entire execution procedure, i.e., the reception of input data, the process of data with Mappers and Reducers, and the collection of outputs from Reducers  Fig. 7\(b\, Word Count Performance on Repeated Input Data with Different Reducer Numbers First, we observe performances of Word Count [1 e c a us e  it is a canonical application often used in performance evaluation of MapReduce runtime systems. In Word Count, we program a Mapper to emit to disks a string çword 1é for each word in input data provided by PHPMR. Besides, we program a Reducer to merge groups of words in intermediate data collected by PHPMR from the corresponding Mappers according to a key-length-based hash function. Finally, we record the execution time and bandwidth consumed by transferring intermediate data among nodes. We test the application respectively with a 100 MB input file having repeated data and a 100 MB input file having non-repeated data In Figs. 7\(a\nd 7\(b\, we observe that increasing Mappers and Reducers both can improve performances of Word Count in processing repeated input data. Because Word Count can process more input data with multiple Mappers, we observe in Fig. 7\(a\ that increasing Mappers can get more performance gains than increasing Reducers. We observe that SIDT outperforms other intermediate data arrangement procedures although SIDT does not compress intermediate data to the minimal size. For example, we observe in Fig. 7\(b\hat SIDT can use 1 Mapper and 7 Reducers to improve 40% \(i.e. \(188111\/188 = 41%\performance of the native case but gzip only can improve 3% \(i.e. \(188-183\188 = 3%\rformance Although bzip2 and gzip can compress intermediate data to a size much smaller than SIDT, we observe that their high computation overheads can not make them get performances better than SIDT. Similarly, we note that Huffman coding seriously degrades performances of the native case due to the high computation overhead  Fig. 8\(a\, Word Count Performance on Non-Repeated Input Data with Different Mapper Numbers  
12 
12 


Fig. 8\(b\, Word Count Performance on Non-Repeated Input Data with Different Reducer Numbers When testing Word Count with non-repeated input data in Figs. 8\(a\nd 8\(b\, we observe that SIDT works as efficiently as other intermediate data arrangement procedures, even though SIDT generates intermediate data similar to the native case. Because a Mapper can not generate intermediate data compressible to SIDT when processing non-repeated input data we are not surprised by the phenomenon that SIDT arranges intermediate data similar to the native intermediate data in transmission. Although bzip2 and gzip can compress intermediate data very well, we observe that their performances just are slightly improved due to high computation overheads Nevertheless, we show that SIDT sill can maintain performances in processing non-repeated input data even though Mappers can not generate intermediate data compressible to SIDT. According to Figs. 7 and 8, we note that all intermediate data arrangement procedures can get better performances in processing repeated input data than nonrepeated input data, because the runtime system costs less time in collecting the application result of repeated input data than that of non-repeated input data  Fig. 9\(a\, Quick Sort Performance on Repeated Input Data with Different Mapper Numbers In this subsection, we observe performances of Quick Sort 8 w ith dif f e r e n t  in te rm ediat e d ata  ar ran g e m e n t  pr oc e d u r es  In  Quick Sort, we implement a Mapper to process input data and save intermediate data into different intermediate files according to the digit of a number. We implement a Reducer to sort its corresponding intermediate data with the Quick Sort algorithm [8 W e ex pe ct  th a t th e ap pl ic ati on c o sts  m u ch  C P U time in Reducers because Mappers merely classify numbers according to their digits. We test the application respectively with a 100 MB input file having repeated data and a 100 MB input file having non-repeated data  Fig. 9\(b\, Quick Sort Performance on Repeated Input Data with Different Reducer Numbers According to Figs. 9\(a\ and 9\(b\ Quick Sort can get a better performance by increasing Reducers than by increasing Mappers because increasing Reducers can alleviate computation overheads. When processing repeated input data Quick Sort gets the best performance with SIDT by saving much bandwidth consumption in transferring intermediate data For example, we observe in Fig. 9\(b\ that SIDT can use 1 Mapper and 7 Reducers to improve 30% \(i.e. \(218-153\/218 30%\performance of the native case, which is much better than other intermediate data arrangement procedures. Although bzip2 and gzip can reduce intermediate data more than SIDT Quick Sort hardly gets many performance improvements from them because they cost much time in compressing and decompressing intermediate data We show results of processing non-repeated input data in Figs. 10\(a\nd 10\(b\. According to the curves, we know that increasing Reducers still improves performances much more than increasing Mappers when Quick Sort processes nonrepeated input data. Although SIDT does not reduce intermediate data to the minimal size because no intermediate data is compressible to SIDT, we observe that SIDT still maintains performances close to bzip2, gzip, and the native case. According to Figs. 9 and 10, we observe that Quick Sort gets better performances with all intermediate data arrangement procedures in processing repeated input data than in processing non-repeated input data, because Reducers cost more time to search arrays and sort non-repeated numbers In brief, we show that SIDT is a practicable intermediate data arrangement procedure and better than other intermediate data arrangement procedures such as Huffman coding, bzip2 and gzip in experiments because: 1\ SIDT can compress intermediate data to a certain degree but outperform other intermediate data arrangement procedures when processing 
D Performance of Quick Sort 
 
13 
13 


J. Dean, S. Ghemawat, "MapReduce: Simplified Data Processing on Large Clusters", Communications of the ACM, Volume 51, Issue 1 2008, pp. 107-113  2 S. P. Ahuja, A. C. Rolli, "Survey of the State-of-the-Art of Cloud Computing", International Journal of Cloud Applications and Computing, Volume 1 Issue 4, 2011, pp.34-43  3 B. P. Rimal, E. Choi, I. Lumb, "A Taxonomy and Survey of Cloud Computing Systems", in Proceedings of Fifth International Joint Conference on INC, IMS and IDC, 2009, pp. 44-51  4 M. Sharma, "Compression Using Huffman Coding", IJCSNS International Journal of Computer Science and Network Security, Vol 10, No. 5, 2010, pp. 133-141  5 D. S. Hirschberg, D. A. Lelewer, "Efficient Decoding of Prefix Codes Communications of the ACM, Vol. 33, Issue. 4, 1990, pp. 449-459  6 M. F. Nowlan, B. Ford, R. Gummadi, "Non-linear compression: Gzip Me Not!", in Proceedings of the 4th USENIX conference on Hot Topics in Storage and File Systems, 2012, pp. 11-11  7 S. Trent, M. Tatsubori, T. Suzumura, A. Tozawa, T. Onodera Performance comparison of PHP and JSP as server-side scripting languages", in Proceedings of the 9th ACM/IFIP/USENIX International Conference on Middleware, 2008, pp. 164-182  8 C. A. R. Hoare, "Quicksort", the Computer Journal, Vol. 5, Issue 1 1962, pp.10-16 
 
repeated input data; and 2\ SIDT can maintain performances close to bzip2, gzip, and the native case without degrading performances when processing non-repeated input data  Fig. 10\(a\, Quick Sort Performance on Non-Repeated Input Data with Different Mapper Numbers  Fig. 10\(b\, Quick Sort Performance on Non-Repeated Input Data with Different Reducer Numbers V CONCLUSION In this paper, we propose Smart Intermediate Data Transfer SIDT\ to reduce bandwidth consumption in transferring intermediate data from Mappers to Reducers. We use SIDT to arrange intermediate data when the runtime system moves intermediate data from Mappers to Reducers. By compressing intermediate data without costing much CPU time, we use SIDT to successfully improve performances without negative impacts on application results. Because of keeping MapReduce intact without any modification, we make SIDT and MapReduce applications portable to various platforms. For verifying practicability of SIDT, we implement SIDT on a MapReduce runtime system to help Word Count and Quick Sort process repeated input data and non-repeated input data. In the experiments, we show that SIDT outperforms other intermediate data arrangement procedures such as Huffman coding, bzip2, and gzip in processing repeated input data and non-repeated input data because: 1\ SIDT compresses intermediate data to a certain degree but outperforms other intermediate data arrangement procedures when processing repeated input data; and 2\ SIDT maintains performances close to bzip2, gzip, and the native case without degrading performances when processing non-repeated input data Accordingly, we are convinced that SIDT can improve performances of MapReduce applications in clusters by reducing intermediate data without negative impacts on application results A CKNOWLEDGMENT  We thank the National Science Council of Taiwan for their support of this project under grant number NSC 102-2221-E262-014. We thank Lunghwa University of Science and Technology for providing us with devices. We further offer our special thanks to the reviewers for their valuable comments and suggestions R EFERENCES  1 
        
14 
14 


 
Qi Qi, Zhenyu Chen, Jia Liu, Chengfeng Hui, and Qing Wu, çUsing inferred tag ratings to improve user based collaborative filtering,é New York: Proceedings of the 27th Annual ACM Symposium on Applied Computing, pp. 2008-2013, March 2012 2 SuyunWei, Ning Ye, Shuo Zhang , Xia Huang, and Jian Zhu Collaborative Filtering Recommendation Algorithm Based on Item Clustering and Global Similarity,é Business Intelligence and Financial Engineering \(BIFE\, 2012 Fifth International Conference on. IEEE, pp 69-72, August 2012 3 Nathan N. Liu, Min Zhao, Evan Xiang, and Qiang Yang. çOnline evolutionary collaborative filtering,é New York: Proceedings of the fourth ACM conference on Recommender systems, pp. 95-102 September 2010 4 Heung-Nam Kim, Ae-Ttie Ji, Inay Ha, Geun-Sik Jo, çCollaborative filtering based on collaborative tagging for enhancing the quality of recommendation,é Electronic Commerce Research and Applications vol. 9,no. 1, pp. 73-83, 2010 5 N. Hurley, M. Zhang, çNovelty and diversity in top-N recommendations-analysis and evaluation,é ACM Transactions on Internet Technology, vol. 10, no. 4, pp. 1-29, 2011 6 Goldberg D, Nichols D, Oki B, Terry D, çUsing collaborative filtering to weave an information tapestry,é Communications of the ACM, vol. 35 no. 12, pp. 61-70, 1992 7 Resnick P, Iacovou N, Suchak M, Bergstorm P, and Riedl J GroupLens: an open architecture for collaborative filtering of netnews Proceedings of the 1994 ACM conference on Computer supported cooperative work, pp. 175-186, October 1994 8 Shardanand, Upendra, and Pattie Maes, çSocial information filtering algorithms for automating çword of mouthé,é Proceedings of the SIGCHI conference on Human factors in computing systems, pp.210217, 1995 9 Hill W, Stead L, Rosenstein M, and Furnas, G, çRecommending and evaluating choices in a virtual community of use,é Proceedings of the SIGCHI conference on Human factors in computing systems, pp.194201, 1995  Zan Huang, Hsinchun Chen, and Naniel Zeng, çApplying associative retrieval techniques to alleviate the sparsity problem in collaborative filtering,é ACM Transactions on Information Systems \(TOIS\, vol. 22 no. 1, pp. 116-142, 2004  Jes˙s Bobadilla, Fernando Ortega, Antonio Hernando, Jes˙s Bernal, çA collaborative \002ltering approach to mitigate the new user cold start problem,é Knowledge-Based Systems, vol. 26, pp. 225-238, 2012  Yehuda Koren, Robert Bell, çAdvances in collaborative filtering,é US Recommender Systems Handbook, pp. 145-186 ,2011  Benjamin Marlin, Richard S. Zemel, Sam Roweis, Malcolm Slaney Collaborative Filtering and the Missing at Random Assumption Computer Science, vol. 34, no. 3, pp. 78-81, 2012  Cheng-Jun Zhang, An Zeng, çBehavior patterns of online users and the effect on information filtering,é Physica A: Statistical Mechanics and its Applications, vol. 391, no. 4, pp. 1822-1830, 2012  ZHOU Jun-Feng, TAN Xian, and GUO Jing-Feng, çAn Optimized Collaborative Filtering Recommendation Algorithmé, Journal of Computer Research and Development, vo1. 41, no. 10, pp. 1842-1847 October 2004 012<\007oJ  8\034\022  F¡\034C\022  003‘/°\004Ï\010Í,X\011#\011‡Eõ$∏\031|9$1k  Au1k\035\016-Ë0J\003‚\011•\022 41\(10\ 1842-1847, 20   ZHANG Guang-Wei, LI De-Yi, LI Peng, KANG Jian-Chu, CHEN GuiSheng, çA Collaborative Filtering Recommendation Algorithm Based on Cloud Model,é Journal of Software, vol. 18, no. 10, pp. 2403-2411 2007 024Ù\007\035\011  035"\025  035"T  024ã\024Œ\007  L\034\036\026  015Œ\004b 004e\037ı\015_,X\011#\011‡E 031|9$1k  EC\004 021:\030y 18\(10\-2411, 200  Deng AL, Zhu YY, Shi BL, çA Collaborative Filtering Recommendation Algorithm Based on Item Rating Predictioné, Journal of Software, vol. 14, no. 9, pp. 1621-1628, 2003 Fg\(\005\035k  035\005\030@\010  033ë\005\003 004  015Œ\004bNM,¬Aò\007⁄NX#\037,X\011#\011‡Eõ$∏\031|9$1k  EC\004 \021:\030y 14\(9 1621-1628, 200  Herlocker J, Konstan J A, Riedl J, çAn empirical analysis of design choices in neighborhood-based collaborative filtering algorithms Information retrieval, vol. 5, no. 4, pp. 287-310, 2002  McLaughlin M R, Herlocker J L, çA collaborative filtering algorithm and evaluation metric that accurately model the user experience Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval. pp. 329-336, July 2004  Hao Ma, Irwin King, and Michael R. Lyu, çEffective missing data prediction for collaborative filtering,é Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pp. 39-46, July 2007  HUANG Chuang-Guang, YIN Jian, WANG Jing, LIU Yu-Bao, WANG Jia-Hai, çUncertain Neighborsê Collaborative Filtering Recommendation Algorithm,é Chinese Journal of Computers, vol. 33, no. 8, pp. 13691377, August 2010 Tò\007Ô\007\035  011DHH  M 007  K  003·.B\021nE Fè,X\011#\011‡Eõ$∏\031|9$1k  Au1k\035\016\021:\030y 33\(8\, 1369-1377, 2010  J. Bobadilla, F. Ortega, A. Hernando, A. GutiÈrrez, çRecommender systems survey,é Knowledge-Based Systems, vol. 46, pp. 109Ö132 2013  Xiang Cui, Guisheng Yin, çMethod of collaborative filtering based on uncertain user interests cluster,é Journal of Computers, vol. 8, no. 1, pp 186-193, January 2013  
007 
                       
represents the value of parameter and ordinates represents the MAE results As Fig 3 shows, the MAE of the algorithm proposed in this paper is lower than the traditional collaborative filtering recommendation algorithm in all conditions above. So it can be concluded that using impact factor to improve the similarity metrics can significantly improve the accuracy of recommendation. The collaborative filtering recommendation using the adjusted similarity measure that improved by the impact factor have the best performance V CONCLUSION Firstly, in order to deal with the problem that user rating data are extremely sparse, this paper analyzed the problems of the traditional similarity measurements that their measurement accuracy depends on the number of common ratings of users or items. Then it is proposed to improve Collaborative Filtering Recommendation Algorithm by using similarity impact factor which can reduce the negative impact due to data sparsity. The experimental results show that the proposed similarity impact factor can improve the accuracy of the similarity measures effectively, and the improved collaborative filtering recommendation algorithms are more precise A CKNOWLEDGMENT  This work is partially supported by NSFC\(No.61003054 No.61170020\; Science and Technology Support Program of Suzhou\(No. SG201257\; Science and Technology Support program of Jiangsu province\(No. BE2012075\; Open fund of Jiangsu Province Software Engineering R&D Center \(No SX201205\ the Opening Project of Suzhou High-tech Key Laboratory of Cloud Computing & Intelligent Information Processing\(No. SXZ201302 R EFERENCES  1 
303 
303 


Figure 3 Figure adapted from our previous work sho wing the m o v ement of dif ferent units in the Afghanistan w a r  Figure 4 Figure adapted from our previous work sho wing patterns in the Afghanistan w a r data P atterns mark ed by A  C show lots of events happening at the same time 1  3 show lack of events and periodicity of pauses  P  Ball J Asher  D  Sulmont and D Manrique Ho w many peruvians have died American Association for the Advancement of Science Tech Rep 2003  P  Ball E T abeau and P  V erwimp The bosnian book of dead Assessment of the database full report Households in Conîict Network Tech Rep 2007  D Bo yd and K Cra wford Critical questions for big data Provocations for a cultural technological and scholarly phenomenon Information Communication  Society  vol 15 no 5 pp 662Ö679 2012  J.-B Michel Y  K Shen A P  Aiden A V eres M K Gray  J P Pickett D Hoiberg D Clancy P Norvig J Orwant et al  Quantitative analysis of culture using millions of digitized books science  vol 331 no 6014 pp 176Ö182 2011  L Mano vich Data stream database timeline the forms of social media 2012 On A v ailable http://lab softw arestudies.com/2012/10/datastream-database-timeline-new.html  Z S Syed T  Finin and A Joshi W ikipedia as an ontology for describing documents in ICWSM  2008  M L Jock ers Macroanalysis Digital Methods and Literary History  University of Illinois Press 2013  S Jones and H Petrie Charte x Disco v ering spatial descriptions and relationships in medieval charters 2013  C Gro v e r  T rading conferences  2012  S Soderland B Roof B Qin S Xu O Etzioni et al  Adapting open information extraction to domain-speciìc relations AI Magazine  vol 31 no 3 pp 93Ö102 2010  Guardian.co.uk  Afghanistan w a r logs  2011 Online Available http://www.theguardian.com/world/the-war-logs  M Mateas and A Stern F ac ade An experiment in building a fully-realized interactive drama in Game Developers Conference Game Design track  vol 2 2003 p 82  N Kiya vitskaya N Zeni J R Cordy  L  Mich and J Mylopoulos Cerno Light-weight tool support for semantic annotation of textual documents Data  Knowledge Engineering  vol 68 no 12 pp 1470Ö1492 2009  C L Sidner   T o w ards a computational theory of deìnite anaphora comprehension in english discourse DTIC Document Tech Rep 1979 44 


 J A Ha wkins Deìniteness and Indeìniteness  Humanities Press 1978  J Me yer and R Dale Mining a corpus to support associati v e anaphora resolution in Proceedings of the Fourth International Conference on Discourse Anaphora and Anaphor Resolution  2002  R Mitk o v  B  Bogurae v  and S Lappin Introduction to the special issue on computational anaphora resolution Computational Linguistics  vol 27 no 4 pp 473Ö477 2001  B W ebber  M  Egg and V  K ordoni Discourse structure and language technology Natural Language Engineering  vol 18 no 4 pp 437Ö490 2012  M Dimitro v  K Bontche v a  H  Cunningham and D Maynard A lightweight approach to coreference resolution for named entities in text Anaphora Processing Linguistic cognitive and computational modelling  vol 263 p 97 2005  S Lappin and H J Leass  A n algorithm for pronominal anaphora resolution Computational linguistics  vol 20 no 4 pp 535Ö561 1994  R Mitk o v   F actors in anaphora resolution the y are not the only things that matter a case study based on two different approaches in Proceedings of a Workshop on Operational Factors in Practical Robust Anaphora Resolution for Unrestricted Texts  Association for Computational Linguistics 1997 pp 14Ö21  K V a n Deemter and R Kibble On coreferring Corefer ence in muc and related annotation schemes Computational linguistics  vol 26 no 4 pp 629Ö637 2000  C Northw ood T ernip temporal e xpression recognition and normalisation in python Ph.D dissertation Masters thesis University of Shefìeld 2010  A Auger and J Ro y  Expression of uncertainty in linguistic data in 11th International Conference on Information Fusion 2008  IEEE 2008 pp 1Ö8  M J Druzdzel V erbal uncertainty e xpressions Literature review Pittsburgh PA Carnegie Mellon University Department of Engineering and Public Policy  1989  E Marshman Expressions of uncertainty in candidate knowledge-rich contexts A comparison in english and french specialized texts Terminology  vol 14 no 1 pp 124Ö151 2008  D Klein and C D Manning  Accurate unle xicalized parsing in Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1  Association for Computational Linguistics 2003 pp 423Ö430  W  W  Cohen and J Richman Learning to match and cluster large high-dimensional data sets for data integration in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining  ACM 2002 pp 475Ö480  S F  Chen and J Goodman  A n empirical study of smoothing techniques for language modeling in Proceedings of the 34th annual meeting on Association for Computational Linguistics  Association for Computational Linguistics 1996 pp 310 318  B.-J Hsu and J Glass Iterati v e language model estimation efìcient data structure  algorithms in Proceedings of Interspeech  vol 8 2008 pp 1Ö4  T  Zhang and D Johnson  A rob ust risk minimization based named entity recognition system in Proceedings of the seventh conference on Natural language learning at HLTNAACL 2003-Volume 4  Association for Computational Linguistics 2003 pp 204Ö207  L Ratino v and D Roth Design challenges and misconceptions in named entity recognition in Proceedings of the Thirteenth Conference on Computational Natural Language Learning  Association for Computational Linguistics 2009 pp 147Ö155  A X Chang and C Manning Sutime A library for recognizing and normalizing time expressions in LREC  2012 pp 3735Ö3740  A Shrestha Y  Zhu B Miller  and Y  Zhao Storygraph Telling stories from spatio-temporal data in Lecture Notes in Computer Science  vol 8034 Springer 2013 pp 693 703  A T  P ang C M W ittenbrink and S K Lodha  Approaches to uncertainty visualization The Visual Computer  vol 13 no 8 pp 370Ö390 1997  W  T  C T  F orce W orld trade center task force intervie ws  2001  B Shneiderman The e yes ha v e it A task by data type taxonomy for information visualizations in Visual Languages 1996 Proceedings IEEE Symposium on  IEEE 1996 pp 336Ö343  C T rack er  Incidents e xport  I n visible Children and Resolve Tech Rep 2013  ECCC  Anne x 25 Written record of intervie w 09 june 1999 1999  ECCC Written record of intervie w o f Prak Khan  2007  ECCC Written record of intervie w o f Duch by CIJ o n 2101-2008 2008  M Halbw achs and L A Coser  On collective memory  University of Chicago Press 1992 45 


application or middleware platform to collect request ows Thus it is much easier to deploy FChain in large-scale IaaS clouds Blacksheep correl a t e s t he change poi nt of s y s t em-l e v el metrics e.g cpu usage with the change in count of Hadoop application states i.e events extracted from logs of DataNodes and TaskTrackers to detect and diagnose the anomalies in a Hadoop cluster Kahuna-BB correl a t e s b l ack-box dat a system-level metrics and white-box data Hadoop console logs across different nodes of a MapReduce cluster to identify faulty nodes In comparison FChain is a black-box fault localization system which is application-agnostic without requiring any knowledge about the application internals We believe that FChain is more practical and attractive for IaaS cloud systems than previous white-box or gray-box techniques V C ONCLUSION In this paper we have presented FChain a robust blackbox online fault localization system for IaaS cloud computing infrastructures FChain can quickly pinpoint faulty components immediately after the performance anomaly is detected FChain provides a novel predictability-based abnormal change point selection scheme that can accurately identify the onset time of the abnormal behaviors at different components processing dynamic workloads FChain combines both the abnormal change propagation knowledge and the inter-component dependency information to achieve robust fault localization FChain can further remove false alarms by performing online validation We have implemented FChain on top of the Xen platform and conducted extensive experimental evaluation using IBM System S data stream processing system Hadoop and RUBiS online auction benchmark Our experimental results show that FC hain can achieve much higher accuracy i.e up to 90 higher precision and up to 20 higher recall than existing schemes FChain is light-weight and non-intrusive which makes it practical for large-scale IaaS cloud computing infrastructures A CKNOWLEDGMENT This work was sponsored in part by NSF CNS0915567 grant NSF CNS0915861 grant NSF CAREER Award CNS1149445 U.S Army Research Ofìce ARO under grant W911NF-10-1-0273 IBM Faculty Awards and Google Research Awards Any opinions expressed in this paper are those of the authors and do not necessarily reîect the views of NSF ARO or U.S Government The authors would like to thank the anonymous reviewers for their insightful comments R EFERENCES   A m azon E las tic Com pute Cloud  h ttp://a w s  a m azon com ec2   V i rtual c om puting lab  http://vcl ncs u  e du  P  Barham  A  D onnelly  R I s aacs  a nd R M o rtier   U s ing m agpie f or request extraction and workload modelling in 
 2004  M  Y  Chen A  A ccardi E  K icim an J  L lo yd D  P atters on A  F ox and E Brewer Path-based failure and evolution management in  2004  R F ons eca G  P o rter  R H  K atz S  S h enk e r  and I  S toica X T race A pervasive network tracing framework in  2007  I  Cohen M  G o lds z m i dt T  K elly  J  S ym ons  a nd J  S  Chas e Correlating Instrumentation Data to System States A Building Block for Automated Diagnosis and Control in  2004  I  C ohen S  Z h ang M  G o lds z m i dt J  S ym ons  T  K elly  a nd A  F ox Capturing indexing clustering and retrieving system history in  2005  S  D uan S  Bab u  a nd K  M unagala F a A s ys tem for a utom ating failure diagnosis in  2009  S  K andula R Mahajan P  V erkaik S  A garw al J  P a dhye a nd V  Bahl Detailed diagnosis in computer networks in  2009  A  J  O liner  A  V  K ulkarni and A  A ik en  U s ing c orrelated s u rpris e to infer shared inîuence in  2010  P  Bahl R Chandra A  G r eenber g  S  K andula D  A  M altz and M Zhang Towards highly reliable enterprise network services via inference of multi-level dependencies in  2007  Z  G ong X  G u  a nd J  W ilk es   P RE S S  P Redicti v e E las tic ReS ource Scaling for Cloud Systems in  2010  H  N guyen Y  T a n and X  G u P A L  P ropagation-a w are a nom aly localization for cloud hosted distributed applications in  2011  B Gedik H Andrade K L  W u P  S  Y u and M  D oo SP ADE  t he system s declarative stream processing engine in  2008  A pache H adoop S y s tem   http://hadoop apache  or g/co re   Rice uni v e rs ity bidding s y s tem   http://rubis  objectw eb  o r g   M Ben-Y e huda D  B reitgand M F actor  H  K o lodner  V  K r a v ts o v  and D Pelleg NAP a building blo ck for remediating performance bottlenecks via black box network analysis in  2009  Y  T a n X  G u  a nd H  W a ng  A dapti v e s ys tem anom aly prediction f or large-scale hosting infrastructures in  2010  D  L  M ills   A b rief his t ory o f N T P tim e m e m o irs o f a n i nternet timekeeper  2003  Y  T a n H  N guyen Z  S h en X  G u C V e nkatram ani and D  R ajan PREPARE Predictive Performance Anomaly Prevention for Virtualized Cloud Systems in  2012  M  Bas s e ville and I  V  N ikiforo v   Prentice-Hall Inc 1993  L  Cherkaso v a  K  O zonat N Mi J  S ym ons a nd E  Sm irni  Anom aly application change or workload change towards automated detection of application performance anomaly and change in  2008  P  Barham and e t al   X e n a nd the a rt of virtualization  i n  2003  T he ircache p roject  h ttp://www.ircache.net  H ttperf  h ttp://code google com  p htt p er f  S  K u llback  T h e ku llback-leibler distance  1987  X  Chen M  Z hang Z  M  M a o and P  B ahl  A utom ating n etw ork application dependency discovery experiences limitations and new solutions in  2008  M Y u  A  G reenber g  D  M altz J  Re xford L  Y u an S  K andula and C Kim Proìling network performance for multi-tier data center applications in  2011  M K  A guilera J  Mogul J  W iener  P  R e ynolds  a nd A  Muthitacharoen Performance debugging for distributed systems of black boxes in  2003  S  A g arw ala F  A l e g re K  S chw a n and J  M ehalingham  E 2E P r of A utomated end-to-end performance management for enterprise systems in  2007  P  Re ynolds  J  L  W iener  J  C M ogul M  K  A guilera and A  V ahdat  WAP5 black-box performance debugging for wide-area systems in  2006  R Apte L  Hu K  S chw a n and A  G hosh L ook W ho s T alking Discovering dependencies between virtual machines using cpu utilization in  2010  G Khanna I  L aguna F  A rshad an d S Bagchi Distr ibuted diagnosis of failures in a three tier e-commerce system in  2007  R R S a m b as i v an A  X  Z heng M  D e Ros a  E  K re v at S  W h itm an M Stroucken W Wang L Xu and G R Ganger Diagnosing performance changes by com paring request ows in  2011  J  T a n a nd P  N a ras i m h an  RA M S and B lackS h eep I nferring w h ite-box application behavior using black-box techniques CMU Tech Rep 2008  J  T a n X  P a n E  Marinelli S  K a vulya R  G andhi a nd P  N a ras i m h an Kahuna Problem diagnosis for mapreduce-based cloud computing environments in  2010 
OSDI NSDI NSDI OSDI SOSP ICDE SIGCOMM DSN SIGCOMM CNSM SLAML SIGMOD ICAC PODC Computer Communication Review ICDCS Detection of abrupt changes theory and application DSN SOSP The American Statistician OSDI NSDI SOSP DSN WWW HotCloud SRDS NSDI NOMS 
207 
30 
30 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





