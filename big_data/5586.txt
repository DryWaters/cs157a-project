 An Intelligent Recommendation Model with a Case Study on u-Tour Taiwan of Historical Momuments and Cultural Heritage  Wei-Ding Liao, Don-Lin Yang Dept. of Information Engineering and Computer Science Feng Chia University Taichung, Taiwan weidingliao@gmail.com, dlyang.tw@gmail.com Ming-Chuan Hung Dept. of Industrial Engineering and Systems Management Feng Chia University Taichung, Taiwan mchong@fcu.edu.tw   Abstract Although there are many recommendation systems in use, they all face various challenges including the integration 
of diverse source data, improvement of prediction precision and meeting the userês satisfaction. In order to increase success rate of satisfied recommendations as well as the applications in different domain fields, we propose an Intelligent Recommendation Model and conduct a case study on the historical monuments and cultural heritage of u-Tour Taiwan to show the feasibility of our model.  In this research we use a hybrid approach to combine effective techniques such as popularization-based, community filtering, demographic profiling, and expertise-based in accordance with the type of users and the amount of available data to adjust weight values 
We also use association rules of data mining technique to find potential patterns in the web access log, while clustering is used to assign users into different groups suitable for them. The incremental approach of our method can calculate the ranking value of content to be more precise. Finally, Adobe Flex is used to present the recommendation result of Taiwanês 300 years of rich historical monuments and cultural heritage that provides more effective and efficient user interaction with less effort Making full use of the valuable digital information of historical sites with our model, we hope to revitalize contemporary 
cultural and historical meaning that can bring people a brighter future and colorful life Keywords-Recommendation mode 002 association rule; cluster analysis; classification; interactive graphic interface I   I NTRODUCTION  A  Background Nowadays various recommendation systems have been widely used in many industry sectors such as e-commerce  u s i c [3] an d ou r dail y l i f e t h at ben e f it people g r eatl y   Incorporating with the emerging technologies such as ontology e os pat i a l 2,4 5] an d s o f t w a re ag e n t s 10  
web intelligen m i n i ng 1,4,7,8,9 12,1 3], an d  social network can do any e n qu ir y  v i a t h e  Internet with a desktop personal computer or mobile device using wired or wireless networks Most recommendation systems can help people find what they are interested in, but do not know how to describe or explain them. With the rapid advancement of ITC technologies, a domain expert or agent can take advantage of recommendation systems to help people find anything they want directly or show the content of recommendation results they may like to know There are many applications of recommendation system in our daily life. For example, when people watch video on 
YouTube, the system will recommend new videos related to the videos they have watched before or from the result of analyzing their behavior history More examples can be seen on the shopping websites such as eBay and Yahoo!Shopping, where people spend a lot of time to search products and compare their prices and shipping costs with each other. Each website may have hundreds or thousands of products. It is very hard to browse all relevant products in every website to compare their prices. By using a recommendation system, people can find the products meeting their expectation very easily In addition, interactive graphic user interfaces have become more attractive and personal when showing the 
final results to users. With an effective approach, there is little difference on result presentation between a client and a server B  Motivation A good recommendation system can deliver what a user really wants without much effort in describing or explaining the details of target objects. For example, personal recommendation is a very important feature to find out interesting or useful items when users browse websites such as YouTube, eBay, and Amazon Although there are many applications of recommendation system in use, they do not have a unified definition of data source and can not integrate with each others easily. In view of the past experiences, we propose an 
intelligent recommendation model that any domain expert can use to make recommendations without knowing the details on how it works. In addition, we conduct a case study on the historical monuments and cultural heritage of u-Tour Taiwan to show the feasibility of our model. Users can not only use it with a desktop personal computer, but also smart mobile device via wired or wireless networks In our model, users are divided into two types: general guests and registered members. We provide different types 
2010 International Conference on Technologies and Applications of Artificial Intelligence 978-0-7695-4253-9/10 $26.00 © 2010 IEEE DOI 10.1109/TAAI.2010.23 72 


of user with different hybrid techniques to make recommendations. Finally, advanced interactive graphic user interfaces are used to present the final results to users II  R EALTED W OTK  A  Recommendation Systems Nowadays the applications of recommendation system have been widely used in our life such as tourism, eLearning system, television program, music, personalized route planning, shopping website, web page, image, daily menu, guideline, etc. Although the result of above applications can be shown easily, the core techniques of recommendation systems are very complex and challenging In a rev ie w ed th e pers on alized tou rist  recommendation during the last decade and analyzed the steps of how to develop a tourism recommendation system The author also summarized the advantages and disadvantages of client side with desktop computer and mobile device. In Ang e l et al. prop os ed a recommendation system called SPETA to provide an etourism advisor and use ontology to describe the e-tourism they built. Finally they presented a use case to explain the detail of how the system works to help users Yuxia et al. [13 al y zed h i erarchy bas ed pers on alized recommendations for tourist attractions. They used Bayesian network to classify users into several groups and applied ontology to integrate heterogeneous information about tourist attractions over the Internet. In addition, they built a map server and showed all the recommendation contents on the map Silv s ed a s o f t w are ag e n t as ex pert trav el  agent in a system called Traveller. Although a hybrid approach of recommendation techniques is used, they did not consider the types of users and the weighting factor of each technique is fixed. Therefore, the recommendation results are not highly personalized B  Data Mining Data mining is successfully used in many applications such as supermarket basket analysis and fraud detection. It also plays an important role in recommendation systems Many techniques of data mining, such as association rules and sequential patterns mining, are used to find useful rules from databases and web access logs. Others use Bayesian network to classify users into target groups Dirksen et al. [4 use d se que nt ia l p a tte r n s m i ni n g to  recommend touring routes and used a scenario or bar tour guide to explain how it works in Google Maps as well as why the bar tour will be recommended to users. Loc Nguyen  m e n ded learnin g con cept to s t u d en t s  w h en they are studying objects. Authors applied many algorithms such as AprioriAll, GSP \(Generalized Sequential Pattern SPADE \(Sequential PAttern Discovery using Equivalent classes\ and FreeSpan \(Frequent pattern-projected Sequential pattern\aluate their efficiency. Mei-Ling Shyu s ed th e m e t hod of collaborativ e f iltering by  mining association rules from user access sequences. They proposed a measure called MRD \(Minimum Reaching Distance\late the similarity between users We will use various techniques of data mining in different applications. First, association rule mining is used to predict user behavior; the main goal of clustering is to partition heterogeneous data into several groups with higher homogeneous features; Classification is aimed at training a classifier according to the known object of class and attributes C  Historical Momuments and Cultural Heritage Taiwan's ancestors in early migration had survived in arduous environment, despite of dangers and difficulties of living in an island. As we have accumulated more than 300 years of rich history and heritage resources, many historical stories and knowledge have been preserved that can be used to improve the well-being and life of the residents as well as the human beings in general. Although promoting historical monuments and cultural heritage has many challenges, it is of great value in our society. Finding our root and paying back to the society is a motto of Taiwanese people As a small island, Taiwan has many diverse cultural features influenced by China, Japan, Spain, Holland Portugal, etc. With a systematic effort to digitalize these historical monuments and cultural heritage, Taiwan's people can be recognized more than the land and people. Tourists can find experiences different from any other countries There is a considerable advantage to promote Taiwan's tourism industry Due to its multicultural history, religion in Taiwan is a way of life and being celebrated in every corner of city and town. For example, a very famous and popular temple in Taiwan called Dajia Mazu. Since it has hundreds of years in history and houses many statues of various Buddha, Dajia Mazu temple is regarded as a sacred building where different Mazu statues are worshiped as shown in Figure 1 This is one of the sites with historical monuments and cultural heritage that are worth of promoting  Figure 1. The Mazu statue 
73 


D  Adobe Flex Google promotes services such as GMail, Google Map Google Earth, etc. with web interfaces of RIA \(Rich Internet Application\hat are more smooth and easy to operate than traditional websites. Therefore people start to learn more on RIA The RIA technique used by Google is called AJAX Asynchronous JavaScript And XML\. AJAX improves the drawbacks of traditional web pages by combining JavaScript with XML. This web interface has the convenience of application program interface in desktop personal computer The architecture of web RIA for Adobe Flex is shown in Figure 2. It consists of three main components: 1\lex platform, 2\, 3\e server. Flex platform is executed in the environment of Flash Player. In the Web Application Server we can use JAVA, .NET, PHP etc. to develop web application interfaces   Figure 2. Adobe Flex Architecture III  OUR  R ECOMMENDATION S YSTEM  In this paper we propose a Recommendation Model based on various efficient techniques to recommend suitable contents to end users in many application domains. Our system as architecture is shown in Figure 3  Figure 3. System Architecture  In our research, we use this architecture to develop a prototype system that can recommend historical monuments and cultural heritage to users when they are planning a tour taking a tour, or finishing a tour. It can be used as a ubiquitous tour guide that provides useful information to enquiry, memorable experience to share, and those services we all like to have in a u-Tour website A  Targeted End Users Anyone can use our system anytime, anywhere, in any wired or wireless networks. One can use the system with a personal computer, laptop, or smart phone to get recommendation results from the Web easily and conveniently B  Interactive Graphic User Interface Although there are many different presentation techniques, we adopt a new Interactive Graphic User Interface \(IGUI\ed on Adobe Flex architecture. Using this IGUI, users can have more multimedia information with better visual effect. Since the transition between web pages is very smooth, users cannot tell a client side from a server side that makes the system seems more efficient C  Recommendation Model 1  Model Components Our model consists of seven components as follows a  Web Access Log When a user browses the system through the Internet the web server \(Apache, IIS\will record all the user behavior in the system. The format of web access log is shown in Table 1. It consists of several parts: source IP address, user id, timestamp, access URL, status code, and object return length Table 1. Web Access Log  b  User Ranking Matrix Only registered members can rank content in the system no matter if they have any engagement with the recommendation result. They can update the original ranking value orgValue ytime. Four default options n  4\or calculating the ranking value with user engagement are visit the site, emotion, weather and site status. Initially each of these four weights optionWeight 0.25. They can be adjusted as needed. The formula of recalculating the user ranking value  is shown in \(1 1  n i reRank C orgValue optionWeight  002  1 
74 


c  Complaint Matrix Every member can have any complaint with the content in the system no matter if they have any engagement or not Similar to user ranking matrix, there are some options to measure memberês complaint about the corresponding content that will affect the final complaint value d  Experise An expert has professional domain knowledge to provide the best and precious source data in our system How to use the knowledge effectively is very important. We store all the knowledge of expertês suggestions into the database for better recommendation With professional domain knowledge, the expertês weight of ranking value is more important than user ranking value since expertês rating is more reliable and usable e  User Profile The main goal of using this component is to calculate more precise recommendation value of content for userès preference. We use many attributes to record personal information in the database. We encode many ordinal variables to replace text in user profile for efficiency f  Content Attributes In order to calculate more precise recommendation values that match user preference and save execution time we cluster contents into groups that can be used to recommend to corresponding users more precisely Due to the various types of historical monuments and cultural heritage, we use international standards to design metadata. For bibliographic records of libraries and digital collections cataloging, we use Dublin Core to record the content attributes. Similar to user profile component, we encode ordinal variables to replace lengthy words g  Feedback When guests or members browse the content, our system will make recommendations based on proper techniques. When the system presents the final results of personal recommendation, users can select some conditions to prune those do not interest them or unimportant information such as a place too far away from the userês current location 2  Recommendation Technique Recommendation model is the core of the system. We use many recommendation methods such as \(a\larizationbased, \(b\ommunity filtering, and \(c\ demographic profile d\pertise-based as well as association rules mining in our system a  Popularization-based In this approach we recommend contents to users with higher ranking value of users and experts, meanwhile we also consider the complaint value of users in the formula The formula of calculation is shown in \(2    1  1    ii i i Popu C userRank C expertRank C complaint C          2 Here is the ratio between the number of rankings and the number of complaints while   Currently, the weight is 50% for each of them and it can be adjusted as needed. The definition of ranking and complaint are described as follows Definition of ranking only registered members or domain experts can rank content using values from 0 to 5 no matter if they have any engagement with the recommendation result. They can update the ranking value in the system anytime as depicted in formula \(1 Definition of complaint members can provide complaints for the content in the system no matter if they have any engagement or not. Similar to the user ranking there are some options to measure memberês complaints that may affect the final complaint value stored in the system b  Community Filtering When a user registers with the system to become a member, s/he will be assigned to a unique group based on the user profile. After that, the system will increase his/her weight value with ranking value from the ranking database and complaint value from the complaint database accordingly. The formula to calculate the weight value of Item C j to member M i  is shown in \(3  1 11    1 mn ij ij ij ij userRank M C Complaint M C Commu M C mn             3 Given all items C j with j 1 to n and all m-1 members of the group that M i belongs to, the system uses \(3 calculate each Commu value and selects the item with the largest weight value to recommend to member M i  c  Demographic Profile Before using demographic profile technique to recommend content to members, there are two things must be preprocessed: one is to use clustering to assign members into various groups, the other one is to use classification to distribute content to corresponding groups When a member logs in, the system will try to make recommendation based on his/her preference. First the system uses user profile to find out the userês group, and then calculates the similarity between content groups. After that, we can start to compare all contents in the groups with the memberês preference. The formula to calculate the weight value of current member with content group c g is shown in \(4     ig D emo C similarity memberPreference C   4 Where C g contains C 1 to C n and C i has the highest similarity value 1 i n  d  Expertise-based The knowledge of domain experts and their recommendations to the users are no doubt the most convincing content and can be of high quality in terms of meeting user expectations. In order to fully utilize the advice of experts to make proper recommendations for the users in appropriate groups, we carefully assess the 
75 


similarity of user groups and the suitable content to match their needs and high expectations. The formula of calculation is shown in \(5  1   n ij j i expertRank C Expt C n     j  1 to m  5 Here n is the total number of experts m is the total number of content items, and expertRank\(C ij  is the ranking value of content item C j  from the expert i For each content item, the system calculates the ranking value from expert matrix and divides it by the number of experts 3  Our Proposed Hybrid Approaches Our system employs a hybrid approach to make recommendations according to user types. With the help of recording user behavior for members, we take advantage of community filtering and demographic profile to combine with association rules mining from web access log for recommendation; otherwise we use both of popularizationbased and expertise-based methods for guests. The two hybrid approaches are described in more details below a  Populace-based We use the technique of data mining to find out the potential association rules from web access log. The source data are continuous and have a large amount of user behavior history. The mined results are reliable and useful for recommending content to quests. The amount of web access log increases tremendously every day. Since the traditional algorithms are not suitable, we devise an incremental approach to mining potential patterns or related rules  Figure 4. The Architecture of Populace-based Approach  The architecture of populace-based approach for guests is shown in Figure 4. We use many efficient methods and techniques to find out precise recommendation contents that match user preferences and needs. This approach takes advantage of popularization-based and adopts association rules mining from web access log First, the system calculates all content values by adding ranking values of experts and members as well as complaint values stored in the database.  We also use an incremental mining algorithm to mining frequent patterns and generating association rules that can predict the content of userês next browsing step. After that, the system will add the content value after normalization and the support count of content Finally, we sort the recommendation content list in descending order and present top-n recommendations to the guest. The formula of calculation is shown in \(6        i ii ii i Avg sup C popularization C com plaint C initialPhase M ax ARs C initialPhase C otherwise Populace C           6 In the initial phase, the system will calculate the average of adding support count of content to the total values of ranking and complaint values after normalization Later, if a user hits any recommended content, then the system will be able to consider the results of association rule mining as a factor to recognize which one has a higher recommendation value b  Neighbor Collaboration For registered members we use another hybrid approach called neighbor collaboration as shown in Figure 5. This approach takes advantage of community filtering and demographic profile techniques. We use these two techniques to provide higher precision recommendation contents that can satisfy memberês preferences, needs and personal information by considering user profile and community groupsê historical behaviors  Figure 5. The Architecture of Neighbor Collaboration Approach  First, as shown in Figure 5, the system finds out which group the member belongs to, and then calculates related content values according to the information of ranking and complaint matrices. After that, we compare the similarity between the memberês group and all content groups Meanwhile, the incremental mining algorithm is used to mining frequent patterns of recommended content and then 
76 


generating association rules from web access log. Finally we sort the recommendation content list after normalization from higher to lower and present the top-n recommendations to the member. The formula of calculation is shown in \(7         i ij ji ii i Avg sup C Community M C Demographic M C initialPhase Max ARs C initialPhase C otherwise Neighbor C    7 D  Database  Several databases are used in our system. User profile database records the userês personal information such as age hobby, and location when they register to the system Content database is used to store all the content information including the multimedia data of picture and audio as well as statistic description. Web access log database stores all the users behavior records when they browse and find out the potential patterns. Ranking database is used to record member and expertês ranking to corresponding content items. Complaint database records membersê complaints without considering their engagement with the content items Finally, the knowledge database is used to store the recommendation content from the expert IV  E XPERIMENTAL R ESULTS  In this paper we conduct a case study on a project of the historical monuments and cultural heritage to implement our model and show the feasibility of the proposed approach The process of system implementation is divided into three phases: 1\nput, 2\sformation, 3\lt generation. In the first phase, we input two main components into the model: user profile and content. User profile is used to cluster users into groups, where the members of a group have some of the same interests or live in the same neighborhood. The content will be classified into several groups and each has some attributes in common or with high similarity. Meanwhile we have to do preprocessing for transforming web access log into the format that our model can accept as shown in Table 2 Table 2. The Web Access Log after Preprocessing  In the second phase, the input data will be transformed into the format suitable to our model. Before starting the process of mining the web access log or clustering the user profile, we need to do some preprocessing in the transformation phase. Then we can use the user profile and acquired content to mining the potential rules or clustering users into corresponding groups. When guests or members login, the system will be able to make recommendations according to the user types. The third phase is used to present the result In the experiments, we use real data from the geospatial and multimedia database of the historical monuments and cultural heritage project supported by MOE Taiwan. This project starts from Oct. 2008 to Oct. 2010 and has many types of users, such as student, teacher, and traveler. Until now the system has 67 members and more than 1,200 POI Point of interests\tent. We use their web access log as source data to analyze the prediction precision of recommendation content A  Implementation In our experimental platform, we use Microsoft Windows Server 2008 running on an Intel Quad Core Xeon E5420 2.5 GHz processor with 1333 FSB and 2x6MB L2 cache. MS SQL Server 2008 is used as the database management system. In the client side we use an interactive graphic user interface based on Adobe Flex to display all information on the screen as shown in Figures 6 and 7 Figure 6 shows the initial interface after a user enters our recommendation system. The top left block is an opening film that introduces our system with informative contents and the top right table shows the most popular content items in our system in descending order by recommendation value The bottom part of the main panel displays all contents in the system. Clicking any image or title will display the detail data as shown in Figure 7. One can also use some conditional filters to prune non-interesting contents. In addition, the system also provides basic functions such as user login, register as a member, contact us, forget password etc  Figure 6. System screenshot of the initial interface  Figure 7 displays the detail description of the selected content item in Figure 6. The information includes recommendation value, geospatial location through Google Maps as well as multimedia information such as image video, audio, etc. We use many Adobe Flex components and libraries to present the multimedia data. For example, since 
77 


every content item may have many images and corresponding text data, we use the viewer of electronic book to present the detail as shown in the top right corner of Figure 7  Figure 7. The screenshot of a detailed content item  B  Performance of Prediction Results Although we can recommend content according to different types of users, how to calculate the prediction value is an important issue. We devise a formula to define the accuracy rate of prediction value as shown in \(8  1  nh a a i P osi nr Prediction M nh      8 Whereas the symbols of \(8\cribed as follows nh the number of hit items \(i.e., being selected for viewing nr the number of total recommendation items Posi a   nr position of a th item in the recommendation list 1, where a  1 to nh  For example, if the system recommends 10 content items a to j for member Weiding, and he hits item b in the position 2 and then hits item a in the position 1, then the prediction value of member Weiding is Prediction \(Weiding 10 / 10 2 = 9 5   Finally we use the formula of prediction value to compare the four recommendation techniques used in our two hybrid approaches as shown in Figures 8 and 9 In Figure 8 the result is shown in accordance with the age attribute for the comparison among the four main recommendation techniques and our two proposed hybrid approaches. As we can see the experimental results clearly show that our two hybrid approaches have better accuracy rates than others. One main reason is that we not only consider user profile but also user behavior history as well as potential association rules mined from web access log.  By taking these important factors into account and using complementary techniques, we can recommend more precise content to users that match their preferences better 0 20 40 60 80 100 1 0 y 1 8 y 19y~30y 3 1 y 4 5 y 4 6 y  60y 61y~75y Popularizationbased Community filtering Demographic profile Expertise-based Populace-based Neighbo r Collaboration Figure 8. The accuracy rates of prediction based on the age attribute Currently we have six member groups in our system. The number of member groups will increase when more members register in the system. In Figure 9 we compare recommendation techniques and our hybrid approaches with respect to different member groups. In these results our approaches have prediction accuracy rates of more than 90 because we cluster users into suitable groups and we also use classification to assign content items into proper groups 0 10 20 30 40 50 60 70 80 90 100 m g1 mg2 mg3 m g4 mg5 mg6 Popularizatio n-based Community filt ering Demographic profile Expertisebased Populacebased Neighbor Collaboration Figure 9. The accuracy rates of prediction for six member groups  V  D ISCUSSION  In this section we summarize the advantages and disadvantages of our recommendation model and discuss the extension research topics A  Advantages 1  Our recommendation model is an intelligent and feasible approach that can be deployed in promoting historical monuments and cultural heritage 2  We provide an interactive graphic user interface based on Adobe Flex architecture 3  We use a hybrid approach to recommend more precise content to users 4  Personalized recommendation is made according to the type of users 
78 


5  According to the user feedback and history of user behavior, we can provide more precise personal recommendation B  Disadvantages 1  Difficult to judge the precision of prediction results 2  When the system does not have enough data of user profile and web access log, the precision of prediction result will decrease 3  Need domain experts to add more knowledge data and maintain the knowledge rules periodically to reflect the changes C  Extensions Because this recommendation model is a prototype there are still many improvements to be made in the future Some directions of extension research are described below 1  Source data transformation At this time our model can only transform the source data of historical monuments and cultural heritage. We need to adopt an integrated transformation approach that can be used in other domains easily 2  Metadata definition Any domain has their own metadata; it is hard to integrate with each other. The development of a unified metadata definition for any domain is a challenge 3  Ontology Using semantic expression can make users easily understand the meaning of contents in the proper context 4  Agent mechanism Agents can be used to search content, prune uninterested ones and make inference automatically 5  Active recommendation In our recommendation model, the system make recommendations passively. We need triggering mechanisms to provide recommendation in an automatic way 6  Intelligent assistant To reduce user load, an intelligent assistant can record all the user behavior using some automatic processes based on usersê actions 7  Location awareness service The system can provide in time services to travelers or bikers based on where they are and their user behavior history VI  C ON C LUSION AND F UTURE W ORK  In this paper we propose an intelligent recommendation model with a case study on the historical monuments and cultural heritage to show the feasibility of our approach Different from the past experience of using only one technique to make recommendations to users, we adopt a hybrid approach to recommend content according to the type of user. We also use association rules of data mining technique to find the potential patterns in web access logs while classification is used to assign users into different groups suitable for them. The incremental algorithm of our method can calculate the ranking value of content to be more precise. Finally, a new technique is used to present the recommendation results with interactive graphic user interfaces In the future research we will continue to enhance the recommendation model and deploy this model in the cloud computing environment. Besides, we will also keep working in some extension direction, such as intelligent assistant source data transformation, and location awareness service Finally, another future work is to make recommendations for a group of people instead of an individual A CKNOWLEDGMENT  This research is supported in part by Ministry of Education and National Science Council in Taiwan, under grant numbers NSC 98-2221-E-035-059-MY2 and NSC 982218-E-007-005  R EFERENCES  1   Abolghasem Sadeghi Niaraki, Kyehyun Kim. Ontology based personalized route planning system using a multi-criteria decision making approach. Expert Systems with Applications 36 \(2009 pp.2250-2259 2   Angel Garcia-crespo, Javier Chamizo, Ismael Rivera, Myriam Mencke, Ricardo Colomo-Palacios, Juan Miguel Gomez-Berbis SPETA: Social pervasive e-Tourism advisor. Telematics and Informatics, 26 \(2009\ pp. 306-315 3   Bo Shao, Dingding Wang, Tao Li, and Mitsunori Ogihara. Music Recommendation Based on Acoustic Features and User Access Patterns. IEEE Transactions on Audio, Speech, Language Processing, vol. 17, no. 8, November 2009. pp.1602-1611 4   Dirksen Liu and Maiga Chang. Recommend Touring Routes to Travelers According to Their Sequential Wandering Behaviors IEEE 10 th International Symposium on Pervasive Systems Algorithm, and Networks, 2009. pp. 350-355 5   Hyoseop Shin, Minsoo Lee, and Eun Yi Kim. Personalized Digital TV Content Recommendation with Integration of User Behavior Profiling and Multimodal Content Rating. IEEE Transactions on Consumer and Electronics, vol. 55, issue. 3, August 2009. pp. 14171423 6   Katerina Kabassi. Review Personalizing recommendations for tourists. Telematics and Informatics 27 \(2010\, pp. 51-66 7   Loc Nguyen, Phung Do. Learning Concept Recommendation based on Sequential Pattern Mining. IEEE 3 rd International Conference on Digital Ecosystems and Technologies, 2009. pp. 66-71 8   Mei-Ling Shyu, Choochart Haruechaiyasak, Shu-Ching Chen and Na Zhao. 2005. Collaborative Filtering by Mining Association Rules from User Access Sequences. IEEE International Workshop on Challenge in Web Information Retrieval and Integration, 2005 9   Rana Forsati, Mohammad Reza Meybodi, Afsaneh Rahbar. An Efficient Algorithm for Web Recommendation Systems. IEEE/ACS International Conference on Computer Systems and Applications 2009. pp. 579-586    Silvia Schiaffino, Analia Amandi. Building an expert travel agent as a software agent. Expert Systems with Applications 36 \(2009\ pp 1291-1299    Tonderia Maswera, Ray Dawson, Janet Edwards. E-commerce adoption of travel and tourism organisations in South Africa Kenya, Zimbabwe and Ugande. Telematics and Informatics 25 2008\. pp. 187-200    Yan Zheng Wei, Luc Moreau, and Nicholas R. Jennings. Learning Usersê Interests by Quality Classification in Market-Based Recommender Systems. IEEE Transactions on Knowledge and Data Engineering, vol. 17, no. 12, December 2005. pp. 1678-1688    Yuxia Huang, Ling Bian. A Bayesian network and analytic hierarchy process bases personalized recommendations for tourist attractions over the internet. Expert Systems with Application 36 2009\, pp. 933-943  
79 


020\021 n 020\021 n r\021\027 r\030\031\032 r\021\027 r\030\031\032 r\021\027 r\030\031\032 002\003\004 002\003\004 C  002\005\006\007 006\f\002 Proj. DBs of <A,B  Proj. DBs of <A,B,C   its supports and con\223dence can be computed from the projected databases w.r.t in is is neither in Seq ID  Statistics PHT Fig 1  line 6 in Algorithm 2 for any with 022\005 A  C        Proj. DBs of <A,C                                             i.e i.e i.e r 021 002 003\004\005 003\004\005 D Algorithm Comparative Analysis 016\017\017 r 016\017\017 021 016\017\017 016\017\017 007\b\002 005 005 016\017\017 007\b\002 004 004 004 003 004 003 004 004 004 005 005 005 005 005 005 005 005 005 005 005 005 005 005 005 005 005 007 007 007 002 003 003 002 006 007 007 033 033 033 033 033 033 033 033 033 B  034 034 034 034 034 034 034 034 034 034 034 034 034 034 034 034 034 n 034 034 034 034  013 032 032 013  t\r\016 t\r\016 007\b\b  b\004 b\004 b\004 b\004 b\004 b\f\002 b\004 b b b\004 b\004 b\004 b 002\003\004\005\006 006 006 006 b b 006 006 006   and       yet the pre-condition  The worst case complexity of mining frequent patterns of length at most is O  The complexity of our approach is at most O       The 223rst term of the 223rst formula i.e 007\b\002 r 007\b\002 007\b\002 007\b\002 007\b\002 007\b\002 007\b\002 007\b\002 021 007\b\002 007\b\002 021 007\b\002 007\b\002 021 007\b\002 007\b\002 007\b\002 007\t\n\013 007\t\n\013 007\t\n\013 007\t\n\013 007\t\n\013 007\t\n\013 007\t\n\013 007\t\n\013 007\t\n\013 intoa bucket using 034 021 021 021 021 034 034 034 034 034 034 034 034 020 017 020 017 n 034 021 034 021 034  032     034   034    034  e.g 007\007 017\020\021\022 017\020\021\022 017\020\021\022 003\006\007\b 003\006\007\b 003\006\007\b Pre\223x Hash Tree 034 b  034 b  034 b  We store Eliminating Redundant Rules 004\002\003 004\002\003 t t t t 002 003 003 002 002 004 003 003 003 003 004 005 005 005 005 005 005 005 005 005 1051 032\034 032#\b 032\034 032#\b 032#\b 032#\b 032#\b 032\034 032\034 032#\b 032 032 032 032\034 032#\b 032 032 032 032 weuse the formula de\223ned in Proposition 1 and use the projected database Pre\223x Hash-Tree PHT Data Structure Embedding the Anti-Monotonicity Property Sequence     002 002  002 r 003 016 003 016 003 004 003    002 003  002 r 003 016 003 016 003 004 003   002\003  iii how to compute the supports and con\223dence of a rule iv how to test whether a rule is redundant  For  all patterns are stored in the PHT in reverse order Each node in this tree represents a pattern obtained by following the path from root to this node and is associated with its corresponding projected databases We store both stored in the PHTs To compute   the same triple  and 3 Composition of premises and consequents to form non-redundant rules Let us also assume that both the premises and consequents have a maximum length of 003 002 003 005 002 005 and  if we 223nd has a low support or con\223dence we can skip scanning  and continue to and and and nor and and and 002\003\004\005\006 002\003\004\005\006 with each  For example in Figure 1 suppose it is the PHT of post-condition candidates for some pre-condition candidate we compare to all occurrences of  is the size of  For these cases we need to re-scan the database to 223nd the instance support of has a sequence support instance support and con\223dence of 2 2 and 100 These are the same as those of rule  Although subsumes is not considered redundant due to by Theorem 2 Similarly is not considered redundant due to 220s with the same supports and con\223dence  a hash table Then for each bucket we want to remove the redundant rules   the ones with their super-pattern in the same bucket Consider a database containing a set of sequences with events coming from an alphabet  database scan operations Consider a set of patterns and works in two steps 1 Mine a pruned set of pre-conditions obeying minimum sequence support threshold and Theorem 2 from  2 For each pre-condition mine a set of post-conditions obeying the minimum con\223dence and minimum instance support thresholds At the end of the 223rst step for each pre-condition  LKL08 constructs a projected database  Another mining operation is then performed on this projected database The complexity is thus O  once a rule has a low support or low con\223dence we can skip scanning the whole subtree below and look for common sequences where the premise  For computing by Theorem 3 We map rules  and the last is due to the composition of premises and consequents to form rules Remember many rules can be constructed without requiring any additional database scan operation Some however require the re-scanning of the database to compute their instance support values see Section VI-C The algorithm in LKL08 tak es as input a s equence database Computing Supports and Con\223dence to facilitate the pairing procedure ii in what order to pair each We use a pre\223x hash-tree PHT data structure to organize the set of candidates  Each projected database is stored implicitly discussed in Section VI-B Each node has a hash table to quickly locate one of its child in a constant lookup operation given an event Figure 1 shows an example of a PHT  in a PHT in reverse order and scan in DFS order By following the DFS order to visit nodes in the PHT a post-condition is always scanned earlier than its backward extensions This feature enables us to embed the anti-monotonicity properties of con\223dence Theorem 1 and sequence support into the pairing algorithm  in PHTs For a rule occurs 223rst before  The ratio of the occurrences of which is usually in the PHTs However there are cases where is a signi\223cant rule while In line 8 of Algorithm 2 we want to remove redundant rules Some redundant rules have been pruned early in stage I But to eliminate all redundant rules this step is needed To illustrate the need for this stepafter applying Theorem 2 and 3 consider the following database    The rule from such a database is O  the worst case complexity of constructing from patterns in  database scan operations In our analysis we use database scan as the unit of operation We ignore the time needed to eliminate redundant rules since no database scan operation is involved In BOB we perform 1 two mining operations on the original sequence database to obtain the set of  2 construction of  is due to the mining of the premises the second is due to the mining of the consequents the third and fourth are due to the construction of  Notice that if 


006 006 016\017\017 007\b\002 007\b\002 r conf=50 In Figure 4 and 5 we plot the runtime needed and the number of rules mined from the D10C10N10R0.5 dataset when varying the minimum sequence support threshold and the minimum con\223dence threshold respectively Again BOB is up to 10 times faster than LKL08 and the gap between Datasets 022\005 1052 034   034 b  Environment and Pattern Miners 005 005 033 033   we store summary information e.g Rules right for D5C20N10R0.5 dataset when varying min  Results 005 005 005 all pruning properties do not work In the worst case all possible rules up to a particular length are signi\223cant and none of them is redundant The following points summarize reasons behind the superiority of our approach as compared to 1 We employ two new pruning strategies described by Theorem 1 and 3 These strategies are embedded into our new mining algorithm to remove search space not pruned before by the approach in 2 The projected database created in could be v ery lar ge especially if patterns 002\003\004 002\003\004 b b\004 b\004 b\004 b\004 b\004  032 016\017\017 r especially in cases where the number of repetitions of Fig 2 Runtime left  i.e and worst case is not large the complexity of our approach is smaller by an exponential factor than that of LKL08 In the worst case in a sequence many times For a premise  the projected database could be larger than the original  for each pattern In Figure 2 and 3 we plot the runtime needed and the number of rules mined from D5C20N10R0.5 when varying the minimum sequence support threshold and the minimum con\223dence threshold respectively For simplicity sake we set the minimum instance support threshold to be equal to the minimum sequence support threshold Each of the runtime graphs has two lines corresponding to the two algorithms\220 runtime at various thresholds We note that BOB is up to one order of magnitude i.e 10 times faster than LKL08 BOB pruning strategy is also effective in reducing required runtime when the minimum con\223dence threshold is raised from 50 to 90 On the other hand no signi\223cant performance change could be noticed in LKL08 case though However in frequent pattern mining worst case analysis is often not interesting This is so as in the recur within each sequence containing it is high Also projecting w.r.t a premise can produce a very localized dataset i.e a single sequence is split into its many suf\223xes resulting in hard-to-mine dense dataset with a large number of frequent patterns even at a high support threshold 3 During the mining of premises and consequents from  This information is used to immediately prune insigni\223cant rules not satisfying minimum sequence support and con\223dence thresholds see Section VI-C For this pruning we do not need to re-scan the database rather only the summary information needs to be analyzed The algorithm in can only perform database scan operations to prune candidate rules To compare effectiveness of various pruning strategies experiments on various datasets are needed We perform this empirical evaluation in Section VII VII E MPIRICAL E VA L UAT I O N Experiments have been performed to evaluate the scalability of our approach A case study on analyzing traces from an instant messaging application has also been conducted All experiments are performed on a Pentium Core 2 Duo 3.17GHz PC with 3GB main memory running Windows XP Professional Algorithms are written in Visual C#.Net We compare the approach presented in with our approach W e refer to the tw o approaches as LKL08 and BOB respectively To reduce the threat of external validity i.e the generalizability of our result we investigate a variety of datasets Four datasets two synthetic and two real are studied IBM synthetic data generator is used It is modi\223ed to produce sequences of events rather than sequences of sets of events The generator accepts a set of parameters We focus on four parameters D C N and R They correspond to the number of sequences in 1000s the average number of events per sequence the number of different events in 1000s and the repetition level range 0 to 1 respectively All other parameters of the synthetic data generator are set to their default values We experiment with two synthetic datasets D5C20N10R0.5 and D10C10N10R0.5 Dataset D5C20N10R0.5 contains sequences with an average length of 64.4 and a maximum length of 275 Dataset D10C10N10R0.5 contains sequences with an average length of 31.2 and a maximum length of 133 D5C20N10R0.5 has less sequences of longer lengths On the other hand D10C10N10R0.5 has more sequences of shorter lengths We also experiment on a click stream dataset   Gazelle dataset from KDDCup 2000 which has also been used to evaluate frequent sequential pattern miners i.e CloSpan and BIDE The dataset contains 29,369 sequences with an average length of 3 and a maximum length of 651 Compared to the two synthetic datasets this real data has a lower average length but contains sequences of longer lengths The gap in the lengths of long and short sequences is also wider To evaluate our algorithm performance on mining from program traces we generate traces from TotInfo program in the Siemens Test Suite The test suite comes with 893 correct test cases We run these test cases to obtain 893 traces Each trace is a sequence of events where every event is a method invocation We refer this dataset as the TotInfo dataset The TotInfo dataset contains sequences with an average length of 12.1 and a maximum length of 136 s-sup at min  


Rules right for D5C20N10R0.5 dataset when varying min  conf at min  s-sup 2.4 the performance of BOB and LKL08 is increased when the minimum con\223dence threshold is raised from 50 to 90 We do not experiment mining at lower con\223dence thresholds as low con\223dence rules have little use and are likely to only capture noises Rules right for D10C10N10R0.5 dataset when varying min  Rules right for D10C10N10R0.5 dataset when varying min  conf at min  s-sup 0.5 In Figure 6 and 7 we plot the runtime needed and the number of rules mined from the Gazelle dataset when varying the minimum sequence support threshold and the minimum con\223dence threshold respectively We notice that BOB improves the performance of LKL08 by two orders of magnitude i.e more than 100x faster At support level 0.020 LKL08 is not able to 223nish within 8 hours Thus BOB can successfully mine rules at a lower support threshold that is not minable by LKL08 in a reasonable amount of time Rules s-sup at min  Rules right for for Gazelle dataset when varying min  Rules right for TotInfo dataset when varying min  s-sup at min  Rules right for TotInfo dataset when varying min  s-sup 5.6 LKL08 is not able to run 006 006 006 006 006 006 006 006 006 006 006 006 006 006 conf=50 conf=50 sup=0.034 In Figure 8 and 9 we plot the runtime needed and the number of rules mined from the TotInfo dataset when varying the minimum sequence support threshold and the minimum con\223dence threshold respectively At all support levels shown in Figure 8 and 50 minimum con\223dence threshold LKL08 is not able to run due to an out of memory exception while BOB is able to 223nish in less than 2 minutes When we decrease the minimum con\223dence threshold we see an exponential increase in the runtime of LKL08 BOB runtime on the other hand remains the same LKL08 performs a pattern mining operation for every projected database of each mined pre-conditions this causes the high runtime values  conf=50 Fig 3 Runtime left  Fig 4 Runtime left  Fig 5 Runtime left  Fig 6 Runtime left  Fig 7 Runtime left  conf\(at min  Fig 8 Runtime left  Fig 9 Runtime left  conf at min  In past studies recurrent rules either in full or restricted form have been mined from various software datasets 27 Mined rules correspond to interesting temporal properties extracted from execution traces of programs In Lo et al mined rules from e x ecution traces generated by running test cases of JBoss Application Server In in a study within Microsoft Lo et al mined a restricted form of two-event recurrent rules with quanti\223cation from Windows device drivers and other Windows applications In this study we consider another trace data obtained from user interactions with a drawing utility of an instant messaging application Many systems like the one that we analyze do not 1053 Case Study s-sup at min  right for Gazelle dataset when varying min  


Handbook of Statistics Int Conf on Data Eng Int Conf on Tools and Algo for the Constr and Ana of Sys Knowledge and Info Sys Int Conf on Data Eng Int Conf on Data Eng Int Conf on Data Eng Int Conf on Database Sys for Adv App Int Conf on Soft Eng Model Checking Soft Product Line Conf Euro Conf on Soft Maintenance and Re-Eng Data Mining and Knowledge Disco Data Mining and Knowledge Disco SIAM Int Conf on Data Mining Int Conf on Soft Eng  getMyJID  Cambridge 2004  H K ugler  D Harel A Pnueli Y  Lu and Y  Bontemps 215T emporal logic for scenario-based speci\223cations,\216 in  2006  D Lo S.-C Khoo and C Liu 215Ef 223cient mining of iterati v e patterns for software speci\223cation discovery.\216 in  2007  204\204 215Ef 223cient mining of recurrent rules from a sequence database 216 in  2008  M Dwyer  G A vrunin and J Corbett 215P atterns in property speci\223cations for 223nite-state veri\223cation.\216 in  1999  E Clark e O Grumber g and D Peled  2004  R Capilla and J Duenas 215Light-weight product-lines for e v olution and maintenance of web sites,\216 in  2003  R Agra w al and R Srikant 215F ast algorithms for mining association rules,\216 in  1994  204\204 215Mining sequential patterns 216 in  1995  H Mannila H T oi v onen and A V erkamo 215Disco v ery of frequent episodes in event sequences,\216  1999  M Huth and M Ryan  2005  W  Jamroga 215 A temporal logic for stochastic multi-agent systems 216 in  2008  P  Monteiro D Ropers R Mateescu A Freitas and H de Jong 215Temporal logic patterns for querying dynamic models of cellular interaction networks,\216  2008  M Arenas P  Barcelo and L Libkin 215Combining temporal logics for querying xml documents,\216 in  2007  S Zhang 215 A temporal logic for supporting historical databases 216  2000  N P asquier  Y  Bastide R T aouil and L Lakhal 215Disco v ering frequent closed itemsets for association rules,\216 in  1999  M Zaki 215Mining non-redundant association rules 216  2004  J Pei J Han B Mortaza vi-Asl H Pinto Q Chen U Dayal and M.C Hsu 215Pre\223xspan Mining sequential patterns ef\223ciently by pre\223xprojected pattern growth.\216 in  2001  X Y an J Han and R Afhar  215CloSpan Mining closed sequential patterns in large datasets.\216 in  2003  J W ang and J Han 215BIDE Ef 223cient mining of frequent closed sequences.\216 in  2004  S Harms J Deogun and T  T adesse 215Disco v ering sequential association rules with constraints and time lags in multiple sequences.\216 in  2002  G Garriga 215Disco v ering unbounded episodes in sequential data 216 in  2003  B Ding D Lo J Han and S.-C Khoo 215Ef 223cient mining of closed repetitive gapped subsequences from a sequence database.\216 in  2009  M Hutchins H F oster  T  Goradia and T  Ostrand 215Experiments on the effectiveness of data\224owand control-\224ow-based test adequacy criteria,\216 in  1994  D Lo G Ramalingam V P  R anganath and K V asw ani 215Mining quanti\223ed temporal rules Formalism algorithms and evaluation.\216 in  2009  215Jeti V ersion 0.7.6 Oct 2006 http://jeti.sourcefor ge.net 1054 PictureChat showWindow PictureChat PictureHistory      vol 1 pp 259\205289 1997  M Spiliopoulou 215Managing interesting rules in sequence mining 216 in method This identi\223er is later af\223xed to the object drawn Finally the canvas records the operation in a object VIII C ONCLUSION This work proposes a new approach to mine recurrent rules in the form of 215Whenever a series of events occurs another series of events also occurs\216 The proposed approach is more scalable than the previous approach in Rather than performing a mining operation for each non-redundant pre-condition as proposed in our ne w approach emplo ys a number of new pruning strategies embedded in a new mining algorithm that requires only two mining and some additional database scanning operations Under a condition which holds in many cases the complexity of the proposed algorithm is smaller by an exponential factor than the complexity of the one proposed in W e ha v e e xperimented on v arious datasets synthetic and real Experiments have shown that the new algorithm improves the runtime of the previous algorithm by up to two orders of magnitude In the future we are looking into more applications of the mining algorithm and opportunities to further speed up the mining process R EFERENCES  M Gupta and S Ray  215Sequence pattern disco v ery with applications to understanding gene regulation and vaccine design.\216 in PC-  nu.fw.jeti.plugins.drawing.shapes.PictureChat PH -  nu.fw.jeti.plugins.drawing.shapes.PictureHistory Fig 10 Jeti Instant Messaging Application Drawing Scenario We use Jeti a popular instant messaging application which supports many features We record 30 interactions with the drawing tool of Jeti application and collect 30 traces The traces have an average length of 1,430 and a maximum length of 11,838 events Each event is a method call The purpose of this case study is to show the usefulness of the mined rules by discovering frequent and signi\223cant rules describing behaviors of the drawing sub-component of Jeti Using minimum sequence support and instance support of 25 traces/instances and a minimum con\223dence threshold of 90 BOB could complete in 57 seconds while LKL08 is only able to complete in 2844 seconds A total of 19 rules are collected after applying the following post-processing steps 1 Density Only report a mined rule iff the number of its unique events is more than 80 of its length 2 Ranking Order mined rules according to their lengths and support values A sample mined rule is shown in Figure 10 The rule captures the scenario when a user draws an object e.g a rectangle a line etc to a canvas First a resource i.e a Map object is created by a KDD Int Conf on Very Large Data Bases Paci\223c Rim Int Conf on Multi-Agents Int Symp on Intel Sys Euro Conf on Prin and Prac of Knowledge Disco in Databases come with suf\223cient test cases We ask a student who is not involved in this study to interact with the system  MIT Press 1999  S Deelstra M Sinnema and J Bosch 215Experiences in softw are product families Problems and issues during product derivation,\216 in Euro Conf on Prin and Prac of Knowledge Disco in Databases Bioinformatics ICDT Legend Consequent Map PC.createWritersMap void PC.showWindow void PC.unselect void PC.showWindow JID PC.getMyJID void PC.draw\(Shape  PH.addShapeDrawnByMe\(\203 Premise Logic in Computer Science ICDT Working Conf on Reverse Eng method are made by different callers When the application starts an empty window is 223rst shown or displayed After an object is drawn the drawn object i.e rectangle line etc would request the canvas i.e object Next multiple invocations of object to 215unselect\216 and redraw itself The system next retrieves the identi\223er of the user that draws the object by the invocation of 


TABLE II.     SUMMARY OF METRICS COLLECTED FOR DS2 Load Generator Processor Time Orders/minute Network Bytes Sent/sec Network Bytes Received/Sec Tomcat Processor Time Threads Virtual Bytes Private Bytes MySQL Processor Time Private Bytes Bytes written to disk/sec Context Switches/sec Page Reads/sec Page Writes/sec Committed Bytes In Use Disk Reads/sec Disk Writes/sec I/O Reads Bytes/sec I/O Writes Bytes/sec TABLE I.   AVERAGE PRECISION AND RECALL of Test Scenarios Duration per Test hours Size of Data per Test Avg Precision Avg Recall DS2 4 1 360 KB 100% 52 JPetStore 2 0.5 92 KB 75% 67 Enterprise System 13 8 4.5 MB 93% N/A  


37  Analysis of Test B: The goal of this experiment is to show that the rules generated by our approach are stable under normal system operation. Since Test B shares the same configuration and same load as Test A, ideally our approach should not flag any metric Our prototype did not report any problematic metric in Test B. The output is as expected, since Test B uses the same configuration as Test A and no performance bug was injected Analysis of Test C: In test C, we injected a databaserelated bug to simulate the effect of an implementation error This bug affects the product browsing logic in DS2. Every time a customer performs a search on the website, the same query will be repeated numerous times, causing extra workload for the backend database and Tomcat server Our approach flagged a database related metric \(# Disk Reads/sec Threads and # private bytes signaling that the metrics are violated during the whole test The result agrees with the nature of the injected fault: each browsing action generates additional queries to the database As a result, an increase in database transaction leads to an increase of # Disk Reads/sec. When the result of the query returns, the application server uses additional memory to extract the results. Furthermore, since each request would take longer to complete due to the extra queries, more threads are created in the Tomcat server to handle the otherwise normal workload. Since 3 out of 6 expected problematic metrics are detected, the precision and recall of our approach in Test C are 100% and 50% respectively Analysis of Test D: We injected a configuration bug into the load driver to simulate that a wrongly configured workload is delivered to the system. This type of fault can either be caused by a malfunctioning load generator or by a performance analyst when preparing for a performance regression test [14 In the case where a faulty load is used to test a new version of the system, the assessment derived by the performance analyst may not depict the actual performance of the system under test In Test D, we double the visitor arrival rate in the load driver. Furthermore, each visitor is set to perform additional browsing for each purchase. Figure 7 below shows the violated 


metrics reported by our prototype. The result is consistent with the nature of the fault. Additional threads and memory are required in the Tomcat server to handle the increased demand Furthermore, the additional browsing and purchases lead to an increase in the number of database reads and writes. The extra demand on the database leads to additional CPU utilization Because of the extra connections made to the database caused by the increased number of visitors, we would expect the # context switch metric in the database to be high throughout the test. To investigate the reason for the low severity of a databases context switch rate \(0.03 examined the rules flagged the # context switch metric. We found that the premises of most rules that flagged the context switch metric also contain other metrics that were flagged with high severity. Consequently, the premises of the rules that flagged # context switch are seldom satisfied resulting in the low detection rates of the # context switch metrics. Since 7 out of 13 expected metrics are detected, the precision and recall of our approach in this test are 100% and 54% respectively B. Studied System: JPetStore System description: JPetStore [1] is a larger and more complex e-commerce application than DS2. JPetStore is a reimplementation of Sun's original J2EE Pet Store and shares the same functionality as DS2. Since JPetStore does not ship with a load generator, we use a web testing tool to record and replay a scenario of a user logging in and browsing items on the site Data collection: In this case study, we have conducted two one-hour performance regression tests \(A and B performance signatures are extracted from Test A during which caches are enabled. Test B is injected with a configuration bug in MySQL. Unlike the DS2 case study where the configuration bug is injected in the load generator, the bug used in Test B simulates a performance analysts mistake to accidentally disable all caching features in the MySQL database. Because of the nature of the fault, we expect the following metrics of the database machine to be affected: CPU utilization, # threads context switches, # private bytes, and # I/O read and write bytes/sec Analysis of Test B: Our approach detected a decrease in memory footprint \(# private bytes sec in the database, and increase in # disk reads/sec and 


threads in the database. The I/O metrics include reading and writing data to network, file, and device. These observations align with the injected fault: Since the caching feature is turned off in the database, less memory is used during the execution of the test. In exchange, the database needs to read from the disk for every query submitted. The extra workload in the database  Figure 7. Performance Regression Report for DS2 Test 4 \(Increased Load TABLE III.     SUMMARY OF INJECTED FAULTS FOR DS2 Test Fault Injected Expected Problematic metric A No fault N/A B No fault No problem should be observed C Busy loop injected in the code responsible for displaying  item search results Increase in # I/O reads bytes /sec, and disk read/sec in database Increase in # threads, # private and virtual bytes, and CPU utilization in the Tomcat server D Heavier load applied to simulate error in load test configuration Increase in CPU utilization, # threads private and virtual bytes in the Tomcat server Increase in database CPU utilization disk reads, writes and I/O read bytes per second, and # context switches Increase in # orders/minute and network activities in the load generator 38 translates to a delay between when a query is received and the result is sent back, leading to a decrease in # IO write bytes/sec to the network Instead of an increase, an unexpected drop of the # threads was detected in the database. Upon verifying with the raw data for both tests, we found that the thread count in Test A \(with cache without cache 


and 21 respectively. Upon inspecting the data manually, we do not find that the decrease of one in thread count constitutes a performance problem and this is therefore a false positive Finally, throughout the test, there is no significant degradation in the average response time. Since 4 out of 6 expected problems are detected, our performance regression report has a precision of 75% and recall of 67 C. Studied System: A Large Enterprise System System description: Our third case study is conducted on a large distributed enterprise system. This system is designed to support thousands of concurrent requests. Thus, performance of this system is a top priority for the organization. For each build of the software, performance analysts must conduct a series of performance regression tests to uncover performance regressions and to file bug reports accordingly. Each test is run with the same workload, and usually spans from a few hours to a few days. After the test, a performance analyst will upload the metric data to an internal website to generate a time series plot for each metric. This internal site also serves the purpose of storing the test data for future reference. Performance analysts then manually evaluate each plot to uncover performance issues. To ensure correctness, a reviewer must sign off the performance analysts analysis before the test can be concluded. Unfortunately, we are bounded by a NonDisclosure Agreement and cannot give more details about the commercial system Data collection: In this case study, we selected thirteen 8hour performance regression tests from the organizations performance regression testing repository. These tests were conducted for a minor maintenance release of the software. The same workload was applied to all tests. In each test, over 2000 metrics were collected Out of the pool of 13 tests, 10 tests have received a pass status from the performance analysts and are used to derive performance signatures. We evaluated the performance of the 3 remaining tests \(A, B and C the performance analysts assessment \(summarized in table 4 In the following sections, we will discuss our analysis on each target test \(A, B and C Analysis of Test A: Using the history of 10 tests, our approach flagged all throughput and arrival rate metrics in the system. The rules produced in the report imply that throughputs 


and arrival rates should fall under the same range. For example component A and B should have similar request rate and throughput. However, our report indicates that half of the arrival rates and throughput metrics are high, while the other half is low. Our approach has successfully uncovered problems associated with the arrival rate and throughput in Test A that were not mentioned in the performance analysts report. We have verified our finding with a performance analyst. Our performance regression report has a precision of 100 Analysis of Test B: Our approach flagged two arrival rate metrics, two job queue metrics \(each represents one subprocess consulting with the time-series plots for each flagged metric as well as the historic range, we found that the # database scans/sec metric has three spikes during the test. These spikes are likely the cause of the rule violations. Upon discussing with a performance analyst, we find that the spikes are caused by the systems periodic maintenance and do not constitute a performance problem. Therefore, the # database scans/sec metric is a false positive. Our performance analysis report has a precision of 80 Analysis of Test C: Our approach did not flag any rule violation for this test. Upon inspection of the historical value for the metrics noted by the performance analyst, we notice that the increase of # database transactions/sec observed in Test C actually falls within the metric historical value range. Upon discussing with the Performance Engineering team, we conclude that the increase does not represent a performance problem. In this test, we show that our approach of using a historical dataset of prior tests is more resistant to fluctuations of metric values. Our approach achieves a precision of 100 The case studies show that our approach is able to detect problems in metrics when the faults are present in the systems Our approach detects problematic metrics with high precisions in all three case studies. In our case studies with the two open source systems, our approach is able to cover 50% and 67% of the expected problematic metrics VI. DISCUSSION AND FUTURE WORK A. Quantitive Techniques Although there are existing techniques [10, 11] to correlate anomalies with performance metrics by mining the raw performance data without discretization, these techniques 


usually assume the presence of Service Level Objectives \(SLO that can be used to determine precisely when an anomaly occurs. As a result, classifiers that predict the state of SLO can be induced from the raw performance data augmented with the SLO state information. Unfortunately, SLOs rarely exist during development. Furthermore, automated assignment of SLO states by analyzing metric deviations is also challenging as there could be phase shifts in the performance tests, e.g., the spikes do not align. These limitations prevent us from using classifier based techniques to detect performance regression TABLE IV.      SUMMARY OF ANALYSIS FOR THE ENTERPRISE SYSTEM Test Performance Analysts Report Our Findings A No performance problem found Our approach identified abnormal behaviors in system arrival rate and throughput metrics B Arrival rates from two load generators differ significantly Abnormally high database transaction rate High spikes in job queue Our approach flagged the same metrics as the performance analysts analysis with one false positive C Slight elevation of database transactions/sec. No metric flagged  39 B. Sampling period and Metric Discretization We choose the size of time interval for metric discretization based on how often the original data is sampled. For example an interval of 200 seconds is used to discretize data of the enterprise system, which was originally sampled approximately every 3 minutes. The extra 20 second gap is used because there was a mismatch in sampling frequencies for some metrics. We also experimented with different interval lengths. We found that less metrics are flagged as the length of the interval increases, while precision is not affected 


In our case studies, we found that the false negatives metrics that were expected to show performance regressions but were not detected by our approach no rule containing the problematic metrics was extracted by the Apriori algorithm. This was caused by our discretization technique sometimes putting all values of a metric that had large standard deviation into a single level. Candidate rules containing those metrics would exhibit low confidence and were thus pruned. In the future, we will experiment on other discretization techniques, such as Equal Width Interval Binning C. Performance Regression Testing Our approach is limited to detecting performance regressions. Functional failures that do not have noticeable effect on the performance of the system will not be detected Furthermore, problems that span across the historical dataset and the new test will not be detected by our approach. For example, no problem will be detected if both the historical dataset and the new test show the same memory leak. Our approach will only register when the memory leak worsens or improves D. Passed Tests The historical dataset from which the association rules are generated should contain tests that have the same workload configuration, preferably same hardware, and exhibit correct behavior. Using tests that contain performance problems will decrease the number of frequent item sets extracted, making our approach less effective in detecting problems in the new test. In our case study with the enterprise system, we applied the following measure to avoid adding problematic tests to our historical dataset We selected a list of tests from the repository that have received a pass status from the performance analyst We manually examined the performance metrics that are normally used by a performance analyst in each test from the list of past test to ensure no abnormal behavior was found E. System Evolution and Size of Training Data The system is often updated to support new environments or requirements. These updates may lead to changes in performance. A large variability in metric values will negatively affect the confidence of association rules generated 


in our approach. Therefore, it is necessary to update the set of tests included in the historical dataset. We are currently studying the effect of using a sliding window to select prior tests to include in the historical dataset. A sliding window allows us to automatically discard outdated tests that no longer reflect the current systems performance. However, the optimal size of the sliding window will likely be project-dependent since each project has different release frequency Alternatively, the historical dataset can also be derived from within the run. For example, the first hour of the current test can be used to derive performance signatures. Assuming that the system runs correctly during the first hour, the performance signature generated from this historical dataset will be useful to assess the stability of the system F. Hardware Differences In practice, performance regression tests of a system can be carried out on different hardware. Furthermore, third party components may change in between tests. In the future, we plan to improve our learning algorithm so that, given a new test, our tool will automatically select the tests from the repository with similar configurations G. Automated Diagnosis Our approach automatically flags metrics by using association rules that show high deviations in confidence between the new tests and the historical dataset. These deviations represent possible performance regressions or improvements and are valuable to performance analysts in assessing the system under test. Performance analysts can adjust the deviation threshold to restrict the number of rules used and, thus, limit the number of metrics flagged. Alongside with the flagged metrics, our tool also displays the list of rules that the metric violated. Performance analysts can inspect these rules to understand the relations among metrics. From our case study, we notice that some of the rules produced are highly similar. In the future, we will research for ways to merge similar rules to further condense information for performance analysts to analyze The association rules presented in our performance regression report represent metric correlations rather than causality. Performance analysts can make use of these correlations to manually derive the cause of a given problem VII. RELATED WORK 


Our goal in this work is to detect performance problems in a new test using historical data. Existing approaches monitor or analyze a system through one of two sources of historical data execution logs and performance metrics A. Analyzing Execution Logs Reynolds el al. [18] and Aguilera et al. [22] developed various algorithms for performance debugging on distributed systems. Their approach analyzes message trace of system components to infer the dominant causal paths and identify the components that account for a significant fraction of the systems latency. Unfortunately, the accuracy of the inferred paths decreases as the degree of parallelism increases, leading to low precision in identifying problematic components. Our approach is different from Reynoldss and Aguileras in that we pinpoint performance issues on the metric level rather than locating the system components that contribute significantly to system latency. Jiang et al. introduce a technique [14] to 40 identify functional problems in a load test from execution logs The authors extended this approach to analyze performance in scenarios as well as in the steps of each scenario [13]. Chen et al. proposed Pinpoint [21] to locate the subset of system components that are likely to be the cause of failures. Our work is different from Pinpoint in that Pinpoint focuses on identifying system fault rather than performance regression which can occur even when the system functions correctly In contrast to the above studies, which analyze execution logs, our approach analyzes performance metrics to identify performance problems B. Analyzing Performance Metrics Bondi [9] presented a technique to automatically identify warm-up and cool-down transients from measurements of a load test. While Bondis technique can be used to determine if a system ever reaches a stable state in the test, our approach can detect performance problems at the metric level Cohen et al. [11, 12] applied supervised machine learning techniques to induce models on performance metrics that are likely to correlate with observed faults. Bodik et al. improved Cohens work [8] by using logistic regression. Our approach is different from the above work as we do not require knowledge of violations of Service Level Objectives Jiang et al. proposed an approach [16] for fault detection 


using correlations of two system metrics. A fault is suspected when the portion of all derived models that report outliers exceeds a predefined threshold. Our approach is based on frequent item sets that can output correlations of more than two metrics. Performance analysts can leverage these metric correlations to better understand the cause of a fault. Jiang et al 15] proposed an approach to identify clusters of correlated metrics with Normalized Mutual Information as similarity measure. The authors were able to detect 77% of the injected faults and the faulty subsystems, without any false positives While the approach in [15] can output only the faulty subsystems, our approach can detect and report details about performance problems, including metrics that deviate from the expected behaviors VIII. CONCLUSIONS It is difficult for performance analysts to manually analyze performance regression testing results due to time pressure large volumes of data, and undocumented baselines Furthermore, subjectivity of individual analysts may lead to incorrect performance regressions being filed. In this paper, we explored the use of performance regression testing repositories to support performance regression analysis. Our approach automatically compares new performance regression tests to a set of association rules extracted from past tests. Potential performance regressions of system metrics are presented in a performance regression report ordered by severity. Our case studies shows that our approach is easy to adopt and can scale well to large enterprise system high precision ACKNOWLEDGMENT We are grateful to Research In Motion \(RIM access to the enterprise application used in our case study. The findings and opinions expressed in this paper are those of the authors and do not necessarily represent or reflect those of RIM and/or its subsidiaries and affiliates. Moreover, our results do not in any way reflect the quality of RIMs products REFERENCES 1] iBATIS JPetStore, http://sourceforge.net/projects/ibatisjpetstore 2] MMB3, http://technet.microsoft.com/enus/library/cc164328%28EXCHG.65%29.aspx 3] The Dell DVD Store, http://linux.dell.com/dvdstore 4] The R Project for Statistical Computing. http://www.r-project.org 5] R. Agrawal, R.Srikant, Fast Algorithms for Mining Association Rules 


in Large Databases, Proc. of 20th Intl Conf. Very Large Data Bases 1994 6] A. Avritzer and B. Larson, Load testing software using deterministic state testing, Proc. of Intl Symp. on Software Testing and Analysis 1993 7] A. Avritzer, E. J. Weyuker, The automatic generation of load test suites and the assessment of the resulting software, IEEE Trans. Softw. Eng 21\(9 8] P. Bodik, M. Goldszmidt, A. Fox, HiLighter: Automatically Building Robust Signatures of Performance Behavior for Small- and Large-Scale Systems, Proc. of the  3rd SysML, Dec 2007 9] A. B. Bondi, Automating the Analysis of Load Test Results to Assess the Scalability and Stability of a Component, Proc. of 33rd Intl CMG Conf., San Diego, CA, USA, Dec. 2-7, 2007 10] L.  Bulej, T.  Kalibera, P. Tuma, Regression Benchmarking with Simple Middleware Benchmarks,  Proc. of the 2004 IPCCC, 2004 11] I. Cohen, M. Goldszmidt, T. Kelly, J. Symons, J. S. Chase, Correlating instrumentation data to system states: A building block for automated diagnosis and control, Proc. of 6th OSDI, Dec. 2004 12] I. Cohen, S. Zhang, M. Goldszmidt, J. Symons, T. Kelly, A. Fox Capturing, indexing, clustering, and retrieving system history Proc. of the 20th ACM Symp. on Operating Systems principles, 2005 13] Z. M. Jiang, A. E. Hassan, G. Hamann, P. Flora, Automated Performance Analysis of Load Tests, Proc. of the 25th ICSM, Sept 09 14] Z. M. Jiang, A. E. Hassan, P. Flora, G. Hamann, Automatic Identification of Load Testing Problems, Proc. of the 24th Intl Conf on Softw. Maintenance, Sept 2008 15] M. Jiang, M. A. Munawar, T.  Reidemeister, P A.S. Ward, Automatic Fault Detection and Diagnosis in Complex Software Systems by Information-Theoretic Monitoring, Proc. DSN, Jun 2009 16] M. Jiang, M. A. Munawar, T. Reidemeister, P. A. S. Ward System Monitoring with Metric-Correlation Models: Problems and Solutions Proc. of the 6th Intl Conf. on Autonomic Computing, 2009 17] T. Kalibera, L. Bulej, P. Tuma, Automated Detection of Performance Regressions: The Mono Experience, 13th MASCOTS, 2005 18]  P. Reynolds, J. L. Wiener, J.C. Mogul, M. K. Aguilera, A. Vahdat WAP5: Black-box Performance Debugging for Wide-Area Systems Proc. of the 15th Intl World Wide Web Conf.s, 2006 19] E. J. Weyuker, F. I. Vokolos, Experience with performance testing of software systems: Issues, an approach, andcase study, IEEE Trans Softw. Eng., 26\(12 20] I. H. Witten, E. Frank, Data Mining: Practical Machine Learning Tools 


and Techniques, Morgan Kaufmann, June 2005 21] M. Y. Chen , E. Kiciman , E. Fratkin , A. Fox , E. Brewer, Pinpoint Problem Determination in Large, Dynamic Internet Services, Proc. of the 2002 Intl Conf. on Dependable Systems and Networks, June  2002 22] M. K. Aguilera , J. C. Mogul , J. L. Wiener , P. Reynolds , A Muthitacharoen, Performance debugging for distributed systems of black boxes, Proc. of the 19th ACM Symp. on Operating systems principles, Oct 2003 41 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


