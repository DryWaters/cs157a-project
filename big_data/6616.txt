Vivekananda Inst 
p 
A New Perspe 
asihag70 
er outlines the fundamental a 
builds on some of the most recent findin 
h m i R  
ortunities and challe 
by the structures of existing database arc there must be an alternative way to 
alon 
Anhad Mathur Dept. Of Computer Science Vivekananda Institute of Technology Jaipur, Rajasthan anhad2605@gmail.com  Akas 
Jaipur 
roces 
Dept. Of Co 
pp 
with its o 
stems voluminous, moves too fast, or is im 
Bi 
This 
g y p b p s p a s g n 
c  Abstract 
ossi 
data is data that excee capacity of traditional database s 
addresses, but to 
Scalability, Warehouse 
lucidation, Hetero 
es nor to offer definitive ans 
the number will be going to increase exp 
e g w p e 
Data Acce 
2015 will be thrice of what we are cons 
Data, Ameliorate 
challen 
data science. It does not aim to cover the  I. INTRODUCTIO N  215The growth of data is never ending\216  4 o t u d 9 
g E g eneit y s 
Data The term \217Big Data\220 was coined in 1 
states that, we consume more bytes on t minutes than grains of rice in a year i.e 
so to handle that big amount of data technology and architecture is require 
reflection and discussion  
Keywords -- Bi 
rovide as a refer 
day and a study say the amount of da 
commercial fields like retail 
phenomenon that how the retrieval an 
into databases estimated to contain terabytes of data \205 the equivalent o 
to all this complexity is Big Data  Walmart 
velocity is very high at any instance of ti customer transactions every minute, w 
h o u m l h d e v r a m  1 which has more tha n h f n 
analysis and phenomenon is req 
reflect our modern society Basically, Big Data is not just a tec 
records, web-log files, sensor data information so that an effective automate be made on the real-time based calcul 
restricted to only research field but n can be made more effective, and we ne 
manufacturing, financial services as a 
algorithms which can manage and con 
imperfect, complex and machine-gene 
information contained in all the books i 
in 2008. Till now, the Big Data p 
ects of Bi 
 
ds the 
sp 
puter Science 
 
these data 
m n m h m i R n  
 
Sihag 
gmail.com Er. Gaurav Bagaria Dept. Of Computer Science Vivekananda Institute of Technology Jaipur, Rajasthan bagaria_gaurav@yahoo.co.in 
tute of Technology 
of Congress Similarly, Amazon.com 2 an e-co 
g 
le to be mana 
ed hitectures. Hence 
ajastha 
p b g s g 
handles millions of back-end operatio 
c g B i  
rocessing The data is too 
well as queries from more than half a 
tive to Data Processin 
Semantics, Data 
p 
s in the field of 
 
w    Big Data 
sion, Metadata 
ng ntire s 
enomenon was 
he internet in 30 
obile services 
N  t 4 o t u d 9 h o u m l 
A recent survey 
er 
nce for further 
ectrum of 
ired in many 
0 petabytes and 
70s but got pace 
a we require in 
known as Big 
ming today and a new kind of 
nentially day by 
p a g e p w e 
s 
es. The 
Data l these services 
ers to those it 
tion as the data me. The solution 
ich is imported more than 42 
d conclusion can TB, and 24.7 TB According to McKinse 
s every day, as 
2.5 times the 
nology but is a 
ated data \(data into actionable 
illion third-party sellers. The core technology that ke 
ased and as of 2005 they 
h d e v r e a n h f n m n m e b h c y h 
ert unstructured 
storage of data merce website 
the US Library 
d to create such 
database software tools to record analyze. Big Data has no exact defi 
datasets whose size are beyond t 
16 thousand is Linuxlargest Linux 3 databases, with capa 
utilizing this volume of data, and als organizations who seek to ameliorate  II. CHARACTERI 
datasets which is difficult to be 
Hence there is a requirement of manage Big Data. Big Data technol 
should be in order to be consider 
Big Data is a term used for mana 
not just only about the vastness 
database management tools or traditi applications Basically, Big Data is considered rather it is a phenomenon which repr 
Data 
includes variety and velocity of data forms the 3 V\220s of Big Data    
What is Big Data 
n e o e y s  g m e S  There are three important propert i o 
g 
architectures are designed such that data can be managed economicall 
regulating the different characteristic 
Shalini Rajawat Dept. Of Computer Science Vivekananda Institute of Technology Jaipur, Rajasthan shalinirajawat19@gmail.com 
as advanced data extracting t 
ing large amount of 
ition that how big it 
anaged by on-hand 
and efficiently by 
ities of 7.8 TB, 18.5 
e 2014 IEEE 
 
e ability of typical store, manage and 
sents a challenge in o an opportunity for their effectiveness 
i 
002 
h c y h n e o e y s g m o e S o f data but it also All these attributes  
ad the world\220s three gies can be defined 
es of Big Data. It is 
Big Data refers to nal data processing as a technology, but 
110 978-93-80544-12-0/14/$31.00 c 
d in this category new technology to 
TICS 
of datasets 
ps Amazon running 
chnology and its the values from the 


Examples include images, video a r ig e at can be made is only going to from hierarchies of records and fields social media and weblogs Unstructured data: This category c unstructured and Exabyte of Here, the generalized term for the 215big\216 which represents the size of dat perimental analysis scientists, all these allows it to retrieve usable information financial data, environmental data, a term as some small organizations are lik about the data is that it is certain that increase day by day and it\220s volume is i size of organization. Till now, there i companies to store all kind of datasets l data to handle. The only prediction t particular schema. It has a variable sc f sources which iggest continuing technical challeng the data may be in different forms a The raw data we collect from th TA The Big Data includes the foll I  I de video and such ke medical data e. what we want to day. For Example: Hubble telescope amount of raw data  processing pipelining model   it does not emerge out of a vacuum from a source. For example, we can our real world where we sense an from the source and ood of data, it\220s not it into a warehouse and semiis and querying data  ntains the most gigabytes or terabytes of data in compari organization that have several petabyte s tructu r have a fixed or e processed. In observe the things smell if present in it erates a very huge ltering 111 what we want to pull out of an MRI dependent of the s a tendency in C can be of different types i.e. structured ase having rows Accession and Reco a relational scheme i.e. a prevailing data   Structured data: In this type, the dat o a ly to have some son to big global ganizations have but soon words  and unstructured. With the rise in escalation of sensors, social media and n devices the data has become more comp it includes category of data sets includes data obta Information Pulling and Fi  he data is recorded consider the case of erabytes of data per which is one of the analytics data and today many of the or their datasets in the range of terabytes like petabytes and Exabyte are not so far  Data can be obtained from a variety word volume is iggest telescopes in the world ge data is highly application dependent extract from a picture analyzer is t form of structured datasets that does no recording phase is not in a format Therefore, we cannot leave it in this basic queries, based on the parameter needs of the organization Semi-structured data: In this type, t orm and we have to collection of health data from sensors can generate the required informatio semi-structured the technology etworking, smart lex because now It is a relative rrectly is one of the es. We can note that e data is in the is clustered into algamatio Data It's not information overload still analyze it further. For example records in a hospital, structured transcribed dictations from phy only images but in future may incl Data Integration, A traction process that around us i.e. the presence of air and heart rate of a person, to the can express it in a structured form Doing this task completely and c record Big Data as data accession and ready for analysis ed Representation  Due to this non uniformity and f A B ematic designed and t's filter failure 2014 International Conference on Computing for Sustainable Global Development INDIACom in today it includes    g source. This y responding to and operational tally different from measurements that possibly have Hence, we require an information e wing phases in its  ding a e s h n i n o  a b n b h t h n i o b r p s n multimedia files   III. PHASES OF BIG D A  a e s h n i n r  b n b h t h n i o b r s n A o r  It is mandatory to have a source t o d b x t b n e f s x n o b s u o m l o o T d x t n e f s x n o s u i o m n l ises is to assign this type, one of the general problem a ned or inherited of data such as and simulations performed by the activities produces up to millions of and columns. The data consistency a d many other roper index to data tables for its analy sufficient only to record it and send d statistic and that purely depends on the generati icians and other some uncertainty d configuration scattered data and is most complex to 


amount of heterogeneous data but due to high discussed here  accumulation incidental endogeneity spurious correlation 112 D Query Processing, Data Modeling, and Analysis  Techniques for mining and retrieval of BIG DATA are basically different from our conventional statistical analysis of small scale samples. Big data is generally rowdy, inter-related, signified and untrustworthy. In spite of that, rowdy big data could be more valuable than those small samples because some basic statistical data is obtained from recurrent system Mining 4 Big Data requires filtered, integrated trustworthy, effectively accessible data, scalable algorithms and environments which are suitable for performing Big Data Computations. On the other hand mining Big Data can also be used to improve the standards and trustworthiness of the data At the other hand, data mining can also be used to improve the standard and truthfulness of the data interpret its semantics, and provide tools for intelligent querying      dimensionality and massive sample size, it also have to A set of scientific experiments is suitable example. If we have a bundle of datasets in the warehouse, then it is impossible to find, reuse, and utilize any of such data. But if we have appropriate metadata, there can be a chance but even then, challenges will persist due to the differences in experimental results and data record structures Data analysis is lot more challenging than only being locating, understanding, recognizing and referring data For a very large scale analysis, all of these processes has to be done in an automated approach Hence, this will require a different set of syntax and semantics which are in such forms that a machine readable and machine resolvable. There is a powerful art of work in integration of datasets. Although some additional work is needed to achieve error-free automated problem solutions  face some unique statistical and computational challenges and other measurement errors. Some of the challenges are Data Elucidation and Interpretation One of the main task in the processing pipelining of Big Data is to make analysis in such a form that it can easily be understandable in terms of the user and if an effective Interpretation about the data is cannot be made by the user than it is of limited values. Ultimately decision making devices and algorithms had to interpret the result which includes various stages like examining all the possibilities and assumptions made and to retract the analysis Furthermore, there may be many possible sources of error like all models almost have some kind of assumptions, computer systems and programs can have bugs, and results can be based on wrong and erroneous data. Therefore, for all these reasons, no user will give authority to the computer system and instead he/she will try to understand and verify the results through some other processes. The computer system must take care of that and should make it easy for the user to do so. This is one of the biggest challenges with Big Data due to its complexity since there are many crucial assumptions made behind the data recorded. Even analytical pipelines often involve many steps, again by considering many possibilities and assumptions built in    IV. CHALLENGES IN ANALYSIS OF BIG DATA  Big Data is providing new opportunity to modern society and scientists as it promises to handle large The first important thing about the Big Data is its size and that\220s why it is called so. To manage large and voluminous amount of data is a challenging problem and issue for many years. In the past, this problem was eradicated by following the Moore\220s law which states that the frequency of processor will get doubled in every two years but now this is shifted toward a whole new scenario of cloud computing in which the whole system is in the form of distributed cluster. This technique of resource sharing now requires new ways of deducing how to execute and run the data processing jobs Heterogeneity, Diversity and Incompleteness  The data and information consume by humans possess a lot of heterogeneity 5 but is comfortably tolerable Actually, the nuance and richness of natural language provides a great valuable in-depth But this is not the case with machine algorithms as they expect cannot understand nuance and expect homogeneous data. Therefore, for the data to be effectively processed by these algorithms it should be carefully structured. For example, we can consider the case of patient in a hospital or consumer purchasing goods from a shop. We can create different records for different aspects of the user as in case of a patient we can create one record for laboratory test, one record for hospital stay, or one more record for lifetime for all time interaction of this customer with the hospital. Now, here we can observe that leaning the first design, all other medical procedures and tests per records would be distinct for each patient. All the three designs discussed above are less structured and conversely have successively greater variety The basic requirement of a \(traditional\alysis system is to have data with a well-defined structure Computer systems work most efficiently if they to store multiple items of same size and structure and hence this field requires further work  E A B   2014 International Conference on Computing for Sustainable Global Development INDIACom which includes storage bottleneck scalability noise Scalability 


When the larger data sets are to be processed by a system, it requires more time to analyze them as the larger data sets increases complexity which in turn increases time complexity because the flip side of size is speed. The design of a system which is likely to have a greater speed of handling larger data sets is more suitable for these technologies In many situations result of analysis is required immediately. For example, consider a case of fraudulent credit transaction which should be flagged before the actual transaction is completed to prevent the transition from taking place. Now, as we know a full analysis of a user\220s purchase history will not be feasible at real-time Instead, the system needs to develop a partial result about the user and the card so that an effective conclusion can be made to arrive at quick determination. Hence, the system should be made in such a way that it possesses flexibility in computation  Human collaboration  In spite of enormous advances made in computational analysis, there are many patterns remaining which are not detected by computer algorithms but easily detected by human beings Indeed. CAPTCHAs ruins incisively this fact to inform human web users isolated from computer programs. Ideally Big data analysis will not be all computational instead it is explicitly created to have human in the loop. The new visual analytics sub-field is seeking to do this and at least with respect to the analysis and modelling phase in the pipeline. At all stages of the analysis pipeline there is similar value to input of human In today's complicated world, it usually takes multiple known experts from various different domains to actually understand what is going on. The analysis system for big data must support shared exploration of results and input from multiple human experts. All these multiple experts can be separated in time and space when it is too costly to collect and assemble a whole team in one room. Also the data system has to support their collaboration and accept this scattered expert input  V. CONCLUSION  We are entering in an era of Big Data. With the help of large scale data that are available nowadays, there are great opportunities in making faster advances in different scientific fields to enhance and enrich many organizations. Nevertheless, there are many challenges which are already discussed in this paper must be considered before realizing these opportunities. The challenges must include not just only the salient issues but also Heterogeneity, Diversity, Incompleteness Scalability, Timeliness, and Human collaboration, at all levels of analysis flow from data accession to result explanation. Listed challenges are very common in large application domain. Hence, these are non-cost-effective to understand in reference of single domain. Also, these challenges are not able to be addressed naturally by the products of the new era of industrialization. We are in great favor and support for further research in realizing D    113 Privacy  The data privacy is another big concern and one which continuously hikes in background of Big data .In the electronic records of health ,strict laws are there deciding what can be and what cannot be done Particularly in US all regulations are less forceful for other data. However great public fear is there regarding use of personal data inappropriately and peculiarly data linking from multiple users 3 Privacy management is both a sociological and a proficient problem, which must be addressed collectively from both perspectives realizing the big data promise Consider an example, data collected from services based on location. These brand new architectures will need a user who will share his/her location with provider of the service, resulting in obvious privacy pertain Hiding only the identity without hiding the location will not accurately address these privacy pertains. A location based server or an attacker can deduct the identity of the source of query from its respective location information For example the information of user's location can be determined through many stationary connection points After sometime, user just leave "a trail of packet crumbs which may be related to a certain residence or location of office and thereby used to find the identity of user Several different types of private information like religious preference or health problems can also be unveiled by just observing unknown user's movement usage pattern and movement over time. Generally Barab\341si et al 6 demonstrated that there is a strong and close correlation between people\220s movement pattern and their identities. Note that concealing a location of user is much more challenging than concealing his/her identity This is due to services based on location, the location of the user is required for a successful access to data or collection of data, while the user's identity is not essential  One more dramatic shift that is going underway is the change in the form of traditional input-output subsystem From last few decades, HDDs \(hard disk drives\were used to store data. HDDs had some disadvantages like slower random input=output performances but now these devices are increasingly replaced by solid state drives today, or other technologies like PCM \( Phase Change Memory\d corner. All these newer technologies requires a new thinking of how to design storage subsystems for data processing as these technologies do not have the same large spread in performance between the random and sequential I/O performance in comparison with the older HDDs techniques. Implying all this changing storage subsystem potentially require to touch the every aspect of processing data including database design,  query scheduling processing algorithms recovery methods and concurrency control methods  Timeliness  C 2014 International Conference on Computing for Sustainable Global Development INDIACom E 


1  D ata, data ever yw her e  T h e Economis t 25 February 2010  2 L a yton, J u lia  Amaz o n T echnolo gy   money.howstuffworks.com  3 215 T op t e n Bi g Dat a Secur i t y and Pr i v acy  Challenges\216. Cloud Security Alliance. November 2012  4 Pr inciples of K now ledge dis cover y in databas es  By Osmar R. Zaiane  5 C h al l e nges an d Opp o r t uni t i e s w i t h Bi g Dat a  2011-1. Cyber Centre Technical Report, Purdue University  6 Un der s t anding in dividual hum an mobility  patterns. Marta C. Gonz\341lez, C\351sar A. Hidalgo, and Albert-L\341szl\363 Barab\341si. Nature 453, 779-782 \(5 June 2008  114 2014 International Conference on Computing for Sustainable Global Development INDIACom these technical challenges, in order to get all the benefits of BIG DATA  VI. REFERENCES   


                                          


                  


                             


                        


               


             


                                          


                                             


                      


                                               


   


                                


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


