2010 International Conference on Computer Application and System Modeling \(ICCASM 2010 A Model of Online Attack Detection for Computer Forensics Zhong Xiu-yu School of Computer Science liaying University Meizhou, Guangdong, China e-mail: wch@jyu.edu.cn Abstract-With frequently network attacks, network security products are practically impossible to guard against the intrusion methods. A model of online attack detection for computer forensics is proposed to collect crime evidence of attack. In this model, an algorithm of association rules mining is used to mine the association rules of attack event and build the attack signature database. After gaining network data package and pattern matching according to the protocol analysis result of primary data, the attack behavior is detected and the signature database is unceasingly updated by new attack behavior signature. The SSL encryption authentication is used in data package transmission, which can prevent the information leakage and falsifying, and the data remain original. The serious attack behaviors are detected and saved in the evidence database, which can be used as primitive evidence for computer forensics. Simulation results show that the algorithm of association rules mining improves the efficiency of network attack behavior recognition. After the new attack behavior being discovered, the safety system integrally reconstructs the attack behavior. The model can be used for the next forensic step Keywords- Network attacks; attack detection; association rule mining; pattern match; computer forensics I. INTRODUCTION Along with the popularization and development of internet, network security is confronted with more and more severe threat. Network security products, such as firewall and intrusion detection system, which often cause excessive large false alarm linked to wrong correspondence, are practically impossible to guard against the intrusion methods Kim and Lee used multi-class Support Vector Machines SVM and anomaly detection system in order to solve these problems [1]. 100 D and Hong T designed the anomaly intrusion detection method based on neural network [2 


Ming-Y ang Su proposed a real-time NIDS with incremental mining for fuzzy association rules [3]. Distinguishing new attack behavior and network forensics are advantageous to fundamentally solve the network attacks. Traditional obtaining evidence methods in which data recovery technology is used can not monitor or record attack behavior in real-time, and the evidence which is obtained is inaccurate and incomplete. With the development of network technology and computer attack technology, the traditional methods of obtaining evidence can not get the network information, and the computer forensics demand could not be met. Daniel Ayers proposed the use of super computers and grid computing resources for computer forensic analysis as the second generation forensic tool [4 II. TYPITAL NETWORK ATTACKS AND SIGNATURE A complete attack is usually composed of many interdependent steps. The preceding step success occurrence is often the latter step occurrence prerequisite. The preceding step implementation completes can cause the next attack step occurrence. There are often certain associations among attack behaviors. The usual network attack processes are as follows: first collecting the information, second the network system reconnaissance, finally carrying on the attack with the system vulnerability. According to the attack goal attacks are divided into four kinds as table 1: Denial-of Service attacks, utilization attacks, collecting information attacks, false news attacks [5 TABLEr. ATTACKS TYPE AND TYPICAL ATTACK Attacks Type Typical Attack DoS attacks SynFlood attack, UDP flood, Fraggle attack E-mail bomb, ICMP attack, etc Utilization attacks Password guessing attacks, Trojan horse Buffer overflow, etc Collecting Port scan, Finger server, LDAP server, etc information attacks False news attacks False E-mail, etc Each kmd of attack can manifest the different signature in the network data package. For example, the data package sending from some computer is frequently received in unit time t, and the data package state flag is value A, the SynFlood attack may be happen. The attack attempts to fully occupy the buffer and increases the CPU load. The ports of 


TCP package are alternated frequently in Port scanning attack. In unit time t, the length of data package is larger in ICMP attack. To the password guessing attacks, Bellovin and Merritt showed a protocol for password-based authentication and key agreement, which is capable of bootstrapping a high entropy cryptographic key from a pre-shared but possibly weak password for any two communication participants A and B to protect the password against dictionary attacks [6 c.c. Chang and Y.F. Chang proposed the CC protocol using super-poly-to-one trapdoor function which requires no certificate and can be efficiently constructed from one-way hash functions [7]. Goyal and Kumara proposed an authentication protocol which is easy to implement without 978-1-4244-7237-6/10/$26.00 201O IEEE V8-533 2010 International Conference on Computer Application and System Modeling \(ICCASM 2010 any infrastructural changes and yet prevents online dictionary attacks, their protocol used only one way hash functions and eliminated online dictionary attacks by implementing a challenge response system [8 Once the network invasion event occurred, the attack analysis involves the massive network data, including the host data, the network data package, the diary of network security product like firewall or the intrusion detection system and the other data equipment diary. The attack behavior has own signature and there are relationship among attack behaviors. Using data mining technology, it may discover the specific attack related data and connect each kind of information before the final judgment, and computer forensics carry on to the serious criminality attacks. Some experts proposed the design frame and the standard of network forensics analysis tools, which core is developing the expert system and coordinating with the intrusion detection system or the firewall. In order to enhance the detection system efficiency, Stephen proposed the cyber defense trainer for network defense and computer forensics [9]. In order to enhance the distinction efficiency to the new criminality, Thonnard and Dacier described an analysis framework which specifically developed to gain insights into honey net data [10]. According to research of the above attack and forensics, a model of online attack detection for computer forensics is proposed The system gains data package from the network directly 


carries on the analysis at the same time, and unifies network security products, such as firewall and intrusion detection system. Then the system achieves the real-time data and analyzes all possible attack behavior and attacker's attempt and adopts the measure to cut off link or lures the enemy in deep. Finally, guaranteeing the system safety and receiving the most massive evidence, the evidence is appraised preserved, submitted. The system achieves following four targets: \(1 simultaneously, including capture account number, password IP address, MAC address, time, operation sequence and so on. \(2 can prevent from revising or deleting on the correct evidence 3 different types of attack to carry on essential custom-made especially to constitute crime behavior. \(4 distinguish the new attack III. THE MODEL OF ONLINE ATTACK DETECTION FOR COMPUTER FORENSICS A. System model The model of online attack detection for computer forensics is shown in Figure 1. The system mainly consists of four parts: Data gaining, intrusion detection system, data mining, data analyzing Figure I. The model of online attack detection for computer forensics The main function of the system is recording the data package of network truly and completely, analyzing to each data package according to the protocol, and analyzing TCP UDP, IP, ARP, ICMP as well as the partial application layer protocol. The system defines filtration rules in order to filter the first floor data package, and analyzes the filtering data according to the legal science and the information security analysis technology. The system finally matches the pattern detects attack and extracts evidence. In order to establish user unusual model, the system analyzes program execution and user's behavior sequence associations with data mining technology, and analyzes the signature association of crime time, crime tool technology in the common each kind of attack. In order to discover the new attack behavior signature the system extracts association signature of different attacks and excavates the signature of different attack form and associations of the different evidence in the identical event 


B. Attack detection step The system skeleton is mainly composed of the attack computer, monitor and analyzer. The monitor has two network cards, one network card connects monitoring network, and the other network card connects the institute net which the analyzer is in. Monitor records primitive network data, gains data timely and processes preliminary to the data. Then it analyzes network data package, deposits and withdraws each kind of package section information according to the definition data model. After reading the network data and the diary which is recorded by the attack computer, the analyzer does data mining and analyses data and displays some unusual data. Meanwhile it promulgates the method of new attack and produces the new attack signature database for next data package analysis. The processes are described as following 1 computer is started. The monitoring machine on the network data collection module is started simultaneously 2 the monitor is established 3 is transferred 4 5 and the analyzer 6 7 V8-534 2010 International Conforence on Computer Application and System Modeling \(ICCASM 2010 C. Data mining module For each kind of attack, there is often association in crime time, crime tool and crime technology. An unusual model is build utilizing the association, and the model is added into the attack signature database in order to renew the attack signature database according to the real-time net data for the system. The unusual model is utilized in the data analysis of the attack detection to enhance the data analysis accuracy and validity. For example, it can mine the rules from the system log files, and analyze the relation of the rules to obtain the unusual data model, and judge the current user behavior with the unusual data model. This method 


especially fits into the logic bomb attack analysis Association rule analysis technology discovers the useful dependence or the related knowledge from the massive data The common interdependence is found at the system signature between the procedure execution and the user operation .For example, the illegal user often illegally interpolates to the specific input/output data document when he carries on the data deceit. At the stage of data analysis current user behavior is to judge legal or not through match results of user behavior and association rules which are stored in databases. From judging that the behavior has the attack signature orland is related to some attack, attack evidence which may be constitute crime is withdrawn. The evidence is transmitted to the database with encryption and feeds back to the IDS. The association rule mining technology applies in the magnanimous data analysis to speed up the data analysis and increase the efficiency of detection. It also can distinguish the new attacks D. safety design The system uses the SSL technology to safeguard the data transmission security. The transmission technology between the different equipment and the system, and the inner system, is the SSL. The SSL creates key and certificate by commands. The logical processing is carried on after bidirectional identity verification between the client and the server In the process, the news origin makes mistake if the decipher is fail and the logical processing is break off. In addition, the evidence after encryption is transmitted to the evidence database through the VPN special line IV. MONITOR DESIGN The monitor network card is set as "the promiscuous pattern" to gain all network communication data in order to monitor the network computer The monitor mainly completes the function of network data acquisition and storage. The system is developed on the Linux+Libpcap+MySql platform. With the library of Jpcap the system gathers network data and filtrates, receives the data package directly from data link layer and saves on time all network data stream into the MySql database. Thus, all data current capacity which monitored is saved in the monitor by form of primary data package and takes as 


evidence in the future. The data is transported to the analyzer and used on the line attack analysis. The concrete processes mainly are shown as following 1 founded. The monitor network equipment is established and the monitor network card is set as the promiscuous patter 2 processPacket  Jpcap. Monitor receives data package directly from data link layer and transmits the data package with the send Package function. The design of the data package which is stored is shown in table 2. The invasion signature table is established according to table 2, for example, the ARP deceit attack signature string is set as"3=arp,4 or 3=arp,5", the "3=arp means that the third field value is arp in the memory package information, namely lprotocol=arp TABLE II. FIELD OF DATA PACKAGE name pid timest len Iproto sreM 9dP .. descrip amp gth col AC type Bigint Bigint int VARe VARe ViR: .. text HAR HAR H'R size 200 100 10 10 20 2 Java Long Long Int String String SIrq; String type 3 Package and subclass IPPacket, ARPPacket. The capture data package is classified and is judged whether it is TCP package, UDP package, ICMP package, IP package or the ARP package and so on 4 after filtrating data package 5 simultaneously saved into MySql database for the next analysis step 6 analyzer are leaded in, and the SSL security connection is established 7 send to the analyzer through the SSL transmission V. ANALYZER DESIGN The analyzer backups and analyzes entity class of data package that comes from the monitor, and displays the 


abnormal data, which takes as the attack invasion evidence The analyzer describes the known attack forms establishes invasion signature, and forms the corresponding event patterns which stored in the database. For example, the signature of the common ARP deceit, SYN FLOOD, the ICMP attack and so on is established. The analyzer mainly carries on the analysis from two aspects. On one hand, in high webpage the analyzer gains network integrated data package and matches it with signature information table item of network unusual data package in the database. If match the system sends out the warning, notices manager, and stores connected information in the result database. On the other hand, it analyzes logs and compares with the new record item and diary signature information table in unusual V8-535 2010 International Conference on Computer Application and System Modeling \(ICCASM 2010 databases, and then reports to the manager and stores connected information to the result database if matched E. Produces the strong association rule The association rule has the following two important attributes: Support level peA U B the two items of collections A and B which simultaneously appear in business collection D. Confidence level P\(BIA namely probability that collection A appears in items of in business collection D, items of collection B also simultaneously display. The rules which simultaneously satisfY the smallest support threshold value and the smallest confidence level threshold value are called the strong rule Giving business collection D, the association rule mining creates the rules whose support and confidence level is bigger separately than the smallest support and confidence level which the user assigns The first step of the algorithm is to discover all frequency collection. These appearing frequency of items collections are not smaller than the pre-definition minimum support. The second step generates the strong association rule by the frequency collection [11]. The algorithm produces frequent record compendium in the system is shown as follows I  dataarray 2 inklist, I 3 i=2; i<ll;i 4  linklist[i-1 ],i 


5 inklist[i],i,dataarray 6 linklist[i],i 7 Illustration: function creatCIO obtains the first candidate collection, function getLO obtains the frequent collection function getCO obtains the candidate collection, function countO calculates the record number of the candidate collection The algorithm final outcomes the frequent record compendium, which corresponds to an instance of Note class Its attribute significance is shown as follows: \(I means the quantity of the record appearance. \(2 Vector set, which indicates the set of dimension and stores the record field. \(3 corresponding value set of field. The frequent record set produces the strong association rules, which are stored in the signature database F. pattern match to the user's behaviour The matching process of signature character string is: the match function "match\(cha:String entity class is established. Parameter "cha" which will be matched is signature string. For example, the SYS Flood attack matching string is "3,1O=A,5", 3 expresses the 3rd field, "A=lO" expresses the 10th field value is" A", 5 expresses returns values of field 5, which is a Boolean type and true value means matching. Pattern match process is shown as follows 1 2  ni=si 3 columns of Note is judge. It will go to step 4 if in, otherwise the matching process is stop and the process returns false 4 or "< ". Comparing the string of character with the value if the corresponding value of dimension "ni" from data set in Note is " = "; Making the mathematical logic comparison if the value is">" or 5 returns false and stop the matching process G. Case of Use 1 


Before the aggressor carries on the attack to the destination host, he must scan the each port of destination host to find loophole. Aggressor must attempt to connect each port of destination host to observe whether this kind of service is started or not. Therefore there are many records that a host receives multiple connections to different ports in short period of time. In the normal condition, many accessing services of port are prohibition and the connections with the port are rejected, which would have much "REJ marks in connecting records The destination host Takes as the axis attribute simultaneously also takes the quotation attribute in order to discover the frequent sequence pattern of the same destination host in two seconds window time. The patterns of implicit attacks are obtained using the frequent sequence mining 2 Attacker sends massive SYN connection requests to the destination host in short time, but each connection is half connection in direction only. That will cause the connection request buffer of the destination host occupied completely thus make the destination host unable to handle new connection requests, and the attacker achieves the purpose that the host will refuse to serve. This kind of situation will display in connecting records, in short period of time, some ports on a host can receive a number of "50" connecting state which means half connection. The server takes as the axis attribute and the destination host takes as the quotation attribute to discover the frequent sequence service pattern of the same destination host in two seconds window time Using frequent sequence mining method obtain patterns of implicit attacks, this pattern indicated that the HTTP service of the host has encountered the SynFlood attack VI. EXPERIMENTAL RESULT AND ANALYSIS A. Test environment The test has built a simplification network using the internal laboratory in campus net and the accommodations area network B. Test resultt The system has realized four functions: \(I model receives data package from the data link layer and saves all network data stream into the MySql database on time. \(2 


the manager. The analysis result and the suspicious data are V8-536 2010 International Conference on Computer Application and System Modeling \(ICCASM 2010 stored in the result database. \(3 realized two functions: One is intrusion detection system based on the log, another is attack detection based on the primitive network data package. \(4 the related data in the result database, such as IP address MAC address, port number, protocol type; and analyze the saving data after the event, specially analyze the unusual behavior in the data stream The analysis time slice can be set, as shown in Figure 2 The signature of your own and the smallest support can be also set. Partial _?!!alx?i?;j.!!!erf?ce is shown in Figure 3 Forensics_Analyser Figure 2. Set analysis time slice Forensics_Analyser oJ><.J. __ .... .. _oJ>'J T ......... _  ,  .-, ......... " .... __ C:::::':':::::.::::::: :::::: ... - . ...... M 'M ,,-    _ Figure 3. Partial analysis results VII. CONCLUSION Network attack examination, data security transmission and data mining algorithm were studied, and the online attack detection model for computer forensics is presented Network data collection, invasion detection based on pattern matching technology, database saving, analysis and inquiry of result database are combined together in this model. The analysis of attack evidence is the important step for computer forensics. By using pattern matching and protocol analysis technology, the model analyses the primary data protocol and calls the corresponding attack signature database to match the pattern according to the analysis result Association rule mining algorithm is uses to build and update the signature database. It can reduce the number of matching times greatly and improve the efficiency of attack detection The simulation result illustrates that the association rule mining algorithm can discover the new invasion behavior which make the computer forensics to be initiative. The invasion behavior is reappeared completely and the evidences have legal attributes ACKNOWLEDGEMENT 


The authors want to thank the Natural Science Foundation of Guangdong Province and the Science and Technology Plan Project of Guangdong Province for their general support for the research \(with grant NO 9151009001000043 and 0911050400004 respectively REFERENCES I] Gil-Han Kim, and Hyung-woo Lee, "False Alann Minimization Scheme based on Multi-Class SVM," UCSNS International Journal of Computer Science and Network Security, VOL,6 No,3B, 2006, pp.167-172 2] Joo D, Hong T, Han I, "The neural network models for IDS based on the asymmetric costs of false negative errors and false positive errors," Expert Systems with Applications, VoL 25, No. 1, 2003, pp 69-75 3] Ming-Yang Sua,Gwo-Jong Yub,Chun-Yuen Lin, "A real-time network intrusion detection system for large-scale attacks based on an incremental mining approach," Computers & security, VoL 28,2009 pp.301-309 4] Daniel Ayers, "A second generation computer forensic analysis system," Digital investigation, VoL 6,2009, pp,34-42 5] Ning P, Cui Y, Reeves D S, "Techniques and tools for analyzing intrusion alerts, " ACM Transactions on Information and System Security, VoL 7, 2004, pp.274 6] S,M. Bellovin, M. Merrit, "Encrypted key exchanged: password based protocols secure against dictionary attacks," Proceedings of IEEE Computer Society Symposium on Research in Security and Privacy, Oakland, California, 1992, pp,72-84 7] c.c. Chang, YF, Chang, "A novel three-party encrypted key exchange protocol," Computer Standards & Interfaces, VoL 5,2004, pp,471-476 8] Vipul Goyala, Virendra Kumara, Mayank Singha, Ajith Abrahamb Sugata Sanyalc, "A new protocol to counter online dictionary attacks," Compute r s & security, VoL 25, 2006, pp. I I4-120 9] Stephen Bruecknera,David Guasparia, Frank Adelsteina, Joseph Weeksb, "Automated computer forensics training in a virtualized environment," Digital investigation , VoL 5, 2008, pp.105-111 10] Olivier Thonnard, Marc Dacier, "A framework for attack patterns discovery in honeynet data," Digital investigation, VoL 5, 2008 pp.128-139 I I] Jiawei Han, Micheline Kamber, Data Mining: Concepts and Techniques[M].2003,pp, 152-15 V8-537 


processed, until all attributes in A have been exhausted and we get the final fuzzy version of the dataset E. At the end, all attributes would have categorical values for each record in the fuzzy dataset E. Thus, by applying the aforementioned pre-processing, given any dataset D with initial crisp attributes \(set A fuzzy records. And each of these is further iteratively converted to generate more fuzzy records, until each crisp attribute has been taken into account and we get our final fuzzy dataset E  VI. COUNTING IN FUZZY ASSOCIATION RULES Crisp ARM algorithms calculate support of itemsets in various ways Record-by-record counting; as in Apriori Counting using tidlists; for example, ARMOR Tree-style counting; as in FPGrowth In this section, we describe how counting is done in various fuzzy ARM algorithms using membership functions and how our pre-processing technique can be used to generate fuzzy datasets which can be used by any fuzzy ARM algorithm Table I. t-norms in Fuzzy sets  t-norm TM\(x, y x, y TP\(x, y TW\(x, y x + y ? 1, 0  A. Counting in Fuzzy Apriori The first pass of Apriori counts item occurrences to determine the large 1-itemsets. Any subsequent pass k consists of two phases. First, the large itemsets found in the k-1 the kth pass. Next, the database is scanned and the supports of candidate itemsets are counted. In any pass k, each record is selected in a sequential manner and the supports for the candidate itemsets, occurring in that particular record, are increased by one. Thus, the counting in Apriori is done in a record-by-record manner Fuzzy Apriori is a modified version of the original Apriori algorithm, and can deal with fuzzy records. Fuzzy 


Apriori counts the support of each itemset in a manner similar to the counting in Apriori; the only difference is that it calculates sum of the membership function corresponding to each record where the itemset exists. Thus the support for any itemset is its sum of membership functions over the whole fuzzy dataset. This calculation is done with the help of a suitable t-norm \(see Table I We generated the fuzzy dataset required for Fuzzy Apriori using our pre-processing methodology. The crisp dataset \(FAM95 sections 4 and 5, and the resultant fuzzy dataset was used as input to the Fuzzy Apriori algorithm. More details of how FPrep was used for pre-processing before Fuzzy Apriori can be found in [21] \(though the pre-processing methodology used in [21] is not explicitly names as FPrep  B. Counting in Fuzzy ARMOR Each record in the dataset is marked by a unique number called transaction id \(tid order. A tid-list of an itemset X is an ordered list of TIDs of transactions that contain X. ARMOR is based on the Oracle algorithm and is totally different from Apriori in that it calculates the support of each itemset by creating its tidlist and counting the number of tids in the tidlist. The count of any itemset is equal to the length of its corresponding tidlist The tidlist of an itemset can be obtained as the intersection of the tidlists of its mother and father \(for example, ABC is generated by intersecting AB and BC started off using the tidlists of frequent 1-itemsets In a similar manner, Fuzzy ARMOR also creates the tidlist for each itemset by intersecting the tidlists of its mother and father itemsets. And for each tid in the tidlist, it calculates the membership function  \(again using a suitable t-norm support for an itemset is thus the sum of the membership functions associated with each tid in its tidlist We have also developed an initial implementation of Fuzzy ARMOR [21]. This algorithm uses the same fuzzy dataset as input as that was used for Fuzzy Apriori. There is no change, whatsoever, made to this fuzzy dataset after it was generated initially \(for Fuzzy Apriori processing technique. Even though Fuzzy Apriori and Fuzzy 


ARMOR operate in different ways and process data differently, the fuzzy dataset created using our preprocessing technique can be used as input for both the algorithms. This is because the fuzzy dataset is generated in a standard manner of fuzzy data representation \(as described in section 5 ARM algorithm. More details of how FPrep was used for pre-processing before Fuzzy ARMOR can be found in [21 C. Counting in Fuzzy FPGrowth FPGrowth uses a compact data structure, called frequent pattern tree \(FP-tree structure and stores quantitative information about frequent patterns. Only frequent length-1 items will have nodes in the tree, and the tree nodes are arranged in such a way that more frequently occurring nodes will have better chances of sharing nodes than less frequently occurring ones. FP-treebased pattern fragment growth mining starts from a frequent length-1 pattern, examines only its conditional pattern base constructs its \(conditional recursively with such a tree. The support of any itemset can be calculated from its conditional pattern base and from the nodes in the FP-tree, which correspond to the itemset Fuzzy FPGrowth also works in a similar manner by constructing an FP-tree, with each node in the tree corresponding to a 1-itemset. In addition, each node also has a fuzzy membership function  corresponding to the 1itemset contained in the node. The membership function for each 1-itemset is retrieved from the fuzzy dataset while constructing the FP-tree, and the sum of all membership function values for the 1-itemset is its support. The support for a k-itemset \(where k ? 2 corresponding to the itemset by using a suitable t-norm  VII. RELATED WORK 3] describes the current status and future prospects of applying fuzzy logic to data mining applications. In [4] and 5], the authors discuss two facets of fuzzy association rules namely positive rules and negative rules, and describe briefly a few rule quality measures, especially for negative rules. The authors in [6] take this discussion further by describing in detail the theoretical basis for various rule quality measures using various t-norms, t-conorms, S 


implicators, and residual implicators. [8] and [9] illustrate quality measures for fuzzy association rules and also show how fuzzy partitioning can be done using various t-norms, tconorms, and implicators. The authors in [8] go a step further and do a detailed analysis of how implicators can be used in the context of fuzzy association rules Last, [7] and [10] take diametrically opposing stands on the usefulness of fuzzy association rules. The authors of [7 do a data-driven empirical study of fuzzy association rules and conclude that fuzzy association rules, after all, might not be as useful as thought to be. But the authors of [10 defended the usefulness of fuzzy association rules, by doing more experimental work, and then corroborating their stand through the successful results of their empirical research In addition to the fuzzy clustering based methodology briefly mentioned in [7], [19] and [20] describe methodologies for generating fuzzy partitions \(using nonfuzzy hard clustering original dataset into a fuzzy form. [19] uses k-Medoids CLARANS  CURE for the same. The hard clusterings so generated are then used to derive the fuzzy partitions. In such cases, where hard clustering is used, typically the middle point of each fuzzy partition is taken as reference \(membership  = 1 with respect to which the memberships for other values belonging to that partitions are calculated. [22] goes even a step further, and uses Multi-Objective Genetic Algorithms in the process for finding fuzzy partitions. Such methodologies which use hard clustering, or non-fuzzy methods are one way to obtain fuzzy versions of original datasets before any fuzzy ARM can ensue. But, with FPrep we use only fuzzy methods, fuzzy clustering to be more specific, in order to ensure consistency, and to have the notion of fuzziness maintained throughout. The main motive behind doing so is to ensure that any processing preceding the actual fuzzy ARM process, also involves fuzzy methods. Thus, the whole end-to-end process, right from the moment the processing of original crisp dataset starts till the time the final frequent itemsets are generated, involves only fuzzy methods and is holistic in nature  VIII. EXPERIMENTAL RESULTS 


The experimental results of FPrep as compared to other such non-fuzzy methods, on the basis of various parameters, are described below  A. Results from First Dataset We have tested FPrep against the automated methods for generating fuzzy partitions proposed in [19], [20]. These use hard clustering algorithms CLARANS \(k-Medoids CURE respectively. The main tangible metric to compare our approach to the ones proposed in [19], [20] is the time taken for execution. And, the dataset used for doing so is the USCensus1990raw dataset http://kdd.ics.uci.edu/databases/census1990 has around 2.5M transactions, and we have used nine attributes present in the dataset, of which five are quantitative and the rest are binary. The attributes, with their respective number of unique values, on which the evaluation was done, are as follows Age - 91 unique values Hours  100 unique values Income1  55089 unique values Income2  13707 unique values Income3  4949 unique values  Using each of the three methodologies being evaluated three fuzzy partitions were generated for each of these attributes. The results are illustrated in fig. 7, which has the y-axis in log10 form for ease of perusal.  The same are also available in Table II. As far as speed is concerned, for attributes having very low number of unique values \(~ 100 there is no big difference among the three methods. FPrep and CURE perform five times better than CLARANS for the attributes Age and Hours, both of which have around 100 unique values. But, the real differences become apparent for higher number of unique values. For attribute Income3, with 4949 unique values, we see that FPrep is nearly nine times faster than CURE, and nearly 2672 times faster than CLARANS, and for attribute Income2, with 13707 unique values, it is 27 times faster than CURE, and 13005 times faster than CLARANS. For attribute Income1, having 55089 unique values, FPrep is 46 times faster than CURE. No comparison was done with CLARANS for this attribute, as 


the time needed for execution exceeded 100000 seconds Thus, from this analysis we see that FPrep, which uses FCM clustering, clearly outperforms the CLARANS and CURE based methods on the basis of speed. The execution times for CLARANS and CURE mentioned in fig. 7 and Table II do not include the time required to create fuzzy sets, and calculate the membership value  for each numerical data point in every fuzzy set for the numerical attribute under consideration. These times also do not take into account the time required to transform crisp numerical attributes to fuzzy attributes, and derive the fuzzy dataset from the original crisp dataset The fuzzy partitions generated for each of the five numerical attributes for the USCensus1990raw dataset are shown in Table III. Coincidentally, generating three fuzzy partitions for each numerical attribute seemed a perfect fit In addition to the superior speeds achieved by FPrep, as illustrated in fig. 7 and Table II, Table III indicates the semantics and the quality of the fuzzy partitions generated by FPrep. Moreover, the number of frequent itemsets generated by a fuzzy ARM algorithm \(like fuzzy ARMOR and fuzzy Apriori minimum support threshold, is illustrated in fig. 8   Fig. 7. Algorithm, numerical attribute comparison based on speed \(log10 seconds   Fig. 8. Number of frequent itemsets for various minimum support values  B. Results from Second Dataset We have also applied FPrep on the FAM95 dataset http://www.stat.ucla.edu/data/fpp transactions. Of the 23 attributes in the dataset, we have used the first 18, of which six are quantitative and the rest are binary. For each of the six quantitative attributes, we have generated fuzzy partitions using FPrep. A thorough analysis with respect to execution times, has already been performed on the USCensus1990raw dataset \(which is manifolds bigger in size than the FAM95 dataset both on the basis of number of transactions and number of unique values for numerical 


attributes dataset has been done solely to provide further evidence of the quality and semantics of the fuzzy partitions generated by FPrep. The details of the same are in Table IV. In this case, the number of fuzzy partitions is different for different numerical attributes. Thus, the number and type of fuzzy partitions to be generated is totally dependent on the attribute under consideration. A graphical representation of the fuzzy partitions generated for the attribute Age has already been provided in fig. 5, and clearly shows the Gaussian nature of the fuzzy partitions. The nature and shapes of fuzzy partitions for the rest of the attributes are also similar. Last, the number of frequent itemsets generated for different minimum support values is illustrated in fig. 8  C. Analysis of Results With FPrep, we can analyze and zero in on the number and type of partitions required based on the semantics of the numerical attributes, which the methods detailed in [19 20] do not necessarily facilitate. Then, FPrep, backed by FCM clustering, takes care of the creating the fuzzy partitions, especially assigning membership values for each numerical data point in each fuzzy partition. In section 8.A we have already shown that FPrep is nearly 9 to 44 times faster than the CURE-based method, and 2672 to 13005 times faster than the CLARANS-based method. FPrep is not only much faster than other related methods, but also generates very high quality fuzzy partitions \(Table III and IV much user-intervention. We have created a standard way of representing any fuzzy dataset \(converted from any type of crisp dataset efficacy of the same is corroborated by the successful implementation of Fuzzy Apriori and Fuzzy ARMOR on the fuzzy dataset \(converted from crisp version of FAM95 dataset an initial implementation of Fuzzy ARMOR, are very encouraging. FPrep, when used in conjunction with these fuzzy ARM algorithms, generates a pretty good number of high-quality frequent itemsets \(fig. 8 frequent itemsets generated for a particular minimum support is same, irrespective of the fuzzy ARM algorithm 


used IX. CONCLUSIONS In this paper we have highlighted our methodology, called FPrep, for ARM in a fuzzy scenario. FPrep is meant for seamlessly and holistically transforming a crisp dataset into a fuzzy dataset such that it can drive a subsequent fuzzy ARM process. It does not rely on any non-fuzzy techniques and is thus more straightforward, fast, and consistent. It facilitates user-friendly automation of fuzzy dataset 1 0 1 2 3 4 5 Age - 91 Hours - 100 Income3 4949 Income2 13707 Income1 55089 Ti m e lo g1 0 se co nd s Numerical Attribute - Number of Unique Values FCM CURE CLARANS 0 500 1000 1500 2000 2500 


3000 0.075 0.1 0.15 0.2 0.25 0.3 0.35 0.4 N o o f F re qu en t I te m se ts Minimum Support USCensus1990 FAM95 generation through FCM, and subsequent steps in preprocessing with very less manual intervention and as simple and straightforward manner as possible. This methodology involves two distinct steps, namely creation of appropriate fuzzy partitions using fuzzy clustering and creation of fuzzy records, using these partitions, to get the fuzzy dataset from the original crisp dataset FPrep has been compared with other such techniques, and has been found to better on the basis of speed. We also illustrate its efficacy on the basis of quality of fuzzy partitions generated and the number of itemsets mined by a fuzzy ARM algorithm which is preceded by FPrep. This preprocessing technique provides us with a standard method of fuzzy data \(record that it is useful for any kind of fuzzy ARM algorithm irrespective of how the algorithm works. Furthermore, this pre-processing methodology has been adequately tested with two disparate fuzzy ARM algorithms, Fuzzy Apriori and Fuzzy ARMOR, and would also work fine with other fuzzy ARM algorithm REFERENCES 1] Zadeh, L. A.: Fuzzy sets. Inf. Control, 8, 338358 \(1965 2] Chen G., Yan P., Kerre E.E.: Computationally Efficient Mining for Fuzzy Implication-Based Association Rules in Quantitative Databases. International Journal of General Systems, 33, 163-182 


2004 3] Hllermeier, E.: Fuzzy methods in machine learning and data mining Status and prospects. Fuzzy Sets and Systems. 156, 387-406 \(2005 4] De Cock, M., Cornelis, C., Kerre, E.E.: Fuzzy Association Rules: A Two-Sided Approach. In: FIP, pp 385-390 \(2003 5] Yan, P., Chen, G., Cornelis, C., De Cock, M., Kerre, E.E.: Mining Positive and Negative Fuzzy Association Rules. In: KES, pp. 270-276 Springer \(2004 6] De Cock, M., Cornelis, C., Kerre, E.E.: Elicitation of fuzzy association rules from positive and negative examples. Fuzzy Sets and Systems, 149, 7385 \(2005 7] Verlinde, H., De Cock, M., Boute, R.: Fuzzy Versus Quantitative Association Rules: A Fair Data-Driven Comparison. IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics 36, 679-683 \(2006 8] Dubois, D., Hllermeier, E., Prade, H.: A systematic approach to the assessment of fuzzy association rules. Data Min. Knowl. Discov., 13 167-192 \(2006 9] Dubois, D., Hllermeier, E., Prade, H.: A Note on Quality Measures for Fuzzy Association Rules. In: IFSA, pp. 346-353. Springer-Verlag 2003 10] Hllermeier, E., Yi, Y.: In Defense of Fuzzy Association Analysis IEEE Transactions on Systems, Man, and Cybernetics - Part B Cybernetics, 37, 1039-1043 \(2007 11] Agrawal, R., Imielinski, T., Swami, A.N.: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Record, 22, 207216 \(1993 12]  Agrawal, R., Srikant, R.: Fast Algorithms for Mining Association Rules. In: VLDB, pp. 487-99. Morgan Kaufmann \(1994 13] Han, J., Pei, J., Yin, Y.: Mining Frequent Patterns without Candidate Generation. In: SIGMOD Conference, pp. 1-12. ACM Press \(2000 14] Han, J., Pei, J., Yin, Y., Mao, R.: Mining Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree Approach. Data Mining and Knowledge Discovery, 8, 5387 \(2004 15] Pudi V., Haritsa J.R.: ARMOR: Association Rule Mining based on Oracle. CEUR Workshop Proceedings, 90 \(2003 16] Dunn, J. C.: A Fuzzy Relative of the ISODATA Process and its Use in Detecting Compact, Well Separated Clusters. J. Cyber., 3, 32-57 1974 17] Hoppner, F., Klawonn, F., Kruse, R, Runkler, T.: Fuzzy Cluster Analysis, Methods for Classification, Data Analysis and Image Recognition. Wiley, New York \(1999 


18] Bezdek J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms. Kluwer Academic Publishers, Norwell, MA \(1981 19] Fu, A.W., Wong, M.H., Sze, S.C., Wong, W.C., Wong, W.L., Yu W.K. Finding Fuzzy Sets for the Mining of Fuzzy Association Rules for Numerical Attributes. In: IDEAL, pp. 263-268. Springer \(1998 20] Kaya, M., Alhajj, R., Polat, F., Arslan, A: Efficient Automated Mining of Fuzzy Association Rules. In: DEXA, pp. 133-142. Springer \(2002 21] Mangalampalli, A., Pudi, V. Fuzzy Association Rule Mining Algorithm for Fast and Efficient Performance on Very Large Datasets In FUZZ-IEEE, pp. 1163-1168. IEEE \(2009 22] Kaya, M., Alhajj. Integrating Multi-Objective Genetic Algorithms into Clustering for Fuzzy Association Rules Mining. In ICDM, pp. 431434. IEEE \(2004  Table II. Algorithm, numerical attribute comparison based on speed \(seconds  Algorithm Age - 91 Hours - 100 Income3 - 4949 Income2 - 13707 Income1 - 55089 FCM 0.27 0.3 3.13 6.28 79.4 CURE 0.25 0.25 28.67 163.19 3614.13 CLARANS 1.3 1.34 8363.53 78030.3 Table III. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions Age Old Middle Aged Young Hours More Average Less Income1 High Medium Low Income2 High Medium Low Income3 High Medium Low  Table IV. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions AGE Very old Around 25 Around 50 Around 65 Around 35 HOURS Very High Zero Around 40 Around 25 INCHEAD Very less Around 30K Around 50K Around 100K INCFAM Around 60K Around 152K Around 96K Around 31K Around 8K TAXINC Around 50K Around 95K Around 20K Very less FTAX Around 15K Very less Around 6K Very high Around 33K  


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


