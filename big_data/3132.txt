A Prediction Method of Fuzzy Association Rules Jianjiang Lu 132s3 Baowen Xu Jixiang Jiang Department of Computer Science and Engineering Southeast University Nanjing 21 0096 China PLA Universiv of Science and Technology, Nanjing 21 0007 China 3Jiangsu Institute of Soflare Quality Nanjing 21 0096 China E-mail jjlu@seu,edu.cn I Abstract Quantitative attributes are partitioned into several fuzzy sets by fuzzy c-means algorithni and search technology of 
Apriori algorithm is improved to discover interesting fuzzy association rules The first prediction method of fuzzy association rules is presented and shortcoming of this prediction method is analyzed Then the second prediction method of fuzzy association rules with the variable threshold is presented In the second prediction method a little error between prediction value and actual value is allowed When the error is less than a given threshold prediction value is regarded as acceptable or rational The secondprediction method can obtain the different prediction precision corresponding to the different error threshold chosen by the users 
so it is more flexible and effective than the first prediction method Keywords data mining; fuzzy association rules; genetic algorithm; prediction; fuzzy clustering 1 Introduction The mining of association rules is one of the most important issues in the field of data mining The problem of mining Boolean association rules is introduced in I,Z Parallel mining and weighted mining of Boolean association rules have also been studied in 3,4 Srikant and Agrawal first introduce the problem of mining quantitative association rules[s1 The algorithm in 5 finds association rules by partitioning the attribute 
domain combining adjacent partitions and then transforming the problem into binary one Although this mining algorithm for quantitative association rule can solve problems introduced by quantitative attributes it introduces some other problems The first problem is that equi-depth partitioning cannot embody the actual distribution of the data On the one hand it may not work very well on highly skewed data and tends to split adjacent values with high support into separate intervals though their behavior would typically he similar On the other hand it is not easy to distinguish the degree of membership For instance age of 50 and age of 70 will both be classified into the old. However, we intuitively how that age 
of 70 is much older than age of 50 The second problem is caused by the sharp boundary between intervals The algorithm either ignores or over-emphasizes the elements near the boundaly of the intervals in the mining process M.K Chan uses fuzzy set to soften the partition boundary of domains and presents the concept of fuzzy association mled6 but it does not present partitioning algorithm which can embody the actual distribution of the data and does not present the mining algorithm for fuzzy association rules which fits for large databases Ref 7 use linguistic clouds to soften partition boundary of the domains and presents 
mining algorithm for association rules with linguistic cloud models Ref 8 uses the relational fuzzy c-means algorithm to partition the quantitative attributes into several fuzzy sets then the problem of mining fuzzy association rules is introduced by combining fuzzy sets The relational fuzzy c-means algorithm can embody the actual distribution of the data Furthermore fuzzy sets can soften partition boundary But combining fuzzy sets can obtain excessive fuzzy association rules so the mining algorithm in 8 cannot fit for large database Recently applying association rules to solve classification problem have been studied and discussed in some papers Lent Swami proposes association rule clustering system ARCS This 
system mines association rules based on clustering and then uses these rules to classify but this system is only fit for the case which there are only two attributes in a rule Liu, Hsu and Ma propose classification based on associations \(CBA which uses iterative method to find the frequent and accurate possible rule set and then uses method of elicitation to build classification system But it is important to note that the intervals involved in quantitative association rules may not be concise and  This work was supporled in pan by the National Natural Science Foundation of China NSFC 
60073012 National Grand Fundamenlal Research 973 Program of China 2002CB3l2000 National Research Foundation for the Doctoral Program of Higher Education of China Natvral Science Foundation of Jiangsu Province China BK2001004 Opening Foundation of State Key Laboratory of Soflware Engineering in Wuhan University and Opening Foundation of Jiangsu Key Laboratory of Computer Information Processing Technology in Soochow University 0-7803-8242-0/03 17.00 0 2003 IEEE 98 


meaningful A method for constructing fuzzy decision trees FID and a number of inference procedures based on conflict resolution in tule-based systems and efficient approximate reasoning methods have been presented in I I Given a database, this approach can be used to build a fuzzy decision tree and the resulting tree can be used for inference in Ill A approach that can be used for mining interesting rules for classification with degree of membership in 12,13 These approaches represent the revealed regularities and exceptions using fuzzy sets. The use of fuzzy sets allows human users to better understand the discovered rules because of the affinity with the human knowledge representation Furthermore these approaches are capable of finding interesting relationships among attributes without any subjective input required of the users Some results show that the classification methods in 12,13 have good precision and interpretability than C4.5 and CBA In this paper, quantitative attributes are partitioned into several fuzzy sets by fuzzy c-means algorithm, and search technology of Apriori algorithm is improved to discover interesting fuzzy association rules Then two prediction methods of fuzzy association rules are presented In these prediction methods the parameters of triangular fuzzy numbers are adjusted to improve the prediction precision by genetic algorithm. Last we compare the precision of two prediction methods The results of experiment show that the second prediction method can obtain the different prediction precision corresponding to the different enor threshold chosen by the users so it is more flexible and effective than the first prediction method The rest of this paper is organized as follows In section 2 Apriori algorithm is improved for mining fuzzy association rules In section 3 the first prediction method of fizzy association tules is presented In section 4 the second prediction method of fuzzy association rules with the variable threshold is presented In section 5 we compare the precision of two prediction methods and give an experiment The conclusions are briefed in section 6 2 Mining fuzzy association rules Let T={r,,r  1 be a relational database fj represents the j-th record in T let I  i,,i2, ...,im be the attribute set where ii denotes a Boolean categorical or quantitative attribute rj[i represents value of the j-th record in atkibute i Valued of he redid in attribute need to be partitioned into several fuzzy sets for mining fuzzy association tules Let A and A be two values of the record in Boolean attribute then two values can be partitioned into two fuzzy sets A and A  where  Categorical attribute with fewer values can be partitioned into several fuzzy sets with the same method Each quantitative attribute is partitioned into several fuzzy sets represented with triangular fuzzy numbers by fuzzy c means \(FCM Comparing with 8 there are two differences. One is that FCM algorithm can cost less time than the relational fuzzy c-means algorithm, another is that fuzzy sets are not expressed with normal fuzzy numbers as in SI but expressed with triangular fuzzy numbers in this paper For example the process of using FCM algorithm to partition quantitative amibute i4 into c fuzzy sets is given below Put the values taken by attribute ik together as a set of samples X Let d  x vi m=2 matrix norm 1 U is the maximum element of the matrix After clustering with FCM algorithm, partition matrix U and c centers U are obtained The elements of the row corresponding to the maximal center are interpreted as the membership degree of the samples in the maximal fuzzy set grade The method of representing fuzzy sets in triangular fuzzy numbers is as follows Let pi\(xj be membership degree of x in the fuzzy set with center vk  let A  x  pi x z pi xi  vj E I,z   c  We first find the samples with minimum membership degree at both sides of the center vk in Xi u\(v let the left sample with minimum membership degree be x  its membership degree be pk\(x and let the right sample with minimum membership degree be X  its membership degree be pk\(x then the expression of triangular fuzzy numbersf\(x with center vk is In order to mine fuzzy association tules we construct a new database through original database T In this new database, attributes are fuzzy sets; values of the record in attributes are obtained as follows Let i be a fuzzy set of 99 


attribute ik i is an attribute in new database Value of the j-tb record in attribute i is il\(fi[ii  il\(ti[ik is membership degree of tj[ii with respect to fuzzy set i  In this new database, because attributes are fuzzy sets we call attributes fuzzy attributes next Let I still be the fuzzy attribute set, let ti\(yk represent value of the j-th record in fuzzy attribute y ti\(yk falls in O,l Let X=lY,,Y,,...,Yp Y=\(yl+,,Y,*,,...,Yptl XnY=0 An association rule is an implication of the form X j Y  Because attributes in Xand Yare fuzzy attributes X Y is called fuzzy association rule The support and confidence of fuzzy association rule are defined as follows Definition 1 The support ofXis defined as follows fRlj\(Y sUp\(x  i=l m=l n Fuzzy attribute sets with at least a minimum support are called frequent fuzzy attribute sets Definition 2 The support of X  Y is defined as follows Definition 3 The confidence of X Y is defined as follows Because t,\(yk falls in O,l we can know that all subsets of a frequent fuzzy attribute set must also be frequent according to definition 1 With the above finding it is easy to modify Apriori algorithmiz1 to mine fuzzy association rules 3 Prediction method of fuzzy association rules Suppose we have obtained I rules as follows by the improved algorithm for mining fuzzy association rules If i isf,',and i,isf and i,,-,isfi then im isf j 1,2;..,1 where f are triangular fuzzy numbers To these fuzzy association rules we can use inference method of standard additive model to make prediction When input values of antecedents i1,i2;..,im are x,.x2,.".x,_ respectively based on these fuzzy association rules we can obtain the prediction result of consequent i as follows I XfL x  IJLW vi  LfL\(x c  Because triangular fuzzy numbers only approximately represent the fuzzy partition we usually cannot obtain perfect prediction precision So genetic algorithm is applied to improve the prediction precision The optimizing process of genetic algorithm is to minimize the value of objective function E C:=,lyaj y by adjusting triangular fuzzy numbers in antecedent and consequent of fuzzy association rules where y is the actual output value y is the prediction result of consequent i  n is the total number of samples The whole process of the algorithm is described as follows I Load the initial data and set the parameters required for genetic algorithm Load training dataset initial triangular fuzzy numbers of each attribute and fuzzy association rules obtained. Set the size of population L maximum number of generations T and set crossover probability P individual mutation probability Pm parameter of crossover U parameter of mutation b Use t count current generation I n 2 Generate initial generation Po Each individual forming the genetic population will consists of triangular fuzzy numbers of all attributes that have been encoded by real values Each triangular fuzzy number is represented by three real parameters The triangular fuzzy numbers of all attributes are joined into an individual C with the length H  C;,3.rj  which is shown as follows C uj,,b c,,,....aj,,~b~,,,c c  c,,c,,'''c,m where C codes the fuzzy partition of the i-th attribute Ii denotes the number of fuzzy sets that the i-th attribute is partitioned and m denotes the number of attributes The encoded initial fuzzy partition is taken as the first individual of initial generation denoted as Cp  and other initial individuals are generated randomly according to the variable range of each gene. Each gene c,,,h=1,2,...,H is the parameters of fuzzy numbers and their variable 100 


ranges cL,c are identified by Cp The variable ranges of three parameters C,,C,+,,C of a fuzzy number are the following Since the size of population is L initial generation can be denotedas Po Cp,C  C 3 Evaluate individuals To each individual of current generation population P  we use function E to evaluate its degree of adaptation Smaller the value of error function is higher the individual adaptation is 4 Generate a new generation P through genetic A\Selection Applying the selection schemes of stochastic universal sampling and keeping the elitist we generate parent individuals from current generation for other genetic operators operators B Crossover The Max-Min-Arithmetic crossover is applied to the selected individuals according to the crossover probability P If two individuals to he crossed are C  c  c  cH and C  c  c  c;i  then four offsprings are generated as follows C a.\(C I-a C a C  I a C C with C min{c,,c C with C max\(c,,c Select the two best of them as the results of the crossover operator, where a is a constant that is set beforehand C\Mutation The nonuniform mutation operator"6 is applied to the selected individuals according to the mutation probability P Suppose that mutation is happened on the gene ck E[c:,c of an individual C  c _ c  c  and generate new individual C:=\(c  c  c  H The process of mutation operator is as follows ck+A\(l,cL-ck 5=0 i ck A\(f,ct CL p  I c  where p is a random integer of O,l function A\(r,y retums a value of Oy such that the function value tends to zero with the augment of I Here we choose A\(r,y  y\(l r where r is a random value of O,l Tis the maximum number of generation t denotes current generation and parameter b determines the degree of dependence to the iterative number 5 Check the condition of termination The iteration terminated until to the maximum number of generation T otherwise retum to the third step and begin a new iteration 4 Prediction method of fuzzy association rules with the variable threshold In section 3 the optimizing process of genetic algorithm is to minimize the value of objective function E Iyoj y,l and obtains one and only parameters of triangular fuzzy numbers and prediction precision last In practice it is usually allowed that there is a little error between prediction value and actual value When the error is less than a given threshold, prediction value is regarded as acceptable or rational For example we use some attributes to predict the economy-growing rate of 2002 year Suppose the prediction value of the economy growing rate is about 7.3  the actual economy-growing rate of 2002 year is 7.8  Although there is a little emor 0.5 between prediction value and actual value the prediction value is regarded as acceptable or rational Let input antecedents be i,,i2,...,i m  output consequents be i  The users choose an allowed error threshold Ae according to the actual need When the error between prediction value yoj and actual value y is less than Ae prediction value is regarded as acceptable or rational Let m be the total number of rational samples n be the total number of samples then the prediction precision is defined as  The optimizing process of genetic algorithm is to maximize the prediction precision by adjusting triangular fuzzy numbers in antecedent and consequent of fuzzy association rules The adjusting process is similar to section 3 In the prediction method of fuzzy association rules with variable threshold the users can choose a different error threshold Ae according to actual need and can obtain different parameters of triangular fuzzy numbers and prediction precision at the same time So this prediction method is more flexible and effective than the prediction method given in section 3 5 Experimental analysis I n Experimental database Abalone is taken from UCI Machine Learning Repository which has nine attributes 101 


and 4177 records. Each attribute is quantitative except for the attribute sex Attribute sex is denoted as io  the quantitative attributes including length, diameter, height whole weight shucked weight viscera weight shell weight and rings are respectively denoted as i,;..,i Nine attributes are partitioned into three fuzzy sets large middle and small these fuzzy sets are denoted as im\(l 2 3 0,1  8 Table 1 shows some original parameters of triangular fuzzy numbers Table 1 Some original parameters attributes large middle length 0.50,0.63,1.2 0.34,0.50,0.63 diameter 0.39,0.5,0.97 0.25,0.39,0.5 height 0.13,O 18,1.67 0.09,O. l4,O. 18 whole weight 0.96,1.56,4.622 0.36,0.93,1.52 shucked weight 0.42,0.69,2.38 0.16,0.41,0.67 viscera weight 0.21,0.34,1.22 0.08,0.22,0.34 shell weight 0.28,0.45,1.59 0.1 1,0.27,0.44 rings 9.8,16.29,44.67 6.84,10.52,16.33 We use the former eight attributes i,,...,i to predict the attribute i rings which denotes the age of abalone In order to make prediction it is required to mine fuzzy association rules with such as If io is io  and i is i,\(I then i is is 1,2,3 m=0,1,..,,8 We randomly select 2784 records in database as the training samples the rest as the testing samples We first apply 300 fuzzy association rules to make prediction directly and get the mean linear error 3.634351 on the training samples and mean linear error 3.771415 on the testing samples Let T=lOOO L=61 P,=0.6 P,=O.I a=0.35 b=5 using the prediction method in section.3 we get mean linear error 1.659679 on the training samples and mean linear error 1.770291 on the testing samples Table 2 shows some new parameters of triangular fuzzy numbers Table 2 Some new parameters attributes large middle length 0.54,0.8,0.99 0.34,0.55,0.68 diameter 0.4,O.S 1,1.01 0.23,0.33,0.51 height 0.1 1,0.17,1.49 0.07,0.13,0.19 whole weight 1.02,2.65,4.15 0.2,0.71,1.62 shucked weight 0.39,0.59,2.08 0.23,0.40,0.64 viscera weight 0.19,0.54,1.03 0.02,0.16,0.3 shell weight 0.27,0.37,1.322 0.09,0.26,0.5 1 rings 10.08,17.33,38.7 8.39,13.1,18.9 The experimental results show that it is effective to improve prediction precision by adjusting triangular fuzzy numbers of fuzzy partition with genetic algorithm, which makes it possible to make an effective prediction of the mined fuzzy association rules Table 3 shows the prediction precisions at the different number of fuzzy association rules Although the prediction precision is not perfect and should be improved farther we claim that the prediction precision is acceptable in this high-dimensional system with nine attributes Table 3 The prediction precisions Number of fuzzy Training mean Testing mean association rules linear error linear error 300 1.659679 1.770291 100 1.752211 1.840357 60 1.814739 1.915229 40 1.952426 2.048995 We mine 300 fuzzy association rules and let T=lOOO L=61 Pc=0.6 P,=O.I a=0.35 b=5 Ae=2  using the prediction method in section 4 we get prediction precision 72.5575 on the training samples and testing precision 71.0696 on the testing samples. At the same time we use the results obtained from the prediction method in section 3 we get prediction precision 72.1624 on the training samples and testing precision 70.5671 on the testing samples This shows that if we allow a little error between prediction value and actual value the prediction method in section 4 is more flexible and effective than the prediction method in section 3 6 Conclusions Quantitative attributes are partitioned into several fuzzy sets by fuzzy e-means algorithm and search technology of Apriori algorithm is improved to discover interesting fuzzy association rules We present the first prediction method of fuzzy association rules and analyze the shortcoming of this prediction method Last we present the second prediction method of fuzzy association rules with the variable threshold and compare the two prediction methods In the second prediction method a little error between prediction value and actual value is allowed When the error is less than a given threshold prediction value is regarded as acceptable or rational The results of experiment show that the second prediction method can obtain the different prediction precision corresponding to the different error threshold chosen by the users so it is more flexible and effective than the first prediction method 102 


References I R Agrawal T Imieliski and A Swami 223Mining association rules between sets of items in large databases\224 Proc of ACM SIGMOD Conference on Management of Data Washington DC 1993 pp.207-2 16 z R Agrawal R Srikant 223Fast algorithms for mining association rules\224 Proc of the 1994 International Conference on Very Large Databases Santiago Chile 1994, pp.487-499 131 R Agrawal J C Shafer 223Parallel mining of association rules design implementation and experience\224 Special Issue on Data Mining IEEE Transactions on Knowledge and Data Engineering 1996,8\(6\pp.962-969 141 Lu Jianjiang 223Research on algorithms of mining association rules with weighted items\224 Journal of Computer Research and Development 2002,39\(10 pp.1281-1286 in Chinese 151 R Srikant R Agrawal 223Mining quantitative association rules in large relational tables\224 Proc of the ACM SIGMOD Conference on Management of Data Montreal, Canada 1996 pp.1-12 61 M.K Chan F Ada, and H.W. Man 223Mining fuzzy association rules in database\224 SIGMOD Record 1998,27\(1\41-46 171 Lu Jianjiang Qian Zuoping Song Ziling 223Application of normal cloud association rules on prediction\224 Journal of Computer Research and Development 2000 37\(1 I 1317-1320 \(in Chinese SI Lu Jianjiang Song Zilin Qian Zouping 223Mining linguistic valued association rules\224 Journal of Soflare 2001,12\(4 pp.607-611 in Chinese 191 B Lent A Swami J Widom 223Clustering association rule\224 Proceedings of the International Conference on Data Engineering Birmingham England 1997 pp.220-23 1 IO B Liu W Hsu Y Ma 223Integrating classification and association rule mining\224 Proceedings of the International Conference on Knowledge Discovery and Data Mining New York 1998, pp.80-86 I I C.Z Janikow 223Fuzzy decision trees Issues and methods\224 IEEE Trans on Systems Man and Cybernetics  Part B Cybernetics 1998,28\(1\pp.1 14 1121 W.H Au K.C.C Chan 223Classification with degree of membership A fuzzy approach\224 Proc of the 1st IEEE Int\222l Con on Data Mining San Jose CA 2001 pp.35-42 1131 Zou Xiaofeng Lu Jianjiang Song Zilin 223Classification system based on fuzzy class association rules\224 Journal of Computer Research and Development 2003,40\(5 65 1-656 \(in Chinese 141 R.J Hathaway J.W Davenport and J.C Bezdek 223Relational dual of the c-means algorithms\224 Pattern Recognition 1989,22\(2 pp.205-212 IS Huang Chongfu Tanslated 223Fuzzy engineering\224 Xian Xi\222an JiaoTong Uniuersiq Press 1999 in Chinese I161 Z Michalewicz, \223Genetic algorithms  data structure  evolution programs\224 Springer- Verlag, Newyork 1994 103 


not share an y itemsets with the b oundary of an y itemset X k 2X whic hisac hild of X  In other w ords for eac h c hild X k 2X of X w e remo v e from F  X c  all mem ber itemsets in F  X k c   Then these pruned b oundaries ma ybe used in order to generate the rules The resulting algorithm is illustrated in Figure 6 This algorithm uses as input the itemsets X whic h are generated in the 014rst phase of the algorithm at the appropriate lev el of minsupp ort  The algorithm FindBoundary of Figure 5 ma y b e used as a subroutine in order to generate all the b oundary itemsets These b oundary itemsets are then pruned and the rules are generated b y using eac h of the itemsets corresp onding to the b oundary in the an teceden t 4.1 Rules with constrain ts in the an teceden t and consequen t It is easy enough to adapt the ab o v e rule generation metho d so that particular items o ccur in the an teceden t and/or consequen t Consider for example the case when w e are generating rules from a large itemset X  Supp ose that w e desire the an teceden t to con tain the set of items P and the consequen t to con tain the set of items Q W e assume that P  Q 022 X  W e shall refer to P as the ante c e dent inclusion set  and Q as the c onse quen t inclusion set  In this case w e need to rede\014ne the notion of maximalit y and b oundary itemsets A v ertex v  Y  is de\014ned to b e a maximal ancestor of v  X  at con\014dence lev el c an teceden t inclusion set P  and consequen t inclusion set Q if and only if P 022 Y  Q 022 X 000 Y  S  Y  S  X  024 1 c  and no strict ancestor of Y satis\014es all of these constrain ts Equiv alen tl y  the b oundary set con tains all the itemsets corresp onding to maximal ancestors of X  It is easy to mo dify the algorithm discussed in Figure 5 so that it tak es the an teceden t and consequen t constrain ts in to accoun t The only di\013erence is that w e add an un visited v ertex v  T  to LIST if and only if S  T  024 S  X  c  and T 023 P  Also a v ertex v  R  is added to BoundaryList  only if it satis\014es the mo di\014ed de\014nition of maximalit y  5 Generation of the adjacency lattice In this section w e discuss the construction of the adjacency lattice The pro cess of constructing the adjacency lattice requires us to 014rst 014nd the primary itemsets There are t w o main constrain ts in v olv ed in c ho osing the n um ber of itemsets to prestore 1 Memory Limits In order to a v oid I/O one ma y wish to store the primary itemsets and corresp onding adjacency lattice in main memory  1 Recall that Theorem 2.1 c haracterizes the size required b y the adjacency lattice for this purp ose Assume that w e desire to 014nd N itemsets Note that b ecause of ties in the supp ort v alues of the primary itemsets supp ort v alues ma y not exist for whic h there are exactly N itemsets Th us w e assume that for some slac kv alue N s w e wish to 1 Storing the adjacency lattice on disk is not suc h a bad option after all The total I/O is still prop ortional to the size of the output rather than the n um b er of itemsets prestored Recall that the graph searc h algorithms used in order to 014nd the large itemsets and asso ciation rules visit only a small fraction of the v ertices in the adjacency lattice F unction NaiveFindThr eshold\(Numb er ofIt emset s N Slack N s  b egin High  max i f Supp ort of item i g Low 0 Gener ated 0 while  Gener ated 62  N 000 N s N  b egin Mid  High  Low   2 Gener ated  DH P  Mid  end return Mid  end Algorithm ConstructL attic e\(Numb er ofItem sets N Slack N s  b egin p  NaiveFindThr eshold\(N N s  F or eac h itemset X  f i 1 i r g with S  X  025 p do Add the v ertex v  X  to the adjacency lattice with lab el S  X  Add the edge E  X 000f i k g X  for eac h k 2f 1 r g end Figure 7 Constructing the adjacency lattice 014nd a primary threshold v alue for whic h the n um ber of itemsets is b et w een N 000 N s and N  2 Prepro cessing Time There ma y b e some practical limits as to ho wm uc h time one is willing to sp end in prepro cessing Consequen tly ev en if it is not p ossible to 014nd N itemsets within the prepro cessing time it ough t to b e able to terminate the algorithm with some v alue of the primary threshold for whic h all itemsets with supp ort ab o v e that v alue ha v e b een found A simple w a y of 014nding the primary itemsets is b y using a binary searc h algorithm on the v alue of the primary threshold using the DHP metho d discussed in Chen et al as a subroutine This metho d is somewhat naiv e and simplistic and is not necessarily e\016cien t since it requires m ultiple executions of the DHP metho d This metho d of 014nding the primary threshold is discussed in the algorithm NaiveFindThr eshold of Figure 7 The time complexit y of the pro cedure can b e impro v ed considerably b y utilizing a few simple ideas 1 It is not necessary to execute the DHP subroutine to completion in eac h and ev ery iteration F or estimates whic h are lo w er b ounds on the correct v alue\(s of the primary threshold it is su\016cien t to terminate the procedure as so on as N or more large itemsets ha v e b een generated at the lev el of supp ort b eing considered 2 It is not necessary to start the DHP pro cedure from scratc h in eac h iteration of the binary searc h pro cedure It is p ossible to reuse information b et w een iterations Let I  s  denote the itemsets whic hha v e supp ort at least s  It is p ossible to sp eed up the preprocessing algorithm b y reusing the information a v ailable in I  Low  Generating k itemsets in I  Mid  is only a matter of pic king those k itemsets in I  Low  whic h ha v e supp ort at least Low  This do es not mean that ev ery itemset in I  Mid  can b e immediately generated using this metho d Recall from 1 ab o v e that the DHP algorithm is often terminated b efore completion if more than N itemsets ha v e b een generated in that iteration Consequen tly  not all itemsets in I  Low  ma ybea v ailable but only those k itemsets for whic h k 024 k 0  for some k 0 are a v ailable Th us w eha v e all 


 0 1 2 3 4 5 6 7 8 9 x 10 4 0 0.002 0.004 0.006 0.008 0.01 0.012 0.014 0.016    Primary threshold Number of itemsets prestored T10.I4.D100K  T10.I6.D100K  T20.I6.D100K  Figure 8 Threshold v aration with itemsets prestored DataSet Conf Sup DHP Online T10.I4.D100K 90 0  3 100 sec instan taneous T10.I6.D100K 90 0  3 130 sec instan taneous T10.I6.D100K 90 0  2 240 sec 2 seconds T20.I6.D100K 90 0  5 100 sec instan taneous T able 3 Sample illustration s of the order of magnitude adv an tage of online pro cessing those k itemsets in I  Mid  v ailable for whic h k 024 k 0  These itemsets need not b e generated again 6 Empirical Results W e ran the sim ulation on an IBM RS/6000 530H w orkstation with a CPU clo c k rate of 33MHz 64 MB of main memory and running AIX 4.1.4 W e tested the algorithm empirically for the follo wing ob jectiv es 1 Prepro cessing sensitivit y The prepro cessing tec hnique is sensitiv e to the a v ailable storage space The larger the a v ailable space the lo w er the v alue of the primary threshold W e tested ho w the primary threshold v alue v aried with the storage space a v ailabili t y W e also tested ho w the running time of the prepro cessing algorithm scaled with the storage space 2 Online pro cessing time W e tested ho w the online pro cessing times scaled with the size of the output W e also made an order of magnitude comparison b et w een using an online approac h and a more direct approac h 3 Lev el of redundancy W e tested ho w the lev el of redundancy in the generated output set v aried with user sp eci\014ed lev els of supp ort and con\014dence W e sho w ed that the lev el of redundancy in the rules is quite high Th us redundancy elimination is an imp ortan t issue for an online user lo oking for compactness in represen tation of the rules 6.1 Generating the syn thetic data sets The syn thetic data sets w ere generated using a metho d similar to that discussed in Agra w al et al Generating the data sets w as a t w o stage pro cess 0 1 2 3 4 5 6 7 8 9 x 10 4 0 2 4 6 8 10 12 14 16 18 x 10 4    Number of itemsets prestored Relative Computational Effort for preprocessing T10.I4.D100K  T10.I6.D100K  T20.I6.D100K  Figure 9 Computation v ariation with itemsets prestored 0 5000 10000 15000 0 10 20 30 40 50 60    Number of rules generated Response Time in seconds T10.I4.D100K  T10.I6.D100K  T20.I6.D100K  Figure 10 Online resp onse time v ariation with rules generated 20 30 40 50 60 70 80 90 100 0 2 4 6 8 10 12   Support fixed at 0.15 Confidence Total Rules Generated Essential Rules  T10.I4.D100K  T10.I6.D100K   Figure 11 Redundancy lev el v ariation with con\014dence 


 0.1 0.15 0.2 0.25 0 10 20 30 40 50 60 70 80 90   Confidence fixed at 90 Support Total Rules Generated Essential Rules  T10.I4.D100K  T10.I6.D100K   Figure 12 Redundancy lev el v ariation with supp ort 1 Generating maximal p oten tially large itemsets The 014rst step w as to generate L  2000 maximal p oten tially large itemsets These p oten tially large itemsets capture the consumer tendencies of buying certain items together W e 014rst pic k ed the size of a maximal p oten tially large itemset as a random v ariable from a p oisson distribution with mean 026 L  Eac h successiv e itemset w as generated b y pic king half of its items from the curren t itemset and generating the other half randomly  This metho d ensures that large itemsets often ha v e common items Eac h itemset I hasaw eigh t w I asso ciated with it whic hisc hosen from an exp onen tial distribution with unit mean 2 Generating the transaction data The large itemsets w ere then used in order to generate the transaction data First the size S T of a transaction w as c hosen as a p oisson random v ariable with mean 026 T  Eac h transaction w as generated b y assigning maximal p oten tially large itemsets to it in succession The itemset to b e assigned to a transaction w as c hosen b y rolling an L sided w eigh ted die dep ending up on the w eigh t w I assigned to the corresp onding itemset I  If an itemset did not 014t exactly itw as assigned to the curren t transaction half the time and mo v ed to the next transaction the rest of the time In order to capture the fact that customers ma y not often buy all the items in a p oten tially large itemset together w e added some noise to the pro cess b y corrupting some of the added itemsets F or eac h itemset I w e decide a noise lev el n I 2 0  1 W e generated a geometric random v ariable G with parameter n I  While adding a p oten tially large itemset to a transaction w e dropp ed min f G j I jg random items from the transaction The noise lev el n I for eac h itemset I w as c hosen from a normal distribution with mean 0.5 and v ariance 0.1 W e shall also brie\015y describ e the sym b ols that w eha v e used in order to annotate the data The three primary factors whic hv ary are the a v erage transaction size 026 T  the size of an a v erage maximal p oten tially large itemset 026 L  and the n um b er of transactions b eing considered A data set ha ving 026 T  10 026 L  4 and 100 K transactions is denoted b y T10.I4.D100K W e tested ho w the primary threshold v aried with the n um b er of itemsets prestored This result is illustrated in Figure 8 The 014gure sho ws that the primary threshold initially drops considerably as the n um b er of primary itemsets increases but it b ottoms out after a while W e also illustrate the v ariation of the computational e\013ort required with the a v ailable storage space in Figure 9 W e note that for the itemset T10.I4.D100K the computational e\013ort required in order to 014nd additional large itemsets after 014nding 20000 itemsets increases considerably with the n um b er of itemsets prestored This is b ecause for this particular data set the a v erage size of a maximal p oten tially large itemset or bask et is only 4 Consequen tly  the total n um b er of p ossible large itemsets is relativ ely limited On the other hand the computational e\013ort for prepro cessing required b y the data sets T20.I6.D100K and T10.I6.D100K is relativ ely similar This sho ws that the computational e\013ort required to 014nd a sp eci\014c n um b er of primary itemsets is more sensitiv e to the size of a t ypical bask et in the data rather than to the size of a transaction W e also tested the v ariation in the online running time of the algorithm with the n um b er of rules generated W e ran the online queries for v arying lev els of input parameters in order to test the correlation b et w een the running time and the n um b er of rules generated This is illustrated in Figure 10 This result is signi\014can t in that it sho ws that the running time of the algorithm increases linearly with the n um b er of rules generated for all the data sets used The absolute magnitude of time required in order to generate the rules w as an order of magnitude smaller than the time required using a direct itemset generation approac h lik e DHP  A brief summary of some sample relativ e 014ndings is illustrated in T able 3 W e also discuss the lev el of redundancy presen t in the rule generation pro cedure Figures 11 and 12 illustrate that the n um b er of redundan t rules is often m uc h larger than the n um b er of essen tial rules The b enc hmark for measuring the lev el of redundancy is referred to as the redundancy ratio and is de\014ned as follo ws Redundancy Ratio  T otal Rules Generated Essen tial Rules 1 Th us when the redundancy ratio is K  then the n um ber of redundan t rules is K 000 1 times the n um b er of essen tial rules The redundancy ratio has b een plotted on the Y-axis in Figures 11 and 12 W e see that in most cases the n um ber of redundan t rules is signi\014can tl y larger than the n um ber of essen tial rules This illustrates the lev el to whic h useful rules often get buried in large n um b ers of redundan t rules Also the redundancy lev el is m uc h more sensitiv e to the supp ort rather than the con\014dence The lo w er the lev el of supp ort the higher the redundancy lev el 7 Conclusions and Summary In this pap er w ein v estigated the issue of online mining of asso ciation rules The t w o primary issues in v olv ed in online pro cessing are the running time and compactness in represen tation of the rules W e discussed an OLAP-lik e approac h for online mining asso ciation rules whic ha v oids redundancy  


Ac kno wledgemen ts W ew ould lik e to thank V S Ja yc handran and Jo el W olf for their extensiv e commen ts and suggestions References  Aggarw al C C and Y uP  S Online Generation of Asso ciation Rules IBM R ese ar ch R ep ort R C 20899  Agra w al R Imielinski T and Sw ami A Mining association rules b et w een sets of items in v ery large databases Pr o c e e dings of the A CM SIGMOD Confer enc e on Management of data pages 207-216 W ashington D C Ma y 1993  Agra w al R and Srik an tR.F ast Algorithms for Mining Asso ciation Rules in Large Databases Pr o c e e dings of the 20th International Confer enc eon V ery L ar ge Data Bases pages 478-499 Septem b er 1994  Agra w al R and Srik an t R Mining Sequen tial P atterns Pr o c e e dings of the 11th Internation al Confer enc e on Data Engine ering pages 3-14 Marc h 1995  Agra w al S Agra w al R Deshpande P  M Gupta A Naugh ton J F Ramakrishnan R and Sara w agi S On the Computation of Multidimensi on al Aggregates Pr o c e e dings of the 22nd International Confer enc eon V ery L ar ge Datab ases pages 506-521  Chen M S Han J and Y uP  S Data Mining An Ov erview from Database P ersp ectiv e IEEE T r ansactions on Know le dge and Data Engine ering V olume 8 Num b er 6 Decem b er 1996 pages 866-883  Dyreson C Information Retreiv al from an Incomplete Data Cub e Pr o c e e dings of the 22nd International Confer enc eon V ery L ar ge Datab ases pages 532-543 Mumbai India 1996  Gupta A Harinara y an V and Quass D Aggregatequery pro cessing in data w arehousing en vironmen ts Pr o c e e dings of the 21st Confer enc eon V ery L ar ge Datab ases Zuric h Switzerland Septem b er 1995  Han J and F u Y Disco v ery of Multiple-Lev el Assocaition Rules from Large Databases Pr o c e e dings of the 21st Internation al Confer enc eon V ery L ar ge Data Bases Zuric h Switzerland 1995 pages 420-431  Harinara y an V Ra jaraman A and Ullman J Implemen ting Data Cub es E\016cien tly  Pr o c e e dings of the 1996 A CM SIGMOD c onfer enc e on Management of Data Mon treal Canada June 1996 pages 205-227  Houtsma M and Sw ami A Set-orien ted Mining for Asso ciation Rules in Relational Databases Pr o c e e dings of the 11th Internation al Confer enc e on Data Engine ering Marc h 1995 pages 25-33  Kaufman L and Rousseeu wP J Finding Gr oups in Data A n Intr o duction to Cluster A nalysis Wiley Series in Probabilit y and Mathematical Statistics 1990  Klemen ttinen M Mannila H Ronk ainen P  T oiv onen H and V erk amo A I Finding in teresting rules from large sets of disco v ered asso ciation rules Pr o c e e dings of the Confer enc e on Information and Know le dge Managements Gaithersburg MD USA 28 No v 2 Dec 1994  Len t B Sw ami A and Widom J Clustering Asso ciation Rules Pr o c e e dings of the Thirte enth International Confer enc e on Data Engine ering pages 220-231 Birmingham UK April 1997  Mannila H T oiv onen H and V erk amo A I Ef\014cien t algorithms for disco v ering asso ciation rules AAAI Workshop on Know le dge Disc overy in Datab ases pages 181-192 Seattle W ashington July 1994  Ng R T and Han J E\016cien t and E\013ectiv e Clustering Metho ds for Spatial Data Mining Pr o c e e dings of the 20th Internation al Confer enc eon V ery L ar ge Data Bases San tiago Chile 1994 pages 144-155  P ark J S Chen M S and Y uP  S An E\013ectiv e Hash Based Algorithm for Mining Asso ciation Rules Pr o c e e dings of the 1995 A CM SIGMOD International Confer enc e on Management of Data pages 175-186 Ma y 1995  Piatetsky-Shapiro G Disco v ery  Analysis and Presentation of Strong Rules Know le dge Disc overy in Datab ases 1991  Sa v asere A Omiecinski E and Na v athe S An E\016cien t Algorithm for Mining Asso ciation Rules in Large Data Bases Pr o c e e dings of the 21st Internation al Confer enc eon V ery L ar ge Data Bases Zuric h Switzerland 1995 pages 432-444  Sh ukla A Deshpande P  M Naugh ton J F and Ramasam y K Storage Estimation for Multidimensi on al Aggregates in the Presence of Hierarc hies Pr o c e e dings of the 22nd Internationa l Confer enc eon V ery L ar ge Datab ases pages 522-531 Mum bai India 1996  Srik an t R and Agra w al R Mining Generalized Asso ciation Rules Pr o c e e dings of the 21st International Confer enc eon V ery L ar ge Data Bases  pages 407-419 Septem b er 1995  Srik an t R and Agra w al R Mining quan titativ e association rules in large relational tables Pr o c e e dings of the 1996 A CM SIGMOD Confer enc e on Management of Data Mon treal Canada June 1996  T oiv onen H Sampling Large Databases for Asso ciation Rules Pr o c e e dings of the 22nd International Conferenc eonV ery L ar ge Datab ases pages 134-145 Mumbai India 1996  Ziark o W The Disco v ery  Analysis and Represen tation of Data Dep endencies in Databases Know le dge Disc overy in Datab ases 1991 


CMP A Fast Decision Tree Classifier Using Multivariate Predictions  449 H Wang and C Zaniolo Mining Recurrent Items in Multimedia with Progressive Resolution Refinement  461 0 Zai'ane J Hun and H Zhu Panel Session 22 Is E-Commerce a New Wave for Database Research Moderator Anant Jhingran IBM T.J Watson Research Center USA Panelists Sesh Murthy IBM T.J Watson Research Center USA Sham Navathe, Georgia Institute of Technology USA Hamid Pirahesh IBM Almaden Research Center USA Krithi Ramamrithan University of Massachusetts-Amherst USA Industrial Session 23 Java and Databases Pure Java Databases for Deployed Applications  477 N Wyatt Database Technology for Internet Applications  700 A Nori Session 24 Association Rules and Correlations Finding Interesting Associations without Support Pruning  489 E Cohen M Datar S Fujiwara A Gionis P Indyk R Motwani J Ullman and C. Yang Dynamic Miss-Counting Algorithms Finding Implication and Similarity Rules with Confidence Pruning  501 S Fujiwara J Ullman and R Motwani Efficient Mining of Constrained Correlated Sets  512 G Grahne L Lakshmanan and X Wang Session 25 Spatial and Temporal Data Analyzing Range Queries on Spatial Data  525 J Jin N An and A Sivasubramaniam Data Redundancy and Duplicate Detection in Spatial Join Processing  535 J.-P Dittrich and B Seeger Query Plans for Conventional and Temporal Queries Involving Duplicates and Ordering  547 G Slivinskas C Jensen, and R Snodgrass xi 


Industrial Session 26 XML and Databases Oracle  The XML Enabled Data Management System  561 S Banerjee V Krishnamurthy M Krishnaprasad, and R Murthy XML and DB2  569 J Cheng and J Xu Session 27 High-Dimensional Data Independent Quantization An Index Compression Technique for High-Dimensional Data Spaces  577 S Berchtold, C Bohm H Jagadish H.-P. Kriegel and J Sander Deflating the Dimensionality Curse Using Multiple Fractal Dimensions  589 B.-U Pagel F Korn and C. Faloutsos Similarity Search for Multidimensional Data Sequences  599 S.-L Lee S.-J Chun D.-H Kim, J.-H Lee and C.-W Chung Session 28 Web-Based Systems WRAP An XML-Enabled Wrapper Construction System for Web Information Sources  611 L Liu C Pu and W. Hun Self-Adaptive User Profiles for Large-scale Data Delivery  622 U Cetintemel M Franklin and C. Giles Industrial Session 29 Main Memory and Small Footprint Databases In-Memory Data Management in the Application Tier  637 The TimesTen Team SQLServer for Windows CE -A Database Engine for Mobile and Embedded Platforms  642 P Seshadri and P. Garrett Join Enumeration in a Memory-Constrained Environment  645 I Bowman and G Paulley xii 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


