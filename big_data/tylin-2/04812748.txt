The ARNO Project: Challenges and Experiences in a Large-scale Industrial Software Migration Project  Werner Teppe Amadeus Germany GmbH 61348 Bad Homburg, Germany wteppe@de.amadeus.com  Abstract The following paper presents the experience gained from a large scale software migration project It describes how the migration strategy was selected and how the project was planned. It emphasizes the importance of keeping the cost s as low as possible. It then goes on to outline the methods and techniques used in the migration with special emphasis on the migration process. After that, it deals with the tools used in converting the code, migrating the data and reimplementing the job control. As with all migration projects, testing played a big role and consumed a significant part of the resources. Thus, the testing techniques and tools are given particular emphasis The paper ends with a summary of the lessons learned from this project and how they can be applied to other similar projects Keywords ARNO Project, Software Migration START System, Replacement of mainframe systems Software reengineering, BS2000, UNIX 1  Amadeus and the Amadeus Germany START Amadeus Germany GmbH is Germany’s leading provider of IT solutions for the travel industry. With its Corporate Solutions division, Amadeus Germany also offers full-service business travel solutions to facilitate efficient corporate travel management. An extensive selection of training courses rounds out the portfolio. In Germany, 85 percent of all travel agencies running approximately 45,000 PCs deploy the state-ofthe-art and highly efficient Amadeus System In Germany, the Amadeus System can be used to make bookings with: approximately 500 airlines, more than 75,000 hotels, 22 car rental companies, nearly 200 tour and bus operators, 74 public transport associations, 40 European railway companies, 30 ferry companies, 6 insurance companies, 3 event ticketing systems representing more than 1,000 event organizers, as well as 8 cruise lines The sole owner of Amadeus Germany is the Amadeus IT Group SA, a worldwide leading provider of technology and sales/distribution solutions for the travel and tourism industry. More than 90,270 travel agencies and 29,660 airline ticketing offices – using over 600,000 PCs – in more than 217 markets around the globe utilize the company’s network and efficient data center. The START System is the link between provider systems on one side and travel agencies and end customers on the other \(see Figure 1\tion to travel information, booking and reservation requests and confirmations, travel documents and accounting data are provided, and the payment flows between the participating business partners are organized. The Amadeus Germany System is linked to the international Amadeus System described above This paper summarizes the process, methods, tools and techniques used in migrating large application systems The goal of the project pres ented, was to migrate all applications on a mainframe platform to UNIX, replacing the entire system platform and substantially reducing hardware, software license and infrastructure costs The migration project was named ARNO as an acronym for “Application Re location to New Operating System”. The special cha llenge of this project was rooted in the fact that the application systems migrated interconnect to some 200 external partner systems Another challenge was to keep the current systems in operation throughout the migration. It had to be possible to issue new application releases every month during the course of a project, which spanned several years. Besides converting the applications, the highperformance file handling sy stem had to be replaced by a relational database, the middleware had to be converted and a multitude of job control scripts had to be ported. The systems in question must run virtually 24 hours a day, 7 days a week with a peak processing load of 1500 transactions per second. This same performance had to be duplicated in the new environment 2  Initial Situation 2.1  Continuous Evolution of the Applications The Amadeus START System originally developed between 1976 and 1979 and went into production in 1979 with a proprietary network system for over 800 dumb” terminals, 500 printers and 3 computer links to provider systems. The developers of that time used the then latest software engineering knowledge in the 
2009 European Conference on Software Maintenance and Reengineering 1534-5351/09 $25.00 © 2009 IEEE DOI 10.1109/CSMR.2009.64 145 
European Conference on Software Maintenance and Reengineering 1534-5351/09 $25.00 © 2009 IEEE DOI 10.1109/CSMR.2009.64 149 


course of the implementati  1  5   m a ki ng i t  possible to be evolve the system for another  30 years see above\l as to efficiently implement fundamental changes in the technology of the mainframe end device and network areas, while still retaining the basic system architecture The applications ran on a number of mainframe computer systems deploying the BS2000 operating system. Several UNIX and LINUX satellite systems have since been implemented systems to provide add-on services   Figure 1: The Amadeus Germany System     Figure 2: The Main Migration Process \(MMP   Travel booking agencies Companies Ticket agencies End customers Computer centers 2 Business travel Travel agents Privat customers Corporate customers Front-Office Mid-Office Products Amadeus Selling Platform Internet products Business  Travel Management  Amadeus Germany Airlines 488 Hotels 235 chains 54.000 Hotels Cars 43 providers 29.550 stations Event organizers 12.000 Events Deutsche Bahn German nat. Rail Tour- and bus operators 170 Hotels Isurance / Creditcards 8 / 5 Providers Ferry operators 33 Railways 40 Public transport 61 providers Rail 2 providers Cruise  3 providers  Train and  public transit bus operators 4 Amadeus 
146 
150 


2.2  Reasons for a Migration The START system was implemented on the main frame using the System Programming Language, a language developed in the early 1970’s to support structured programming at th e machine level. With it higher level control commands were combined with lower level IO operations, making it a good language for systems programmers. As time progressed, new and more promising software development tools and languages have emerged, especially in the UNIX environment. There are also ha rdly any programmers left who are familiar with the SPL language and young programmers are not inclined to learn it. The fact is that the SPL language was conceived for mainframe systems programming and was never intended to be used as an application language in a distributed environment. Thus, the language was no longer suitable for modern distributed applications. The time had come to retire it. Besides that the BS2000 operating system was no longer the state of the art and the data management system was antiquated. Something had to be done to ensure the continuity of the service 3  Procedural Model Used in the Project 3.1  Study As a first step, the project managers conducted an expert study to determine how the systems could be replaced. Following that, they performed an analysis of the costs of the current system and of the potential new systems as well as an evaluation of the maintenance further development and opera tion of the applications in the old and new environments. Three alternate migration strategies were up for debate 1. New development of the applications with the same scope of functionality 2. Automated conversion of the existing programs scripts \(jobs\les 3. Development of a cross compiler to generate UNIX object code from SPL sources thus allowing the SPL source text to be retained Here are the main criteri a for the decision, which was made on the basis of extensive individual evaluations: Alternative three would have meant that further development continues in the current language. This was not a solution to the inadequacies of the development environment, the constraints of the language and the lack of available personnel. The product stakeholders calculated the costs of alternative two – the conversion – to be hardly higher than the costs of developing a cross compiler. The costs of the first alternative new development – were estimated to far exceed the costs that would be incurred by the other two alternatives. It would have been tantamount to the development of a new system with the same scope of functions the disadvantage being, however, that all the bugs inherent in a newly developed system would have had to been detected and corrected. It would have taken years for the new system to attain the same state of stability as the old one. In contrast, conve rsion errors are local and can be easily located Besides, a new development with all its costs and risks is difficult to sell to users who are basically satisfied with the functionality they have. That was the case here. There was simply no business justification of the new system es the risks would have been greater than the calculated benefits. For this reason the product stakeholders opted for alternative two automated conversion Java was ruled out as a target language because the gap to the source language – SPL – was too great. Despite years of research no one has yet to resolve the problem of automatically converting a procedural language into an object-oriented one. On the other hand the stakeholders wanted to retain the option of evolving the applications to an object-oriented paradigm This left C++ as a compromise solution. The SPL code could be converted over into procedural C, which could be processed by a C++ compiler. Later the code could be objectivized by defining objects within the existing components without having to switch over to another language. PERL was selected as a script language for the job control becau se of its greater flexibility. Oracle was selected to be the new database system because it was already a co rporate standard. Several applications targeted for migration were already running under the UTM transaction monitor. The same Siemens product is also available under UNIX with the same functionalities. Thus, it was obvious to continue with it rather than having to deal with yet another product. It was considered risky enough to be exchanging the programming language, the database system and the process control language without dealing with the transaction monitor [10  The migration of the SPL source, the SDF script and the existing data files was planned from the beginning to be automated. Since there were no such conversion tools on the market, they had to be developed Since the development of these tools was a one-time project-specific task and tool development was not a core competency of the user organization it was decided early on to outsource this work. However, in view of the expertise and knowledge of the various processes, it was decided to perform the migration work itself in house. This fundamental decision turned out to be a cornerstone fo r the success of the project   
147 
151 


3.2  Project Organization There were many stakeholders in this project - various business divisions and legally independent companies - participating in the project. For this reason it was essential to include the top management of the respective companies in the pr oject steering committee which functioned as a central point of coordination and decision making instance for the project manager. This was the only way to ensure the necessary prioritization over other projects and facilitate short decision-making processes. It also guara nteed the required “management awareness”. A major risk was that projects with visible" benefits to the customer would be squeezed in during the course of the project, thus taking resources away from the migration. Experience had shown that this leads to resource conflicts and excessive implementation times and is a major reason why long running migration projects are abandoned. It is impossible to freeze a living system during migration Changes and corrections must be made. The longer a migration lasts the more the migrating system will change. Thus, a goal is to push through the migration as fast as possible. Even then a well defined change management process is vital to the project success. All changes made to the old code must be replicated in the new code and this in a controlled manner 3.3  Preparing the Migration \(“first renovate then migrate Prior to converting old software it is wise to first clean it up. This has been argued by Sneed, in respect to his 30 years of reengineering and migration expe at migrating low quality and redundant software will only increase the migration costs and carry the problems over to the new environment. Here, a complete inventory of all program sources, scripts \(jobs\d third-party utility programs was taken in order to determine the contents of the migration package and to define the precise scope of the project. To this end, th e team identified elements which were no longer require d and could be removed from the system, for example “dead code” and unreferenced data. A project to clean up the systems in the BS2000 environment was carried out prior to the conversion. In addition, the team consulted with the product managers to discuss wh ich products could be taken off the market before the project began. These obsolete or redundant products were then removed from the migration portfolio. This helped to reduce the conversion and testing costs, and to spare resources. The migration project acted here as a catalyst for streamlining the product range 3.4  The Main Migration Process \(MMP For cost reasons, it was decided that the language conversion tools should not convert the complete language scope of the respective systems   The conversion therefore required an ite rative approach. The basis for all conversions was the current source versions \(see Figure 2\Each source was converted by the appropriate conversion tool, e.g., STC. If no errors occurred, it was included in the repository of generated C sources. If errors in the compilation occurred, an analysis was made to determine whether the SPL source should be adjusted or whether the tool should be improved. Among other things, this depended on the number of similar “problem cases”. Once all required objects were converted, they were compiled and linked by the C++ compiler \(makepp ing this operation, the decision was made again whether to adjust the converted source text or the conversion tool. If the error occurred only seldom it was cheaper to correct the sour ce. If it occurred often it was worth it to correct the tool. As such, this was truly a cost driven project If an object was successfully linked, it was tested. If the test was successful, all the linked components were considered to be migrated. Otherwise, the convert compile and link process had to be repeated. The same procedure applied to all programs, jobs, files and data interfaces. The modifications to the original objects in BS2000 were then replicated in the converted objects before they were submitted to an overall functional test. Thus, at the time of the final system switchover the SPL and C++ sources were functionally equivalent It was possible to compare the outputs of the one with the outputs of the other \(“verification in production The status of each individual source member was recorded and monitored in the configuration management system. The specific conversion tool version used to convert an object was also archived for reuse, since modifications to the tool might have rendered already successful conversions to be unsuccessful if they were repeated later 3.5  The Integrated Test Process \(ITP The test process involved not only testers from the project team but also persons from the user departments, from product management and the help desk. A test process was set up to document the test results with the aid of the configuration management system In the case of errors, the error messages were recorded as RFD’s \(“Requests for Development”\est coordination team maintained a constant overview of the test progress and the status of reported errors. The RFD’s were passed on to the responsible developers for correction. Objects correct ed by the migration team 
148 
152 


were resubmitted to the test process for regression testing  Automa tic tests Master RFD p er Master RFD p er Product Manage Develop ment Depar tment Mana Departm ent Manager issue In fo In fo Rep r es entativ Repres entativ Automatic / manual Test O K To test To test Test OK  N OK  Sub Sub In fo In fo ARNO Test Coordination    Standar d Error Figure 3: The Integrated Test Process \(ITP 3.6  Staff At the start of the project, it was recognized that both the development and production staff working with the mainframe environment would also have to be migrated”, i.e. retrained, in order to retain their application and production know-how. It was clear that these employees and their know-how were major assets to the company which needed to be preserved Process knowledge is more important to a user organization than technical knowledge, e.g. the command of a particular programming language or operating system. This required the staff to be thoroughly trained in maintaining and using the target systems. Numerous informational events as well as the establishment of project WIKIs and BLOGs ensured the compilation and distribution of information. This retraining effort simplified the subsequent re-documentation of the existing systems and gave rise to an important change process within th e entire company 3.7  Involving the Line Organizations Since the operation of the computer center and the applications was distributed among different company departments, the migration project team had little influence on the system administration and organization of the computer center. Th e responsibility for the application systems was only temporarily turned over to the migration team for the duration of the project. The project charter specified th at once the migration was completed and validated via system, acceptance and performance tests, the responsibility would be transferred back to the line orga nization. This also meant that the converted sources had to be handed over to the responsible development departments. Involving the affected development, maintenance and production departments in the migration project helped to prepare them for their later respons ibility. Their staff was given roles in the project which would enable them to carry out similar tasks later in the line organization 4  Technical Implementation 4.1  SPL to C As pointed out above, the applications being migrated were programmed in SPL, a procedural programming language to support system level programming under the operating system BS2000. The large volume of source code prohibited manual rewriting There were altogether 25.000 source members with some 2 million source statements and more than 4 million lines of code. There wa s no alternative to an automated conversion process. The target language of choice would have been C because it is the closest to the procedural structure of the SPL and also because it is widely used in the UNIX world. However, since C does not contain equivalent counterparts of all SPL language elements, these had to be provided in the form of a 'SPLCompat' compatibility layer. This meant that not C but C++ was chosen since C++ provides the mechanisms to implement the missing elements Among other things, the compatibility layer contains the data types that are available in SPL, but for which there are no corresponding types in C, e.g. bit strings and hexadecimal strings. In C++ these types could be implemented as classes for string and bit string processing with a complete set of operators to produce exactly the same resu lt as in SPL. The library also includes the IO operations which in SPL are at the system level. These too could be replicated in technical classes [5 Using the SPLCompat library, the STC converter SPL to C++\could generate a UNIX-compatible C code. The conversion addressed numerous non-trivial incompatibilities such as the visibility of variables. For each and every incompatibility a mapping rule was defined for mapping that particular SPL constructs to a functionally equivalent C++ construct. The use of these strict mapping rules e liminated the need to address every individual incompatibility problem. The individual incompatibilities could be resolved with a general solution. This implied that the original SPL source had to be adapted to comply with the general rules. Here again is a case where reengineering is a prerequisite to migration. The SPL code had to first be unified in order to be converted automatically. Compliance with the rules in the SPL code was verified before the conversion and de viations were corrected 
149 
153 


and tested in the SPL code under BS2000, thus providing the basis for an unambiguous conversion. During the conversion phase, details of these rules had to be adjusted several times. This meant that the SPL source also had to be readjusted Another problem arose with respect to the definition of variables and constants created in C++ where the appropriate type could not be defined until their actual usage was determined. This is a problem with all typed languages. Many of these variables and constants were also defined in include members, the SPL equivalent of C header files, which were used in many modules. As it turned out the different SPL modules did not use these variables and constants in exactly same way. Each had its own view of them. This led to the generation of different incompatible C++ includes for one and the same SPL include The first solution to reducing the number of these multiple definitions was to transform them into a common all encompassing definition. Where that was not possible the developers were compelled to adapt the code of the include members to a common view before generating the corresponding C++ header files Here again, reengineering had to precede migration The few remaining problems were then manually corrected in the target language after the conversion Thus, despite all efforts, a fully automated conversion of the code was not possible. Even the comprehensive set of guidelines and the reduction of the language scope did not enable the creation of unambiguous conversion rules for all cases. The STC converter generated alerts for the remaining problem cases which were then manually reviewed and corrected as necessary. This process was managed with a wellorganized configuration management system specifically customized for this job. A high degree of automation was achieved for the language conversion process but it was never 100% \(educated guesses said: more than 95%\bitious goals like the elimination of all GO TO branches and the creation of objects with multiple instances were avoided. A fully automated conversion to support these features would have driven up the costs of the projects. The goal here was to make the conversion as cheap and with the least risk as possible. This is a goal of most technical migrations in industry 4.2  JCL to Perl In addition to the programs, a large number of BS2000 job control procedures had to be migrated to equivalent UNIX scripts. In view of the large number manual conversion was out of the question. In contrast to the source code of the programs, the existing control procedures were far less homogeneous and less welldefined. On the one hand, the Job Control Language JCL\S2000 actually comprises several different languages like ISP and SDF. Furthermore many procedures included utilities such as ‘merge’ and sort' routines. Other procedures invoked system specific utilities, such as the EDT editor in BS2000. This required special proprietary commands. To make the command code homogeneous and to create a basis for automated conversion, the first step was to preprocess all procedures with the ai d of the Fujitsu Siemens SDF-CONV converter to transform them into uniform job procedures in a well-defined subset of the SDF language. In the second step, a somewhat simpler JTP JCL to Perl\er was able to convert the preprocessed procedures into Perl For the most common utilities, equivalent routines were written in Perl. Although there are comparable substitutes for most utilities in the Unix environment e.g., the 'sort' utility under Unix, the functional scopes of the products are different. This prohibited the simple substitution of the utility calls. They had to be rewritten by hand. Conversely formally defined conversion rules enabled the JTP converter to insert the newly written Perl utilities in place of the former BS2000 utility calls. Nevertheless, some control procedures still had to be rewritten prior to conversion.  For example, it was cheaper to rewrite those procedures containing the internal procedural language of the BS2000 editor \(EDT\before the conversion than to rewrite the procedures containing these elements in UNIX. Analogous to the STC procedure, JTP generated a complete list of the job control procedures that could not be fully converted. This left it to the developers to manually complete the conversi   4.3  Filehandler to Oracle Data management under BS2000 was handled with a proprietary, low-level program library called the “Filehandler”, containing the essential access operations of a database system – insert, select, fetch, delete, update, etc. A conversion of these operations would have been extremely costly, due to their dependence on the host operating system. A new development of the library routines would have been equally expensive Some IO routines were written in OS assembler After a thorough evaluation, the approach chosen was to deploy the Oracle 10 database on the Siemens host machine. Since the previous Filehandler had a clearly defined call interface the application logic did not have to be modified. Based on the existing interface, a middleware access layer was inserted, whose task it was to translate the previous Filehandler calls into SQL database queries of the Oracle database. This not only required remodeling the functional scope, but 
150 
154 


also converting, e.g., Oracle error messages to the corresponding Filehandler error codes. Since the existing interface was well-defined, a fully automated conversion was possible. It was also possible to test the new access layer with the relational database on the mainframe prior to the actual conversion The new access layer achieves a clean decoupling of the application layer from the data access layer, thus ensuring data independence of the applications. This encapsulation makes it rela tively easy to replace the data storage system at a late r date, e.g. when opting for a different database supplier 4.4  DCAM Monitor to openUTM The mainframe applications encompassed a proprietary middleware layer, the DCAM monitor – based on the basic communication methods provided by BS2000 Data Communication Access Method\nce it had been specifically developed to the requirements of the START applications, it was extremely efficient To achieve its performance goals the DCAM monitor had been implemented using many Assembler level routines built into the SPL code. Besides having to convert the SPL code to C++, it would also have been necessary to translate all of the system calls to their exact equivalents in Unix. This, in turn, would have required an additional middleware layer for the emulation of all BS2000 system calls. Not only would this have given rise to a signi ficant amount of additional work and expense, but it would also have led to a BS2000 communication emulator under Unix. This was contrary to the goals of the project which was to free the application software from all dependencies on the BS2000 operating system Therefore, the decision was made to deploy a standard middleware, openUTM \(Universal Transaction Monitor\ Fujitsu Siemens under both BS2000 and Unix. Fortunately the product was available in both environments. In the first step this transaction monitor was installed under BS2000 and the applications there switched over to it. Thus, the new transaction monitor was already in production, even before the migration began. The applications using it could then be converted without changes to the architecture or middleware interfaces a nd were directly operable under Unix. This platform change was transparent to the communication partners. Consequently, the migration was not burdened by a conversion of the transaction framework. There were no additional costs for running it in the target environment. This also made regression testing a lot easier 5  Migration Tools and their architecture One of the key success factors of ARNO were the dedicated migration tools, specifically developed for this project. They enabled a high degree of automation as has been pointed out in previ   Of particular importance in choosing the right conversion tool is, whether the programming language of the legacy system should be preserved on the target system or not. In the case of COBOL, it is mostly preserved on the target system despite of all criticism. In this case only an adaption of the dialect is necessary For the conversion process a simple task pattern recognition algorithm based on regular expressions is adequate descri bes a t ool whi c h convert s di fferent  COBOL dialects. Controlled by a graphic Eclipse interface the complex pattern recognition algorithms of Perl are used However, when converting one programming language into another, the use of regular expressions falls short.  A complex translation processes requires complex translation techniques. Translators are tools which automatically convert source programs in language A into source programs in language B For the ARNO migration project, a tool to translate SPL into C++ had to be developed. This was accomplished using a transformation architect ure similar to that of a compiler. This transformation model had already been under development for many years. It is displayed in picture 4 and described in   The architecture of a transl ator is based on the classic compiler model.. But there are some differences 1 Preservation of comments: Translators don’t discard comments. They are preserved during the whole conversion process to insert them later into the target code 2 Preservation of preprocessor information: According to the classic co mpiler model preprocessor instructions are discarded after they have been resolved. In a translator, they have to be preserved to the end of the conversion process, e.g. macros of the source programs have to appear optionally as macros in the target program and th e include instructions are needed during the generation for the partitioning of target sources into several files 3 Interface between source- and target representation: In translation, there exists a strong separation between the two. The parser provides an attributed syntax tree of the source pr ogram. Out of that the converter generates an attributed syntax tree of the target program 4. Postprocessor: Compar ed to the classic compiler model this component is unique to a translator Among the main tasks of a translator are the division 
151 
155 


of the complete attributed syntax tree of the target program into partial syntax trees, the insertion of preprocessor instructions of the target language and the optional integration of the program’s comments. These actions are based on syntax trees rather than on files 5 Generator: The generator writes out source code by traversing separate partial syntax trees. During a conversion to C++ several header files and a main program have to be handled in one pass. Because of maintainability requirements, the formatted output should be configurable. The operation of the postprocessor can be described as follows: Within a classic compiler the task of a postprocessor is to execute preprocessor instructions, which results in a single source code file without preprocessor instructions. Preprocessor instructions contain important information, e.g which include files the source program consists of which macros appear at which position within the include files and which comments occur at which positions. In contrast to classic compiling this information is preserved during the translation. The preprocessor and scanner insert new tokens into the token stream which will be read by the parser. After parsing they will be assigned to the nodes of the syntax tree based on the source code position. The converter put them in the position to the corresponding nodes of the target syntax tree. The postprocesso r uses this information for the subsequent tasks 6 Division of the targ et syntax tree: There comes a partial syntax tree for each include file with semantically equivalent content. For that the target syntax tree must be broken up into partial trees, which are later joined together in the final source  Figure 4: The Translator architecture   7. Placement of macros All partial trees resulting from macros in the source program are annotated in the target syntax tree The postprocessor compares these partial trees, defines a macro of the target language and replaces the syntax trees by syntax trees of the corresponding macro calls. The partial syntax tree of the macro is inserted at the corresponding position within the full syntax tree 8 Reinsertion of comments: Comments are reassigned to the single partial tree of the target program heuristically as pre- and pos t-comments. Then they are placed before or after th e corresponding target code fragment by the generator For the development of the translators in the ARNO project the following, self-developed generator tools Meta tools\n figure 4 BTRACC Backtracking Compiler Compiler, a parser generator based on the backtracking process, which can optionally generate parser in C, Java or Perl CTree A tool for the declarative description of syntax tree models CBack A tool for the generation of structured C/C code from syntax trees. The formatting of the code can be configured optionally Experience has shown that the development effort for translators amounts to 3 to 3.5 man-years. By using the meta-tools described here the effort was reduced by at least 25 6  Test The explicit objective of this migration project was to execute the conversion in such a way that would not negatively affect the customers and, ideally, would go 
152 
156 


unnoticed by them. Since no new functions were added to the system during the conversion, it had to be verified that the new system would be functionally equivalent to the ol  Several t e st s were requi red t o  confirm this. The batch processing tests were comparatively simple, “only” requiring verification that the jobs under UNIX and BS2000 generated identical output files from identical input files. Of course, the different encoding on both systems \(EBCDIC in BS2000 Latin-9 under UNIX\account when comparing the files The online testing presented a greater challenge. Due to the large number of interconnected partner systems each with their own databases, the comparison of the online application results was much more difficult 6.1  Test Procedures for Regular Version Releases For the applications running under BS2000, approximately twelve versions were created and rolled out per year. Other than the routine error corrections and changes, these updates also included newly developed functions. For testing them, an established test procedure had been in place for many decades. In addition to verifying the implementation of the new functions the tests focused on ensuring that the modifications did not adversely affect the other functions. To this end simple regression tests we re performed. For some areas that were rarely modified and had little interaction with new or modified components, the test coverage was not in-depth enough for the migration project For example, negative tests were missing in many cases. Particularly for components that were older, only few automatable test cases were available 6.2  Test Automation To ensure adequate coverage of the migrated systems, test automation was considered essential. The objective was to promptly test each newly migrated application system without having to involve the responsible development departments. To achieve this more than 500 automated test cases were developed by the existing test teams, making it possible to test several program versions a day without the need for setting up a new test team for the duration of the project The newly created automated tests had another positive effect. They could be reused for the further evolution of the migrated sy stems. They are now fully integrated into the regular test runs and contribute to assuring the quality of each new release on the new system platform 6.3  Load Tests To avoid unnecessary costs, the applications in question were not subjected to regular load tests under BS2000. Since the consecutive version releases only slightly influenced the response under load, monitoring was limited to behavior in productive operation. The migration to UNIX completely changed the performance profile of many components. For example, a new database layer was added to the architecture which giving rise to a completely different behavior under load. Therefore, verification that the required load could be handled was an essential prerequisite for going productive under UNIX To assure this, a load test environment had to be set The fact, that individual components had to be tested in a complex application network, made the setup extremely difficult. But, a test was defined that realistically simulated the application environment and enabled a long-term test monitoring. This application environment included among other things, components for the simulation of external partner systems. This test required realistic test scenar ios that could be repeated any number of times. With some test cases, e.g., booking a seat on an airplane, this was difficult to implement. If a seat had been reserved, it was not possible to book it a second time since it was already occupied Despite the difficulties incurred, the tests could be conducted successfully, and the application was in operation for more than twelve hours under the required load. During the long-term test, it was also verified that no leaks existed, e.g. when using memory semaphores or other resources. These tests not only verified compliance with th e requirements, they also facilitated the early detection and correction of several bugs which had not become manifest in the development environments without parallel processing 6.4  Replay of a Productive Online Session Message Protocol Test Even sophisticated tests do not reveal all bugs and errors. Not every data constellation is considered when designing the test cases. Some bugs only become manifest by a coincidence of circumstances. Therefore some problems with complex application can only be detected after productive operation has commenced In order to further enhance test coverage, a further test was designed. All messages generated by the applications being migrated under BS2000 during productive operation were recorded and, using an identical time sequence, subsequently sent to the new applications running under UNIX. In this scenario, the linked partner systems were simula ted since it was not possible to otherwise integrate them into the test run. The results were then compared and the deviations evaluated At first sight, this test might appear to be simple However, the details of it presented tremendous diffi 
153 
157 


culties. For example, prior to each test run, the database of the application had to be reset to precisely the state before the start of the recording. Even small deviations would result in a vast number of errors.  Furthermore, all components involved in the application network had to be simulated. Exact compliance with the recorded time pattern was essential here, as well To this end, some of the simulators from the load test were used. The system time had to be synchronized with the test data stream. Otherwise, requests for flights on a certain date would have triggered an error message instead of a positive response, since the desired departure date of the original message would have referred to a date in the past Comparing the results had similar difficulties. Even for identical test runs, the results did not exactly match the reason being that on multiprocessor systems, there is no deterministic sequence in which sub-tasks at the micro-level are processed if routines with synchronized subtasks are started at the same time. Therefore incoming messages were not always processed in the same order, which resulted in differences in message logs and databases. Consequently, 24-hour test sessions never yielded the exact same final status. An automated analysis to determine whether such differences only arose from these effects proved difficult. In the end, some deviations remained that could only be evaluated manually 6.5  Test Results The many different tests – integration testing, functional testing, performance testing and regression testing – resulted in high quality software being rolled out on time. Each alternate test method revealed different types of errors that were not detected by the other tests making each test method indisp ensable. Due to the fact that this application had never before been subjected to such in-depth testing, a surprisingly large number of bugs and errors were identified that the application system already contained under BS2000, but which had not become manifest until the migration. Of course, these bugs and errors were also migrated to UNIX along with the entire software. It is typical of migration projects that they reveal many buried errors  7  Conclusions Within the scope of the ARNO project, many questions and issues came up which were addressed and discussed by the experts, specialists and knowledge holders involved. The active involvement of users and developers led to improvements in the software development environment, configuration management, test systematic and the test systems. The software development and rollout processes were also reviewed and optimized. These improvements cannot be conclusively assessed yet with regard to reducing costs and facilitating more rapid software development Decisive success factors of this complex and unique project included consistent change management of the project requirements and the modifications to the objects being migrated during the course of the project utilizing a configuration management system. Other key success factors were the support of top management in the steering committ ee, the practical distribution of tasks, including the outsourcing of the tool development, and a highly motivated project team The factor that contributed most to the success of this project was the constraints on the migration. The responsible managers resisted the temptation to get bogged down by technical issues such as restructuring and objectivizing the code. This was not a reengineering project. Reengineering of the original code and data was limited to what was absolutely necessary to enable the conversion. The lesson learned was that it is very important to separate  The concern of migration is to transform a system from one environment to another with a minimum of side effects. It remained the guiding principle of the ARNO project References and related work  Denert, E.: Software-Engi neering. Springer Berlin 1991  Di jkst ra, E A Di sci p l i n e of Program m i ng Prentice-Hall, Englewood Cliffs, N.J., 1976 3 m en g e r, U.; Kaiser, U.; Lo o s A.; Uh lig  D Methoden und Werkzeuge für die Software Migration   83-97 4 Gim n ich   R., Kaiser, U.; Qu an te, J.; W i n t er, A eds.\ 10th Workshop Software Reengineering \(WSR 2008\. 126, Bonn, 2008  Gut t a g J.V Abst ract dat a  t y pes and t h e devel opment of data structures. CACM, 20\(6 6 as, D.L.: On th e crite ria to be used in decomposing systems into modules. CACM, 15\(12\10531058, 1972  Systemtest, Hanser Verlag München, 2008  Sneed, H Aufwandsschät z ung von R eengi neeri ng Projekten. Softwaretechnik-Trends, 23\(2  odernizing Legacy Systems Software Technologies, Engineering Processes, and Business Practices. Addison-Wesley, Boston, 2003   Teppe W   Eppi g, R   Das AR NO Projekt  Herausforderungen und Erfahrungen in einem großen industriellen Software-Migrationsprojekt  99-113  B r odi e, M  L. et al  M i grat i ng Legacy Sy st em s  Gateways, Interfaces & Th e Incremental Approach Morgan Kaufmann, San Francisco, 1995 
154 
158 


 11 R EFERENCES     Reddy, M.K. and S.M. Reddy 223Detecting FET Stuck-Open Faults in CMOS Latc hes and Flip-Flops,\224 IEEE Design and Test of Computers Vol. 3 , No. 5 , pp. 17-26, October 1986   2 R Mad g e , M. Vilg is, a nd V. Bhide, "Achieving Ultra High Quality and Reliability in Deep Sub-Micron Technologies using Metal Layer Configurable Platform ASICs", MAPLD 2005    Kewal Sal u ja, \223Di g i t a l Sy st em Fundam e nt al s, Lect ure 11\224, Department of Electrical Engineering, University of Wisconsin Madison    Yu W e i  Papos ng  M oo Ki t Lee Peng W e ng Ng C h i n  Hu Ong, \223IDDQ Test Challenges in Nanotechnologies: A Manufacturing Test Strategy\224, Asian Test Symposium 2007. ATS apos;07. 16 th Volume , Issue , 8-11 Oct. 2007 Page\(s\211 \226 211  NASA GSFC Advi sory NA-GSFC 2004-06   Dan El ft m a nn, Sol o m on W o l d ay and M i nal  Sawant  New Burn In \(BI\ethodology for Testing of Blank and Programmed Actel 0.15 \265m RTAX-S FPGAs MAPLD 2005   M i nal Sawant Dan El ft m a nn,  W e rner van den Abeel an John McCollum, Solomon Wolday and Jonathan Alexander 223Post Programming Burn-in of Actel O.25um FPGA\222s\224 MAPLD 2002  B IOGRAPHY   John worked 2 years at Faichild R&D on bipolar switching performance specifically platinum dopedlife time control and the development of Ion Implantation.  He worked 15 years at Intel developing Intel's first bipolar PROM, Ion Implantation, the world's first 16K DRAM, as well as 64K and 256K DRAMs.  Mr. McCollum developed Intel's first dual layer metal CMOS technology for the 386 microprocessor.  He co-founded Actel and worked the last 20 years on process, antifuse and flash cell development and FPGA Architecture at Actel.  He holds over 50 patents   covering Process Technology, Antifuse and NVM technology, FPGA Architecture, Analog Processing and Radiation Hardening.  He has presented numerous papers at IEDM, MAPLD, CSME, SPWG, and the FPGA Symposium. He is currently a Fellow in the Technology Development Department 


Time Time 50 350   10 0                   10 1                   10 2 12.5 50 350   10 0                   10 1  12 expected from Figure 7, the width of the uncertainty region is compressed by the curvature of the monopulse response resulting in a detection-primitive with greater uncertainty than the variance admits.  A filter lag or so-called cluster tracking can easily result in a 5% or greater offset and degraded consistency.  After 300 seconds the curves peak up because the target is appr oaching a low-elevation beampoint limit.  This occurs anytime a target is tracked into the edge of the radar\222s field of re gard and can lead to radar-toradar handover difficulty         300 300 80  s D 2 k,1 k y    s D 2 k,1 k y   100 150 200 250 10 1                    10 5 1 0  Figure 8 - Consistency versus distance from beam center Monopulse Mismatch The next set of curves plotted in Figure 9 show the sensitivity of detection-primitive consistency to a mismatch in the monopulse slope.  All of these curves were generated using a linear monopulse response derived from the slope of the true monopulse response at beam center.  The slope of the 80% curve is 0.8 times th e beam-center slope; the 90 curve is 0.9 times the beam-center slope; and so on for 100%, 110% and 120%.  Again, the order of curves in the graph is the same as the legend order A steep slope tends to expand y I 222s uncertainty while a gentle slope tends to compress it.  An expanded uncertainty leads to a smaller consistency while a compressed uncertainty leads to a larger consistency.  This behavior can be observed in the family of curves in Figure 9.  Curves for the steeper slopes are on the botto m while curves for more gentle slopes are on top.  The notable feature of this set of curves is that the sensitivity to a mismatch in the monopulse slope is not very significant       100 150 200 250 10 1                    90 100 110 120  Figure 9 - Consistency versus monopulse mismatch Range-Bias Error The complex nature of the monopulse radar models presents ample opportunity to introduce errors in the software implementation.  One such e rror introduced in a \275 rangecell-width bias in the detection-primitive range which in turn resulted in a significant degradation to 2 9 k D The fact that 2 9 k D is measured in different coordinates compared to the bias made it difficult to determine which value or algorithm was to blame.  Examining the intermediate consistency values led directly to the error source A comparison between biased 2 1 k D  2 2 k D and 2 3 k D values and unbiased 2 2 k D values is shown in Figure 10.  The unbiased 2 2 k D is the bottom-most curve and the biased 2 3 k D is the top-most curve with a value around 80.  This large value for 2 3 k D indicates that there is a lot more uncertainty in the range measur ements compared to what is predicted by the range varian ce.  Since the range-variance calculation is easy to confirm, the problem must be in the algorithms that model or manipulate range A notable feature of Figure 10 is the sensitivity of the centroiding algorithm to range bias in the detection primitives.  The range bias is ba rely noticeable in the biased 2 1 k D and 2 2 k D curves.  Of course, if the unbiased 2 2 k D  curve existed as a baseline it would be relatively easy to spot the error 


Time Time Time 50 350   10 0                   10 1                   10 2 50 350   10 1                   10 0                   10 1 50 350   10 0                   10 1  13         Isolated No SNR Adjust  Figure 11 - Centroiding for isolated range cells Filter Tuning Now that the centroided m easurements are reasonably consistent, the parameters that govern track filtering can be examined.  As previously promised, the effects and corrections for atmospheric refr action and sensor bias have been disabled so that 2 8 k D can be analyzed using a sliding window.  Of course the full analysis would include these effects and 2 8 k D at each time step would be collected and averaged over many trials Plots of the effect of changing process noise in a nearlyconstant-velocity filter are shown in Figure 12 and Figure 13 for Cartesian position and velocity respectively.  In both figures, the plotted values have been divided by 3 so that the desired value is always 1.  Increasing the process noise up to a point should increase the updated uncertainty and reduce 2 8 k D values.  Except near th e end of the trajectory when the measurements are off of beam center, the curves in Figure 12 and Figure 13 appear inconclusive for this expected trend If 2 8 k D values are way out of range there are additional intermediate filter values that can be examined.  For example, the state extrapolati on algorithms can be examined by comparing the consistency of 1 210 Isolated With SNR Adjust 300 300 300 0.005 212 212 212 212 212 kkkk T kkkk D xhzSxhz 35        s D 2 k Range   D 2 k,2 biased D 2 k,1 biased D 2 k,2  Figure 10 - Range bias error in detection primitive Centroiding Algorithm From Section 3, assuming that the centroided-range uncertainty for an isolated range cell is the same as its detection-primitive uncertainty may be incorrect Collecting and plotting 2 3 k D values only from isolated range-cell measurements can be used to analyze such assumptions.  The plots in Figure 11 compare differences between the isolated-cell algorithm defined in Section 3, an algorithm that modifies the uncertainty based on the SNR in the isolated cell, and the 2 3 k D values from all measurements 34\was used to modify the range uncertainty for the upper line labeled Isolated with SNR Adjust    4 22  2 2  resRi o R R Rn bdp bm  s D 2 k,3 Range    s D 2 k,8 Position     212 1 can also be examined using \(35 The residual is also commonly used to determine the assignment cost  212 kk z  P  k  k1 with z k The consistency of the innovation covariance k T kkkkk RHPHS 100 150 200 250 10 1                    D 2 k,3 biased 100 150 200 250 10 2                    100 150 200 250 10 1                    0.5 50  Figure 12 \226 Position consistency, filter tuning example  r  t t 34 If the All Centroided curve \(middle\as the baseline doing nothing \(lower\imates the uncertainty and 33\imates the uncertainty.  Dividing by the square root of the observed SNR leads to a more consistent covariance; however, there is currently no statistical evaluation to justify it             210 210 1 1 1 2 All Centroided 


Time 50 350   10 1                   10 0                   10 1  14         300 0.005  s D 2 k,8 Velocity   100 150 200 250 10 2                    0.5 50  Figure 13 \226 Position consistency, filter tuning example 5  C ONCLUSION  Calculating and observing the behavior of covariance consistency at different levels  in the radar signal processing chain represents a very powerfu l tool that can be used to assess the accuracy and softwa re implementation of radar signal-processing algorithms.  Analyzing covariance consistency is applicable to radar systems both in the field and in simulations.  The primary challenge in both arenas comes down to properly accounting for the true target states that contribute to detections, detection primitives measurements, and state estimates For a fielded radar syst em, achieving covariance consistency is usually a s econdary consideration behind achieving and maintaining track s.  Indeed, until recently radar specifications did not even include requirements for covariance consistency.  Recent covariance consistency requirements stem from the fact that the use of radar systems in sensor netting applications is on the rise Currently the combined e ffects of off-beam-center measurements, atmospheric correction, bias correction clustering and centrioding, data association, and filtering on state covariance consistency throughout a target\222s trajectory are not well known.  This is particularly true for radars using wideband waveforms and multiple hypotheses or multiple frame trackers.  Numerical results presented here indicate that algorithms early in the radar signal processing chain can significantly degrad e covariance consistency and that some errors are better tolerated than others For a simulated target in a modeled system, truth relative to some global reference is known. However, transforming truth through different refere nce frames and accounting for changes that occur during various radar processing algorithms is not as simple as it appears.  The techniques in this paper help expose this hidden complexity and provide a framework for discussing and expanding the future development of covariance consistency techniques.  Such future developments include issues related to mapping truth through the convolution operation typically used to simulate wideband signal processing and the fast Fourier transforms typically used in pulse-Doppler processing.  Sophisticated tracking algorithms that carry multiple hypotheses, associate across multiple frames, or weight the association of multiple targets within a single frame pose significant challenges in properly associating truth with state estimates.  Additional work, including an investigation of track-to-truth assignment, is needed before covariance consistency techniques can be applied to these algorithms Another area that needs furthe r analysis is the use of a sliding window to approximate the covariance behavior expected during a set of Monte-Carlo trials.  Various timedependent variables such as the target\222s range and orientation, the transmit waveform, the radar\222s antenna patterns toward the target, missed detections, and false alarms could easily viol ate the assumption that measurement conditions are nearly stationary over the time of the window.  It is importa nt to understand the conditions when this assumption is violated Finally, the examples presented here included a relatively benign arrangement of targets.  Further analysis in dense target environments with the related increase in merged detections, merged measurements, and impure tracks is needed.  Further analysis for targets traveling over different trajectories is also needed Even so, the techniques presented here can be extended to many of these analyses R EFERENCES  1  S. Blackman and R. Popoli Design and Analysis of Modern Tracking Systems Artech House, 1999 2  Y. Bar-Shalom and X. R. Li Multitarget-Multisensor Tracking: Principles and  Techniques YBS Publishing, Storrs, CT, 1995 3  Y. Bar-Shalom, Editor Multi-target-Multi-sensor Tracking: Advanced Applications and  Vol. I Artech House, Norwood, MA, 1990 4  D. B. Reid, \223An Algorithm for Tracking Multiple Targets,\224 IEEE Trans. on Automatic Control Vol. 24 pp. 843-854, December 1979 5  T. Kurien, \223Issues in the Design of Practical Multitarget Tracking Algorithms,\224 in Multitarget-Multisensor Tracking Y. Bar-Shalom \(ed.\43-83, Artech House, 1990 6  R.P.S. Mahler, Statistical Multisource-Multitarget Information Fusion, Artech House, 2007 


 15 7  B.-N. Vo and W.-K. Ma, \223The Gaussian Mixture Probability Hypothesis Density Filter,\224 IEEE Trans Signal Processing Vol. 54, pp. 4091-4104, November 2006 8  B. Ristic, S. Arulampalam, and N. Gordon Beyond the Kalman Filter Artech House, 2004 9  Y. Bar-Shalom, X. Rong Li, and T. Kirubarajan Estimation with Applications to Tracking and Navigation, New York: John Wiley & Sons, pg. 166 2001 10  X. R. Li, Z. Zhao, and V. P. Jilkov, \223Estimator\222s Credibility and Its Measures,\224 Proc. IFAC 15th World Congress Barcelona, Spain, July 2002 11  M. Mallick and S. Arulampalam, \223Comparison of Nonlinear Filtering Algorithms in Ground Moving Target Indicator \(GMTI Proc Signal and Data Processing of Small Targets San Diego, CA, August 4-7, 2003 12  M. Skolnik, Radar Handbook, New York: McGrawHill, 1990 13  A. Gelb, Editor Applied Optimal Estimation The MIT Press, 1974 14  B. D. O. Anderson and J. B. Moore Optimal Filtering  Prentice Hall, 1979 15  A. B. Poore, \223Multidimensional assignment formulation of data ass ociation problems arising from multitarget and multisensor tracking,\224 Computational Optimization and Applications Vol. 3, pp. 27\22657 1994 16  A. B. Poore and R. Robertson, \223A New multidimensional data association algorithm for multisensor-multitarget tracking,\224 Proc. SPIE, Signal and Data Processing of Small Targets Vol. 2561,  p 448-459, Oliver E. Drummond; Ed., Sep. 1995 17  K. R. Pattipati, T. Kirubarajan, and R. L. Popp, \223Survey of assignment techniques for multitarget tracking,\224 Proc  on Workshop on Estimation  Tracking, and Fusion: A Tribute to Yaakov Bar-Shalom Monterey CA, May 17, 2001 18  P. Burns, W.D. Blair, \223Multiple Hypothesis Tracker in the BMD Benchmark Simulation,\224 Proceedings of the 2004 Multitarget Tracking ONR Workshop, June 2004 19  H. Hotelling, \223The generalization of Student's ratio,\224 Ann. Math. Statist., Vol. 2, pp 360\226378, 1931 20  Blair, W. D., and Brandt-Pearce, M., \223Monopulse DOA Estimation for Two Unresolved Rayleigh Targets,\224 IEEE Transactions Aerospace Electronic Systems  Vol. AES-37, No. 2, April 2001, pp. 452-469 21  H. A. P.  Blom, and Y. Bar-Shalom, The Interacting Multiple Model algorithm for systems with Markovian switching coefficients IEEE Transactions on Au tomatic Control 33\(8  780-783, August, 1988 22  M. Kendall, A. Stuart, and J. K. Ord, The Advanced Theory of Statistics, Vol. 3, 4th Edition, New York Macmillan Publishing, pg. 290, 1983 23  T.M. Cover and P.E. Hart, Nearest Neighbor Pattern Classification, IEEE Trans. on Inf. Theory, Volume IT-13\(1 24  C.D. Papanicolopoulos, W.D. Blair, D.L. Sherman, M Brandt-Pearce, Use of a Rician Distribution for Modeling Aspect-Dependent RCS Amplitude and Scintillation Proc. IEEE Radar Conf 2007 25  W.D. Blair and M. Brandt-Pearce, Detection of multiple unresolved Rayleigh targets using quadrature monopulse measurements, Proc. 28th IEEE SSST March 1996, pp. 285-289 26  W.D. Blair and M. Brandt-Pearce, Monopulse Processing For Tracking Unresolved Targets NSWCDD/TR-97/167, Sept., 1997 27  W.D. Blair and M. Brandt-Pearce, Statistical Description of Monopulse Parameters for Tracking Rayleigh Targets  IEEE AES Transactions, Vol. 34 Issue 2,  April 1998, pp. 597-611 28  Jonker and Volgenant, A Shortest Augmenting Path Algorithm for Dense and Sparse Linear Assignment Problems, Computing, Vol. 38, 1987, pp. 325-340 29  V. Jain, L.M. Ehrman, and W.D. Blair, Estimating the DOA mean and variance of o ff-boresight targets using monopulse radar, IEEE Thirty-Eighth SSST Proceedings, 5-7 March 2006, pp. 85-88 30  Y. Bar-Shalom, T. Kirubarajan, and C. Gokberk 223Tracking with Classification-Aided Multiframe Data Association,\224 IEEE Trans. on Aerospace and Electronics Systems Vol. 41, pp. 868-878, July, 2005   


 16 B IOGRAPHY  Andy Register earned BS, MS, and Ph  D. degrees in Electrical Engineering from the Georgia Institute of Technology.  His doctoral research emphasized the simulation and realtime control of nonminimum phase mechanical systems.  Dr. Register has approximately 20 years of experience in R&D with his current employer, Georgia Tech, and product development at two early-phase startups. Dr. Register\222s work has been published in journals and conf erence proceedings relative to mechanical vibration, robotics, computer architecture programming techniques, and radar tracking.  More recently Dr. Register has b een developing advanced radar tracking algorithms and a software architecture for the MATLAB target-tracking benchmark.  This work led to the 2007 publication of his first book, \223A Guide to MATLAB Object Oriented Programming.\224  Mahendra Mallick is a Principal Research Scientist at the Georgia Tech Research Institute \(GTRI\. He has over 27 years of professional experience with employments at GTRI \(2008present\, Science Applications International Corporation \(SAIC Chief Scientist \(2007-2008\, Toyon Research Corporation, Chief Scientist 2005-2007\, Lockheed Martin ORINCON, Chief Scientist 2003-2005\, ALPHATECH Inc., Senior Research Scientist 1996-2002\, TASC, Principal MTS \(1985-96\, and Computer Sciences Corporation, MTS \(1981-85 Currently, he is working on multi-sensor and multi-target tracking and classification bas ed on multiple-hypothesis tracking, track-to-track association and fusion, distributed filtering and tracking, advanced nonlinear filtering algorithms, and track-before-detect \(TBD\ algorithms He received a Ph.D. degree in  Quantum Solid State Theory from the State University of New York at Albany in 1981 His graduate research was also based on Quantum Chemistry and Quantum Biophysics of large biological molecules. In 1987, he received an MS degree in Computer Science from the John Hopkins University He is a senior member of the IEEE and Associate Editor-inchief  of the Journal of Advances in Information Fusion of the International Society of Information Fusion \(ISIF\. He has organized and chaired special and regular sessions on target tracking and classific ation at the 2002, 2003, 2004 2006, 2007, and 2008 ISIF conferences. He was the chair of the International Program Committee and an invited speaker at the International Colloquium on Information Fusion \(ICIF '2007\, Xi\222an, China. He is a reviewer for the IEEE Transactions on Aerospa ce and Electronics Systems IEEE Transactions on Signal Pr ocessing, International Society of Information Fusion, IEEE Conference on Decision and Control, IEEE Radar Conference, IEEE Transactions on Systems, Man and Cybernetics, American Control Conference, European Signal Processing Journal and International Colloquium on Information Fusion ICIF '2007   William Dale Blair is a Principal Research Engineer at the Georgia Tech Research Institute in Atlanta, GA. He received the BS and MS degrees in electrical engineering from Tennessee Technological University in 1985 and 1987, and the Ph.D. degree in electrical engineering from the University of Virginia in 1998. From 1987 to 1990, he was with the Naval System Division of FMC Corporation in Dahlgren, Virginia. From 1990 to 1997, Dr Blair was with the Naval Surface Warfare Center, Dahlgren Division NSWCDD\ in Dahlgren, Virg inia. At NSWCDD, Dr Blair directed a real-time experiment that demonstrated that modern tracking algorithms can be used to improve the efficiency of phased array radars. Dr Blair is internationally recognized for conceptualizing and developing benchmarks for co mparison and evaluation of target tracking algorithms Dr Blair developed NSWC Tracking Benchmarks I and II and originated ONR/NSWC Tracking Benchmarks III and IV NSWC Tracking Benchmark II has been used in the United Kingdom France, Italy, and throughout the United States, and the results of the benchmark have been presented in numerous conference and journal articles. He joined the Georgia Institute of Technology as a Se nior Research Engineer in 1997 and was promoted to Principal Research Engineer in 2000. Dr Blair is co-editor of the Multitarg et-Multisensor Tracking: Applications and Advances III. He has coauthored 22 refereed journal articles, 16 refereed conference papers, 67 papers and reports, and two book chapters. Dr Blair's research interest include radar signal processing and control, resource allocation for multifunction radars, multisen sor resource allocation tracking maneuvering targets and multisensor integration and data fusion. His research at the University of Virginia involved monopulse tracking of unresolved targets. Dr Blair is the developer and coordinator of the short course Target Tracking in Sensor Systems for the Distance Learning and Professional Education Departmen t at the Georgia Institute of Technology. Recognition of Dr Blair as a technical expert has lead to his election to Fellow of the IEEE, his selection as the 2001 IEEE Y oung Radar Engineer of the Year, appointments of Editor for Radar Systems, Editor-InChief of the IEEE Transactions on Aerospace and Electronic Systems \(AES\, and Editor-in- Chief of the Journal for Advances in Information Fusion, and election to the Board of Governors of the IEEE AES Society,19982003, 2005-2007, and Board of Directors of the International Society of Information Fusion   


 17 Chris Burton received an Associate degree in electronic systems technology from the Community College of the Air force in 1984 and a BS in Electrical Engineering Technology from Northeastern University in 1983.  Prior to coming to the Georgia Institute of Technology \(GTRI\ in 2003, Chris was a BMEWS Radar hardware manager for the US Air Force and at MITRE and Xontech he was responsible for radar performance analysis of PAVE PAWS, BMEWS and PARCS UHF radar systems Chris is an accomplished radar-systems analyst familiar with all hardware and software aspects of missile-tracking radar systems with special expertise related to radar cueing/acquisition/tracking for ballistic missile defense ionospheric effects on UHF radar calibration and track accuracy, radar-to-radar handover, and the effects of enhanced PRF on radar tracking accuracy.  At GTRI, Chris is responsible for detailed analysis of ground-test and flight-test data and can be credited with improving radar calibration, energy management, track management, and atmospheric-effects compensation of Ballistic Missile Defense System radars   Paul D. Burns received his Bachelor of Science and Masters of Science in Electrical Engineering at Auburn University in 1992 and 1995 respectively. His Master\222s thesis research explored the utilization of cyclostationary statistics for performing phased array blind adaptive beamforming From 1995 to 2000 he was employed at Dynetics, Inc where he performed research and analysis in a wide variety of military radar applications, from air-to-air and air-toground pulse Doppler radar to large-scale, high power aperture ground based phased array radar, including in electronic attack and protection measures. Subsequently, he spent 3 years at MagnaCom, Inc, where he engaged in ballistic missile defense system simulation development and system-level studies for the Ground-based Midcourse defense \(GMD\ system. He joined GTRI in 2003, where he has performed target tracking algorithm research for BMD radar and supplied expertise in radar signal and data processing for the Missile Defense Agency and the Navy Integrated Warfare Systems 2.0 office.  Mr. Burns has written a number of papers in spatio-temporal signal processing, sensor registration and target tracking, and is currently pursuing a Ph.D. at the Georgia Institute of Technology  


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


