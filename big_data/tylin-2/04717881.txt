The Cost-Constrained Web Replica Placement Design in Computer Networks \226 an Approximate Approach  Marcin Markowski, Andrzej Kasprzak Wroc aw University of Technology Chair of Systems and Computer Networks Wybrzeze Wyspianskiego 27, 50-370 Wroclaw, Poland   Abstract 227 Paper deals with the resource replication problem which often must be solved in wide area network design process It is important especially for big wide area networks, in which huge amount of data is exchanged between users and servers Then replication of resources \(for example servers\may prevent decreasing the quality of service in the network. Connecting 
replica to a node of the network is connected with some connecting cost. In practice, the connecting costs are often limited. Then, in the paper, an approximate algorithm for simultaneous server\222s replication, capacity and flow assignment problem with limited cost of connecting replica to the nodes of WAN is presented. Important and useful part of the paper are the results of computational experiments performed with the proposed algorithm. Important properties of the considered problem have been described. They may be helpful for planning and optimizing of WAN. Analysis of quality of the approximate solutions is also reported in the paper Keywords \227 network capacity planning, quality of service 
resource allocation, CFA problem, packet switched networks I   I NTRODUCTION  Wide area computer networks are more and more important and useful tools in all kind of business, marketing and communication. A lot of industries decide to build their own wide area networks to ensure a reliable and fast data exchanging. Fast increase of the network users, information traffic\ sent through the network and the number of connected data storages \(web servers, databases\ implicates the necessary of perpetual expanding of the network. The updating process and the designing process need special optimization algorithms 
to ensure the quality of service in the network and minimize the building costs In practice, a major part of all traffic in the network is data exchanged between servers and users connected to the nodes Badly allocated servers could decrease the quality of service of the network causing delays and lost of data. Some proposals of solving the server\222s allocation problem can be found in the literature [1, 2  Sin c e a l o t  o f u s e r s  c o n n ect e d  to  d i f f e r e n t  nodes of WAN, are downloading a huge amount of data from a server, it is beneficially to replicate the server in the network Then, we can allocate replicas of server in different parts of the network, so the flow rates on channels decrease. The bigger 
number of replicas we allocate in the network, the smaller delay \(or the bigger quality of service\e can obtain From the other side, adding new replicas is connected with certain costs. They are costs of server hardware, costs of node upgrade \(for example upgrade of router in the node\ costs of service and the other. The connecting costs are often different for different nodes of WAN. The total connecting cost connecting cost of all replicas at nodes\is usually limited Then, very important is to find the solution for replication problem with connecting cost constrain. Some algorithms for the resource replication problem can be found in  I n t h e  pape h er e i s  a n  a ppro x i m a t e  a l gor i t hm fo r rep l i c a 
allocation problem without constraint on maximal feasible replica connecting cost. Problems with connecting cost constrain have not been considered yet We consider the replication problem with connecting constrain together with the capacity and flow assignment CFA\ problem. It makes our problem much more general than problems known in the literature \(solutions for the classical CFA problem are proposed in [6, 7, 8, 9  T h e goa l i s  t o  simultaneously optimize the routing, the capacities of channels and the allocation of servers replicas in the network subject to capacity budget \(total capacity leasing cost\onstraint and connecting budget \(total connecti ng cost on replicas at nodes 
constraint. Optimization criterion is the linear combination of the total average delay per packet \(given by Kleinrock\222s formula in  a nd t h e t o t a l co nnec t i ng c o s t  o f ser v er s a t  nodes. We denote that the maximal capacity leasing cost is limited and the maximal feasible cost of connecting replicas at nodes is limited. We present an approximate algorithm obtaining the near-optimal solution. We consider a discrete cost-capacity function, most important from the practical point of view. Channels capacities can be chosen from the sequence defined by ITU-T recommendations. Such formulated problem is NP-complete. In the literature such formulated problem has not been considered yet II  P 
ROBLEM F ORMULATION   We use assumptions presented in [5  L et     r r Y X be the selection, where r X determines the capacities of channels and r Y determines the replicas allocation at the nodes of WAN Let R be the family of all possible selections. Let r kh Y y 
002 be the decision binary variable for k th server allocation equal to 


one if the replica of k th server is connected to node h and equal to zero otherwise Let  r Y U be the connecting cost of all replicas at nodes. Let    r r Y X T be the minimal average delay per packet in the wide area network    r r Y X T is given with the Kleinrock’s formula and can be obtained solving a multicommodity flow problem in the network [6, 9, 10   T h e  multicommodity flow problem can be solved with efficient FD method [9  I f  t h e f o r m ul a t ed abo v e  pr ob l e m ha s no s o l u t i o n  then we perform      r r Y X T  Let        r r r r r Y U Y X T Y X Q     be the criterion function    0   0,1\nd  1  t B be the capacity budget \(maximal feasible leasing capacity cost\, and let BU be the connecting budget \(maximal feasible connecting cost of replicas at nodes\. The considered replica allocation channel capacity and flow assignment problem in WAN with replica allocation cost constraint may be formulated as follows   r r Y X Y X Q r r  min    1 subject to  R Y X r r    2  B X d r   3  BU Y U r   4 III  C ONCEPTION OF AN A LGORITHM  The calculation scheme of an approximate algorithm is following. In first step we solve the considered problem without constraint \(4\, that means we solve the problem \(13 in the network. Then, in consecutive iterations we reduce the connecting cost of replicas at nodes, until the constraint \(4\s satisfied in the network A  Solving CFA Problem Since users connected to one node can use different replicas of one server, then solving of the capacity and flow assignment \(CFA\ problem for the network is very difficult because we do not exactly know the traffic requirements between nodes i.e. we do not know unique destinations Then, to simply solve the capacity and flow assignment problem in the network we create the network model in the following way. We add to the network K artificial nodes numbered from n 1 to n  K Node n  k corresponds to the k th server. Moreover we add to the network new artificial channels n+k  h such that k M h  and 1  kh y for   1 K k  The capacities of the artificial channels are equal to infinity. We assume, that k th server is connected to the node n+k  In the described above network model we can easily assign the traffic requirements between nodes in the network. Let hk a  be the average packet rate transmitted from users connected to node h to k th server, and let  kh a be the average packet rate transmitted from k th server to all users connected to node h  Moreover let gh r be the average packet rate transmitted from users connected to node g to users connected to node h Then let gh r be the average packet rate transmitted from node g to node h                  n g,h h g n h K n g n a K n h n n g a n g,h r r h n g n h g gh gh for  and  for     for   for  for  0 1 1       For such constructed network we can solve the CFA problem. The calculation skills for the CFA problem can be found in the literature [9, 10   B  Constrained Replica Placement The main problem of the presented approximate algorithm is the way of reducing the connecting cost of replicas at the nodes of WAN. If the replicas are allocated close to each other \(are concentrated in a certain part of the network\, then there are long distances between nodes in the network and the replicas. It causes high delay in the network. It is beneficial to allocate replicas uniformly in the network in order to obtain the minimal packet rate and the minimal total average delay. Then, we propose the criterion for replica reallocation. Criterion takes into account not only the connecting cost difference but also the total average delay difference Let gh v be the distance, in hops, between node g and node h It means that gh v is the minimal number of channels between nodes g and h Let kg V be the distance between node g and all replicas of k th server   k M h gh kh kg v y V  Choosing the node for reallocating replica of k th server we should choose such nodes g for which the value of kg V  is maximal Let one of the replica of k th server is allocated in node g Let k gl  be the criterion for reconnecting the replica of k th server from node g to node l   kl kl kg k gl V a a       5 We will reconnect replicas to those nodes, for which the value of criterion \(5\ is maximal 


IV  C ALCULATION S CHEME OF AN A LGORITHM  A  Step 1. Calculating    1 1 Y X Solve the problem \(1  3\n the network. In order to solve the problem without constraint \(4\e can use the method presented in [5 o r w e can  m odi fy t h e n e t w ork  m odel  i n  the way presented in III.A\ and use one of the algorithms presented in [7, 8; 9  If th e p r o b lem h a s n o so lu ti o n th en  stop – the problem \(1  4\s no solution. Otherwise perform 2  r and go to step 2 B  Step 2.  Calculating the replica allocation r Y Find such k th server and such nodes k M l g  that 1  kg y 0  kl y  kg kl a a  and for which the value of k gl   is maximal. Perform 0  kg y and 1  kl y If such nodes do not exist then stop – if BU Y U r    then an approximate solution   r r Y X of the problem \(1  4\ is found. Otherwise the problem \(1  4\ has no solution C  Step 3.  Calculating channel capacities r X Modify the network, according to the way described in chapter III.A. Solve the CFA problem in the network where the replica allocation is given by  r Y To solve the CFA problem we can use either an approximate algorithm or an exact algorithm presented in [7  P e r f or m  1   r r and go to step 2 V  C OMPUTATIONAL R ESULTS  In the Fig. 1 the typically dependence of Q on the connecting budget of replicas at nodes BU for different values of budget B is presented  0,0190 0,0220 0,0250 0,0280 125 135 145 155 BU Q B = 75 000 B = 85 000 B = 95 000  Figure 1. Typical dependence of the optimal value of Q on connecting budget BU  Conclusion 1 The dependence of Q on BU is decreasing function It follows from the Fig. 1. that there exist such connecting budget BU  that the problem \(1  4\ has the same solution for each budget BU greater or equal to BU  It means that the optimal solution of the problem \(1  4\ on the connecting budget constraint \(4\or BU  BU  and it is inside the set of feasible solution for BU  BU    In the paper we consider two kinds of cost – the regular capacity cost \(cost of leasing of all channels capacities, borne regularly i.e. once a year\d the connecting cost \(borne once when the network is built\ Taking into account, that the regular costs must be borne for years, sometimes it is beneficially to increase the disposable connecting cost in order to decrease the accumulated capacity cost. Then, the presented above computational results can be very helpful in making such decisions. In example, if we want to obtain the network with the quality of service Q 0,022 \(according to Fig. 1.\e can denote B 95000 [$/y d BU 125000 or w e  can  denote B 85000 [$/y d  BU 145000 t h e n e t w ork  should be utilized for few years, the second proposal is more economical The typical dependence of Q on the capacity budget B is presented in the Fig. 2. It follows from experiments, that the value of B has the critical impact on the value of criterion Q  Moreover, it is observed for each testified network, that there exist such capacity budget B  that the dependence of Q on B is very weak for each budget B greater or equal to B   0,0185 0,0235 0,0285 0,0335 62000 72000 82000 92000 102000 B Q BU = 125 000 BU = 135 000 BU = 145 000  Figure 2. Typical dependence of the optimal value of Q on capacity budget B  Conclusion 2 The dependence of Q on B is decreasing function and it may be approximated by the function belonging to class  3 2 1       B Q where 3 2 1      are real coefficients The results of computational experiments shows that the value of capacity budget B is more critical for the quality of service \(indicated by Q in the network, than the value of connecting budget BU  Knowing the dependence of Q on B and of Q on BU  and knowing the desirable value of Q network designer can choose the best values of B and BU in order to optimize the total network cost, combined from disposable connecting building\ cost and accumulated capacity leasing cost The impact of the values of coefficients  and  on the solution of the problem \(1  4\ has been also examined. The typical dependence of the value of total connecting cost U  


obtained by solving the problem \(1  4\ in the network, on the coefficients ratio    for different capacity budget B is presented in the Fig. 3  125,000 128,000 131,000 134,000 137,000 0 10 000 20 000 30 000  U B = 70 000 B = 80 000 B = 90 000  Figure 3. Typical dependence of the optimal value of U  on coefficients ratio   Conclusion 3 There exist such division of value of     that dependence of U on    is increasing function. For huge values of    the total connecting cost U becomes constant To test the quality of our algorithm, solutions of the problem \(1  4\, obtained with presented algorithm, have been compared with optimal solutions. Let Q  denotes the distance between heuristic and optimal solutions. Let    100   012     solutions all  of number Q for which  solutions  of number b a b a  The dependence   b a  on divisions [0%-1%\[1%-5 5%-15%\, etc. is shown in the Fig 4. As it is shown in the Fig. 4, about 40% approximate solutions differ from optimal solutions at most 1% and about 75% approximate solutions differ from optimal solutions at most 5 VI  C ONCLUSION  It follows from computational experiments that approximate solutions are not much worse than optimal solutions. Moreover, the approximate solutions may be obtained in shorter computation time than optimal solutions The presented approximate algorithm may be used whenever an optimal solution is not necessary or for very big wide area networks \(over 50 nodes\, when it takes too many time to obtain the optimal solution. Proposed criterion function allows to decide which of the indicators \(quality of service or   0 10 20 30 40 0-1 1 5   5 1 5   1 5 3 0  3 0  Q   Figure 4. The distance between approximate and optimal solutions connecting cost\ is more important. The network support cost and the building \(connecting\ cost may be limited according to demands. Some properties of considered problem have been discovered and some conclusions has been formulated. The very important dependence between two kinds of cost and the criterion Q has been shown  it follows from experiments, that the network designer can denote the desirable value of criterion Q and decide, which kind of cost \(regular or disposable\ should be minimized. It allows to decrease the regular capacity cost usually born for years This work was supported by a research project of The Polish State Committee for Scientific Research in 20052007 R EFERENCES  1  M. A. Al-Fawzan, and F. Hoymany, “Placement of Network Servers on Wide-Area Network”, Computer Networks, vol. 34, 2000, pp. 355–361 2  K. Chari, “Resource Allocation and Capacity Assignment in Distributed Systems”, Computers Ops Res., Vol. 23, No. 11, 1996, pp. 1025-1041 3  P. Radoslavov, R. Govindan, and D. Estrin, “Topology-Informed Internet Replica Placement”, Computer Communications, Volume: 25 2002, pp. 384-392 4  L. Qiu, V. N. Padmanabhan, and G. M. Voelker, “On the Placement of Web Server Replicas”, Proceedings of 20th IEEE INFOCOM Anchorage, 2001, pp. 1587 –1596 5  M. Markowski, and A. Kasprzak, “An Approximate Algorithm for Replica Allocation Problem in Wide Area Networks”, Proc. of 12th GI/ITG MMB together with 3rd PGTS, VDE Verlag, Berlin, 2004, pp 161-166 6  L. Fratta, M. Gerla, and L. Kleinrock, “The Flow Deviation Method: an Approach to Store-and-Forward Communication Network Design Networks 3, 1973, pp. 97-133 7  A. Amiri, and H. Pirkul, “Routing and Capacity Assignment in Backbone Communication Networks”, Computers & Operations Research, vol. 24, 1997, pp. 275-287 8  G. K. Chung, and H. H. Tan, “Combined Channel Allocation and Routing Algorithms in Packed Switched Networks”, Computer Communications, vol. 20, no. 13, 1997, pp. 1175-1190 9  A. Kasprzak Topological Design of the Wide Area Networks Wroclaw University of Technology Press, Wroclaw, 2001   M. Pioro, and D. Medhi Routing, Flow, and Capacity Design in Communication and Computer Networks Elsevier, Morgan Kaufmann Publ., San Francisco, 2004  


  5 averaged, then it is treated in the same manner as the other variables, subjected to nearest neighbor method for regridding. Only valid data values are averaged together All the data have been filtere d according to how the level 3 AIRS data products are filtered AMSRE The AMSRE data comes already on a regular grid of 0.25x0.25 degrees.  The data products are 1  total water vapor 2  standard deviation of total water vapor 3  number of data points of total water vapor within each regridded box 4  cloud liquid water 5  standard deviation of cloud liquid water 6  number of data points of cloud liquid water within each regridded box 7  sea surface temperature 8  standard deviation of sea surface temperature 9  number of data points of sea surface temperature within each regridded box 10  rain rate 11  standard deviation of rain rate 12  number of data points of rain rate within each regridded box All of AMSRE data are aver aged within each regridded box. Only valid data values are averaged together to derive the data products with their standard deviation and number data points MLS The MLS data products are 1  water vapor profile 2  water vapor profile error estimate 3  total ice water content 4  profile ice water content 5  profile ice water content error estimate The MLS data products were converted to reflect measurements comparable to AIRS\222 standard levels.  For ice water content profile, it was converted to a layer quantity and summed up in order to get the total ice water content from pressure levels of 237.1378 mb \226 61.89647 mb.   The precision estimate was calculated according to the MLS recommended method.  The water vapor profile was converted from a level quantity to a layer quantity matching AIRS standard pressure layers.  The same technique was applied to derive the error estimate for water vapor profile CF-Metadata The generated merged product contains measured quantities from various A-Train instruments. But to maximize interoperability of data access, we adopted the climate and forecast \(CF\e conventions define what each da ta variable represents and its temporal and spatial properties. This enables a consistent set of comparable properties among different data sources and visualization tools that support the CF conventions. The CF conventions have been gaining acceptance by various projects and groups, including the NASA Earth Science Data Systems Worki whi c h have representations from the various major Earth science data centers 5  D ATA S ERVICES A RCHITECTURE  The merged NEWS Level 2 data being generated by Fetzer\222s NEWS project are uploaded to the NEWS Data Information Center \(NDIC r distribution to the NEWS science comm access limitations only allow SFTP access of the data on the NDIC server with no central data catalog. This mechanism precludes other programmatic forms of automated data access. Figure 3 below illustrates the manual approach and the redundant processing that results from independent client-side processing of the same set of data  Figure 3. Older limited access capabilities from SFTP 


  6 only access to NEWS merged Level 2 data. Illustrates the redundant and non-reusabl e processing of manual data access  We leveraged Service Oriented Architecture \(SOA technologies to avoid the redundant downloading and processing of the voluminous data sets by promoting serverside processing and client-sid e data access. Our services enable scientists to customize the conditional subsetting of voluminous NEWS Level 2 data and the production of Level 3 data using a customizable Level 3 Quantization data reduction technique. An implicit benefit of these services is the transparent data search and access mechanism that previously was done manually in a separate step. The services include a temporal and spatial query that searches for the merged Level 2 data that matches the user\222s given space-time constraints Enabling Distributed Exploratory Computing We understand that scientists routinely utilize their own trusted code and/or perform analysis in their own familiar working environments of choice. Rather than developing another web portal or new data access tool that forces the scientists to depart from their working environments, we developed service-oriented components that can be integrated into their own code environments \(figure 4 below\This facilitates the \223exploratory computing\224 that scientists are familiar with where they can remain fully in their analysis environment and serendipitously explore the data  Figure 4. Overview of th e distributed system for accessing and manipulating the merged NEWS data Multiple Web Service client s enable users to perform analysis directly within their familiar computing environments  By taking the proven Web Services-based approach, we ensure interoperability across different platforms and strengthen the interconnectedness and reuse of the Earth science data. This will not only promote actual use of service-oriented architectures, but also facilitate the streamlining of transparent access and manipulation of Earth science data from within scientists\222 own tools. For example, users can within th eir own Matlab analysis code call a function to get global-scale monthly averages of data in a given time period of water vapor. This seamlessly performs a remote query for all merged NEWS data on the server matching the space-time bounding box, and then averages by calendar month all water vapor data. The results are seamlessly downloaded to the user\222s machine from within the Matlab environment. The Matlab function call then returns with the data files that can immediately be used for analysis right within Matlab Data Storage Conventions The voluminous instrument and merged NEWS Level 2 data files that are stored on the server can potentially increase complexity in file access. Typically this can be addressed by indexing the files with a crawler that populates a searchable database. Earth sc ience data systems have also tried creating in-house crawlers that populate scalable database such as the open source MySQL and PostgresSQL Others have also tried using the open source Lucene search engine for fully i  Regardless of data files being index and cataloged additional methods can be applied to increase file access performance. We store data files following a consistent file and path-naming scheme where data files are located under directories for instrument names, year, month, and day From this approach, one can generate the full file path location of the data given just the data\222s type and timestamp. When reading the f iles for processing, no search is then required to find the corresponding set matching a temporal range 6  E NDPOINT S ERVER D EVELOPMENT  A set of proven SOA technologies have coalesced into the Web Services Protocol Stack. Built upon common web standards, Web Services can ensure interoperability in a distributed environment. Additionally, the proliferation of open source implementations adhering to these standards ensure that we do not get lock ed-in with a specific vendor\222s proprietary implementation. We utilized this set of proven protocols to create our distributed architecture for Level 2 and Level 3 data processing We also leveraged the proven Simple Object Access Protocol \(SOAP\ng platform-independent XML messages over a distributed network environment Using SOAP enables platform-independent Remote Procedure Calls \(RPC ws us to create interoperable remote method calls to the backend data processing of subsetting, clustering, and summarization. One major benefit of SOAP is its ability to work over various transport 


  7 protocols, though http/https is the most common and versatile With SOAP enabling the calling of remote methods, we also create Web Services Description Language \(WSDL\XML documents that define each of the exposed interfaces WSDL provides a programmatic way to define the public interface names, arguments, and re turned information that is independent of the underlying platform, programming language, and therefore native data types Collectively, this set of Web Services protocols enables us to develop an architecture that promotes reuse at the service level rather than code level. SOAs facilitate the exposing of legacy processes as reusable services. We first developed an averaging service of the merge NEWS Level 2 data Averages are easily understandable from the science community and are an immediately useful capability to provide. Then leveraging the same infrastructural developments, a subsetting service created, followed by an offline Level 3Q quantitative summary service We took a layered architecture approach where each layer is responsible for a well-defined role. Figure 5 below shows an example for the averaging service that takes as input the set of merged A-Train Level 2 data and produces globalscale averages of a specific time range  Figure 5. The layered architecture of the Web Service clients and Web Service endpoints  Web Service Endpoints To maximize interoperability between all of our multiplatform clients, we want to utilize more standards compliant implementations of Web Services. Sun\222s Java API for XML Web Services \(JAX-WS\has a reference implementation that supports JAX-WS 2.0/2.1 \(JSR 224 WS-I Basic Profile 1.1, WS-I Attachments Profile 1.0, WSI Simple SOAP Binding Profile 1.0, WS-Addressing 1.0 Core, SOAP Binding, and WSDL Bindi  With JAX-WS, we were able to quickly develop Web Services endpoints utilizing Java annotations that generate much of the boilerplate code. By marking Java code with metadata annotations, JAX-WS was able to generate both client and server code that performs much of the SOAP functionality. JAX-WS also includes JAXB that performs the datatype marshalling/unmarshalling to and from SOAP XML representations Processing Layer Naturally there may alread y exist tested and proven processing code that scientists want to make available as a service. We require the ability to wrap into a Web Service executables that were developed in IDL, Matlab, Python native C/C++, and Fortran. By executing this layer as a forked process, the Web Services layer is shielded from process and memory issues related to the processing code Unstable processing code should not bring down the Web Services as well We developed Java-based abstractions that exposed the generic interaction to IDL, Matlab, Python, and C. By defining a domain neutral interface to each of these platforms from Java, we were ab le to quickly integrate any domain-specific processing code into the Java environment 223Business Logic\224 Model Layer To reduce the complexity of interacting with the processing layer, we developed a \223business logic\224 model layer that defines an interface to a model representation of the processing. This allows us age of the model interface without having to know specifics of interacting with the lower-level process execution mechanism. An example of this benefit would be replacing a current IDL processing engine with a Matlab varian t, where the model interface would not need to be modified Invocation Sandboxes To avoid conflicts arising from near simultaneous service calls, it became clear that each service request must be given its own process environment and directory space within which to work.  Every new invocation of a Web Service creates a sandbox environment on the server that serves to house the working environment for that endpoint invocation.  Each sandbox is given a unique name that corresponds to the specific service being requested, the unique date/time timestamp, and a process id. This ensures that each sandbox is unique and avoids the possibility of process collision. The invocations sandboxes are automatically removed when a service is done. Any data that needs persistence are moved out of the sandbox to more permanent directory locations Long Processing Times Unlike commonplace Web Services such as those for getting stock quotes, the processing services presented here 


  8 often have long run times. Depending on the temporal and spatial range of data requested, processing wall-clock times may range from minutes to weeks. The notion of a service call changes from a quick \223getting\224 of a data product, to an 223ordering\224 of a product. The normal synchronous aspect of the services must then be changed to an asynchronous model Job Management Additionally with long-running services, there will inevitably be overlapping processing times on the server Therefore given the finite computing resources available overloading the computational resources with simultaneous service requests should be avoided. We require the ability to define the appropriate level of computational resources to be utilized and want any remaining service requests to be queued until sufficient computing resources are available This then maximizes processo r utilization of each service request Distributed resource management \(DRM\stems can be leveraged to manage the distribution of workload to available compute resources Each Web Service request would map to a job request on the processing server. DRMs can monitor the current state of all resources and assign the jobs to the best-suited resources Rather than directly interfacing with one specific DRM implementation, we leveraged the Open Grid Forum\222s Distributed Resource Management Application \(DRMAA pronounced \223drama\224\API for job submission, monitoring control, and retrieval of resu lts Thi s l a nguage and vendor agnostic API frees us from language specific implementations, as well as vendor specific implementations of job management, and allows us to focus on the abstract representation of resource management This API is supported by several different vendors of job scheduling implementations, including Sun Grid Engine SGE and Torque.  Though bot h Torque and SGE fi t  our requirement of an open source scheduler, Torque is still based on an older PBS implementation and does not offer the scalability and Java bindings that SGE does Asynchronous Services By default, a Web Service call is a synchronous call where a request is sent from the client to the server and blocks until a response is available to be sent back to the client \(figure 6\ut for long running processing routines, it is impractical to hold a network connection and block until the service has completed. A scientist on the client side may not be able to wait for the response after a completed longrunning job. We want to enable complete network disconnects where the client may be potentially shut down Additionally, service calls where seamless data access is intrinsic would require the gene rated data to be downloaded as well before the service call unblocks and completes. The client user would have to wait for both processing and downloading times There have been extensions to existing web services-based standards to augment with asynchronous capabilities. WSNotification, a standards-base d approach to enable eventdriven capabilities to Web Se rvices using the publishsubscribe pattern. OASIS\222 Web Services Notification WSN\ses this notification pattern to allow subscribing to a Web Service\222s event information and be notified of such information. OGC\222s Web Pr ocessing Service \(WPS spatially referenced data have been augmented with  OGC\222s W e b Notification Service \(WNS\so provides messaged-based notifications between services. By using client-side modules, each web service client can receive notifications without resorting to polling. However, all of these approaches require a richer set of clients that may only be available in Web Service-rich language platforms such as Java. Our goal requires that our asynchronous solution work in more simpler and less rich platforms such as C and IDL What we desire is a simpler approach that minimizes clientside requirements of running any form of a messaging server. Preferably, the solution should be simple enough to work with clients that have a minimal capable of doing HTTP GET, such as a basic REST service call  Figure 6. Sequence diagram showing potentially long blocking calls for long processing times from Web Services Asynchronous Services using Web Services A more desirable model of behavior would be to use Asynchronous Web Services \(figure 7\he one synchronous blocking call is par titioned into smaller atomic Web Service calls. The Apache Axis2 project and Sun\222s JAX-WS Reference Implementation provide a set of asynchronous atomic calls that allow a service client to submit a service, check if it is done, and then get the results 


  9 when it is done. However, even this model does not fit the desired model of behavior  Figure 7. Sequence diagram showing the push and pull methods from Asynchronous Web Services  Asynchronous Web Services can use either a Synchronous or Asynchronous MEP \(Message Exchange Protocol transmitting and receiving protoc ol messages.  There are two methods supported for both synchronous and asynchronous MEP types:  polling and callback. Both support non-blocking client side behavior The polling method represents the \223pull\224 model of processing where the client determines when the response is received.  The callback method has the client passing a callback handler to the Web Service Endpoint. This callback is essentially an endpoint on the client side running in a separate thread that waits for the server to respond.  The callback method represents the \223push\224 model of processing where the server determin es the notification For polling-based asynchronous MEP, we find that the client still needs to be active while it polls. That is, a service submission would normally return a response object that is used to check if the job is done. That same response object is also used later to retrieve the results when available. For long duration processing times, the client must still maintain the same response object. Though the response object may in principle be marshalled out for longer persistence implementation-specific network connection reliance be prove to be impractical. The preferred approach would be to move the role of asynchronous waiting from the Web Services request/response objects down further to the job scheduler The remaining option would be to use normal synchronous Web Service calls, but cleanly se parate the different atomic operations into individual synchronous Web Service calls These individual calls consists of submitting a job canceling a job, getting the status of the job \(running cancelled, etc\ting the progress of the job \(console output, etc\and finally getting the results of the job \(figure 8\ Each synchronous call is then delegated to an underlying job manager for the actual service job management. Collectively, the client is provided with an asynchronous service model For particularly long running service jobs, we find that users also want to see progress of the processing activity. In addition to getting the status of the job state \(such as queued, running, done, cancelled\so provide the capability to see progressive real-time output form the processing job. Our service endpoint manages a console buffer of a processing job\222s STDOUT/STDERR. When a Web Service call to get the progress of a particular job is invoked, the current buffer contents of the progress is returned and then flushed from the server-side buffer. This enables clients to build GUI applications on top of this asynchronous service and see the real-time console output of the server-side processing on their client GUI window Additionally, the console refresh rate, determined by the client, can be adjusted to an appropriate rate for that client\222s usage. Other approaches are possible including publishing the progress to a custom Atom feed. But here, the console content is only handled and retrieved when the client needs it  Figure 8. Sequence diagram showing atomic synchronous calls that implement job management capabilities of a service  Though callbacks \(registerable handlers\ally the preferred paradigm over polling, this client example demonstrates the utility of keeping it simple. We require maintaining a set of lightweight Web Service clients across multiple platforms. Not all platforms and Web Service APIs support asynchronous Web Services. For true callbacks 


  10 implementations would also require the clients to run Web Service endpoints of their own This asynchronous service model that is composed of synchronous atomic Web Service calls can also be used at varying levels of complexity as desired by the client user For the novice, the service \223submit-isDone-get\224 sequence can be encapsulated into one simple function call. For the intermediate user, the same aggregated function call can also provide real-time notifications \(Observer design pattern\job currently running on the server. For the advance user network disconnects and client shutdown after a service job has been submitted can be achieved. The unique job id that is returned upon service submission can be stored for persistence enabling users to submit a product request from laptop, shutdown their laptop, and restartup at later time to retrieve the generated products Using AXIS2 and JAX-WS for the various client-side implementations introduces an extra dependency for users of some clients and therefore is less preferable. We also want to maximize ease-of-use for the user where we lessen the burden of installation, set up, and library dependencies Whenever possible, we used the \223vanilla\224 SOAP implementation that is intr insically available for each platform to keep the client footprints small Another intended use of our services is to be orchestratable by Web Services-enabled workflow engines. The callback approach to asynchronous Web Services is currently not as interoperable as standard synchronous Web Services However, current Web Service workflow engines can be set up to operate with standard Web Services that have the atomic actions exposed as multiple synchronous Web Service calls that poll for when a job request is done before continuing Data Delivery Once processing has completed, the service response is returned to the user. In th e Earth science domain, these results typically are generated data products that may be large in size. Given that the SOAP approach to Web Service uses XML as the underlying content being transferred transferring binary data in the SOAP message would not be efficient for large binary data sizes. Large binary file data support in XML currently still exhibits technical and performance issues. Approaches ex ist to encode binary data in various encoding schemes such as MIME and base64 More recently, there have b een new W3C recommendations for handling large binary data transfers such as XML-binary Optimized Packaging \(XOP Transmission Optimization Mechanism \(MTOM and Resource Representation SOAP Header Bl  However these new extensions may not be supported or compatible with all the client platforms that we need to support \(such as Matlab, IDL, and Python We settled on an approach that is compatible across the major client platforms. Rather then forcing data to be encoded in any scheme in SOAP, we simply allow the binary data to be transferred efficiently in standard http/https. When a Web Service job has completed, the generated product data files are placed in a unique URLaccessible location, and these UR Ls are sent back in the service response. The clients are then responsible for downloading the product files from the URLs A side benefit of this approach is observed when orchestrating our services fro m workflows. The URL results from one workflow operator are passed onto the next operator, which then is responsible for pulling the data from the given URLs. This method allows each operator to control the rate of accessing th e previous operator\222s data results OPeNDAP Another mechanism we support for data access is Data Access Protocol  \(DAP\ore specifically, we leverage the  for requesting and transporting data that is generated by the services. OPeNDAP also enables remote subsetting of data using constraint expressions, and the translation of data from one format to another. HDF5 data has been recently shown to work well over OPeNDAP using the Hy  The OPeNDAP protocol in recent years has become more widely used and accepted in the Earth Science community The Earth System Grid \(ESG on Environment and Water \(CREW\two large Earth science data centers using OPeNDAP for data access Date Time Handling For time handling, leveraging the Java GregorianCalendar simplifies handling timezones, time ranges, as well as correct leap years.  The bus iness logic model layer fully leverages the GregorianCalendar model to allow us to support manipulating multiple time granules, from seconds to years, and of any duration for each time granule. There is also a related XMLGregorianCalendar that we leverage for representation of W3C XML Schema 1.0 date/time datatypes across the Web Services For averaging, given a particular start and end time in GregorianCalendar format, along with an integer time duration amount, the model is able to determine the correct time ranges \(i.e. number of averaged files\d partitions the work accordingly for the av eraging engine \(in this case the engine is IDL\anCalendar individual time elements are retrieved and passed on to the engine for 


  11 processing, and the result is a list of files that propagates up to the server side layer For subsetting, latitude and longitude values are handled at the model layer, and translated \(when necessary correct coordinate values for the underlying engine.  Also to reduce the number of parameters a user would need to provide, we support user inputs of date/time string using the ISO 8601 format, the International Standard for the representation of dates and tim For exam pl e t h e input string \2232008-10-31T12:00:00Z\224, would be converted into a GregorianCalendar inst ance representing that exact date/time for the model layer Using the combination of GregorianCalendar and the ISO 8601 standard allows us to easily handle timezone-aware and timezone-na\357ve inputs. Internally to our \223business logic\224 model layer, all date/times are timezone-aware and set to the UTC timezone to match most of the instrument data conventions. But if the user provides a timezone-na\357ve date/time, that is with no timezone specified, then we promote it to UTC standard time. We also support user input in any timezone here we leverage the GregorianCalendar to convert any timezone to the UTC internal representation This simplifies science studies where users simply provide the local time at the area of interest to query data for  7  C LIENT I MPLEMENTATION FOR A NALYSIS E NVIRONMENTS  We implemented client-side modules to adapt to the major working environments favored by most scientists: \(1 4\thon, \(5\/C++, and \(6 Fortran90. Unlike other approaches that force the scientists to leave their familiar working environment to access data our services tool set brings the data access and manipulation back into their working envi ronments. Whenever possible we also aimed to develop the ability to automatically download and construct the native data objects in each respectively environment. This eliminates the need for the end user to worry about data file downloads, local file management, and loading them into in-memory data arrays for manipulation. A consistent experience is given to the user, both across the different tools and across the different platforms, with common interfaces and usage conventions This form of seamless integration directly facilitates the transparent access and manipula tion of heterogeneous data as called for by NASA\222s ACCESS NRA goals Java The Java client was designed to be an importable jar library from any user Java application. Since the Web Service endpoint server was already written with Sun\222s JAX-WS Reference Implementation, we also chose the same for the Java client implementation. This maximizes interoperability since both client and server utilize the same library. The client contains high-level methods for calling the Web Service and automatically downloading the custom-created files, allowing the entire process of service querying and downloading data to be contained in a single method call Lower level methods are also exposed in the service allowing the user more fine-grain control over the data flow and interface with the Web Services Matlab Mathwork\222s Matlab is a popular working environment used by scientists to perform science analysis. The 2008 release of Matlab has built-in support fo r Web Services with autocode-generation from WSDL URLs. It leverages the Java integration that Mathworks has already worked into Matlab We leveraged this built-in capability to develop Matlab modules that access the same server-side Web Services for Level 2 and Level 3 data access and manipulation Our Matlab service client consists of a number of .m files file extension for Matlab custom code\and requires no Java package dependencies beyond the JVM native to the Matlab environment.  Built-in SOAP functions help to create, send, and parse the SOAP message, which is used to communicate with our remotely hosted Web Services For automatic data file downloads, classes and methods standard with Java version 1.5 \(standard with Matlab 7.6 were used to access and download the files via http.  The resulting client allows all of the Web Services and downloading functionality to be transparent to a user Matlab supports the construction and handling of full Java data objects and the invocation of Java class methods directly form within Matlab. We made use of built-in functions that served as the bridge between a Matlab script and a SOAP service call \(createSoapMessage.m callSoapService.m, and parseSoapResponse.m\. These built-in SOAP functions in Matlab constrained us to passing a narrow range of Matlab data types due an incomplete set of Matlab datatype to W3C SOAP datatypes. Particularly the date-time and arrays of strings must be manually handled as more primitive types Two methods of datetime passing were settled upon.  One relies simply upon passing a tuple representation of datetime\227a number for the year, and others for the month, day hour, minute, and second.  While such a method worked well \(converting simple numerical data types\t was seen to be cumbersome and made for far less readable code to have to use six parameters to specify a single datetime.  We turned then using short char acter strings to represent datetimes, following the ISO 8601 standard.  Data type conversion of character strings between Matlab and XML is similarly easy as numerical types, and made for very concise and readable function calls 


  12 Passing arrays of strings required a retreat from any kind of array-like data structure.  Instead, a single, long string was created from an array of strings in Matlab, separated by a distinct delimiter \(a \223,\224 in this case\s single long string is passed through the service interface to the endpoint server, which then parses the string back into a list format before continuing on with the rest of the call sequence IDL ITT\222s IDL is another powerful visualization and analysis tool popular with the Earth science analysis community  Unl i k e M a t l a b, IDL \(as of versi on 7.0 have any built-in Web Services support. IDL can be made to speak the Web Services languages via an IDL-Java bridge delegating the Web Services capability to a linked Java library.  This does allow calling the Web Services, but there were some issues encountered along the way First, the IDL-Java bridge connectivity required some setup and handling by the end user. Environment variables and jar classpaths must be properly configured. While this is easily resolve with an installer, there was a strong desire to minimize the IDL client foot print to where there are no dependencies. We wanted to provide an IDL client code that could be dropped into a directory somewhere and should \223just work\224 Second, there are some known issues working with objects in IDL. We encountered memory errors during execution which appeared to be a memory leak in the IDL-Java bridge there was previously a known memory leak that had been patched\also ex tra overhead when interfacing between IDL and Java where data is converted from Java objects to IDL types Our current effort is focused on building a \223poor man\222s\224 SOAP as part of our IDL client that will allow us to directly call and interface with the Web Services, without having to go through Java.  We plan to utilize IDL\222s built-in simple http support to send manually constructed SOAP messages Though this approach forgoes the robustness of the JAXWS implementation, it will however provide a pure IDL client to our end users with no external dependencies Python The Python scripting environment has become a popular working environment for fast prototyping and exploratory science processing. Among all of the clients here, the Python Web Services client is the most trivial. We leveraged the suds package, a lightweight SOAP client for consuming Web Services in Pyt Though ot her open-source Python Web Service packages exist \(such as ZSI\we have found suds to be the most easy to use and more dynamic in nature. Suds does not require class code generation and can read WSDLs at runtime to dynamically construct a proxy object with an interface representing the WSDL C/C We want any C/C++ client to be able to interact with the server-side Web Services of Level 2 and Level 3 data generation. By using the using the popular open-source gSOAP Toolkit for SOAP Web Services package  client-side modules can interact with the data generation services developed on the server-side. gSOAP also includes facilities to autogenerate C/C++ RPC code from our published WSDL definition files of the Web Service on the server. We have also found that gSOAP has a good selfcontained XML bindings facility Fortran Fortran90 modules can be made capable of remotely accessing and the Level 2 and Level 3 data. Though Fortran has no built-in libraries to perform Web Services, we leveraged our C/C++ Web Service API via gSOAP to do the work. Fortran can call an 223externed\224 C Web Service API and pass back the relevant data into the Fortran environment. This would enable Fortran to fully delegate the Web Services operations to the C/C++ implementation 8  NEWS  L EVEL 2  P RODUCTS  With the availability of the software infrastructure supporting server-side processing, and seamless client-side data query and access, downstream data products can now be generated from the source merged NEWS Level 2 data Averaged One of the most common wa ys to summarize the large amount of data is to calculate the averages of data within a given temporal and spatial boundary. For example, it is very useful for scientists to make daily, weekly, monthly averages of some parameters in a regular latitude-longitudepressure grid, make a global map of the average, and analyze any global patterns and trends. In order to facilitate the needs, we developed an averaging Web Service to generate averaged data products that a user can customize The input arguments for the averaging Web Service are the time range, time granule, and a list of parameter names to produce averaged products with. The time range specifies the start and end time to access the NEWS data from. The time granule specifies the aver aging time period. The list of parameter names specifies the choice of parameters that are requested to make averaged products Subsetted Subsetting a data set is a fundamental way to access specific data from a large collection of data. We developed a usercustomizable subsetting Web Service that supports three general subsetting conditions 1  Spatial condition \(latitude, longitude, vertical range 


  13 2  Temporal condition \(e.g. from 2002-05 to 2002-07 3  Parameter selection \(e.g. te mperature and atmospheric water vapor only The combination of these three conditions allows a user to subset data in time, location, and parameter space 9  NEWS  L EVEL 3  P RODUCTS  Many of these quantities in NEWS L2 product interact through fundamental physical processes \(e.g. temperature affects cloudiness, and also the converse\ Consequently the observations should be treated as statistically separate variables, though traditiona l methods of summarizing satellite data do just that. We applied statistical clustering methods to a multiple-parameter set of observations from the A-Train instruments over the multi-year record. The resulting Level 3 quantitative summaries are made accessible through our serv ice-oriented tool Level 3Q Level 3Q data sets are statistical summaries of underlying Level 2 data. Like traditional Level 3 products they are 223gridded\224 in the sense that they provide a summary of Level 2 data belonging to space-time grid cells. These cells are typically defined as one or fi ve degree spatial regions over a time period of one or eight days, or one calendar month Unlike traditional Level 3 products, the Level 3Q \(L3Q grid cell summaries provide nonparametric multivariate estimates of the joint probability distributions of multiple geophysical parameters. Distribution estimates are derived from the underlying Level 2 data using informationtheoretic principles that balance the quality of the estimate against the amount of data reduction achieved  Figure 9. Raw and summarized data for one grid cell Raw data belonging to that grid cell can be listed in a data table with one row for each of N data points and one column for each variable \(here, two alternate representation: a scatter plot. In both cases each data point has weight 1. On the right are two representations of the compressed summary. The data table has K<<N rows and two extra columns showing cluster count and distortion. Counts are shown in the corresponding scatter plot by the bar heights  Data reduction replaces a larg e number of individual data points with a smaller number of representative data points and associated weights and quality measures. Figure 9 illustrates the basic concept. The idea is to treat a set of coincident measurement of different geophysical parameters for the same footprint as a multivariate vector, and collect all such vectors belonging to a given spatial-temporal grid cell as a set of points in high-dimensional data space. These data are partitioned into disjoi nt groups, called clusters, and we report the following statistic s for each: i\the centroid which is the representative, ii\he number or proportion of original data points assigned to it, and iii\the average squared distance between member data points and the centroid. This latter quantity is also called the cluster distortion The method that assigns data points to clusters is an adaptation of a signal-processing algorithm called Entropyconstrained Vector Quantizati EC VQ i s si m i l a r t o  the well-known K-means clusteri  Kmeans finds an assignment of raw data points to K clusters that minimize distortion. ECVQ finds an assignment that minimizes a quantity based partly on distortion, but also on the entropy of the probability distribution defined by the clustering. Entropy is a measure of information-theoretic complexity, and it is also well known that greater complexity is required to achieve lower distorti  ECVQ was originally proposed as a way of estimating this trade-off. The algorithm may find fewer than K groups as it attempts to balance the competing goals of fidelity to the original data and parsimony of representation.  This produces the smallest, or more properly, the least complex output data set that achieves a given level of fidelity to the original data. Our version of ECVQ is adapted in a number of ways for use as a massive data set reduction tool. These are described in detail in  and 26  W e  have al so previously employed our version of ECVQ to produce monthly summaries of Atmospheric Infrared Sounder data  The algorithm\222s output is best thought of as an estimate of the multivariate distribution of the data in a given space-time grid box. The original data have a distribution that puts probability  N on each multivariate data point, where N is the number of data points. ECVQ coarsens this distribution by collecting similar points into clusters, representing them by cluster centroids, and assigning probabilities N k k where N k is the number of point assigned to the k th cluster. In addition we also report the within-cluster mean squared error distortion\, which is a measure of the quality of the cluster representative as a stand-in for the original data assigned to it 1 N to cluster  


  14 10  A NALYSIS R ESULTS  AIRS, AMSR-E, MODIS and CLOUDSAT data have been merged into a dataset by the NEWS effort, and a framework of Web Services for averaging, subsetting and statistical analysis have been developed. Collectively it facilitates the data access and analysis of hydr ological processes. Here we present an example usage of instrument intercomparison Comparing Data Products Prior to Merging A necessary step in creating a formal merged data product is intercomparison of component data sets.  This ensures that the mutual random and systematic differences between the two data sets are quantified.  This approach does not provide information about absolute bias, which can be obtained only from comparisons with unbiased standard data sets.  For example, wate r vapor and temperature biases are typically constrained through comparisons with in situ observations as from radiosonde.  Such comparisons are usually the responsibility of the data provides, so the analyses described below assume some knowledge of satellite measurement biases An example of comparing component data sets is presented here with a single atmospheric state variable, in this case observed by AIRS \(Atmos pheric Infrared Sounder AMSR-E \(Advanced Microwave Scanning Radiometer for EOS\For this example five variables \(AIRS Total Water column, AMSR-E total water column, AIRS cloud fraction AIRS total water error estimate, AMSR-E liquid water path stemming from two different instruments \(AIRS and AMSR-E\pared and correlated The Atmospheric InfraRed Sounder \(AIRS\he Advanced Microwave Scanning Radiometer \(AMSR-E\are two instruments aboard the AQUA spacecraft. AMSR-E estimates water vapor over water surfaces and AIRS estimates water vapor over ocean and land. A map of the daily average of terrestrial water vapor column is shown in figure 10. This figure maps th e AIRS estimate of average total \(column\er vapor in mm during March 2003 at a spatial resolution of 1 degree in latitude and longitude  Figure 10. Map of averaged AIRS Total column water vapor for 2003-03   Figure 11. Scatter plot of monthly AIRS and AMSR-E column water vapor. AIRS and AMSR-E water vapor agree very well on the co incident locations  A similar a subset of AMSR-E water vapor over the ocean was prepared with our services and merged with the AIRS dataset at the same spatial and temporal resolutions. A scatter plot of the values estimated with AMSR-E is compared to the collocated va lue of AIRS in figure 11 Figure 11 also shows a red line to mark the location where all points should fall if the AIRS and AMSR-E estimates were the same. The figure shows a tendency by AMSR-E to estimate higher total water vapor than AIRS. However there are locations where AIRS does show higher values. A map of the averaged differences \(AIRS-AMSR-E\15 selected monthly means between the years 2003--2006 is shown in figure 12 This map highlights locations where each instrument tends to overe stimate compared with the other. Blue tones identify regions where AIRS estimates are larger than AMSR-E and shades of brown locate the regions where the opposite is true 


  15  Figure 12. Map of average differences over 15 months between AIRS and AMSR-R water vapor  Figure 12 highlights regions th at are characterized by different hydrological regimes. AIRS overestimates coincide with regions where cold western boundary currents cause frequent cold marine stratocumuli. AMSR-E tends to estimate higher total water vapor in regions characterized by warm sea surface temperatures and frequent convective activity. This result is consistent with previous comparisons    Figure 13. AIRS Total Cloud Fraction sum for 2003-03 Sum over all pressure levels AIRS is an IR measurement that cannot estimate water vapor in regions overcast with optically thick clouds. This property introduces a bias that depends on the cloud fraction. Figure 13 shows an estimate of the cloud fraction using a surrogate for cloud fraction over several AIRS pressure levels. It adds up the cloud fraction at the different levels \(because there may be overlaps, the sum over all pressure levels can be larger than one between the areas with large cloud fraction sums and large AMSR-E overestimates with respect to AIRS and vice versa, areas with the smallest cloud fraction sums coincide with the areas with large AIRS water vapor estimates A proxy for the "thickness" of the clouds in the overcast regions is the liquid water content of such clouds. The advantage of this proxy over others is that it also conveys information about the physical and hydrological characteristics of the scenes co rrelated with the differences Figure 14 shows a PDF of the differences as a function of AIRS water vapor and AMSR-E cloud liquid water path. A black contour line marks the change of sign in the differences. It shows that AIRS estimates higher total water vapor at low liquid water paths with a characteristic quasilinear increase between 5--20 mm. AMSR-E estimates larger total water vapor at 1 x 1 degree regions where the liquid water path is high. The pattern of the differences raises questions about why does AMSR-E estimates differ so quickly from AIRS at low water vapor contents and low liquid water paths  Figure 14. AIRS-AMSR-E differences \(in mm function of AIRS total water and AMSR-E cloud liquid water for the month 2003-03  


  16  Figure 15. AIRS-AMSRE differences as a function of AIRS error estimate over one day  AIRS has an error estimate of the total water vapor value that it calculates. The diffe rences between AIRS and AMSR-E are shown as a function of this estimate in figure 15 and very little correlation is found 11  R ELEVANT W ORK  Merged A-Train Level 2 Data A merged product that preserves the relationship of observed atmospheric water properties facilitates the hydrological studies by enabling scientists to get directly at the model data without worrying about the logistics of finding, collecting, and coordinating the measured quantities from different instruments. Previously there did not exist a capability to discover and access data from the A-Train\222s multiple instruments as merged multi-parameter data sets Enabling Orchestratable Service Workflows Our distributed service-oriented approach of loosely coupled services also enable s a higher level of reusability and orchestration with other services. Increasing numbers of workflow engines are already supporting Web Services as components/operators, which can then be orchestrated together into higher-level meta/virtual services SciFlo, a Scientific Dataflow Execution Environment, is a workflow engine that already supports assembling reusable SOAP Services, native execu tables, local command-line scripts, and codes into a distributed computing flow \(a graph of operators\8 SciFlo can u tilize o u r g en eric SOAP services as part of a larger coordinated data flow The Taverna Workbench is a free software tool for designing and executing workflows. Like SciFlo, it can orchestrate SOAP-based Web Services as components within a workflow. Taverna provides a visual editor to construct and edit the sequence of services in the workflow We have found that Taverna can dynamically introspect a given WSDL and construct the workflow component interface representing it Giovanni Giovanni, an acronym for the Goddard Earth Sciences Data and Information Services Cent er, or GES DISC, Interactive Online Visualization and Analys is Infrastructure, is a webbased tool to help visualize Earth science data  It  provides a simple and intuitive way to visualize, analyze and access vast amounts of Eart h science remote sensing data without having to download the data. Similar to the services developed here, it addresses the difficulties of traditional data acquisition and analysis methods by moving the complexity to the server-side Giovanni provides multiple in terface instances based on instrument and measurement ty pes. For example, the \223ATrain Along CloudSat Track Inst ance\224 can provide plots of vertical profiles of clouds, temperature, humidity, cloud and aerosol classification across the multiple instruments of the A-Train A distinction between Givanni\222s A-Train data and the data set in this paper is that we are using a formal merged product of the A-Train. We leverage the NEWS effort that is based on error- and resolution-weighted mean of the input data sets, with associated uncertainty estimates. This provides a formal model of the collective A-Train observations rather than the collection of the individual instrument measurements Each of Giovanni\222s multiple interface instances provides a very simple and easy to use web interface. However, we recognized that sometimes scientists want more than the simple interfaces. Some scien tists may want to process Level 3 products using their own trusted code, or may want to perform variations of their own plots. With Giovanni, the individual scientist wanting more custom advanced capabilities must depend on the Giovanni development team Giovanni is based on the web portal paradigm where users visit a web page and use web tools to find and visualize data. Similar to Giovanni, our client APIs also make data acquisition more seamless. However, our services are based on the different paradigm were the power and flexibility of data analysis and processing are shifted back into the scientists own familiar computing environments. We realize that scientists generally want to perform \223exploratory computing\224 where they can sere ndipitously analyze the data using their own familiar and trusted code 


  17 Giovanni 2 was inherently synchronous where processing was bounded to a single http session. Long service running times still require the user to hold the same http session Similar to our asynchronous Web Service we discussed, the upcoming Giovanni 3 will be supporting asynchronous sessions. They will be using a RSS feed to monitor the service request. Version 3 will also be based on a servicesoriented architecture, wher e Giovanni services can be offered as a standard SOAP Web Services. This is similar to our approach, as well as SciFlo\222s services 12  C ONCLUSIONS  To achieve the science research goal of investigating longterm and global-scale trends in climate, water and energy cycle, and weather variability, we enhanced and improved on existing algorithms to work with distributed and heterogeneous data and information systems infrastructure By developing a service-oriented architecture for discovering, accessing, and mani pulating of NEWS merged A-Train data sets, we can strengthen the interconnectedness and reusability of these services across broader range of Earth science investigations The merged NEWS Level 2 data is a formal model containing the voluminous data from the AIRS, AMSR-E MLS, MODIS, and CloudSat instruments. Previously scientists wanting to perform long-term and global-scale studies encompassing simultaneous measured quantities would quickly face a data acce ss hurdle of first finding the data, then manually downloading them, and finally merging the data into a cohesive model\227before starting their analysis. Additionally the voluminous nature of the data particularly because of the MODIS data\each scientist potentially downloading the same data resulting in redundancy of reprocessing on the client sides. Our paradigm pushes more of the commonly repeated processing onto the server side. Moreover, this avoids repeated downloading of the same data among the science users. We can deliver customi zed averaged, subsetted, and summarized data of the merged A-Train observations to the scientists for them to immediately begin their analysis work We recognized that scientists also often want to perform 223exploratory computing\224 where they can freely explore the aspects of the data and run serendipitous exploration in their own familiar environment. We developed client-side distributed APIs in popular analysis environments such as Matlab, IDL, and Python. Our APIs hide the complexity of Web Services and allow the service capabilities to be embedded in the scientists own computing environments By purposely avoiding the \223web portal\224 paradigm and providing the suite of platform specific APIs in each of these language platforms, we enable the scientists to remain within their own familiar environments to select, process and download the data seamlessly into their environment for their own further analysis. Alternative methods involving web portals force the scientists to leave the environment and manually interact with the web portal to search and download the data We can examine not only long-term changes in amplitude of a single variable but also those among multiple variables Our L3Q clustering method was specifically designed to preserve information about the covariability of multiple observations, such as those from the A-Train.  Weather and climate variability is characterized by changes among atmospheric observables, but those changes have been limited by a lack of observations and analytical techniques We are not aware of any multi-parameter analyses to date The full potential of the A-Train climate record will not be realized until the multi-parameter climatology is understood. The work presented is one method of approaching this difficult problem Our service tool addresses several objectives of the NASA Earth science data community including 1\mprove interoperability to facilitate the transparent access and manipulation of heterogeneous and distributed data by science users, 2\ransition and deploy existing Earth science research analysis tools and software using a 223Service Oriented Architecture\224 \(SOA\ to enhance their reuse potential for other science domains and improve overall awareness and access of these tools by a broad community, 3\ increase users\222 ability to customize their discovery, access, deliv ery, manipulation, and preferred format of data and information 12  F UTURE W ORK  On-demand Level 3T Summaries from Level 3Q We plan to develop services for creating custom summaries of the L3Q data into more refined Level 3T summaries L3T\create their own custom Level 3 products on demand from L3Q. The custom Level 3 products are the transformation of L3Q data based on user-specific objectives such as regression and correlation analyses. The cust om production will generate not only the transformed data but also the statistical estimation of the accuracy of the summarized data based on the distribution of L3Q and the quality of L3Q Delegating the Temporal-Spatial Data Querying Currently, our processing layer utilizes existing and legacy processing code that was developed in IDL, Matlab, and C++. Though the original intent was to be able to adapt existing code and wrap as a service, this meant maintaining its original form of accessing the source data for processing Small modifications were made to enable these codes to quickly access the data based on file path and file naming schemes. However, we want to decouple the file accessibility and processing roles 


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


