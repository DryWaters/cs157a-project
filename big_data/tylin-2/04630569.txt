Abstract Political and social issues play a big role in economical systems Macroecono mic variables which are affected by the above mentioned factors can be used in economical forecasting. Time series are used as a very powerful tool in economical systems for short time predicting. As time series predict the future output according to the past behaviors of the system, therefore they can not sense sudden changes in the behavior of the economical sy stem. In this paper macroeconomic variables are used as exogenous variables in forecasting model. Traditional methods using transformation and differentiation suffer 
from a decrease in accuracy forecasting To get rid of the prob lems in the above mentioned methods a Neuro-Fuzzy NF re is used as a strong nonlinear mapping tool even on nonstationary time series Combination of statistical methods on time series and other dynamical models with NF structu re, provide a better model in forecasting. Using çNF-ARMAXé,éNN-ARMAX models and implementing them on real-life da ta of çTehran Stock Market show a good accuracy in our new designed predictive model I I NTRODUCTION t is obvious that bolstering the stock market has a very important role in reinforcing the economical institutes 
Forecasting share prices of companies provide information about their growth rate and stability in future For instance, this case cause economical decision in such a way that decreases its risk and probable loses, therefore predicting in the stock market is valuable not only for the investors but also for institu tes and even the dealers. The stock market can be described by nonlinear and complex tim e series m a y be nonstationary or have seasonal behavior. Since the structures such as neural or NF networks benefit from nonlinear mapping and behave rather well in encountering nonst ationary time series \(i.e having learning ability ese structures in the stock market prediction systems avoids transformation and 
differentiation on data Neural networks benefit from the learning property and can predict the system output by learning from training data  3 4 but the problem is that  neural networks are not interpretable while fuzzy systems try to mimic the human logical decision which makes it  6  Mehdi Roopaei is with the Dept. of Computer Science and Eng., Shiraz University corresponding author, Post Box: 71955-177  SHIRAZ-IRAN phone 98-7116309641 mobile 98-9177137538 Mehdi Roopaei e-mail: mehdi.roopaei@gmail.com Mansoor zolghadri is with the Dept. of Computer Science and Eng Shiraz University, ZAND street SHIRAZ, IRAN, \(e-mail:  zjahromi@shira zu.ac.ir 
Abbas Emadi is with the Dept. of Electrical Eng., Virtual Shiraz University, e-mail: abbas.emadi@gmail.com The combination of these two methods provides a powerful tool which is called NF In system identification many statistical methods have been developed to model stochastic system and time series Time series and system identif ication models provide more precisely forecasting response than econometric models in  Aut oregressive Moving Average eXogenous \(ARMAX\odels In this paper by combining this model w   NN-ARMAX" and also with NF structure 12 13  NF-ARMAX", the share prices in TEHRAN stock market 
can be forecasted In section II, NF structures are discussed. In section III presents the effect of exogenous variables on economical forecasting Section IV discusses time difference in various economic markets and simultaneous forecasting. Section V shows our simulation results And finally, conclusion is included in section VI II NF DYNAMIC STRACTURES In economical systems, it is common to use "ARMA" and ARIMA models but there are no parts as inputs among them. For the reasons which are explained in the next section we use the combinati on of "ARMAX" model which has input term and its struct ures is shown below, with the Neural Network \(ANN II. ARMAX MODEL 
This model in linear form is as follows             t e q C nk t x q B t y q A 1 Where in the above relation y\(t is output x\(t is Extra Input or Exogenous Variable and e\(t is the white noise A\(q B\(q and C\(q are the polynomials in terms of time shift operator of the orders 
n a n b and n c respectively and n k is the delay This structure is shown in the Fig-1 Fig.1- The ARMAX model block diagram Economical Forecasting by Exogenous Variables Mehdi Roopaei, Mansoor Zolghadri, Abbas Emadi I 1491 978-1-4244-1819-0/08/$25.00 c  2008 IEEE 


Neural Network ARMAX This model is the combination of the MLP neural network with ARMAX model. The estimated value of y\(t shown by    t y is expressed as follows    1     1   1        nc t t na t y t y nk nb t x nk t x f t y 2 In which, a neural netw ork provides the mapping for nonlinear function f   issued to denote the model parameters. This structure \(show in Fig. 2 same as the structure of Fig.1 but, instead of using a simple summation block a neural network structure is used to provide the mapping Fig.2- NN-ARMAX model block diagram NF ARMAX This model is the combination of NF structure with the ARMAX model An Adaptive Neuro-Fuzzy Inference System \(ANFIS\apping of equation 2\odel is shown in Fig. 3 Fig.3- The NF-ARMAX model block diagram III THE EFFECT OF EXOGENOUS VARIABLES ON ECONOMICAL FORECASTING Many factors such as the wo rd political and social events and the way people think about a social issue affect economical systems For instance, share prices may decrease enormously due to a political change or increase dramatically because of a social issue. "ARMA" and ARIMA time series, having no variable to measure these factors as mentioned above, which are called exogenous are appropriate models for forecasting the future based on generalizing the series past behavior. But to increase the accuracy of the forecasting the effect of these factors should be considered quantitatively It seems that quantitative variables could be found in such a way that they would be affected from the above qualitative factors and at the same time, they themselves have influence on stock market Therefore if these quantitative variables are used as inputs for forecasting model, the effect of these factors is considered in forecasting and the accuracy of prediction is improved It is clear that these factors have influence on other markets such as oil, gas, gold, and exchange as well. But the intensity of influence is not the same. So indicators, whose markets have more influence than the above factors, are used as representative of these factors for implementing the forecasting model Fig-4 shows the above case. The main effect of these factors while being applied compared to the case in which it is not applied is distinguishabl e in the simulations Fig. 4- The effect of exogenous variables on economical forecasting IV TIME DIFFERENCE IN VARIOUS ECONOMIC MARKETS AND SIMULTANEOUS FORECASTING As stated in section II and shown in Fig.1, Fig.2 and Fig.3 x  t      t y    t x i variables are not the main inputs for generating data and are affected simultaneously from those qualitative factors \(Fig-4\These variables can be representative of common factors among the markets. The mentioned qualitative factors are also part of above variables and their variations have a big role in stock market Therefore to consider their effects x  t  inputs and non-compliance, may cause delay in the forecasting model and making the model invalid 1492 2008 IEEE International Conference on Fuzzy Systems FUZZ 2008 


At first glance, it looks like that x  t  is not reachable, but if x  t input variables prices could be found a few hours before opening the stock market in a specific location where the forecasting is needed and implementing them to the model which is trained based on these variables before\(it is necessary that the forecasting model be trained using such x  t  forecasting is obtained. Nevertheless is necessary to say that some social mental factors which affect the stock market intensively are the local factors. So, one method can use both global and local market variables simultaneously. In conventional methods such as Cointegration method under specific conditions, synchronized equilibrium relations are used, but if the needed conditions are not satisfied then, our dynamic model explained in this paper can be useful in this situation R EMARK 1 Since the predicting model faces an increment in forecasting error after a while, \(because the model is trained only once and is used for some period of time\odel should be updated, \(trained and tested with the new data after intervals. The updating intervals depend on the testing phase of result. i.e. , if the predicting error between the forecasting and the actual value in testing phase is reasonable for 50 steps the forecasting model must be updated after 50 steps though it is better to be updated earlier Fig. 5- The updating time in which the model is valid R EMARK 2 The effects of exogenous variables are different in the modeling i.e for the simulations used in this paper, the oil price is mostly effect compared to the exchange market variable which has the least influence Therefore, to increase the speed of the modeling computation, the exchange market variable could be ignored. Par ticularly in "NF-ARMAX model which allocates along time to train the model R EMARK 3 The "NF-ARMAX" model has acceptable results when the data are not normalized. The main problem is the massive computation created in the training phase. Especially by increasing the number of membership functions and the order of the model, \(increasing the b a n n  and c n causes increase in presence of lags in x y  and  computational volume increases exponentially R EMARK 4 Since the applied model in this paper is only used for forecasting and not for econometrics analysis, therefore the purpose is not the parameter estimation uniquely and the whiteness and smallness of error plus the convergence of the error covariance matrix are of the great importance. \(in testing phase V SIMULATION RESULTS To be sure of desirable performance of the models being discussed it is necessary to us e them practically. The data applied in this pape r are related to Arak Machine Manufacture shares price in "Tehran Stock Market" from Nov. 2003 till Jan. 2005 The forecasting model input variables, correspond to this time depend on the price of oil gas gold, dollar and euro  The data are based on the average daily prices but the holidays and the days with no dealing are omitted Since the ranges of the data are very different and usually big in various variables, therefore normal izing the data has an important role in forecasting model improvement. This has been verified as below though other methods for normalizing are also existed. If  r\(t\e series and M=max{r\(t\=min{r\(t   t r N is the normalized signal, it can be formulated as below  m M m t r t r N     The fitness criterion being used in this paper is Fit = 100 \(1 - norm y  y\\(y-mean\(y Where in the above relation y  is the predicted value of the real output y and the used norm can be chosen as 2-norm T EST 1- NN-ARMAX In this experiment, MLP neural network with a hidden layer Levenberg-Marquardt" training algorithm and MSE criterion are used in "NN-ARMAX" model Figs. 6 and 7 illustrate one-step ahead prediction compared to the actual data. In this simu lation, all of the five inputs are being used In TABLE-I , the simulation results for any various inputs are shown. In any case, the fitness value indicates the more better forecasting results obtained by our designed model NN-ARMAX ear, increasing the inputs cause an increase in forecasting accuracy 2008 IEEE International Conference on Fuzzy Systems FUZZ 2008 1493 


Fig. 6- One-step ahead forecasting with NN-ARMAX model in training phase Ö using five mentioned inputs and the fitness is %94.2 Fig. 7- One-step ahead forecasting with NN-ARMAX model in testing phase Ö using five mentioned inputs  and the fitness is %83.3 TABLE I Experiment Results for çNN-ARMAXé  Model Number of utilized extra inputs Fitness in training phase  Fitness in testing phase  Model order Number of hidden neurons No extra input 83.5 63.1 na=1  nc=3 9 1 x 90.3 75.4 na=1 nb=2 nc=3 11 2  1 x x 91.6 77 na=1 nb i 2 nc=3 i=1,2 5 3  2  1 x x x 94.5 77.7 na=1 nb i 2 nc=3 i=1,2,3 7 4  3  2  1 x x x x 94 80.3 na=1 nb i 2 nc=3 i=1,2,3,4 5 5 4 3 2 1     x x x x x 94.2 83.3 na=1 nb i 2 nc=3 i=1,2,3,4,5 5 T EST 2 NF-ARMAX Similar to test-1 Figs. 8 and 9 illustrate one-step ahead prediction compared to the actual data for NF-ARMAX model In the above model all of the five inputs are being used and their applying effect of such input variables with the fitness criterion for any input are being discussed in TABLE II Fig.8- One-step ahead forecasting with NF-ARMAX model in training phase Ö using five mentioned inputs and the fitness is %93.5 Fig. 9- One-step ahead forecasting with NF-ARMAX model in testing phase using five mentioned inputs and the fitness is %85.3 TABLE II Experiment Results for çNF-ARMAXé  Model Number of the utilized extra inputs Fitness in training phase percent Fitness in testing phase percent Model order Number of Membership functions No extra input 90.8 65.1 na=1 nc=3 3 1 x 92.8 76.1 na=1 nb=2 nc=3 3 2 x  1 x 93.7 77.3 na=1 nbi=2 nc=3 i=1,2 3 3 x  2 x  1 x 94 78 na=1 nbi=2 nc=3 i=1,2,3 3 4 x  3 x  2 x  1 x 93.1 81.2 na=1 nbi=2 nc=3 i=1,2,3,4 3 5 x  4 x  3 x  2 x  1 x 93.5 85.3 na=1 nbi=2 nc=3 i=1,2,3,4,5 3 1494 2008 IEEE International Conference on Fuzzy Systems FUZZ 2008 


VI CONCLUSION In this paper, by using the macroeconomic variables as representative of exogenous variables for forecasting model and implementing the "NN-ARMAX" and "NF-ARAMX models, high capability of the mentioned models in the share prices forecasting of the stock market has been evaluated R EFERENCES  Rong-Jon Li, Zhi-Bin Xiong "Forecasting Stock Market with Fuzzy Neural Networks" Proceedings of 4th Int. Con. On Machine Learning and Cybernetics, Guangzhou,18-21 Aug. 2005  Hornik k  A pproxim ation Capabilities of Multi-Layer Feed forward Networksé, Neural Networks 4\(1991  Jain B.A. and Nag B.N., çPerform ance Evaluation of Neural Network Decision Modelsé, Manage Information Systems 14\(1997  Skapura D.,Building Neural Networks Addition W e slay New York 1  Shapiro A.F., çThe Merging of Neural Networks,Fuzzy Logic and Genetic Algorithm Insurance, Mathematics and Economics 31\(2002  Pao Y.H., Adaptive Pattern Recognition and Neural Networks Addition Weslay,New York , 1989  Ljung L., System Identification. Theory for the user, Prentice Hall PTR, 1999  Ham id A., Applied Econom etrics. New Approaches Tehran Univ Pabl., 2002  Norgaard M., Neural Networ k Based System Identification Toolbox Ver.2, Department of Automation Technical Univ. of Denmark, 2000  R.Fletcher" Practical M e thods of Optim ization", W iley 1987  Jang, J.-S. R., "ANFIS: Adaptive-Network-based Fuzzy Inference Systems," IEEE Transactions on Systems, Man, and Cybernetics, Vol 23, No. 3, pp. 665-685, May 1993  Jang J.-S R Sun, C.-T M i zutani, E., 1997. NF and Soft Com puting A Computation Approach to Learning and Machine Intelligence Prentice Hall, Upper Saddle River, NJ  Drake, J.T 2000. Com m unications Phase Sy nchronization Using The Adaptive Network Fuzzy Inference System. Ph.D. Thesis, New Mexico State Univ., Las Cruces, New Mexico,USA 2008 IEEE International Conference on Fuzzy Systems FUZZ 2008 1495 


  Association between PNQ adoption and the lack of companies and professionals with integrated comprehension of both problems and solutions Q1.13   Association between ISO 9001 adoption and the lack of quantitative knowledge about performance \(Q1.3   Association between ITIL adoption and the lack of clear alignment among IT actions and strategic goals of the organization \(Q1.2   Association between ITIL adoption and the lack of formal decision making process \(Q1.1   Association between People CMM adoption and frequent IT project delays \(Q1.5 The Table 10 presents the contingency table between PNQ adoption and the question 1.13. This association has the contingency coefficient C value of 0.288 and the p-value of 5.40x10 4 allowing us to reject the null hypothesis of no association between the variables. However, we can see that neither the frequency of respondents that agree with this IT problem nor the frequency of respondents that disagree change significantly in the group of those are implementing PNQ. Then, we cannot affirm that there is an association between the adoption of PNQ and the decrease of this IT problem Table 10: Association between PNQ adoption and the lack of companies and professionals with integrated comprehension of problems and solutions \(Q1.13 PNQ Impl. Planning Won't implement Total  Q1.13  Answers Agree 63.6 40.5 74.5 64.5 107 Disagree 36.4 59.5 25.5 35.5 59  100.0 100.0 100.0 100.0 166 The Table 11 presents the contingency table between ISO 9001 adoption and the question 1.3. This association has the contingency coefficient C value of 0.258 and the p-value of 3.46x10 3 allowing us to reject the null hypothesis of no association between the variables We can see in the Table 11 a negative association between the adoption of ISO 9001 and the lack of quantitative knowledge about performance. The frequency of respondents that agree with this IT problem falls from 66.7% to 57.1% in the group of those are implementing ISO 9001. On the other hand the frequency of respondents that disagree with this IT problem rises from 33.3% to 42.9% in the group of those that are implementing ISO 9001. Then, we can affirm that, at least in the analyzed sample, the adoption of ISO 9001 tends to decrease the lack of quantitative knowledge about performance Table 11: Association between ISO 9001 adoption and the lack of quantitative knowledge about performance \(Q1.3 ISO9001 Impl Planning Won't implement Total  Q1.3  Answers Agree 57.1 56.7 83.1 66.7 106 Disagree 42.9 43.3 16.9 33.3 53  100.0 100.0 100.0 100.0 159  The Table 12 presents the contingency table between ITIL adoption and the question 1.2. This association has the contingency coefficient C value of 0.249 and the p-value of 4.42x10 3 allowing us to reject the null hypothesis of no association between the variables In this case, there is also a negative association between ITIL adoption and the lack of clear alignment among IT actions and strategic goals of the Table 9: Contingency coefficient C among quality instruments and IT problems IT problems PNQ ISO9001 COBIT eSCM People CMM ITIL CMMI OPM3 Six Sigma Q1.1 0.037 0.119 0.067 0.163 0.092 0.247 0.056 0.132 0.099 Q1.2 0.104 0.156 0.112 0.151 0.030 0.249 0.102 0.089 0.116 Q1.3 0.186* 0.258** 0.158 0.173 0.166 0.097 0.035 0.094 0.204 Q1.4 0.033 0.075 0.120 0.107 0.141 0.212* 0.090 0.077 0.070 Q1.5 0.076 0.108 0.085 0.142 0.246** 0.109 0.150 0.107 0.047 Q1.6 0.202* 0.155 0.071 0.094 0.097 0.083 0.171 0.102 0.156 Q1.7 0.209* 0.219* 0.065 0.122 0.167 0.048 0.046 0.121 0.193 Q1.8 0.215 0.043 0.069 0.128 0.062 0.087 0.080 0.067 0.196 Q1.9 0.033 0.131 0.124 0.113 0.112 0.180 0.047 0.139 0.127 Q1.10 0.190* 0.061 0.051 0.101 0.078 0.111 0.127 0.050 0.115 Q1.11 0.108 0.153 0.074 0.180 0.104 0.079 0.113 0.172 0.196 Q1.12 0.080 0.147 0.132 0.054 0.151 0.071 0.022 0.080 0.118 Q1.13 0.288*** 0.207 0.038 0.031 0.131 0.067 0.142 0.104 0.148 statisticall y si g nificant at: * 5% level, ** 1% level and *** 0.1% level  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 


organization \(Q1.2\The frequency of respondents that agree with this IT problem falls from 40.9% to 21.3 in the group of those are implementing ITIL. On the other hand, the frequency of respondents that disagree with this IT problem rises from 59.1% to 78.7% in the group of those that are implementing ITIL. Then, we can affirm that, at least in the analyzed sample, the adoption of ITIL tends to decrease the lack of clear alignment among IT actions and strategic goals of the organization Table 12: Association between ITIL adoption and the lack of clear alignment among IT actions and strategic goals of the organization \(Q1.2 ITIL Impl. Planning Won't implement Total  Q1.2 Answers Agree 21.3 50.6 44.4 40.9 67 Disagree 78.7 49.4 55.6 59.1 97  100.0 100.0 100.0 100.0 164  The Table 13 presents another association about ITIL. It shows the contingency table between ITIL adoption and the question 1.1, which is about the lack of formal decision making process. This association has the contingency coefficient C value of 0.247 and the p-value of 4.79x10 3 allowing us to reject the null hypothesis of no association between the variables Table 13: Association between ITIL adoption and the lack of formal decision making process \(Q1.1 ITIL Impl. Planning Won't implement Total  Q1.1 Answers Agree 25.5 44.3 60.5 42.7 70 Disagree 74.5 55.7 39.5 57.3 94  100.0 100.0 100.0 100.0 164  In this case, there is also a negative association between ITIL adoption and the lack of formal decision making process \(Q1.1\The frequency of respondents that agree with this IT problem falls from 42.7% to 25.5% in the group of those are implementing ITIL. On the other hand, the frequency of respondents that disagree with this IT problem rises from 57.3% to 74.5% in the group of those that are implementing ITIL. Then, we can affirm that, at least in the analyzed sample, the adoption of ITIL tends to decrease the lack of formal decision making process The Table 14 presents the contingency table between People CMM adoption and the question 1.5 This association has the contingency coefficient C value of 0.246 and the p-value of 5.84x10 3 allowing us to reject the null hypothesis of no association between the variables Table 14: Association between People CMM adoption and frequent IT project delays \(Q1.5 People CMM Impl Planning Won't implement Total  Q1.5 Answers Agree 40.0 70.9 76.5 70.0 112 Disagree 60.0 29.1 23.5 30.0 48  100.0 100.0 100.0 100.0 160  In this case, there is also a negative association between People CMM adoption and frequent IT project delays \(Q1.5\The frequency of respondents that agree with this IT problem falls from 70% to 40% in the group of those are implementing People CMM. On the other hand, the frequency of respondents that disagree with this IT problem rises from 30% to 60% in the group of those that are implementing People CMM Then, we can affirm that, at least in the analyzed sample, the adoption of People CMM tends to decrease the frequency of IT project delays  5. Discussion  The data analysis yields some interesting insights First, the data give indication that the adoption of People CMM tends to decrease the frequency of IT project delays. The most adopted quality instrument in Brazil, CMMI, claims the benefit of improve the organizations ability to fulf ill their project deadlines Based on the literature review we could expect that the adoption of CMMI would decrease the frequency of IT project delays 9  30 But the data analysis has shown no statistically significant association between the CMMI adoption rate and the frequency of IT project delays. The People CMMI, on the other hand, is focused on improving the management and development of the human assets of an organization and claims no compromise with decreasing the frequency of project delays 31 The data give us indication that investments in the adoption of People CMM has been more effective in the avoidance of IT project delays than investments in adoption of CMMI at least in the analyzed sample. This is not a surprise once it is well accepted that well managed people are very important to organizations performance and it could be a possible explanation of the findings related to People CMM. The surprising finding is that People CMM seems not attractive to the organizations when compared to CMMI. While 52.1% of the respondents have no plan to adopt People CMM, just 6.5% won t Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


implement CMMI. In fact, 22.2% of the respondents consider the People CMMI unimportant, against 1.1 in the case of CMMI. One possible explanation is the commercial appeal of CMMI. Several companies and government agencies requires the CMMI certification from their IT suppliers in Brazil. It does not happen in the case of People CMMI. The organizations could be then focusing in the market advantages proportioned by the CMMI certification instead of the benefits proportioned by People CMM adoption. Another possible explanation is just a lack of knowledge about the benefits of People CMM adoption, because the data shown that it is one of the littlest known quality instrument within the Brazilian organizations, while CMMI is the best-known one The same findings lead us to another research question. The CMMI is a quality instrument related to processes improvements efforts and People CMM is a quality instrument related to people management However, the collected data doesn t allow us to infer if the organizations are choosing between people management approach and the process improvement approach or if it is just a matter of model choice Therefore, it is a question that could be addressed in future works Another finding is that data give indication that organizations adopting ITIL tend to adopt COBIT as well. This finding is aligned with previous studies 32 3 4 that show both models as complementary approaches. A possible explanation, already proposed in the literature, is that organizations could be using COBIT as a way to provide their ITIL programs with governance and control mechanisms The data also give indication that organizations adopting ITIL and organization adopting ISO 9001 tend to adopt Six Sigma as well. Again, these results are aligned with the literature  and could be possible explained by the existence of synergy between ITIL and Six Sigma and between ISO 9001 and Six Sigma. Besides, Six Sigma could provide a tool for statistical control of process in both ITIL and ISO 9001 programs  6. Conclusions  This paper presents results from a quantitative research that was conducted among 260 participants of a software quality event in Brazil. It explores the perception of the respondents of a survey about the quality instruments and IT problems throughout statistical analysis. As a limitation of this study we could highlight the use of a convenience sample, which could introduce some bias in the collected data. But we consider this sample adequate due to the number of participants and the fact of being one of the main events on this area in Brazil The main contribution is the identification of the respondent s vision about the relationship of the quality instruments adoption and the several IT problems. The results obtained through this survey provided an overview of the impact of the analyzed IT problems in the organizations, the degree of knowledge, importance and adoption of each one of the analyzed quality instruments. Through the appliance of statistical tests, we also obtained understanding about the impact of the quality instruments in some IT problems, like lack of quantitative knowledge about performance, frequent IT project delays and lack of strategic alignment As future work, some topics could be addressed First, further research is necessary to verify if the organizations are choosing between people management approach and the process improvement approach or if the discrepancies found in the rate of adoption and knowledge about CMMI and Six Sigma are just a matter of model popularity and level of competitive advantage provided by the models themselves. In addition, as future works other dimensions of the survey will be analyzed and a framework to integrate quality instruments will be developed by the authors  7. References  1 Fre e d m a n, M T h e  A r t And Disc ipli ne Of Stra te g i c  Leadership. USA: McGraw-Hill, 2003  2 Dig m a n L   A  Stra te g i c Ma na g e m e nt: Com pe ting in a  Global Information Age. USA: Dame Thomson Learning 2002   Kakab a d s e N  Ka kab a d s e  A  IS  I T  G o vern an ce  Need  for and integrated model. Corporate Governance, United Kingdom, v. 1, n. 4, p. 9-11,  mar. 2001  4 IT SMF-Inte r na tio na l. IT Se rv ic e Ma na g e m e nt: A n  Introduction. Holland: Van Haren Publishing, 2002  5 W e ill P e te r; Ross Je a nne  IT  G o v e rna nc e How  T op Performers Manage IT Decision Rights for Superior Results Harvard Business Scholl, 2004  6 W e ill, P e te r. Don t Just L e a d  G ove rn: How  T opPerforming Firms Govern IT. Mis Quarterly Executive Minnesota, v. 3, n. 1, p. 1-17, Mar. 2004  7 ISO International Organization for Standardization Switzerland, 2000  8 FNQ - Na ti ona l Q u a lity Foun da tio n. S o P a ul o 20 04    Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


9 CMM - Ca pa bility Ma turi ty Mode l. G u ide line s f o r  Improving the Software Process. SEI Series, SEI \(Software Engineering Institute\, 2004  10 IT G o v e rna nc e Institute   CobiT 4.0 EUA, Ilinois: IT Governance Institute, 2005, 207pp  11 m p ie ri, Robe rt o H C o ll a do, Ca rl os F  L u c i o   Pila r B. MetodologÌa de la investigaciÛn. MÈxico: McGraw-Hill 1991  12 Y i n, R. K  Es tu do de Ca s o  P l a n e j a m e n to e M todos 2  ed. S„o Paulo: Bookman, 2001  13 H o ppe n  N o rbe r t o e t a l A v a lia  o de a r tig os de pesquisa em sistemas de informaÁ„o: proposta de um guia In.: ENANPAD, 21, 1997, Rio das Pedras. Anais... Rio das Pedras: ANPAD, 1997  14 Fink  A r le ne T h e s u rv ey ha nd bo ok T h o u s a nd O a k s   Sage Publications, 1995  15  Sa m b am ur thy  V.; Zm ud R  W  A r r a ng em e n ts f o r  information technology governance: A theory of multiple contingencies. MIS Quarterly, Vol.23, No.2 pp.261-290 1999  16 Mo tjol opa ne I; B r ow n, I. Stra te g i c bus ine s s IT alignment, and factors of influence: a case study in a public tertiary education institution. ACM International Conference Proceeding Series, vol. 75, pp. 147-156, 2004  17 L u f t m a n, J A s s e ssing busine ss-IT  a lig n m e n t m a turit y   Communications of the Association for Information Systems 4, 14, 1-50, 2000  1 Kaplan  R S  N o rton  D   P  T h e Balan ced S c o r ecard  Measures That Drive Performance. Harvard Business Review, 70\(1\:72 79, 1992  19 L e e  J.; Hu y nh, M. Q.; K w ok R. C.; P i S. IT outsourcing evolution---: past, present, and future. Commun ACM 46, 5, 84-89, 2003  20  V o as, J. L i m ited So f t ware  W a rranties. Eng i neering of Computer Based Systems, \(ECBS2000\ Proceeding, pp. 5661, 2000  21 Ma  Z.; C o ll of e llo, J.S Sm i t h-Da n ie ls, D.E Im prov ing  software on-time delivery: an investigation of project delays IEEE Aerospace Conference Proceedings, vol.4, pp.421-434 2000  22  Va n Sol i ng e n R  B e rg hou t, E   Va n L a tum  F Interrupts: just a minute never is. IEEE Software, vol. 15 issue 5, pp.97-103, 1998  23  V o a s J  Ce rtif ic a tion: Re d u c ing the hi dde n c o s t  of poor  quality, IEEE Software, pp.22-25, 1999  24 G r iff iths, J.; Els on B.; A m os, D. A c u stom e r-supplie r interaction model to improve customer focus in turbulent markets. Managing Service Quality, 2001, vol. 11\(1\, pp.5766  25 Om a r R.; Za ila ni, S.; Sula im a n M.; Ra m a y a h, T   Supplier involvement, customer focus, supply chain technology and manufacturing performance: Findings from a pilot study. IEEE International Conference on Management of Innovation and Technology, vol.2, pp.876-880, 2006  2  S u rm acz J T u rn o v er is Exp e n s ive CIO M a gazin e  1 5  June 2004  27 B u rn es P.T  V o lu n t ary E m p l o y ee T u ro v e r: W h y I T  Professionals Leave. IT Professional, vol.8 , issue: 3, pp.4648, 2006  28 B h a r a ti P., Be rg D Se rv ic e qua lity f r o m the ot he r side   Information systems management at Duquesne Light International Journal of Information Management, Volume 25, Issue 4, August 2005, Pages 367-380, 2005  29 D e a n J  W  Bow e n, D   E. Ma na g e m e nt the o ry a nd total quality: Improving research and practice through theory development. Academy of Management Review, 3\( 19\, 392418, 1994  30 K u lpa   M. K  J o h n s o n  K  A  Inte rpre ti ng the CMMI A Process Improvement Approach. 2. ed. USA: Auerbach Publications, 2008  31 C u rtis, B.; He f l e y W  E.; Milla r, S. A  T h e P e o p le  Capability Maturity Model: Guidelines for Improving the Workforce. SEI Series in Software Engineering, USA Addison-Wesley Professional, 2001  32 Ca te r-Ste e l A  T a n, W  G   T o le m a n, M. Cha lle ng e of  Adopting Multiple Process Improvement Frameworks Proceedings of the 14th European Conference on Information Systems, 2006  33 Mi ng a y S.; Bitting e r, S. Com bine CobiT a nd IT I L  f o r  Powerful Governance. Gartner Inc, 2006  34 SallÈ, M. IT Serv ice Ma n a g e m e n t an d IT G o v e rn an ce  review, comparative analysis and their impact on utility computing: Hewlett-Packard Company, 2004  3 E d gem a n  R L   Bi gi o  D   F e rl eman  T  S i x S i g m a an d  Business Excellence: Strategic and Tactical Examination of IT Service Level Management at the Office of the Chief Technology Officer of Washington, DC. Quality and Reliability Engineering International, vol. 21, 2005   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


0 200 400 600 800 1000 1200 Days 24h since Rover 1 440 nm Rover 1 880 nm Figure 12 Daily values of the atmospheric constant T 8 CONCLUSIONS This paper has described the Pancams EDL Rover 2 440 nm 2.5 T 1.5 0 200 400 600 800 1000 1200 Days 24h since EDL Rover 2 880 nm 2.5 EDL 2.5T 2 on the MER on the horizon was carried out at the Jet Propulsion Laboratory California Institute of Technology under on all was conducted Based means that the cover glass On never get high are significant ghost images of the or imply its endorsement by the United States Government or the Jet Propulsion Laboratory California Institute of Technology REFERENCES 1 URL Jet Propulsion Laboratory Pasadena US vers.,html cited 12/29/2004 2 J F Bell III et al Pancam Multispectral Imaging Results from the Opportunity Rover at Meridiani Planum Science Vol 306 Issue 5702 1703-1709  3 December 2004 3 G.H.Smith E.C.Hagerott L.M.Scherr K.E.Herkenhoff J.F.Bell III Optical designs for the Mars 03 sun close to the horizon is presented It shows that locations rover 2 880 nm the ghost contains 10 of the primary image intensity A future mission to Mars Mars Science Laboratory will potentially land at high latitudes rover cameras Proceedings of SPIE Vol 4441-14 4 A Eisenman et al Mars Exploration Rover Engineering Cameras SPIE Proceedings of International Symposium sun Also on the Martian surface During Martian winter time this a contract with the National Aeronautics and Space Administration References herein to any specific commercial product process sun in the pictures caused by reflections in the sun will sun images an image of the can be similar brightness sun images A number of representative images have been presented It is observed that there sun near the horizon in order to obtain attitude information An example of EDL 2.5 0 200 400 600 800 1000 1200 Days 24h since rovers that acquire the sun and the position measurement processing of over the horizon and the pancams will have to image the as the an investigation to determine the atmospheric constant of the Martian atmosphere in the 440 nm and the 880 nm band was established to be 0.9 A seasonal change and dust storms in u is observed ACKNOWLEDGEMENT The research described here or service by trademark manufacturer or otherwise does not constitute on 10 0 200 40D 600 24h sin 1200 Days 24h since 


Remote Sensing 17 21 September 2001 Toulouse France 5 A Eisenman C.C.Liebe R.Perez Sun Sensing on the Mars Exploration Rovers Proceedings of the 2002 IEEE Aerospace Conference Volume 5 9-16 March 2002 Pages 5-2249 5-2262 vol.5 6 C.C.Liebe accuracy Performance of Star Trackers A Tutorial IEEE Transactions on Aerospace and Electronic Systems AES volume 38 No 2 April 2002 pp 587 599 7 Howard A Perko John D Nelson and Jacklyn R Green Review Of Martian Dust Composition Transport Deposition Adhesion And Removal Proceedings of Space 2002 a biannual conference of the American Society of Civil Engineers _ Dr Lawrence Scherr is an  l 2-li designer He received an MS.S in _  Il _  Physics from university of  received his Ph.D in biophysics from U C Berkeley in 1980 He   I ranalyzed stray light on an enjoyable variety of optiCS Tese include prototype medical instruments intraocular lenses scaterometers large surveillance telescopes automated optical test systems and Mars camera lenses 8 Geoffrey A Landis Materials Adherence Experiment Results From Mars Pathfinder Proceedings of the 26th IEEE Photovoltaic Specialists Conference 1997 IEEE NJ 1997 pp 865-869 ISBN 0-7803-3767-0 9 C.C.Liebe L.Scherr R Wilson Sun Induced Veiling Glare in Dusty Camera Optics SPIE Optical Engineering Febuary 2004 Vol 43 No 2p 493-499 Biographies Dr Carl Christian Liebe received the M.S.E.E degree in 1991 and the Ph.D degree in 1994 from the Department of Electrophysics Technical University of Denmark Since 1997 he has been an employee at the Jet Propulsion Laboratory California Institute of Technology Currently he is a senior member of the technical staff in the Active Optical Sensing Group He has authored/co-authored more than 50 papers Jim Alexander received an A.B from U.C Berkeley and an M.A and C Phil from UCLA Since 1983 he has been heavily involved at JPL in star tracker and scanner testing analysis requirements scene simulation calibration algorithm design and implementation for missions such as the Astro-1 shuttle experiment Mars Pathfinder and the Cassini spacecraft Currently he is the MSL lead for surface GNC and the supervisor for the Precision Motion Control and Celestial Sensors group at JPL 11 


 12 The process  t  Y  t  h  t  is called the intensity process  of the counting process.  It is a stochastic process that is determined by the information contained in the history process F t via Y  t  Y  t  records the number of individuals experiencing the risk at a given time t   As it will become clear that   t is equivalent to the failure rate or hazard function in tr aditional reliability theory, but here it is treated as a stochastic process, the most general form one can assume for it. It is this generalization that encapsulates the power that counting stochastic process approach can offer to survival analysis  Let the cumulative intensity process H  t be defined as   t t ds s t H 0 0      53  It has the property that  E  N  t  F t  E  H  t  F t  H  t   54  This implies that filtration F t, is known, the value of Y  t  fixed and H  t es deterministic H  t is equivalent to the cumulative hazards in tr aditional reliability theory  A stochastic process with the property that its expectation at time t given history at time s < t is equal to its value at s is called a martingale That is M  t martingale if           t s s M F t M E s  55  It can be verified that the stochastic process         t H t N t M 56  is a martingale, and it is called the counting process martingale The increments of the counting process martingale have an expected value of zero, given its filtration F t That is  0      t F t dM E 57 The first part of the counting process martingale [Equation 56  N  t non-decreasing step function.  The second part H  t smooth process which is predictable in that its value at time t is fixed just prior to time t It is known as the compensator  of the counting process and is a random function. Therefore, the martingale can be considered as mean-zero noise and that is obtainable when one subtracts the smoothly varying compensator from the counting process  Another key component in the counting process and martingale theory for survival analysis is the notion of the predictable variation process of M  t oted by    t M It is defined as the compensator of process   2 t M  The term predictable variation process comes from the property that, for a martingale M  t it can be verified that the conditional variance of the increments of M  t  dM  t  h e inc r em ents of   t M That is          t M d F t dM Var t  58  To obtain      t F t dM Var  one needs the random variable N  t  variable with probability   t of having a jump of size 1 at time t The variance of N  t   t  1 t   since it follows b i nom ial distribution    Ignoring the ties in the censored data 2  t  is close to zero and Var  dM  t  F t    t  Y  t  h  t   This implies that the counting process N  t on the filtration F t behaves like a Poisson process with rate  t    Why do we need to convert the previous very intuitive concepts in survival analysis in to more abstract martingales The key is that many of the sta tistics in survival analysis can be derived as the stochastic integrals of the basic martingales described above The stochastic integral equations are mathematically well structured and some standard mathematical techniques for studying them can be adopted  Here, let   t K be a predictable  process An example of a predictable process is the process   t Y Over the interval 0 to t the stochastic integral of su ch a process, with respect to a martingale, is denoted by     0 u dM u K t It turns out that such stochastic integrals them selves are martingales as a function of t and their predictable variation process can be found from the predictable variation process of the original martingale by           0 2 0 u M d u K u dM u K t t 59 The above discussion was drawn from Klein and Moeschberger \(2003 also provide examples of how the above process is applied. In the following, we briefly introduce one of their exampl es ó the derivation of the Nelson-Aalen cumulative hazard estimator  First, the model is formulated as           t dM dt t h t Y t dN  60  If   t Y is non-zero, then               t Y t dM t d t h t Y t dN 61  Let   t I be the indicator of whether   t Y  is positive and define 0/0 = 0, then integrating both sides of above \(61 One get 


 13                     0 0 0 u dM u Y u I u d u h u I u dN u Y u I t t t 62  The left side integral is the Nelson-Aalen estimator of H t           0  u dN u Y u I t H t 63  The second integral on the right side           0 u dM u Y u I t W t 64  is the stochastic integral of the predictable process      u Y u I with respect to a martingale, and hence is also a martingale  The first integral on the right side is a random quantity    t H   t du u h u I t H 0        65  For right-censored data it is equal to   t H  in the data range, if the stochastic uncertainty in the   t W is negligible Therefore, the statistic    t H is a nonparametric estimator of the random quantity    t H   We would like to mention one more advantage of the new approach, that is, the martingale central limit theorem.  The central limit theorem of martingales ensures certain convergence property and allows the derivations of confidence intervals for many st atistics.  In summary, most of the statistics developed with asymptotic likelihood theory in survival analysis can be derived as the stochastic integrals of some martingale.  The large sample properties of the statistics can be found by using the predictable variation process and martingale central limit theorem \(Klein and Moeschberger \(2003  2.7. Bayesian Survival Analysis  Like many other fields of statistics, survival analysis has also witnessed the rapid expansion of the Bayesian paradigm.  The introduction of the full-scale Bayesian paradigm is relative recent and occurred in the last decade however, the "invasion" has been thorough.  Until the recent publication of a monograph by Ibrahim, Chen and Sinhaet 2005\val anal ysis has been either missing or occupy at most one chapter in most survival analysis monographs and textbooks.  Ibrahim's et al. \(2005\ changed the landscape, with their comprehensive discussion of nearly every counterp arts of frequentist survival analysis from univariate to multivariate, from nonparametric, semiparametric to parametric models, from proportional to nonproportional hazards models, as well as the joint model of longitudinal and survival data.  It should be pointed out that Bayesian survival analysis has been studied for quite a while and can be traced back at least to the 1970s  A natural but fair question is what advantages the Bayesian approach can offer over the established frequentist survival analysis. Ibrahim et al 2005\entified two key advantages. First, survival models are generally very difficult to fit, due to the complex likelihood functions to accommodate censoring.  A Bayesi an approach may help by using the MCMC techniques and there is available software e.g., BUGS.  Second, the Bayesian paradigm can incorporate prior information in a natural way by using historical information, e.g., from clinical trials. The following discussion in this subsection draws from Ibrahim's et al. \(2005  The Bayesian paradigm is based on specifying a probability model for the observed data, given a vector of unknown parameters This leads to likelihood function L   D   Unlike in traditional statistics is treated as random and has a prior distribution denoted by   Inference concerning is then based on the posterior distribution which can be computed by Bayes theorem d D L D L D              66 where is the parameter space  The term    D is proportional to the likelihood    D L  which is the information from observed data, multiplied by the prior, which is quantified by   i.e          D L D 67 The denominator integral m  D s the normalizing constant of    D and often does not have an analytic closed form Therefore    D often has to be computed numerically The Gibbs sampler or other MCMC algorithms can be used to sample    D without knowing the normalizing constant m  D xist large amount of literature for solving the computational problems of m  D nd    D   Given that the general Bayesian algorithms for computing the posterior distributions should equally apply to Bayesian survival analysis, the specification or elicitation of informative prior needs much of the atte ntion.  In survival analysis with covariates such as Cox's proportional hazards model, the most popular choice of informative prior is the normal prior, and the most popular choice for noninformative is the uniform Non-informative prior is easy to use but they cannot be used in all applications, such as model selection or model comparison. Moreover, noninformative prior does not harness the real prior information. Therefore, res earch for informative prior specification is crucial for Bayesian survival analysis  


 14 Reliability estimation is influenced by the level of information available such as information on components or sub-systems. Bayesians approach is likely to provide such flexibility to accommodate various levels of information Graves and Hamada \(2005\roduced the YADAS a statistical modeling environment that implements the Bayesian hierarchical modeli ng via MCMC computation They showed the applications of YADES in reliability modeling and its flexibility in processing hierarchical information. Although this environment seems not designed with Bayesian surviv al analysis, similar package may be the direction if Bayesian survival analysis is applied to reliability modeling  2.8. Spatial Survival Analysis  To the best of our knowledge spatial survival analysis is an uncharted area, and there has been no spatial survival analysis reported with rigor ous mathematical treatment There are some applications of survival analysis to spatial data; however, they do not address the spatial process which in our opinion should be the essential aspect of any spatial survival analysis. To develop formal spatial survival analysis, one has to define the spatial process first  Recall, for survival analysis in the time domain, there is survival function   R t t T t S  Pr   68  where T is the random variable and S  t e cumulative probability that the lifetime will exceed time t In spatial domain, what is the counterpart of t One may wonder why do not we simply define the survival function in the spatial domain as   S  s  Pr S s  s R d  R > 0  69  where s is some metric for d-dimensional space R d and the space is restricted to the positive region S is the space to event measurement, e.g., the distance from some origin where we detect some point object.  The problem is that the metric itself is an attribute of space, rather than space itself Therefore, it appears to us that the basic entity for studying the space domain has to be broader than in the time domain This is probably why spatial process seems to be a more appropriate entity for studying  The following is a summary of descriptions of spatial processes and patterns, which intends to show the complexity of the issues involved.  It is not an attempt to define the similar survival function in spatial domain because we certainly unders tand the huge complexity involved. There are several monographs discussing the spatial process and patterns \(Schabenberger and Gotway 2005\.  The following discussion heavily draws from Cressie \(1993\ and Schabenberger and Gotway \(2005  It appears that the most widely adopted definition for spatial process is proposed in Cressie \(1993\, which defines a spatial process Z  s in d-dimensions as    Z  s  s D R d  70  Here Z  s denotes the attributes we observe, which are space dependent. When d = 2 the space R 2 is a plane  The problem is how to define randomness in this process According to Schabenberger and Gotway \(2005 Z  s an be thought of as located \(indexed\by spatial coordinates s  s 1  s 2  s n  the counterpart of time series Y  t  t   t 1  t 2  t n   indexed by time.  The spatial process is often called random field   To be more explicit, we denote Z  s  Z  s   to emphasize that Z is the outcome of a random experiment  A particular realization of produces a surface    s Z  Because the surface from whic h the samples are drawn is the result of the random experiment Z  s called a random function  One might ask what is a random experiment like in a spatial domain? Schabenberger and Gotway \(2005\ offered an imaginary example briefly described below.  Imagine pouring sand from a bucket on to a desktop surface, and one is interested in measuring the depth of the poured sand at various locations, denoted as Z  s The sand distributes on the surface according to the laws of physics.  With enough resources and patience, one can develop a deterministic model to predict exactly how the sand grains are distributed on the desktop surface.  This is the same argument used to determine the head-tail coin flipping experiment, which is well accepted in statistical scie nce. The probabilistic coinflip model is more parsimonious than the deterministic model that rests on the exact \(perfect\but hardly feasible representation of a coin's physics. Similarly deterministically modeling the placement of sand grains is equally daunting.  However, th e issue here is not placement of sand as a random event, as Schabenberger and Gotway 2005\phasized.  The issue is that the sand was poured only once regardless how many locations one measures the depth of the sand.  With that setting, the challenge is how do we define and compute the expectation of the random function Z  s Would E  Z  s   s  make sense  Schabenberger and Gotway \(2005\further raised the questions: \(1\to what distribution is the expectation being computed? \(2\ if the random experiment of sand pouring can only be counted once, ho w can the expectation be the long-term average  According to the definition of expectation in traditional statistics, one should repeat the process of sand pouring many times and consider the probability distributions of all surfaces generated from the repetitions to compute the expectation of Z  s is complication is much more serious 


 15 than what we may often reali ze. Especially, in practice many spatial data is obtained from one time space sample only. There is not any independent replication in the sense of observing several independent realizations of the spatial process \(Schabenberger and Gotway 2005  How is the enormous complexity in spatial statistics currently dealt with? The most commonly used simplification, which has also been vigorously criticized, is the stationarity assumption.  Opponents claim that stationarity often leads to erroneous inferences and conclusions.  Proponents counte r-argue that little progress can be made in the study of non-stationary process, without a good understanding of the stationary issues Schabenberger and Gotway 2005  The strict stationarity is a random field whose spatial distribution is invariant under translation of the coordination.  In other word s, the process repeats itself throughout its domain \(Schabenberger and Gotway 2005 There is also a second-order or weak\ of a random field  For random fields in the spatial domain, the model of Equation \(70  Z  s  s D R d  is still too general to allow statistical inference.  It can be decomposed into several sub-processes \(Cressie 1993             s s s W s s Z  D s  71  where  s  E  Z  is a deterministic mean structure called large-scale variation W  s is the zero-mean intrinsically stationary process, \(with second order derivative\nd it is called smooth small-scale variation   s is the zero-mean stationary process independent of W  s and is called microscale variation  s  is zero-mean white noise also called measurement error. This decomposition is not unique and is largely operational in nature \(Cressie 1993\he main task of a spatial algorithm is to determine the allocations of the large, small, and microscale components However, the form of the above equation is fixed Cressie 1993\, implying that it is not appropriate to sub-divide one or more of the items. Therefore, the key issue here is to obtain the deterministic  s  but in practice, especially with limited data, it is usually very difficult to get a unique  s   Alternative to the spatial domain decomposition approach the frequency domain methods or spectral analysis used in time series analysis can also be used in spatial statistics Again, one may refer to Schabenberger and Gotway \(2005  So, what are the imp lications of the general discussion on spatial process above to spatial survival analysis?  One point is clear, Equation \(69 S  s  Pr S s   s R d is simply too naive to be meaningful.  There seem, at least, four fundamental challenges when trying to develop survival analysis in space domain. \(1\ process is often multidimensional, while time pr ocess can always be treated as uni-dimensional in the sense that it can be represented as  Z  t  t R 1  The multidimensionality certainly introduces additional complications, but that is still not the only complication, perhaps not even the most significant 2\One of the fundamental complications is the frequent lack of independent replication in the sense of observing several independent realizations of the spatial process, as pointed out by Cressie \(1993\\(3\e superposition of \(1 and \(2\brings up even more complexity, since coordinates  s of each replication are a set of stochastic spatial process rather than a set of random variables. \(4\n if the modeling of the spatial process is separable from the time process, it is doubtful how useful the resulting model will be.  In time process modeling, if a population lives in a homogenous environment, the space can be condensed as a single point.  However, the freezing of time seems to leave out too much information, at least for survival analysis Since the traditional survival analysis is essentially a time process, therefore, it should be expanded to incorporate spatial coordinates into original survival function.  For example, when integrating space and time, one gets a spacetime process, such as          R t R D s t s Z d   where s is the spatial index \(coordinate t is time.  We may define the spatial-temporal survival function as   S  s  t  Pr T t  s D  72  where D is a subset of the d dimensional Euclidean space That is, the spatial-temporal survival function represents the cumulative probability that an individual will survive up to time T within hyperspace D which is a subset of the d dimensional Euclidian space.  One may define different scales for D or even divide D into a number of unit hyperspaces of measurement 1 unit  2.9. Survival Analysis and Artificial Neural Network   In the discussion on artificial neuron networks \(ANN Robert & Casella \(2004\noted, "Baring the biological vocabulary and the idealistic connection with actual neurons, the theory of neuron networks covers: \(1\odeling nonlinear relations between explanatory and dependent explained\ariables; \(2\mation of the parameters of these models based on a \(training\ sample."  Although Robert & Casella \(2004\ did not mentioned survival analysis, their notion indeed strike us in that the two points are also the essence of Cox s \(1972\roportional Hazards model  We argue that the dissimilarity might be superficial.  One of the most obvious differences is that ANN usually avoids probabilistic modeling, however, the ANN models can be analyzed and estimated from a statistical point of view, as demonstrated by Neal \(1999\, Ripley \(1994\, \(1996 Robert & Casella \(2004\What is more interesting is that the most hailed feature of ANN, i.e., the identifiability of model structure, if one review carefully, is very similar to the work done in survival anal ysis for the structure of the 


 16 Cox proportional hazards model. The multilayer ANN model, also known as back-propagation ANN model, is again very similar to the stratified proportional hazards model  There are at least two advantages of survival analysis over the ANN.  \(1\val analysis has a rigorous mathematical foundation. Counting stochastic processes and the Martingale central limit theory form the survival analysis models as stochastic integral s, which provide insight for analytic solutions. \(2\ ANN, simulation is usually necessary \(Robert & Casella \(2004\which is not the case in survival analysis  Our conjecture may explain a very interesting phenomenon Several studies have tried to integrate ANN with survival analysis.  As reviewed in the next section, few of the integrated survival analysis and ANN made significant difference in terms of model fittings, compared with the native survival analysis alone The indifference shows that both approaches do not complement each other.  If they are essentially different, the inte grated approach should produce some results that are signifi cantly different from the pure survival analysis alone, either positively or negatively Again, we emphasize that our discussion is still a pure conjecture at this stage   3   B RIEF C ASE R EVIEW OF S URVIVAL A NALYSIS A PPLICATIONS     3.1. Applications Found in IEEE Digital Library  In this section, we briefly review the papers found in the IEEE digital library w ith the keyword of survival analysis  search. The online search was conducted in the July of 2007, and we found about 40 papers in total. There were a few biomedical studies among the 40 survival-analysis papers published in IEEE publications. These are largely standard biomedical applications and we do not discuss these papers, for obvious reasons  Mazzuchi et al. \(1989\m ed to be the first to actively  advocate the use of Cox's \(1 972\ proportional hazards model \(PHM\in engineering re liability.  They quoted Cox's 1972\ original words industrial reliability studies and medical studies to show Cox's original motivation Mazzuchi et al \(1989 while this model had a significant impact on the biom edical field, it has received little attention in the reliability literature Nearly two decades after the introducti on paper of Mazzuchi et al 1989\ears that little significant changes have occurred in computer science and IEEE related engineering fields with regard to the proportional hazards models and survival analysis as a whole  Stillman et al. \(1995\used survival analysis to analyze the data for component maintenance and replacement programs Reineke et al. \(1998 ducted a similar study for determining the optimal maintenance by simulating a series system of four components Berzuini and Larizza \(1996 integrated time-series modeling with survival analysis for medical monitoring.  Kauffman and Wang \(2002\analyzed the Internet firm su rvival data from IPO \(initial public offer to business shutdown events, with survival analysis models  Among the 40 survival analysis papers, which we obtained from online search of the IEEE digital library, significant percentage is the integration of survival analysis with artificial neural networks \(ANN In many of these studies the objective was to utilize ANN to modeling fitting or parameter estimation for survival analysis.  The following is an incomplete list of the major ANN survival analysis integration papers found in IEEE digital library, Arsene et 2006 Eleuteri et al. \(2003\oa and Wong \(2001\,  Mani et al 1999\ The parameter estimation in survival analysis is particular complex due to the requirements for processing censored observations. Therefore, approach such as ANN and Bayesian statistics may be helpful to deal with the complexity. Indeed, Bayesian survival analysis has been expanded significantly in recent years \(Ibrahim, et al. 2005  We expect that evolutionary computing will be applied to survival analysis, in similar way to ANN and Bayesian approaches  With regard to the application of ANN to survival analysis we suggest three cautions: \(1\he integrated approach should preserve the capability to process censoring otherwise, survival analysis loses its most significant advantage. \(2\ Caution should be taken when the integrated approach changes model struct ure because most survival analysis models, such as Cox's proportional hazards models and accelerated failure time models, are already complex nonlinear models with built-in failure mechanisms.  The model over-fitting may cause model identifiability  problems, which could be very subtle and hard to resolve 3\he integrated approach does not produce significant improvement in terms of model fitting or other measurements, which seemed to be the case in majority of the ANN approach to survival analysis, then the extra complication should certainly be avoided. Even if there is improvement, one should still take caution with the censor handling and model identifiability issues previously mentioned in \(1\ and \(2  3.2. Selected Papers Found in MMR-2004  In the following, we briefly review a few survival analysis related studies presented in a recent International Conference on Mathematical Methods in Reliability, MMR 2004 \(Wilson et al. 2005  Pena and Slate \(2005\ddressed the dynamic reliability Both reliability and survival times are more realistically described with dynamic models. Dynamic models generally refer to the models that incorporate the impact of actions or interventions as well as thei r accumulative history, which 


 17 can be monitored \(Pena and Slate 2005\he complexity is obviously beyond simple regression models, since the dependence can play a crucial ro le. For example, in a loadsharing network system, failure of a node will increase the loads of other nodes and influences their failures  Duchesne \(2005\uggested incorporating usage accumulation information into the regression models in survival analysis.  To simplify the model building Duchesne \(2005\umes that the usage can be represented with a single time-dependent covariate.  Besides reviewing the hazard-based regression models, which are common in survival analysis, Duchesne 2005\viewed three classes of less commonly used regression models: models based on transfer functionals, models based on internal wear and the so-called collapsible models. The significance of these regression models is that they expand reliability modeling to two dimensions. One dimension is the calendar time and the other is the usage accumulation.  Jin \(2005\ reviewed the recent development in statisti cal inference for accelerated failure time \(AFT\odel and the linear transformation models that include Cox proportional hazards model and proportional odds models as special cases. Two approaches rank-based approach and l east-squares approach were reviewed in Jin \(2005\. Osbo rn \(2005\d a case study of utilizing the remote diagnostic data from embedded sensors to predict system aging or degradation.  This example should indicate the potential of survival analysis and competing risks analysis in the prognostic and health management PHM\ since the problem Osborn \(2005 addressed is very similar to PHM. The uses of embedded sensors to monitor the health of complex systems, such as power plants, automobile, medical equipment, and aircraft engine, are common. The main uses for these sensor data are real time assessment of th e system health and detection of the problems that need immediate attention.  Of interest is the utilization of those remote diagnostic data, in combination with historical reliability data, for modeling the system aging or degradation. The biggest challenge with this task is the proper transformation of wear  time The wear is not only influenced by internal \(temperature, oil pressures etc\xternal covariates ambient temperature, air pressure, etc\, but also different each time   4  S UMMARY AND P ERSPECTIVE   4.1. Summary  Despite the common foundation with traditional reliability theory, such as the same probability definitions for survival function  S  t  and reliability  R  t  i.e  S  t  R  t well as the hazards function the exact same term and definition are used in both fields\rvival analysis has not achieved similar success in the field of reliability as in biomedicine The applications of survival analysis seem still largely limited to the domain of biomedicine.  Even in the sister fields of biomedicine such as biology and ecology, few applications have been conducted \(Ma 1997, Ma and Bechinski 2008\ the engin eering fields, the Meeker and Escobar \(1998\ monograph, as well as the Klein and Goel 1992\ to be the most comprehensive treatments  In Section 2, we reviewed the essential concepts and models of survival analysis. In Section 3, we briefly reviewed some application cases of survival analysis in engineering reliability.  In computer science, survival analysis seems to be still largely unknown.  We believe that the potential of survival analysis in computer science is much broader than network and/or software reliability alone. Before suggesting a few research topics, we no te two additional points  First, in this article, we exclusively focused on univariate survival analysis. There are two other related fields: one is competing risks analysis and the other is multivariate survival analysis The relationship between multivariate survival analysis and survival analysis is similar to that between multivariate statistical analysis and mathematical statistics.  The difference is that the extension from univariate to multivariate in survival analysis has been much more difficult than the developm ent of multivariate analysis because of \(1\rvation cens oring, and \(2\pendency which is much more complex when multivariate normal distribution does not hold in survival data.  On the other hand, the two essential differences indeed make multivariate survival analysis unique and ex tremely useful for analyzing and modeling time-to-event data. In particular, the advantages of multivariate survival analysis in addressing the dependency issue are hardly matched by any other statistical method.  We discuss competing risks analysis and multivariate survival analysis in separate articles \(Ma and Krings 2008a, b, & c  The second point we wish to note is: if we are asked to point out the counterpart of survival analysis model in reliability theory, we would suggest the shock or damage model.   The shock model has an even longer history than survival analysis and the simplest shock model is the Poisson process model that leads to exponential failure rate model The basic assumption of the shock model is the notion that engineered systems endure some type of wear, fatigue or damage, which leads to the failure when the strengths exceed the tolerance limits of th e system \(Nakagawa 2006 There are extensive research papers on shock models in the probability theory literature, but relatively few monographs The monograph by Nakagawa \(2006\s to be the latest The shock model is often formulated as a Renewal stochastic process or point process with Martingale theory The rigorous mathematical derivation is very similar to that of survival analysis from counting stochastic processes  which we briefly outlined in section 2.6  It is beyond the scope of the paper to compare the shock model with survival analysis; however, we would like to make the two specific comments. \(1\Shock models are essentially the stochastic process model to capture the failure mechanism based on the damage induced on the system by shocks, and the resulting statistical models are 


 18 often the traditional reliability distribution models such as exponential and Weibull failure distributions.  Survival analysis does not depend on specific shock or damage Instead, it models the failure with abstract time-to-event random variables.  Less restrictive assumptions with survival analysis might be more useful for modeling software reliability where the notions of fatigue, wear and damage apparently do not apply.  \(2\hock models do not deal with information censoring, the trademark of failure time data.  \(3\ Shock models, perhaps due to mathematical complexity, have not been applied widely in engineering reliability yet.  In contrast, the applications of survival analysis in biomedical fields are much extensive.  While there have been about a dozen monographs on survival analysis available, few books on shock models have been published.  We believe that survival analysis and shock models are complementary, a nd both are very needed for reliability analysis, with shock model more focused on failure mechanisms and the survival analysis on data analysis and modeling  4.2. Perspective   Besides traditional industrial and hardware reliability fields we suggest that the following fields may benefit from survival analysis  Software reliability Survival analysis is not based on the assumptions of wear, fatigue, or damage, as in traditional reliability theory.  This seems close to software reliability paradigm. The single biggest challenge in applying above discussed approaches to softwa re systems is the requirement for a new metric that is able to replace the survival time variable in survival analysis. This "time" counterpart needs to be capable of characterizing the "vulnerability" of software components or the system, or the distance to the next failure event. In other words, this software metric should represent the metric-to-event, similar to time-toevent random variable in survival analysis.  We suggest that the Kolmogorov complexity \(Li and Vitanyi 1997\n be a promising candidate. Once the metric-to-event issue is resolved, survival analysis, both univariate and multivariate survival analysis can be applied in a relative straightforward manner. We suggest that the shared frailty models are most promising because we believ e latent behavior can be captured with the shared fra ilty \(Ma and Krings 2008b  There have been a few applications of univariate survival analysis in software reliability modeling, including examples in Andersen et al.'s \(1995\ classical monograph However, our opinion is that without the fundamental shift from time-to-event to new metric-to-event, the success will be very limited. In software engineering, there is an exception to our claim, which is the field of software test modeling, where time variable may be directly used in survival analysis modeling  Modeling Survivability of Computer Networks As described in Subsection 2.3, random censoring may be used to model network survivability.  This modeling scheme is particularly suitable for wireless sensor network, because 1\ of the population nature of wireless nodes and \(2\ the limited lifetime of the wireless nodes.  Survival analysis has been advanced by the needs in biomedical and public health research where population is th e basic unit of observation As stated early, a sensor networ k is analogically similar to a biological population. Furthermore, both organisms and wireless nodes are of limited lifetimes. A very desirable advantage of survival analysis is that one can develop a unified mathematical model for both the reliability and survivability of a wireless sensor network \(Krings and Ma 2006  Prognostic and Health Management in Military Logistics PHM involves extensive modeling analysis related to reliability, life predictions, failure analysis, burnin elimination and testing, quality control modeling, etc Survival analysis may provide very promising new tools Survival analysis should be able to provide alternatives to the currently used mathematical tools such as ANN, genetic algorithms, and Fuzzy logic.  The advantage of survival analysis over the other alternatives lies in its unique capability to handle information censoring.  In PHM and other logistics management modeling, information censoring is a near universal phenomenon. Survival analysis should provide new insights and modeling solutions   5  R EFERENCES   Aalen, O. O. 1975. Statistical inference for a family of counting process. Ph.D. dissertation, University of California, Berkeley  Andersen, P. K., O. Borgan, R D. Gill, N. Keiding. 1993 Statistical Models based on Counting Process. Springer  Arsene, C. T. C., et al. 2006.  A Bayesian Neural Network for Competing Risks Models with Covariates. MEDSIP 2006. Proc. of IET 3rd International Conference On Advances in Medical, Signal and Info. 17-19 July 2006  Aven, T. and U. Jensen. 1999. Stochastic Models in Reliability. Springer, Berlin  Bazovsky, I. 1961. Reliability Theory and Practice Prentice-Hall, Englewood Cliffs, New Jersey  Bakker, B. and T.  Heskes. 1999. A neural-Bayesian approach to survival analysis. IEE Artificial Neural Networks, Sept. 7-10, 1999. Conference Publication No 470. pp832-837  Berzuini, C.  and C. Larizza 1996. A unified approach for modeling longitudinal and failure time data, with application in medical mon itoring. IEEE Transactions on Pattern Analysis and Machine Intelligence. Vol. 18\(2 123 


 19 Bollob·s, B. 2001. Random Graphs. Cambridge University Press; 2nd edition. 500pp  Cawley, G. C., B. L. C. Talbot, G. J. Janacek, and M. W Peck. 2006. Sparse Bayesian Ke rnel Survival Analysis for Modeling the Growth Domain of Microbial Pathogens  Chiang C. L. 1960. A stochastic study of life tables and its applications: I. Probability distribution of the biometric functions. Biometrics, 16:618-635  Cox,  D. R. 1972. Regression models and life tables J. R Stat. Soc. Ser. B 34:184-220  Cox, D. R. 1975.   Partial likelihood Biometrika 62:269276  Cox, D. R. & D. Oakes. 1984 Analysis of Survival Data  Chapman & Hall. London  Cressie, N. A. 1993 Statistics for Spatial Data John Wiley Sons. 900pp  Duchesne, T. 2005. Regression models for reliability given the usage accumulation history. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty Y. Armijo. pp.29-40. World Scientific, New Jersey  Eleuteri, A., R. Tagliaferri, L. Milano, G. Sansone, D D'Agostino, S. De Placido,  M. Laurentiis. 2003.  Survival analysis and neural networks. Proceedings of the International Joint Conference on Neural Networks, Vol. 4 20-24 July 2003 Page\(s\:2631 - 2636  Ellison, E., L. Linger, and M Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013, 1997  Fleming, T. R. & D. P. Harrington. 1991. Counting process and survival analysis. John Wiley & Sons. 429pp  Graver, J. and M. Sobel 2005. You may rely on the Reliability Polynomial for much more than you might think Communications in Statistics: Theory and Methods  34\(6\1411-1422  Graves, T. and M. Hamada. 2005. Bayesian methods for assessing system reliability: models and computation. In Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson, et al. pp.41-53  Grimmett, G. 2006 The Random-Cluster Model Springer  Grimmett, G. 1999 Percolation Springer  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis.  Springer. 481pp  Jin Z. 2005. Non-proportional semi-parametric regression models for censored data. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.279-292 World Scientific  Kalbfleisch, J. D. & R. L. Prentice. 1980 The Statistical Analysis of Failure Time Data John Wiley & Sons.  New York. 1980  Kalbfleisch, J. D. &  R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data.  Wiley-InterScience, 2nd ed 462pp  Lisboa, P. J. G. and H. Wong. 2001. Are neural networks best used to help logistic regression? Proceedings of International Joint Conference on Neural Networks, IJCNN 01. Volume 4, 15-19,  July 2001. Page\(s\:2472 - 2477 vol.4  Kauffman, R. J. and B. Wang. 2002. Duration in the Digital Economy. Proceedings of th e 36th Hawaii International Conference on System Sciences \(HICSSí03\ Jan 2003  Kaplan, E. L. & P.  Meier.  1958.  Nonparametric estimation from incomplete observations J. Amer. Statist. Assoc  53:457-481  Klein, J. P. and P. K. Goel 1992. Survival Analysis: State of the Art.  Kluwer Academic Publishes. 450pp  Klein, J. P. and  M. L Moeschberger. 20 03. Survival analysis techniques for ce nsored and truncated data Springer  Krings, A. and Z. S. Ma. 2006.  "Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks MILCOM 2006, Military Communications Conference, 2325 October, 7 pages, 2006  Krings, A. W. 2008.  Survivable Systems.  in Information Assurance: Dependability and Security in Networked Systems Yi Qian, James Joshi, David Tipper, and Prashant Krishnamurthy, Morgan Kaufmann Publishers. \(in press  Lawless, J. F. 1982. Statistical models and methods for lifetime data.  John Wiley & Sons. 579pp  Lawless, J. F. 2003. Statistical models and methods for lifetime data.  John Wiley & Sons. 2nd ed. 630pp  Li, M. and P. Vitanyi. 1997. Introduction to  Kolmogorov Complexity and Its Applications. 2nd ed, Springer  Ma, Z. S. 1997.  Survival analysis and demography of Russian wheat aphid populations.  Ph.D dissertation, 307pp University of Idaho Moscow, Idaho, USA 


 20 Ma, Z. S., and E. J. Bechinski. 2008.  Developmental and Phenological Modeling of Russian Wheat Aphid Annals of Entomological Soc. Am In press  Ma, Z. S. and A. W. Krings. 2008a. The Competing Risks Analysis Approach to Reliability Survivability, and Prognostics and Health Management.  The 2008 IEEEAIAA AeroSpace Conference. BigSky, Montana, March 18, 2008. \(In Press, in the same volume  Ma, Z. S. and A. W. Krings 2008b. Multivariate Survival Analysis \(I\e Shared Frailty Approaches to Reliability and Dependence Modeling. The 2008 IEEE-AIAA AeroSpace Conference. BigSky Montana, March 1-8, 2008 In Press, in the same volume  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(II\ Multi-State Models in Biomedicine and Engineering Reliability. 2008 IEEE International Conference on Biomedical Engineering and Informatics BMEI 2008\27th-30th, 2008 Accepted   Mani, R., J. Drew, A. Betz, P. Datta. 1999. Statistics and Data Mining Techniques for Lifetime Value Modeling ACM Conf. on Knowledge Discovery and Data Mining  Mazzuchi, T. A., R Soyer., and R. V Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Meeker, W. Q. and L. A. Escobar. 1998. Statistical Methods for Reliability Data. Wiley-Interscience  Munson, J. C. 2003. Software Engineering Measurement Auerbach Publications  Nelson, W. 1969. Hazard plotting for incomplete failure data J. Qual. Tech 1:27-52  Nakagawa, T. 2006.  Shock and Damage Models in Reliability Theory. Springer  Osborn, B. 2005. Leveraging remote diagnostics data for predictive maintenance.   In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp. 353-363  Pena, E. A. and E. H. Slate. 2005. Dynamic modeling in reliability and survival analysis. In "Modern Statistical and Mathematical Methods in Reliability". Edited by A. Wilson N. Limnios, S. Kelly-McNulty, Y. Armijo. pp.55-71  Reineke, D. M., E. A. Pohl, and W. P. Murdock. 1998 Survival analysis and maintenance policies for a series system, with highly censore d data.  1998 Proceedings Annual Reliability and Maintainability Symposium. pp 182-188  Schabenberger, O. and C. A. Gotway. 2005. Statistical Methods for Spatial Data Analysis.  Chapman & Hall/CRC  Severini, T. A. 2000. Likelihood methods in statistics Oxford University Press  Shooman, M. L. 2002. Reliability of Computer Systems and Networks: Fault Tolerance, Analysis and Design. John Wiley and Sons. 551pp  Stillman, R. H. and M. S. Mack isack, B. Sharp, and C. Lee 1995. Case studies in survival analysis of overhead line components. IEE Conferen ce of the Reliability and Distribution Equipment. March 29-31, 1995. Conference Publication No. 406. pp210-215  Therneau, T. and P. Grambsch. 2000 Modeling Survival Data: Extending the Cox Model Springer  Wilson, A.  N. Limnios, S Kelly-McNulty, Y. Armijo 2005. Modern Statistical and Mathematical Methods in Reliability. World Scientific, New Jersey  Xie, M. 1991. Software Reliability Modeling. World Scientific Press    B IOGRAPHY   Zhanshan \(Sam\ Ma holds a Ph.D. in Entomology and is a Ph.D. candidate in Computer Science at the University of Idaho. He has published approximately 30 journal and 30 conference papers, mainly in the former field.  Prior to his recent return to academia, he worked as senior network/software engineers in software industry.  His current research interests include reliability and survivability of wireless sensor networks, fault tolerance survival analysis, evolutionary game theory, evolutionary computation and bioinformatics  Axel W. Krings is a professor of Computer Science at the University of Idaho.  He received his Ph.D. \(1993\ and M.S 1991\ degrees in Computer Science from the University of Nebraska - Lincoln, and his M.S. \(1982\ in Electrical Engineering from the FH-Aachen, Germany.  Dr. Krings has published extensively in the area of Computer Network Survivability, Security, Fault-Tolerance and Realtime Scheduling. In 2004/2005 he was a visiting professor at the Institut d'Informatique et MathÈmatiques AppliquÈes de Grenoble, at the Institut National Polytechnique de Grenoble, France.  His work has been funded by DoE/INL DoT/NIATT, DoD/OST and NIST 


