  Human Flesh Search Model Incorporating Network Expansion and GOSSIP with Feedback  Bing Wang, Bonan Hou, Yiping Yao, Laibin Yan School of Computer Science National University of Defense Technology Changsha, P.R. CHINA wangbing, bnhou, ypyao, lbyan}@nudt.edu.cn   Abstract With the development of on-line forum technology and the pervasive participation of the public, the Human Flesh Search is becoming an arising phenomenon which makes a great impact on our daily life. There arose big research interests in social, legal issues resulted from HFS, however very little work has been conducted to understand how it comes into being and how it dynamically evolves. This paper proposes a modeling and simulation approach incorporating 
network expansion and GOSSIP propagation with feedback for a better understanding of the human flesh search phenomenon. Based on the acquisition and analysis of the netizens’ surfing behavior data, the evolution of the HFS is modeled as a network growth process  with proper dynamic input, which is characterized by heavy-tail and burst-oriented distribution, modeling as a Weibulloid process. Then, an improved GOSSIP model with feedback is proposed to represent the information propagation, processing and aggregation during the HFS. New insights for HFS are gained through a set of simulation experiments Keywords-human flesh search\(HFS\ocial network comeplex network; distributed problem solving; crowdsourcing GOSSIP  with feedback I  
 I NTRODUCTION  The uncovering of fake pictures of “South China Tiger and series of events happened during the past two years show the great power of crowds of netizens online. These eye-catching phenomenons, rising recently over the Internet are called Human flesh searching \(HFS\, which is literally translated from the Chinese phrase ‘RenRouSouSuo’ [3 Human flesh search is a mass campaign, which comes into vogue  through the medium of internet, targeting at searching for the identity of a certain  person or the truth about a certain event, whose data collection depends partially on the human force to filtering the information gained from the 
search engine , and partially on the anonymous or real-name information announcement, “Human flesh” illustrates that the role of human force played in the search and the distinction between itself and the traditional machine-based search engine W ith t h e adve nt  of We b 2  0 tech n o log y  and the online community, everyone can publish, share review, and comment at his/her wish. An interesting or controversial topic would always attract a great number of online participants gathering together to dig the “truth” out This process usually involves hundreds and thousands of netizens and occurs in a relatively short time, e.g., in several hours or a couple of days. Surprisingly, in most cases, the 
answer they pursue is accurately found out, no matter how hazy the problem seems to be Although HFS has attracted numerous controversies and even triggered legal dilemmas between privacy violation and public opinion, it has already become a phenomenon of profound impact. It is necessary to acquire better understanding of its structure and evolving process. However the HFS is very complex due to the intrinsic nature of the phenomena itself. Despite a great interest has been paid on the HFS phenomena, little knowledge on how it arise, how people involved are organized and how to describe the evolution process have already been gained Several researches have been conducted on the Web 2.0 
related phenomenon, such as blogospher cr o w ds ourc in g  22  Gua n xi [1 5], to n a m e a few  Co m p le x Ne two r k theory has been adopted as a powerful approach to study these systems that are best described as networks with complex topology. Different variants of complex network have been proposed. Traditionally, random network, where the vertices are randomly connected, was widely used, but the predictions were rarely justified in the real world. Smallworld models were introduced by Watts and Strogatz [1 o  describe the six degree of separation. The scale-free \(SF netw r opo s e d by Bar a b  s i a n d Albe r t are r e g a r d ed  to be the best suitable models for large amount of systems 
from the Internet to social networks. SF networks are characterized by the vertex connectivity following a powerlaw distribution These approaches could be utilized as an infrastructure of the HFS phenomenon, but great challenges are still in the way towards a better understanding of the phenomena. First it’s still hard to define and classify the HFS. As a novel and arising phenomena, the HFS overlaps multi-principles including Crowdsourcing, Social Search i s t r i b u t e d  Problem Solving, and collective intelligence o n d  some characteristics of HFS, such as the vast number of participants and the dramatic evolving process, bring difficulties to a well explanation through traditional social 
research methods. For these reasons, quantitative analyse through modeling and simulation of the phenomena and reproducing the dynamic process, is a promising approach to acquiring new insights In this paper, we propose a network expansion model to describe the HFS, focusing on the growth and evolution of the network topology formed by the posted messages 2009 13th IEEE/ACM International Symposium on Distributed Simulation and Real Time Applications 1550-6525/09 $26.00 © 2009 IEEE DOI 10.1109/DS-RT.2009.36 82 


  Further, we improve the GOSSIP model with feedback by taking the propagation, the processing and the aggregation of the information into consideration The remainder of this paper is structured as follows. In section II, we explain the background and related works especially the HFS phenomena and the participation pattern of the netizens in HFS. Section III proposes a network expansion model of HFS process with proper dynamics input Section IV describes an improved GOSSIP model with information feedback. Section V discusses the results of the HFS simulation. Finally, conclusions and future works are described in section VI  II  B ACKGROUND AND R ELATED W ORK  With the rapid prevalence of Internet, the number of netizens is rapidly increasing. According to the “24th Statistical Survey Report on the Internet Development in China”, by the end of June of 2009, the amount of Internet users in China had reached 338 million, with the increase rate of 32.1% during the past half year Ad dit i o n al l y  online Forums, BBS, Blogs and such social network sites SNS\ facilitate people to express their thoughts, voice their opinions, and share their experiences and ideas. These media are easily and widely accessed by the public, even more every netizen could be both information provider and reader The content created by one netizen could be seen by others immediately, which may intrigue further reactions. The interactions among huge amounts of people have emerged collective wisdom” or crowdsourcing which acts as the storehouse of overwhelming amounts of knowledge about the topic. All these conditions fertilize the rising-up of HFS Figure 1 shows the popularity of the HFS in China in recent years. The data reflects the search amount of HFS worldwide through Google, the keyword is typed in Chinese and the result is gained from the Google trend analysis http://www.google.cn/trends  Figure 1  The google trends for “RenRouSouSuo”\(HFS It is still hard to give the definition of HFS. As a Web 2.0 related phenomena, however, HFS has similar characteristics with crowdsourcing, social search, collective intelligence and distributed problem solving. HFS is a particular form of crowdsourcing, outsourcing a task to an undefined, generally large group of people or community in the form of an open call [2 l s o it c a n b e tr ea ted as a dis t r i b u ted pro bl e m  solving paradigm which takes advantage of numerous voluntary contributions to acquire the right answer. HFS can also be classified into social search in which users utilize social interactions with others in the search  The HFS dynamics is jointly determined by the netizens Internet information seeking habit and the popularity of social network sites. The typical process of human flesh search is described below First, someone publishes an open question, or part of a question, with some information or clue, such as pictures or videos on the Blog or online forums, the content of the question may be of interest or controversy, clear or opaque Then, the online netizens respond while browsing these contents. Some volunteers would transship it to several other large social network websites \(tianya.cn, mop.com etc  would issue an open call to dig the truth out, which in turn intrigues more outraged reaction. Many anonymous participants would bid against the topic and several key clues to the issue were gathered and analyzed. Although most of the topics on the internet will vanish, several hot topics maintain a measureless social influence The characteristics of HFS could be summarized as follows  Accessibility the online media can be easily reached by all, and the content can be shared among numerous netizens immediately after it was created. The content may be transshipped among several mainstream social network sites by volunteers  Popularization the problem of HFS, such as looking for the right person through some pictures or other ambiguous clues, is often not too hard to need creative talent and can be easily solved once enough attention and therefore enough information was gathered according to the smallworld phenomeno aly sis r e sul t s s how t h at a lar ge percent of responds do not contribute to the problem directly but just similar to the mutation in Biology or genetic algorithms, they are not absolutely useless. Rather, in some degree, the scale of the participant population plays an important role on whether the problem could be solved or not in tolerable time  Centerlessness there is no central control on the topic in the process of HFS and some digressions are unavoidable  Information Timeliness the timeliness is vital to the HFS life cycle. The topic can easily attract public attention, but also suffer quickly outdating  Convergence Though under real-time constraint crowd perform worse than single-person decision, providing enough time and continuous attention, the HFS will somehow achieve the goal The HFS is the collaboration among the online mass towards a common goal. Underneath the bustle surface, there is an active virtual social network containing many individuals with frequently interactions Some related works have been conducted to study the online community activities. Yongqin Gao et al 8 o cu sed on the Open Source Software developers and project network and proposed a modified scale-free network modeling approach. Nitin Agarwal et al 9 so m e  key elements of research in Blogosphere. Other attractive researches includes social network, communication pattern  n fo r m a t i o n p r op a g a t i o n 12 o n line a c t i vi t i es 7 an d so on 83 


  In the current literatures, there is still not much work devoted to the research of HFS phenomena with quantitative modeling and simulation techniques. Due to the intrinsic complexity, a moderate abstract and intuitionistic perspective will facilitate better understanding. In this work we combined the network forming up with information propagation, and thus propose a prototype HFS model to provide new insights into this rising phenomenon III  THE HFS  P ROCESS AS NETWORK E XPANSION  In this section, we start by collecting data from several HFS topics on some famous social network sites. Based on the analysis of the data, the activity pattern distribution of the participation in a HFS is shown A  Data Collection Social network sites are selected as data sources for the research of HFS. Utilizing web crawling techniques, we collected a random set of hot HFS topics since 01/03/09 from online forums at www.tianya.cn, one of the largest online communities in China. Each topic contains a list of the originate message, replies, and iterative replies to replies The dataset of each reply including the author ID, the date and the text content are recorded for further analysis The statistics of these topics indicate that there exist differences in their shape scale and lifetime, but sharing a common evolving trend. Right after the topic is created, the number of responds increasing quickly. As time goes across the turning point, the growth slows down. Figure 1\(a\cts the accumulative participants during the life cycle of a typical hot topic. The according histogram of responds in each unit of time is shown in \(b\ndicating the burstoriented and heavy-tail nature of HFS dynamic patterns     0 2 4 6 8 10 x 10 4 0 200 400 600 800 1000 T Accumulative Nodes   Probability Density Function Histogram Weibull t 80000 70000 60000 50000 40000 30000 20000 10000 0   f\(t 0.24 0.22 0.2 0.18 0.16 0.14 0.12 0.1 0.08 0.06 0.04 0.02 0   a b Figure 2  a\ The accumulative number of responds to a typical HFS evolving along with time. \(b\ The HFS’s histogram of activity pattern and its probability density function with Weibull distribution fitness a 0.60122 b 4.5126 B  Input Modeling As an essential part of the HFS network model, proper input modeling is a precondition for validity [16 D u ri ng th e  model construction, a new node is created and added to the network at each step. In literature, it is often assumed that the arrival of new nodes is a Poisson process [1 where th e  probability of a number of replies events occurring with a fix average rate and independently of the time since the last event. After an investigation on the topic posted and replied the statistical results show that the probability of new replies and how many replies are strong related with the time eclipsed. As the burst-oriented nature of human behavior  the time intervals between consecutive online activities follow heavy-tailed process which decays slowly, in contrast to the exponential expected for Poisson processes. Further Dezso et al 7 a nal y zed the dy na m i c access on web p o rta l s found both the popularity and accessibility are equally important, and assert the news document visitation behavior is best approximated by a power law  nt t  with 0.3 0.1  Since the news portals are different from most of the online forums, in which the most recently updated \(either through create or reply\ are always aligned as the first of topic list on the front page, while the deprecated topics will go down according to their rank. Yet, it’s necessary to identify the distribution with time intervals between consecutive updates Previous observation in Figure 2 \(b\has shown that the dynamic accesses quickly burst at the beginning, then slowly decay, heavy-tailed process, and the number of activities \(n in unit time is better approximated by the Weibulloid process in \(1 1    t t nt e  1 84 


  Where  are related with the characteristics of the concrete topic, for example, in Figure 2 a equals 0.60122 and b equals 4.5126 C  Assumptions and HFS Network Model A wide range of social systems share some properties of complex networks. Traditionally, random network of ER model were widely used to describe these complex systems which assumes the vertices are randomly connected with probability p After investigating the topology of several large networks, as WWW and citation patterns, Barabási et al 2 found  they are b e tt er describe d a s t h e sca l e f r e e  network, in which the vertex degrees follow a scale-free power-law distribution, which is p   kk with 2 3 in contrast to the exponential distribution expected in random networks. Afterwards, the instant messaging, email web subsequently have been empirically approved as scale-free network [6 We model the participation and their interaction in the HFS as network, in which nodes represent user participation and links represent the interactions occurred between nodes Once a message is posted, replies would be inspired and attached to the message. Even more, the reply may be followed by further replies, because the reply itself is also a message. As the HFS goes on, the network expands as new messages are added and new links formed For the first step, we ignore the sophisticated semantics among the textual messages. Instead, these messages are simply classified as approval, disapproval, and critical clue Additionally, we apply James Surowiecki’s asserts, in his book Wisdom of the Crowds which declares that providing that the process is sound, the more people involve in solving a problem, the better the result will be [17 Follow i ng t h is  philosophy, we take the total number of participants and the maximum degree as the measurements for the successful accomplishment of a HFS. That is to say, in many situations the more people involve in the HFS and the more convergent the participations are, the higher probability it will finally be solved The next question is how to construct the network in order to simulate the process of the HFS evolving process There are several mature paradigms could serve as the skeleton, in many cases, the relationship between different participation shows scale-free properties. The Barabási Albert \(BA  m o d e l give s an al gori thm  f o r ge neratin g  random scale-free network using a preferential attachment mechanism. The process is shown as follows 1\ First, the network begins with an initial network of m 0  nodes and the degree of each node should be at least 1 2\ Second, at each time step, a new node is created and is connected to existing nodes according to the principle of preferential attachment, whereby each new node is connected to m of the existing with a probability that is proportional to the degree of the existing node. The probability p i is given by the following formula  ii j j pk k where k i  is the degree of node i  The BA model was modified in our simulation to build the HFS network. Based on the observation that new nodes are not always added at constant frequency in reality modification is made at the step 2 to capture the evolving nature of the HFS network  IV  I MPROVEMENT THROUGH G OSSIP WITH F EEDBACK  The network based HFS model built in previous section has not taken the information propagation in actual situation into consideration. In the procedure of HFS, along with the expansion of the participation, the information propagation takes place. Being different from the original Gossip paradigm, the nodes in the HFS network not only carries out the propagation, but also contributes to the search progress and provides feedback to predecessors. Based on a Gossiplike expansion of the participation network, the information processing, content contribution and feedback are introduced to enhance the credibility of the model In the improved model with Gossip, each node in the network is modeled as an autonomous agent with the capability of information processing. When new nodes are added to the network, they will absorb the information flowing from their neighbors. The information will be assimilated or may be mutated as the influence of different beliefs hold in each agent. Then, the most important is that the agents can provide feedback which is vital to the goal of HFS. The information will provide feedback in the cascade form until it reaches the hub. The information aggregation occurred as the result of cascade feedback. Once enough information is fed back and properly aggregated, we can say the problem of HFS is solved. The process of modified gossip model is illustrated by Figure 3. Before we get into the feedback gossip algorithm, we need to formally define the problem/solution, information propagation, mutation, and aggregation  hub node link 1. Newly added node 2. Information flow 3. Feedback 4. Information aggregation  Figure 3  The cycle of information flow, feedback and aggregation for the HFS network The Problem under HFS is defined as an n-dimension vector, with each dimension as one factor in the Problem Space n S illustrated by the following formula. If certain dimensions are not clear yet e is marked 85 


  123       ni n n n Qqqq qqNe QS SNeNe Ne     2 Then, we define the information propagation as 1  neighbour m j jn n PS Sm Set 3 Propagation happens when a new node is added to the network. The newly added node gains a new Problem instance according to the Problem instances in the neighbor nodes Once the agent got the problem, it will contribute to the problem solving progress with its own knowledge through the mutation process in \(4  j nn M SS   4 Where j M works as bellows 1\ Select k Set s.t  0 k s et k k N k n  randomly 2\ For each i q in k s et  ii qfq the mutation function   f Ne Ne   3\  Select a subset of the n dimensions k Set  4\ For each dimension selected, utilize function f to reflect the contribution from a node in the Gossip-based information propagation As the information is fed back cascadingly, it will result in the problem instance ready for the aggregation on the predecessor’s side. The information aggregation is defined in 5 1  neighbour m j jnn n Aggregation S S S m Set  5 When the number of solved problem dimensions has reached certain threshold, it can be declared that the HFS is convergent The modified GOSSIP model with information feedback is shown in pseudo code 1  Algorithm 1 GOSSIP protocol with information feedback  1 while network expanding 2        add a new node to the network 3        information propagation from neighbors 4        information processing and mutation 5 for each neighbor, trigger feedback 6            neighbor.aggregation 7            neighbor.Feedback\(cascadingly 8 end  9  V  S IMULATION E XPERIMENTS AND R ESULTS  We have implemented a prototype simulation integrating the above model and conducted experiments with different parameters value as a preliminary demonstration. JUNG network modeling library was utilized to support visualization. The results reveal the structure of the evolving network, highlighting connectivity, clustering and strengths of relationships among user participations. Animated output allows viewers to see the evolution of the social network over time Figure 5 illustrates the simulation of the network structure during the evolving process of a typical HFS topic At the initialization of the simulation, a new message is created as an initiator. After a relatively short period, a number of netizens are attracted and participated into the action by reviewing, commenting and copy the topic to another website. The information flows and feedbacks among these nodes. Also, the information is accumulated and aggregated during the expansion process. As time advances, more and more participants are intrigued to join in Additionally, some nodes with large degree formed the hub of information aggregation. The increasing number of participants follows a decaying and long-tailed process Panel \(f\ indicates the accumulative nodes and maximum degrees in the network With the formula \(2\ynamic input to the model, we got results with different pair of parameter values, as illustrated in figure 4. The solid lines indicate the accumulative participations occurred in the corresponding simulated HFS event, while dash lines mean the maximum degree the nodes in the network have. The relations between the line shape and parameters could be utilized to better identify the characteristics of HFS. Also, it is obvious that the HFS will converge from a tripping point as the number of participation increasing. The large accumulative participation is important on the convergence of the HFS, although a relatively small percent really contributing to the problem  0 10 20 30 40 50 60 70 80 90 100 0 50 100 150 200 250 300 350 400 450 Steps Number of Nodes          C\(a=1,b=1 M\(a=1,b=1  M\(a=1.5,b=1  C\(a=1.5,b=1 C\(a=2,b=1 M\(c=2,b=1  M\(a=2.5,b=1  C\(a=2.5,b=1 C\(a=1,b=1.5 M\(a=1,b=1.5  M\(a=1,b=2  C\(a=1,b=2 C\(a=1,b=2.5 M\(a=1,b=2.5   Figure 4  Different pair a,b arameter values and their corresponding accumulative nodes \(real line\and maximum degree \(dash line\during the network expansion  86 


    a\step b\step c\step   0 10 20 30 40 50 60 70 80 0 50 100 150 200 250 300 Steps A ccumu l at i ve no d es  d e\0 step f\ccumulative nodes Figure 5  The Evolving Process of the HFS Network a 0.5 b 2.0 VI  C ONCLUTION AND F UTURE W ORK  The recently arising HFS phenomenon attracts attentions from sociologists, psychologists, legalists, and computer scientists. It poses great challenges as its complex structure and dramatic evolving process, and there is still little research has been devoted to studying how it comes into being and dynamically evolves. This study demonstrated a modeling and simulation approach to a detailed, quantitative understanding of the HFS. Based on the empirically investigation, the HFS model was built incorporating network expansion and GOSSIP with feedback. The netizen participation dynamics is properly modeled. It quickly bursts at the beginning, and then slowly decays as a heavy-tailed process. Additionally, a modified GOSSIP diffusion algorithm with feedback was proposed to better describe the information flow, which forms the HFS problem solving This paper facilitates the gain of new insight from the HFS phenomena. Still more problems are left wanting for further investigation. For example, the scale-free property of the HFS needs further verification; and the psychological motivation of the participants during a HFS process also deserved more attention A CKNOWLEDGMENT  This work is supported by the National Natural Science Foundation of China \(NSF\ Grant \(No.60773019, No 60873120\ and the Ph.D. Programs Foundation of Ministry of Education of China \(No. 200899980004  R EFERENCES  1  Gabriele D’Angelo, Ste-fano Ferretti. “Simulation of Scale-Free Networks,” SIMUTools ’09, Rome, Italy, 2009 2  A.-L. Barabási and R. Albert. “Emergence of scaling in random networks”. Science, 286, 1999 3  Times ONLINE. Human flesh search engines: Chinese vigilantes that hunt victims on the web. [O  A v a ilable  http://technology.timesonline.co.uk tol/news/tech_and_web article4213681.ece.2008 4  RenRouSouSuo. 2008. [O v ailable  http://zh.wikipedia.org/wiki/%E4%BA%BA%E8%82%89 E6%90%9C%E7%B4%A2.\(in Chinese 5  A.-L. Barab&aacute;si, "The origin of bursts and heavy tails in human dynamics," May 2005. [Online A v a ilable   http://arxiv.org/abs/cond-mat/0505371 6  Re´ka Albert* and Albert-La´szlo´Barabási. Statistical mechanics of complex networks. REVIEWS OF MODERN PHYSICS, VOLUME 74, JANUARY 2002   7  Z. Dezsö, E. Almaas, A. Lukács, B. Rácz, I. Szakadát, and A. L Barabási, "Dynamics of information access on the web Physical Review E \(Statistical, Nonlinear, and Soft Matter Physics vol. 73 no. 6, pp. 066 132+, 2006 8  Y. Gao and G. Madey, "Towards understanding: a study of the sourceforge.net community using modeling and simulation," in SpringSim '07: Proceedings of the 2007 spring simulation multiconference San Diego, CA, USA: Society for Computer Simulation International, 2007, pp. 145-150. [O A v ailable  http://portal.acm.org/citation.cfm?id=1404703 9  N. Agarwal and H. Liu, "Blogosphere: research issues, tools, and applications SIGKDD Explor. Newsl vol. 10, no. 1, pp. 18-31 2008 10  H. Wei H. Xiao-Pu Z. Tao and W. Bing-Hong Heavy-tailed statistics in short-message communication Chinese Physics Letters  87 


  vol. 26, no. 2, pp. 028 902+, 2009. [Onlin a b l e http://dx.doi.org/10.1088/0256-307X/26/2/028902 11  D. J. Watts and S. H. Strogatz, "Collective dynamics of `small-world networks Nature vol. 393, no. 6684, pp. 440-442, June 1998  http://dx.do i.org 10.1038/30 918  12  Y. Fernandess and D. Malkhi, "O n spreading recommendations via social gossip," in SPAA '08: Proceedings of the twentieth annual symposium on Parallelism in algorithms and architectures  New York, NY, USA: ACM, 2008, pp. 91-97. [Onlin l e  http://dx.doi.org/10.1145/1378533.1378547 13  Reka Albert and Albert-L´aszl´o Barabási. Statistical mechanics of complex networks. Review of Modern Physics, 74\(1\7–97, 2002 14  E. M. O’Grady, M. Rouleau, M. Tsvetovat. Network Fracture: How Conflict Cascades Regulate Network Density Agent 2007 Chicago IL, 2007 15  Barry Wellman, with Wenhong Chen and Dong Weizhen Networking Guanxi. Social Networks in China: Institutions, Culture and the Changing Nature of Guanxi. Cambridge University Press 2001 16  Banks, J., J. S. Carson, B. L Nelson, and D. M. Nicol. “Discreteevent system simulation”. 4th ed Upper Saddle River, New Jersey Prentice-Hall, Inc. 2005 17  Satnam Alag. Collective Intelligence in Action. Manning Publications Co. 2009 18  Brynn M. Evans, Ed H. Chi. Towards a Model of Understanding Social Search. CSCW’08, San Diego, California, USA. November 8 12, 2008 19  China Internet Network Information Center \(CNNIC\24rd Statistical Survey Report on the Internet Development in China. June. 2009 Online Available:http://research.cnnic.cn/img/h000/h11/attach200907161306 340.pdf 20  J. Howe, "Wired 14.06: The rise of crowdsourcing." [O Available: http://www.wired.com/wired/archive/14.06/crowds.htm 21  L.-H. Liu, F. Fu and L. Wang. Information propagation and collective consensus in blogosphere: a game-theoretical approach. eprint arXiv:physics/0701316 22  Crowdsourcing online, August 2009. [Onlin v ailable  http://en.wikipedia.org/wiki/Crowdsourcing    88 


The interactions between roles are also modeled. In the figure BookPicShow is realized by three roles PicShowControl  ImageFetch and ImageContainer  ImageContainer is an image container for picture visualization ImageFetch is to fetch image data, and PicShowControl takes the responsibility of controlling image fetching and showing. So, we can see that ImageFetch is involved by PicShowControl which means the former is activated by the latter, and PicShowControl accesses ImageContainer to set the image. They are the interactions between roles from a same feature. On the other hand PicShowControl is involved by BookSearch to take effect. It’s the interaction between roles from different features. In particular, the role BorrowProcess controls the execution process, so it involves all the roles mapped from the sub-actions in the composition action Borrow  We also model the role interactions related to abstract roles \(see in section 3.1.3\This kind of interactions usually crosscut multiple parts of a system For example, in figure 3 Log will be informed to activate by all the events of readers, e.g AddReaderRec  DelReaderRec etc. So, abstract role ReaderEvent is modeled to generalize the two roles and to simplify the interactions towards them Roles are instantiated by different kinds of assets in the bottom level. For instance, the role SetBookAmt is instantiated by the setAmount interface in the component Book The role Penalty Param corresponds to the penalty segment in a configuration file which is assessed by the Calculate Penalty role. The role ImageContainer as a resource role, is instantiated by a component Canvas as well as a code fragment to be used for its initialization in UI. The two RewardPoint Calculation Type roles are optional roles and they are realized by two methods which are not included in any component but only one of the methods will be composed into the base programs. The property Minimal Storage can be instantiated into an interface accessing the related variable in component Book The role log is instantiated as a log method to perform its function as well as a schema to create the log table in database for storing operation records         Figure 3. The traceability model on library management domain 
217 


5. Discussion about th e feature-oriented traceability model  In this section, we will briefly discuss the featureoriented traceability model: why we adopt the feature implementation model and ignore the architecture, how the model instructs the product derivation and SPL evolution phases. On the other hand, related tools are presented  5.1. Feature implementation model instead of SPL architecture  A product line architecture \(PLA\pecifies the architecture for a set of closely related software products u s u a l l y i n t e r m s of co m p o n en t s  connectors and configurations A P L A  f o cu s e s on modeling the variability for a domain and promotes the reuse in the SPL development. However, in the traceability context, feature tangling and scattering still exist between the feature model and the PLA, i.e several components may contribute to a single feature and a single component may contain implementations for several features. Therefore, it is difficult to explain how the functions of features are splitted as well as what is the intrinsic semantic of the feature interaction In our model, the feature implementation model is introduced to replace the complex many-to-many traceability between features and implementation artefacts with two sets of clear trace links. Roles that decompose features and the inter-relationships between the feature parts \(role interactions\o defined Role model provides the design decisions from the viewpoint of system designers, which are not concerned by the requirement analysers. It also captures the inner structure of a feature and records the semantic of feature implementation \(reason for splitting feature functions\ and feature interactions \(the intrinsic relationships between features\ a finer level Furthermore, roles are explicitly assigned to different implementations artefacts including components and other forms of implementations. Thus, the semantic for traceability from requirements to implementations is complemented and extended  5.2. Traceability-based product derivation and SPL evolution  Traceability is the basis of product derivation because we need to find out the variability-related program implementations according to the variable requirements and combine them with the base implementations [9  I n  o u r m ech an ism  r a th e r th a n  feature-driven customization and composition, we involve traceability-based role level customization and program-level composition. The first step is to decide whether the variable roles will be included in the final product. Some can be determined directly by the related features’ bounding status, others especially the internal variability-r elated roles should be customized by developers in role-level since they are unconcerned by clients. The next step is to select and configure the variability-related implementation artefacts according to the role instantiation traceability. In particular, role interaction, as an important kind of traceability, guides the program-level composition, which is to instruct what kinds of program implementations should be composed and how they can be composed using AOP mechanis   Traceability also plays an important role in SPL evolution. We are able to locate the features, roles and program implementations that are involved in the evolution through traceability and then make decisions on the evolution of the models, the implementations, as well as the traceability links. Usually evolution is driven by two factors. One is the requirement changes the other is bug fixing. Our mechanism is useful in these perspectives for it assists change impact analysis to identify the involved part of a SPL driven by a specific evolution request  5.3. Tool support  The tools OntoFeature  a n d FDAPD featuredriven and aspect-based product derivation tool   are developed to support the manual traceability capturing and visualizing. They are now separated and are used by different stakeholders \(requirement analyzer, system designer\n the SPL development process OntoFeature is a graphic feature modeling tool used to accomplish the ontology-based feature modeling which includes actions, facets and the relationships between these elements. FDAPD is a role modeling tool and is integrated with OntoFeature. It captures all the features and dependencies in the previous tool and provides graphic editing space for each of the features, where the roles, interactions and the corresponding implementation artefacts can be modeled. In practice, the two tools are cooperated, i.e OntoFeature for representing requirements and FDAPD for further design and implementation FDAPD is also developed to accomplish the role customization and program-level composition by invoking the AOP mechanism i.e. the variabilityrelated implementations are selected as aspects to be woven into the base programs according to the interaction types. The detailed composition process can be referred to  
218 


6. Conclusion and future work  In this paper, we focus on the visualized representation of the traceability in a software product line, and introduce a comprehensive feature-oriented traceability model. The model explicitly represents the product line artefacts in different abstraction levels. It also contains various kinds of traceability links explicit/implicit\mong the artefacts. Based on it, the traceability information from requirements to implementations is extended, described in finer-grained levels. In the whole model, the feature implementation model is innovative that it integrates the knowledge of both business logics and implementation techniques, i.e how the features are splitted and what is the intrinsic semantic of the feature interaction. This level helps to reduce the key problem of the mapping between the problem space and the solution space by providing evidences that the relationships between features and programs can be reasonably obtained through a third party ‘role’ rather than simply connecting the two sides As a result, the new concep t contributes to the ideal mechanism that the traceability of a system can be clearly presented However, the method we propose is far from mature in the following perspectives 1\The model is an informal representation model Now the models and the traceability links are manually identified and recorded by means of the developed tools. However, the automatic traceability capture deduction and validation relying on a formal basis are not available thus they are expected in the future work 2\When the product line grows larger, the traceability model will be farther complex and difficult to define. However, the problem cannot be avoided perfectly unless we make the model focus on the core part of a product line rather than the whole boundary This tailored model can be expected to describe the variable part in detail while the base program curtly. It will also be our future work that we think it will be helpful in the variability management domain especially tracing the variabil ity at different abstraction levels 3\The meta-model can be extended to other SPL development perspectives. For example, traceability related to testing is necessary in many development processes. Thus, we will complement the current traceability on design and implementation with the trace to testing artefacts in the future work  Acknowledgments This work is supported by National Natural Science Foundation of China under Grant No 60703092, 90818009, and National High Technology Development 863 Program of China under Grant No 2007AA01Z125  7. References  1 D  M  We is s a nd C  T. R   La i S of tw a r e Pr o duc t Line  Engineering: A Family-Based Software Development Process”, Addison-Wesley, 1999  te l a n d A.Fi nke lste i n  A n Ana l ysis of t h e  Requirements Traceability Problem”, in Proceeding of 1st International Conference on Requirement Engineering, 1994  Pe n g We ny un Zha o Y unjia o Xue a n d Yijia n W u   Ontology-Based Feature Modeling and ApplicationOriented Tailoring”, in Proceedings of International Conference on Software Reuse \(ICSR2006\, pp.87-100 4 n P e ng  L i wei S h en W e n y u n Z h ao F eat u r e Implementation Modeling based Product Derivation in Software Product Line”, in Proceedings of International Conference on Software Reuse\(ICSR2008  b isc h R.Brc i na  O pti m iz ing De sign f o r Va ria b ilit y  Using Traceability Links”, in Proceedings of International Conference on Engineering of Computer Based Systems ECBS2008\, pp.235-244 6  L a go  E  Ni emel a H Van Vl i e t  T o o l S u pp o r t f o r  Traceable Product Evolution”, in Proceedings of the Eighth European Conference on Software Maintenance and Reengineering \(CSMR2004\, pp.261-269 7 M.R i e b is c h  S u p p o r t i ng E v oluti ona r y D e ve lo pm e n t b y  Feature Models and Traceability Links”, in Proceedings of IEEE International Conference and Workshop on the Engineering of Computer-Based Systems \(ECBS2004\ pp 370-377  y sne i ros  J.Le ite   N onf un c tiona l Re q u ire m e n ts: Fr om  Elicitation to Conceptual Models”, in IEEE Transactions on Software Engineering, Vol. 30, No. 5, May, 2004 9 A  G  J  J a ns e n R  Sm e d i nga  J  va n G u r p a n d J  B o s c h   First class feature abstractions for product derivation”, in IEE Proc.-Softw., Vol. 151, No. 4, August 2004 10 W e i Z h ang  H o ng  M e i  H a i y an Z h ao   F eat u r ed r i v e n  requirement dependency analysis and high-level software design”, in Requirements Eng \(2006\ Vol.11, pp: 205–220  M  A l e k s y  T.H ile n b r a nd C  O b e r gf e ll, M  Sc hw i nd A  Pragmatic Approach to Traceability in Model-Driven Development”, in Proceedings of the Multikonferenz Wirtschaftsinformatik 2008 \(MKWI2008  B  R a m e s h  M.J a r k e   T ow a r ds r e f e r e nc e m ode ls f o r  requirements traceability”, in IEEE Transactions on Software Engineering, 2001, 27\(1\:58 14 J Bo sch Desi g n an d Use o f S o ft ware Arch i t ect u r es  Adopting and Evolving a Product Line Approach”, Pearson Education \(Addison-Wesley & ACM Press\, May 2000 15 N. M e dv i d o v i c R  N T a yl o r  A C l assi f i cat i o n and Comparison Framework for Software Architecture Description Languages”, in IEEE Transactions on Software Engineering, 2000. 26\(1\: p. 70-93 16 Yi j u n Yu  Yi q i ao W a ng  J M y l o p o u l o s  et al  Rev erse  Engineering Goal Models from Legacy Code”, in Proceedings of IEEE International Conference on Requirements Engineering \(RE2005\, pp.363-372  
219 


Automated Windowed Coefficient Trending Earlier we attempted to find explanatory models whose coefficients changed over time in a predictable way This would then let us predict future coefficient values for these models and apply these trended models towards future estimates The problem was that we were not able to find such stable models at least manually We automated our search for stable models by building a system we refer to as the Automated Coefficient Trend Search ACTS tool A patent is currently pending for the method implemented in this software Against the Far Out data set ACTS is designed to automatically permute fields run chronologically-windowed regressions with the chosen fields establish linear and nonlinear trends for the resulting coefficients and then use these trended coefficients to estimate a future set of data ACTS also automatically tries up to seven different transformations on any given dependant or independent variable The length of calibration windows and the increment between them are automatically varied ACTS establishes most promising coefficient trends and also rates the performance of predictions done using trended-coefficient models With so many parameters transformations windows length and intervals to evaluate ACTS needed to search a huge problem space Despite assembling a cluster of 8 computers to run ACTS if necessary for months the problem still required pruning Each run of ACTS was therefore restricted to producing models with a different number of variables between 3 and 15 We quickly found that the best models contained very few parameters due to over-fitting on wider models permitting us to pare down the problem Neural Netv 1.00 0.75 0.50 0.25 I  M The system produced literally thousands of forecasts for each project If the distribution of project costs and project costs estimates were known a confidence interval could be computed using the cost estimates and a method such as Bayesian model averaging used to integrate them 5 Not having a priori knowledge of this distribution we chose instead to use the inter-quartile range of the estimates that is we looked at whether the estimate fell between the first and third quartile of the estimates The top and bottom 25 were disregarded because there are always outliers and it was presumed the system could work well even when many of the individual estimates were not very good Given the very large number of estimates produced by the system literally thousands or tens of thousands depending on the settings we could afford to squander a few Results were best with only three variables per equation In eight out of 31 projects 26 of the projects the project's actual costs fell within the inter-quartile range of the forecasts of costs for that project produced by the ACTS models For 30 out of 31 projects the inter-quartile range of estimates was made up of over a thousand estimates while for the 3 1st project only 446 estimates were part of the inter-quartile range The significance of these numbers is revealed in the next paragraph When equations with four variables were used only one of the true project costs fell within the inter-quartile range of forecasts produced by ACTS When more than 4 variables were used none of the true project costs fell within the interquartile range From this we concluded that ACTS performed best when only 3 variables were included in each model Using more variables appears to produce over-fitted models Aork MRE By Lauch Month Chronologic 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Launch Month Since 1960 Figure 10 Neural network magnitude of relative error sorted chronologically 11 space 


incapable of accurate forecasts Of course the wider the inter-quartile range of forecasts the more likely it is that the true forecast will fit within that range But how parsimonious is the range As figure 11 shows in about 55\2600 of the inter-quartile ranges created by ACTS the 3rd quartile of estimates is 50%0 larger than the Ist quartile of estimates For about 90\2600 of the ranges created the difference between the I't and 3rd quartile is less than 75\2600 and for 97\2600 of the projects the top of the range is 100 or less than the bottom of the range 45\2600 of the true project costs falling into the new range When the top and bottom of the range were changed by 50\2600 the range estimates encompassed 77\2600 of the true project costs In the end however we felt that the adjusted ranges were too wide for practical use Adaboosting using binomial logit classifiers Short for adaptive boosting the Adaboost algorithm 6 is a type of machine learning algorithm where a set of classifiers are adjusted to favor previous misclassifications These classifiers are actually other estimating algorithms Percentage of Time 3rd Quartile no more than XO/o Greater than 1st Quartile 120 100 80 60 40 20 0 25 50 75 100 Figure 11 Variation in ACTS-produced estimates measured between 3rd and 1st quartile Percentage of time True Cost Falls in Range Produced by ACTS 3 variable ersion 90 8070 60 5040 30 20 10 l 0 Interquartile  or10  or25  or50  or75 Figure 12 The effect of widening the estimating range measured by  of estimates now lying within Of the eight projects that fell within the average percentage difference between the 1st and 3rd quartile was 55 with median of 52 This indicated that the inter-quartile range might be too restrictive We therefore proceeded to widen the range of cost estimates for each project multiplying the ISt quartile estimate by 1 X%0 and the 3rd quartile estimate by 1  X using various Xs In each instance we compared actual project costs to estimates produced and determined how frequently the actual fell into the range of estimates Results are summarized in figure 12 When the top and bottom of the range were changed by 10 32 of the actual estimates fell within the new range Extending each of the range boundaries by 25 resulted in Adaboost works by iteratively weighting observations so that poorly classified ones are given more weight during the next iteration With the algorithm focused on improving estimates for exceptional cases it is somewhat more sensitive to noisy data and outliers though less likely to over fit We chose to test this approach due to the latter property The boosting algorithm seemed structured particularly for binary decisions though we are seeking to estimate a continuous variable cost We therefore staged the method by estimating cost bands so that the estimate for each band reverted to a binary decision The algorithm would separately estimate whether an observation was 12 


our evaluation method tested potential models using either ordinary least squares least absolute above or below X dollars then Y dollars then Z such that instead namely that we were examining a proportion rather X  Y  Z than a binary outcome but this was impractical to use and we judged that the problem was not strongly biased towards The boosting algorithm still required a classifier and model either context For a classifier we implemented binary logit78 a discrete Adaboost MRE By COST Lu was similar to ACTS but with 0 1.00 0.75 0.50 0.25 0.00 0.25 0 50 0 75 1 00 1 25 1 50 1 75 2 00 Cost Figure 13 Adaboost magnitude of relative error sorted by actual cost Adaboost MRE CHRONOLOGICALLY Year Figure 14 Adaboost magnitude of relative error sorted chronologically choice model specifically intended for a binary dependant variable is the cost above or below this point There may have been some support for using the probit model 7 This was done by a statistical programmer with output checked against the statistics program STATA 8 we obtained from this first stage system We also attempted to use OLS as a classifier but our experiments showed no significant effect from adding relatively orthogonal i.e unrelated models Since using more than one model did not make a difference boosting using OLS was little more than a single OLS model with estimates divvied up in bands no dynamic re-weighting of classifiers and thus no boosting effect to speak of To obtain models some changes and improvements and again put were discarded and resolved by the latter 13 Lu 1.00 0.75 0.50 0.25 0.00 0 0.50 0.75 1 00 1.25 1 50 1 75 2 00 error which better discounts outliers and weighted least squares The models which we built another windowed test system that our computing cluster to use trying to find good ones from were then used to bootstrap the binary logit method although coefficients a sea of potential transformations This time 


This exercise required a number of decisions about how many classifiers to use how many bands to estimate and how many variables should be used in the models All these choices were made empirically Models with 3 variables were marginally more accurate than those with 4 More models seemed generally better to a point so we used 99 models We also used 22 cost bands checking whether an estimate was above or below 12M 35M 44M and so on through 519M It is interesting to note that there was no data scarcity-induced limit on the number of bands we could have used since each was estimated using all observations lying above and below and not just between bands As with the neural network training was carried out using 52 missions from the 1960s through 1996 and testing occurred on 58 missions from 1996 through 2007 The trend in estimate deviation would again permit us to gauge whether predictive accuracy decayed over time Results are shown sorted by cost in figure 13 and chronologically in figure 14 Figure 13 shows that MREs are large and negative for missions at 56M actual mission cost and below Above this point results are mixed although not as poor as at the very low range There seems to be no degradation in predictive accuracy as projects increase in scale The correlation between estimates and actual values is approximately 0.66 Sorted chronologically in figure 14 no trend can be seen as estimates go farther out up to 11 years as was seen with the neural network Again this may have been due to the normalization process in which we mapped each variable to its percentage ranking based on future and past minimum and maximum values basically embedding the effect of time within the factors being input to the model It also may be due to a relatively stable era Weighted combinations of simple models A persistent problem in multivariate estimation methods which has been mentioned is that a model may be produced that over fits the observations used for calibration It would thus not be adequately generalized to estimate out of that sample Techniques such as jack-knifing and outlier removal can mitigate this We chose another approach developing models that were little more than simple rules of thumb and then combining these in a weighted manner so each would have a say in the estimate The weightings were formulated using Excel's Solver to minimize estimation error on a set observations lying 15 to 25 years before the test set We refer to this method as continuous boosting because as with the Adaboost method it too involves a mix of classifiers that are combined according to their performance against a set of test observations Unlike Adaboost which solves a binary problem the algorithms are here estimating on a continuous scale Also unlike Adaboost the models are re-weighted rather than the observations to maximize prediction Before presenting results it is worth noting that we ran a control test where all variables from the simple rules of thumb were combined into a single model and run on the same windows performance was nowhere near as good We chose the models by hand guided by the criteria that they should be relatively orthogonal rely on differing parameters so they could each contribute a unique influence to the combined result Figure 15 shows that MREs are mostly positive though for lower cost missions errors are regularly negative and large Continuous Boost MRE By COST M M 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Cost Figure 15 Continuous boosting magnitude of relative error sorted by actual cost 14 


CONCLUSIONS Given that this project was intended to estimate missions lying 10-15 years out we structured it differently than one intended to estimate contemporary projects A variety of conventional techniques were not used as we felt they would over fit training observations and thus not be suitable for prediction We also were not sure which approach would work so we tried many We heavily favored ensemble methods where models are combined because we surmised that any one model could not be guaranteed to have the best view of the future However a single neural network ultimately yielded the most competitive results Another finding was that methods built upon simple models such as with three variables generally did work best not surprisingly because they were less likely to over fit calibration data A graph of the three best methods is shown in figure 18 with estimates for the same missions sorted by cost For the neural network calibration occurred until just before the results shown for continuous boosting calibration occurred no more recently than 15 years prior to the results and for Adaboost calibration also occurred up until the results shown A graph sorted by year is not shown we think it instructive that no results seemed to degrade over time As mentioned but obvious from figure 17 the neural network performs best followed by Adaboost and then continuous boosting It is interesting to note that the three cases in which the neural network goes haywire and predicts too low also correspond to worst performances for the Adaboost method pointing to exceptional data points We will be further investigating these regularly errant results and other outliers which may result in estimating improvements ACKNOWLEDGEMENTS This work was carried out under Small Business Innovation Research contract FS9453-05-C-0023 with the Air Force Research Lab The authors wish to gratefully acknowledge Judy Fennelly and later Ross Wainwright our technical points of contact at AFRL for their continuous support and encouragement Our contract officer Timothy Provencio also provided invaluable assistance Our critical seed stock of data was provided by Joseph Hamaker who previously was Director of NASA Headquarters Cost Analysis Division and now is Senior Cost Analyst with SAIC Ainsley Chong and Dale Martin USAF Ret also lent considerable assistance with data gathering APPENDIX A MISSIONS COLLECTED Active Cavity Radiometer Irradiance Monitor Satellite Active Magnetospheric Particle Tracer Explorer Adeos Advanced Communications Technology Satellite Advanced Composition Explorer Alexis Amos-I AMSC-1 Anik El Anik E2 Applications Technology Satellite-I Applications Technology Satellite-2 Applications Technology Satellite-5 All MREs Lu 1.00 0.75 l 0.50l 0.25 l 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75-2.00oi v 1   Cost o N X  X~~~~~Cl Figure 16 Comparison of estimating error for three best methods 15 


Applications Technology Satellite-6 Aqua Argos Atmospheric Explorer AURA Aurora 2 Calipso Cassini Cassini Spacecraft  Huygens Probe Chandra X Ray Observatory CHIPSat Clark Clementine CloudSat COBE Columbia 5 Contour CRRES DART Dawn DBS-1 Deep Impact Flyby Spacecraft  Impactor Deep Space 1 Deep Space 2 Defense Meteorological Satellite Program-5D Defense Meteorological Satellite Program-5D3 Defense Support Program DSCS 3 FIO DSCS 3 F7 DSCS I DSCS-II DSCS-IIIA DSCS-IIIB Dynamics Explorer-I Dynamics Explorer-2 Earth Observing Satellite 1 Earth Radiation Budget Experiment EchoStar 5 Extreme Ultraviolet Explorer Far Ultraviolet Spectroscopic Explorer FAST FLTSATCOM 6 Galaxy 5 Galaxy 11 Galaxy Evolution Explorer Galileo Orbiter  Probe Gamma Ray Large Area Space Telescope GE 1 GE 5 Genesis GFO 1 Globalstar 8 Glomr GOES 3 GOES 9 GOES N GPS-1 GPS-IIR GPSMYP GRACE Gravity Probe-B GRO/Compton Gamma Ray Observatory GStar4 Hayabusa HEAO-1 HEAO-2 HEAO-3 HESSI-II High Energy Transient Explorer-II HETE HST ICESat Ikonos IMAGE IMP-H Inmarsat 3-F5 Intelsat K INTELSAT-II INTELSAT-IV International Ultraviolet Explorer Iridium James Webb Space Telescope Jason 1 JAWSAT KEPLER KOMPSAT LANDSAT1 LANDSAT-4 LANDSAT-7 Lewis Lunar Orbiter Lunar Prospector Magellan Magsat Mariner-4 Mariner-6 Mariner-8 Mariner1 0 MARISAT Mars Exploration Rover Mars Express/Beagle 2 Mars Global Surveyor Mars Observer Mars Odyssey Mars Surveyor 2001 Orbiter Mars Pathfinder  Sojourner Rovers Mars Polar Lander Mars Reconnaissance Orbiter Mars Telecommunication Orbiter Mars Climate Orbiter Messenger Meteor Mid-course Space Experiment MightySat Milstar 3  Adv EHF Model-35 Morelos NATO III Near Earth Asteroid Rendezvous NEAR Shoemaker New Horizons 16 


NIMBUS NOAA 8 NOAA 15 NPOESS Preparatory Project Orbital Maneuvering Vehicle Orbcomm Orbiting Carbon Observatory Orbiting Solar Observatory-8 Orbview 2 Orion 1 P78 Pas 4 Phoenix Pioneer P-30 Pioneer Venus Bus/Orbiter Small Probe and Large Probe Pioneer-I 0 Polar QuikSCAT Radarsat Reuven High Energy Solar Spectroscopic Imager REX-II Rosetta Instruments Sage III SAMPEX Satcom C3 Satcom C4 SBS 5 SCATHA Seastar SMART-1 SNOE Solar Dynamics Observatory Solar Maximum Mission Solar Mesosphere Explorer Solar Radiation and Climate Experiment Solar Terrestrial Relations Observatory Space Interferometry Mission Space Test Program Small Secondary Satellite 3 Spitzer Space Telescope SPOT 5A Stardust  Sample Return Capsule STEPO STEPI STEP3 STEP4 STRV ID Submillimeter Wave Astronomy Satellite Surfsat Surveyor Swift Gamma Ray Burst Exporer Synchronous Meterological Satellite-I TACSAT TACSAT 2 TDRS F7 Tellura Terra Terriers Tethered Satellite System Thermosphere Ionosphere Mesosphere Energetics and Dynamics TIMED TIROS-M TIROS-N TOMS-EP Total Ozone Mapping Spectrometer TOPEX TRACE Triana Tropical Rain Measuring Mission TSX-5 UFO I UFO 4 UFO 9 Ulysses Upper Atmosphere Research Satellite Vegetation Canopy Lidar VELA-IV Viking Viking Lander Viking Orbiter Voyager Wilkinson Microwave Anisotropy Probe WIND WIRE XMM X-Ray Timing Explorer XTE XSS-iO XSS-ii APPENDIX B FIELDS COLLECTED Mission Mission Scenario Mission Type Launch Year Launch Vehicle Bus Model Bus Config Bus Diameter Location Expected Life Flight Profile Flight Focus Technical Orbit Currency Total Cost Including Launch Unit Sat Costs Average Sat Cost Production Costs Launch Costs Operations Cost Orbit Weight Wet Weight Dry Weight Max Power EOL Power BOL Power  of Batts 17 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


