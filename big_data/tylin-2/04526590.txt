Augm-enting Data Collection and Analysis of Operational Sim-ulations with RDF and SPARQL Brian Mihok Lockheed Martin Aeronautics Advanced Development Programs IO111 Lockheed Way Palmdale CA 93599 661-572-4793 brian.mihok@lmco.com Richard Stocking Lockheed Martin Aeronautics Advanced Development Programs IO111 Lockheed Way Palmdale CA 93599 661-572-2496 rich.stocking~lmco.com Douglas Holmes Java Professionals Inc 11835 Maple Crest Moorpark CA 93021 805-523-2650 dholmes~j avaprofessionals.com Abstract-In this paper we describe a toolset Framework and associated analytic methodology that are intended to support 
the development of architectural models for Network-Centric intelligent air systems The Framework has been developed as an architecture-driven concept design and evaluation environment for network-centric aeronautical systems that leverages a range of Semantic Web technologies The Framework provides a flexible distributed simulation environment that will accommodate a wide range of pluggable Operations Analysis OA simulations and design tools to support assessment of systems under consideration We employ a general metamodel of the common domain of discourse that contains organizes and correlates all the information 
that each of the 4plugged-in applications needs to function as well as any additional information particular to the composed system This model becomes the basis of communications between the cooperating systems These models are then used to configure a user-selected set of distributed simulations and data analysis tools 12 We represent the meta-model in RDF/XML which we believe provides several important advantages RDF is a knowledge representation language that allows information to be described as a set of factual sentences When these sentences are constrained by 
and are consistent with an OWL ontology this enables the use of an existing robust collection of automatic reasoners and other logical tools to classify assess query apply rules and otherwise operate on the model itself independently of the cooperating applications Also important to the analysis derived from RDF within the framework is the non-RDF compliant textual output from legacy plug-in systems We describe the process by which an example textual simulation output is converted to RDF We describe the problem definition how the template 
ontology was created and how the data was extracted and combined with the template ontology to form the structured output This 4ontologizing process uses the JENA Semantic Web Framework package to produce a structured output ontology We then discuss the use of SPARQL queries and SWRL rules that effectively expands the 1  1-4244-1488-1/08/$25.00 
C 2008 IEEE 2 IEEEAC 
paper  1236 Version 2 Updated December 14 2007 functionality of the system and greatly improves the analysis of the output of the system of 
cooperating simulations and tools TABLE OF CONTENTS 1 INTRODUCTION 1 2 THE ILium FRAMEWORK 1 3 EXAMPLE APPLICATION MISSION LEVEL ANALYSIS AND COLLEGE FoOTBALL.........................2 4 COLLEGE FoOTBALL EXAMPLE OVERVIEW 3 5 ONTOLOGY DEVELOPMENT 4 6 ONTOLOGIZING THE DATA 6 7 ANALYSIS 8 8 SUMMARY AND CONCLUSIONS...................8 REFERENCES 9 BIOGRAPHY....................................9 1 INTRODUCTION For several years we have been conducting systems requirements investigations that have been dependent on the interoperation of a number of legacy engineering and operations 
analysis OA modeling and simulations systems In most OA processes conventional practice is to develop a concept of operations CONOPS that typically features some advanced technology and to use a variety of simulations and analysis tools to examine some particular aspect of the design Each OA tool provides some insight to a select set of questions in the particular CONOPS no tool provides a comprehensive perspective across the range of situations The analysis process is usually accomplished 
4manually by a team of human operators who first analyze the results of one model and then infer appropriate input to configure operations in a subsequent tool or model This process is fraught with delays analyst judgments on data conversion rules contextual discontinuities etc Our efforts seek to bridge those obstacles and delays and replace them with semantically enabled processes that allow analysts to concentrate on analysis vice data conversions 2 THE ILium FRAMEWORK To come to an economically efficient remedy for these 1 


kinds of problems we have developed and employed a Semantic Web Framework called the Ilium Framework as means to assemble a collection of OA simulations and tools to automate systems interoperation to situate all systems in the same information context and to gain a broader perspective that includes among other things the effects of cooperating systems in the context of our investigation In such efforts ensuring semantic consistency across an increasingly wide range of systems has been a major concern We developed the Ilium Framework as an integration environment that leverages and to a certain extent creates a local Semantic Web The framework supports an API that allows legacy OA applications to be configured as Java plug-ins The framework provides communication control user interface and other services to synchronize and otherwise orchestrate the activities of the collection Figure 1 illustrates an intensive OA application configuration of the Ilium Framework Figure 1 Ilium Framework Architecture In this view several types of Ilium Objects manage interactions among legacy OA models and simulations Basic Ilium Objects act mirror and manage interactions between equivalent objects or concepts in the legacy application and other systems plugged into the Framework Broker Objects provide fine grained control when appropriate requesting specific services and monitoring the state of their surrogates Agents provide an intelligent interface between specific legacy simulation objects or processes and the rest of the Framework Finally the framework incorporates Jena 3 a popular open source set of Semantic Web tools and supports Protege 4 as well as an associated set of DL reasoners and rule engines This latter set of tools and services supports various knowledge based applications and is used to ensure semantic consistency across the plugged-in systems 3 EXAMPLE APPLICATION MISSION LEVEL ANALYSIS AND COLLEGE FOOTBALL As described earlier the tool used to perform a simulation changes depending upon the level of analysis campaign mission engagement Similarly within a level of simulation several different tools exist with the use of any given tool depending upon government acceptance contract parameters or even the users skill sets or preferences Each tool however is uniquely created producing proprietary data output developed independently of any other consideration As a result the analyst is left with the task of combining unlike data structured in different potentially antiquated formats into one analysis if he/she decides that more than one tool is required for proper analysis Figure 2 shows this post-processing dilemma As such a tremendous need exists for a data integration manager that will take in proprietarily-formatted output files capture the useful information and structure it in such a way as to facilitate easy access for the end user Figure 2 Combination of data from multiple different simulations into a useful format for the user The above figure shows all of the magic happening in the green box labeled Data Integration Manager This green box takes in the unique output from all of the different simulation tools translates it in a Rosetta Stone-like way and produces a common output that can then be used in whatever manner is most useful to the user Some such possibilities include directly accessing the output utilizing a database with attached GUI to view the data or any other user-defined application However this begs the question 2 3Oberle Daniel Semantic Management of Middleware Springer 2006 4OWL Web Ontology Language Guide available at http://www.w3.org/TR/owl-guide 2 r odw a r 


passed to M[ar Hart rshed f 3 I123 13 55 ChadL Henne EST_46E 13::0 Chad Herne passed to CHST35S 1t3 22 Mike Ha.rt rushed for  ha for 24 vards ghar for 7 yards 12 X12:44 of what happens within the Data Integration Manage Figure 3 provides our proposed implementation  magic The bold input arrow represents the aggregat from all of the simulation tools displayed on the left Figure 2 This input data could be represented in a  of ways including ASCII text flat files binary messages passed over sockets or any means This  then parsed where the term parse is loosely defined process used to extract the useful information and its c from the myriad of input Since this is programmatically the extracted information is tyl temporarily housed in data structures within a progran 2r box of the e input of the variety files data is as any ontext done nironllx plually ff 4 ff I i Figure 3 Breakdown of the Data Integration Manager box from the previous figure The magic truly begins in the next phase where the parsed data is combined with a domain-specific ontology Here we use the benefits of RDF and ontologies described earlier to serve as the universal translator In addition to providing the desired universal encoding scheme for the separate OA tools creating an ontology in RDF opens up the data to the benefits of query languages such as SPARQL and automated reasoners that enable machine processing of the data In creating an ontology and populating it with the data from the OA simulations the end user has the information from all the tools at his/her fingertips in a single language which can then be accessed using a multi-order query to provide a desired nugget of information in a greatly reduced time The remainder of the paper will describe in detail the development of an example and how our proposed methodology changes the analysis process However due to the proprietary and more importantly classified nature our work our mission-level analysis example cannot be adequately described here As a substitute we describe an analogous example using college football while also elaborating on the battle simulation where we can This example was actually used in the prototyping and prove-out of systems and methodologies and remains a benchmark for troubleshooting George Carlin references aside the similarities between post-processing several war simulation tools and post-processing a collection of football games is remarkably similar We will describe the similarities in the next section which covers the overview of the college football example Following the example overview the remainder of the paper will discuss the ontology development parsing the data otologizing the data permanent storage in a database developing a user interface and finally querying the data using SPARQL 4 COLLEGE FOOTBALL EXAMPLE OVERVIEW College football was selected as an analogous example to modeling of combat due to the similarity in the process required to extract information from its sources and make it useful to the end user In both domains the information comes from a variety of sources all of which represent the data which is fundamentally the same in a unique format but is usually contained in ASCII-text flat files that either represent chronological logs of the events or post-processed versions of the events For combat simulations such as EADSIMTM this is seen in output files such as the C31 log file or the various files created by the post-processor most of which are ASCII-text recounts of the events For Thunder the output is contained within the many ASCIItext transaction files In the parallel football example the script of events comes from the various sports websites that produce play-by-play events of the games As the name suggests the play-by-plays are a chronological recap of what happened on every play of the game They are available to view either in real-time or after the game from the internet Two such play-by-play sources were used in this example The origins were from espn.com and from cbssportsline.com A screenshot of a comparable section of the 2006 match-up between University of Michigan and The Ohio State University is shown in the figures below Quarter State Bukeyes at 1 5:0 CHST35 t:5 Pretoriuas kticked an Wolver ines rus for tB vard Ca pass co;le to Mar  1Mike rt for t'.e OS I for a Figure 5 Screenshot of espn.com example of OSU vs Michigan 3 a t14:SE MI20 14:30 Mike the e;xtra point Figure 4 Screenshot of cbssportsline.com example of OSU vs Michigan 26 Cad Hene Chad Hrenne passed to Mario Manninham for 5 71 e2L32 Touchdown Mike aHart trused for I yard Ga.rrett Rivas maede MI47 13:45 Chad Herne passed to Ma,i 


Another key similarity between the domains is that both typically have a list of actors in the scenario The various OA simulations typically rely on a collection of input files to create a scenario OPUSTm has an input XML file EADSIMTM includes files such as laydown and scenario files to help define the scenario and THUNDERTm has tens of dat files that define the input all of which are ASCII-text files that require parsing The parallel in college football is the roster for each of the teams The roster files like the play-by-play files can be found on the various sports websites are ASCII-text and are sufficient to define the players in the scenario As before examples from both espn.com and cbssportsline.com are shown below for comparable sections of a roster 1 Freeman MarCU 11 er 10 sm:ith Figure 6 osU  AtOn'0ni 3 r  m FL Aar on 5 oZ SE RUert DE 180 248 225 0 0o 6-1 180 r 6-1 205 r clev,eland OH 10 Troy Smith Qs 6-1 215 Sr WR 6-0 175 Fr Cleveland OH 5 chlmdl chek<wa DB 6-1 180 Fr clarmant FL 5 Mike D'Andrea SO Belle Glade Y B ent WF o Troy QB Screenshot 1 1 11 3R O'Neal 5 Jamario b 4 sml1 Ra WR 4 Col ean Kurt CB 5 chekwa5 chimdi B b 5 Dukles,f A1lbert WR 5 D'An.dlrea 6-;3 240 br 65 260 FL 6 Larry Granit LB 6-3S 225 J r Sr.0 LVnldhiurst OH 9 Brian Hartline W!R 6-3 180 Fr Nbrthi Canton OH 9 Robdert Rose DE 6-5 260 Fr Rose 9 ul _1 _.1 242 6-1 190 1 75 OH 7 Antornj Helltonl Qs 6-2 210 Fr FGrt valle r GA 8 Aar orn Garlit S 6-0 205 Fr orchiardi Lak MI 8 Ray Hall1 WR 6-3 240 I GI nn G 9 1 Lr 2 3 en1kins Mal col WR 6-0 180 NGrcrGss GsA 6 James Wallace WR 6-H5 206 So Hilliard OH 7 Ted G;i nri 3r LB 6-3 248 Sr AVon Lake OH 5 Al1 ibert Dukes WR 6-1 190 Mik1e LB 6 Gr ant,f 7 HeHtor r 6-1 203 3 r 6-1 215 sr of cbssportsline.com roster for 1 Marcus Freeman LE 6-2 242 SO Huber Hei ghts OH 2 Malcolm Jenkins CB 6-1 202 S 1 Piscataway NJ 3 JamaHi o O'Neal S 6-1 200 So C1eveland OH 4 Kurt Coleman CB 5-C 1 185 Fr clayton OH 4 Ray small clevelarid Glenville OH Figure 7 Screenshot of espn.com roster for OSU chaotic and unconstrained despite traditionally defined roles The emphasis in battle is not on what a platform was designed to do but instead what it actually can do The F/A-22 Raptor is a contemporary example While designed for air supremacy and attack its advanced sensor suite could be used for SIGINT purposes or even to perform the duties of an AWACS As such ontological flexibility is required to represent the reality of modem-day warfare A final similarity between the domains is the requirement by seasoned analysts to perform queries with multiple levels of filtering in order referred to from here on out as n-th order queries to extract the information they need Examples of such queries for both domains will be provided in further detail below 5 ONTOLOGY DEVELOPMENT The ontology was created using Protege a free ontology editor produced by researchers at Stanford University http://protege.stanford.edu Given the complex nature of the ontology it cannot be shown in its entirety here However we will delve into subsets of the ontology that serves as microcosms of the whole We took an event-centric approach on the ontology basing it around two major themes the plays that occurred and the players involved Both of these sections of the ontology will be discussed in greater detail in the subsequent paragraphs We used the same event-centric approach when developing the ontology for the combat simulation world Instead of rushes passes and kicks the ontology was based around shots detections or other such events where the players are platforms or airbases instead of running backs and middle linebackers A third similarity is that the set of players in the given scenario could perform only a certain list of actions defined by the domain but within those actions great flexibility existed While football seems to be quite a simple game to represent in that only a few types of plays exist pass rush punt field goal kick off etc the complexity explodes when you consider that each play could have a theoretically infinite chain of events that could happen That is lateral fumbles or the like could be strung together to make for an endless play A finite example of the complex nature of a football play is in the famous Stanford band play that occurred at the end of the University of California Berkeley vs Stanford University on November 20 1982 http://en.wikipedia.org/wiki/The Play The famous play involved five laterals on a kick off return on the last play of the game ultimately resulting in a touchdown being scored while the Cal player ran over one of the band's trombone players Also football is not as positionally structured as a layperson might believe with people such as the San Diego Chargers LaDainian Tomlinson throwing catching and rushing for a touchdown in the same game while also being eligible to provide a tackle on an interception or for returning a kick Clearly the battlespace is analogously First we will examine how we represented the players Figure 8 shows a subset of the ontology related to the players We first started with a class called Person that had properties of height name and weight A subclass of Person was then created called Player that had the additional properties of jerseyNum position and yearInSchool Value restrictions were put on the position and yearInSchool their values could only be of a defined list Two examples of the position restrictions were QB and DL with FRESHMAN and SOPHOMORE serving as examples for yearInSchool Also as the figure denotes with an asterisk a Player was allowed to have multiple positions 4 SO QB Jr 5 Ted 


Figure 8 Subset of the ontology showing information related to the players In addition the Player class is connected to a class called Team Our simplified representation of Team has just one attributed that being name Two inverse predicates link a Player to a Team those being playsOn and hasPlayerOf The edge playsOn has player as its domain and Team as its range whereas hasPlayerOf has Team as its domain and Player as its range One further difference is that the hasPlayerOf link can be attached to multiple players where playsOn can only be tied to one Team This represents the reality that a player can only play on one team while a team can have multiple players With the players developed we will now turn our attention to a subset of the ontology dealing with plays This section of the ontology is understandably much more complicated than the first For even the simplest play in football there is deceptively more information to represent than might be expected A play includes everything from the quarter and time that the play occurred to who was involved to even the side of the field and how many yards to go until a first down However the data can be placed into two broad categories data that is common to most any play and data that is specific to a certain type of play Two classes were created to represent these categories with a class named Play to correspond to the former and class called PlayType to correspond the latter We will now examine each of these categories First we will look at the information that is common to any given play Even this seemingly simple section of the ontology is more complicated than would appear A graphical representation is shown in Figure 9 As the figure shows the only two pieces of information that are common to all plays are the time at which the play occurred and the quarter in which it occurred Both the espn.com and cbssportsline.com play-by-plays did not track the side of field and yardline for point conversion attempts after a touchdown While this information is known for the vast majority of attempts penalties could affect this information Since the information was not tracked for the point conversions it was left out of the ontology for these plays p IsideOfField String p 1 ExtraPoint X p 1 down Integer p 1 m-iade Boolean TwoPtConversion _ p I muade IB oolean p1 yardsToGo Integer p1 kicker Instanc e p1 Player p1 pyardline Integer Figure 9 Subset of the ontology that deals with information common to all plays The figure shows the final representation of the common information The Play class served as the parent class of three subclasses These subclasses were called PlayFromScrimmage ExtraPoint and TwoPtConversion The PlayFromScrimmage class represented all the non-point conversion plays As such it has properties of down yardsToGo sideOfField and yardline The down is restricted to be 1 2 3 or 4 and sideOfField is restricted to own opponent or 50 The ExtraPoint class has properties of made and kicker where made is a boolean value representing whether the attempt was successful and kicker is an instance of the Player class representing the player who kicked the ball This methodology of having edges tie to instances of the Player to provide information about the event is used extensively in the PlayType section of the ontology We next turn to the PlayType section which again is the subset of the ontology that is related to information that is specific to a certain type of a play A portion of this section of the ontology is shown in Figure 10 One of the most critical predicates in this section of the ontology is the typeOf property of the PlayType class The typeOf property ties together the Play and PlayType classes When the subclasses of the PlayType are brought into consideration the typeOf edges help fully encode a play An Englishequivalent example taken from Figure 9 and Figure 10 would be that an instance of a Pass is a type of a PlayFromScrimmage When all of the properties of the Pass and PlayFromScrimmage classes are filled in a complete record of the actual play is encoded in the ontology Thus in order to put a play into the ontology an instance of one of the Play subclasses is created its properties are populated an instance of a PlayType subclass is created its properties are populated and then the typeOf predicate for the PlayType is set to the instance of the Play Figure 10 shows that some of the most critical properties in the subclasses of the PlayType class are filled in by instances of the Player class This provides the link back to the input files which is critical in the simulation world 5 


Figure 10 A portion of the section of the ontology that deals with information specific to the type of play Figure 10 also provides a glimpse into how the theoretical infinite chaining of events is handled in the ontology The subclasses of PlayType were given properties to indicate specific types of special occurrences which we termed InPlay Events Figure 11 shows a portion of this part of the ontology As the figure shows the In-Play Events were represented by one parent class called InPlayEvent and three subclasses InPlayFumble InPlayLateral and InPlayFumble These classes represent a fumble lateral and penalty that happened during the course of a play It should be noted that an InPlayPenalty was unique from an instance of the Penalty class which was a subclass of PlayType An example of an InPlayPenalty would be a facemask penalty tacked onto the end of a running play while an example of a Penalty would be a holding call on the offense which thereby invalidates any result of the play on which it occurred and thus becomes its own play Figure 11 A selection of the ontology dealing with InPlay Events As Figure 10 shows these InPlayEvents are referenced by pairs of properties The subclasses of PlayType where relevant contained a Boolean flag indicating whether a specific type of InPlayEvent occurred and a link to the instance of the InPlayEvent The general form of this pair of predicates was name of action]Occurred and action]ThatOccurred where action was either fumble lateral or penalty This section of ontology is perhaps best clarified through an example As an example let's consider the famous hookand-ladder play at the end of regulation of the Boise St vs Oklahoma game in the 2007 Fiesta Bowl In this play Jared Zabransky threw the ball to Drisan James for 15 yards James then lateraled the ball to Jerard Rabb who ran for an additional 35 yards and a touchdown with seven seconds remaining This play would first be represented as a PlayFromScrimmage seeing that it was not a point conversion attempt and as a CompletedPass with the typeOf link pointing to the instance of the PlayFromScrimmage The passer predicate of the CompletedPass would point to the instance of a Player representing Jared Zabransky and the receiver would point to an instance of Drisan James The fumbleOccurred and penaltyOccurred properties of the CompletedPass would be set to false but the lateralOccurred would be set to true Then an instance of an InPlayLateral would be created with the lateraler property set to Drisan James and the receiver set to Jerard Rabb The touchdown is then associated with the InPlayLateral Had another In-Play Event occurred all that would need to be done to expand the chain would be to set one of the action]Occurred variables of the InPlayLateral to true and create an instance of the next InPlayEvent subclass This methodology can be expanded to encode a theoretically infinite play Two paragraphs above we stated that the subclasses of PlayType contain the pairs of predicates where relevant The where relevant appears because it is not always possible for all of the In-Play Events to occur for a given subclass of PlayType For example on a stand-alone Penalty play such as a holding no fumble or lateral could possibly have occurred so the two pairs of properties representing an InPlayFumble and InPlayLateral were not included in the Penalty class This same type of reasoning was applied to all the subclasses of the PlayType to limit the options only to that which were permitted by the game of football 6 ONTOLOGIZING THE DATA With an ontology created we now turn our attention to the processing of putting the data into the ontology a process we refer to as ontologizing the data To ontologize both the football example and the combat simulations we wrote programs in Java The program for each domain had the same basic structure which was as follows 1 Read the ontology into the program 2 Read in and parse the input files 3 Put the information from the input files into the ontology 4 Read in and parse an event from the action log 5 Put the information from the action log into the ontology being sure to use the information from the input files 6 Repeat steps 4 and 5 for all the events 7 Write the ontology to permanent storage 6 


This recipe for ontologizing the data is a more detailed explanation of what is shown pictorially in Figure 3 It is important to realize that essentially all that is happening in this process is that the data are being extracted through parsing and then combined with the template ontology to form an encoded output just as the figure shows The steps listed above will be discussed in further detail in the subsequent paragraphs of this section The first step in ontologizing the data was to read the ontology into the Java program In order to perform this and all other actions required to manipulating the ontology the Jena Semantic Web Framework package or Jena was used.5Jena is an open-source Java Framework that serves as a programmatic environment for RDF RDFS OWL and SPARQL while also including a rule-based inference engine ref By reading a template ontology into Jena the user can programmatically create instances populate properties perform queries or perform a myriad of other tasks on the ontology Jena served as the backbone of our program With the ontology read in and ready to accept data in Jena we now turn to combining the relevant data with the template ontology to produce the structured output as shown in Figure 3 To this end steps 2 and 3 are essentially the same as steps 4 and 5 with only the source of the information being different For football the parsing of the files was made simpler by the fact that each player and play was given its own line in their respective files and the files were simply ASCII-text written in a format that was consistent from team to team for the rosters and within a play type in the play-to-play files That is each roster was structured in exactly the same way regardless of the team and each unique type of play had the same structure within a game and between games As a result traditional parsing techniques were used to parse the files For the rosters this simply meant that Java tools such as readLine and StringTokenizers were used The play-by-play was understandably much more complex but was parsed with a combination of using regular expressions to match the type of play and then using built in Java functions to perform further sentence examination This same methodology was used in the combat simulation program For structured delimited output tools such as readLine and StringTokenizers were used Conversely for freeform chronological accounts of events regular expressions were used to perform the coarse matching while built-in Java functions were used for the precise extraction of information within a sentence While parsing the play-by-play file or the output files in the simulation domain is quite time consuming to write and difficult to get perfected it is conceptually the easiest at a high level The program simply examines a line determines if it matches a pattern extracts the necessary information and stores it in data structures until it is put into the ontology 7 5 htp.H Jena.sourceforge.nlet As mentioned earlier Jena handled all interactions with the ontology including combining the extracted data with the template ontology We refer to the original ontology as the template ontology because it provides the definition of the structure that holds the data but has not yet been realized with actual instances of the data thereby serving as a template for the associated classes When the combination of actual data with the template structure happens we say we have an output ontology which can then be queried using SPARQL Thus in order to transition from the template ontology to the output ontology we need to create instances of the event classes and fill in their properties Creating an instance of an RDF class in Jena follows the same pattern regardless of the domain First the template of the class is found in the ontology using the getOntClass function This template is then used to create a specific instance of the class using the createlndividual function Now that an individual is created its properties are set by either using the addProperty or setPropertyValue functions depending upon whether the property can have more than one value or not The getProperty and createTypedLiteral functions are typically required to provide the arguments when using either of the property-filling functions This same methodology can literally be used for creating any instance of a RDF function in Jena and serves as the substance to the magic referred to originally in Data Integration Manager of Figure 2 To further clarify the process we will now discuss the specifics of the how the template ontology was transformed into the output ontology for the football example As mentioned in the steps for ontologizing the data above the input files or the rosters of the teams in the case of football were put into the ontology first When the roster file was opened an instance of the Team class was created and the name property populated using the methodology described in the previous paragraph Then the information for each individual player was read one at a time As mentioned previously the Player class was used to represent a player in the ontology As a result an instance of the Player class was created for all the individuals on the roster and the setPropertyValue function of Jena was used to set its properties By repeating this for every player on the roster all the actors in the scenario were put into the ontology Once the definition of an individual Player was completed the Player was added to the team using the addProperty function which was used to allow for multiple Players to be on the same team With the input files completely parsed the event log files were parsed next In both domains the majority of the information came in the form of ASCII-text strings that included the actors the action performed and the context of action For football this information typically included the involved players names the type of play the game situation and the stats about the play such as the yards that were gained As mentioned above regular expressions were 7 


used to match the type of the play and Java functions extracted the specific information However to be useful this information must be matched with the ontology To this end the type of play was translated into a specific subclass of the PlayType class while the context of the play was used to determine the appropriate subclass of the Play class and to fill in the properties of these two classes The names of the players involved in the play were used to find the actual instances of the specified Player in the ontology taken from the roster files By reading the rosters and creating the teams first we can leverage the actors in this way when ontologizing the events of the game This helps protect the validity of the data by ensuring that all references to an individual actor refer to the same entity regardless of how they are referred to in individual events This point is best clarified with an example To use the previous example from the Boise St vs Oklahoma game by referring back to the input files we ensure that all references in the play-by-play to Jared Zabransky Zabranksy number 5 for Boise St or any other tangent reference to a property are referring to the same Player in the ontology Maintaining data integrity in this way goes to the heart of using an ontology which strives to eliminate all reference ambiguity in the data The final step established above for ontologizing the data was to write the data to permanent storage The necessity of performing this step is predicated upon whether a database connection was established in the beginning or not In the early development of both domains we represented the ontology using Jena s memory model which stores the model and its associated triples in memory However given that combat simulations can produce many gigabytes of data with the number ever growing an alternative was required as every time the program was run with the memory model it crashed due to running out of heap space As Figure 2 suggests establishing a database connection quickly became one of the few viable options Towards these ends an Oracle lOg database was used to house the data due to Oracle's commitment in the 10g and the subsequent release of the 1 Ig to store RDF triples uniquely as opposed to performing a conversion to a traditional relational database every time Putting the information into the database had the desired benefit of greatly expanding the constraint on file size Additionally once the database was established optimizations could be performed on the data storage to improve the access speeds of common queries By putting the information in a sufficiently large database parsing becomes a truly one-time expense where it never has to be repeated If a database is used the program no longer has to read in the template ontology at the beginning and then write the output ontology to a file for permanent storage at the end Instead the program can simply establish a connection to the database when started and it then has access to all of the preexisting data including the template ontology 7 ANALYSIS Up to this point we have described in extensive detail the problem definition how the template ontology was created and how the data was extracted and combined with the template ontology to form the structured output We will now examine the methods we used to analyze the data For both domains the major obstacle to performing analysis of significant depth is the massive amount of data associated with the event In order for analysts to answer substantive questions they must be able to efficiently filter the data for the information pertinent to the question at hand To achieve this data filtering we used SPARQL.6 The benefit of SPARQL for our application is that it allows for the easy creation of what we referred to as N-th order queries that filter the data to preciously what we need in accordance with the ontology By N-th order queries we mean a SPARQL query that has multiple layers or orders of filtering that restrict the data This is best seen through an example For illustrative purpose we will again use the Boise St vs Oklahoma game Let's say an analyst was interested in finding out if Boise St had any big pass plays when they were deep in their own territory The occurrence of such plays would speak to Boise St.'s ability to use big plays to change the game as well as to provide some barometer for how conservative their play calling on offense was As such the analyst might develop a SPARQL query that has an equivalent English sentence of Give me all the completed passes thrown by Boise St where they were on their own 25 or less that went for 15 or more yards This particular query is a fifth-order query Explicitly the layers of filtering are 1 completed pass 2 Boise St 3 own side of the field 4 less than 25 yardline and 5 gained 15 or greater yards To answer the posed question such an occurrence occurred twice in the Boise St vs Oklahoma game The first was a 19 yard gain from Jared Zabransky to Legedu Naanee starting at the Boise St 17 at the beginning of the second quarter and the other was a 36 yard completion from Zabransky to Derek Schouman starting at the 22 yardline on the last drive of regulation The ability to perform such N-th order queries is absolutely critical to operations analysis in combat simulations A sample question an operations analyst might ask for in a scenario is Give me all the times a F-22 on a DCA mission was shot and killed at by a specific SAM type within a geographic region bounded by minimum and maximum latitudes and longitudes This is a ninth-order query where the layers are 1 shot fired 2 kill resulted 3 target of F-22 4 attacker of a specific SAM type such as SA-10 5 F-22 mission of DCA 6 shot location greater than minimum latitude 7 shot location less than maximum latitude 8 shot location greater than minimum longitude and 9 shot location less than a maximum longitude Given unstructured nature of the output data from most 8 6htro ii uerV 8  1 11 I_ 


simulations answering such questions previously required extensive datamining that could take at least hours if not days to perform and was highly susceptible to error However a SPARQL query could return the results in tenths of seconds for small data sets and minutes for very large datasets Plus if the parse is well formed and properly debugged there is no chance for errors either returning a false positive or an omission Another tremendous benefit received from the structuring of the data and querying it with SPARQL is the increased ease of debugging the definition of the scenario This is not necessarily a problem in football but it is a significant issue in combat simulation The very large scenarios blessed by the government to represent certain encounters of interest have thousands of platforms each individually created with unique properties As such even a low error rate yields a non-trivial amount of mistakes some of which could profoundly affect results Many man-hours are sunk into validating the details of a scenario such as the networks of the IADS or the command chain of platforms all of which is tedious Several well placed SPARQL queries can help greatly reduce the scenario debugging time and make the debugging less cumbersome in the process One of the lone downsides to employing the abovedescribed methodology for data mining and analysis in the combat simulation domain is the lack of familiarity of most analysts with these techniques While the learning curve is shallow schedule and mindset challenges limit most seasoned analysts ability to learn about Java RDF ontologies and SPARQL To help facilitate the creation of N-th order queries we developed a GUI to wrap around the code in the combat simulation domain The GUI was developed using NetBeans Matisse GUI developer and made creating N-th order queries simply a manner of selecting the desired options on the GUI As a result the analysts did not need to be Java programmers with extensive RDF knowledge in order to use the program Instead they simply needed to have knowledge of the domain which is already a prerequisite for performing the analysis 8 SUMMARY AND CONCLUSIONS The combat simulation domain is faced with the problem of having several different tools that represent data that is fundamentally the same in their own unique way The domain requires a method for combining disparate data into a common ontology as well as a way to then mine through the masses of data and filter out only that which is relevant to the current analysis In this paper we proposed a solution to this problem using RDF ontologies to commonly represent the domain and SPARQL queries to filter the results Due to the propriety classified nature of the work however we developed and presented a solution to a similar domain that being analyzing college football College football was chosen due to its similarities to the combat simulation domain which include 1 the output comes from a variety of sources represented in their own unique way but typically from ASCII text chronological log files 2 both domains contain ASCII lists of actors performing actions specific to the domain 3 both domains allow for a limited number of actions yet require tremendous ontological flexible within the actions and actors and 4 both domains require analysts to apply many-order filters to extract the desired in-depth information The bulk of the paper focused on the specifics of how the ontology was created how the ontology was combined with the data and finally the methods used in querying the data To these ends Stanford's Protege was used to create the ontology A Java program was then used to parse the data from input and output files after which the information was combined with the template ontology through the use of the Jena Semantic Web Framework or Jena The data were stored in an Oracle lOg database for permanent storage SPARQL was then used to query the data with a GUI assisting in this process The work yielded some significant conclusions SPARQL provided a simple methodology for creating the N-th order queries required by both domains Furthermore the execution of these queries provided access to data and answers to questions in seconds that required hours or days to get previously This drastic reduction in time and increase in the ease of use could literally transform the modem day combat simulation analyst from a glorified data miner back into an analyst Abstracting away the data mining and post-processing allows the analyst to focus on the forest instead of the type of bark on the trees in the forest Furthermore the unification of several sources into a common ontology allows for the analyst to be an expert in the entire domain not an expert in a specific tool Furthermore the encoding of the output into an ontology helps ensure the validity of the data and provides for additional scenario debugging assistance The validity of the data is augmented by the fact that the ontology eliminates all ambiguity in which entity is being used regardless of how it was referenced as well as any ambiguity in the definition of a certain event The assistance in debugging the scenario comes from the use of SPARQL queries on the input to the scenario such as verifying the members of a network in the IADS or the command chain of the platforms The SPARQL queries help provide quick access to the information and allow it to be displayed in a manner that is less cumbersome to crosscheck This allows for faster debugging of the input scenario REFERENCES Oberle Daniel Semantic Management of Middleware Springer 2006 2OWL Web Ontology Language Guide available at http://www.w3.org/TR/owl-guide_ 9 


3 http1//ena.sourceforge.net 4 http www.w3.org/TR/rdf-spargg-quary BIOGRAPHY Brian Mihok is a software engineer at Lockheed Martin Aeronautics Advanced Development Programs The Skunk WorksTM He is currently working on modernizing the methodology used by the Operations Analysis group with the approaches described in this paper and on adding autonomy to Lockheed's small UAVs Mr Mihok graduated from the University of Colorado at Boulder with two B.S degrees one in Aerospace Engineering and one in Computer Science He also attended MIT where he received a M.S in Aeronautical and Astronautical Engineering Richard Stocking is the lead Program Investigator/PM for Net Centric Operations Warfare Analysis efforts for Lockheed Martin Aeronautics Advanced Development Programs The Skunk WorksTM He is currently leading efforts researching autonomous UAV operations Current efforts include the integration of Multiple Agent Systems and other autonomy systems within the Ilium Framework He has over thirty years experience and over 11,000 flight hours with multiple C4ISR systems in the US Army and US Navy Mr Stocking has a M.S in Systems Technology from the Naval Postgraduate School Douglas Holmes is co-founder and Senior Partner of Java Professionals In the past twenty-two years he has lead and participated in numerous knowledge-based programs for DARPA and other research agencies as well as commercial applications in the petroleum and other sectors He is currently developing ontologies and domain models in the military unmanned systems domain He also has over twenty years experience as an Air Force Fighter Pilot and Fighter Weapons School Instructor Mr Holmes has a B.S in Mathematics and Basic Sciences from the U.S Air Force Academy and a M.S in Management Information Systems from Golden Gate University 10 


12 R Krishnapuram D Casasent Determination of threedimensional object location and orientation from Range Image IEEE Transactions on Pattern Analysis and Machine Intelligence 11\(11 1158 1167 November 1989 13 R Craig I Gravseth R P Earhart J Bladt S Barmhill L Ruppert C Centamore Processing 3D flash LADAR point-clouds in real-time for flight applications Proc SPIE 2007 Vol 6555 BIOGRAPHY Stphane Ruel is a project manager at Neptec Design Group in Ottawa Canada He currently leads development of novel 3D computer vision algorithms and sensor software for space applications Previously he served as an operations engineer on several Space Vision System SVS Space Shuttle missions He has a B.Sc in Computer Engineering and M.Sc in Aerospace Engineering from Laval University He is also alumni of the International Space University SSP in 2004 Tim Luu is a Vision Systems Engineer at Neptec Design Group Ltd At Neptec he develops 3D vision systems for space and defense applications Most of his time is spent developing real-time 3D pose estimation algorithms using sparse data from non-cooperative targets This research is mostly geared toward the field of autonomous rendezvous and docking Filling out his time is also control systems design 2D image registration onto 3D data as well as other 3D visualization techniques Tim received his M.A.Sc from Carleton University in Mechanical Engineering and his BASc from the University of British Columbia in Engineering Physics specializing in Mechanical Engineering 11 


Time Time 50 350   10 0                   10 1                   10 2 12.5 50 350   10 0                   10 1  12 expected from Figure 7, the width of the uncertainty region is compressed by the curvature of the monopulse response resulting in a detection-primitive with greater uncertainty than the variance admits.  A filter lag or so-called cluster tracking can easily result in a 5% or greater offset and degraded consistency.  After 300 seconds the curves peak up because the target is appr oaching a low-elevation beampoint limit.  This occurs anytime a target is tracked into the edge of the radar\222s field of re gard and can lead to radar-toradar handover difficulty         300 300 80  s D 2 k,1 k y    s D 2 k,1 k y   100 150 200 250 10 1                    10 5 1 0  Figure 8 - Consistency versus distance from beam center Monopulse Mismatch The next set of curves plotted in Figure 9 show the sensitivity of detection-primitive consistency to a mismatch in the monopulse slope.  All of these curves were generated using a linear monopulse response derived from the slope of the true monopulse response at beam center.  The slope of the 80% curve is 0.8 times th e beam-center slope; the 90 curve is 0.9 times the beam-center slope; and so on for 100%, 110% and 120%.  Again, the order of curves in the graph is the same as the legend order A steep slope tends to expand y I 222s uncertainty while a gentle slope tends to compress it.  An expanded uncertainty leads to a smaller consistency while a compressed uncertainty leads to a larger consistency.  This behavior can be observed in the family of curves in Figure 9.  Curves for the steeper slopes are on the botto m while curves for more gentle slopes are on top.  The notable feature of this set of curves is that the sensitivity to a mismatch in the monopulse slope is not very significant       100 150 200 250 10 1                    90 100 110 120  Figure 9 - Consistency versus monopulse mismatch Range-Bias Error The complex nature of the monopulse radar models presents ample opportunity to introduce errors in the software implementation.  One such e rror introduced in a \275 rangecell-width bias in the detection-primitive range which in turn resulted in a significant degradation to 2 9 k D The fact that 2 9 k D is measured in different coordinates compared to the bias made it difficult to determine which value or algorithm was to blame.  Examining the intermediate consistency values led directly to the error source A comparison between biased 2 1 k D  2 2 k D and 2 3 k D values and unbiased 2 2 k D values is shown in Figure 10.  The unbiased 2 2 k D is the bottom-most curve and the biased 2 3 k D is the top-most curve with a value around 80.  This large value for 2 3 k D indicates that there is a lot more uncertainty in the range measur ements compared to what is predicted by the range varian ce.  Since the range-variance calculation is easy to confirm, the problem must be in the algorithms that model or manipulate range A notable feature of Figure 10 is the sensitivity of the centroiding algorithm to range bias in the detection primitives.  The range bias is ba rely noticeable in the biased 2 1 k D and 2 2 k D curves.  Of course, if the unbiased 2 2 k D  curve existed as a baseline it would be relatively easy to spot the error 


Time Time Time 50 350   10 0                   10 1                   10 2 50 350   10 1                   10 0                   10 1 50 350   10 0                   10 1  13         Isolated No SNR Adjust  Figure 11 - Centroiding for isolated range cells Filter Tuning Now that the centroided m easurements are reasonably consistent, the parameters that govern track filtering can be examined.  As previously promised, the effects and corrections for atmospheric refr action and sensor bias have been disabled so that 2 8 k D can be analyzed using a sliding window.  Of course the full analysis would include these effects and 2 8 k D at each time step would be collected and averaged over many trials Plots of the effect of changing process noise in a nearlyconstant-velocity filter are shown in Figure 12 and Figure 13 for Cartesian position and velocity respectively.  In both figures, the plotted values have been divided by 3 so that the desired value is always 1.  Increasing the process noise up to a point should increase the updated uncertainty and reduce 2 8 k D values.  Except near th e end of the trajectory when the measurements are off of beam center, the curves in Figure 12 and Figure 13 appear inconclusive for this expected trend If 2 8 k D values are way out of range there are additional intermediate filter values that can be examined.  For example, the state extrapolati on algorithms can be examined by comparing the consistency of 1 210 Isolated With SNR Adjust 300 300 300 0.005 212 212 212 212 212 kkkk T kkkk D xhzSxhz 35        s D 2 k Range   D 2 k,2 biased D 2 k,1 biased D 2 k,2  Figure 10 - Range bias error in detection primitive Centroiding Algorithm From Section 3, assuming that the centroided-range uncertainty for an isolated range cell is the same as its detection-primitive uncertainty may be incorrect Collecting and plotting 2 3 k D values only from isolated range-cell measurements can be used to analyze such assumptions.  The plots in Figure 11 compare differences between the isolated-cell algorithm defined in Section 3, an algorithm that modifies the uncertainty based on the SNR in the isolated cell, and the 2 3 k D values from all measurements 34\was used to modify the range uncertainty for the upper line labeled Isolated with SNR Adjust    4 22  2 2  resRi o R R Rn bdp bm  s D 2 k,3 Range    s D 2 k,8 Position     212 1 can also be examined using \(35 The residual is also commonly used to determine the assignment cost  212 kk z  P  k  k1 with z k The consistency of the innovation covariance k T kkkkk RHPHS 100 150 200 250 10 1                    D 2 k,3 biased 100 150 200 250 10 2                    100 150 200 250 10 1                    0.5 50  Figure 12 \226 Position consistency, filter tuning example  r  t t 34 If the All Centroided curve \(middle\as the baseline doing nothing \(lower\imates the uncertainty and 33\imates the uncertainty.  Dividing by the square root of the observed SNR leads to a more consistent covariance; however, there is currently no statistical evaluation to justify it             210 210 1 1 1 2 All Centroided 


Time 50 350   10 1                   10 0                   10 1  14         300 0.005  s D 2 k,8 Velocity   100 150 200 250 10 2                    0.5 50  Figure 13 \226 Position consistency, filter tuning example 5  C ONCLUSION  Calculating and observing the behavior of covariance consistency at different levels  in the radar signal processing chain represents a very powerfu l tool that can be used to assess the accuracy and softwa re implementation of radar signal-processing algorithms.  Analyzing covariance consistency is applicable to radar systems both in the field and in simulations.  The primary challenge in both arenas comes down to properly accounting for the true target states that contribute to detections, detection primitives measurements, and state estimates For a fielded radar syst em, achieving covariance consistency is usually a s econdary consideration behind achieving and maintaining track s.  Indeed, until recently radar specifications did not even include requirements for covariance consistency.  Recent covariance consistency requirements stem from the fact that the use of radar systems in sensor netting applications is on the rise Currently the combined e ffects of off-beam-center measurements, atmospheric correction, bias correction clustering and centrioding, data association, and filtering on state covariance consistency throughout a target\222s trajectory are not well known.  This is particularly true for radars using wideband waveforms and multiple hypotheses or multiple frame trackers.  Numerical results presented here indicate that algorithms early in the radar signal processing chain can significantly degrad e covariance consistency and that some errors are better tolerated than others For a simulated target in a modeled system, truth relative to some global reference is known. However, transforming truth through different refere nce frames and accounting for changes that occur during various radar processing algorithms is not as simple as it appears.  The techniques in this paper help expose this hidden complexity and provide a framework for discussing and expanding the future development of covariance consistency techniques.  Such future developments include issues related to mapping truth through the convolution operation typically used to simulate wideband signal processing and the fast Fourier transforms typically used in pulse-Doppler processing.  Sophisticated tracking algorithms that carry multiple hypotheses, associate across multiple frames, or weight the association of multiple targets within a single frame pose significant challenges in properly associating truth with state estimates.  Additional work, including an investigation of track-to-truth assignment, is needed before covariance consistency techniques can be applied to these algorithms Another area that needs furthe r analysis is the use of a sliding window to approximate the covariance behavior expected during a set of Monte-Carlo trials.  Various timedependent variables such as the target\222s range and orientation, the transmit waveform, the radar\222s antenna patterns toward the target, missed detections, and false alarms could easily viol ate the assumption that measurement conditions are nearly stationary over the time of the window.  It is importa nt to understand the conditions when this assumption is violated Finally, the examples presented here included a relatively benign arrangement of targets.  Further analysis in dense target environments with the related increase in merged detections, merged measurements, and impure tracks is needed.  Further analysis for targets traveling over different trajectories is also needed Even so, the techniques presented here can be extended to many of these analyses R EFERENCES  1  S. Blackman and R. Popoli Design and Analysis of Modern Tracking Systems Artech House, 1999 2  Y. Bar-Shalom and X. R. Li Multitarget-Multisensor Tracking: Principles and  Techniques YBS Publishing, Storrs, CT, 1995 3  Y. Bar-Shalom, Editor Multi-target-Multi-sensor Tracking: Advanced Applications and  Vol. I Artech House, Norwood, MA, 1990 4  D. B. Reid, \223An Algorithm for Tracking Multiple Targets,\224 IEEE Trans. on Automatic Control Vol. 24 pp. 843-854, December 1979 5  T. Kurien, \223Issues in the Design of Practical Multitarget Tracking Algorithms,\224 in Multitarget-Multisensor Tracking Y. Bar-Shalom \(ed.\43-83, Artech House, 1990 6  R.P.S. Mahler, Statistical Multisource-Multitarget Information Fusion, Artech House, 2007 


 15 7  B.-N. Vo and W.-K. Ma, \223The Gaussian Mixture Probability Hypothesis Density Filter,\224 IEEE Trans Signal Processing Vol. 54, pp. 4091-4104, November 2006 8  B. Ristic, S. Arulampalam, and N. Gordon Beyond the Kalman Filter Artech House, 2004 9  Y. Bar-Shalom, X. Rong Li, and T. Kirubarajan Estimation with Applications to Tracking and Navigation, New York: John Wiley & Sons, pg. 166 2001 10  X. R. Li, Z. Zhao, and V. P. Jilkov, \223Estimator\222s Credibility and Its Measures,\224 Proc. IFAC 15th World Congress Barcelona, Spain, July 2002 11  M. Mallick and S. Arulampalam, \223Comparison of Nonlinear Filtering Algorithms in Ground Moving Target Indicator \(GMTI Proc Signal and Data Processing of Small Targets San Diego, CA, August 4-7, 2003 12  M. Skolnik, Radar Handbook, New York: McGrawHill, 1990 13  A. Gelb, Editor Applied Optimal Estimation The MIT Press, 1974 14  B. D. O. Anderson and J. B. Moore Optimal Filtering  Prentice Hall, 1979 15  A. B. Poore, \223Multidimensional assignment formulation of data ass ociation problems arising from multitarget and multisensor tracking,\224 Computational Optimization and Applications Vol. 3, pp. 27\22657 1994 16  A. B. Poore and R. Robertson, \223A New multidimensional data association algorithm for multisensor-multitarget tracking,\224 Proc. SPIE, Signal and Data Processing of Small Targets Vol. 2561,  p 448-459, Oliver E. Drummond; Ed., Sep. 1995 17  K. R. Pattipati, T. Kirubarajan, and R. L. Popp, \223Survey of assignment techniques for multitarget tracking,\224 Proc  on Workshop on Estimation  Tracking, and Fusion: A Tribute to Yaakov Bar-Shalom Monterey CA, May 17, 2001 18  P. Burns, W.D. Blair, \223Multiple Hypothesis Tracker in the BMD Benchmark Simulation,\224 Proceedings of the 2004 Multitarget Tracking ONR Workshop, June 2004 19  H. Hotelling, \223The generalization of Student's ratio,\224 Ann. Math. Statist., Vol. 2, pp 360\226378, 1931 20  Blair, W. D., and Brandt-Pearce, M., \223Monopulse DOA Estimation for Two Unresolved Rayleigh Targets,\224 IEEE Transactions Aerospace Electronic Systems  Vol. AES-37, No. 2, April 2001, pp. 452-469 21  H. A. P.  Blom, and Y. Bar-Shalom, The Interacting Multiple Model algorithm for systems with Markovian switching coefficients IEEE Transactions on Au tomatic Control 33\(8  780-783, August, 1988 22  M. Kendall, A. Stuart, and J. K. Ord, The Advanced Theory of Statistics, Vol. 3, 4th Edition, New York Macmillan Publishing, pg. 290, 1983 23  T.M. Cover and P.E. Hart, Nearest Neighbor Pattern Classification, IEEE Trans. on Inf. Theory, Volume IT-13\(1 24  C.D. Papanicolopoulos, W.D. Blair, D.L. Sherman, M Brandt-Pearce, Use of a Rician Distribution for Modeling Aspect-Dependent RCS Amplitude and Scintillation Proc. IEEE Radar Conf 2007 25  W.D. Blair and M. Brandt-Pearce, Detection of multiple unresolved Rayleigh targets using quadrature monopulse measurements, Proc. 28th IEEE SSST March 1996, pp. 285-289 26  W.D. Blair and M. Brandt-Pearce, Monopulse Processing For Tracking Unresolved Targets NSWCDD/TR-97/167, Sept., 1997 27  W.D. Blair and M. Brandt-Pearce, Statistical Description of Monopulse Parameters for Tracking Rayleigh Targets  IEEE AES Transactions, Vol. 34 Issue 2,  April 1998, pp. 597-611 28  Jonker and Volgenant, A Shortest Augmenting Path Algorithm for Dense and Sparse Linear Assignment Problems, Computing, Vol. 38, 1987, pp. 325-340 29  V. Jain, L.M. Ehrman, and W.D. Blair, Estimating the DOA mean and variance of o ff-boresight targets using monopulse radar, IEEE Thirty-Eighth SSST Proceedings, 5-7 March 2006, pp. 85-88 30  Y. Bar-Shalom, T. Kirubarajan, and C. Gokberk 223Tracking with Classification-Aided Multiframe Data Association,\224 IEEE Trans. on Aerospace and Electronics Systems Vol. 41, pp. 868-878, July, 2005   


 16 B IOGRAPHY  Andy Register earned BS, MS, and Ph  D. degrees in Electrical Engineering from the Georgia Institute of Technology.  His doctoral research emphasized the simulation and realtime control of nonminimum phase mechanical systems.  Dr. Register has approximately 20 years of experience in R&D with his current employer, Georgia Tech, and product development at two early-phase startups. Dr. Register\222s work has been published in journals and conf erence proceedings relative to mechanical vibration, robotics, computer architecture programming techniques, and radar tracking.  More recently Dr. Register has b een developing advanced radar tracking algorithms and a software architecture for the MATLAB target-tracking benchmark.  This work led to the 2007 publication of his first book, \223A Guide to MATLAB Object Oriented Programming.\224  Mahendra Mallick is a Principal Research Scientist at the Georgia Tech Research Institute \(GTRI\. He has over 27 years of professional experience with employments at GTRI \(2008present\, Science Applications International Corporation \(SAIC Chief Scientist \(2007-2008\, Toyon Research Corporation, Chief Scientist 2005-2007\, Lockheed Martin ORINCON, Chief Scientist 2003-2005\, ALPHATECH Inc., Senior Research Scientist 1996-2002\, TASC, Principal MTS \(1985-96\, and Computer Sciences Corporation, MTS \(1981-85 Currently, he is working on multi-sensor and multi-target tracking and classification bas ed on multiple-hypothesis tracking, track-to-track association and fusion, distributed filtering and tracking, advanced nonlinear filtering algorithms, and track-before-detect \(TBD\ algorithms He received a Ph.D. degree in  Quantum Solid State Theory from the State University of New York at Albany in 1981 His graduate research was also based on Quantum Chemistry and Quantum Biophysics of large biological molecules. In 1987, he received an MS degree in Computer Science from the John Hopkins University He is a senior member of the IEEE and Associate Editor-inchief  of the Journal of Advances in Information Fusion of the International Society of Information Fusion \(ISIF\. He has organized and chaired special and regular sessions on target tracking and classific ation at the 2002, 2003, 2004 2006, 2007, and 2008 ISIF conferences. He was the chair of the International Program Committee and an invited speaker at the International Colloquium on Information Fusion \(ICIF '2007\, Xi\222an, China. He is a reviewer for the IEEE Transactions on Aerospa ce and Electronics Systems IEEE Transactions on Signal Pr ocessing, International Society of Information Fusion, IEEE Conference on Decision and Control, IEEE Radar Conference, IEEE Transactions on Systems, Man and Cybernetics, American Control Conference, European Signal Processing Journal and International Colloquium on Information Fusion ICIF '2007   William Dale Blair is a Principal Research Engineer at the Georgia Tech Research Institute in Atlanta, GA. He received the BS and MS degrees in electrical engineering from Tennessee Technological University in 1985 and 1987, and the Ph.D. degree in electrical engineering from the University of Virginia in 1998. From 1987 to 1990, he was with the Naval System Division of FMC Corporation in Dahlgren, Virginia. From 1990 to 1997, Dr Blair was with the Naval Surface Warfare Center, Dahlgren Division NSWCDD\ in Dahlgren, Virg inia. At NSWCDD, Dr Blair directed a real-time experiment that demonstrated that modern tracking algorithms can be used to improve the efficiency of phased array radars. Dr Blair is internationally recognized for conceptualizing and developing benchmarks for co mparison and evaluation of target tracking algorithms Dr Blair developed NSWC Tracking Benchmarks I and II and originated ONR/NSWC Tracking Benchmarks III and IV NSWC Tracking Benchmark II has been used in the United Kingdom France, Italy, and throughout the United States, and the results of the benchmark have been presented in numerous conference and journal articles. He joined the Georgia Institute of Technology as a Se nior Research Engineer in 1997 and was promoted to Principal Research Engineer in 2000. Dr Blair is co-editor of the Multitarg et-Multisensor Tracking: Applications and Advances III. He has coauthored 22 refereed journal articles, 16 refereed conference papers, 67 papers and reports, and two book chapters. Dr Blair's research interest include radar signal processing and control, resource allocation for multifunction radars, multisen sor resource allocation tracking maneuvering targets and multisensor integration and data fusion. His research at the University of Virginia involved monopulse tracking of unresolved targets. Dr Blair is the developer and coordinator of the short course Target Tracking in Sensor Systems for the Distance Learning and Professional Education Departmen t at the Georgia Institute of Technology. Recognition of Dr Blair as a technical expert has lead to his election to Fellow of the IEEE, his selection as the 2001 IEEE Y oung Radar Engineer of the Year, appointments of Editor for Radar Systems, Editor-InChief of the IEEE Transactions on Aerospace and Electronic Systems \(AES\, and Editor-in- Chief of the Journal for Advances in Information Fusion, and election to the Board of Governors of the IEEE AES Society,19982003, 2005-2007, and Board of Directors of the International Society of Information Fusion   


 17 Chris Burton received an Associate degree in electronic systems technology from the Community College of the Air force in 1984 and a BS in Electrical Engineering Technology from Northeastern University in 1983.  Prior to coming to the Georgia Institute of Technology \(GTRI\ in 2003, Chris was a BMEWS Radar hardware manager for the US Air Force and at MITRE and Xontech he was responsible for radar performance analysis of PAVE PAWS, BMEWS and PARCS UHF radar systems Chris is an accomplished radar-systems analyst familiar with all hardware and software aspects of missile-tracking radar systems with special expertise related to radar cueing/acquisition/tracking for ballistic missile defense ionospheric effects on UHF radar calibration and track accuracy, radar-to-radar handover, and the effects of enhanced PRF on radar tracking accuracy.  At GTRI, Chris is responsible for detailed analysis of ground-test and flight-test data and can be credited with improving radar calibration, energy management, track management, and atmospheric-effects compensation of Ballistic Missile Defense System radars   Paul D. Burns received his Bachelor of Science and Masters of Science in Electrical Engineering at Auburn University in 1992 and 1995 respectively. His Master\222s thesis research explored the utilization of cyclostationary statistics for performing phased array blind adaptive beamforming From 1995 to 2000 he was employed at Dynetics, Inc where he performed research and analysis in a wide variety of military radar applications, from air-to-air and air-toground pulse Doppler radar to large-scale, high power aperture ground based phased array radar, including in electronic attack and protection measures. Subsequently, he spent 3 years at MagnaCom, Inc, where he engaged in ballistic missile defense system simulation development and system-level studies for the Ground-based Midcourse defense \(GMD\ system. He joined GTRI in 2003, where he has performed target tracking algorithm research for BMD radar and supplied expertise in radar signal and data processing for the Missile Defense Agency and the Navy Integrated Warfare Systems 2.0 office.  Mr. Burns has written a number of papers in spatio-temporal signal processing, sensor registration and target tracking, and is currently pursuing a Ph.D. at the Georgia Institute of Technology  


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


