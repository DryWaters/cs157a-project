The Multi-Knowledge Service-Oriented Architecture Enabling Collaborative Research for E-health  Michele Amoretti    Francesco Zanichelli Dip. di Ingegneria dell Informazione, University of Parma, Italy michele.amoretti, francesco.zanichelli}@unipr.it   Abstract  The introduction of e-Health services is facilitating access to healthcare, regardless of geographical location or time, thanks to innovative telemedicine and personal health systems. E-Health is also breaking down many barriers, enabling health 
service providers public author ities, hospitals different countries to work more closely together. In this context, the Multi-Knowledge platform may be regarded as a milestone, allowing geographically dispersed groups of biomedical researchers, dealing with different data sources as well as technological and organisational contexts, to create, exchange and manipulate new knowledge in a seamless fashion Moreover, the Multi-Knowledge project defined a methodological framework for research experiments involving heterogeneous data, that can easily be extended to include additional sources of knowledge and expertise \(biomedical data, images 
environmental data\nd can be applied to wider sectors of medical research     1. Introduction  E-Health can be broadly defined as the use of Information and Communication Technologies tools and services to provide better healthcare. Whether eHealth tools are used behind the scenes by healthcare professionals, or even directly by patients, they play a significant role in improving the health of citizens. EHealth covers the interaction between patients and health-service providers, institution-to-institution 
transmission of data, or peer-to-peer communication between patients and/or health professionals Examples include health information networks electronic health records, telemedicine services wearable and portable systems recording or communicating data, health portals, and many other ICT-based tools assisting disease prevention diagnosis, treatment, health monitoring and lifestyle management The Multi-Knowledge proj w h ich is  funded by the European Commission in the context of the Sixth Framework Programme for Research and Technological Development \(thematic area 
Information Society Technologies\es from the data processing needs of a network of medical research centres, located in Europe and USA performing research activities in the field of metabolic and cardiovascular diseases. These needs are mostly related to the integration of three main sources of information, namely clinical data, patientspecific genomic/ proteomic data \(in particular information acquired by means of microarray technology\d demographic data Critical and difficult issues addressed in the project concern the management of data which are 
heterogeneous in nature \(continuous and categorical with different order of magnitude, with different degree of precision, etc.\ \(statistical programs manual introduction from an operator, etc.\ and coming from different data environments \(from the clinical setting to the molecular biology lab Still, the main objective of the Multi-Knowledge project is related to the development and validation of a collaborative IT platform for knowledge management, allowing geographically dispersed 
groups of researchers, dealing with different data sources as well as technological and organisational contexts, to create, exchange and manipulate new knowledge in a seamless fashion. Moreover, the Multi-Knowledge project defined a methodological framework for research experiments involving heterogeneous data, that can easily be extended to include additional sources of knowledge and expertise \(biomedical data, images, environmental data\d can be applied to wider sectors of medical research The Multi-Knowledge platform enables workflow design and execution based on novel operating 
procedures to manage and combine heterogeneous data and make them easily available for data analysis It is based on a service-oriented architecture, where functionality is grouped around business processes and packaged as interoperable services. The MultiKnowledge platform is thus based on an IT infrastructure which allows different applications to exchange data with one another as they participate in Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 1 978-0-7695-3450-3/09 $25.00 © 2009 IEEE 


business processes. The aim is a loose coupling of services with operating systems, programming languages and other technologies which underlay applications The paper is organized as follows. In section 2 we discuss relevant literature on computer supported cooperative work \(CSCW\d the role of serviceoriented architectures in this context. In section 3 we recap the basic concepts of service-oriented architectures, which we deem necessary to fully appreciate the description of the MK platform provided in section 4, focusing on its functional properties which are made available by the synergetic interaction of several modules. In section 5 we describe the experiments which are being conducted across different sites. Finally, in section 6 the main results and contributions are summarized  2. Related work  From the technical point of view, the MK platform is a Computer-Supported Cooperative Work CSCW\stem. Many researchers of information systems criticize current CSCW systems based on their usability and users satisfaction. The MultiKnowledge project has been entirely conducted with the supervision of end users, i.e. biomedical researchers, biostatisticians and also practicing physicians. This approach, suggested for example by Ru lead th e con s ortium to obtain a higher-quality application, and better acceptance which is particularly important in the case of collaborative systems The first feature demanded to the MK platform is knowledge sharing support. As suggested by Kindberg et w e distingu ish e d bet w ee n  s e v e ral  types of knowledge: data, domain, users \(their competences, their needs\. The premise of MKsupported activities is that researchers from different organizations and institutes agree on sharing the anonymized\ey have on their patients, and to exchange specialized knowledge. The issue of exchaning patient-related data is being partially solved by the increasing adoption of electronic medical records \(EMRs\nstead of traditional paper medical records \(PMRs\. Bringay et erv e d  that practitioners still prefer to use the PMR to collaborate, because electronic medical documents do not allow reproducing some practices of collaboration carried out with the PMR, in particular one practice: annotations which are used as support for the collaboration. In the research context of the Multi-Knowledge project, EMRs are required since clinical data must be automatically integrated with genomic/proteomic data \(in particular information acquired by means of microarray technology\nd demographic data Knowledge sharing is just one kind of cooperative activity the MK platform was demanded to support The other categories, according to Bardram s classification org an ization of  w o r k plan n i ng and scheduling, and communication, with the general objective of creating new knowledge. In the first phase of the project we explicitly detailed these activities, with particular emphasis on the identification of different roles for human actors \(as we summarized in section 2\ole-Based Collaboration \(RBC\ theory is a natural approach to integrate the theory of roles into the CSCW systems 6,7 From the specification of user requirements, the importance of the Process Modelling component including Knowledge Extraction\ of the MultiKnowledge system definitely emerged. Workflow management technology is not used in healthcare as often as in other domains. Healthcare workflows have transactional elements, such as admitting a patient or taking a blood glucose measurement, but focusing on individual transactions obfuscates the most important element of healthcare workflows - the need to flexibly promote and maintain the highest possible standard of care for patient t h e f i eld  of biomedical research, Workflow Management Systems \(WMS\een as a viable solution for the creation and deployment of new flexible and extensible data integration and analysis network tools 10,1 o m e WM S  h a v e been propos ed [12,1 3] an d are now under careful testing aimed at the verification of their actual ability to cope with the data integration issue. While their potential is clear some limitations are now arising, including both practical issues \(e.g. quality of service, speed, access restrictions\d computational issues \(e.g., long running jobs, huge input/output At the end of section 3 we discuss some serviceoriented platforms for e-Health, compared with Multi-Knowledge objectives  3. Service-oriented architectures  The Service-Oriented Architecture \(SOA design style defines the use of loosely coupled software services to support the requirements of distributed applications. SOAs achieve loose coupling among interacting services by employing a small set of simple and ubiquitous interfaces to all participating software entities, together with Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 2 


descriptive messages constrained by an extensible schema delivered through the interfaces In an SOA-based distributed environment, shared resources \(applications and data, mainly\made available on demand as independent services that can be accessed without knowledge of their underlying platform implementation. Service requestors can be end users \(with client tools\ or other services SOA promotes reuse at the macro \(service\level rather than micro levels \(e.g. objects\us simplifying interconnection to and usage of existing IT \(legacy\ssets. This fundamental property derives from services being composable, stateless discoverable and loosely bound together by a common compliance to a standard communications framework \(while the encapsulated programming logic needs not to comply with any platform or technology set\ details, SOAs are messageoriented, since interoperability is achieved by means of messages, which are data formatted as XML documents and built compliant with the rules dictated by the contract associated with the service  SOAs based on the traditional client/server  paradigm are characterized by server applications hosted by always-on end systems, which provide services to many other client applications hosted by sometimes-on end systems. Servers are often required to have static, well-known IP addresses In a peer-to-peer based SOA, arbitrary pairs of peer application entities communicate directly with each others. In a peer-to-peer architecture, none of the participant hosts is required to be always on moreover, a participating host may change its IP address each time it comes on. Scalability is one of the greatest strengths of peer-to-peer architectures For example, in a peer-to-peer file sharing application, most users are also downloading files that other users share, thus contributing vast storage and networking resources, so that the overall performance highly exceeds what can be offered by centralized servers. However, because of the highly distributed and decentralized nature of peer-to-peer applications, they can be difficult to manage. A service can be offered or withdrawn by a peer at any time. Clients have to discover whether, and where, a service is provided Several design patterns for SOAs have emerged with reference to issues such as integration styles messaging systems and system management. For a detailed description of these patterns, which is out of the scope of this paper, refer to [27   3.1. Service workflow paradigms  Workflow is the operational aspect \(process logic of a work procedure: how tasks are structured, who performs them, what their relative order is, how they are synchronized, how information flows \(routing rules\ to support the tasks and how tasks are being tracked. Significant effort is being put into defining workflow patterns at c a n be us ed to co m p are and contrast different workflow engines across both people-based and rule-based processes Current workflow solutions define client-server system architectures where the workflow server is monolithic and centralizes critical orchestration services such as process management, activity distribution, work list management, and directory services. Centralizing workflow functionalities offers important benefits \(e.g. tracking\, yet it becomes difficult to address the needs of distributed workflows on a WAN. Since the workflow reference model trodu ced on l y  th e idea o f dis t rib u ted processes without defining a notation for expressing peer-to-peer interactions, later models have been proposed. The orchestration model provides a scope specifically focusing on the view of one participant whereas a choreography model encompasses all parties and their associated interactions giving a global view of the system on e s i de, t h e choreographic model aims at constraining the behaviour of the services involved in the system ruling the exchange of their messages and distributing the state of the activity among the entities. On the other hand, orchestration is based on a central entity \(the orchestrator\hich carries out a business activity invoking other services and maintaining the global state  3.2. Service-oriented platforms for e-Health  In the e-Health context, the COCOON [14  project is aimed at activating regional semanticsbased healthcare information infrastructures with the goal of reducing medical errors. Another recent initiative is ARTEMIS [15   w h o s e o b j ectiv e is to  develop a semantic framework for the healthcare domain, building upon a peer-to-peer architecture in order to facilitate the discovery of healthcare services. Examples of such services are those listed by the Biological Web Services \(BWS\ page  Among all, GeneCruise a Web S e rv ice f o r the annotation of microarray data, developed at the Broad Institute \(a research collaboration of MIT Harvard and its affiliated hospitals\eCruiser allows users to annotate their genomic data by mapping microarray feature identifiers to gene Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 3 


identifiers from databases, such as UniGene, while providing links to web resources, such as the UCSC Genome Browser. It relies on a regularly updated database that retrieves and indexes the mappings between microarray probes and genomic databases Genes are identified using the Life Sciences Identifier standard. A more complex example of Web Serviceoriented architecture providing transparent access to biomedical applications on distributed computational resources is the National Biomedical Computation Resource \(NBCR h i c h i s bas e d on Gri d  technologies such as Globus Toolkit. NBCR users are allowed to design and execute complex biomedical analysis pipelines or workflows of services Compared to Multi-Knowledge, all these initiatives lack in the definition of a collaborative platform in which different actors participate in the workflow execution, with experiment steps defined by and conducted under the responsibility of a principal investigator coordinating a research team which may include biomedical researchers bioinformaticians, statisticians, etc  4. The Multi-Knowledge platform  In this section we analyze the main functionalities of the Multi-Knowledge distributed platform \(see figure 1\which is a service-oriented architecture SOA\ based on the peer-to-peer paradigm, in which services are provided by different nodes and \(by default\not listed in a centralized directory. The modularity of the system allows for a wide range of compositions of service-providing nodes, which may share their services by means of distributed publish/disocvery mechanisms, but also using a centralized repository if the specific application does not present scalability issues Actually, the Multi-Knowledge platform integrates the following reusable modules   MK-PORTAL platform entry point, report browser, etc  MK-SEC security management tool  MK-DCNS data entry mask designer, data entry and integration tool  MK-DA statistical analysis and data mining tool  MK-VIZ visualization tool  MK-REP reporting tool  MK-WF workflow designer, workflow execution engine  We discuss how these modules are involved in knowledge management, data collection, data analysis and workflow design and execution. We also provide some technical insights, even though basically all modules adopt Web Services in order to be interoperable and to facilitate the workflow engine in their orchestration  4.1. Knowledge management  The Multi-Knowledge platform can be configured to cope with the usability needs of completely different users: practicing physicians involved in clinical data collection, biomedical researchers charged with genomic data normalization, principal investigators and biomedical researchers interested in browsing and downloading workflow descriptions and experiment reports. The entry point to the system is always the MK-PORTAL, which allows to obtain the MK-DA \(which includes the MK-REP\e MKWF, and the MK-VIZ, and to browse documents reports, workflow templates, etc The MK-PORTAL module is based on two technologies: Joomla!, which is a popular Content Management System \(CMS\ and Web Application Framework \(evolution of Mambo\, e and Java Server Pages \(JSP\, which enables rapid development of web-based applications that are server- and platform independent     Figure 1. The Multi-Knowledge platform and the users which participate in the knowledge extraction process  The MK-SEC module, developed in Java, is the glue between the MK-PORTAL and the other MK modules, which have been developed with J2EE/.NET technologies. The integration of Joomla Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 4 


and J2EE/.NET technologies is unusual, to the best of our knowledge. From this point of view, the MKSEC is a pioneering artefact, defining a strategy for session sharing among Joomla s PHP pages and the JSP/ASP pages of the other MK modules, which are deployed in different web application engines. By achieving session sharing, it has been possible to introduce Single Sign-On \(SSO\method of access control that enables a user to authenticate once and gain access to differently deployed MultiKnowledge modules. Another feature is role-based access, for which e.g. users with principal investigator access level are allowed to access most Multi-Knowledge modules, but they are not allowed to associate patients' clinical data with their vital statistics, for which only only practicing physicians are responsible  4.2. Data collection and normalization  The Data Collection and Normalization System \(MKDCNS\s composed by two sub-modules, namely Data Entry System \(MK-DES\d Data Integration System \(MK-DIS The data entry procedure, allowed by the MK-DES module, is straightforward \(users in this context are practicing physicians\ clicking on the Data Entry link of the MK-PORTAL, a new window of the browser opens, showing the main page of the MKDES. Here the user can perform a search \(by first name, last name, or code\create a new entry. The MK-DES forms accept clinical data \(conforming to HL7 standards\ well as genomic and proteomic data resulting from microarray measurements. These are given as a set of feature extraction \(FE\iles, each one representing an experiment and containing all the data derived from the related microarray. Each expression FE file contains data on about 40000 genes and each protein FE file contains data on about 100 proteins. Typically, the first transformation applied to expression data, referred to as normalization, adjusts the individual hybridization intensities to balance them appropriately so that meaningful biolog ical comparisons can be made There are a number of reasons why data must be normalized, including unequal quantities of starting RNA, differences in labeling or detection efficiencies between the fluorescent dyes used, and systematic biases in the measured expression levels  Furthermore, metabolomics data are given as tab delimited text files with two columns. The first column contains the meta bolite description and the second column contains the corresponding numerical values and units. Finally, IMT and FMD data from each patient are entered to the system through the MK-DCNS GUI. The MK-DIS module allows to generate and normalize a data matrix integrating clinical data, FE files, IMT and FMD data. Both MKDES and MK-DIS rely on a set of Web Services running in a JBoss container  4.3. Data analysis, visualization, and reporting  A Multi-Knowledge experiment is a set of successive data analysis cycles, aimed at extracting new knowledge from integrated heterogeneous data clinical, demographical, genomic and proteomic managed by a diverse set of researchers. Data analysis steps form the core of the experiment's analysis cycle. Through them, the data sample is successively analysed by different classes of researchers \(having different "scientific cultures" and backgrounds\hat use different analysis tools, work in different environments, at geographically dispersed sites The Data Analysis Tool \(MK-DA\n Eclipse plug-in, developed in Java, supporting several data mining processes, such as GO \(Gene Ontology analysis, classification, clustering, class discovery and sequence motifs finding. The MK-DA is integrated with the Visualization Tool \(MK-VIZ module, which includes a Java-based Graph Generator component and a set of .NET-based Web Services that bring information related to the genes by accessing external genomic databases. The Graph Generator component is based on Mayday  open source, Java-based tool for processing matrix related data and providing interactive data visualization. The Mayday platform has been fully customized in order to adapt to Multi-Knowledge requirements, input data set and desired output schemas and graphical plots and figures The MK-DA is also integrated with the Report Generator and Manager \(MK-REP\ module including a Report Generator which builds PDF reports using the experiment description produced by the MK-WF module, together with data analysis results and images produced by the MK-VIZ tool The MK-REP module also includes a ReportManager Web Service, which can be accessed by the MK-DA tool to store or retrieve reports in a specific database  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 5 


  Fig. 2. MK-VIZ ranked heat map and gene information screens for gene NM_024656  4.4. Workflow management  We have illustrated how the MK platform supports data collection and normalization, as well as data analysis, visualization and reporting. In the following we focus on the orchestration of a whole experiment. As we already stated, a MultiKnowledge research experiment consists of a set of experiment steps, defined by and conducted under the responsibility of a research team, coordinated by a Principal Investigator, that aims at achieving an predefined scientific goal Each data analysis step may generate new knowledge elements that contribute to create and successively expand an experiment-related body of knowledge \(EBoK Based on an analysis of the EBoK \(performed from their different scientific point of views\ research team members can propose the execution of additional experiment steps or to further carry on the process. This means that we may see the process as a spiral in which every cycle allows a better focus on the scientific objective which was initially stated for the experiment. The combination of conducting experiments based on previously conducted ones and the evaluation of each  Fig. 3. Example of workflow design  experiment is very important for the researcher to assess the value of each experiment and in the overall scientific process It is important to note again that the crucial objective of the Multi-Knowledge Process Modelling component is to guarantee the seamless integration of the different contribution brought in by the different research team members. This is not an always easy task because, for instance, some data analysis steps may be proposed/requested by researchers that do not have the specific expertise to conduct them while vice versa, those who have the expertise to conduct them may not be able to understand the full, specific implications of the resulting knowledge The Workflow Designer and Execution Engine MK-WF\ws principal investigators to define experiment steps to be conducted asynchronously or according to declared workflow patterns, passing control back and forth from different researchers. The experiments consist of dynamical cycles of data collection and analysis that aim at progressively achieving the scientific goal initially stated for the experiment When a team member, possibly after receiving a suggestion sent by another team member or by the principal investigator, decides to execute an experiment step, he/she  revises the proposed experiment step definition \(a XML file\d possibly improves it based on her/his specific knowledge  executes the experiment step  generates a report, presenting the motivations for the experiment step as well as comments on step's execution and outcome  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 


During the execution of the experiment step, the workflow engine service activates the required modules \(e.g. MK-DCNS for the data matrix generation, MK-DA for the data analysis, MK-VIZ for imaging, MK-REP for reporting\. More specifically, the Workflow Engine service communicates with the appropriate modules of the platform in each experiment step, providing the necessary functionality to the user for accomplishing each specific step The MK-WF module interacts with the Data Collection and Normalization \(MK-DCNS\module for setting the various experiment parameters for normalized data matrix to be generated. The MKDCNS module generates the normalized matrix and stores it in a database Finally, the MK-WF module interacts indirectly with the Data Analysis \(MK-DA\ the following cases   Retrieving the normalized matrix in order to be accessible from the MK-DA module running on the client machine   Checking if the MK-DA application is installed in the client machine. If it is not installed then the module downloads it from the MK-PORTAL and installs it on the client machine After data analysis has been performed and a report has been generated, the MK-WF module uploads the XML file containing the executed experiment log to the MK-PORTAL and stores the contained information to the Workflow database for future reference to the performed experiment Another functionality of the MK-WF module is that it enables users to conduct experiments based on previously conducted ones. In other words previously conducted experiments can be loaded and new experiments can be executed after changing same parameters The MK-WF module was implemented under the Microsoft .NET Framework version 2.0. This includes the Workflow Execution Engine implemented as a Web Service, as well as the workflow and knowledge extraction user interfaces constructed as a Microsoft smart-client application This approach combines the advantages of web applications with the enriched user interface of a native application and is suitable for user needs concerning experiments execution  5. Pilot experiments  In a first instance of Multi-Knowledge pilot study clinical, laboratory, instrumental and genomic information has been collected from 50 subjects by the Department of Internal Medicine of the University of Parma. The sample has been used to validate the first Multi-Knowledge platform prototype, in particular the system modules related to data collection and normalization. Presenting biomedical results is out of the scope of this paper but the interested reader can refer e.g  The second instance of the pilot experiment has required further recruitment, up to a sample of about 150-200 subjects. This study is being performed to test a full-featured Multi-Knowledge system prototype, involving Internal Medicine Department at University of Parma, King s College London Stanford University Medical Center, Technion Tel Aviv, University of Milan, and in conjunction with two related projects, namely the European Pocemon  d th e Italian S y m p ar 22 T h es e partn e rs are  testing in particular the MK-WF and the MK-DA to exchange information about analysis workflow and to perform incremental data analysis as the pilot data set is modified  6. Conclusions  The Multi-Knowledge platform is a relevant advancement in the e-Health landscape, supporting geographically dispersed groups of researchers to create, exchange and manipulate new knowledge in a seamless way. In the first part of the paper we presented relevant CSCW literature, focusing on knowledge sharing and workflow management systems. We compared the Multi-Knowledge project to other e-Health initiatives funded by the EU, which have similar purposes such as creating and sharing integrated biomedical information for better health In the second part of the paper, we described the functionalities provided by the MK platform, with particular emphasis on services. Finally we summarized the activities carried out in the first pilot experiment, and those that are being conducted in the second \(more complex\periment  7.  Acknowledgments  This work has been partially supported by the European Commission in the frame of the Project MULTI-KNOWLEDGE \(Sixth Framework Programme, IST Priority, Grant #FP6-IST-2004027106    Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


8.  References  1 MU L T I-KN O W L E DG E C ons orti um proje c t  homepage http://www.multiknowledge.eu   2 Rup p e l C., K one c n y  J   T h e role of IS P e rs onne l in  Web-based Systems Development: The Case of Health Care Organization. In: ACM SIGCPR Conference on Computer Personnel Research, pp. 130--135. ACM Press New York \(2000  3 K i n dbe rg T Bry a nn-K i nns N  Ma k w a n a  R  Suppoting the shared care of diabetic patients. In International ACM SIGGROUP Conference on Supporting Group Work, pp. 91--100. ACM Press, New York \(1999  4 Bring a y S., Ba rr y  C., Cha r le t, J  A nnota tions A  Functionality to support Cooperation, Coordination and Awareness in the Electronic Medical Record. In Cooperative Systems Design \(COOP 06\p. 39--54. IOS Press, Amsterdam \(2006  5 Ba rdra m   J  E.: Colla bor a tion, Co ord i na ti on, a n d  Computer Support, An Activity Theoretical Approach to the Design of Computer Supported Cooperative Work. PhD Thesis, University of Aarhus, Denmark \(1998   E d w a rd s, W  K P o l i cies an d Ro les i n Co l l ab o r ative  Applications. In: ACM Conference on ComputerSupported Cooperative Work \(CSCW 96\p. 11--20 Cambridge, USA \(1996  7 G u z d ia l M Ric k J., a n d Ke rim b a e v  B.: Re c o g n iz ing  and Supporting Roles in CSCW. In:  ACM Conference on Computer-Supported Cooperative Work \(CSCW 00\p 261--268. Philadelphia, Pennsylvania, USA \(2000  8 Sm ith, R. B., H i x on, R. a nd H o ra n, B.: Su pp orti ng  Flexible Roles in a Shared Space. In:  ACM Conference on Computer-Supported Cooperative Work \(CSCW 98\p 197--206. Seattle, Washington, USA \(1998  9 L e Moine  D.: G o ing w ith the Flow Inte r a c tion De sig n  for Healthcare. In: Journal of Design, Cooper Consulting 2003  10 S t e i n, L  Cre a ting a bi oinf o r m a tic s na tion. I n Na ture   No. 417, pp. 119--120 \(2002  11 Rom a no P  Ra s i C. a nd Ma rra D  T h e a u tom a tion of bioinformatics processes through workflow management systems. In: Seventh Spanish Symposium on Bioinformatics and Computational Biology \(JdB06 Zaragoza, Spain \(2006  12 O i nn, T A ddis  M. e t a l   T a ve rna  a tool f o r the  composition and enactment of bioinformatics workflows In: Bioinformatics, Vol. 20, No. 17, pp. 3045--3054 \(2004  13  Ste v e n s  R., R obi ns o n A  a nd G oble  C  m y G r id personalised bioinformatics on the information grid. In Bioinformatics, Vol. 19, No.1, pp. 302--304 \(2003 1 COCOON con s o r ti u m p r o j e c t h o m ep age   http://www.cocoon-health.com  15 A R T E MIS c ons or tium  proje c t hom e p a g e    http://www.srdc.metu.edu.tr/webpage/projects/artemis/inde x.html  16 H u ll, D  T h e Biol og ic a l W e b Se rv ic e s pa g e  http://taverna.sourceforge.net/index.php?doc=services.html  17 L i e f e l d, T Re ic h, M G oul d, J   Zha n g   P T a m a y o   P. and Mesirov, J. P.: GeneCruiser: a Web Service for the annotation of microarray data. In: Bioinformatics, Vol. 21 No. 18, pp. 3681--3682 \(2005  18 K r is h n a n  S., Ba l d ri dg e  K G r e e nbe rg J Ste a r n, B   and Bhatia, K.: An End-to-end Web Services-based Infrastructure for Biomedical Applications. In: 6th IEEE/ACM International Workshop on Grid Computing Seattle, Washington, USA \(2005  1 Dietzsch  J  G e h l en bo rg N  an d Nieselt K M a y d a y  a microarray data analysis workbench. In: Bioinformatics 2006 22\(8\:1010-1012  20 Ste i nf e l d, I., N a v on, R A r dig  D Za v a roni I. a n d  Yakhini, Z.: Semi-supervised class discovery using quantitative phenotypes CVD as a case study. In  BMC Bioinformatics, 8\(Suppl 8  S6 \(2007  21  Poc e m on pro j e c t de s c ripti on a t P C S homepage http://www.pcs.at/index.php?id=38&L=1#c193  22 Sy m p a r C ons ortium  proje c t h o m e pa g e   http://www.sympar.org  23 He H What is Service-Oriented Architecture  http://webservices.xml.com/pub/ a/ws/2003/09/30/soa.html September 2003  24 v a n de r A a lst W  M P   te r Hof s te de  A  H.M  Kiepuszewski, B., Barros, A.P.: Workflow Patterns. In Distributed and Parallel Databases, 14\(3\pp. 5--51, July 2003  25 Ho lling s w o rth, D.: T h e W o rk f l o w Re fe re nc e Mode l  Workflow Management Coalition, January 1995  26  P e ltz C. : W e b se r v ic e s orc he s t ra tion a n d  choreography, IEEE Computer, Volume: 36,  Issue: 10, pp 46- 52, October 2003  27 Er l, T  SOA De sig n P a tte rns  P r e n tic e Ha ll 20 08    28 Q u a c k e nbus h, J  M i c r oa rra y da ta norm a liz a tion a n d  transformation. Nature Genetics Supplement, 32, pp. 496-501, December 2002   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


 arctan   9 the longitude of the site at Cartesian coordinates \(x,y centered on the pole.  The equation uses the variable L1 for the longitude where the horizon mask crosses \226   L sin 0 0       212     212 002 else 0 1 1 1  xy 212 005  t 004   004 L  DTE L  002   t  005  002 212  Hp d R     7  Comparing these theoretical e quations with the computed illumination and DTE communications averages required using an adjusted lunar reference altitude at the poles.  The lunar reference geoid radius of 1738 km is not a good average value for the terrain at either pole.  The lunar south pole is part of the Aitken Basin, and the GSSR Digital Elevation Model elevations ha ve an average of -1880 m within 200 km of the south pole.  The lunar north pole region has an average elevation of -1673 m within 150 km of the north pole.  These averag es were subtracted from base site reference altitude to get the value of \221h\222 used in equation 2.  This effectively gaves the base sites a higher elevation relative to the local average elevation  For selected base sites, solar illumination and DTE communications averages were computed for tower heights from 2 m to 1500 m, and in some cases 3000 m.  Then these tower heights plus adjusted altitudes and base locations were used in equations 1 through 7 to get predicted values of the yearly average metrics.  The results are shown in figures 14 and 15.  Figure 14 shows black curves of equation 5 plotted parametrically with distance from the pole.  For the selected sites, the computed yearly average solar illumination is shown by squares, connected by a dashed line with the theoretical yearly average for the same location and tower heights The theoretical values are marked with X\222s.  South lunar Pole sites are in blue and the lunar north pole sites are in red.  The plots show reasonable agreement of computed values and theory, with the computed value never exceeding the theoretical.   Figure 15 shows black curves of equation 7 plotted parametrically with distance from the pole.  Th e value of L0 was set to zero for the parametric curves.  Fi gure 15 include computed and theoretical values of the yearly average DTE communications for the same sites as figure 14, using the same symbol and color conventions  At no site will a tower of 1500 m or less provide both 100 average yearly solar illumination and 100% average DTE communications.  Towers of 1500 m at sites NB and NC will raise the average yearly solar illumination to 100 But for the lunar south pole sites, like B1, towers of over 2500 m are required to rais e the average yearly solar illumination to 100%.  This is because the peaks Malapert 002 peaks  t\002 1  212  0 1 4 R      6  The yearly average Earth co mmunications, DTE, is then given by the equation        L L  L L Hm R  8  The obscuring effect of these p eaks is also shown in figure 5, where the peak Malapert t 006 002    L  2 cos 180 abs 180 sin 006 212 003 003  002 latitude These variables are   003  and Leibnitz 002 obscure the view of the sun from the lunar south pole.  Equation 8 gives the elevation, Hp, above a spherical surface needed to see the winter sun over a peak of elevation Hm at a distance, d from the pole.  Equation 8 provided values in Figures 4 and 14 for the Malapert 003 and Leibnitz 212 003 is at +10 degrees azimuth  Figure 15 shows the yearly average DTE communication varies more with distance fr om the pole toward the prime meridian that with tower height above the local terrain Towers up to 3000 m at the poles do not make significant improvements in the DTE co mmunications average.  The computed yearly average DT E communications for site NG exceed the predicted values fo r towers between 32 and 1500 meters tall.  This is because the view northward from site NG toward the Earth looks across the floor of Byrd Crater which averages about -2500 m elevation  Considerations for Photovoltaic Array Design  The lunar pole sites create some difficult challenges for designing a continuously operating photovoltaic power system.  A site with continuous solar illumination will see the Sun travel through 360 degrees of azimuth during 1 synodic month.  A photovoltaic power system would require several permanently mounted photovoltaic arrays pointed in several directions, or would have to continually track the Sun with a single array.  Sin ce a single array requires much less mass, the design of the solar tracking system is examined here  The requirement to continually track the Sun could be met by installing the photovoltaic array on a vertical axle oriented parallel to the lunar spin axis.  At the lunar south pole, this axle would rotate counterclockwise with the Sun to keep the array normal to the solar radiation.  This requires a foundation to support the axle cross beam structures to hold the array on the axle, and a drive system to turn the array.  This system also requires a solution to the \221cable wrap\222 problem, in order to continually track the Sun.  After 1 synodic month the array has turned through 360 degrees and needs to continue rotating in the same direction.  Power and control cables from the array need to go through multiple slip rings, or the array needs to rotate 360 degrees clockwise to unwrap the cables.  A slip ring design is further complicated by the requirement to tolerate the fine dust on    arccos 004  005  b t cos if 


  10 the lunar surface.  Using the \221cable unwrap\222 method means stopping power production for a short amount of time, once per synodic month  A heliostat design using a focusing mirror may provide a low-mass solution to the \221cable wrap\222 problem [Ref 10   Figure 16 shows a photovoltaic array installed horizontal to the local terrain.  A parabolic section mirror is installed above the array, angled at approximately 45 degrees to the local horizontal.  This mirror is suspended from the crossbar of a mast placed next to the array.  The mast could also be the structure for the Lunar Co mmunications Terminal \(LCT antennas.  The mirror is suspe nded from an axle that is oriented parallel to the lunar spin axis and rotates once per synodic month to continually reflect solar radiation onto the array.  The mirror can continue to track after 360 degrees revolution, since there are no cables attached to the moving part.  This heliostat \221periscope\222 design provides continuous solar tracking, but is partially shaded once per revolution when the Sun passes behind the support mast  Shaping the mirror into a para bolic arc concentrates solar power for a small increase in mass.  The tradeoffs should consider the relative mass-perunit area of the mirror and the photovoltaic panels, the reflectivity of the mirror material and the relative masses of an array-support axle versus a mirror heliostat axle plus mast  Additional flat \221periscope\222 mirrors mounted on the crossbar could be used to direct solar light to areas nearby.  This provides an efficient method of lighting work areas because it bypasses the inefficiencies of the photovoltaic system battery storage, and electric lighting.  If the photovoltaic power system is located on a crat er rim \(as in Figure 16\a flat \221periscope\222 mirror could provide continuous lighting to exploration crews in the perman ently shadowed areas within the crater.  Sufficiently large heliostat mirrors could redirect enough sunlight to run photovoltaic arrays at remote locations.  This would provide very efficient power transfer for operating within the permanently shaded areas  5  C ONCLUSIONS   Separating the lunar pole solar illumination problem into terrain horizon mask and solar motion computations simplified the production of illumination metrics.  Using a simplified model of average solar motion allowed quick comparisons between lunar south pole sites.  The results show that within 100 km of the lunar south pole, solar illumination is maximized at previously identified site locations.  These sites are small areas on the peaks of isolated mountains and crater rims.  No site provides 100 yearly average for both solar illumination and DTE communication.  Lunar north pole sites were also examined No site at either pole prov ides 100% yearly average for solar illumination unless a tower at least 1500 m is placed on one of the best lunar north pole sites  The best lunar south pole sites are on the rim of Shackleton crater, and the ridge west of Sh ackleton crater.  These 3 sites have yearly average of solar illumination between 93 and 97%.  The Sites have 100% solar power generation capability about 85 to 92% of the year.  For both of these metrics, Site B1 on the ridge West of Shackleton crater has the highest solar illumination values.  Site B1 has direct-toEarth visibility of the entire Earth disk about 51% of each month.  More detailed analysis of site B1 can use the terrain horizon masks presented here, coupled with more detailed true-of-date solar motion computations  Examination of the site solar illumination profiles at 40meter resolution shows the peaks of illumination are distributed over many points.  The locations with greater than 80% solar illumination are spread over 100s of meters This supports the conclusion that the Digital Elevation Model has adequate sampling to show the true terrain characteristics  The lunar north pole base sites NB and NC have yearly average solar illumination around 97%, and have 100 solar power generation capability about 88 to 92% of the time for the average year  The yearly average for 100% DTE communications at the best lunar north pole and south pole sites are all around 50%.  The yearly average DTE communications does not vary significantly with tower height, but does increase to 100% for sites about 100 km from the poles toward the lunar prime meridian 6  A CKNOWLEDGMENTS  This research was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration  The author expresses thanks to Martin A. Slade of JPL\222s GSSR group for his time and assistance with the 2006 GSSR Digital Elevation Model data.  The author also wished to thank Charles Ruggier and Laif Swanson of the SCiP Study Team at JPL for their support of this work  R EFERENCES   1. Scott Hensley, Eric Gurrola Paul Rosen, Martin Slade Joseph Jao, Micheal Kobrick, Raymond Jurgens, Eric De Jong, and Barbara Wilson, \223RADAR generates highresolution topographic map of the Moon\224, Society of PhotoOptical Instrumentation Engineers Newsroom, June 16 2008  2.  J. L. Margot, D. B. Campbell, R. F. Jurgens, M. A Slade, \223Topography of the Lunar Poles from RADAR 


  11 Interferometry: A Survey of Co ld Trap Locations\224, Science Vol 284, pg 1658-1660, 4 June 1999  3.  NASA, \223The Vision for Sp ace Exploration\224,  Feb. 2004 http://www.nasa.gov pdf/55584main_vision_space_explorat ion-hi-res.pdf  4.  M. Kruijff, \223The Peaks of Eternal Light on the Lunar South Pole: How they were found and what they look like\224 4th International Conference on Exploration and Utilization of the Moon \(ICEUM4\ESA/ESTEC, SP-462, September 2000  5.  J. Fincannon, \223Lunar South Pole Illumination: Review Reassessment, and Power Sy stem Implications\224, 5 th  International Energy Conversion Engineering Conference and Exhibit \(IECEC\, 25-27 June 2007, AIAA 2007-4700  6.  J. Fincannon, \223Charact erization of Lunar Polar Illumination From a Power System Perspective\224, 56 th AIAA Aerospace Sciences Meeting and Exhibit, 7-10 January 2008, AIAA 2008-0447  7.  M. Zuber, I. Garrick-Be thell, \223What Do We Need to Know to Land on the Moon Again?\224, Science, Vol 310, pg 983-985, 11 November 2005  8.  Gazetteer of Planetary Nomenclature, International Astronomical Union, 03/31/2008  9.  Ewan A. Whitaker, \223The Lunar South Polar Regions\224 Journal of the British Astronomical Association, Vol. 64 No. 6, pp. 234-242  10. James D. Burke, \223Merits of A Lunar Polar Base Location\224, Lunar Bases and Space Activities of the 21st Century. Houston, TX, Lunar and Planetary Institute, edited by W. W. Mendell, 1985, p.77  B IOGRAPHY   Scott H. Bryant is a member of the senior staff at the Jet Propulsion Laboratory. He has a bachelor\222s in Aeronautics and Astronautics Engineering from MIT and a master\222s in Astronautics from USC.  He has worked for the Jet Propulsion Laboratory for over 10 years, working on several of JPL\222s Deep Space Network \(DSN\s.  His projects include the receivers, exciters, and spacecraft tracking subsystems.  He has been principally involved with software design and development for spacecraft tracking, including holding the position of cognizant design engineer for the current DSN Ranging system. Scott is currently the implementation and design lead for the spacecraft tracking and ranging portion of JPL\222s Network Simplification Project.  He has also work ed with the study groups examining communications and navigation issues for the Constellation system lunar studies.  The work presented here was performed for the SCiP Lunar Architecture study group  


  12  Figure 1:  Lunar South Pole Elevation Map from GSSR 2006 Digital Elevation Model  Figure 2:  Lunar North Pole Elevation Ma p from GSSR 1997 Digital Elevation Model 


  13  Figure 3:  Shackleton Crater T opographic Map, with Base Sites  Figure 4: Solar Lines-of-Si ght at Lunar South Pole 


  14  Figure 5:  Site B1 Terrain horizon ma sk with 1 degree azimuth spacing  Figure 6:  Site B1 Terrain horizon mask with 1 de gree azimuth spacing, in e quatorial coordinates 


  15  Figure 7: Lunar South Pole Solar Illumination Yearly Average  Figure 8:  Lunar South Pole DTE Visibility Yearly Average 


  16  Figure 9: Lunar North Pole Sola r Illumination Yearly Average  Figure 10:  Lunar North Pole D TE Visibility Yearly Average 


  17  Figure 11: Site A1 Elevation Topography  Figure 12: Site A1 Yearly Average Solar Illumination and DTE visibility, Medium Resolution 


  18   Figure 13:  Site LB Te rrain Horizon Mask  Figure 14:  Theory and Computed values of Average Yearly Solar Illumination 


  19  Figure 15:  Theory and Computed values of Average Yearly DTE Communication  Figure 16:  Heliostat Mirror Design to Eliminate Cable Wrap 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


