Mapping Mutable Genres in Structurally Complex Volumes Ted Underwood Michael L Black Department of English University of Illinois Urbana-Champaign Urbana IL USA tunder@illinois.edu black7@illinois.edu Loretta Auvil Boris Capitanu Illinois Informatics Institute University of Illinois Urbana-Champaign Urbana IL USA lauvil@illinois.edu capitanu@illinois.edu Abstract To mine large digital libraries in humanistically meaningful ways we need to divide them by genre This is a task that classiìcation algorithms are well suited to assist but they need adjustment to address the speciìc challenges of this domain Digital libraries pose two problems of scale not usually found in the article datasets used to test these algorithms 1 Because libraries span several centuries the genres being identiìed may change gradually across the time axis 2 Because volumes are much longer than articles they tend to be internally heterogeneous and the classiìcation task also requires segmentation We describe a multilayered solution that trains hidden Markov models to segment volumes and uses ensembles of overlapping classifers to address historical change We demonstrate this on a collection of 469,200 volumes drawn from HathiTrust Digital Library I I NTRODUCTION Many attempts to mine large historical collections have treated them as a single pool of documents differentiated only by publication date 1 While some of this w ork has been groundbreaking humanists have often expressed skepticism about the underlying assumption that culture changes as a uniìed whole For instance the increasing frequency of the word toddler in Google Books has been offered as evidence that postmodern novelists had a soft spot for children But if toddler became more common because more parenting manuals were published this evidence might have no relation at all to the history of the novel Before text mining can make a real contribution to the humanities then we need a way to divide large collections into meaningful subsets In practice humanists are especially interested in categories we call genres  novels for instance or lyric poems or sermons Unfortunately metadata is not readily available to classify volumes by genre Librarians estimate that genre information is present in the expected MARC eld for less than a quarter of the volumes in HathiTrust Digital Library  Moreo v e r  e v en if we had complete information at the volume level we would still confront the deeper problem that volumes are internally heterogeneous A nineteenth-century novel often begins with a nonìction life of the author and ends with twenty pages of publishers ads To distinguish the history of the novel from the history of its paratext we need some way of separating genres inside a volume A great deal of work has already been done on classiìcation by genre itês a problem that has preoccupied linguists and information scientists as well as critics since Aristotle But no consensus has emerged and many scholars now suggest that genre may lack a stable deìnition 5 At one end of the spectrum a genre can be a clearly-deìned form like a sonnet or an index But the word also embraces formally protean discourses like satire and science ction which have no xed structure In practice writers use genre to describe any rhetorical situation that isnêt wholly reducible to a topic But this need not imply that genre and topic are perpendicular axes of classiìcation some genres like the travel guide are deìned largely by subject matter Fortunately itês not necessary to conìne this exible concept in order to support humanistic research Our task is not to redeìne genre but simply to sort texts into categories that researchers already in practice nd socially meaningful The instability of the underlying concept is important mainly because it warns us not to expect that genre will reside in any speciìable dimension of language e.g in form rather than content or syntax rather than semantics In practice many genres can be identiìed using the same bag-of-words model that supports other forms of text classiìcation 7 T o adapt this model for genre it s only necessary to put back some of the things that researchers take out when they want to focus on subject classiìcation For instance we nd that itês not a good idea to lemmatize words or remove stopwords verb tenses and prepositions provide signiìcant clues But these are minor adjustments At its core genre classiìcation can be a straightforward application of learning algorithms that are already known to work well on text Naive Bayes and regularized logistic regression both produce good results The problem becomes more interesting however as we move away from its algorithmic core and toward speciìc kinds of scale that complicate the humanistic domain II H ISTORICAL HETEROGENEITY  The rst of these complications involves time in a dataset that spans centuries genres canêt be treated as mutually exclusive categories or as xed points of reference For instance suppose we want to identify book-length works of prose ction published in English in the eighteenth 978-1-4799-1293-3/13/$31.00 ©2013  IEEE 


and nineteenth centuries HathiTrust Digital Library can provide a digital collection of 469,200 nonserial English-language volumes from that period along with metadata from the member libraries This is a good place to start but selecting works of ction from this collection is challenging Existing metadata rarely provides unambiguous information about genre More troublingly when you dig into the problem it becomes clear that no amount of manual categorization will ever produce a deìnitive boundary between ction and nonìction in a collection with a signiìcant timespan because the boundary changes over time Form and content didnêt necessarily align in earlier centuries as they do for us Nineteenth-century biographies that invent imagined dialogue often read exactly like a novel eighteenth-century essays like Richard Steeleês Tatler use thinly ctionalized characters as a veil for nonìction journalism If ction and nonìction are hard to separate subtler generic categories like fantasy and science ction will be even harder So a study of genre has to begin by acknowledging that genres blur and overlap Oddly enough this becomes easier to acknowledge if you represent genre quantitatively Although we like to mock computers reliance on binary logic the point of using numbers is after all to represent questions of degree The discipline of machine learning would call genre a problem of multi-label classiìcation a given work can belong to any number of genres to different degrees Those de grees of membership neednêt always operate under a zero-sum logic Some categories like ction and nonìction probably do trade off against each other in zero-sum fashion but a novel doesnêt necessarily become less like gothic as it becomes more like romance Classiìcation algorithms adapt very naturally to these shades of gray expressing their predictions as degrees of probability Moreover because algorithmic classiìcation doesnêt require a great deal of human labor it can be undertaken in a provisional and exploratory way To be sure there are aspects of genre that algorithms based on word frequency will fail to capture For instance theyêre not good at discerning cases where a work instantiates a genre in order to parody it Some genres may not be recognizable at all with a bagof-words model But for broad distinctions between ction drama poetry and nonìction prose classiìcation algorithms can be a powerful and surprisingly exible mapping tool On the other hand in pursuing this approach itês vital not to lose sight of humanistic historicism Social categories like ction donêt have constant deìnitions As we v e seen the boundary separating ction from nonìction can change over time Moreover language itself changes Some of the linguistic features that characterize ction remain constant from the eighteenth century through the nineteenth past-tense verbs of speech and personal pronouns are prominent But the particular assortment varies eighteenth-century characters do a lot of replying where nineteenth-century characters interrupt and exclaim One simple way to acknowledge the mutability of language and of genre is to train multiple classiìers with evidence drawn    0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 Probability of fiction predicted by 18c classifier Probability of fiction predicted by 19c classifier Fig 1 Scatterplot of 1000 volumes randomly selected from a corpus of 469,200 18thand 19th-century volumes Classiìers trained on different centuries predicted the probability that each volume was ction from different slices of time Then we can use publication dates to weight their votes on the probability that a given document belongs to a given genre In some cases as Fig 1 indicates there will be a great deal of agreement between different classiìers Here weêve trained two naive Bayes classiìers on collections respectively of eighteenthand nineteenthcentury volumes We categorized volumes manually to distinguish prose ction prose nonìction drama and nondramatic poetry The classifers learned to model ction by observing the differences that separated it from other genres they were then calibrated to express their predictions probabilistically using a logistic rather than linear functionÑwhich is why there are more dots in the corners of the graph than in the middle A logistic function changes its value more rapidly in the middle of its range than at the ends In this case there was in practice a strong positive correlation 0.963 between models trained on evidence from different centuries But this wonêt always be true In attempting to recognize gothic novels weêve found it difìcult to train a single classiìer that will recognize both Romantic-era gothic e.g Frankenstein  and late-nineteenth-century gothic e.g Dracula  If we want to model gothic as a single genre weêll have to do it with a chain of overlapping but signiìcantly different models spread out over time Of course the continuity of gothic is actually a debatable premise some critics would call gothic a mode like tragedy rather than a coherent genre like detective ction Statistical modeling canêt conìrm or deny a premise like this single-handedly but it can resist the assumptions we bring to it by showing that on the level of diction at least gothic lacks the coherence of some other genres e.g the Robinsonade see below 96 


Fig 2 Mean probability that ction is written in rst person 1700-1899 Based on a corpus of 31,811 volumes of ction extracted from HathiTrust Digital Library Points are mean probabilities for ve-year spans of time a trend line with standard errors has been plotted with loess smoothing To extract ction from the HathiTrust collection we actually used three overlapping classiìers one for the eighteenth century one for the nineteenth and one for the period 17501850 We weighted their votes according to the date of each volume and combined the evidence of diction with a cautious lter based on metadata We ended up with a collection of 31,811 volumes which may be conservative The distribution strongly leans toward the end of the timespan both because publication generally increases in the nineteenth century and because the prominence of ction increased steadily from 1700 when it constituted less than 5 of the HathiTrust collection to 1899 when it amounted to at least 20 Many of these volumes are reprints or multiple parts of a single work so the collection doesnêt actually cover 30,000 distinct titles We will eventually deduplicate the corpus But for historical purposes itês often an advantage to leave the reprints in an argument can be made that frequently-reprinted works should have more weight in our collection A An experiment point of view Once youêve mapped genre in a collection what can you do with it Many important aspects of literary history have been ignored because theyêre difìcult to address at the scale of reading we ordinarily inhabit For instance the choice between rstand third-person points of view is one of the most fundamental decisions an author can make But critics know surprisingly little about its history Histories of ction that focus on selected examples can do an excellent job of tracing formal innovations because it makes sense in that case to focus on a small number of innovators Accordingly we know quite a lot about reìnements of perspective like free indirect discourse But the broad categories of rstand thirdperson narrative have been around for centuries to describe the shifting balance between them would require evidence about proportions rather than innovation as such A few hundred canonical examples donêt tell us much about proportions Tens of thousands of novels were published in the nineteenth century and it seems quite likely that critics preferences diverged from the preferences of the broader book-buying public Of course itês also possible that library collections diverged from the tastes of the book-buying public but they can give us at least a larger sample Fortunately point of view can be mapped using the same techniques we use to map genre Itês rather easy to distinguish rstand third-person works of ction using a bag-of-words model because point of view markedly affects among other things the frequencies of personal pronouns First secondand third-person pronouns are always present in dialogue but narrative perspective alters their proportions in the work as a whole Of course the choice between rst and third person isnêt an all-or-nothing question there are works like Dickens Bleak House  where they alternate They can also blur in less explicit ways a rst-person narrator can be so reticent as to almost disappear from his own narrative But we need to stress once again that classiìcation algorithms are comfortable mapping shades of gray An algorithm canêt intimately characterize the narrators of Jane Austen and Henry James but it can recognize cases where the ideal types of rstand third-person perspective are mixed Our algorithm in fact recognized that Bleak House was a boundary case with nearly equal proportions of rstand third-person narration B Methods We have suggested that recognition of rhetorical concepts like genre and point of view requires no special recipe For the most part we use supervised learning algorithms that are already known to work well in other contextsÑnaive Bayes and regularized logistic regression On the other hand libraries of volume-length documents do differ from the article datasets ordinarily used to test these algorithms in one simple but important way the average volume is much longer than the average article This can introduce tricky problems of internal heterogeneity discussed at more length below But in other respects it makes the task of classiìcation easier Text classiìers ordinarily have to grapple with very sparse data there are thousands of potentially relevant words but only a few hundred may actually be present in a given web page or article Many of those words occur only once For this reason classiìers sometimes use a Bernoulli model ignoring the number of times a word occurs and attending only to the binary question of its presence or absence in a document The volumes in HathiTrust average roughly 100,000 words long This makes sparsity a much less signiìcant problem A Bernoulli model would be counterproductive here because documents are distinguished less by their strongly overlapping vocabularies than by details of word frequency It also 97 


TABLE I F EATURES THAT CHARACTERIZED POINTS OF VIEW IN A TRAINING SET OF 288 VOLUMES Third Person First person  1 his 11 youth 1 i 11 got 2 himself 12 lover 2 we 12 ship 3 herself 13 whom 3 us 13 running 4 her 14 silence 4 our 14 eight 5 he 15 declared 5 me 15 another 6 she 16 speech 6 myself 16 get 7 daughter 17 beauty 7 ourselves 17 two 8 him 18 who 8 mine 18 weather 9 whose 19 woman 9 four 19 water 10 husband 20 marriage 10 three 20 twenty becomes possible to do some other things that arenêt usually attempted in text mining For instance corpus linguists have recommended a Wilcoxon ranksum test as the best way to identify words that characterize one corpus in contrast to another Instead of summing all occurrences of a word in both corpora the Wilcoxon test considers documents individually to ensure that the word is consistently more common in instances drawn from one corpus This test is often used for selecting features in bioinformatics b u t rarely in te xt miningÑperhaps because comparing word frequencies in short documents produces many ties or near-ties that are hard to interpret Given this sparse data a measure like mutual information might be more robust But volume-length documents are more comparable to a genome words are expressed at many different frequency levels and itês often possible to rank their frequencies without ties Weêve used the Wilcoxon test heavily to identify sets of features for classiìcation For instance in classifying point of view we started with information about the 1200 most common words in 288 training examples divided between rstand third-person novels But from that list of 1200 words we allowed the Wilcoxon test to select 20 words that positively characterize rst-person novels and 20 words that are more common elsewhere We trained a naive Bayes model on this reduced set of 40 features Tenfold cross-validation conìrms that this is an effective model see Table II Other sets of features might work equally well but weêre interested in feature selection not only to increase accuracy but as a way of interpreting the lexical contrast between two corpora For this purpose the Wilcoxon test has signiìcant advantages instead of representing the internal mechanics of classiìcation like information gain itês immediately legible as a recurring contrast between instances of two classes In classifying novels by point of view this method turned out to be interesting for instance because it drew our attention to the odd fact that rst-person novels rely disproportionately not only on I and me but on numbers TABLE II T ENFOLD CROSSVALIDATION OF MODEL Actual Predicted Third person First person Third person 165 4 First person 4 115 For rst person recall precision and F1 are all 0.966 C Interpreting the results The most important thing we discovered with this experiment was that the the proportion of ction written in rstperson point of view declines very signiìcantly toward the end of the eighteenth century Fig 2 plots the mean probability of rst-person narration at ve-year intervals but the decline looks much the same whether youêre considering mean values or medians or the sheer number of individual volumes above 5 probability In every case thereês a dramatic shift from a regime where rst person narration is used roughly half of the time to a new regime where it represents roughly a quarter of the corpus Thereês a great deal of variation in the rst half of the eighteenth century because the total size of the ction corpus is still small and single publication events can have a strong inîuence on the mean But the trend from 1750 forward is nevertheless clear In fact the decline is probably sharper than it appears in this dataset The HathiTrust collection includes multiple editions so many works of ction dated to the nineteenth century are actually reprints of popular novels from the eighteenth as a result the collection will tend to understate the magnitude of changes that would be more dramatic if reprints were excluded Explaining the decline of rst-person perspective is probably too large a task for a single article this is a major new piece of evidence for literary scholars and it may take a few years for us to process it But we do already know that eighteenth-century novelists enjoyed formal experiments that allowed ction to masquerade as a real journal or autobiography or collection of letters According to Ian Wattês timeworn but durable thesis the novel was in fact distinguished from earlier forms of ction by this pretense of documenting arbitrary slices of individual experience And of course no v els imitating autobiography and correspondence would need to rely heavily on rst-person perspective The change at the end of the eighteenth century is harder to explain But thereês some consensus that this period saw signiìcant advances in the management of third-person narration arguably culminating in Jane Austenês use of free indirect discourse which allows readers to see through a characterês eyes while retaining a third-person narratorês distanced judgment These technical advances might have made third-person narration exible enough that it could become a default norm in the nineteenth century But this is speculative thereês room here for much more discussion The lists of features in Table I are almost as interesting as the historical trend itself Although the most important features 98 


for distinguishing points of view are predictably personal pronouns the Wilcoxon test also identiìed other themes as distinguishing rstand third-person narration Family relationships  daughter husband marriage  were prominent in third-person novels Numbers and perhaps nautical terms  ship water weather  were prominent in the rst person On seeing evidence like this our rst thought is always that itês an errorÑfor instance a quirk caused by overrepresentation of sea stories in our training set But the same associations visible in Table I recur if you simply mine correlations in the whole corpus of 31,811 ction volumes by sorting the top 5000 words according to their degree of correlation with the ratio of rstto third-person pronouns Even in that much larger set of works rst-person point of view correlates signiìcantly with language of quantiìcation Quantity and three are the only examples visible at the top of Table III but other number words also correlate positively and fell just below the cutoff for inclusion here  Four two six eight ve several and twenty are all above r  0.15Ñnot a huge correlation in absolute terms but certainly signiìcant in a dataset where n  31,811 Likewise although daughter is the only kinship term visible in Table III child husband and lover also have signiìcant negative correlations with the rst person and are not far behind in the list So these patterns arenêt quirks caused by the contours of our training set Here we were conscious of the potential problem of historical heterogeneity described earlier although the association of rst-person narrative with numbers and third-person with domesticity is visible in the entire corpus it might actually be caused by segregation of the corpus in some particular period So we divided the corpus into eighteenthand nineteenthcentury halves and mined correlations in each half But the same pattern recurred in each half of the corpus rst-person pronouns are strongly associated with numbers and thirdperson pronouns with terms of kinship Of course there are several other striking facts about the correlations in Table III In the list of negative correlations body parts and verbs associated with emotion are even more salient than kinship words Thereês also a clear emphasis on feminine gender  girl daughter woman  Itês possible that women wrote relatively more often in the third person but also possible that these patterns are explained by reliance on thirdperson narration in a particular genre like courtship ction We plan to test both hypotheses soon On the other hand rst-person pronouns are positively correlated with terms that describe travelÑand especially travel by sea We found one clue to this association when we looked at a list of titles sorted by our classiìer A remarkable number of the works recognized as most strongly rst-person in character belonged to the genre of the Robinsonade We were aware that Robinson Crusoe 1719 was a widely-reprinted and frequently-imitated novel but we hadnêt realized just how long and vigorously this tradition endured The Swiss Family Robinson 1812 was joined by an English Family Robinson 1852 for instance and eventually by a Robinson Crusoe of the Nineteenth Century 1884 and many others TABLE III S TRONGEST CORRELATIONS WITH THE RATIO FIRST PERSON PRONOUNS  THIRD PERSON PRONOUNS IN A CORPUS OF 31,811 FICTION VOLUMES  EXCLUDING PRONOUNS AND THE VERB to be  Negative correlations Positive correlations  eyes 0.209 northeast 0.445 voice 0.181 southeast 0.405 face 0.175 tonne 0.324 lips 0.174 shore 0.293 had 0.173 pieces 0.276 girl 0.162 quantity 0.264 smile 0.159 provisions 0.254 daughter 0.153 island 0.251 glance 0.153 voyage 0.243 pale 0.152 three 0.242 silent 0.150 latitude 0.228 loved 0.148 habitation 0.227 turned 0.148 boat 0.217 trembling 0.147 powder 0.212 woman 0.145 ship 0.211 The castaway premise obviously encourages a story to rely on rst-person narration and we suspect it may also encourage quantiìcation Crusoeês obsession with enumeration and accumulation is famously part of the colonial logic of his narrative which transforms ostensibly uninhabited land into a well-governed productive system  about a Y ear and a Half I had a ock of about twelve Goats Kids and all and in two Years more I had three and forty besides several that I took and killêd for my Food After that I enclosed ve several Pieces of Ground to feed them in with little Penns to drive them into to take them as I wanted and Gates out of one Piece of Ground into another But the genre of the Robinsonade by itself wouldnêt explain the strength of a correlation that holds across the whole corpus of eighteenthand nineteenth-century ction We suspect that the combination of a rst-person narrator a colonial setting and acquisitive quantiìcation extends beyond the Robinsonade to encompass other kinds of adventure ctionÑfrom imperial romances like King Solomonês Mines 1885 to sea-stories and westerns and perhaps even stranger forms of ethnographic travel like Wells Time Machine 1895 But this hypothesis needs to be conìrmed by further investigation D Conclusion the historical value of classiìcation We began by presenting classiìcation as a practical support system for humanistic data miningÑdividing digital collections into known generic categories in order to support further inquiry Classiìcation algorithms can in fact support an initial map of this kind They can supplement existing metadata in cases where itês patchy or insufìciently detailed and they can help 99 


us grapple with problems that involve differences of degree rather than clear categorical boundaries But classiìcation algorithms can also become a method of conceptual exploration in their own right Often we associate exploratory analysis with unsupervised methods like topic modeling Supervised learning may seem pedestrian by contrast since supervised algorithms can only answer questions we already know how to pose But knowing how to pose a question doesnêt mean youêll get the answer you expect You may after all discover that your question was badly formed Genre is not a well-deìned concept and in attempting to map it digitally weêre beginning to discover that it breaks up into a range of different categories For instance even the initial explorations described here have revealed genres of four different kinds 1 At the broadest level concepts like prose ction and lyric poetry are relatively stable across a timespan of centuries and relatively easy to map using the evidence of diction Then there are 2 generic phenomena of a briefer kind but almost equally easy to map using the evidence of diction Weêve trained a model that recognizes Romantic-era gothic novels Matthew Jockers has done the same thing for many other subgenres of nineteenthcentury ction and he conìrms Franco Morettiês speculation that these genres often seem to have a roughly generational 30-year duration But the tradition of literary scholarship we inherit also describes 3 speciìc subgenres of ction drama and poetry that endure a century or longer like the gothic novel broadly construed or science ction Some of these concepts may turn out to be difìcult to recognize lexically for instance we havenêt been able to train a single uniìed model of the gothic across the whole nineteenth century But of course the failure of our algorithm doesnêt imply that existing concepts of the gothic lack coherence Thereês no reason to assume that all genres must be recognizable at the level of diction Finally in some cases we may discover 4 long-lived subgenres characterized by a surprising kind of lexical coherence The two-century-long tradition of the Robinsonade is one example it may even reveal an undescribed dimension of narrative that unites previously distinct genres of eighteenthand nineteenth-century ction through a shared emphasis on enumeration But this hypothesis needs more investigation Itês apparent that genre and point of view arenêt independent axes of description but the nature of the relationship between those axes hasnêt yet been described adequately III I NTERNAL HETEROGENEITY One limitation of the research described above is that we have treated volumes as if they were uniìed entities In reality the volumes in a digital library are almost always internally heterogeneous In the most extreme case the collected works of an author may mix poetry and drama with personal letters and prose essays Nineteenth-century periodicals are equally miscellaneous But nearly every volume in a library poses some version of this problem because bookplates indexes and publishers ads donêt belong to the same genre as the body text itself Since that paratext represents 10 or more of the text in many volumes itês a problem that really needs to be addressed The Text Encoding Initiative represents an ideal solution to this problem for small and medium-sized collections but itês not immediately practical to manually mark up the millions of volumes in HathiTrust If document formats were consistent we might devise segmentation procedures that mimicked a human reader by looking for changes in font size or even parsing the grammar of a title page But in a library that co v ers several centuries the protocols of book layout are anything but consistent Moreover optically-transcribed documents pose a problem for algorithms that tolerate noise poorly Our OCR correction procedures catch common errors like eighteenthcentury s  f substitution that would systematically distort our ndings But it s not safe to assume that an y particular line will be transcribed correctly A Proposed solution Weêve accordingly opted for a more robust strategy that uses classiìcation itself as a segmentation tool We donêt after all need to separate the parts of a volume unless theyêre generically distinct It may ultimately be desirable to separate the individual chapters of a novel or individual poems in a collection but our immediate goal is the simpler one of locating page ranges that contain poetry and separating them from pages that contain ction This problem can be addressed most directly by training classiìcation algorithms to recognize the genres in question We do this training initially at the page level The parts of a volume donêt always begin on a new page but divisions within a page can be more easily addressed after generically coherent page ranges have been identiìed To treat pages as separate documents would however discard the valuable evidence provided by sequence The sequential arrangement of pages provides clues of several kinds Most obviously there are parts of a volume like a bookplate or an index that tend to be located at the beginning or end More importantly sequence helps us smooth out noise in the middle of a long sequence of ction pages a page identiìed as nonìction is likely to be an error The relative position of volume parts can also be signiìcant biography is relatively difìcult to distinguish from ction at the level of the individual page but biographical introductions often precede a table of contents and ction rarely does One exible way to represent this kind of knowledge about volume structure might be to train a hidden Markov model on top of the predictions made by page-level classiìers In other words we model volume structure as a sequence where the genre of each new page depends stochastically on the genre of the page that preceded it We train this model on manually labeled ground truth so it can learn for instance that a page of ction is very likely to be followed by another page of ction less likely to be followed by an index and not at all likely to be followed by a table of contents However this is a hidden Markov model because the hidden states genres are 100 


 g1  g2 p1 g3 p2 p3 Fig 3 Hidden Markov model The hidden states genre1 genre2  are inferred from observed states page1 page2  and from a transition matrix deìning the probabilities of transition between different hidden states never directly observable outside of the training environment  Instead twenty classiìers mak e predictions about the probability that a page instantiates each of twenty genres The hidden Markov model deìnes a principled smoothing procedure whereby we combine those predictions about individual pages with our knowledge about the probabilities of transition between genres to infer the most likely hidden state genre for every page in the sequence B Results How much does this technique improve our results First we should characterize the accuracy of page-level classiìcation without Markov smoothing We trained pagelevel classiìers using the Weka implementation of regularized logistic regression L2 norm because itês a robust algorithm for text classiìcation that expresses predictions probabilistically Nai v e Bayes and SVM might mak e equally accurate decisions but expressing their predictions as probabilities of genre membership would require an additional calibration step We trained classiìers for individual classes and when not using smoothing we identiìed the genre of a page simply by accepting the prediction with the highest probability the onevs-all method We also used a practical trick well attested in industrial practice and gi v e n a theoretical basis by Nigam McCallum and Mitchell when a class sho wed internal variation we identiìed it through an ensemble of classiìers designed to recognize its subclasses For instance nonìction prose is a large category containing genres like biography and autobiography that are especially difìcult to distinguish from ction We trained classiìers for all of these subclasses and if any one of them turned out to be the most probable genre for a given page that page was identiìed as nonìction Similarly we divided front matter into subclasses like title page and bookplate and back matter into subclasses like index and date due slip Regularized logistic regression makes feature selection less crucial since it minimizes features that turn out not to be useful for discriminating a particular class but resists over microaveraged macroaveraged advertisements, etc back matter drama front matter nar. & lyric poetry nonfiction prose prose fiction 0.00 0.25 0.50 0.75 1.00 F1 measure markov raw smoothed Fig 4 Gains from hidden Markov smoothing The top seven rows are F1 measures for individual genres the bottom two rows reîect macroand microaveraged F1 measures for all genres tting 20 W e empirically optimized the number of features and ended up using the 250 most common words in the corpus including a few word categories compressed into single features like personal-name and roman-numeral We also used information about the relative length of pages their position in the volume and the density of line-initial capitalization We manually recorded genre labels for 101 volumes 31,586 pages drawn randomly from 469,200 eighteenthand nineteenth-century volumes and performed classiìcation using tenfold cross-validation with the corpus divided by volume rather than page Dividing the corpus by page would make the experimental task much easier than it is in reality since you would be comparing authors to themselves division by volume is also necessary in order to make an apples-toapples comparison with the smoothed results The F1 scores for different genres are presented in the raw bars of Fig 4 The F1 metric is important in this context because the classes are very different in size and it would be easy to maximize a crude accuracy metric by wholly ignoring some of the smaller classes Then using the same tenfold division of the corpus we smoothed the classiìers predictions using a different hidden Markov model for each iteration of cross-validation The model was never allowed to see the test set The smoothed results are presented in the smoothed bars of Fig 4 and show substantial improvement Most of the improvement takes place in the larger categories that comprise the majority of pages in a volume nonìction prose ction etc and it seems to be produced mostly by smoothing out minor variations within long generically-coherent sequences of pages C Remaining challenges and scalability Itês fairly clear that Markov smoothing can be a useful tool for segmenting heterogeneous volumes but also clear that it will need to be supplemented by other strategies F1 scores in 101 


the range of 80  90 are good as algorithms go but humanists have a lower tolerance for error As we adapt this technique for practical use we can improve it in several ways First 100 volumes is actually a tiny training set given the diversity of this corpus and it should be enlarged manually This technique would also be well suited to a semi-supervised bootstrap approachÑwhere the algorithm is allowed to generate additional data for its own further training Because there are actually two different models involved here a volume-structure model and a bag-of-words model they should tend to correct each other and prevent cascading error in a system known as co-training It may also be possible to improve our model The sequence of pages in a volume isnêt actually a rst-order Markovian sequence rather it tends to have a hierarchical character governed by large structural divisions between front matter preface body text and back matter A hierarchical hidden Markov model might be appropriate or we could use conditional random elds Weêll also want to reìne our techniques to make divisions below the page level This will require two different strategies In some cases a volume part simply begins halfway down a page Once weêve identiìed coherent page ranges it should be possible to catch most cases like this by deìning a similarity metric for each range and scanning the pages on either side of the range boundary for points of inîection An XML tag can then be inserted at an appropriate location Small errors here are not likely to affect macroscopic results A more important problem occurs where two genres are mixed throughout a page range For instance nineteenth-century poems often have voluminous prose footnotes on every page Some generic mosaics of this kind will be difìcult to divide algorithmically But weêve found that the most common and important caseÑa mosaic that mixes prose and verseÑis fairly straightforward In the eighteenth and nineteenth centuries verse is easily distinguished from prose on a line-by-line level using a combination of line length and initial capitalization This would be difìcult in the twentieth century since verse isnêt always capitalized But since twentieth-century poems donêt usually have long prose footnotes the solution is in practice tailored to the problem In this article weêve focused on scale mainly as a conceptual problem involving historical timeframes and heterogeneous volumes But itês also of course a processing problem Compressed the dataset described in this article amounts to 0.7TB the processes we describe are implemented in Python and Java and have been parallelized in straightforward ways we write multithreaded Java and use multiple nodes in our campus cluster Although we expect the size of the problem to double or triple as we expand the project it remains manageable for researchers comfortable with batch queues But scalability still poses a social problem because most humanists are not in fact comfortable with batch queues If we want our methods to be adopted by other scholars weêll need to share them in a form that can be managed more easily on a personal workstation The most realistic path to this goal is to integrate our analytical methods in the digital library that hosts the original data For that reason weêre collaborating with HathiTrust Research Center to implement our methods in their online research portal this project is supported by the National Endowment for the Humanities and the American Council of Learned Societies IV C ONCLUSION I NTEGRATING BOTH DIMENSIONS OF THIS PROBLEM  This article has described two different problems of scale one that involves segmenting individual volumes and one that involves mapping historically mutable genres To solve both problems at once weêll need to deìne relationships between different levels of classiìcation At the page level itês not necessary to distinguish subgenres like the gothic novel or science ction Subgenres of that kind arenêt very useful for segmenting volumes because it rarely makes sense to treat them as mutually exclusive categories a novel can be at once gothic and science ction as Mary Shelley would remind us Instead page-level classiìcation will focus on relatively broad categories like prose ction nonìction drama and nondramatic poetry These categories can generally be positioned in a zero-sum relationship to each other and at the page level itês not too much of a distortion to choose one of them in exclusion to the others Once weêve separated these categories within volumes we can treat cohesive page ranges as works for subtler multi-label classiìcation at a historical scale where unlimited numbers of subgenres can be allowed to overlap in a single work This multilayered approach doesnêt however imply that the problem of volume segmentation can be treated entirely ahistorically Volume structure does vary across time eighteenthcentury volumes tend to be prefaced by lists of subscribers whereas nineteenth-century volumes end with advertisements So weêll probably need to train multiple Markov models of volume structure on different spans of time If patient readers are at this point dismayed by the complexity of the problem we sympathize Humanistic data mining is immensely rewarding because it enlarges our eld of vision in a way that can lead to signiìcant discoveries But there are many potential sources of error in this domain from data-cleaning problems to hasty assumptions about historical continuity A principled method is necessary here because researchers will otherwise be overwhelmed by innumerable ad hoc patches addressing speciìc kinds of uncertainty We understand machine learning not as a black box that produces authoritative results but as a exible way to explore large collections that incorporates reîection on uncertainty as an integral part of its logic The Bayesian concepts underpinning this discipline seem to dovetail rather well with humanistic insights like historicism Ideally humanists will approach machine learning not just as a collection of tools but as a principled statistical language that allows them to formalize and scale up their own habits of inquiry 102 


A CKNOWLEDGMENTS Work on this project was supported by the Andrew W Mellon Foundation and by an NEH Digital Humanities Start-Up Grant The collections we used were drawn from HathiTrust Digital Library with assistance from HathiTrust Research Center Early drafts of the rst half of this project were presented at the Nebraska Forum on Digital Humanities and the Stanford Literary Lab and online at The Stone and the Shell  where feedback came from many participants and commenters David Bamman Sayan Bhattacharyya Tim Cole Eleanor Courtemanche M J Han William Underwood and Laura White gave particularly valuable advice R EFERENCES  M Egnal 2013 Ev olution of the No v e l i n the United States The Statistical Evidence in Social Science History  vol 37 pp 231-254 2013  J.-B Michel et al 2011 Quantitati v e Analysis of Culture Using Millions of Digitized Books in Science  vol 331 pp 176-182 14 Jan 2011  M Egnal Crunching Literary Numbers  i n The New York Times  July 12 2013  T  Cole and M J Han personal communication 2013  M Santini 2004 State-of-the-Art on Automatic Genre Identiìcation Information Technology Research Institute Technical Report Series ITRI University of Brighton Jan 2004  Michael B Prince 2003 Mauv ais Genres  i n New Literary History  vol 34 pp 597-615  B Y u  2008 An Ev aluation of T e xt Classiìcation Methods for Literary Study in Literary and Linguistic Computing Vol 23 2008 327-343  S Allison R Heuser  M  Jock ers F  Moretti and M W itmore 2011 Quantitative Formalism An Experiment Stanford Literary Lab Pamphlet Series A v ailable http://litlab stanford.edu/?page  id=255  G Tsoumakas and Ioannis Katakis Multi-Label Classiìcation An Overview in International Journal of Data Warehousing and Mining Vol 3 pp 1-13  M W itmore 2012 The T ime Problem Rigid Classiìers Classiìer Postmarks in Wine-Dark Sea April 12 2012 A v ailable http://winedarksea.org/?p=1507  A Kilgarrif f 2001 Comparing Corpora in International Journal of Corpus Linguistics Vol 6 pp 97-133  O Okun Feature Selection and Ensemble Methods for Bioninformatics Hershey PA Medical Information Science Reference 2011  I W att The Rise of the Novel Berkeley University of California Press 1957  D Defoe The Life and Strange Surprizing Adventures of Robinson Crusoe of York Mariner p 174 London 1722  M L Jock ers Macroanalysis pp 86-88 Urbana University of Illinois Press 2013  W  Underw ood 2010 Grammar Based Recognition of Documentary Forms and Extraction of Metadata in The International Journal of Digital Curation Vol 5 pp 148-159  T  Underw ood and L Auvil 2012 Basic OCR correction  Online Available http://usesofscale.com/gritty-details/basic-ocr-correction  L R Rabiner and B H Juang 1986  A n Introduction to Hidden Markov Models in IEEE ASSP Magazine pp 4-16  M Hall E Frank G Holmes B Pf ahringer  P  Reutemann and I H Witten 2009 The WEKA Data Mining Software An Update SIGKDD Explorations Volume 11 Issue 1  D Sculle y  et al 2011 Detecting Adv ersarial Adv ertisements in the Wild in KDD  K Nigam A McCallum and T  Mitchell Semi-Supervised T e xt Classiìcation Using EM in Semi-Supervised Learning ed O Chapelle B Sch  olkopf and A Zien Cambridge MA MIT Press 2006  A Y  Ng 2004 Feature Selection L 1 vs L 2 Regularization and Rotational Invariance in ICML 04  I H W itten A Frank and M A Hall Data Mining p 297 Burlington MA Morgan Kaufmann 2011 103 


en-US Keynote VI I I  en-US GreenCom iThings CPSCom 2013   Towards Carrier Cloud   Dr. Tarik Taleb  Senior Researcher and 3GPP Standards Expert  NEC Europe Ltd, Heidelberg, Germany  Email tarik.taleb@nw.neclab.eu    Abstract   Mobile operators are in need of means to cope with the ever increasing mobile data traffic, introducing minimal additional capital expenditures on existing infrastructures, principally due to the modest Average Revenues per User ARPU Network virtualizat ion and cloud computing techniques along with the principles of the latter in terms of service elasticity on demand and pay per use could be important enablers for various mobile network enhancements and cost reduction This talk discusses the recent tr ends the mobile telecommunications market is experiencing showcasing some of the emerging consumer products and services that are facilitating such trends. The talk also discusses the challenges these trends are representing to mobile network operators. T he talk also demonstrates the possibility of extending cloud computing beyond data centers towards the mobile end user providing end to end mobile connectivity as a cloud service. The talk introduces a set of technologies and methods for the on demand pro vision of a decentralized and elastic mobile network as a cloud service over a distributed network of cloud computing data centers; federated cloud. The concept of Follow Me Cloud whereby not only data but also mobile services are intelligently following t heir respective users is also introduced. The novel business opportunities behind the envisioned carrier cloud architecture and service are also discussed, considering various multi stakeholder scenarios   Bio   Tarik Taleb is currently working as Senior Researcher and 3GPP Standards Expert at NEC Europe Ltd Heidelberg, Germany. Prior to his current position and till Mar. 2009, he worked as assistant professor at the Graduate School of Information Sciences, Tohoku University, Japan, in a lab fully funded by KDDI, the second largest network operator in Japan From Oct 2005 till Mar 2006 he was working as research fellow with the Intelligent Cosmos Research Institute Se ndai Japan He received his B E   degree in Information Engineering with distinction M.Sc and Ph.D degrees in Information Sciences from GSIS Tohoku Univ., in 2001, 2003, and 2005, respectively   Dr Taleb  s research interests lie in the field of architectural enhancements to mobile core networks particularly 3GPP  s mobile cloud net working mobile multimedia streaming congestion control protocols handoff and mobility management inter vehicular communications and social media networking Dr Taleb has been also directly engaged in the development and standardization of the Evolved  Packet System as a member of 3GPP  s System Architecture working group. Dr. Taleb is a board member of the  IEEE Communications Society Standardization Program Development Board  As an attempt to bridge the gap between academia and industry Dr Taleb has f ounded and has been the     Dr Taleb  is/was on the editorial board of the IEEE Wireless Communications Magazine IEEE Transactions on Vehicular Technology, IEEE Communications Surveys & Tutorials, and a number of Wiley journals. He is serving as vice chair of the Wireless Communications Tech nical Committee, the largest in IEEE ComSoC He also served as Secretary and then as Vice Chair of the Satellite and Space Communications Technical Committee of IEEE ComSoc 2006  2010 He has been on the technical   
lxxxii 


program committee  of different IEEE c onferences including Globecom, ICC and WCNC and chaired some of their symposia   Dr Taleb is the recipient of the 2009 IEEE ComSoc Asia Pacific Best Young Researcher award Jun 2009 the 2008 TELECOM System Technology Award from the Telecommunicati ons Advancement Foundation Mar 2008 the 2007 Funai Foundation Science Promotion Award Apr 2007 the 2006 IEEE Computer Society Japan Chapter Young Author Award Dec 2006 the NiwaYasujirou Memorial Award Feb 2005 and the Young Researcher's Enc ouragement Award from the Japan chapter of the IEEE Vehicular Technology Society \(VTS\\(Oct. 2003\ Some of Dr. Taleb  s research work has been also awarded best paper awards at prestigious conferences. Dr. Taleb is a senior IEEE member      
lxxxiii 


en-US Keynote I X  en-US GreenCom iThings CPSCom 2013   How Densely Should the Data Base Stations  B e Deployed in Hyper Cellular Networks   Professor Zhisheng Niu  Tsinghua National Lab for Information Science and Technology  Tsinghua University, Beijing 100084, China  E mail niuzhs@tsinghua.edu.cn    Abstract   One of the key approaches to make the mobile communication networks more GREEN Globally Resource optimized and Energy Efficient Networks\is to have the cellular architecture and radio resource allocation more adaptive to the environment and traffic varia tions including making some lightly loaded base stations \(BSs\go to sleep. This is the concept of so called TANGO \(Traffic Aware Network planning and Green Operation and CHORUS Collaborative and Harmonized Open Radio Ubiquitous Systems published by th e author earlier. To realize this, a new cellular framework, named hyper cellular networks HCN has been proposed in which the coverage of control signals is decoupled from the coverage of data signals so that the data coverage can be more elastic in ac cordance with the dynamics of traffic characteristics and QoS requirements. Specifically, the data base stations \(DBSs\in HCN can be densely deployed during peak traffic time in order to satisfy the capacity requirement, while a portion of DBSs can be swi tched off or go to sleep mode if the traffic load is lower than a threshold in order to save energy. A fundamental question then arises how densely should the DBSs be deployed in order to balance the QoS requirements and the energy consumption in hyper ce llular networks     In this talk, we characterize the optimal DBS density for both homogeneous and heterogeneous hyper cellular networks to minimize network cost with stochastic geometry theory For homogeneous cases both upper and lower bounds of the optimal DBS density are derived For heterogeneous cases our analysis reveals the best type of DBSs to be deployed for capacity extension or to be switched off for energy saving. Specifically, if the ratio between the micro DBS cost and the macro DBS cost  is lower than a threshold which is a function of path loss and their transmit power then the optimal strategy is to deploy micro DBSs for capacity extension or to switch off macro DBSs \(if possible\for energy saving with higher priority Otherwise the  optimal strategy is the opposite Based on the parameters from EARTH numerical results show that in the dense urban scenario compared to the traditional macro only homogeneous cellular network with no DBS sleeping deploying micro DBSs can reduce about 40 of the total energy cost, and further reduce about 20% with DBS sleeping capability   Bio   Zhisheng Niu graduated from Northern Jiaotong University currently Beijing Jiaotong University Beijing China in 1985 and got his M.E and D.E degrees fr om Toyohashi University of Technology Toyohashi, Japan, in 1989 and 1992, respectively. After spending two years at Fujitsu Laboratories Ltd Kawasaki, Japan, he joined with Tsinghua University, Beijing, China, in 1994, where he is now a professor at the  Department of Electronic Engineering and the deputy dean of the School of Information Science and Technology. His major research interests include queueing theory, traffic engineering, mobile Internet radio resource management of wireless networks, and g reen communication and networks   Dr Niu has been an active volunteer for various academic societies including council member of Chinese Institute of Electronics 2006 10 vice chair of the Information and Communication Network Committee of Chinese In stitute of Communications 2008 12 Councilor of IEICE Japan 2009 11 and membership development coordinator of IEEE Region 10 \(2009 10\ In particular, in IEEE Communication 
lxxxiv 


Society, he has been serving as an editor of IEEE Wireless Communication Magaz ine \(2009 12\ director of Asia Pacific Region \(2008 09\ director for Conference Publications \(2010 11\ chair of Beijing Chapter 2001 08 and members of Award Committee 2011 13 Emerging Technologies Committee 2010 12 On line Content Committee 20 10 12 and Strategy Planning Committee He has also been serving as general co   co    chairs o f    Prof. Niu is a co recipient of the Best Paper Awards from the 13th and 15th Asia Pacific Conference on Communication APCC in 2007 and 2009 respectively and received Outstanding Young Researcher Award from Natural Science Foundati on of China in 2009 He is now the Chief Scientist of the National  Energy and Resource Optimized Hyper Cellular Mobile Communication System 2012 2016 which is the first national project green communications in China He is the fellow of IEEE and IEICE and a distinguished lecturer of IEEE Communication Society \(2012 2013  
lxxxv 


