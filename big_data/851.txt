The Visual Wiki: A New Metaphor for Knowledge Access and Management   Christian Hirsch, John Hosking, John Grundy, Tim Chaffe, David MacDonald, Yuriy Halytskyy University of Auckland, Private B ag 92019, Auckland, New Zealand chir008@ec.| john@cs.| john-g@cs.| t.cha ffe@| dmac127@ec.| yhal003@ec.}auckland.ac.nz   Abstract  Successful knowledge management results in a competitive advantage in todayês information- and knowledge-rich industries. The elaboration and integration of emerging web-based tools and services has proven suitable for collecting and organizing 
intellectual property. Due to an increasing information overload, information and knowledge visualizations have become effective methods for representing complex bodies of knowledge in an alternative fashion by using visual languages. The focus of this research is the development of a çVisual Wikié, which combines the notion of a textual and a visual representation of knowledge. A Visual Wiki model has been proposed which provides a unified framework to design and discuss different approaches. Three prototypes of Visual Wikis have been implemented and evaluated according to the improvements to certain knowledge management applications that they facilitate    
 1  Introduction  The web 2.0 a set of technologies, tools, and concepts, has had an enormous impact on how information is processed using the internet. A more recent trend can be observed as the web 2.0 starts to become incorporated into the enterprise, often referred to as the enterprise 2.0 Information and knowledge processing in these contexts is becoming more dynamic and with more social aspects than previously. In this domain wikis 
have become very popular knowledge management systems utilizing primarily textual information representations Information and knowledge visualizations  provide effective methods to represent and convey information- and knowledge-rich scenarios. They support this by providing information in forms more suitable to the human cognitive processing system. In both areas human factors play an important role including ability to navigate, relate, remember and understand complex information, and possibly to 
collaboratively share and utilize knowledge. Thus applications in these areas have a significant potential to improve tasks related to knowledge management by making it more accessible, understandable sharable and dynamic This research focuses on the design and implementation of a Visual Wiki We define a Visual Wiki to be a combination or integration of two representations of the same underlying body of knowledge visual and textual which are sharable 
and group-editable in a wiki style. Our goal is to improve the capability of a wiki as a knowledge management tool The remainder of this paper is organized as follows. Section 2 gives an overview of related research areas. Section 3 and 4 describe the chosen approach and methodology of our research. Central to both is the proposed Visual Wiki model. Section 5 describes the design and implementation of three Visual Wiki prototypes. Section 6 provides an evaluation of those prototypes. Finally section 7 outlines possible future work and section 8 summarizes our conclusions 
 2  Background  2.1  Web 2.0 and Enterprise 2.0  The term web 2.0 was created by Tim O Reilly in It tax o n o m i s e s  n e w tec h n o lo g i es   application design patterns and business models which have effected enormous changes on how people publish and consume information using the internet. Some of the most important recent applications of the web 2.0 are weblogs, podcasts RSS feeds, and social software such as wikis, social 
networking, and social bookmarking. These support rich, dynamic information sharing mechanisms for users. Furthermore combinations of existing web applications and services, called mashups are an important part of the new web. Some of the oftenmentioned key characteristics of the web 2.0 are user participation and mass collaboration [22 scalab ility  which leverages the long u s er  experience, and remixability  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 1 978-0-7695-3450-3/09 $25.00 © 2009 IEEE 


Enterprises have begun to explore and apply technologies and concepts of the web 2.0 to business intranets. These applications are often referred to as enterprise 2.0 a term coined by Andrew McAfee   T h e popu larity a n d i m portan ce of th e s e  enterprise 2.0 applications are constantly growing  T h ey off er s t rong pot e n t i a l  f o r m a n a g i ng a n d developing business information and knowledge assets by providing a new, more democratic, and more evaluative way to exploit information within organizations h eref ore th e y ca n  h a v e e f f e cts  on project management, innovation processes communication patterns, as well as knowledge management [21    The core concept of the web 2.0 and enterprise 2.0 can be summarized as a gradual and ongoing change in how information is processed These changes range from the ways information is created  and organized via its distribution and search to its application These five stages of the information lifecycle, as illustrated in Figure 1, can briefly be described as follows Fundamental transformation can be observed in the creation of information. The old web \(often called web 1.0 was dominated by proprietary produced information provided by big media or news companies. The web is now gradually becoming a platform of mass collaboration and peer produced content mainly driven by tools like wikis and blogs In the corporate context this leads to a shift from a top-down to a bottom-up creation of content Hyperlinks and web directories based on fixed taxonomies were some of the few means for organizing the information in the web 1.0. In the web 2.0 information is organized in more democratic ways by end users. The resulting folksonomy can also be observed in the company environment where it replaces predetermined taxonomies In both the internet and the intranet the changes in distribution are mainly about push- versus pulldistribution. More personal and efficient pull distributions such as RSS feeds replace push distributions such as email allowing users to subscribe to what they want and be sent it proactively rather than receiving unsolicited information Searching and finding information depends on how well it was organized previously. Search in the old web was dominated by navigating web directories and predefined taxonomies. Search algorithms and folksonomies both created directly or indirectly by the user, are examples of search mechanisms for web 2.0. In enterprise 2.0, tool integration additionally helps to make information more accessible and searchable Lastly the application of information is changing from a rather anonymous, personal one-way publication and use to a more social use that allows instant feedback and two-way communication about information. The usage of the information itself can add new information The next notable step in this gradual and ongoing change in information processing is arguably called the web 3.0 It is often referred to as the semantic web  w h ich is  s e e n b y  m a ny a s co m p le m e n t ar y  to the web 2.0 [2, 10   2.2  Information & Knowledge Visualization  Due to the increasing information overload brought a bout by the large-scale digitization of information, visualizations have become effective methods for representing and organizing knowledge and information-rich scenarios Vis u alizations themselves are tools for knowledge management which make use of the human cognitive processing system in order to create and convey content more efficiently. Information and knowledge visualization both employ similar techniques Based on mapping rules, resource objects are translated into visual objects as meaningful representations, offering easy and comprehensive access to the subject matter presented   Several models for both domains have been introduced which attempt to describe and classify Figure 1. The information lifecycle in the internet and intranet Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 2 


different aspects of the visualization process, most importantly the resource objects, the visual objects and the purpose The resource objects in information visualizations are mostly different types of concrete data [13, 20  while those in knowledge visualizations are generally described as more abstract types of knowledge such as declarative knowledge, procedural knowledge, and mo The visual objects are commonly represented using domain specific visual languages \(DSVL\ such as UML, BPMN, mind maps, and many mo  DSVL uses visualizations to communicate concepts and ideas. Visual objects are here defined as the basic components of any DSVL \(e.g. nodes and edges of a graph based DSVL\here is no clear separation between whether a DSVL is used for information or knowledge visualizations. However, information visualizations tend to use rather simple but clearly defined visual languages. Knowledge visualizations use more abstract and flexible ones Lastly the purpose of visualizations is to support different tasks. Information visualizations focus on concrete tasks such as navigating and filtering an information sp T h e m a i n task of  kn o w ledg e  visualization is to make knowledge explicit and usable [23  Fur t he r t a s k s i n c l ud e  mo t i va t i o n a n d  elaboration [1 ow led g e m a pping  w h ic h is an important and useful type of knowledge visualization is a good method to capture and share explicit knowledge [26   T h i s l e a d s t o  a m o r e ge ne r a l p u r p o s e  for both information and knowledge visualizations: to support all five steps in the lifecycle \(see Figure 1 including the creation, organization, distribution search, and application of information and knowledge respectively  3  Our Approach  Our approach to this research was to combine the highly dynamic and social concept of the new web with the concept of information visualization. Both concepts are suitable for tasks related to knowledge management. Web 2.0 tools and technologies, such as wikis, are of interest in knowledge management as summarized in s e of th eir dy n a m i cs an d  their ability to capture knowledge. Traditional wikis are an editable repository of information, usually predominantly textual, that multiple users share, edit annotate and use. Human knowledge often exists in flows and can be described as a real time assembly of multiple fragmented memori s y ste m th at  supports this kind of loose and ad hoc creation of content is more likely to capture knowledge in an adequate way. Visualizations are tools of knowledge management. They are an alternative and very efficient way to represent and organize knowledge and information-rich scenario r g o al is to combine these two knowledge representations in a Visual Wiki In respect to our research we are mainly concerned with explicit knowledge. For the sake of simplicity we will refer to both information and knowledge as content which can easily be codified into our prototypes A Visual Wiki can be defined as a combination or integration of two representations \(a textual and a visual one\he same underlying body of knowledge. Both or either of the two representations may be editable in a shared traditional wiki style The purpose of a Visual Wiki is to increase the capability of a wiki as a knowledge management tool by elaborating the synergetic effects of integrating visual enhancements In the context of the web 2.0, the enterprise 2.0 and their applications, information visualizations are already used in a variety of contexts. Generally they can be found in two formats: active and passive Passive visualizations use existing aspects or characteristics of a system, such as tags or page connections in a wiki, visualizing them. Examples are simple tag clouds, wiki structures \(e.g  mashups using wikis and Google Maps \(e.g  social graphs \(e.g T h e res u lti ng vis u alizatio n s  are passive in the way that they are created passively However, they can be used in an active way, e.g. to navigate a certain structure Active visualizations on the other side are realized as rich internet applications allowing the user to create diagrams using a DSVL, and then integrate them with a wiki or other collaboration mechanisms Examples are the Gliffy diagramming tool http://gliffy.com\ntegrated into the Confluence wiki \(www.atlassian.com\ and Many Eyes where users can collaboratively create and discuss visualizations The major problem of existing approaches to realizing the concept of a Visual Wiki seems to be a lack of integration between the visual and the textual representations. In other words the approaches are either too active or too passive. One of the major goals of this research is a set of possibilities for overcoming this drawback. In order to not only discuss drawbacks but also to better understand the concept of the Visual Wiki in general, a model is needed. Such a model, as seen in Figure 2, can be used to classify, analyze, and discuss different existing as well as new approaches As shown in Figure 2 we define a Visual Wiki to consist of four components: the underlying concept the textual and visual representation, and a mapping Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 3 


between those two. Each of the four components has a set of parameters which can take specific values. In the following, the four components and their most important parameters are described in more detail  Visualization Mapping Text Concept Figure 2. The four components of the Visual Wiki  The concept component, which underlies all of the others, consists of two parameters: purpose and content of the Visual Wiki. \(More precisely: purpose and content of the visual component in respect to the whole Visual Wiki\he pu rpose of the visualization can be any of the five steps in the lifecycle as discussed earlier: creation, organization, distribution search, and application of knowledge \(or any combination of these\The content of the visualization \(in respect to the whole body of knowledge contained in the wiki\ can range from general to specific The text and visualization component are very similar. Both use a language to represent the content of the underlying knowledge base: a visual and a natural language. However, the wiki itself \(the textual component\n also be seen as a visual language e.g. consisting of headlines, text body, tags, etc Therefore both components have the same parameters such as complexity, creation, and editing of the metamodel, as well as the creation and editing of the graph and text respectively  A crucial component is the mapping between the two knowledge representations. This component determines how the two representations are linked together and how they influence each other. The two most important parameters of this component are the type of mapping and the consistency between both representations. The type of mapping is the core of the mapping component and describes how the visual entities of the visualization are mapped to the textual entities of the wiki. This can basically be done in three ways: 1:1, 1:m, n:m. The consistency of the mapping describes how changes in one representation affect the other representation. There are three possibilities: no consistency, one-way consistency and two-way consistency Figure 3 shows a more detailed view of the Visual Wiki model. It includes four of the most important parameters \(type of mapping, consistency, content and purpose\d their possible values  4  Methodology  Our model shows that there are many different possibilities to design such a Visual Wiki. Each possibility might differ in some or all of the described parameters of the four components. Furthermore some of the parameters seem to depend on each other. In order to study the proposed metaphor of the Visual Wiki the following methodology has been used   Literature review and model development: A literature review has been carried out which ultimately led to the Visual Wiki model as described in the previous section   Identification of a dependent parameter: Based on this model, which describes the different parameters of the Visual Wiki, we have chosen one parameter as a dependent parameter. The selected parameter is the purpose parameter. It is of special interest as it defines which knowledge management tasks the Visual Wiki supports creation, organization, distribution, search, or application of knowledge\. The dependent parameter is shown at the bottom in Figure 3. Our prototypes and evaluations of these prototypes will help us gauge how well each Visual Wiki scenario meets these purpose parameter values   Identification of independent parameters: As a next step, three of the remaining parameters have been selected according to their suggested potential to influence the dependent parameter The selected independent parameters are: content of visualization, type of mapping, and consistency of mapping \(see Figure 3   Prototype design and implementation: Three Visual Wikis have been designed and prototypes implemented. The three prototypes have been   Figure 3. Design of the three prototypes  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 4 


designed so that they differ in the three selected independent parameters. The coloring/shading in Figure 3 show these chosen parameters for the three prototypes. Detailed descriptions of the prototypes follow in the next section   Evaluation: A formal evaluation has been carried out to determine what influence the prototypes and therefore the combinations of independent parameters have on the dependent parameter  As a result this research provides three different outcomes. First of all our model provides a classification framework, which can be used to analyze, discuss, and compare different approaches to the Visual Wiki. Secondly, our research provides a proof of concept for the implemented prototypes Lastly, the evaluation of these prototypes provides suggestions on how the chosen combinations of the independent parameters influence the different purposes of a Visual Wiki  5  The Prototype Visual Wikis  This section describes the requirements, design and prototypical implementati on of the three Visual Wikis as mentioned in the previous section. A common design choice for all prototypes was the use of Thinkmap \(www.thinkmap.com\or the visual representation implementation platform as it can be easily extended and integrated Thinkmap is a software platform for developing customized visualization interfaces. It consists of loosely coupled components which provide users the ability to retrieve a result set from data sources, and then visualize, navigate, and organize it. The Thinkmap Software Development Kit \(SDK provides ways to easily extend and adjust the suite as well as to integrate it with other web and database technologies [9    5.1  Thinkbase  The first prototype Thinkbase is an integration of Thinkmap and Freebase \(www.freebase.com Freebase is a web application described as an open shared database of the world s knowledge also be described as a semantic wiki. The three main requirements for the Thinkbase prototype according to the model in Figure 3 are: the content represents general knowledge the type of mapping is a 1:1 mapping and the consistency is in both ways  Freebase has been chosen for this prototype as it represents very general knowledge. Furthermore it provides both read and write access to the content through an API. The Thinkbase application accesses the Freebase content using that API and represents it in a Thinkmap using a custom data source layer. As a result the visualization represents very general knowledge. In this specific case the visualization represent Freebase topics \(articles\d their semantic relationships Figure 4 show several screenshots of Thinkbase a\shows the general user interface \(UI\with the Thinkmap on the left side and Freebase on the right The interactive graph can be used to navigate and explore the knowledge space. The names of the relationships between nodes in the graph are shown on mouse over and some relationships are combined  Figure 4. Thinkbase screenshots  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 5 


using aggregation nodes. Furthermore, a context menu provides several functionalities such as the ability to further extend or collapse parts of the graph. Figure 4 b\demonstrates what the context menu for adding new semantic relationships looks like, and c\ows how a second visual representation is used to establish these new relationships Central to the Visual Wiki is the mapping between the two representations. Thinkbase provides a 1:1 mapping. This means that each node in the visualization is mapped to one page in the textual component \(Freebase\nd one edge is mapped to one semantic relationship between those components Each Freebase topic is represented in the graph using different icons depending on the type \(e.g. person movie, etc As both representations receive their data from a common source \(the underlying Freebase database two-way consistency is realized. This means that changes to both representations affect the other one Changes to the textual representation can be achieved by editing the Freebase UI. This includes for example creating, editing, or deleting of topics and the relationships between them. The visual representation is editable as well. However, this is limited to the creation of new relationships which is done using the context menu  5.2  VBKE  The second prototype, the Visual Body of Knowledge Explorer VKBE\, is an integration of Thinkmap and Confluence, a corporate wiki www.atlassian.com\KE is a web application that allows visual exploration and creation of a knowledge repository combined with the ability to map to wiki pages. It was developed for internal use within the authors institution. The three main requirements for the VBKE prototype according to the Visual Wiki model in Figure 3 are: the content represents more specific knowledge the type of mapping is a 1:m mapping and there is no consistency between the representations Figure 5 shows several screenshots of VBKE a\shows the general UI with Thinkmap on the left side and a search result within the Confluence wiki on the right side. The visual component includes further functions such as filtering and layout functions; b\ demonstrates how the context menu can be used to perform different types of searches as well as to access editing features; c\hows one of the editing forms for the visual component. In this case the modification of a node The visual component of VBKE has its own customized database, which describes the IT infrastructure of the institution in a graph like structure. The textual component \(the Confluence wiki\s a general knowledge repository operated by the institution s IT Services. As a result the content of the visualization represents more specific knowledge as it does not represent the complete content of the textual component \(like Thinkbase However, it is still quite flexible and extendable The mapping between the visual and the textual component represents a 1:m mapping. This means that one node in the graph is mapped to a set of wiki pages. This set is determined by a search across the wiki space for appropriate content in the pages  Figure 5. VBKE screenshots Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 


Further mapping possibilities include a Google search result and assignment of a concrete URL As the visual component has its own data source which is completely decoupled from the wiki, there is no automated consistency between the two representations. Both are editable independently. The visual component provides a role based moderated editing mechanism. That is, users can suggest changes to nodes and edges which need to be approved by a moderator. These changes do not automatically affect the wiki. In the same way changes to the wiki do not affect the visual component. However, changes to both representations might influence the mapping as search results may change  5.3  ProcessMapper  The third prototype ProcessMapper is an integration of Thinkmap and a MediaWiki ProcessMapper is an application which visualizes an interactive business process representation and maps it to a wiki which is intended to be a process documentation repository. The three main requirements for the ProcessMapper prototype according to the Visual Wiki model in Figure 3 are the content represents specific knowledge the type of mapping is a n:m mapping and there is a one-way consistency between the representations Figure 6 shows several screenshots of the ProcessMapper: a\ shows the general UI with an example process on the left side \(in this case a university enrolment process\d a search result within the wiki on the right side; b\ shows the entity specific context menu of an activity. Different searches which translate into different mappings to the wiki are possible. Figure 6 c\hows an alternative display of the same process. When the user navigates to a business unit, the visualization dynamically changes towards an org chart like display The visual language used for the process is very similar to BPMN. It consists of activities, business units, resources, and gateways. Each entity has different attributes and constraints. Business units for example have an URL attribute \(amongst others resources can only be connected to activities; etc. As a result the underlying metamodel is rather complex compared to the previous two prototypes and the content can represent very specific knowledge The mapping between the visual and the textual component represents a n:m mapping. This is achieved in two ways: First, connected entities in the visualization can be used to perform a combined search in the wiki. Second, connected entities can be used to perform separate searches across the wiki \(or open concrete URL s\ in two different frames. An example of this is shown in Figure 6 a The ProcessMapper provides an XML interface which can be used to load business processes defined in the XML format. The required format can easily be transformed to and from BPEL \(even though this is not implemented for this prototype\d therefore the tool can potentially be integrated with other BPM tools. When the process is loaded for the first time ProcessMapper automatically creates wiki pages for the different entities based on templates. In case the XML file gets modified \(e.g. new business units added\ew pages will also be added to the wiki. This  Figure 6. ProcessMapper screenshots  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


results in the desired one-way consistency. Changes in the visualization will have an impact on the wiki but not the other way. To prevent unwanted deletes wiki pages are not deleted in case an entity is deleted in the process definition  6  Evaluation  We undertook a qualitative evaluation to answer the question: how do the different Visual Wiki prototypes support and improve different knowledge management tasks \(e.g. transfer of knowledge, search and navigation within a body of knowledge, etc.\ In other words: what kind of effects do the chosen independent parameters have on the dependent parameter \(see Figure 3 During an earlier stage of the project, an informal survey was carried out amongst a small group of 5 people who were stakeholders for the development of one or all three of the prototypes. This helped us to determine suitable questions for the actual evaluation We then recruited a group of 14 participants for the evaluation of the three prototypes. Among the participants were 3 undergraduate students, 7 postgraduate students, and 4 staff members of the authors institution. All three prototypes were shown to the participants together with a tutorial in order to gain familiarity with the tools. The participants then answered a set of questions for each of the prototypes as well as some general questions regarding their previous experience with wikis. The answers were gathered using an anonymous online survey tool All of the participants were at least a little bit  experienced with the use of wikis, even though about 14% never used them in the corporate context \(which in this case refers to the usage of wikis e.g. for project collaboration at the university\About 20 described themselves as power users of wikis in the public domain, 30% as power users in the corporate context. Power users are users which document and collaborate on a lot of projects using wikis Half the participants were familiar with the Confluence wiki. Freebase was less known amongst the participants. About 40% never heard of it, and only 1 participant had used it actively \(write The questions regarding the three prototypes were structured in the same way. They were aimed to find out how much the four different tasks \(creating organizing, transfer, and search\mproved as perceived by the participants. For example participants were asked to first only consider the Confluence wiki and then rate how useful the tool is to create or organize content. Then they were asked to look at the VBKE prototype \(which combines Confluence with a visualization\d rate how much the usefulness of the tool for these tasks has improved The first interesting result was revealed when asking the participants about the usefulness of the wikis alone. That is Freebase, Confluence, and MediaWiki. For all three tools the participants rated the usefulness of the four tasks very similarly However, there are differences between the three tools. The usefulness of the MediaWiki for the four tasks was rated lower than the one for Confluence Freebase was rated highest. This ranking corresponds to the degree of structure within the tools. MediaWiki is least structured, Confluence is a bit more structured e.g. by providing spaces, labels, plugins, etc.\nd Freebase with its semantics is the most structured  Figure 7. Results of the evaluation  The main focus of the evaluation was then on comparing these ratings with the ratings of the actual Visual Wiki prototypes. The results can be seen in Figure 7. The diagram shows the four tasks \(create organize, transfer, search\e x-axis. The y-axis shows a scale of how much the usefulness of the prototypes improved on average for each of the four tasks. The value 3 corresponds to very little  improvement, 4 to moderate and 5 to high  improvement. The full scale reaches from 1  worse  very high Overall the results for the three prototypes are very similar. The highest improvement was perceived for the tasks of searching and organizing The improvement in search \(and navigation\ relates to the benefit of visualizations as an effective method to represent knowledge rich scenarios h e tas k of org an izi n g  is closely related to the one of searching, which could be a reason why it was rated quite high as well. The improvements in distributing knowledge were rated as somewhat less useful even though they are still significant. Lastly, the Visual Wiki concept had the least impact on the task of creating knowledge. A reason for that could be that this is a less intuitive task \(as opposed for example to search\d therefore participants were not as comfortable using the visual interfaces to create new content. Interaction with creation support in the visualizations is also currently more limited than the textual wikis. Therefore, this Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


particular finding might be limited to the prototypes only. Further related aspects, such the willingness of participants to collaboratively edit visualizations were out of scope for our evaluation Besides the general similarities between the three prototypes as found in these results, there are also some differences between them. Most notably the ProcessMapper was rated to provide the lowest improvements for all the tasks. A reason for that could be that it is the least mature of the prototypes which means that its functionalities are limited in some ways and it also provides only empty template wiki pages and no real content. Other differences among the prototypes are: VBKE was rated highest for the task of organization for which the reason could be that it provides the most sophisticated editing functions; Thinkbase was rated highest for search and navigation for which the reason could be that the content represents very general knowledge. A specific type of search which could be called search by exploration was described as being incredibly addictive by several people who used the tool Further results from the survey also refer to properties of the Visual Wiki model which have not been explicitly mentioned. Participants rated the approach of Thinkbase for example most useful where the textual representation already provides some kind of semantics, while VBKE was rated more appropriate for semi-structured content. This refers to the metamodel property of the textual component A weakness of our evaluation is the sample size which is quite small. Furthermore, the participants are not necessarily representative for the tools  intended target user group. As at least two of the tools \(Thinkbase and VBKE\are or will be used in a real setting, a more appropriate evaluation with actual users of the tools will be possible in the future  7  Future Work  All three Visual Wiki prototypes have potential to be improved. Thinkbase was recently made publicly available and was received very positively by the international community. The received feedback also helps us to identify new features. This includes different visualization and layout techniques; more control over the visualization itself; and more advanced search mechanisms in order to even better navigate and understand the knowledge space. Such improvements would again put emphasize on search and navigation an area where Thinkbase was already rated as very useful in our evaluation VBKE was also received very positively within the authors institution where it is intended for internal use as an IT infrastructure exploration tool. A production project is currently being initialized in order to further develop VBKE. The ProcessMapper tool was meant as a purely proof of concept prototype which could for example further be developed to explore different mapping mechanisms Future work also includes further improving our Visual Wiki conceptual model \(see section 3\e see this model as a starting point rather than a final or complete one. Based on the model, further new and existing prototypes of Visual Wikis could be designed, implemented and evaluated in a similar way as has been done in this research  8  Conclusions  There are three outcomes of our research so far The proposed Visual Wiki model, a proof of concept for three implemented prototypes of a Visual Wiki and an evaluation of those prototypes Based around the metaphor of a Visual Wiki, a literature review has been carried out. Two research areas were of special interest to us here: new emerging internet tools and technologies and their applications in the corporate context; and the area of information and knowledge visualization and their applications to the field of knowledge management Our literature review suggests synergic effects between those two fields which can be elaborated for knowledge management tools. However it also shows a lack of integration within existing approaches. The literature review ultimately led to the proposal of a conceptual Visual Wiki model. This model can be used to design, analyze, and discuss the different existing as well as new approaches to Visual Wikis Based on our model, three prototypes have been designed and implemented. Those prototypes provide a proof of concept for the Visual Wiki metaphor Most notably it has been demonstrated that different kinds of mappings between the two representations are possible. This crucial component was missing in most of the existing approaches to a Visual Wiki Finally our initial evaluation shows not only that the prototypes are practicable to implement \(proof of concept\t also in what kind of context they are most likely to be useful. This is possible as the proposed model can be used to choose different sets of parameters for the prototypes and then evaluate them according to those parameters. The selected parameter of interest for this research was the purpose parameter. The results of the evaluation suggest that the Visual Wiki metaphor in general improves tasks related to search and organization of information. It furthermore suggests some smaller but noticeable correlation between the chosen independent parameters and specific purposes of the Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


Visual Wiki. Thinkbase appears to be most useful for search \(navigation\ within a knowledge space. VBKE appears to be most useful for organizing knowledge All three outcomes, the model, the prototypes and the evaluation, are extendable and can, as described in section 7, be used to further explore the Visual Wiki metaphor and its applications in the field of knowledge management  9  References  1 A nde rs on, C  The Long Tail: Why the Future of Business Is Selling Less of More 2006: Hyperion 2 A n k o le k a r A e t a l  The two cultures: mashing up web 2.0 and the semantic web Proc. of the 16th international conference on World Wide Web. 2007: p. 825-834  A u er S   S  Dietzo ld and T   Riech ert  OntoWiki-A Tool for Social, Semantic Collaboration Proc. of the 5th International Semantic Web Conference \(ISWC 05\273 in LNCS, Springer, 2006, p. 736 749  Bern ers-L ee T   J Hen d l er an d O L a ssila  The semantic Web Scientific American. 2001. 284\(5\: p. 34-43 5 Bo lla c k e r K R Co ok a n d P  T u f t s   Freebase: A Shared Database of Structured General Human Knowledge Proc. of the national conference on Artificial Intelligence, 2007. 22\(2\. 1962 6 Burk hard, R  Towards a Framework and a Model for Knowledge Visualization: Synergies between Information and Knowledge Visualization LNCS 3426. Springer Berlin, Heidelberg, New York, 2005  Ch atzkel J   Conference report 2006 KMWorld Conference Review Journal of Knowledge Management 2007. 11\(4\: p. 159-166 8 Dav i s, M Semantic Wave 2008 Report: Industry Roadmap to Web 3.0 & Multibillion Dollar Market Opportunities 2008 [29-02-2008 p  proje c t 1 0 x  c o m   9 Desig n P The Thinkmap White Paper 2008-03-08   http://www.thinkmap.com 10 Dotsik a  F. a nd K P a tric k   Towards the new generation of web knowledge VINE: The journal of information and knowledge management systems, 2006 36\(4\: p. 406-422 11 E pple r  M. a n d R. Burk ha r d   Knowledge Visualization  in Encyclopedia of Knowledge Management, Idea Group 2005 12 J a e s c h k e G M. L e is s l e r a nd M. H e m m je  Modeling Interactive, 3-Dimensional Information Visualizations Supporting Information Seeking Behaviors in Knowledge and Information Visualization: Searching for Synergies Springer 2005: p. 119-135 13 K e im D  A  Information visualization and visual data mining IEEE Transactions on Visualization and Computer Graphics, 2002. 8\(1\: p. 1-8 14 Keller T  an d  S.O. T e rg a n  Visualizing Knowledge and Information: An Introduction in Knowledge and Information Visualization: Searching for Synergies Springer 2005: p. 1-23 15 L e ng le r, R. a nd M  J  Ep p l e r  Towards a Periodic Table of Visualization Methods of Management Proc. of the Conference on Graphics and Visualization in Engineering, 2007, pp. 1-6 16 Mc A f e e A  P  Enterprise 2.0: the dawn of emergent collaboration MIT Sloan Management Review, 2006 34\(3\. 38-38 17 McKin s ey  How businesses are using Web 2.0: A McKinsey Global Survey The McKinsey Quarterly 2007 18 M i k a P Flink: Semantic Web technology for the extraction and analysis of social networks Journal of Web Semantics, 2005. 3\(2-3\ p. 211 223 19 O Reilly, T What Is Web 2.0: Design Patterns and Business Models for the Next Generation of Software  O'Reilly Media 2005 20 Sh ne ide r m a n, B The eyes have it: a task by data type taxonomy for information visualizations Proc. IEEE Symp Visual Languages, IEEE CS Press, 1996: p. 336-343 21 T a psc o tt D  Winning with the Enterprise 2.0 New Paradigm Learning Corporation, 2006 22 T a psc o tt D. a n d A  D. W illia m s  Wikinomics: how mass collaboration changes everything 2006: Portfolio 23 T e rg an S.O. an d T   Keller Knowledge and Information Visualization: Searching for Synergies LNCS 3426. Springer, 2005 24 T r e d innic k L  Web 2.0 and Business: A pointer to the intranets of the future Business Information Review 2006. 23\(4\: p. 228 2 Ullm an  A  J an d J Ka y   WikiNavMap: a visualisation to supplement team-based wikis Conference on Human Factors in Computing Systems, 2007. p. 2711-2716 26 V a il  E F  Knowledge Mapping: Getting Started with Knowledge Management Information Systems Management, 1999. 16\(4 27 V i  g a s F.B e t a l  Many Eyes: A Site for Visualization at Internet Scale Proc. of Infovis, 2007  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


  11 Figures 4 and 5 overall may provide some significant insight into the state of th e art for rocket design and manufacture. While the 10% maximum acceptable verification risk level is not an unusual verification risk value, a mission assurance requirement of 0.9973 may be rare. Better than half of the 41 surrogate systems in figure 5 achieved mission assurance in ground tests better than 0.7 with no more than 10% verification risk, and only 15 exceeded an achieved mission assurance of 0.8 Considering the discussion in section 3 of this report concerning how these surrogate systems broadly bracketed the performance and design characteristics of the Orion LAS, it is illuminating to investigate the aggregate mission assurance performance. Table 4 provides the achieved verification risk for a mission assurance level of 0.9973 and the achieved mission assurance level for a 10% verification risk for the aggregated data in Table 1 Table 4: Achieved Mission Assurance and Verification Risk based on Aggregate Surrogate Systems Data Data Type 361 s 361 f  Achieved Risk for 0.9973 Mission Assurance Achieved Mission Assurance at 10 Risk  Ground Tests 263 5 99.97% 0.9680 Flights 4381 8 7.19 0.9974 Totals 5094 13 43.28 0.9964  The parameters in the rightmost two columns of Table 4, as calculated from the flight data, suggest that systems similar to the proposed Orion LAS generally perform in flight with mission assurance slightly better than the proposed required level of 0.9973, with a no more than a 10% risk. However the calculations using the aggregate ground test data suggest that surrogate systems were verified at lower mission assurance levels at 10% verification risk. Considering the aggregate data from these surroga te systems, it appears that if a mission assurance of 0.97 is verified at 10% risk, then better mission assurance performance, perhaps close to the proposed Orion LAS 0.9973 level, can be achieved in actual flight at less than a 10% risk. Thus, an ambitious yet valid mission assurance requirement to be verified at 10% risk to be considered for the Orion LAS then would be at 0.97. The remaining validation question is whether or not verification of a 0.97 mission assurance requirement is achievable and cost feasible Conditional Orion LAS Verifica tion Plan Using Historical Ground Test Data As discussed in section 2 of this report, a verification plan can be developed taking advantage of historical surrogate data. The historical surrogate data that is appropriate for verification of a mission assurance requirement for the Orion LAS is the aggregate ground test data in Table 1 Table 5 presents the plans developed using classical statistical procedures and using conditional approaches to verify a 0.97 mission assurance requirement for the Orion LAS with a 10% risk Table 5: Data Required for Successful Verification of 0.97 Mission Assurance with 10% Risk Using Classical Methods and Conditional Approaches taking Advantage of Surrogate Systems\222 Ground Test Data Classical Conditional n s  P V success  n s  P V success  n f  76 9.88% 18 57.80 0 128 14.90 59 67.00 1 174 15.49% 100 74.80 2 218 15.63% 140 81.5 3  The verification plans in the two leftmost columns of Table 5 are again developed using the binomial test, and cannot take advantage of the surrogate test data. The reduction of required test successes needed in column one of Table 5 vice in Table 2 is solely due to the relaxed mission assurance requirement to be verified As indicated in [11 o n e o f th e in itiato rs o f  th is investigation was a discussion about the numbers of tests needed to verify the proposed 0.9973 mission assurance requirement with no more than a 10% risk \(stated in  as a 90% confidence\The total number of tests being considered as cost feasible based on heuristics was 15, very close to the 18 required in Table 5 if no failures occur when conditional approaches are used in verification planning for the mission assurance requirement of 0.97. The probability of verification success given that the 0.97 mission assurance requirement is satisfied in the as-built Orion LAS does not reach the preferred level above 90%. However, there is a better than two-fold improvement over the original conditional verification plan and five-fold improvement over the classical verification plan 


  12 5  C ONCLUSIONS  The investigations in this report yield three important conclusions for improving valid ation and verification of aerospace systems\222 performance requirements, and suggest a number of investigatory extensions. The improvements should be extensible to validation and verification of any performance requirement for any aerospace system Verification Planning Usi ng Conditional Approaches Reduces Numbers of Tests Required Conditional approaches are used as the basis for applying decision theory. Veri fication for aerospace systems always supports the decision making process for acceptance of the system. Therefore, verification planning using conditional approaches is suitable and appropriate for aerospace systems As demonstrated in section 2 of this report, conditional approaches can be used with models of maximum objectivity to eliminate most, if not all, questionable assumptions. This is not possible when developing a verification plan using classical statistical methods. As observed with the Orion LAS mission assurance requirement example, the use of maximum objectivity models in verification planning did not introduce any unnecessary inherent conserva tism, producing verification plans using fewer numbers of tests, with higher probabilities of verification success than those developed using classical statistical methods. This results in more cost feasible verification plans due to the reduced numbers of tests that need to be performed Requirements Validation is Improved by Using Historical Data When considering verification planning for a stringent aerospace system requirement, hi storical surrogate systems data may be used with conditional methods to investigate actual achieved requirement performance levels in both verification and operations as a function of residual risk Surrogate systems that were fiel ded, were verified to satisfy their performance requirements at stated verification risk levels, and were cost feasible in design, manufacturing, and verification. Conditional approaches allow this validation process to be performed without any knowledge of the actual requirements levied on the surrogate systems, and without using any indefensible assumptions As was observed in this report for the proposed mission assurance requirement for the Orion LAS, 41 surrogate systems actually performed close to the Orion LAS requirement in operations considering the aggregated data However, none of these surrogate systems were verified to the level of the proposed Orion LAS mission assurance requirement with the stated residual verification risk. This suggests that the industry that produced the surrogates, and that will produce the Orion LAS, may indeed design and built such systems to perform at levels that cannot be verified at those levels at a feasible cost. Relaxing the stringency of the Orion LAS mission assurance requirement to the 0.97 level for verification purposes should be valid based on results from processing the aggregate surrogate systems data Further, consider that the Ares I rocket will be designed manufactured, and verified by the same industry that does so for the Orion LAS. A 0.97 mission assurance for the Ares I means that there is a 3% risk of failure. The Orion LAS with a mission assurance requirement of 0.97 has a 3 risk of failing, given that the Ares I fails. With these values for mission assurance requirements for both vehicles, the actual risk of loss of crew due to failure of the Orion LAS when the Ares I fails is the product of these two risks, a 0.09% risk When ground test and flight data from surrogate systems is available, conditional approaches may be used to validate both the level of required performance and required maximum acceptable verification risk for the system of interest Greater Verification Plan Achievability and Feasibility Results from Combining Conditional Approaches with Historical Data Beyond the improvements offered to verification planning when using conditional approaches, the cost feasibility can be further improved if historical surrogate system data are available. As seen for verification of the Orion LAS mission assurance requirement in Table 5, once relaxed to a validated level, a verification plan taking advantage of the historical surrogate systems\222 ground test data can be developed that can be cost feasible, with a greatly improved probability of verification success Extensions to the Investigations Conditional approaches may be used to predict future system performance based on observed data, i.e., test results 7 n s id erin g th at an ticip ated use of the Orion LAS may be limited to fewer than 100 flights, it is possible to predict the probabilities for number of failures of the Orion LAS among a fixed number of flights, given numbers of surrogate ground test and fli ght successes and failures and actual Orion LAS verification results. Factoring these results into verification planni ng, coupled with requirements validation based on processing surrogate data using conditional approaches, may yield further improvements in cost feasibility for verification of stringent requirements for aerospace systems  


  13 R EFERENCES  1 th o n y J. Hayter, Pro b ab ility an d Statistics fo r En g i n eers and Scientists, Third Edition, Belmont, CA, Duxbury 2007 2 C. J. Clo p p er  E S Pear son, \223The Use of Confidence or Fiducial Limits Illustrated in the Case of the Binomial,\224 Biometrika, 26, 404-413, 1934   Jam e s O. Berger, Statis tical Decision Theory and Bayesian Analysis, Second Edition. Springer-Verlag, New York, 1980 4 A. Po well, \223Op tim al an d  Ad ap tab l e Reliab ility Test Planning Using Conditional Methods,\224 Proceedings from the 14 th Annual International Symposium, International Council on Systems Engineering, Toulouse, FR, June 20\226 24, 2004  Harold Jeffreys, Theory of Probability. Oxford University Press, Oxford, 1939  Edward Jaynes, \223Prior Pr obabilities\224. IEEE Transactions Systems, Science and Cybernetics, 4, 227-291, 1968   Jose Bernardo and Adrian Smith, Bayesian Theory. John Wiley & Sons, LTD, New York, 1994   Howard Raifa and Robert Sc hlaifer, Applied Statistical Decision Theory. John Wiley & Sons, Inc. New York 1960 9 Brian Mu irh ead 223Co n stella tion Architecture and System Margins Strategy,\224 Proceedin gs from the International Astronautical Congress, Glasgo w, Scotland, September 29 226 October 3, 2008 10  Sco tt \223Do c\224 Ho ro witz, \223Cu rren t  Co n stellatio n  Planning,\224 presented to the House Science and Technology Committee Staff, March 13, 2007   M a rk A. Powell, Safety and M i ssion Assurance Special Assessments, CxP Orion LAS Verification Planning Evaluation Report, Repository Number: JS-2008-002 November 12, 2007      B IOGRAPHY  Mark Powell has practiced Systems Engineering for over 35 years in a wide range of technical environments including DoD, NASA, DOE, and commercial. More than 25 of those years have been in the aerospace arena. His roles in these environments have included project manager, engineering manager, chief systems engineer, and research scientist. He is currently an adjunct member of the Stevens Institute of Technology Syst ems Engineering Faculty, and of the University of Houston, Clear Lake Systems Engineering Faculty. Mr. Powell maintains an active engineering and management c onsulting practice \(currently in affiliation with SAIC\ North America, Europe, and Asia. Beyond consulting, he is sought frequently as a symposium and conference speaker and for training workshops, and tutorials on various topics in Systems Engineering, Project Management, and Risk Management Mr. Powell is an active member of AIAA, Sigma Xi, the International Society for Bayesian Analysis, and the International Council on Systems Engineering, where he serves as Assistant Direct or for Systems Engineering Processes  


  14  


2008, vol. 278/2008. Boston: Springer, July 2008, pp. 477  492 24] T. Neubauer, C. Stummer, and E. Weippl  Workshop-based Multiobjective Security Safeguard Selection  in Proceedings of the First International Conference on Availability, Reliability and Security ARES IEEE Computer Society, 2006, pp. 366  373 25] T. Neubauer and C. Stummer  Interactive Decision Support for multiobjective COTS Selection  in Proceedings of the 40th Annual Hawaii International Conference on System Sciences, no. 01, 2007 26    Extending Business Process Management to Determine Ef?cient IT Investments  in Proceedings of the 2007 ACM Symposium on Applied Computing, 2007, pp. 1250  1256 27] W3C  OWL - web ontology language  http://www.w3.org/TR/owlfeatures/, February 2004 28] J. Burtles, Principles and Practice of Business Continuity: Tools and Techniques. Rothstein Associates Inc., 2007 29] T. R. Peltier, Information Security Risk Analysis, 2nd ed. Auerbach Publications, 2005 30] S. Kairab and L. Kelly, A Practical Guide to Security Assessments Boston, MA, USA: Auerbach Publications, 2004 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


well as from practitioners, is available, IL strategy including DWH/BI strategy IL strategies are addressed at all, either actual artifacts/documents are proposed without an integrating meta model/methodology, or the strategy making process is described without proposing specific and consistent result templates/structures Since it consumes a significant amount of resources and may constitute significant potentials for business, IL needs strategy. IL strategy must not be limited to hardware/software selection and architectural considerations, but should address the entire business scope of sourcing services, integrating acquired and self-made services into customer-oriented IL solutions, and delivering such solutions to create customer value Our survey of the state of IL strategy in practice reveals that IL sourcing, IL delivery and IL portfolio strategies are regarded as important strategy components. The larger companies are, the more international their focus is, and the more their IL is organized according to the CC model, the more components of a supply-chain oriented explicit IL strategy they are likely to have deployed The IIM model provides a suitable conceptual foundation for structuring such strategy components and also provides best practices from IT management which often can be easily adapted to IL. Regarding IL product/service development and maintenance certain functional oriented strategy sub-components are differentiated in our framework. These strategy components are adapted from an established data management functional framework in order to reflect IL specifics. While traditional, more technically oriented sub-components such as system and data architecture are covered in most companies, business oriented components like change management and project/business requirements management are covered less frequently. Additional research is necessary to develop appropriate solution components based on existing fragments and experiences Based on a more complete comprehension of IL strategy and its components, the strategy development and update process needs to be addressed in future research as well. Instead of developing and updating business strategies, IT strategies and IL strategies in independent processes, dependencies and cycles need to be addressed. A comprehensive understanding of IL strategy and respective processes may also serve as a foundation for establishing maturity models, reference models and best practices  References 1] Arnott, D. and G. Pervan, Eight key issues for the decision support systems discipline. Decision Support Systems 44\(3  2] Baum  l, U., Strategic Agility through Situational Method Construction. Proceedings of the European Academy of Management Annual Conference 2005, 2005  3] Burton, B., et al., Activity Cycle Overview: Business Intelligence and Information Management. Gartner Research G00138711, 2006  4] Chan, J.O., Optimizing Data Warehousing Strategies Communications of the IIMA, 5\(1  5] Earl, M., Management Strategies for Information Technology, Prentice Hall, New York et al., 1989  6] Eckerson, W.W., Data Quality and the Bottom Line 


6] Eckerson, W.W., Data Quality and the Bottom Line Achieving Business Success through a Commitment to High Quality Data. TDWI, Chatsworth, 2002  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 7] Elliott, T., Implementing Business Intelligence Standards. BusinessObjects, 2004  8] English, L.P., Improving Data Warehouse and Business Information Quality: Methods for Reducing Costs and Increasing Profits, Wiley Computer, New York et al., 1999  9] Foshay, N., Best Practices in Business Intelligence Strategy. Blue Hammock, 2006  10] Friedman, T. and B. Hostmann, Management Update The Cornerstones of Business Intelligence Excellence Gartner Research G00120819, 2004  11] Gonzales, M., Creating a BI Stragey Document. DM Review, 2004\(November  12] Henderson, J.C. and N. Venkatraman, Strategic alignment: Leveraging information technology for transforming organizations. IBM Systems Journal, 32\(1  13] Hoffmann, O., Performance Management - Systeme und Implementierungsans  tze. 3 ed, Haupt, Bern et al 2002  14] Klesse, M. and R. Winter, Organizational Forms of Data Warehousing: An Explorative Analysis. in: IEEE Computer Society, Proceedings of the 40th Hawaii International Conference on System Sciences \(HICSS-40 Alamitos, 2007  15] Laudon, J. and K. Laudon, Management Information Systems: Managing the Digital Firm. 10 ed, Prentice Hall 2006  16] Losey, R., Enterprise Data Warehouse Strategy: Articulating the Vision. Dm Review, 2003\(January  17] Luftman, J.N. and R. Kempaiah, Key Issues For IT Executives 2007. MISQ Executive, 7\(2  18] MAIS and AIMS, A Business Intelligence Strategy Proposal for The University of Michigan. 2005  19] Melchert, F., Metadatenmanagement im Data Warehousing. Ergebnisse einer empirischen Studie. Institut f  r Wirtschaftsinformatik, Universit  t St. Gallen, 2004  20] Mosley, M., DAMA-DMBOK Functional Framework Version 3. DAMA International, 2008  21] Olszak, C.M. and E. Ziemba, Business Intelligence as a Key to Management of an Enterprise. in: Informing Science Institute, Informing Science + Information Technology Education, Pori, Finland, 2003  22] R  egg-St  rm, J., The New St. Gallen Management Model: Basic Categories of an Approach to Integrated Management, Palgrave Macmillan, Basingstoke, NY, 2005  23] Sommer, T., et al., Business Intelligence-Strategie bei der Volkswagen AG. in: Integrierte Informationslogistik B. Dinter and R. Winter, Editors, 2008, Springer, Berlin Heidelberg. pp. 261-284  


 24] Subramaniam, A., et al., Strategic planning for Data warehousing. Information &amp; Management, 33, 1997, pp 99-113  25] Totok, A., Entwicklung einer Business-IntelligenceStrategie. in: Analytische Informationssysteme - Business Intelligence-Technologien und -Anwendungen, P. Chamoni and P. Gluchowski, Editors, 2006, Springer, Berlin et al pp. 51-70  26] Vaduva, A. and T. Vetterli, Metadata Management for Data Warehousing: An Overview. International Journal of Cooperative Information Systems, 10\(3 298  27] Watson, H.J., D.L. Goodhue, and B.H. Wixom, The benefits of data warehousing: why some organizations realize exceptional payoffs. Information &amp; Management 39\(6  28] Watson, H.J., C. Fuller, and T. Ariyachandra, Data warehouse governance: best practices at Blue Cross and Blue Shield of North Carolina. Decision Support Systems 38\(3  29] Winter, R. and M. Meyer, Organization Of Data Warehousing In Large Service Companies: A Matrix Approach Based On Data Ownership and Competence Centers. Proceedings of the Seventh Americas Conference on Information Systems \(AMCIS 2001  30] Winter, R., Enterprise-wide Information Logistics Conceptual Foundations, Technology Enablers, and Management Challenges. ITI2008, 2008  31] Zarnekow, R., W. Brenner, and U. Pilgram, Integrated Information Management. Applying Successful Industrial Concepts in IT. 1 ed, Springer, Berlin, 2006  32] Zeid, A., Your BI Competency Center: A Blueprint for Successful Deployment. Business Intelligence Journal 11\(3    Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


The HyspIRI mission utilizes innovative techniques to both reduce the amount of data that must be transmitted to the ground and accommodate the required data volume on the ground The infrastructure and techniques developed by this mission will open the door to future high data volume science missions The designs presented here are the work of the authors and may differ from the current HyspIRI mission baseline A CKNOWLEDGMENTS This research was carried out at the Jet Propulsion Laboratory California Institute of Technology and was sponsored by the Space Grant program and the National Aeronautics and Space Administration R EFERENCES  K W ar\002eld T  V  Houten C Hee g V  Smith S Mobasser B Cox Y He R Jolly C Baker S Barry K Klassen A Nash M Vick S Kondos M Wallace J Wertz Chen R Cowley W Smythe S Klein L Cin-Young D Morabito M Pugh and R Miyake 223Hyspiri-tir mission study 2007-07 002nal report internal jpl document,\224 TeamX 923 Jet Propulsion Laboratory California Institute of Technology 4800 Oak Grove Drive Pasadena CA 91109 July 2007  R O Green 223Hyspiri summer 2008 o v ervie w  224 2008 Information exchanged during presentation  S Hook 2008 Information e xchanged during meeting discussion July 16th  R O Green 223Measuring the earth wi th imaging spectroscopy,\224 2008  223Moore s la w Made real by intel inno v ation 224 http://www.intel.com/technology/mooreslaw/index.htm  T  Doggett R Greele y  S Chein R Castano and B Cichy 223Autonomous detection of cryospheric change with hyperion on-board earth observing-1,\224 Remote Sensing of Environment  vol 101 pp 447\226462 2006  R Castano D Mazzoni N T ang and T  Dogget 223Learning classi\002ers for science event detection in remote sensing imagery,\224 in Proceedings of the ISAIRAS 2005 Conference  2005  S Shif fman 223Cloud detection from satellite imagery A comparison of expert-generated and autmatically-generated decision trees.\224 ti.arc.nasa.gov/m/pub/917/0917 Shiffman  M Griggin H Burk e D Mandl and J Miller  223Cloud cover detection algorithm for eo-1 hyperion imagery,\224 Geoscience and Remote Sensing Symposium 2003 IGARSS 03 Proceedings 2003 IEEE International  vol 1 pp 86\22689 July 2003  V  V apnik Advances in Kernel Methods Support Vector Learning  MIT Press 1999  C Bur ges 223 A tutorial on support v ector machines for pattern recognition,\224 Data Mining and Knowledge Discovery  vol 2 pp 121\226167 1998  M Klemish 223F ast lossless compression of multispectral imagery internal jpl document,\224 October 2007  F  Rizzo 223Lo w-comple xity lossless compression of h yperspectral imagery via linear prediction,\224 p 2 IEEE Signal Processing Letters IEEE 2005  R Roosta 223Nasa jpl Nasa electronic parts and packaging program.\224 http://nepp.nasa.gov/docuploads/3C8F70A32452-4336-B70CDF1C1B08F805/JPL%20RadTolerant%20FPGAs%20for%20Space%20Applications.pdf December 2004  I Xilinx 223Xilinx  Radiation-hardened virtex-4 qpro-v family overview.\224 http://www.xilinx.com/support/documentation data sheets/ds653.pdf March 2008  G S F  Center  223Tdrss o v ervie w  224 http://msp.gsfc.nasa.gov/tdrss/oview.html 7  H Hemmati 07 2008 Information e xchanged during meeting about LaserComm  223W orldvie w-1 224 http://www digitalglobe.com/inde x.php 86/WorldView-1 2008  223Sv albard ground station nor way.\224 http://www.aerospacetechnology.com/projects/svalbard 7 2008  223Satellite tracking ground station 224 http://www.asf.alaska.edu/stgs 2008  R Flaherty  223Sn/gn systems o v ervie w  224 tech rep Goddard Space Flight Center NASA 7 2002  223Geoe ye-1 f act sheet 224 http://launch.geoeye.com/launchsite/about/fact sheet.aspx 2008  L-3 Communications/Cincinnati Electronics Space Electronics 7500 Innovation Way Mason OH 45040 T-720 Transmitter  5 2007 PDF Spec Sheet for the T720 Ku-Band TDRSS Transmitter  L-3 Communications/Cincinnati Electronics Space Electronics 7500 Innovation Way Mason OH 45040 T-722 X-Band  7 2007 PDF Spec Sheet for the T-722  J Smith 07 2008 Information e xchanged during meeting about GDS  J Carpena-Nunez L Graham C Hartzell D Racek T Tao and C Taylor 223End-to-end data system design for hyspiri mission.\224 Jet Propulsion Laboratory Education Of\002ce 2008  J Behnk e T  W atts B K obler  D Lo we S F ox and R Meyer 223Eosdis petabyte archives Tenth anniversary,\224 Mass Storage Systems and Technologies 2005 Proceedings 22nd IEEE  13th NASA Goddard Conference on  pp 81\22693 April 2005 19 


 M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Evolving a ten year old data system,\224 Space Mission Challenges for Information Technology 2006 SMC-IT 2006 Second IEEE International Conference on  pp 8 pp.\226 July 2006  S Marle y  M Moore and B Clark 223Building costeffective remote data storage capabilities for nasa's eosdis,\224 Mass Storage Systems and Technologies 2003 MSST 2003 Proceedings 20th IEEE/11th NASA Goddard Conference on  pp 28\22639 April 2003  223Earth science data and information system esdis project.\224 http://esdis.eosdis.nasa.gov/index.html  M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Evolution of the earth observing system eos data and information system eosdis\\224 Geoscience and Remote Sensing Symposium 2006 IGARSS 2006 IEEE International Conference on  pp 309\226312 31 2006Aug 4 2006  223Earth science mission operations esmo 224 http://eos.gsfc.nasa.gov/esmo  E Masuoka and M T eague 223Science in v estig ator led global processing for the modis instrument,\224 Geoscience and Remote Sensing Symposium 2001 IGARSS 01 IEEE 2001 International  vol 1 pp 384\226386 vol.1 2001  M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Earth observing system eos data and information system eosdis 227 evolution update and future,\224 Geoscience and Remote Sensing Symposium 2007 IGARSS 2007 IEEE International  pp 4005\2264008 July 2007  D McAdam 223The e v olving role of tape in the data center,\224 The Clipper Group Explorer  December 2006  223Sun microsystems announces w orld s 002rst one terabyte tape storage drive.\224 http://www.sun.com/aboutsun/pr/200807/sun\003ash.20080714.2.xml July 2008  223P anasas 227 welcome 224 http://www panasas.com  R Domikis J Douglas and L Bisson 223Impacts of data format variability on environmental visual analysis systems.\224 http://ams.confex.com/ams/pdfpapers/119728.pdf  223Wh y did nasa choose hdf-eos as the format for data products from the earth observing system eos instruments?.\224 http://hdfeos.net/reference/Info docs/SESDA docs/NASA chooses HDFEOS.php July 2001  R E Ullman 223Status and plans for hdfeos nasa's format for eos standard products.\224 http://www.hdfeos.net/hdfeos status HDFEOSStatus.htm July 2001  223Hdf esdis project.\224 http://hdf.ncsa.uiuc.edu/projects/esdis/index.html August 2007  223W elcome to the ogc website 224 http://www.opengeospatial.org 2008  223Open gis Gis lounge geographic information systems.\224 http://gislounge.com/open-gis Christine M Hartzell received her B.S in Aerospace Engineering for Georgia Institute of Technology with Highest Honors in 2008 She is currently a PhD student at the University of Colorado at Boulder where she is researching the impact of solar radiation pressure on the dynamics of dust around asteroids She has spent two summers working at JPL on the data handling system for the HyspIRI mission with particular emphasis on the cloud detection algorithm development and instrument design Jennifer Carpena-Nunez received her B.S in physics in 2008 from the University of Puerto Rico where she is currently a PhD student in Chemical Physics Her research involves 002eld emission studies of nanostructures and she is currently developing a 002eld emission setup for further studies on nano\002eld emitters The summer of 2008 she worked at JPL on the HyspIRI mission There she was responsible for the science analysis of the data handling system speci\002cally de\002ning the data level and processing and determining potential mission collaborations Lindley C Graham is currently a junior at the Massachusetts Institute of Technology where she is working towards a B.S in Aerospace Engineering She spent last summer working at JPL on the data handling system for the HyspIRI mission focusing on developing a data storage and distribution strategy 20 


David M Racek is a senior working toward a B.S in Computer Engineering at Montana State University He works in the Montana State Space Science and Engineering Laboratory where he specializes in particle detector instruments and circuits He spent last summer working at JPL on compression algorithms for the HyspIRI mission Tony S Tao is currently a junior honor student at the Pennsylvania State University working towards a B.S in Aerospace Engineering and a Space Systems Engineering Certi\002cation Tony works in the PSU Student Space Programs Laboratory as the project manager of the OSIRIS Cube Satellite and as a systems engineer on the NittanySat nanosatellite both of which aim to study the ionosphere During his work at JPL in the summer of 2008 Tony worked on the communication and broadcast system of the HyspIRI satellite as well as a prototype Google Earth module for science product distribution Christianna E Taylor received her B.S from Boston University in 2005 and her M.S at Georgia Institute of Technology in 2008 She is currently pursing her PhD at the Georgia Institute of Technology and plans to pursue her MBA and Public Policy Certi\002cate in the near future She worked on the ground station selection for the HyspIRI mission during the summer of 2008 and looks forward to working at JPL in the coming year as a NASA GSRP fellow Hannah R Goldberg received her M.S.E.E and B.S.E from the Department of Electrical Engineering and Computer Science at the University of Michigan in 2004 and 2003 respectively She has been employed at the Jet Propulsion Laboratory California Institute of Technology since 2004 as a member of the technical staff in the Precision Motion Control and Celestial Sensors group Her research interests include the development of nano-class spacecraft and microsystems Charles D Norton is a Principal Member of Technical Staff at the Jet Propulsion Laboratory California Institute of Technology He received his Ph.D in Computer Science from Rensselaer and his B.S.E in Electrical Engineering and Computer Science from Princeton University Prior to joining JPL he was a National Research Council resident scientist His work covers advanced scienti\002c software for Earth and space science modeling with an emphasis on high performance computing and 002nite element adaptive methods Additionally he is leading efforts in development of smart payload instrument concepts He has given 32 national and international keynote/invited talks published in numerous journals conference proceedings and book chapters He is a member of the editorial board of the journal Scienti\002c Programming the IEEE Technical Committee on Scalable Computing a Senior Member of IEEE recipient of the JPL Lew Allen Award and a NASA Exceptional Service Medal 21 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207ñ216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intíl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intíl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





