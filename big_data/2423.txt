An Ontology-based Framework for Knowledge Retrieval Xiaohui Tao Yuefeng Li Ning Zhong 003  Richi Nayak Faculty of Information Technology Queensland University of Technology Australia 003 Department of Systems and Information Engineering Maebashi Institute of Technology Japan f x.tao y2.li r.nayak g qut.edu.au 003 zhong@maebashi-it.ac.jp Abstract Retrieving accurate information from the Web is a great challenge to users The existing information retrieval systems are mostly term-based and thus need to be enhanced toward knowledge-based User information needs need to be better captured in order to deliver personalized search results In this paper an ontology-based framework is proposed for capturing user information needs using a world knowledge base and the user's local instance repository The framework aims to discover a user's background knowledge for knowledge retrieval The evaluation result is encouraging in which the proposed model achieved the same performance as a manual user model 1 Introduction In the past decades the Web information has exploded rapidly How to gather useful information from the Web nowadays becomes a challenging issue Attempting to solve this problem many information retrieval IR systems have been proposed and made great achievements However there is still not a solution to the challenge eventually The IR systems are mostly based on keyword-match techniques and suffer from the problems of information mismatching and overloading The information needs may be expressed in different queries because of different user perspectives background knowledge terminological habits and vocabulary Thus if a user's background knowledge is discovered more accurate information can be retrieved However discovering user background knowledge through a given query is dif\002cult When users read through the content of a document they can easily 002nd out if it is interesting or not This is because users implicitly have a knowledge system built based on their background knowledge By re-b uilding this kno wledge system a user s background knowledge can expect to be discovered and thus better IR performance can be achieved This route is suggested by as future kno wledge retrie v al systems In this paper we introduce a knowledge retrieval framework for ontology-based information gathering and propose an approach to rebuild users knowledge systems A user's mental model and querying model are formalized aiming to describe the process of how an information need is transformed into a query In response to the query the user background knowledge is discovered from the world knowledge base and user's local instance repository Based on these a personalized ontology is constructed to simulate the user's mental model and capture the information need The semantic relations of hypernym/hyponym holonym/meronym and synonym are speci\002ed in the ontology The proposed approach is evaluated by comparing to a manual model that discovers knowledge by linguists and the evaluation result is promising The ontology-based knowledge retrieval framework is a novel contribution to knowledge engineering and Web information retrieval The paper is organized as follows Section 2 presents related work In Section 3 we introduce the ontology-based knowledge retrieval framework The evaluation of our proposal is described in Section 4 and the results are discussed in Section 5 Finally Section 6 makes conclusions 2 Related Work Ontologies have been used by many groups for personalized information retrieval Tran et al introduced an approach to translate keyword queries to DL conjunctive queries and used ontologies to describe a user's background knowledge Gauch et al learned ontologies for users in order to specify their personalized preferences and interests in Web search King et al de v eloped an ontology based on the Dewey Decimal Classi\002cation for distributed IR systems However these works have problems that either the volume of knowledge covered in ontologies are limited or the knowledge speci\002ed is not clear adequately Learning an ontology to specify knowledge is challenging Maedche formally de\002ned an ontology as a 5tuple of concepts hierarchical relations plain relations instances and axioms He also proposed an ontology learn 
2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology 978-0-7695-3496-1/08 $25.00 © 2008 IEEE DOI 10.1109/WIIAT.2008.226 506 
2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology 978-0-7695-3496-1/08 $25.00 © 2008 IEEE DOI 10.1109/WIIAT.2008.226 510 


ing framework using semi-automatic ontology construction tools with human intervention However the human intervention increases the cost in this framework Aiming to reduce the cost Liu and Singh de v eloped ConceptNet ontology by smartly using free contributions from Web users However as a trade-off the knowledge speci\002ed in ConceptNet is not expert evaluated These works need to be improved for knowledge acquisition Many other works attempted to learn ontologies automatically Li and Zhong mined patterns from W eb contents and used association rules for ontology learning Web content mining techniques were also used by Jiang and Tan to disco v er semantic kno wledge from domainspeci\002c documents for ontology learning Dou et al proposed a framework for developing domain ontologies using pattern decomposition clustering/classi\002cation and association rules mining techniques However as pointed out by these w orks co v er only a limited number of concepts and specify only simple super-class and sub-class relations They suffer from the problem of inadequate knowledge speci\002cation In summary there still remains a research gap in learning an ontology to describe user background knowledge in IR This motivates our research work presented in this paper 3 Ontology-based Retrieval Framework The ontology-based knowledge retrieval framework consists of four models a user's mental model and querying model a computer model and an ontology model A user's mental model is her his background knowledge system A querying model is a user's translation of an information need generated from her his mental model The computer model constructs an ontology for the user The constructed ontology is the ontology model aiming to simulate the user's mental model in IR 3.1 Mental Model A search task starts from a user's information need From observations when a user was in need of some information and commencing a search task we found that the user usually fell into one of the following situations 017 she he knew nothing about that information 017 she he had tried but failed to infer that information from his her already possessed knowledge 017 she he might know something about that information but was not sure so she he needed to con\002rm it Based on the 002ndings apparently a user holds a repository in her his brain that stores the knowledge she he possesses that is why a user can check if knowing something or not The second 002nding suggests that the knowledge possessed by a user may be linked each other so that a user can perform an inference task from what is known to what is unknown The last 002nding indicates that a user actually holds a con\002dence rate to the knowledge she he possesses so that the user knows that she he is certain or uncertain about it However the con\002dence rate may be implicit because a user may not be able to express it clearly Apparently a user has an implicit knowledge system in brain for information search Thus although the mechanism of a user's brain-working in Web search has not yet been clearly understood we can at least have the following assumption Assumption 1 A user has a knowledge repository in which 017 the stored knowledge are embedded in an association structure 017 the stored knowledge are associated with implicit con\002dence rates Based on the assumption by calling a user's implicit knowledge system as a mental model  we formalize it as follows De\002nition 1 A user's mental model is a 3-tuple U  031 hK  b B  Gi  where 017  K is a non-empty set of pairs fh k w k ig  where k is a primitive knowledge unit possessed by the user and w k is the user's con\002dence rate on k  017 b B is the backbone of the mental model that frames the association structure of knowledge units 017  G is a set of gaps f g 1  g 2      g i g existing on b B  in which each gap g is a knowledge unit that the user does not possess Note that we use  031 instead of  because this de\002nition is given under Assumption 1 which is based on observations and cannot be proved in laboratories currently 3.2 Querying Model Filling a knowledge gap in the mental model triggers a user's search task and becomes the user's information need This implicit g in U is expressed in an explicit short phrase by the user through her his own language e.g English In IR we call this phrase as a query which is a set of terms We formally describe a user's query as a querying model in our knowledge retrieval framework De\002nition 2 A user querying model Q is a set of terms f t j t 2 L u g  in which elements are primitive units in the user's language L u  
507 
511 


Generating a query means the process of translating an implicit knowledge gap g 2 G in a user's mental model U  to a set of explicit terms in Q  In contrast capturing an information need means the inverse process of translation from a Q back to a g 2 G in U  However users may use different terms to generate queries even for the same information need because of user perspectives terminological habits and vocabulary Thus capturing a user's information need through a given query only is extremely dif\002cult However if the user background knowledge can be discovered capturing the accurate information need is possible In the following sections we discuss how this can be done 3.3 Computer Model The computer model aims to simulate a user's mental model The computer model discovers a user's background knowledge associated with an information need For the sake of explanation we 002rst formalize the computer model De\002nition 3 A computer model C is a 3-tuple C  h WKB  LIR  F i  where 017 WKB is a world knowledge base that frames a user's background knowledge 017 LIR is a user's local instance repository in which the elements cite the knowledge in WKB  017 F is a set of functions inferences and algorithms that build an ontology for a user using WKB and LIR  To simulate a user's mental model U  an ontology is constructed based on the WKB and personalized using the LIR  co-responding to a querying model Q for a g 2 G  3.3.1 Constructing Ontologies using WKB The world knowledge base is a knowledge frame describing and specifying the background knowledge possessed by humans In this paper we assume the existence of a world knowledge base and use a subject as a primitive knowledge unit in the WKB  Subjects in the WKB are linked by semantic relations Two semantic relations are postulated existing in the WKB  hypernym  hyponym and holonym  meronym  Hypernym/hyponym usually called is a  relations describe the situation that the semantic range referred by a hyponym is within that of its hypernym e.g car is a hyponym of automobile and automobile is a hypernym of car they are on different levels of abstraction or concretion In this paper we treat hypernym/hyponym as one single semantic relation as they are just the two sides of one coin Differently holonym/meronym usually called part of  relations de\002ne the relationship between a holonym subject denoting the whole and a meronym subject denoting a part of or a member of the whole e.g a tyre is a meronym of its holonym car Again we treat holonym/meronym as one relation The world knowledge base is formalized as De\002nition 4 Let WKB be a world knowledge base which is a directed acyclic graph consisting of a set of subjects linked by their semantic relations WKB is formally de\002ned as a 2-tuple WKB  h S  R i  where 017 S is a set of subjects S  f s 1  s 2  001 001 001  s m g  in which each element is a 2-tuple s  h label 033 i  where label is the name of s and label  s   f t 1  t 2      t j j t 2 L u g  and 033 is a signature mapping de\002ning a set of subjects that directly link to s  and 033  s  022 S  017 R is a set of relations R  f r 1  r 2  001 001 001  r n g  in which each element is a 2-tuple r  h r 027  r 034 i  where r 027 022 S 002 S and r 034 is a relation type of hypernym/hyponym or holonym/meronym For each  s x  s y  2 r 027  s x is the subject who holds the r 034 of relation to s y  e.g s x is a hypernym of s y  The relevance of a subject to a query in WKB is determined using the syntax-matching mechanism because they are both represented by a set of terms in the user's language L u see De\002nition 2 and 4 We use sim  s Q  to specify the relevance of a subject s 2 S to Q  sim  s Q   j label  s   Qj  1 A subject with sim  s Q   0 is a positive subject relevant to Q  otherwise a non-relevant negative subject Let S be a subject set dealing with Q  R be a relation set specifying the semantic relations existing in S  The positive subjects in S are 002rst extracted for S  For each s 2 S  the subjects in its 033  s  are also extracted for S  along with their associated semantic relations r 2 R to s extracted for R  The extraction is iteratively conducted for three times as we believe that any subjects out of that range from a positive subject are no longer signi\002cant and can be ignored We can then decompose S into to two sets S  and S 000  based on the extracted subjects sim  s Q  values S   f s j sim  s Q   0  s 2 S g  S 000  f s j sim  s Q   0  s 2 Sg  2 and have their semantic relations speci\002ed as R  f r j r 2 R  r 027 022 S 002 Sg  3 The subjects in S specify the implicit knowledge k in K in a user's mental model U  associated with an information need g 2 G  which is expressed by the user as a query Q  S  speci\002es the knowledge relevant to g  and S 000 speci\002es the subjects paradoxical or non-relevant to g  The relations in R link the subjects in S  and thus provide the backbone of b B for U  By these we have the user's ontology constructed 
508 
512 


3.3.2 Personalizing Ontologies using LIR s A user's constructed ontology is personalized using the user's LIR  An LIR is a collection of information items that are recently visited by the user These items have tags assigned with subjects that cite the knowledge speci\002ed in the WKB  A user's personal background knowledge related to an information need can be discovered from these citations The discovered background knowledge personalizes the constructed ontology The subjects assigned to items in an LIR are the ties connecting the LIR to the WKB  We call an element in LIR s as an instance and denote it by i  Let I  f i 1  i 2  001 001 001  i p g be an LIR  and S 022 S be a set of subjects assigned to the instances in I  The relationships between S and I can be described as the following mappings 021  I  2 S  021  i   f s 2 S j s is cited by i g  021 000 1  S  2 I  021 000 1  s   f i 2 I j s 2 021  i  g  4 where 021 000 1  s  is a reverse mapping of 021  i   These mappings aim to explore the semantic matrix existing between the subjects and instances The beliefs of an instance to its referring subjects are varying An instance cites multiple subjects and these subjects are indexed by their importance to the instance Thus the belief of an instance to a subject can be de\002ned by bel  i s   priority  s i  n  i   5 where n  i  is the number of subjects on the citing list of instance i  priority  s i   1 index  s;i  where index  s i  is the index starting from one of s on the citing list of i  The bel  i s  increases when less subjects occur in the citing list and the higher index of s on the list A user's personal background knowledge is discovered from the semantic matrix existing between subjects and instances Based on the mappings in Eq 4 we 002rst de\002ne the coverset for a subject aiming to discover its synonym subjects A subject's coverset  referring to the extent of instances in an LIR citing the subject is de\002ned by coverset  s   f i j i 2 021 000 1  s  g  6 Assume subject s 1 2 S  and s 2 2 S 000  if coverset  s 1   coverset  s 2  6    they have something in common because s 1 and s 2 both refer to some common instances Thus we may say that s 2 is relevant to s 1  and may further deduce that s 2 may also be interesting to the user because s 1 is a positive subject Based on these let b S  s   f s 0 j s 0 2 S   coverset  s 0   coverset  s  6  g be the synonyms of s 2 S 000 in S   we discover new interesting subjects from S 000 and personalize the S  and S 000 by S   S   f s j s 2 S 000  b S  s  6  g  S 000  S 000 000 f s j s 2 S 000  b S  s  6  g  7 We call the positive subjects extracted by using syntaxmatching mechanism as the initial positive subjects  and the subjects f s j s 2 S 000  b S  s  6  g as newly discovered interesting subjects  for the sake of explanation in this paper 3.3.3 Specifying Con\002dence Rates Recall back to Assumption 1 and De\002nition 1 a knowledge unit k possessed by a user in the mental model U is associated with a con\002dence rate w k  In this section we call this con\002dence rate as the support value sup  s Q  of a subject s to Q  and present how w k is re-produced for subjects The sim  s Q  judges the positive or negative of a subject to Q  and has impact to the sup  s Q   For an initial positive subject we have its sim value measured by Eq 1 We also need to de\002ne the sim values for those new interesting subjects discovered in Section 3.3.2 Because these new interesting subjects are discovered based on their synonyms in initial S   these synonyms also have authority to determine a new interesting subject's sim value Thus we measure the sim of a new interesting subject as sim  s Q   P s 0 2 b S  s  conf  s 0  s  002 sim  s 0  j b S  s  j  8 where s 0 is an initial positive subject and sim  s 0  is determined by Eq 1 conf  s 0  s  is the con\002dence of s received from its synonyms and calculated by conf  s 0  s   j coverset  s 0   coverset  s  j j coverset  s 0  j  9 As a result a new interesting subject overlapping more initial positive subjects would have higher sim value Subjects with higher sim has greater sup  s Q  values A subject's locality in the backbone of ontology affects its sup  s Q   Subjects located toward lower bound levels of backbone are more speci\002c and so more focused on the knowledge they refer to Thus they should have higher sup  s Q  than the subjects located toward upper bound more abstractive levels For the study of a subject's locality we use an ontology mining method Speci\002city  which is introduced by 14 and describes a subject s semantic focus on its referring knowledge Algorithm 1 presents how the speci\002city values are assigned to subjects in an ontology hyponym  s  and meronym  s  are functions that return the set of direct hyponyms or meronyms of s satisfying hyponym  s  022 033  s  and meronym  s  022 033  s   022 is a coef\002cient de\002ning the reducing rate of speci\002city for focus lost in each step up from lower bound toward upper bound levels  022  0  9 in our experiments A subject's speci\002city depends on its child subjects as described in Algorithm 1 As WKB is a directed acyclic graph see De\002nition 4 Algorithm 1 has complexity of only O  n   where n  jSj  
509 
513 


input  the subject and relation set  S  R   a coef\002cient 022 between 0,1 output  speci\002city spe  s  assigned to all s 2 S  let k  1  get the set of leaves S 0 from S  1 for  s 0 2 S 0  assign spe  s 0   k  2 get S 0 which is the set of leaves in case that we remove the 3 nodes in S 0 and the related relations from  S  R   if  S 0    then return the terminal condition  4 foreach s 0 2 S 0 do 5 if  hyponym  s 0     then spe 1  k  6 else spe 1  022 002 min f spe  s  j s 2 hyponym  s 0  g  7 if  meronym  s 0     then spe 2  k  8 else spe 2  P s 2 meronym  s 0  spe  s  j meronym  s 0  j  9 spe  s 0   min  spe 1  spe 2   10 end 11 k  k 002 022 S 0  S 0  S 0  go to step 3 12 Algorithm 1  Analyzing Semantic Relations for Speci\002city The instances in an LIR contain a user's background knowledge and should also count in the sup  s Q  values of the LIR s citing subjects Considering the bel  i Q  calculated by Eq 5 the support value sup  i Q  held by an instance i to Q is calculated by sup  i Q   X s 2 021  i  bel  i s  002 sim  s Q   10 Because the negative subjects have sim  s Q   0  only the positive subjects cited by i count in sup  i Q   Finally sup  s Q  is calculated based on the sim  s Q  from Eq 1 or 8 the spe  s  from Algorithm 1 and the related sup  i Q  from Eq 10 sup  s Q   spe  s  002 sim  s Q  002 X i 2 021 000 1  s  sup  i Q   11 For a subject in the 002nal S 000  the sup  s Q   0 because its sim  s Q   0  The support value sup  s Q  speci\002es the w k of h k w k i 2 K in U considering an information need where s is for a k and Q is for a g 2 G  During the personalized ontology learning the WKB in the computer model C constructs an ontology for a user and the user's LIR is used to personalize the ontology Equations 1 to 11 and Algorithm 1 are used for knowledge extraction They are the F in C  as described in De\002nition 5 3.4 Ontology Model The ontology model is the product from the computer model aiming to simulate a user's implicit mental model dealing with an information need An ontology model can be formalized as follows De\002nition 5 An ontology model associated with a Q is a 4-tuple O  Q   hS  R  tax S  rel i  where 017  S is a set of subjects  S 022 S  consisting of a positive subset S  relevant and a negative subset S 000 nonrelevant to Q  017  R is a set of relations and R 022 R  017 tax S  tax S 022 S 002 S is a function de\002ning the taxonomic structure of ontology containing two directed relations of hypernym/hyponym and holonym/meronym 017 rel is a function de\002ning non-taxonomic relation of synonyms e.g overlapping The ontology model simulates a user's mental model U  The knowledge K in U is speci\002ed by S  in which the S  is relevant and S 000 is non-relevant to a Q representing an information need g 2 G  The w k for k in K is re-produced by sup  s Q  for the subjects in S  The b B in U is speci\002ed by R  tax S and rel in O  Q   The mental model U is rebuilt 4 Evaluation In the IR 002elds a common batch-style experiment is to select a collection of documents testing set a set of topics associated with relevance judgements training set and then compare the performance of experimental models Our experiments follow this style and use the standard testbed and topics as that used in the TREC-11 Filtering track 1  which aims to evaluate IR methods using relevant and nonrelevant training sets Our proposed model called ONTO model in the experiments is compared with an implemented mental model called TREC model in experiments in the experiments The experiment design is illustrated in Fig 1 Against an incoming topic the TREC model generates a training set manually whereas the ONTO model builds a user's personalized ontology automatically and generates a training set from the user's LIR  A training set consists of a set of positive samples D  and a set of negative samples D 000  Each sample is a document d holding a support value support  d  to the given topic The different training sets are used by the common information gathering system to retrieve information from the testing set The performance of the information gathering system is then affected by the training sets input Based upon this we can compare the performances and evaluate our proposed model The Reuters Corpus Volume 1 RCV1 used in the TREC-11 is also used as the testbed in our experiments The RCV1 is a large XML document set 806,791 documents with great topic coverage A set of 50 topics are also provided by the TREC-11 These topics are designed by linguists manually and associated with relevance documents judged by the same linguists All 50 topics are 1 Text REtrieval Conference http://trec.nist.gov 
510 
514 


Figure 1 Experiment Design used in our experiments in order to maintain the high stability as suggested by The topics ha v e title description and narrative However only the titles are used as queries because in real world users only use short phrases to express their information needs 4.1 Information Gathering System An information gathering system IGS is implemented for common use by all the experimental models The IGS is an implementation of a model developed by which uses user pro\002les for information gathering The s model is chosen because not only it is veri\002ed better than the Rocchio and Dempster-Shafer models but also it is extensible in using support values of training documents The input support values associated with documents would affect the IGS's performance sensitively The technical details and the related justi\002cations can be referred to The IGS 002rst uses the training set to evaluate weights for a set of selected terms T  After text pre-processing of stopword removal and word stemming a positive document d becomes a pattern that consists of a set of term frequency pairs  d  f  t 1  f 1    t 2  f 2        t k  f k  g  where f i is t i s term frequency in d  The semantic space referred by  d is represented by its normal form 014  d   which satis\002es 014  d   f  t 1  w 1    t 2  w 2        t k  w k  g  where w i  i  1      k  are the weight distribution of terms and w i  f i P k j 1 f j  A probability function on T can be derived based on the normal forms of positive documents and their supports for all t 2 T  pr 014  t   X d 2 D    t;w  2 014  d  support  d  002 w 12 The testing documents can be indexed by weight  d   which is calculated using the probability function pr 014  weight  d   X t 2 T pr 014  t  002 034  t d  13 where 034  t d   1 if t 2 d  otherwise 034  t d   0  4.2 TREC Model The TREC model is the implementation of a user's mental model For a given topic the TREC linguists read a set of documents and marked each document positive or negative against the topic If a document d is marked positive it becomes a positive document in the TREC training set and support  d   1 j D  j  otherwise it becomes a negative document and support  d   0  Since the linguists who marked the documents are also the people who generated the topics following the assumption that only users know their interests and preferences perfectly the TREC model makes a golden model to our proposed model to mark The modelling of a user's mental model can be proven successful if the ONTO model can achieve the same or close performance to this golden model 4.3 ONTO Model This model is the implementation of our proposed model As illustrated in Fig 1 and required by the IGS the input to this model is a topic and the output is a training set consisting of positive documents  D   and negative documents  D 000  Each document is associated with a support  d  value indicating its support level to the topic The WKB described in Section 3.3.1 is constructed based on the Library of Congress Subject Headings 2 LCSH system The LCSH system is a categorization developed for organizing the large volumes of library collections and for retrieving information from the library The subject headings in the LCSH are transformed into the subjects in WKB  and the LCSH structure is transformed into the backbone of WKB  Eventually the constructed WKB contains over 400,000 subjects covering various topics The semantic relations in the WKB are transformed from the references Broader term  Narrower term and Used-for  speci\002ed in the LCSH The Broader term and Narrower term references are transformed into hyponym/hypernym relations Used-for references are usually used in two situations to describe an action or to describe an object When object A is used for an action A actually becomes a part of that action like using turner in cooking when A is used for object B  A becomes a part of B  likeusing wheels for a car Hence we transform the 2 http://classi\002cationweb.net 
511 
515 


Used-for references in the LCSH into holonym/meronym relations in our WKB  In the experiments we assume that each topic comes from an individual user We attempt to evaluate our model in an environment that covers great range of topics However it is not realistic to expect a participant to hold such great range of topics in personal interests Thus for the 50 experimental topics we assume each one coming from an individual user and learn her his personalized ontology An LIR is collected through searching the subject catalogue of Queensland University of Technology QUT Library 3 by using the title of a topic Librarians have assigned title table of content summary and a list of subjects to each information item e.g a book stored in QUT library The assigned subjects are treated as the tags in Web documents that cite the knowledge in the WKB  In order to simplify the experiments we only use the librarian summarized information title table of content and summary to represent an instance in an LIR  All these information can be downloaded from QUT's Web site and are available to the public Once the WKB and an LIR are ready an ontology is learned as described in Section 3.3.1 and personalized as in Section 3.3.2 The user con\002dence rates on the subjects are speci\002ed as in Section 3.3.3 A document d i in the training set is then generated by an instance i  and its support value is determined by support  d i   X s 2 021  i  s 2S sup  s Q  14 where s 2 S in O  Q  are as de\002ned in De\002nition 5 As sup  s Q   0 for s 2 S 000 according to Eq 11 the documents with support  d   0 go to D 000  whereas those with support  d   0 go to D   4.4 Performance Measures The performance of the experimental models are measured by three methods the precision averages at eleven standard recall levels 11SPR the mean average precision MAP and the F 1 Measure They are all based on precision and recall the modern IR evaluation methods The 11SPR is reported suitable for information gathering and is used in TREC evaluations as a performance measuring standard An 11SPR v alue is computed by summing the interpolated precisions at the speci\002ed recall cutoff and then dividing by the number of topics P N i 1 precision 025 N  025  f 0  0  0  1  0  2      1  0 g  15 N is the number of topics and 025 are the cutoff points where the precisions are interpolated At each 025 point an aver3 http://library.qut.edu.au Figure 2 Experimental 11SPR Results age precision value over N topics is calculated These average precisions then link to a curve describing the recallprecision performance The MAP is a stable and discriminating choice in information gathering evaluations and is recommended for measuring general-purpose information gathering methods The average precision for each topic is the mean of the precision obtained after each relevant document is retrieved The MAP for the 50 experimental topics is then the mean of the average precision scores of each of the individual topics in the experiments The MAP re\003ects the performance in a non-interpolated recall-precision fashion F 1 Measure is also well accepted by the information gathering community which is calculated by F 1  2 002 precision 002 recall precision  recall  16 Precision and recall are evenly weighted in F 1 Measure For each topic the macro F 1 Measure averages the precision and recall and then calculates F 1 Measure whereas the micro F 1 Measure calculates the F 1 Measure for each returned result and then averages the F 1 Measure values The greater F 1 values indicate the better performance 5 Results and Discussions The experiments attempt to evaluate our proposed model by comparing to an implementation of mental model We expect that the ONTO model can achieve at least the close performance to the TREC model The experimental 11SPR results are illustrated in Fig 2 At recall point 0.3 the TREC model slightly outperformed the ONTO model but at 0.5 and 0.6 the ONTO model achieved better results than the TREC model subtly At all other points their 11SPR results are just the same For the MAP results shown on Table 1 the ONTO model achieved 0.284 which is just 0.006 below the TREC model 2 
512 
516 


TREC ONTO p-value Macro-FM 0.388 0.386 0.862 Micro-FM 0.356 0.355 0.896 MAP 0.290 0.284 0.484 Table 1 Other Experimental results downgrade For the average macroand microF 1 Measures also shown on Table 1 the TREC model only outperformed the ONTO model by 0.002 0.5 in macro F 1 and 0.001 0.2 in micro F 1  The two models achieved almost the same performance The evaluation result is promising The statistical test is also performed on the experimental results in order to analyze the evaluation's reliability As suggested by we use the Student's Paired T-Test for the signi\002cance test The null hypothesis in our T-Test is that no difference exists in two comparing models When two tests produce substantially low p-value usually  0.05 the null hypothesis can be rejected In contrast when two tests produce high p-value usually  0.1 there is not or just little practical difference between two models The T-Test results are also presented on Table 1 The pvalue s show that there is no evidence of signi\002cant difference between two experimental models as the produced pvalue s are quite high  p-value 0.484\(MAP 0.862\(macroFM and 0.896\(micro-FM far greater than 0.1 Thus we can conclude that in terms of statistics our proposed model has the same performance as the golden TREC model and the evaluation result is reliable The advantage of the TREC model is that the experimental topics and the training sets are generated by the same linguists manually They as users perfectly know their information needs and what they are looking for in the training sets Therefore it is reasonable that the TREC model performed better than the ONTO model as we cannot expect that a computational model could outperform a such perfect manual model However the knowledge contained in TREC model's training sets is well formed for human beings to understand but not for computers The contained knowledge is not mathematically formalized and speci\002ed The ONTO model on the other hand formally speci\002es the user background knowledge and the related semantic relations using the world knowledge base and local instance repositories The mathematic formalizations are ideal for computers to understand This leverages the performance of the ONTO model As a result as shown on Fig 2 and Table 1 the ONTO model achieved almost the same performance as that of the TREC model 6 Conclusions In this paper an ontology-based knowledge IR framework is proposed aiming to discover a user's background knowledge to improve IR performance The framework consists of a user's mental model a querying model a computer model and an ontology model A world knowledge base is used by the computer model to construct an ontology to simulate a user's mental model and the ontology is personalized by using the user's local instance repository The semantic relations of hypernym/hyponym holonym/meronym and synonym are speci\002ed in the ontology model The framework is successfully evaluated by comparing to a manual user model The ontology-based framework is a novel contribution to knowledge engineering and Web information retrieval References   C Buckley and E M Voorhees Evaluating evaluation measure stability In Proc of SIGIR 00  pages 3340 2000   R M Colomb Information Spaces The Architecture of Cyberspace  Springer 2002   D Dou G Frishkoff J Rong R Frank A Malony and D Tucker Development of neuroelectromagnetic ontologies\(NEMO a framework for mining brainwave ontologies In Proc of KDD 07  pages 270279 2007   S Gauch J Chaffee and A Pretschner Ontology-based personalized search and browsing Web Intelligence and Agent Systems  1\(3-4 2003   X Jiang and A.-H Tan Mining ontological knowledge from domain-speci\002c text documents In Proc of ICDM 05  pages 665668 2005   J D King Y Li X Tao and R Nayak Mining World Knowledge for Analysis of Search Engine Content Web Intelligence and Agent Systems  5\(3 2007   D D Lewis Y Yang T G Rose and F Li RCV1 A new benchmark collection for text categorization research Journal of Machine Learning Research  5:361397 2004   Y Li and N Zhong Mining Ontology for Automatically Acquiring Web User Information Needs IEEE Transactions on Knowledge and Data Engineering  18\(4 2006   H Liu and P Singh ConceptNet a practical commonsense reasoning toolkit BT Technology  22\(4 2004   A D Maedche Ontology Learning for the Semantic Web  Kluwer Academic Publisher 2002   S E Robertson and I Soboroff The TREC 2002 002ltering track report In Text REtrieval Conference  2002   M D Smucker J Allan and B Carterette A Comparison of Statistical Signi\002cance Tests for Information Retrieval Evaluation In Proc of CIKM'07  pages 623632 2007   X Tao Y Li and R Nayak A knowledge retrieval model using ontology mining and user pro\002ling Integrated Computer-Aided Engineering  15\(4 2008   X Tao Y Li N Zhong and R Nayak Ontology mining for personalzied web information gathering In Proc of WI 07  pages 351358 2007   T Tran P Cimiano S Rudolph and R Studer Ontologybased interpretation of keywords for semantic search In Proc of the 6th ICSW  pages 523536 2007   Y Y Yao Y Zeng N Zhong and X Huang Knowedge retrieval KR In Proc of WI 07  pages 729735 2007 
513 
517 


TESTS IN SECOND t INDICATES nl WAS LOWERED TO 2 Training BSTC Top-k RCBT 7 OC Holdout Validation Results RCBT outperforms BSTC on the single test it could finish by more then 7 although it should be kept in mind that RCBT's results for the 24 unfinished tests could vary widely Note that BSTC's mean accuracy increases monotonically with training set size as expected At 60 training BSTC's accuracy behaves almost identically to RCBT's 40 training accuracy see Figure 6 4 Ovarian Cancer OC Experiment For the Ovarian Cancer dataset which is the largest dataset in this collection the Top-k mining method that is used by RCBT also runs into long computational times Although Top-k is an exceptiounally fast CAR group upper bound miner it still depends on performing a pruned exponential search over the training sample subset space Thus as the number of training samples increases Top-k quickly becomes computationally challenging to tune/use Table VI contains four average classification test run times in seconds for each Ovarian Cancer\(OC training size As before the second column run times each give the average time required to build both class 0/1 BSTs and then use them to classify all test's samples with BSTC Note that BSTC was able to complete each OC classification test in about 1 minute In contrast RCBT again failed to complete processing most classification tests within 2 hours Table VI's third column gives the average times required for Top-k to mine the top 10 covering rule groups upper bouhnds for each training set test with the same 2 hour cutoff procedure as used for PC testing The fourth column gives the average run times of RCBT on the tests for which Topk finished mining rules also with a 2 hour cutoff Finally the  RCBT DNF column gives the number of tests that RCBT was unable to finish classifying in  2 hours each THE OC TESTS THAT RCBT FINISHED Training BSTC RCBT 40 92.05 97.66 60 95.75 96.73 80 94 12 98.04 1-133/077 9380 96.12 1070 cJ CZ C 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 BSTC RCBT d Median Median  Mean 260 Near outliers  Far outliers 40 Training 60 Training 0.90.80.70.6BSTC RCBT a 80 Training 1-52/0-50 Training 0.9DNFI 0.80.70.6BSTC RCBT b 1 u0.9DNFI 0.80.70.6BSTC RCBT  RCBT DNF 40 30.89 0.6186 273.37 0/25 60 61.28 41.21  5554.37 19/25 80 71.84  1421.80  7205.43 t 21/22 TIMES FOR THE OC 9 Mean 0 Near outliers  Far outliers 1.01 11 01 1.0 d Fig 6 PC Holdout Validation Results BSTC RCBT a Fig 0.80.8 0.8BSTC RCBT BSTC RCBT b c c i DNF cJ CZ C 40 Training 60 Training 80 Training 1-133/0-77 Training 0.95 DNF DNF DNF 0.9 0.90.90.90.85 0.8 BSTC RCBT TABLE VI AVERAGE RUN 1 133/0-77 70.38  1045.65  6362.86 t 20/23 over the number of tests for which Top-k finished Because RCBT couldn't finish any 80 or 1-133/0-77 tests within 2 hours with nl  20 we lowered nl to 2 Classification Accuracy Figure 7 contains boxplots for BSTC on all four OC classification test sets Boxplots were not generated for RCBT with 60 80 or 1-133/0-77 training since it was unable to finish all 25 tests for all these training set sizes in  2 hours each Table VII lists the mean accuracies of BSTC and RCBT over the tests on which RCBT was able to produce results Hence Table VII's 40 row consists of averages over 25 results Meanwhile Table VII's 60 row results are from 6 tests 80 contains a single test's result and 1-133/0-77 results from 3 tests RCBT has better mean accuracy on the 40 training size but the results are closer on the remaining sizes   4 difference over RCBT's completed tests Again RCBT's accuracy could vary widely on its uncompleted tests CAR Mining Parameter Tuning and Scalability We attempted to run Top-k to completion on the 3 OC 80 training and 2 OC 1-133/0-77 training tests However it could not finish mining rules within the 2 hour cutoff Top-k finished two of the three 80 training tests in 775 min 43.6 sec and 185 min 3.3 sec However the third test ran for over 16,000 mnm  11 days without finishing Likewise Top-k finished one of the two 1-133/0-77 tests in 126 min 45.2 sec but couldn't finish the other in 16,000 min  11 days After increasing Top-k's support cutoff from 0.7 to 0.9 it was able to finish the two unfinished 80 and 1-133/0-77 training tests in 5 min 13.8 sec and 35 min 36.9 sec respectively However RCBT with nl 2 then wasn't able to finish lower bound rule mining for either of these two tests within 1,500 min Clearly CAR-mining and parameter tuning on large training sets is TABLE VII MEAN AcCU1ACIES FOR 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


