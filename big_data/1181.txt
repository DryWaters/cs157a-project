Two-Level Automatic Classification applied to Bulky Data Bases Dorsaf CHERIF\(1 Mohamed BEN AHMED 2 1 RIADI Research Laboratory Ecole Nationale des Sciences de l'Informatique ENSI La Manouba University Tunisia 2 RIADI Research Laboratory Ecole Nationale des Sciences de l'Informatique ENSI La Manouba University Tunisia Abstract Classifying the data of a bulky base on the basis of high number of attributes is not an easy task because of the scarcity of the adequate methods present in the literature These methods generally resort to the reduction of the number of data using sampling 
techniques or the analysis in Principal components APC Problems are often encountered namely the complexity of calculation the slowness of execution and the relevance of the results We developed for this purpose an approach of Two-level automatic classification allowing to transform a bulky base into an exploitable group of classes for the extraction of knowledge and decision-making The robustness the precision and the optimality of our approach are shown through its comparison with the traditional approach of classification classification of the original 
data base and this through the results produced following the application of two approaches to a bulky data base These results include both the clusters and the Knowledge Map formed by association rules generated on the original base on the one hand and the summary of BIRCH on the other hand Key words Automatic Classification KOHONEN maps method KMEANS BIRCH Algorithm decision trees association rules 1 Introduction When assumptions are checked by the data i.e generally when the data are supposed to 
follow laws looking like an exponential family Gaussian Binomial Poisson the statistical techniques of modeling are optimal However as soon as the distributional assumptions are not checked and the assumed relations between the variables are not known other methods must then necessarily be considered taking into account the algorithmic complexity of calculations These methods were often developed in another disciplinary environment Data Mining which calls upon methods of hierarchical classification algorithms of dynamic reallocation KMEANS networks of neurons selforganizing map of KOHONEN 
becomes a credible alternative However when the base is of bulky size the methods of Data Mining become insufficiently powerful to treat bulky series of data Numerous comparative researches on methodologies have been undertaken for years It has become more frequent to resort to the reduction of the number of data It is obvious that methods such as sampling or Principal Components Analysis PCA are very appreciated by researchers They could however show certain weaknesses in term of complexity of calculation and slowness of execution In order to answer these specific problems we 
conceived an approach of automatic classification TwoLevel able to transform a bulky base into a group of exploitable classes for the extraction of knowledge and decision-making Accordingly the starting point of this solution first level is the extraction of a sample representative of the original data This could be carried out either with one of the traditional techniques of sampling random with equal or unequal probabilities or with the APC method through identification of the relevant attributes Nevertheless a sophisticated method was presented in 
ZRL96 It allows a considerable reduction of the initial base by the generation of a compact summary in an tree diagram BIRCH algorithm Balanced Iterative Reducing and Clustering using Hierarchies This is why we opted for it as the core of the first level of our classification approach thus ignoring the sampling procedures and/or the APC The following step second level is the extension of the methods and usual algorithms of automatic classification not supervised at outings of the first level Indeed it was a 
question on the one hand of adapting the approaches of Dated Mining and of choosing its optimal approach acting not to apply them to the data but to the summary generated by BIRCH in the form of graph In addition we set as secondary objective the generation of a chart of knowledge Knowledge Map made up of the group of association rules drawn at the same time from the original base and summary of algorithm BIRCH 2 Some classification algorithms 0-7803-9521-2/06/$20.00 2472006 IEEE 


a state of the art on classification algorithms can be found in HKAOO  and  BBE02  We point out the principal methods here 2.1 KOHONEN Map It is about a not supervised  training network and still competitive in way that one not only learns how to model the space of the entries by prototypes but also to build a chart with one or two dimensions allowing to structure this space These charts are characterized by a topological property called conservation of topology making it possible to guarantee that similar data within the meaning of a certain criterion are projected or visualized on positions close in space to visualization Thus they pursue the double goal of the topographic clustering which consists in determining under relevant groups of the data entity while storing information on the similarity of the clusters According to this topographic process of clustering the data input displayable in vector form with dimensions are brought back to classes which are self-organized according to a one-dimensional string or a two-dimensional structure chart square rectangular hexagonal or even irregular of nodes on which the vicinity relations are set in advance 2.2 KMEANS Given a corpus of data all belonging to the same Euclidean space The average center algorithm or KMEANS allows partitioning these data in K classes K fixed in advance Each class is represented by a point in space commonly called prototype The name of this algorithm comes owing to the fact that during its unfolding each prototype is the center of gravity of the data which it gathers in the corresponding class 2.3 The decision trees The decision trees are composed of a hierarchical structure in form of tree This structure is built thanks to methods of training per induction starting from examples The tree thus obtained represents a function which makes the classification of examples while being based on the knowledge induced starting from a base of training Because of that they are also called graphs of induction Induction Decision graphs A little more formal definition of the decision trees is as follows a decision tree is a directed graph without cycles whose nodes carry a question the arcs of answers and the leaves of conclusions or final classes  Among the methods of training based on the most known decision trees we can find ID3 algorithm QUI75 and CART method BF084 These systems are rather similar the principal difference between these two systems lies in the choice of measurement used for the selection of the attributes during the construction of the tree This measurement is generally founded on the Shannon information theory entropy and profit of information used in ID3 Method ID3 is at the origin of several other systems One of most known is the C4.5 system developed more recently by Quinlan QUI93 2.4 Association Rules Introduced by AGRAWAL and Al AIS93 the method of extraction of the association rules was proposed initially to allow the analysis of the sales of supermarkets in order to extract rules of type If x Then y As HEBRAIL HEB02 points it out even if it were developed in a marketing goal the method can be used in any other fields if the structure of the data allows it An association rule is an implication of the form x  ywherex c I y c Iand xGn y   with the set i j2 i..r of m elements called items and the unit D  lt t2 o tn of n elements called transactions A rule thus comprises a premise part either antecedent or condition made up of a unit of items2 and a conclusion part either consequent or result made up of a unit of y items disjoint of x The relevance of a rule is measured by its support s and its confidence c There are two visions to express the support and confidence From the probabilistic point of view each subset of items associates the event according to which the transaction contains the items of this subset The support is thus expressed by the probability of simultaneously carrying out events X and Y s sup\(X  Y P\(X n Y P\(YX X Where X is the event the transaction contains all the items of the set x  and Y is the event the transaction contains all the items of the set Y Confidence is equal to the probability of realization of the event Y knowing that event Xis carried out c  conf\(X  Y  P\(X   sup\(X  Y sup\(X The higher the support is the more frequent is the rule The more confidence is raised the less there are counterexamples to the rule 1 The possible classes are not known in advance and the examples available as similar to constitute the classes 2 An item is literal corresponding to a value or a set of values for an attribute selected in the data base 0-7803-9521-2/06/$20.00 2472006 IEEE are not labeled C is thus to gather in same a cluster or groups the objects considered 


classification KOHONEN SOM and 2.5 Method BIRCH KMEANS BIRCH Balanced Iterative Reducing and Clustering using Hierarchies is a method of classification conceived by T ZHANG and Al in 1996 ZRL96 It makes it possible to reduce the initial set of data in a set of under-clusters or subclasses presented in a tree diagram CF-tree in order to simplify the problem of classification BIRCH is incremental i.e it needs only one sweeping of the data file It tries to minimize the cost of input/output by organizing the data treated in a structure of tree balanced with a limited size BIRCH addresses mainly to the broad data bases by initially generating a more compact summary which keeps as much information as possible on the distribution of data and then classifying the summary generated instead of the original data base In terms of assigned time of use of the memory quality of classification and stability of the algorithm and by comparing it with other algorithms BIRCH proves to be the best means of classification available which can be adopted at the very broad data bases 3 Application The objective of this article is to come out of a bulky data base with clusters having a semantic relevance maximum homogeneity intra-cluster and heterogeneity inter-clusters maximum this in an automatic way proceeding by two levels and getting supported by traditional techniques of Mining Dated Indeed the traditional approach of classification generates a certain insufficiency in the case of a high number of data We hope to succeed on the one hand in highlighting quite homogeneous groups in an automatic way and on the other hand in confirming the contribution of the Bilevel character of classification and finally in generating a Knowledge Map by extraction of association rules among the various variables of the data base The flow chart of the following page makes it possible to trace the various stages of our step Two approaches were primarily implemented  Traditional approach of classification Application of KOHONEN SOM method and KMEANS to the original base e In our approach we consider the following two levels First level Generation of a compact summary of the initial base the selected method is BIRCH Second level Classification of the summary generated at the time of the first level call of methods of We also generated decision trees of the original base and that produced by algorithm BIRCH We carried out then the extraction of association rules of the type If conditions THEN consequences to reach at the end a Knowledge Map advantageous for decision-making Figure 1 displays the two approaches Figure 1 Our approach vs Classic approach 3.1 Description of the data For our research we considered a data base Spam The Concept Spam is very diverse it can relate to advertisements of marketing on pornographic products the data of our file include a set of N  4601 individuals described by p  58 characteristics or attributes of which 57 are continuous and nominal The majority of the attributes indicate if a word or a particular character is frequently met in the mail Discrete variable Spam splits up the set of data in two categories  mails Spam or commercial  mails not Spam 3.2 Traditional approach of classification Classif ication of the algorithm of KOHONEN to the original base the application of the algorithm of KOHONEN on the original data made it possible to build a string length of 8 0-7803-9521-2/06/$20.00 2472006 IEEE 


 the clusterization proved of low quality It did not exceed 0.753  The clusters obtained were characterized by  Quite unbalanced cardinals of 1664 to 54  close semantics for some cluster 6 and cluster 7 for example  an error of classification of about 5,2 compared to target classification 1813 Spam and 2788 not Spam was detected following the separation of mails Spam and not Spam  classification required a little long execution time 350 ms Application of the method of the KMEANS to the original base We applied the method of KMEANS by fixing in advance the number of clusters at 8 To this sake we use the module Clustering K-Means of the TANAGRA4 software  the quality of clusterization proved to be higher than that produced by the algorithm of KOHONEN 0.86 0.75 previously obtained against with KOHONEN  the cardinals of the classes were very unbalanced from 5 to 2732 and the execution of the algorithm much slower than that of the algorithm of KOHONEN 642 ms against 350 ms  semantic relating to the different clusters were almost identical to those of the clusters produced by KOHONEN Generation of a decision tree starting from the original base Considering that the methods of generation of decision trees ID3 and C4.5 treat only variables with discrete values we rebuilt our data base by discretizing each of its variables This operation was made by division of the set of the observations in ten equal intervals The results so obtained are introduced like entries of application KSW 5 to produce a tree of 102 nodes which required an execution time of about 1027ms Extraction of rules of association starting from the original base Knowledge Map 3 The closer this value is to the unit the higher is quality 4 It is an open source software with the direction that is possible to any researcher to reach the code and to add its own algorithms IT is conceived by R RACOTOMALALA RAK05 for teaching and research 5 It is about a platform of experimentation conceived singularly for the generation of the decision trees and the rules of association The automatic extraction of the rules of association with the help of the algorithm A priori generated 122 rules whose 45 have a confidence lower than 80 representing 3700 of the total and 47 have a confidence equal to 100 representing 38.5 The required Time for extraction of this Knowledge Map was 845 ms We quote here some rules as examples 1 If CF_  10 CRL TOTAL  9 10 6 7 or 8 WF_MEETING  10 then SPAM  0 c  100.0 2 If CF_  8 CRL_TOTAL  5 6 or 7 WF HP  4 WF EDU  5 or 9 WF 1999  5 WF RECEIVE  5 WF LAB  5 WF_85 5 ou 9 WF_WILL 9 or 10 ALORS SPAM 0 c 50.0\260O 3 If CF_ 3 WF REMOVE  5 CF_ 4 or 7 WF_FREE 4 WF_FONT  5 WF_MONEY  5 or 9 WF GEOUGE  5 WF HP  4 WF 650  5 ou 9 CRL LONGES 3 1 2 or 4 WF INTERNE  5 WF RECEIVE 10 then SPAM  0 c  50.00/O 4 If CF_ 3 WF REMOVE  5 CF   4 or 7 WF_FREE 4 WF_FONT  5 WF_MONEY  5 or 9 WF GEORGE 5 WF HP  4 WF 650  5 or 9 CRL LONGEST 3 1 2 or 4 WF INTERNET  5 WF RECEIVE  5 or 9 WF CREDIT  5 WF YOU 10,7 8 5 or3 WF RE 4 SPAM 0 c 87.8 5 If CF_ 3 WF REMOVE  5 CF_ 4 or 7 WF FREE 4 WF_FONT  5 WF_MONEY  5 or9 WF GEORGE  5 WF HP  4 WF_650  5 or 9 CRL LONGEST  3 1 2 or 4 WF INTERNET 5 WF RECEIVE  5 or9 WF CREDIT  5 WF YOU 10 7 8 5 ou 3 then SPAM  0 c 100.0%0 It is significant to notice that several of the extracted rules are not of an interest for the user rules 2 and 3 for example besides we note the existence of rules which are more specific than others without improving quality of predictions rules 4 and 5 for example 3.3 Our approach To avoid the problems arising from the high number of data an approach Two-Level is employed in order to improve the performances First level Application of BIRCH algorithm to the original data base For this level we opted for BIRCH algorithm for the generation of a compact summary of the original data base For this purpose we carried out an implementation of BIRCH in C the application of 0-7803-9521-2/06/$20.00 2472006 IEEE 


this algorithm on the totality of the data allowed to generate a compact summary in the form of CF-tree comprising 1510 node-leaves or subclasses that is to say a reduction of 67,2 of the size of the initial base The maximum size of these subclasses was fixed a priori at 6 The outlook of the tree CF obtained is represented in the shape of a file XML cftree.xml which expresses very well the hierarchical structure of CF-tree that one obtains at the end of the tree construction The produced subclasses of the construction of the tree are given in a textual file clusters.txt These subclasses are only the node-leaves of the CF tree defined by  the number of elements which they contain the maximum is fixed a priori at 6  the linear sum of the data which they contain vector of dimension 57  the sum with square of the components Second level Application of the algorithm of KOHONEN on the summary generated by BIRCH Following the introduction of algorithm BIRCH into the procedure of classification and in comparison with the results of the application of KOHONEN on the rough basis we notice that  the quality of the clusterization increased by 74,900 to 80,7  the time execution was decreased 350 ms to 140 ms that is to say a reduction of 60  the allocated memory was reduced of 1088 KB to 414 KB that is to say a profit of62  an error rate of 1,10 only Thus the introduction of the character Two-Level into the procedure of classification made it possible to improve quality in terms of memory capacity and execution time allocated while producing clusters whose semantic ones seem strongly with those relating to the clusters generated by the traditional approach Application of the algorithm of KMEANS on the summary generated by BIRCH The quality of classification noticed following the classification of the data by the algorithm KMEANS 0.94 the execution time necessary 230 ms against 642 ms for the application of KMEANS to the original base and the error rate 1.7 against 3.400 following the application of KOHONEN to the original base still prove the contribution of our approach compared to the traditional approach The same procedure of discretization of the attributes is applied for the summary of BIRCH The number of nodes obtained is much lower 48 instead of 102 nodes The execution is much faster 796 ms against 1027 ms Extraction of rules of association starting from summary BIRCH Knowledge Map The application of the algorithm A priori to the base formed by the subclasses generated by BIRCH made it possible to extract 64 rules from association among which 16 have a confidence lower than 80 representing 25.0 of the total and 27 have a confidence of 100 or 42.2 The execution time was about 463 ms against 845 ms This still proves the primacy of our approach compared to the traditional approach Some rules are given here as examples 1 if LS_TELNET  5 LS LONGEST  1 LS OUR  3 or 5 LS MEETING  5 or 10 LS RECEIVE  4 8 or 9 LS RE  3 7 5 or 6 LS_LAB  10 or 9 LS-GEORGE  3 then SPAM  1 c  100.0 2 if LS_TELNET  5 LS LONGEST  2 LS YOU 1 or 10 LS TECHNOLOGY  4 10 or8 LS 1999 10 LS GEORGE  10 then SPAM  0 c  100.0%0 3 if LS_TELNET  5 LS LONGEST  3 LS RE  3 9 10 8 7 or5 LS PARTS  5 LS HPL  4 LS_3D  4 9 or 8 LS BUSINESS  9 LS-GEORGE  9 then SPAM 0 c  100.0 Thus and whatever the algorithm chosen at the second level SOM of KOHONEN KMEANS ID3 or A priori our Two-Level approach made it possible to improve quality of classification of the data of study So the application of the traditional techniques of classification on a sample in our case the summary generated by BIRCH extracted the base makes it possible to extend the knowledge produced for this sample to all the data of the original base while guaranteeing results of good quality even in the case of a base with high size The table which follows recapitulates the characteristics of various methods applied on the one hand to the original base and on the other hand with the summary of algorithm BIRCH Generation of a decision tree starting from the summary produced by BIRCH 0-7803-9521-2/06/$20.00 2472006 IEEE 


SRME;r DE L'ALGO RI t E BIRCH SONI delKOHONEN 019 140 T E;NtEANS  0.94 T T ioTi  Qualite L Temps d exicutian  sI T 8~~~~~~~ASE ODlIGINElblE SOM le KOHONEN  i|t I KIEANS I.2 642  D3a 102 nrjeudL 021 1  g R A doi 3t5 e30 TIL D I 43n eud 796I o31t une1 conf  30 Apri ci  2 R A doni W on unLec cnf s45 R 463I Table 1 Characteristics of the methods applied to the original base and the summary of algorithm BIRCH 4 Conclusion The major challenge of this work is to face the deficiencies met by classifying bulky data bases by the way the complexity of calculation the slowness of execution the relevance of the class generated To answer these problems we proposed an approach of classification Two-Level able to transform a high number of data into a unit of exploitable classes for a reliable decision-making extraction of knowledge and better The first level of this approach is centered on the algorithm BIRCH which allows a considerable reduction of the original base by the generation of a compact summary in a tree diagram CF-tree The second level is the application of the usual methods of classification on the summary of BIRCH algorithm thus passing not imply to the formation of representative classes but also to the generation of Knowledge Map composed of the list of the most relevant rules of association In addition and in order to show the effectiveness of our Bi-level approach in comparison with the traditional approach of classification we carried out an experimentation of two approaches on a basis of data Spam Results produced following the application of algorithms SOM of KOHONEN and KMEANS on the original basis on the one hand and the summary of algorithm BIRCH on the other hand as well as Knowledge Map generated on these two bases proved well the superiority of our Two-Level approach compared to the traditional approach of classification 5 Prospects A Two-level approach for classification for the bulky data bases has been proposed by this article However with an aim of an even higher optimizing approach and of extending it over other problems of classification certain axes of thought deserve to be considered Without claiming to be exhaustive here are some  In case of bases storing a large number of data million which handles many variables thousands the proposed Two-level approach could lead to good results Or perhaps would it be necessary to consider more than one level  Up to which level of exactitude the results extracted from a sample of a whole of data could be extended to the whole base  That an algorithm arrives at good result is a good thing still it is necessary that it does it in a reasonable time An algorithm can thus require many resources memories time disk space to achieve a result while another c better conceived one would do it in a more effective way The comparison of the effectiveness of the algorithms is made thanks to the notation large 0 O meaning about or notation of Landon which describes the asymptotic behavior of the mathematical functions and by extension allows to indicate the speed with which it or its descriptive curve increases or decreases Thus by bringing an algorithm closer to one of the notations large 0 one is measurement to directly compare its amplitude with that of another algorithm and thus to see which is the most adapted for work to achieve or the framework in which it is carried out 6 References AIS93 R AGRAWAL T IMIELINSKI and A SWAMI Mining associations between sets of items in large databases ACM SIGMOD Conference on management of data Washington DC USA pp 207 216 1993 BBE02 P BALEZ and M BEAL Algorithmes de Data Mining Proceedings of the Eighteenth National Conference on Artificial Intelligence pp.114 127 2002 BF084 L BREIMAN J FRIEDMAN R OLSHEN A STONE Classification and Regression Trees Belmont CA Wadsworth 1984 HEB02 G HEBRAIL Introduction a la recherche de regles d'associations et de sequences frequentes Bases de donnees et Statistique Dunod Paris France Chap 8 2002 HKAOO J HAN et M KAMBER Data Mining concepts and techniques Morgan Kaufmann Publishers 2000 KOH97 T KOHONEN Exploration of very large databases by self-organizing maps Proceedings of the 0-7803-9521-2/06/$20.00 2472006 IEEE 


IEEE International Conference on Neural Networks Houston Texas USA pp 1-6 1997 KOH95 T KOHONEN Self-Organizing Maps Springer Series in Information Sciences  Springer Berlin Heidelberg New York 1995 QUI93 J QUINLAN C4.5 Programs for Machine Learning Morgan Kaufmann Publishers San Mateo U.S.A 1993 QUI75 J QUINLAN  Machine Learning IJCAI Vol 1 pp 363-369 1975 RAK05 R RAKOTOMALALA TANAGRA  un logiciel gratuit pour l'enseignement et la recherche in Actes de EGC'2005 RNTI-E-3 vol 2 pp.697-702 2005 ZHA00 C ZHANG S ZHANG Association rules mining models and algorithms SpringerVerlag Publishers in Lecture Notes on Computer science Volume 2307 p 243 2000 ZRAOO D ZIGHED et R RAKOTOMALALA Graphes d'induction et Data Mining Hermes Paris 2000 ZRL96 T ZHANG R RAMAKRISHNAN and M LIVNY Birch A new data clustering algorithm and its applications ACM SIGMOD Vol 25 pp 103 114 1996 0-7803-9521-2/06/$20.00 2472006 IEEE 


15] N. Davies, K. Cheverst, K. Mitchell, and A. Efrat  Using and determining location in a context-sensitive tour guide  ZEEE Computer, vol. 34, issue 8, pp.35-41 Aug. 2001 pre></body></html 


 The required delivery date is a Range constraint any date within the next 30 days Attribute Required-Delivery-Date  today today+30 days Attribute S&H query\(UPS Product   say it is 59.95 Attribute Value  query\(Catalog Product  Attribute Price  Attribute Total  Inter-attribute constraints in PO14 Price  Quantity  Value  1-x Total  Price  S&H  1.088 Total  Total1  Total4 In this case the Quantity attribute value has changed to 2 by adding both requests together Furthermore the Delivery-Date attribute value is a result of finding a common range of the two The obvious saving in this case is 2*$39.95 59.95\1.088  21.71 Whether a bunch of POs should be aggregated in a particular way depend on whether costing savings can be achieved while satisfying all the constraints 6.2 Intelligent Aggregation of Purchase Orders in e-Procurement with Negotiations Aggregation under dynamic negotiation is harder because supplier side could be revising its own strategies and parameters on the fly While human intervention in the aggregation process is possible we focus on automated aspects of the aggregation in this paper Suppose we have a simple supplier side rule buy one and get second one half price from LT a supplier of mice keyboard and trackball Suppose we have requests to buy Mice as follows PO5 Attribute Buyer  Organization 223B\224 User 223Joe\224 Location 223PS\224 Attribute Supplier  mpany 223LT\224 Catalog  http://\205/LT Attribute Product  223Optical Mouse\224 Attribute Quantity 1 Attribute Required-Delivery-Date  01/21/05 02/21/05   a r an g e of dat e s  order dat e  deadline d Attribute S&H9  query\(UPS Product   say it\222s 4.95 Attribute Value  query\(Catalog Product  Attribute Price10  Value  say it\222s 29.95 Attribute Total10  Total10  Price10  S&H10  1 tax rate results in a value 29.95  4.95\1.088  39.97 PO6 Attribute Buyer  Organization 223C\224 User 223Al\224 Location 223PS\224 Attribute Supplier  mpany 223LT\224 Catalog  http://\205/LT Attribute Product  223Optical Mouse\224 Attribute Quantity 1 Attribute Required-Delivery-Date  01/25/05 02/28/05   a r an g e of dat e s  order dat e  deadline d Attribute S&H10  query\(UPS Product   say it\222s 4.95 Attribute Value  query\(Catalog Product sayit\222s 29.95 per mouse Attribute Price10 Value Attribute Total10  Total10  Price10  S&H10  1 tax rate results in a value 29.95  4.95\1.088  39.97 6.2.1 PO Aggregation Under Negotiation The rule-based aggregation engine uses the Negotiation service to understand supplier\222s offers and tries to take advantage of the terms in the offers For example by aggregating PO5 and PO6 can be aggregated as follows PO56 Attribute Buyer  Organization B C User 223Joe\224 223Al\224 Location 223PS\224 Attribute Supplier  mpany 223LT\224 Catalog  http://\205/LT Attribute Product  223Optical Mouse\224 Attribute Quantity 2 Attribute Required-Delivery-Date  01/25/05 02/21/01 Attribute S&H10  query\(UPS Product   say it\222s 4.95 Attribute Value  query\(Catalog Product sayit\222s 29.95 per mouse Attribute Price10 1.5*Value Attribute Total10  Total10  Price10  S&H10  tax  1.5 29.95  4.95\1.088  54.26 A saving of 39.97  2 54.26  25.68 or over 32 of savings Note the changes of the 223Quantity\224 and 223RequiredDelivery-Date\224 attributes after aggregation The quantities are added up and the required delivered dates are merged for a common range Due to the constraints on object attributes aggregation may require complex constraint solving Proceedings of the 2005 Ninth IEEE International ED OC Enterprise Computing Conference \(EDOC\22205 0-7695-2441-9/05 $20.00 \251 2005  IEEE 


7 Conclusions and Future Work This paper describes an Intelligent Aggregation facility in enterprise e-Procurement process This facility introduces an information model a rule-based aggregation engine corporate agreement policies and negotiation in aggregating large volume of POs in enterprise eprocurement to reduce cost and maximize efficiency This information model includes extensive use of constraints for and among attributes in a PO These constraints guard the integrity of POs as they are aggregated The intelligent aggregation facility can be inserted as a value-added service in the enterprise e-Procurement workflow An enterprise generates millions of POs every year but the number of distinct products and services the enterprise purchases is actually much smaller in the hundreds rather than in the millions This presents cost saving opportunities by aggregating POs that makes best use of terms and conditions in corporate agreements or supplier offers Some concrete examples are used to show the idea of automated aggregation and the opportunities in reducing procurement cost As millions of POs are generated even a small percentage of savings would mean substantial savings for large enterprises The ideas described in this paper have not been fully implemented in our prototype One area needs more work is the formal representation of policies in corporate agreements which would allow the aggregation engine to automatically explore aggregation opportunities before POs are made to suppliers Another is the semantic model of products which would enable more semantics-based aggregation of POs 8 References  e bX M L  h t t p   w w w ebxml  org  2 e n g  J  S u  S  Y  W  L a m H   a n dH e l a l S   223Achieving Dynamic Inter-Organizational Workflow Management by Integrating Business Processes Events and Rules,\224 Proceedings of the 35th Hawaii International Conference on System Sciences HICSS35 Hawaii USA January 2002 3 u  S  Y  W  L a m H   L e e  M  B a i S   a n dS h e n  Z   An Information Infrastructure and E-services for Supporting Internet-based Scalable E-business Enterprises Proceedings of the 5th International Enterprise Distributed Object Computing Conference Seattle Washington USA September 2001 4 S u S Y  W   H ua ng C  H a mme r J   H u a ng Y   L i  H   Wang,L.,LiuY.,Pluempitiwiriyawej,C.,Lee,M and Lam H 223An Internet-based Negotiation Server for E-commerce,\224 VLDB Journal Vol 10 No 1 2001 pp.72-90 5 M o r r i s S l o m a n  223 P o l i c y D ri v e n M an ag em e n t f o r Distributed Systems\224 Journal of Network and Systems Management Plenum Press Vol 2 No 4 1994  M aarten S teen  J oh n D errick  223 For m ali s ing ODP Enterprise Policies\224 Proceedings of the 3 rd International nterprise Distributed Object Computing Conference Mannheim Germany IEEE CS Press September 1999  J am e s H a ns on  Z oran M i l o s e v i c 223 C o n v e r s at i onOriented Protocols for Contract negotiations\224 Proceedings of the 7th International Enterprise Distributed Object Computing Conference Brisbane Australia IEEE CS Press September 2003  S  N eal J  C ole P.F L i n i ng ton  Z  Milose v i c S Gibson S Kulkarni 223Identifying Requirements for Business Contract Language a Monitoring Perspective\224 Proceedings of the 7th International Enterprise Distributed Object Computing Conference Brisbane Australia IEEE CS Press September 2003 9 T  D im itrak o s  I  D j o rd j e v i c Z  Milo sev i c A  J o san g  C Phillips 223Contract Performance Assessment for Secure and Dynamic Virtual Collaborations\224 Proceedings of the 7th International Enterprise Distributed Object Computing Conference Brisbane Australia IEEE CS Press September 2003 Proceedings of the 2005 Ninth IEEE International ED OC Enterprise Computing Conference \(EDOC\22205 0-7695-2441-9/05 $20.00 \251 2005  IEEE 


absolute values. The results can vary on other computers. But it can be guaranteed that performance ratio of the algorithms will remain the same After making the comparisons with sample data, we came to the conclusion that PD algorithm performs significantly better than the other two especially with larger datasets. PD outperforms DCP and PIP regarding running time. On the other hand, since PD reduces the dataset, mining time does not necessary increase as the number of transactions increases and experiments reveals that PD has better scalability than DCP and PIP. So, PD has the ability to handle the large data mine in practical field like market basket analysis and medical report documents mining 5. References 1] R. Agrawal and R. Srikant, "Fast algoritlnns for mining association rules", VLDB'94, pp. 487-499 2] R. J. Bayardo, "Efficiently mining long patterns from databases", SIGMOD'98, pp.85-93 3] J. Pei, J. Han, and R. Mao, "CLOSET: An Efficient Algorithm for Mining Frequent Closed Itemsets \(PDF Proc. 2000 ACM-SIGMOD International Workshop on Data Mining and Knowledge Discovery, Dallas, TX, May 2000 4] Qinghua Zou, Henry Chiu, Wesley Chu, David Johnson, "Using Pattern Decomposition\( PD Finding All Frequent Patterns in Large Datasets", Computer Science Department University of California - Los Angeles 5] J. Han, J. Pei, and Y. Yin, "Mining Frequent Patterns without Candidate Generation \(PDF  SIGMOD International Con! on Management of Data SIGMOD'OOj, Dallas, TX, May 2000 6] S. Orlando, P. Palmerini, and R. Perego, "The DCP algoritlnn for Frequent Set Counting", Technical Report CS2001-7, Dip. di Informatica, Universita di Venezia 2001.Available at http://www.dsi.unive.itl?orlando/TR017.pdf 7] MD. Mamun-Or-Rashid, MD.Rezaul Karim, "Predictive item pruning FP-tree algoritlnn", The Dhaka University  Journal of Science, VOL. 52, NO. 1, October,2003, pp. 3946 8] Park, J. S., Chen, M.-S., and Yu, P. S, "An Effective Hash Based Algoritlnn for Mining Association Rules", Proc ofthe 1995 ACM-SIGMOD Con! on Management of Data 175-186 9] Brin, S., Motwani, R., Ullman, J., and Tsur, S, "Dynamic Itemset Counting and Implication Rules for Market Basket Data", In Proc. of the 1997 ACM-SIGMOD Conf On Management of Data, 255-264 10] Zaki, M. J., Parthasarathy, S., Ogihara, M., and Li, W New Algoritlnns for Fast Discovery of Association Rules In Proc. of the Third Int'l Con! on Knowledge Discovery in Databases and Data Mining, 283-286 11] Lin, D.-I and Kedem, Z. M., "Pincer-Search: A New Algoritlnn for Discovering the Maximum Frequent Set", In Proc. of the Sixth European Conf on Extending DatabaseTechnology, 1998 12] R. Ramakrishnan, Database Management Systems University of Wisconsin, Madison, WI, USA; International Edition 1998 pre></body></html 


tors such as union, di?erence and intersection are de?ned for pairs of classes of the same pattern type Renaming. Similarly to the relational context, we consider a renaming operator ? that takes a class and a renaming function and changes the names of the pattern attributes according to the speci?ed function Projection. The projection operator allows one to reduce the structure and the measures of the input patterns by projecting out some components. The new expression is obtained by projecting the formula de?ning the expression over the remaining attributes [12 Note that no projection is de?ned over the data source since in this case the structure and the measures would have to be recomputed Let c be a class of pattern type pt. Let ls be a non empty list of attributes appearing in pt.Structure and lm a list of attributes appearing in pt.Measure. Then the projection operator is de?ned as follows ls,lm c id s m f p ? c, p = \(pid, s, d,m, f In the previous de?nition, id ing new pids for patterns, ?mlm\(m projection of the measure component and ?sls\(s ned as follows: \(i s usual relational projection; \(ii sls\(s and removing the rest from set elements. The last component ?ls?lm\(f computed in certain cases, when the theory over which the formula is constructed admits projection. This happens for example for the polynomial constraint theory 12 Selection. The selection operator allows one to select the patterns belonging to one class that satisfy a certain predicate, involving any possible pattern component, chosen among the ones presented in Section 5.1.1 Let c be a class of pattern type pt. Let pr be a predicate. Then, the selection operator is de?ned as follows pr\(c p Join. The join operation provides a way to combine patterns belonging to two di?erent classes according to a join predicate and a composition function speci?ed by the user Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE Let c1 and c2 be two classes over two pattern types pt1 and pt2. A join predicate F is any predicate de?ned over a component of patterns in c1 and a component of patterns in c2. A composition function c pattern types pt1 and pt2 is a 4-tuple of functions c cStructureSchema, cDataSchema, cMeasureSchema, cFormula one for each pattern component. For example, function cStructureSchema takes as input two structure values of the right type and returns a new structure value, for a possible new pattern type, generated by the join. Functions for the other pattern components are similarly de?ned. Given two patterns p1 = \(pid1, s1, d1,m1, f1 p2 = \(pid2, s2, d2,m2, f2 p1, p2 ned as the pattern p with the following components Structure : cStructureSchema\(s1, s2 Data : cDataSchema\(d1, d2 Measure : cMeasureSchema\(m1,m2 Formula : cformula\(f1, f2 The join of c1 and c2 with respect to the join predicate F and the composition function c, denoted by c 1   F  c  c 2   i s  n o w  d e  n e d  a s  f o l l o w s    F  c  c 2     c  p 1   p 2   p 1    c 1  p 2    c 2  F   p 1   p 2     t r u e   5.1.3. Cross-over database operators OCD Drill-Through. The drill-through operator allows one to 


Drill-Through. The drill-through operator allows one to navigate from the pattern layer to the raw data layer Thus it takes as input a pattern class and it returns a raw data set. More formally, let c be a class of pattern type pt and let d be an instance of the data schema ds of pt. Then, the drill-through operator is denoted by c c Data-covering. Given a pattern p and a dataset D sometimes it is important to determine whether the pattern represents it or not. In other words, we wish to determine the subset S of D represented by p \(p can also be selected by some query the formula as a query on the dataset. Let p be a pattern, possibly selected by using query language operators, and D a dataset with schema \(a1, ..., an ible with the source schema of p. The data-covering operator, denoted by ?d\(p,D responding to all tuples in D represented by p. More formally d\(p,D t.a1, ..., t.an In the previous expression, t.ai denotes a speci?c component of tuple t belonging to D and p.formula\(t.a1, ..., t.an instantiated by replacing each variable corresponding to a pattern data component with values of the considered tuple t Note that, since the drill-though operator uses the intermediate mapping and the data covering operator uses the formula, the covering ?\(p,D D = ?\(p not be equal to D. This is due to the approximating nature of the pattern formula 5.1.4. Cross-over pattern base operators OCP Pattern-covering. Sometimes it can be useful to have an operator that, given a class of patterns and a dataset, returns all patterns in the class representing that dataset \(a sort of inverse data-covering operation Let c be a pattern class and D a dataset with schema a1, ..., an pattern type. The pattern-covering operator, denoted as ?p\(c,D all patterns in c representing D. More formally p\(c,D t.a1, ..., t.an true Note that: ?p\(c,D p,D 6. Related Work Although signi?cant e?ort has been invested in extending database models to deal with patterns, no coherent approach has been proposed and convincingly implemented for a generic model There exist several standardization e?orts for modeling patterns, like the Predictive Model Markup Language \(PMML  eling approach, the ISO SQL/MM standard [2], which is SQL-based, and the Common Warehouse Model CWM  ing e?ort. Also, the Java Data Mining API \(JDMAPI 3] addresses the need for a language-based management of patterns. Although these approaches try to represent a wide range of data mining result, the theoretical background of these frameworks is not clear. Most importantly, though, they do not provide a generic model capable of handling arbitrary cases of pattern types; on the contrary only a given list of prede?ned pattern types is supported To our knowledge, research has not dealt with the issue of pattern management per se, but, at best, with peripheral proximate problems. For example, the paper by Ganti et. al. [9] deals with the measurement 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


