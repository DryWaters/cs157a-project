Context-Aware Object Connection Discovery in Large Graphs James Cheng 1 YipingKe 002 2  Wilfred Ng 3  Jeffrey Xu Yu 002 4  Hong Kong University of Science and Technology 1 csjames@cse.ust.hk 3 wilfred@cse.ust.hk 002 Chinese University of Hong Kong 2 ypke@se.cuhk.edu.hk 4 yu@se.cuhk.edu.hk Abstract  Given a large graph and a set of objects the task of object connection discovery is to 036nd a subgraph that retains the best connection between the objects Object connection discovery is useful to many important applications such as discovering the connection between different terrorist groups for counter-terrorism operations Existing work considers only the connection between individual objects however in many real problems the objects usually have a context e.g a terrorist belongs to a terrorist group We identify the context for the nodes in a large graph We partition the graph into a set of communities based on the concept of modularity  where each community becomes naturally the context of the nodes within the community By considering the context we also signi\036cantly improve the ef\036ciency of object connection discovery since we break down the big graph into much smaller communities We 036rst compute the best intra-community connection by maximizing the amount of information 037ow in the answer graph Then we extend the connection to the inter-community level by utilizing the community hierarchy relation where the quality of the inter-community connection is also ensured by modularity Our experiments show that our algorithm is three orders of magnitude faster than the state-of-the-art algorithm while the quality of the answer graph is comparable I I NTRODUCTION Graph is a powerful modeling tool for representing and understanding objects and their relationships Many efﬁcient indexes 1   2    3  and a l gori t h ms for t he di s c o v e ry of correlations 4 5  a nd frequent s ubgraphs 6 7  8  h a v e been proposed for graph databases that consist of a set of small graphs Such graph databases are popular in scientiﬁc domains such as chemistry and bio-informatics However we also observe that another type of graphs which are standalone large graphs exists prevalently in a wide spectrum of application domains Typical examples of such graphs are various social networks in which we want to nd the relationship between people within the network Such information is very useful in applications such as corporate cooperation and management law enforcement national security counterterrorism and many others Other examples of such graphs include resource environment economics ecological networks the Web and many more However efﬁcient knowledge discovery techniques for such graphs are still inadequate Let G be a graph in which nodes are uniquely labeled and edges are undirected and weighted Given a set of objects a typical task of knowledge discovery is to nd the  best  connection among the objects in G  We name such a task as object connection discovery Weuse objects and nodes interchangeably in this paper and name the set of objects under investigation as query nodes  Object connection discovery is important to a wide spectrum of applications For example the relationship between different business people and organizations and how they do business together are important for creating new opportunities of corporate cooperations and for designing more effective internal management strategies the connection between different terrorists and terrorist leaders/groups and how terrorists plan and conduct terrorist activitie s together are useful clues for counter-terrorism and national security operations how various social networks behave is valuable knowledge for understanding the networks so that appropriate actions may be taken It can also be applied to other domains such as resource environment economics gene regulatory networks viral marketing the Internet and the WWW and many more However object connection discovery in a large graph poses signiﬁcant challenges First the graph may contain millions or even billions of nodes and edges therefore discovering the best connection is difﬁcult especially when the query nodes are scattered widely apart from each other Existing work on nding a connection between a set of query nodes 9 10  11 i s ei t h er not ef  c i e nt enough or focus e s o n t he query nodes that are relatively close to each other Apart from computational complexity it is also difﬁcult to deﬁne the  bestness  of a connection It has been shown in 9 that shortest paths maximum ow and other graph distance measures are inadequate in cap turing the relation between query nodes Existing work measures the bestness by the concepts of electrical circuits 9 a nd random walks with restart 10  H o w e v er  t hei r s t udi es focus o n c omput i n g t he connection from query nodes to some important nodes which may neglect the good connection from query nodes to query nodes We address the problem of object connection discovery by rst observing the semantics of the objects in a large graph Since a large graph can contain millions of nodes it is extremely rare that they all belong to the same category or the 
IEEE International Conference on Data Engineering 1084-4627/09 $25.00 © 2009 IEEE DOI 10.1109/ICDE.2009.87 856 
IEEE International Conference on Data Engineering 1084-4627/09 $25.00 © 2009 IEEE DOI 10.1109/ICDE.2009.87 856 
 


same context  In many real-life problems we usually assign objects to a context and we are interested in the relationship between objects in the same cont ext as well as that between contexts For example we identify terrorists by their groups and we want to nd the internal connection within a group as well as the external connection between the groups We propose context-aware object connection discovery  which rst discovers the context for the nodes in a large graph and then discovers thei r best connection accordingly The connection is rst computed for nodes within the same context and then extended between the contexts We illustrate the idea further by the following example Example 1 Suppose we want to nd the connection between  Jim Gray  Jennifer Widom  Michael I Jordan  Geoffrey E Hinton  on the DBLP co-authorship graph Our method rst identiﬁes that the four sc holars are from two different contexts as shown by the two boxes in Figure 1 Gray and Widom are from the database community while Jordan and Hinton are from the machine learning community Then we compute the best connection b etween the scholars within each context After that we compute the connection between the two contexts with respect to the four scholars 002 Michael I Jordan Jim Gray Alexander Aiken 1 12 Jennifer Widom Joseph M Hellerstein 3 Tommi Jaakkola Lawrence K. Saul 3 Zoubin Ghahramani 9 Geoffrey E. Hinton 1 2 2 5 11 2 1 9 1 1 1 3 Stefano Ceri Fig 1 Context-Aware Connection for  Jim Gray  Jennifer Widom  Michael I Jordan  Geoffrey E Hinton  Our rst task is to identify the context for the objects We adopt the concept of modularity 12 t o cl us t e r t he nodes i n a large graph into a set of communities  Modularity is used in the area of community detection to measure the quality of the communities discovered in networks Thus using modularity we are able to decompose a large graph into a set of highquality communities which can be naturally used as the context of the nodes In addition we are also able to retain the relationship between different communities by constructing a community hierarchy tree  By considering the context of the objects our algorithm adopts a partition-and-conquer approach First we partition a large graph into a set of small communities Then we discover the best intra-community connection which is far more efﬁcient since a community is signiﬁcantly smaller than the original graph Finally we extend the connection to the inter-community level which is also efﬁcient by utilizing the community hierarchy tree To compute the best intra-community connection we introduce the concept of information throughput within a community Information throughput is deﬁned based on the concept of electrical circuits or equivalently random walks  Given a pair of nodes s and t  source and target  information throughput of a node i in the community is the total amount of currents owing through i from s to t  Therefore the information throughput of node i measures the importance of node i in connecting s and t  The information throughput of node i indicates the global importance of node i in the community with respect to the information ow from s to t  However an answer graph is a subgraph that displays only partial information of the original graph thus we also compute the amount of information ow in a s tot path through node i  which is regarded as the local importance of the s tot path in the answer graph We propose a goodness function by integrating the global importance of the nodes and the local importance of the s tot paths in the answer graph Then we devise an efﬁcient dynamic programming algorithm to compute the best intracommunity connection that maximizes the goodness function When some query nodes are in different communities we need to nd the inter-community connection  We rst utilize the community hierarchy tre e to nd the connection between the communities to which the query nodes belong Then we compute the actual path that connects the query nodes by following the connection between the communities Since the community hierarchy tree re ects the relationship between the communities as deﬁned by modularity the quality of the inter-community connection is also ensured As a result our algorithm not only achieves high efﬁciency through the partition-and-conquer approach  but also guarantees the quality of the answer at both the intraand intercommunity levels Finally our experimental results show that we are able to nd a set of high-quality communities and our community partition algorithm is efﬁcient For connection discovery we compare with the center-piece subgraph  CEPS 10  O u r results show that our method obtains comparable high-quality answers as CEPS but is over three orders of magnitude faster and also consumes signiﬁcantly less memory The rest of the paper is organized as follows Section II reviews related work Section III briefs our solution framework Section IV discusses community partition Section V discusses intra-community connection Section VI discusses inter-community connection Section VII reports the experimental results Section VIII concludes the paper II R ELATED W ORK There are several studies on nding a good connection for some given query nodes in a large graph Faloutsos et al 9 p ropos e t o  nd t h e connection subgraph for two query nodes They model a graph as an electrical network and use the delivered current from one query node to the other as a goodness measure for the answer graph Later Tong and Faloutsos 10 p ropos e t he center-piece subgraph  CEPS  to deal with multiple query nodes They deﬁne a goodness criterion based on random walks with restart  RWR  Their algorithm iteratively picks up a most promising destination node based on RWR and then nds the path to connect each query node to this destination node Consequently 
857 
857 
 


their answer graph may miss some good connection that does not rst direct to a chosen destination node but directly goes from query nodes to query nodes Koren et al 11 al s o s t udy t h e p robl em of  ndi ng s ubgraphs that connect multiple query nodes However their objective is different from that of the previous work They aim to extract the subgraph that retains most of the proximity between query nodes in the original graph Therefore their algorithm focuses on query nodes that are relatively close to each other Our work is also related to community detection Among the studies on community detection there are only a few that handle large networks Newman 13 p ropos es an al gori t h m that requires O   E    V    V   timeforagraph G  V E   Around the same period Wu and Huberman 14 propos e a method that adopts the circuit model to discover communities in O   E    V   time However their algorithm requires to input the number of communities and some seed nodes that are in different communities which restricts the application of their work In our work we improve Newman’s algorithm 13 to detect communities in 002  V  log  V   and O   V  2  time In addition to the above-mentioned work community detection has also been extensively studied in the context of Web Web communities 15   1 6    1 7    1 8    1 9  a r e u s u a lly ch ar acterized by dense bipartite subgraphs based on the assumption that a topically focused community in the Web tends to contain a dense bipartite subgraph Th e methods developed for Web communities may not be applicable to our problem since a vertex in the Web graph may not be covered by any extracted Web communities III S OLUTION F RAMEWORK In this section we present the framework of our solution which consists of the following two phases The rst phase is community partition  which partitions the input graph into a set of communities  as well as constructs the community hierarchy tree  This phase is done ofﬂine We discuss community partition in Section IV The second phase is object connection discovery whichis done online whenever a user issues a query Our algorithm PCquery  P artition-andC onquer query processing processes a query i.e nds the best connection between the query nodes in two main steps The rst step computes the intracommunity connection for query nodes that fall within the same community Then the second step of PCquery computes the inter-community connection between the communities of the query nodes with the aid of the community hierarchy tree The intraand intercommunity connections are nally integrated to give the nal answer graph We discuss in details the intraand intercommunity connection in Sections V and VI respectively IV C OMMUNITY P ARTITION We discuss an effective partitioning scheme that not only decomposes a graph into a set of high-quality communities  but also retains the relationship between the communities A Modularity Our partitioning scheme is based on the concept of modularity  which measures the quality of the communities discovered in networks Modularity was proposed by Newman and Girvan 12 for c ommuni t y det ect i on W e appl y i t t o t he probl em of object connection discovery on a large graph as an initial step to discover the contexts of the objects For our purpose we re-formulate modularity in our problem as follows Modularity is a quality function that tests whether a particular community partition of a graph is a good partition A community partition  P ofagraph G is a partition  C 1 C 2 C n  on the set of vertices of G  Each C i is called community i Given P  the edges in G are categorized into two types intra-community edges and inter-community edges  depending on whether the two incident vertices of an edge are in the same community or not We denote the set of intracommunity edges in C i as E ii and the set of inter-community edges connecting C i and C j  i 002  j s E ij  respectively To deﬁne modularity we need the following notations The fraction of intra-community edges in C i is deﬁned as 002 ii   E ii   E   Similarly the fraction of inter-community edges between C i and C j  i 002  j sdeﬁnedas 002 ij  002 ji   E ij  2  E  Note that we have totally 2  E  edges here since we consider an edge to be shared by 002 ij and 002 ji  Finally the fraction of the edges that are incident to at least one vertex in C i i.e both intraand intercommunity edges of C i sdeﬁnedas 003 i  002 j 002 ij  Now we give the deﬁnition of modularity 12  Deﬁnition 1 Modularity The modularity of a graph G with respect to a community partition P is deﬁned as follows M  G  P  003 i  002 ii  003 2 i   1 The semantics of modularity can be interpreted as follows If we pick up edges at random to include them into C i the expected fraction of intra-community edges in C i i.e.,theexpected value of 002 ii is 003 2 i Inotherwords 002 i 003 2 i corresponds to a community partition for med by random chance Since 002 ii represents the true fraction of intra-community edges in C i the subtraction  002 ii  003 2 i  indicates the deviation of the community partition P from a randomized partition  When all the vertices in G form a single community or the set of communities is formed by random chance the value of modularity is 0  The semantics of modularity indicates that the higher the value of M  G  P   the higher the quality of P is Intuitively P is a good partition if m ost of the edges in G are intracommunity edges and only a few are inter-community edges The following example illustra tes the concept of modularity Example 2 Figure 2 shows a graph G where  V  12 and  E  16  Consider a community partition P   C 1 C 2 C 3   where C 1   v 1 v 2 v 3 v 4   C 2   v 5 v 6 v 7 v 8 v 9  and C 3   v 10 v 11 v 12   as shown by the three dotted circles in the gure We have  E 12  1 since there is only one edge  v 4 v 6  between C 1 and C 2 Wealsohave  E 11  5 since there are ve intra-community edges in C 1  By deﬁnition we have 002 11   E 11   E   5 16  002 12   E 12  2  E   1 32 and 002 13   E 13  2  E   1 32  We further have 003 1  002 1 003 j 003 3 002 1 j  5 16  1 32  1 32  3 8  
858 
858 
 


Similarly we can compute 002 22  5 16  002 33  3 16  003 2  3 8  and 003 3  1 4  By Equation 1 we calculate the modularity M  G  P  002 1 003 i 003 3  002 ii  003 2 i  15 32  002 v 1 v 2 v 3 v 4 v 5 v 8 v 9 v 6 v 7 v 10 v 12 v 11 Fig 2 A Graph G  and a Community Partition of G B Computing the Community Partition According to the semantics of modularity the optimal community partition is deﬁned as follows P opt  argmax P M  G  P   2 Equation 2 deﬁnes an optimization problem However to nd the maximal value of modularity we need to try all possible community partitions This is clearly intractable since the total number of possible partitions is the  V  th Bell number which is at least exponential in  V  but G may contain millions of nodes We give an approximate solution to this optimization problem by a greedy algorithm We adopt the same greedy strategy as in 13 20  b u t investigate more properties for efﬁcient computation The basic idea of the greedy algorithm is as follows We start from a trivial initial community partition in which each vertex in G forms a single community At each iteration we rst pick up the pair of communities that achieves the greatest increase in modularity and combine them to form a new community We then compute the new modularity that can be achieved for every pair of communities assuming that they are to be combined in the next iteration This process goes on to form larger communities and the approximate optimal community partition is obtained when the modularity decreases for every possible pairs of communities to be combined The key operation in the greedy algorithm is to compute the new modularity and retrieve the largest one at each iteration It has been shown in 20 t hat t he change of modul ari t y can be updated incrementally We o mit the details of the update and refer readers who are interested to 20 We now discuss how to efﬁciently retrieve the largest change of modularity as well as to save update computations Let P k  1   C 1 C i C j C n  be the community partition after the  k  1 th iteration Let 003 M ij k be the change of modularity if C i and C j are to be combined at the k th iteration To efﬁciently retrieve the largest 003 M ij k  we need two max-heaps First we use a local max-heap for each community C i to keep 003 M ij k forevery C j that has connection with C i  Then we further use a global max-heap to keep all the local maximum 003 M ij k  so that we can pick C i and C j that have the largest 003 M ij k to combine at the k th iteration At the k th iteration we rst use the global max-heap to pick C i and C j whichtakes O 1 time Then we need O   C i    C j   time to combine C i and C j into C new We also need O   C new   time to rebuild the local max-heap for C new  Then for each C x connecting to C i or C j  we need O log  C x   time to update the local max-heap of C x  Thus in total we need O   C new  log  C x   O   V  log  V   time to update all C x  Finally we update the global max-heap with the local maximums from the max-heaps of C new and of each C x  which requires O   C new  log P k    O   V  log  V   time Since initially each vertex in G forms a single community at most we have  V  iterations Therefore the algorithm requires O   V  2 log  V   time However this time complexity is still too high in practice for large graphs We further improve the efﬁciency based on the following two heuristics Heuristic 1 Let c local be a small constant  Then the rst c local elements in the local maxheap of a community remain relatively stable during the entire process of community partition Heuristic 1 states that the rst few largest elements in the local max-heap of a community remain to be the rst few largest elements most of the time even the max-heap may be updated frequently during the process of community partition This is true because a commun ity tends to combine with only a few of its connecting communities based on modularity rather than to have an even probability of combining with each of its connecting communities Based on Heuristic 1 we can limit the size of the local maxheap of a community to c local  In this way after combining C i and C j at the k th iteration we can update the local max-heap of each C x in constant time In the worst case when an element that is not one of the rst c local largest elements becomes one of the rst c local largest elements we need O log  C x   time to rst nd the element and then insert it into the local max-heap of C x  However this worst case rarely happens According to our experiment the majority of the updates on the local max-heap of a community during the entire process of community partition are operated on the rst c local elements of the max-heap Heuristic 2 Let c global be a constant  Then the rst c global elements in the global max-heap remain relatively stable during the entire process of community partition Heuristic 2 states that the rst few largest elements in the global max-heap remain to be the rst few largest elements most of the time This is true because the global max-heap keeps the local maximums and according to Heuristic 1 the local maximums remain to be stable By Heuristic 2 we can further reduce the time for each update of the global max-heap from O log P k   to constant time Now for each iteration we still need O   C i    C j   time to combine C i and C j into C new  We need O  c local  O 1 time to rebuild the local max-heap for C new  We need O  c local  C new   O   C new   time to update the local maxheaps of all C x connecting to C i or C j  Finally we need O  c global  C new   O   C new   time to update the global maxheap As a result the total running time for each iteration is now O   C new    
859 
859 
 


Let C  P k  be the new community formed at the k th iteration i.e C new in the above discussion The overall running time is 002  V  k 1 O   C  P k     In the worst case at each iteration we combine C with a community that has a single vertex until C contains the set of all vertices in G  thus 002  V  k 1 O   C  P k   2+3    V   O   V  2   In the best case in terms of time complexity each iteration picks the smallest two communities to combine then 002  V  k 1 O   C  P k    O   V  log  V    Therefore we obtain the running time of the algorithm as 002  V  log  V   and O   V  2   Finally the space requirement of the algorithm is O   V    E    since for each community we only need to keep the communities that connect to it whic h is similar to the structure of the adjacency list representation of a graph C Community Hierarchy Tree In addition to the purpose of breaking a large graph into small communities the process of community partition by the greedy algorithm also builds a community hierarchy tree for computing the inter-community connection to be discussed in Section VI We show an exampl e of a community hierarchy tree H  in Figure 3 v 1 v 2 v 4 v 3 v 5 v 6 v 7 v 9 v 8 v 10 v 12 v 11 Virtual Community Actual Community Intermediate Community Vertex Level 0 1 2 3 4 5 6 7 8 9 10 11 C 1 C 2 C 3 C 4 C 5 C 6 C 7 C 8 C 9 Fig 3 Community Hierarchy Tree H of G The set of nodes at Level 0 of H corresponds to the initial community partition in which each vertex of G forms a community Then each level k  003 k 0  has only one node v  which corresponds to the community formed by combining the two child nodes of v at the k th iteration We categorize the nodes which represent communities in H into three types as follows  Actual communities  which are the optimal communities approximated by the greedy algorithm  Virtual communities  which are communities that are ancestors of the actual communities in H   Intermediate communities  which are communities that are descendants of the actual communities In order to create the community hierarchy tree we run the greedy algorithm until all vertices in G are combined into a single community Thus the set of actual communities are the community partition that is obtained when the modularity reaches its peak after this poi nt the modularity starts to decrease while the set of virtual communities is all communities that are formed after the peak We discuss how each of the three types of communities is used to facilitate object connection discovery in G First,the set of actual communities partitions G into small and highquality components so that we can compute the connection on each component efﬁciently Second the set of virtual comm unities keeps the relationship among the actual communities Each virtual community C keeps a set of community edges  A community edge of C is an edge  C i C j   where both C i and C j are actual communities that are descendants of C  such that C i and C j are connected in G  i.e there is at least one edge between C i and C j in G  To avoid  C i C j  being kept duplicately at all the common ancestors of C i and C j  we keep  C i C j  at an ancestor C if and only if C i is in the left subtree of C and C j is in the right subtree of C  The community edges are essential for computing the inter-community connection between the query nodes that are in different actual communities Each  C i C j  indicates that there exists at least one edge  v i v j  in G where v i 004 C i and v j 004 C j Since  v i v j  connects C i and C j in G  we call  v i v j  a connecting edge  We also associate the set of connecting edges with  C i C j   The following example illustrates the various concepts discussed above Example 3 In Figure 3 we have two virtual communities C 4 and C 5  The set of community edges kept at C 4 is   C 1 C 2   since there is only one actual community at either side of C 4 as its descendants The set of connecting edges associated with  C 1 C 2  is   v 3 v 11   since C 1 and C 2 are connected by the edge  v 3 v 11  in Figure 2 The set of community edges kept at C 5 is   C 1 C 3    C 2 C 3   Theset of connecting edges associated with  C 1 C 3  is   v 4 v 6   and that with  C 2 C 3  is   v 10 v 8    002 We do not use intermediate communities in this paper however intermediate communities are useful when the user wants to know more about each query node itself in addition to the relationship between query nodes This is particularly necessary when a query node is in an isolated component in which case we want to return some useful information to the user rather than the query node itself For this purpose we can deﬁne groups as intermediate communities since small groups present the most relevant  local  information to a query node The community hierarchy tree provides the exibility of allowing the user to specify the group size as well as the fast retrieval of the groups V I NTRA C OMMUNITY C ONNECTION After decomposing the large graph G into a set of much smaller communities we can now process the query nodes that fall within each community to obtain the context-aware connection In this section we rst propose the notions of information throughput and information ow to measure the importance of a node and a path in connecting the query nodes within the community Then we integrate the two 
860 
860 
 


concepts to deﬁne a goodness function by which we measure the quality of the intra-community connection Finally we propose a dynamic programming algorithm to compute the best connection between the query nodes within a community A Information Throughput and Information Flow Let G be the induced graph of a community C in G and W   w ij  be the adjacent matrix of G  We can model G as an electrical circuit  where each edge in G is a resistor whose conductance is the edge weight Our circuit model is source-target dependent that is given a source node s and a target node t  we inject one unit of current into the circuit at s and extract one unit at t Let V st i be the voltage of node i in the circuit and I st i be the current owing through node i  Since the current going into a node is equal to that going out of the node by Kirchhoff’s law of current conservation  I st i is deﬁned as half of the absolute amount of all the currents owing through node i  The concept of information throughput is then formally deﬁned as follows Deﬁnition 2 Information Throughput Given a connected graph G the information throughput of a node i with respect to a pair of source-target nodes  s t  is deﬁned as follows T st i  I st i  004 1 2 002 j w ij  V st i  V st j   if i 002  s t 1  otherwise 3 Intuitively T st i indicates the importance of node i in connecting s and t  In order to compute T st i  we only need to obtain the voltage values by the following equation  W D W  V  I C  4 where W D is a diagonal matrix with entries w D ii  002 j w ij  V   V st i  is the voltage vector of all nodes and I C   I C i  is the cumulative current vector of all nodes  I C s 1  I C t   1 and I C i 0 for i 002  s t  Information throughput T st i measures the total amount of information ow from s to t through i in G  Since there may be many paths from s to t passing through i wealsowant to know the exact amount of information ow through each path Let p st  005 s  i 1 i 2 i n  1 t  i n 006 be a simple path from s to t Wedeﬁnethe information ow of p st as follows T  p st  T  005 s  i 1 i 2 i n  1 t  i n 006   n  1 005 k 1 w i k i k 1  V st i k  V st i k 1  I st i k  5 Note that information throughput T st i is equal to the sum of the information ow of all the paths passing through i  We illustrate the concepts of information throughput and information ow by the following example Figure 4 shows a weighted gr aph where the weight of each edge is represented by the associated integer Assume that s and t are two query nodes The information throughput of each node with respect to s and t is given as the value below each node in Figure 4 and the information ow of each path isgiveninTableI s v 5 v 7 v 4 v 6 t v 1 v 2 v 3 v 10 v 11 v 12 0.19 0.18 0.19 0.18 10 0.13 0.13 0.05 0.18 0.19 1 1 0.58 10 7 78 7 5 5 3 7 7 5 2 3 2 5 3 v 8 0.06 v 13 0.06 5 3 5 v 9 0.06 v 14 0.06 5 5 5 5 Fig 4 An Example Graph TABLE I I NFORMATION F LOW AND V ALUE OF P AT H S I N F IGURE 4 Path p st T  p st  val  p st  p 1  002 s v 4 t 003 0.46 0.267 p 2  002 s v 5 v 6 v 7 t 003 0.18 0.032 p 3  002 s v 1 v 4 t 003 0.04 0.014 p 4  002 s v 4 v 3 t 003 0.04 0.014 p 5  002 s v 1 v 4 v 3 t 003 0.04 0.011 p 6  002 s v 10 v 11 v 12 t 003 0.03 0.006 p 7  002 s v 1 v 2 v 3 t 003 0.05 0.005 Any one of the remaining 8 paths 0.02 0.003 Example 4 Node v 4 has the highest information throughput since it directly connects s and t with the highest weight 10  Moreover v 4 also serves as a bridge to connect v 1 and v 3  which forms several paths from s to t  Therefore v 4 plays a very important role in connecting s and t and it is well indicated by the high value of its information ow The nodes v 10  v 11 and v 12 are also of high information throughput since they form a number of s tot paths However the number of these paths is large and the paths are relatively long which weakens the signiﬁcance of these nodes in connecting s and t  Although this point is not indicated by information throughput of the nodes it is captured by information ow of the paths As shown in Table I all the paths formed by these nodes are of low information ow 002 Given a community C  we only need to compute the corresponding matrix  W D W   1 once Afterwards regardless of which query nodes are asked the information throughput of all nodes and the information ow of all paths with respect to any pair of query nodes can be computed by Equations 3-5 Computing  W D W   1 takes O   C  3  timeforacommunity C  However a community C is signiﬁcantly smaller than the original graph G and hence computing the matrix inversion on C is much more efﬁcient than on G Sincewe only need to compute  W D W   1 once and then use it to answer any queries it is also reasonable to pre-compute it ofﬂine Afterwards the time complexity of computing the vectors of voltage and information throughput with respect to any pair of source and target is O   C  2   which can be processed efﬁciently for online querying since C is not large B The Goodness Function We now deﬁne the goodness function by which we determine which nodes and paths should be used to connect 
861 
861 
 


the query nodes within a community Then we show how to compute the best connection between query nodes that maximizes the goodness function Let Q be a set of query nodes in a community C We compute a connected subgraph G Q  V Q E Q  such that Q 007 V Q 007 C and G Q maximizes a goodness function g  We rst deﬁne g based on information throughput as follows g  G Q  003 s,t 004 Q 003 i 004 V Q T st i  6 Equation 6 indicates the total degree of importance of all the nodes in G Q  However according to Equation 6 g  G Q  is maximum when we include all nodes of C into G Q Toavoid obtaining this trivial answer graph we set a budget b such that our objective is now to maximize g  G Q  with  V Q 010 b The budget is commonly used to control the size of the answer graph for clear visualization 9   10  Although the information throughput of a node indicates its importance in connecting s and t  simply summing up the information throughput of the nodes as in Equation 6 has a problem Given a node i in G Q  T st i actually represents the total amount of information ow from s to t through i in C  Thus T st i may be high only because there are many paths from s to t going through node i  We illustrate this problem of Equation 6 by Example 5 Example 5 Consider the graph in Figure 4 Based on Equation 6 if we set the budget b 6 nodes then the best connection from s to t is shown in Figure 5 The problem is obvious since the path 005 s v 10 v 11 v 12 t 006 has very low information ow of only 0.03 The three nodes v 10  v 11 and v 12 are chosen only because there are many paths passing through them while all these paths have very low information ow and thus only show weak connection from s to t  002 s v 10 v 12 v 11 t v 4 10 10 3 73 7 Fig 5 Answer Graph for Example 5 To address the problem of Equation 6 we attempt to deﬁne the goodness function based on an s tot path directly instead of on individual nodes as follows g  G Q  003 s,t 004 Q 003 p st 004 G Q T  p st   7 Equation 7 indicates the total amount of information delivered by all the s tot paths in G Q  However Equation 6 is not totally meaningless in the sense that a node i having a higher information throughput is indeed more important than another node j with a lower information throughput because the total amount of in formation delivered from s to t through node i is indeed higher than that through node j  Thus considering the information ow of a path alone may also miss some important connection as illustrated by the following example s v 5 v 7 v 6 t v 4 10 10 7 78 7 Fig 6 Answer Graph for Example 6 s t v 2 v 1 v 3 v 4 2 2 10 55 10 3 3 Fig 7 Answer Graph for Example 7 Example 6 Consider again the graph in Figure 4 Based on Equation 7 the best connection from s to t is shown in Figure 6 which are the two paths that have the highest information ow However there are three paths 005 s v 1 v 4 t 006  005 s v 4 v 3 t 006 and 005 s v 1 v 4 v 3 t 006  all passing through the most important node v 4 in the s tot connection The overall information ow of these paths together with the path 005 s v 1 v 2 v 3 t 006  is only 0.01 lower than the path 005 s v 5 v 6 v 7 t 006  Thus if we consider both the strong connection of these paths with the important node v 4 and the comparable overall information ow then Figure 7 gives a better answer graph than Figure 6 More importantly the combination of the paths contributes to a network of signiﬁcant connection from s to t  which is far more informative than a single path 005 s v 5 v 6 v 7 t 006  002 To address the problems of Equations 6 and 7 we take the merits of both information throughput and information ow by integrating them to deﬁne a new goodness function as follows val  p st  006 T  p st   if p st  005 s t 006 T  p st  002 i 002 p st i 003  s,t T st i  p st  2  otherwise g  G Q  003 s,t 004 Q 003 p st 004 G Q val  p st   8 Equation 8 rst deﬁnes the value of a path p st  denoted as val  p st   based on which the goodness function of an answer graph is further deﬁned According to Equation 8 g  G Q  is high when both the information ow of the s tot paths and the average information throughput of the nodes on the paths are high We can interpret Equation 8 as follows T  p st  in Equation 8 quantiﬁes the local information ow in the answer graph while  002 i 004 p st i 005  s,t T st i     p st  2 governs the choice of a node i to be included in the answer graph according to its global importance in the community with respect to s and t  Example 7 Based on Equation 8 the best connection from s to t is shown in Figure 7 The value of each path p st is shown in Table I The goodness score of the answer graph in Figure 7 is 0.311 while those of Figure 5 and Figure 6 are 0.273 and 0.299 respectively 002 C Computing the Optimal Intra-Community Connection Computing the best connection for query nodes within a community can be deﬁned as an optimization problem as 
862 
862 
 


follows G opt Q  argmax G Q g  G Q   9 To solve this optimization problem we rst attempt to apply a dynamic programming DP solution similar to the 0-1 knapsack problem by setting the budget b as the total capacity of the knapsack the set of  p st  paths as the set of items  p st  as the item weight and val  p st  as the item value Let PathSet be the set of  p st  paths and  PathSet  be the number of paths in PathSet  The knapsack problem can be solved in O   PathSet  b  time Unlike the knapsack problem b here is small since b is deﬁned as the size of a graph that a human user is able to visualize clearly However there are still two problems that need to be solved First we need to obtain PathSet  Second the nodes on the paths may overlap while the items in the knapsack problem do not share their weights We address each of the problems as follows 1 Path Selection Clearly we cannot use the set of all  p st  paths within a community because the number o f such paths is too large We select a set of high-value paths by the following strategy We start a breath-ﬁrst-search  BFS  from s to collect a set of  p st  paths The BFS does not visit all nodes but only explores a node j from a node i if  V st i  V st j   0 sincewe require val  p st   0  To control the number of paths for each s t pair we stop a new iteration of BFS when the number of distinct nodes on the collected paths is greater than b  Example 8 Consider the graph in Figure 4 If b 6 theset of s tot paths is selected as follows First the second iteration of BFS selects 005 s v 4 t 006  Then the third iteration of BFS selects 005 s v 1 v 4 t 006 and 005 s v 4 v 3 t 006  The fourth iteration of BFS selects 005 s v 1 v 2 v 3 t 006  005 s v 1 v 4 v 3 t 006  005 s v 5 v 6 v 7 t 006  and 005 s v 10 v 11 v 12 t 006  Now the number of distinct nodes is greater than b  thus we stop the BFS path selection and none of the other paths in Figure 4 are selected 002 The BFS strategy has the following three advantages First it controls the length of the paths because we do not prefer long paths due to the limited budget b  Second the paths collected by BFS tend to have common nodes thereby giving a higher val  p st  within a xed b  Third the BFS strategy also collects high-value paths because T  p st  decreases for each node added to the path as reﬂected by Equation 5 2 Dynamic Programming After we select the set of  p st paths PathSet  we can construct a   PathSet  b  table for the DP solution We rst sort PathSet in descending order of the values of the paths to facilitate the DP process since we prefer high-value paths to low-value ones However the fact that paths may share common nodes signiﬁcantly complicates the problem We address this problem as follows Let Tab be the DP table and p x be the x thpathin PathSet  In the 0-1 knapsack setting Tab  x y  gives the optimal solution for the sub-problem with x paths and the capacity y where Tab  x y  is given as follows Tab  x y  MAX  Tab  x  1 y   val  p x  Tab  x  1 y  p x     10 Equation 10 however has a problem If some nodes on p x also appear on some paths processed previously then it is no longer correct to use Tab  x  1 y  p x   to compute Tab  x y   Let PathSet 1   x  1 be the set of paths in PathSet that are processed by DP before we process p x Wemayhave z nodes on p x that are also on some paths in PathSet 1   x  1 where 0 010 z 010 p x   Therefore we need to check Tab  x  1 y    p x  z  for 0 010 z 010 p x  inorder to compute Tab  x y   However we cannot simply take the maximum Tab  x  1 y    p x  z  for some z sincewe need to make sure that there are indeed z nodes being repeated on the set of paths from which Tab  x  1 y    p x  z  is computed Let PathTab  x  1 y    p x  z  be the set of paths from which Tab  x  1 y    p x  z  is computed We compute Tab  x y  as follows Tab  x y  MAX  Tab  x  1 y   val  p x  MAX  Tab  x  1  y    p x  z    11 where 0 010 z 010 p x  and there are at least z nodes on p x that also appear on some paths in PathTab  x  1 y    p x  z   We give the pseudo-code of the DP algorithm in Algorithm 1 The algorithm is self-explanatory except the handling of the common nodes on different paths In Lines 8-17 we nd the number of nodes on p x that also appear on some paths in PathTab  x  1 y    p x  z  as follows Algorithm 1 IntraConnection Input PathSet  b  Output G Q  1 Sort PathSet in descending order of the path values 2 Create Tab with   PathSet  1 rows and  b 1 columns 3 Initialize the rst row Row 0 of Tab to be all 0’s 4 for each x 1   PathSet   do 5 for each y 0 b  do 6 Tab  x y  004 Tab  x  1 y   7 PathTab  x y  004 PathTab  x  1 y   8 Let p x be the x thpathin PathSet  9 for each z 0   p x   do 10 z 003 004 0  11 for each node v on p x do 12 if  PathSet  v  005 PathTab  x  1 y    p x  z  006  007  13 z 003 004 z 003 1  14 break if z 003 010 z  15 if  z 003 010 z and Tab  x y    val  p x  Tab  x  1 y    p x  z    16 Tab  x y  004  val  p x  Tab  x  1 y    p x  z    17 PathTab  x y  004  PathTab  x  1 y    p x  z  011 p x    18 Combine the paths in PathTab   PathSet  b  to give G Q  Let PathSet  v  be the set of paths in PathSet that contain a node v  For each v on p x  we intersect PathSet  v  with PathTab  x  1 y    p x  z   If the intersection is not an empty set then v is repeated on some path in PathTab  x  1 y    p x  z  Weuse z 006 to keep the total number of nodes being repeated If there are z 006 011 z nodes on p x that also appear on some paths in PathTab  x  1 y    p x  z   we update Tab  x y  according to Equation 11 
863 
863 
 


q 3 q 2 q 1 q 3 q 2 q 1 a\Graph c\aph 2 q 3 q 2 q 1 b\Graph 1 Fig 8 Answer Graph for Example 10 Finally G Q is computed by combining the set of path in PathTab   PathSet  b   We show how the algorithm works by the following example Example 9 Consider the graph in Figure 4 and two query nodes s and t Weset b 6 and after sorted PathSet   p 1 p 2 p 3 p 4 p 5 p 6 p 7  as given in Table I Table II gives the value of each Tab  x y  and the set of paths in each PathTab  x y   To save space we omit Columns 0-2 and Row 0 which are all 0’s Recall that PathTab  x y   p i p j  can be regarded as a subgraph consisting of the set of paths  p i p j  where the number of distinct nodes in the subgraph is y  Thus we can construct PathTab 3     p 1 p 3  from PathTab 2     p 1   since all the nodes on p 3  except v 1  appear already on p 1 InRow5 PathTab 4     p 1 p 2  was replaced by PathTab 5     p 1 p 3 p 4 p 5  since Tab 5   is now greater than Tab 4   ForRow6,since p 6 consists of three new nodes and hence we need to update Tab 6   from Tab 5   However  Tab 5    0  006  Tab 5    thus p 6 is not added and Row 6 remains the same as Row 5 For Row 7 there is only one new node on p 7  thus we add p 7 to PathTab 6   to give PathTab 7     p 1 p 3 p 4 p 5 p 7   which is the nal answer 002 TABLE II V ALUES OF Tab AND PathTab IN E XAMPLE 9 3 4 5 6 1 0.267  p 1  0.267  p 1  0.267  p 1  0.267  p 1  2 0.267  p 1  0.267  p 1  0.267  p 1  0.299  p 1 p 2  3 0.267  p 1  0.281  p 1 p 3  0.281  p 1 p 3  0.299  p 1 p 2  4 0.267  p 1  0.281  p 1 p 3  0.295  p 1 p 3 p 4  0.299  p 1 p 2  5 0.267  p 1  0.281  p 1 p 3  0.306  p 1 p 3 p 4 p 5  0.306  p 1 p 3 p 4 p 5  6 0.267  p 1  0.281  p 1 p 3  0.306  p 1 p 3 p 4 p 5  0.306  p 1 p 3 p 4 p 5  7 0.267  p 1  0.281  p 1 p 3  0.306  p 1 p 3 p 4 p 5  0.311  p 1 p 3 p 4 p 5 p 7  Example 9 shows how to nd the answer graph of only two query nodes The following example further illustrates the case when the number of query nodes is more than two Example 10 Figure 8\(a shows a graph with three query nodes Assume that all paths have the same value If we set b  9  we can either output Figure 8\(b or Figure 8\(c Figure 8\(b shows strong pair-wise relationship between any pair of query nodes while Figure 8\(c shows strong relationship among all query nodes and pair-wise connection is included only because the budget still allows Our algorithm gives Figure 8\(c which reveals another advantage of our method that it values strong relationship among all or the majority of query nodes higher than pair-wise relationship 002 We now analyze the time complexity To compute each Tab  x y   we need to check  p x  number of  Tab  x  1 y    p x  z   entries For each entry we need to perform z intersections of PathSet  v  and PathTab  x  1 y    p x  z   Thus we need O   p x  2  PathSet   time to compute each Tab  x y   Note that we can terminate the intersection as soon as we nd one path in both sets Thus in many cases the intersection terminates earlier In total we need to compute b  PathSet  table entries and hence the total running time is O  b  p x  2  PathSet  2  In practice this running time is very small since  p x    PathSet  and b are all small The space required for PathSet and Tab is O  b  PathSet    We also keep PathTab  x  1 y    p x  z  with each Tab  x  1 y    p x  z   but only for the  x  1 th row and x th row of Tab  Thus the extra space needed is at most 2 b  PathSet    In most cases  PathTab  x  1 y    p x  z   is much smaller than  PathSet   However there is a potential problem in the DP solution that G Q may not be connected or may not contain all query nodes since the DP only picks up paths according to their values Since we select paths for all s t pairs 003 s t 004 Q we simply make G Q connected with all query nodes by adding the corresponding path\(s with the highest value Thus this process may output slightly more than b nodes but should not affect clear visualization VI I NTER C OMMUNITY C ONNECTION In Section V we discuss the intra-community connection between a set of query nodes that are in the same community In this section we discuss the inter-community connection for a query that contains nodes from different communities We give the algorithm for computing the inter-community connection between the query nodes in Algorithm 2 Algorithm 2 InterConnection Input H and the query Q  Output The answer graph G Q  1 Let C be the set of all communities that contain at least one query node 2 Let A be the common ancestor of all communities in C  3 Let E com be the set of all community edges of each virtual community on the path from each C i 012 C to A  4 Construct a graph G com  from E com  5 for each pair of communities C i and C j in C do 6 Compute the shortest path between C i and C j in G com  7 Let P be the set of all shortest paths obtained in Steps 5-6 8 Sort P in ascending order of the path length 9 for each path 002 C i C j 003\012 P do 10 if  C i and C j are not yet connected in G Q  11 Find the actual path between C i and C j andadditto G Q  12 if All query nodes in Q are connected 13 Return G Q  We describe Algorithm 2 as follows In Section IV-C we construct the community hierarchy tree H that shows the relationship between the communities For example if two communities are siblings in H  then they are the closest to each other since they are to be joined as one single 
864 
864 
 


community as measured by m odularity Algorithm 2 utilizes this relationship between two communities C i and C j to rst nd the connection between C i and C j at the community level Lines 2-6 then we nd the actual path in G that connects the query nodes in C i to those in C j Line 11 We rst discuss how we nd the connection between C i and C j at the community level Let p  005 C i C j 006 be the simple path that connects C i and C j in H i.e the path from C i to A and that from C j to A in Lines 2-3 of Algorithm 2 Since all nodes on p  except C i and C j  are virtual communities we need to rst convert p into a path of actual communities Recall from Section IV-C that at each virtual community in H  we keep a set of community edges Let E com be the union of the set of community edges at each virtual community on p  From E com  we can construct a graph G com  Note that G com must contain at least one path of actual communities from C i to C j  because C i and C j are reachable to each other in G  Thus the path that connects C i and C j at the community level can be computed as the shortest path from C i to C j in G com  It has been shown in  t h at s hort e s t pat h i s i n adequat e in capturing the relation between query nodes We also do not use shortest path to nd the relation between query nodes within a community However here the context in which we apply shortest path is differen t because the distance between two communities in the community hierarchy tree does show how close they are to each other For this reason the weight of each community edge in G com is deﬁned as the level at which the community edge is kept in the hierarchy tree since the level of the hierarchy tree shows the time the communities are combined The higher the level the greater the edge weight the later are two communities combined and so the further away is their relationship We further illustrate the concept by the following example Example 11 Consider the graph in Figure 2 and the hierarchy tree in Figure 3 For clear illustration let us assume that we obtain the set of actual communities at Level 7 instead of Level 9 that is the set of actual communities is  C 1 C 2 C 6 C 7 C 8   The set of community edges at each virtual community is as follows we have   C 7 C 8   at Level 8   C 6 C 7    C 6 C 8  at Level 9   C 1 C 2   at Level 10 and   C 1 C 6    C 2 C 8   at Level 11 Suppose that the query nodes are in C 1 and C 7  We collect the community edges along the two paths from C 1 and C 7 to their ancestor C 5 and construct G com accordingly as shown in Figure 9 There are four simple paths connecting C 1 and C 7 but the shortest path is 005 C 1 C 6 C 7 006  It can also be seen from Figure 2 that this shortest path can indeed capture the relationship between the two communities 002 C 6 C 8 C 7 C 1 C 2 11 98 9 11 10 Fig 9 G com for Example 11 We now discuss how to nd the actual path to connect two communities C i and C j based on the discovered community path Line 11 of Algorithm 2 Let p  005 C i C x C y C j 006 be the shortest path between two communities C i and C j in G com  We utilize p to nd the actual path connecting the query nodes in C i and C j  A conceptual view of how we apply p is depicted in Figure 10 C i q i u C x v w  C y C j q j Fig 10 A Conceptual View of Inter-Community Connection Recall that each community edge is associated with a set of connecting edges Thus we start from C i and nd the highest-value path from a query node to some u where  u v  is a connecting edge We nd this highest-value path using the BFS strategy in Section V-C but we do not run dynamic programming since we only need to pick one path Then we nd the highest-value path from v to w in C x  This process goes on until we reach C j  where we connect a query node in C j with the highest-value path to a connecting edge from C y  In the same way we nd the actual path from C j to C i because the connection is directi on-aware We then select the path with higher overall value to connect the query nodes in C i and C j in the answer graph The overall connection between C i and C j is controlled by both interand intracommunity concepts As depicted in Figure 10 the quality of th e connection at the community level is ensured by the community hierarchy tree based on modularity When the connection goes inside a community we apply the concepts of information throughput of nodes and information ow of paths to compute the highest-value intra-community path Finally in Algorithm 2 we rst sort the shortest paths in Line 8 and terminate the process when all query nodes are connected in Lines 12-13 This is a greedy algorithm that computes a connected answer graph by selecting the best inter-community path at each step  Since the inter-community connection is at a much coarser l evel than the intra-community connection connecting all query nodes by the greedy strategy sufﬁces though computing the connection between all communities in C is also possible and incurs only little overhead We now analyze the complexity of the inter-community connection First nding the shortest path in G com takes O   E com  log  V com   time and  O   E com    V com   space Since the sets of community edges kept by the virtual communities are disjoint and we only require the virtual communities on the simple path from C i to C j in H   E com  is small and so is  V com   In total we need to nd   C    C  1  2 shortest paths but  C 010|Q and in general a user does not ask a query with many query nodes Computing the actual path from the shortest path p  005 C i C j 006 takes 002 C k 004 p O   C k  2  where O   C k  2  is the time for computing the voltage and information throughput of the nodes in C k  Note that the time taken for the BFS path 
865 
865 
 


selection is dominated by O   C k  2   Finally the space requirement is O  MAX  C k  2    Since the size of a community is small both the time and space are also small VII E XPERIMENTAL R ESULTS We now evaluate the performance of our algorithm for object connection discovery We run all experiments on an AMD Opteron 248 with 1GB RAM running Linux 64-bit We use the DBLP co-authorship dataset modeled as a graph The graph has approximate ly 316K nodes and 1,834K edges where a node represents an author and the edge weight is the number of papers co-authored between two authors A Performance of Community Partition We rst evaluate the performance of community partition by our greedy algorithm We test three settings of c local 10 50 and 100 and four settings of c global 10 100 1000 10000 We also compare with Newman’s algorithm 13 Figure 11\(a shows that our algorithm is about an order of magnitude faster than Newman’s when c local 10 Theresult also shows that when c local increases the efﬁciency decreases which demonstrates the effectiveness of Heuristic 1 10 100 1000 10000 10 2 10 3 10 4 c g lobal Running Time \(sec c local 10 c local 50 c local 100 Newman a Running Time 10 100 1000 10000 0 50 100 150 200 250 300 c g lobal Peak Memory Consumption \(MB c local 10 c local 50 c local 100 Newman b Memory Consumption Fig 11 Performance of Community Partition For the effect of Heuristic 2 i.e c global  the performance is the best when c global  1000 When c local 10  the running time is 682 613 570 and 667 seconds for c global  10 100 1000 and 10000 respectively The result can be explained as follows When c global is too small many items are not kept in the global max-heap and hen ce we need to rebuild the heap more often When c global is too large there are too many items in the heap and hence the update of the heap takes longer Figure 11\(b shows that the peak memory consumption of our algorithm increases when c local increases since the size of the local max-heaps increases when c local increases Increasing c global  i.e the size of the global heap from 10 to 10000 only increases the memory usage for less than 1 MB since we have only one global heap However in all cases our algorithm consumes considerably less memory than Newman’s We also record that the value of the modularity of the optimal community partition obtained by the greedy algorithm is 0.71 note that all the algorithms compute the same partition According to Newman 13 a m odul ari t y v a l u e o f g reat er t h an 0.3 indicates a signiﬁcant comm unity structure Therefore 0.71 is a very high value of modularity and indicates a highquality community partition B Semantics of Answer Graph A Case Study We conduct a case study to compare our answer graph with the center-piece subgraph  CEPS  10 Thi s s t udy ai ms to rst provide a more intuitive view on the answer graphs obtained by our algorithm and CEPS Then we perform a more systematic comparison in the following subsection We use the query  Jim Gray  Jennifer Widom  Michael I Jordan  Geoffrey E Hinton   The four scholars are from two different communities Gray and Widom are from the database community while Jordan and Hinton are from the machine learning community This is clearly captured by our answer graph as shown in Figure 1 which is displayed in Section I Figure 12 shows the answer graph of CEPS which is very similar to our answer graph The similarity is because both our algorithm and CEPS nd nodes that are closely related to the query nodes in order to connect them Thus the result shows that both algorithms are able to capture important nodes and paths related to the query nodes However our method not only nds a good connection between all query nodes but also for query nodes that are in the same context we put more emphasis on their connection than the existing methods Compare Figure 1 with Figure 12 we clearly see a stronger connection between Gray and Widom through both Ceri and Hellerstein in our answer graph than CEPS Michael I Jordan Jim Gray Alexander Aiken 1 3 Jennifer Widom Michael Stonebraker Tommi Jaakkola Lawrence K. Saul 3 Zoubin Ghahramani 9 Geoffrey E. Hinton 7 1 2 5 11 2 1 9 1 1 5 3 Joseph M Hellerstein Fig 12 The Answer Graph of CEPS Although the quality of the answer graphs is comparable our algorithm signiﬁcantly outperforms CEPS we take only 0.33 seconds to compute our answer graph while the computation of the CEPS takes 925 seconds We further compare the two methods using more systematic measures as follows C Performance of Object Connection Discovery We compare the performance of our algorithm PCquery with CEPS 10  W e s et t h e b udget t o b e t w i ce of t h e query size We also verify that the answer graphs obtained by PCquery and CEPS are of roughly the same size Other settings of CEPS are as its default We generate two types of queries in-community queries and random queries  which are abbreviated as cq and rq in the gures For in-community queries the nodes in a query are randomly selected from a randomly selected community For random queries the nodes in a query are randomly selected from the set of all nodes in the dataset We generate 100 queries for each type and test the query size from 2 nodes to 20 nodes Figure 13\(a reports the average running time of nding the connection for a query The result shows that PCquery is more 
866 
866 
 


than three orders of magnitude faster than CEPS for both query types We nd that PCquery takes more time to process an incommunity query than a random query This is because the computation of the intra-community connection by dynamic programming is more costly than that of the inter-community connection by tracing the c ommunity hierarchy tree 2 4 8 12 16 20 10 2 10 0 10 2 10 4 Quer y Size \(number of nodes Average Response Time \(sec PCquery \(cq CEPS \(cq PCquery \(rq CEPS \(rq a Average Response Time 2 4 8 12 16 20 0 100 200 300 400 500 600 700 Quer y Size \(number of nodes Peak Memory Consumption \(MB PCquery \(cq CEPS \(cq PCquery \(rq CEPS \(rq b Memory Consumption Fig 13 Efﬁciency of PCquery and CEPS Figure 13\(b reports the peak memory consumption during the entire running process The result shows that PCquery also consumes signiﬁcantly less memory than CEPS in all cases In addition to the comparison on efﬁciency we also compare the quality of the answer graphs obtained by PCquery and CEPS For the fairness of comparison We use the quality metrics proposed in CEPS 10  NRatio and ERatio which indicate the percentage of important nodes and edges that are captured by an answer graph respectively We report the result in Figure 14 2 4 8 12 16 20 0 20 40 60 80 100 Quer y Size \(number of nodes NRatio PCquery \(cq CEPS \(cq PCquery \(rq CEPS \(rq a Average NRatio 2 4 8 12 16 20 0 20 40 60 80 100 Quer y Size \(number of nodes Eratio PCquery \(cq CEPS \(cq PCquery \(rq CEPS \(rq b Average ERatio Fig 14 Quality of Answer Graph Figure 14 shows that both PCquery and CEPS obtain high-quality answer graphs Both NRatio and ERatio of our answer graphs are comparable to those of CEPS although on average those of CEPS are slightly better Considering our algorithm is three orders of magnitude faster and also consumes signiﬁcantly less memory we can conclude that our method is both efﬁcient and effective VIII C ONCLUSIONS We propose context-aware object connection discovery in a large graph We adopt a partition-and-conquer approach to achieve both high performance efﬁciency and high quality results Our method rst partitions a large graph into a set of communities The concept of community not only naturally deﬁnes the context of the nodes but also signiﬁcantly improves the efﬁciency of connection d iscovery since a community is much smaller than the original graph We compute the connection between query nodes rst at the intra-community level by maximizing the information throughput of the nodes and the information ow of the paths in the answer graph and then at the inter-community level by retaining the close relation between the commun ities as deﬁned by modularity The quality of both the intraand intercommunity connection is thus controlled by the integration of information throughput/ﬂow and modularity We verify by experiments that our community partition algorithm is efﬁcient and the set of communities obtained has high quality We also show that our method obtains comparable high-quality answers as the state-of-the-art algorithm but is more than three orders of magnitude faster and consumes signiﬁcantly less memory Acknowledgement This work is partially supported by RGC GRF under grant number CUHK419008 and HKUST617808 We thank Mr Hanghang Tong and Prof Christos Faloutsos for providing us the source code of CEPS R EFERENCES  X  Y an P  S  Y u and J  H an  Graph i nde xing bas e d o n d is crim inati v e frequent structure analysis ACM TODS  vol 30 pp 960–993 2005  J  C he ng Y  K e  W  N g a n d A  L u  F g-i nde x t o w a rds v e r i  c a t i on-fre e query processing on graph databases in SIGMOD  2007 pp 857–872  P  Z hao J  X Y u  a nd P  S Y u   Graph i nde xing T r ee  d elta   graph in VLDB  2007 pp 938–949 4 Y  K e J  Cheng and W  N g Cor r e lation s ear ch in gr aph d atabas es   in KDD  2007 pp 390–399 5 Y  K e J  Cheng and W  N g E f  cient c or r e lation s ear ch f r o m g r a ph databases To appear in TKDE  2008  A  I nokuchi T  W as hio and H  M ot oda An apriori-based algorithm for mining frequent substruc tures from graph data in PKDD  2000 pp 13–23  X  Y an and J  H an  Clos e g raph m i ni ng closed frequent graph patterns in KDD  2003 pp 286–295  J  H uan W  W a ng J  Prins  and J  Y ang Spin mining maximal frequent subgraphs from graph databases in KDD  2004 pp 581–586 9 C  F alouts o s  K  S  M c Cur l e y  a nd A  T o m k ins  F as t d is co v e r y of connection subgraphs in KDD  2004 pp 118–127  H  T ong and C  F alouts o s  Center piece s ubgraphs  p roblem de n ition and fast solutions in KDD  2006 pp 404–413  Y  K o ren S C North and C  V olins k y  Meas uring a nd e x tracting proximity in networks in KDD  2006 pp 245–255  M E  J  Ne wm an and M  G irv a n Finding and e v a luating c om m unity structure in networks Physical Review E  vol 69 p 066113 2004  M E  J  Ne wm an  F a s t algorithm f or detecting c om m unity s t ructure i n networks Physical Review E  vol 69 p 066133 2004  F  W u and B  A  H uber m an  F i nding com m unities i n linear tim e a physics approach The European Physical Journal B Condensed Matter and Complex Systems  vol 38 no 2 pp 331–338 2004  R K u m a r  P  Ragha v a n S Rajagopalan and A  T om kins   T r a w ling the web for emerging cyber-communities Comput Netw  vol 31 no 11-16 pp 1481–1493 1999  R K u m a r  P  Ragha v a n S Rajagopa lan and A Tomkins Extracting large-scale knowledge bases from the web in VLDB  1999 pp 639 650  R K u m a r  U Mahade v a n and D  S i v akum ar   A g raph-theoretic approach to extract storylines from search results in KDD  2004 pp 216–225  D Gibs on R K u m a r  and A  T om kins   Dis c o v e ring lar g e d ens e subgraphs in massive graphs in VLDB  2005 pp 721–732  Y  Douris boure F  Geraci a nd M Pe llegrini Extraction and classiﬁcation of dense communities in the web in WWW  2007 pp 461–470  A Claus e t M E  J  Ne wm an a nd C Moore Finding com m unity structure in very large networks Physical Review E  vol 70 p 066111 2004 
867 
867 
 


  13 false \(double\argets Figure 12 illustrates this situation. The conditional update correctly updates the tracker wh ich is tasked with following the target. However, the detector which is partially spatially coincident with the tracker also receives energy from the conditional update. This can lead the detector to initiate falsely\second target nearby the first target This effect can be countered a number of ways. First, we can adjust the speed at which the tracker re-centers itself The double initialization phenomenon occurs when the PDF peaks near the edge of the tracker grid. However, this method has the side effect of potentially allowing probability to fall off of the grid in low SNR environments causing track loss. Of course if the SNR is low enough or measurement outages occur tracks will be dropped. Second a guardband around the tracker that does not allow any detector sufficiently near the tracker to receive reinforcement via the conditional density can mitigate the double target problem. However, this has the side effect of preventing detection of closely spaced targets. Third increasing the spatial extent of the tracker has a similar effect as the using a guardband. It does require increased computation, but generates a better representation of the posterior There are several engineering tradeoffs. The first is that large tracker grids \(or large guard bands\ prevent falsely detecting new targets because of conditional probability spill over. However, if applied too aggressively, this will prevent correctly detecting cl osely spaced targets. Second quick tracker grid translation correctly centers the target mass, again preventing spillover into nearby detectors However, overly liberal trac ker repositioning may in fact move trackers to spurious energy locations and drop true targets off of the finite grid On Ambiguous Targets As discussed earlier, ambiguous targets will eventually move non-physically and this will cause the tracker to remove them via its natural prediction and update process Figure 13 illustrates this phenomenon. There are two real targets that create two persistent ambiguities. All four are detected and tracked automati cally. The ambiguous targets however, eventually move non-physically due to their reliance on the node bearing angles. The tracker automatically penalizes the non-physical motion and the targets\222 present hypothesis decrease quickly over time Ambiguous target removal is done automatically in the Bayesian framework as follows The PDF on target state is predicted forward in time according to the kinematic model True targets will have behavior consistent with the kinematic model \(note the kinematic model is a statistical model so it is predicting a range of possibilities for the future target state\biguous targets may behave consistently with this model for a period of time, but eventually they will appear to perform a non-physical maneuver \(these epochs typically come when the ambiguous target crosses a line of symmetry in the sensor\this point, the predicted target position will be in strong disagreement with the inco ming measurements on that target. This mismatch in predicted target position and measurements leads to a decr ease in the target present hypothesis as calculated in eq. \(4\long, only true targets remain   Figure 12 \226 Improper selection of grid resolution leads to multiple initializations on the same target. Left Measurement update of a Tracker \(red=highest likelih ood, blue=lowest\Right Measurement update of a detector which lies near the Tracker.  Since the track er size has been improperly chosen, some energy from the measurements of a single target leak s on to the detector. This can le ad to false double-initializations 


  14 8  C ONCLUSION  This paper has described a Bayesian approach to detecting and tracking multiple moving targets using acoustic data from multiple passive arrays In contrast to traditional undersea acoustic systems, which develop tracks at the single array level and require track association, our approach fuses data at the m easurement level and operates directly in the target state space We have detailed a well known nonlinear filtering approach to single target detection and tracking [1, 4 and desc ri be d our computationally efficient finite-grid approach to the required density estimation. We have furthermore extended this to the multiple target case by employing a bank of single target detector tracke rs and approximation methods that adjust for closely spaced targets. This approximate approach avoids fully treating the computationally complex joint multitarget problem Future work includes modified approaches to posterior estimation including dynamic grid extent, dynamic grid resolution, and particle filtering. It is anticipated that adaptive sampling of the posterior will lead to computational savings. Furthermore, future work includes more detailed modeling and estimation of closely spaced targets allowing a more accurate representation of the joint target density. Naively implemented, this implies exponential growth \(in the number of targets\r the probability state space being es timated. However, recent work in a related tracking domain on adaptive density factorization [5 c h a stic sa m p lin g  p article filtering    pr ovi de m e t h o d s t h at m i t i g at e t h i s com put at i o n gr owt h  when the full joint density is treated  A PPENDIX  This section discusses the details of how the single target probability density is time evolved on a discrete grid. This discussion is similar to that found elsewhere [15, 14, 13 We wish to compute the single target probability density at time      from the density at time     The relation between these two densities can be expressed using the law of total probability as                We expand     using a second order Taylor series as               where  is the vector of partial derivatives, i.e and is the matrix of second order partial derivatives Then the relation of \(23\ approximated as   Figure 13\226 Left: P h1 over time for four targets, two of which are real and two of which are ambiguous. Although the ambiguous intersections are persistent, eventually the false targets ha ve non-physical motion. The target present hypothesis quickly goes to zero for these targets and they are elimin ated. Right: the tracker estimate of target position and red circles indicating the removal point fo r the false targets 


  15             Where denotes the expectation with respect to the transition distribution    and the omitted terms involve similar terms involving and and cross terms between the and coordinates We use the nearly constant velocity \(NCV\model to specify the transition distribution    This assumption corresponds to one where the target moves at constant velocity except for random jump changes \(i.e nearly constant velocity\is is a plausible model when  is small as it is here Specifically, the NCV model assumes step changes in target velocity defined by the Ito Equations     This model implies  and likewise for  It is furthermore assumed that th e noise processes in each coordinate are independent Under this model, we can eval uate the required terms from 25\ as follows          And likewise for terms involving and Notice that all cross terms \(e.g  have expectation due to the assumption that the noise process is independent in the two coordinates This model simplifies \(25\ to         where the terms omitted are replicas involving the  coordinate Under the assumption that is small, this can be rewritten as    For implementation, this is approximated using an implicit Euler scheme wh ere      Where the indices  represent the discrete    locations where the probability mass is captured Likewise, using forward differencing      and          and similarly for the y coordinate system When substituted into \(28\is leads to a series of equations of the form                This series of equations defi ne the probability at each point at time  It can be efficiently solved via Thomas\222 algorithm \(rather than simply inverted\he matrix is tridiagonal     


  16 R EFERENCES    R o y E. Bet h el Benjam i n Shapo, C h r i st opher M   Kreucher, \223PDF Detection and Tracking\224, under review IEEE Transactions on Aerospace and Electronic Systems  2 y. E B eth e l an d G. J. Paras, \223A PDF Mu lt it arg et Tracker\224 IEEE Transactions on Aerospace and Electronic Systems vol. 30, no. 2, pp. 386-403, April 1994 3   R o y E  B e t h e l a n d G  J  P a r a s  223 A P D F M u l t i s e n s o r  Multitarget Tracker\224 IEEE Transactions on Aerospace and Electronic Systems vol. 34, no. 1, pp. 153-168 January 1998  L   D   Stone, C. A. Bar l ow, and T. L Corwin, \223Bayesian  Multiple Target Tracking\224  Boston: Artech House, 1999  l la and A. Hero, \223Multitarget Tracking using the Joint Multitarget Probability Density\224 IEEE Transactions on Aerosp ace and Electronic Systems  vol. 41, no. 4, pp. 1396-1414, October 2005  M  M o relande, C. Kreucher, K. Kastella, \223A Bay e sian  Approach to Multiple Target Detection and Tracking\224 IEEE Transactions on Signal Processing vol. 55, no. 5 pp. 1589-1604, May 2007  B  Shapo, and R  E B e t h el  223An Overvi ew of t h e Probability Density Function \(PDF\er\224 Oceans 2006 Boston, Sept. 2006  R oy L. St r e it 223M ult i s ensor M ul tit arget Int e nsit y Fil t er 224  International Conference on Information Fusion  Cologne, Germany July 2008  M  Ort on and W Fi t z geral d 223A B a y e si an approach t o  tracking multiple targets using sensor arrays and particle filters\224 IEEE Transactions on Signal Processing, vol. 50 no. 2, pages 216-223, Feb 2002  A. Doucet B Vo, C Andri e u, and M Davy 223Par t i c le filtering for multi-target tracking and sensor management\224, IEEE International Conference on Information Fusion, 2002  H Van T r ees, \223Det ecti o n Est i m a t i on, and M odul at i o n  Theory IV:  Optimum Array Processing\224  J. C St ri k w erda, Fi nit e  Di fference Sch e m e s and Partial Differential Equations, Ch apman & Hall, New York 1989   K. Kast el la and C Kreucher, \223M ult i p l e  M odel Nonl i n ear  Filtering for Low Signal Ground Target Applications\224 IEEE Transactions on Aerospace and Electronic Systems vol. 41, no. 2, April 2005, pp. 549-564  Z. Tang and \334. \326zg\374n er, \223Sensor Fu si on for Target Track Maintenance with Multiple UAVs based on Bayesian Filtering Method and Hospitability Map\224 Proceedings of the 42 nd IEEE Conference on Decision and Control pages 19-24, December 2003  K. Kast ell a 223Fi n it e di ff erence m e t hods for no nl i n ear filtering and automatic target recognition\224 MultitargetMultisensor Tracking: Applications and Advances vol III, pages 233-258, Artech House, 2000 B IOGRAPHY  Chris Kreucher received his Ph.D. in Electrical Engineering from the University of Michigan in 2005. He is currently a Senior Systems Engineer at Integrity Applications Incorporated in Ann Arbor, Michigan. From 1998 to 2007, he was a Staff Scientist at General Dynamics Advanced Information Systems' Michigan Research & Development Facility \(formerly ERIM\. His current research interests include nonlinear filtering \(specifically particle filtering Bayesian methods of multitarget tracking, self localization information theoretic sensor management, and distributed swarm management Ben Shapo earned his Ph.D. in El ectrical Engineering in 1996 from the University of Michigan.  He is currently a Senior Systems Engineer at Integrity Applications Incorporated in Ann Arbor, Michigan.  From 2003 to 2008 he was a Lead Engineer at General Dynamics, where he contributed to a number of RF and acoustics signal processing and tracking efforts.  Dr. Shapo has 12 years experience in the DoD research community in the areas of detection, tracking, and data fusion, with emphasis on highfidelity simulations and applying new methods to real data  Dr. Roy Bethel is currently employed at The MITRE Corporation in McLean, VA. He has been actively involved in development, testing, and evaluation of signal processing and detection and tracking systems. In particular, he has developed many systems that have been implemented on United States Navy airborne, surface, and submerged platforms. He is currently engaged in research and development of innovative approaches to multitarget detection and tracking  A CKNOWLEDGEMENTS  This work was partially funded by the Office of Naval Research contract N00014-08-C-0275. The authors would like to thank Dr. John Tague for his support, and Mr. Scott Spencer and Dr. Charles Choi for their assistance 


2 1 0 00                4 G ro w th 1  1 3 1 0 9 2 0 2 3 0 10  0 56                So ci ode m og ra ph ic c ha ra ct er is tic s 


s 5 A ge y ea rs   21 7 8 7 3 9 0 01 0 22  0 1 4 0 0 8              6 G en de r i s fe m al e2   0 2 4  0 0 6 0 0 2 0 00 0 0 3 0 10    


           7 C ur re nt ly n ot w or ki ng 2  0 0 5  0 0 8 0 04 0 04 0 0 1 0 16  0 16             8 C ur re nt ly in e du ca tio n2   0 6 


6 7  0 01 0 1 9 0 08  0 03 0 6 8 0 0 7 0 3 2           9 C ur re nt ly w or ki ng 2  0 2 8  0 03 0 18  0 1 1 0 0 3 0 64  0 00 0 1 4 0 8 9   


        10 E du ca tio n ac hi ev ed 3  3 5 7 1 5 2  0 04 0 02 0 2 1 0 1 2 0 16  0 02 0 1 6 0 13  0 0 6         11 D is pe ns ab le in co m e   


  21 0 9 2 72 7  0 14  0 0 1 0 09  0 08  0 2 0 0 00 0 0 4 0 18  0 1 6 0 0 1        In te rn et u sa ge                     


  12 A ct iv e in te rn et u sa ge 1  0 0 2 0 9 6 0 2 1 0 25  0 11  0 12  0 10  0 0 4 0 05  0 0 8 0 0 5 0 0 1 0 12        13 H ou rs o nl in e h ou rs 


rs   2 6 5 3 0 3  0 04 0 12  0 1 1 0 0 3 0 40  0 0 7 0 0 7 0 4 7 0 5 3 0 07  0 1 1 0 07       14 W illi ng ne ss to p ay 1  1 8 3 0 6 3  0 03 0 10 


10  0 07  0 08  0 0 2 0 0 4 0 0 1 0 01  0 00 0 0 5 0 14  0 04 0 05      G am e sp ec ifi c va ria bl es                      15 T en 


ur e w ee ks   2 8 2 3 5 2 0 2 6 0 31  0 0 9 0 01 0 12  0 0 4 0 02 0 0 9 0 0 9 0 07  0 02 0 13  0 08  0 0 4    16 C ro ss o ve r on o ffl in e 4  0 1 5 


5 1 1 1 0 1 9 0 11  0 13  0 18  0 2 0 0 1 4 0 0 7 0 14  0 1 1 0 0 4 0 08  0 15  0 0 5 0 01 0 07    17 S at is fa ct io n1   18 7 5 1 3 16  0 18  0 00 


00 0 44  0 52  0 1 4 0 0 3 0 02 0 07  0 0 9 0 1 4 0 10  0 08  0 0 6 0 09  0 0 1 0 13   18 C om m itm en t1  0 6 2 0 8 3 0 3 1 0 13  0 37  0 39  0 0 7 


7 0 0 6 0 02 0 03  0 0 4 0 1 3 0 14  0 17  0 0 5 0 09  0 07  0 19  0 58  S ou rc e O w n ca lc ul at io n N ot e N  1 3 89 o bs er va tio ns S ig ni fic an ce le ve ls 


ls  p  0 05 S D  S ta nd ar d de vi at io n 1 5 po in t L ik er t s ca le ra ng in g fro m 2 to 2  2 du m m y va ria bl e 3 o rd in al v ar ia bl e ra ng in g fro m v oc at io na l e du ca 


tio n to P h D 4 n um be r o f c on ta ct s   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


