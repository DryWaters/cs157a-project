Using Available Remote Memory Dynamically for Parallel Data Mining Application on ATM-Connected PC Cluster Masato Oguchi 1  2 and Masaru Kitsuregawa 1 1 Institute of Industrial Science The University of Tokyo 7-22-1 Roppongi Minato-ku Tokyo 106-8558 Japan 2 Informatik4 Aachen University of Technology Ahornstr.55 D-52056 Aachen Germany oguchi@computer.org Abstract Personal computer/Workstation PC/WS clusters are promising candidates for future high performance computers because of their good scalability and cost performance ratio Data intensive applications such as data mining and ad hoc query processing in databases are considered very important for massively parallel processors as well as conventional scientific calculations Thus investigating the feasibility of data intensive applications on a PC cluster is meaningful Association rule mining one of the best-known problems in data mining differs from conventional scientific calculations in its usage of main memory It allocates many small data areas in main memory and the number of those areas suddenly grows enormously during execution As a result the contents of memory must be swapped out if the requirement for memory space exceeds the real memory size However because the size of each data area is rather small and the elements are accessed almost at random swapping out to a storage device must degrade the performance severely In this paper we investigate the feasibility of using available remote nodes memory as a swap area when application execution nodes need to swap out their real memory contents during the execution of parallel data mining on PC clusters We report our experiments in which application execution nodes acquire extra memory dynamically from several available remote nodes through an ATM network A method of remote memory utilization with remote update operations is proposed and evaluated The experimental results on our PC cluster show that the proposed method is expected to be considerably better than using hard disks as a swapping device The dynamic decision mechanism for remote memory availability and the migration operations are also evaluated 1 Introduction Recently data intensive applications such as data mining and data warehousing have been focused as one of the most important applications for high performance computing As a platform Personal computer/Workstation PC/WS cluster is a promising candidate for future high performance computers from the viewpoint of good scalability and cost performance ratio We previously developed a large scale ATM-connected PC cluster and implemented several database applications including parallel data mining to evaluate their performance and the feasibility of such applications using PC  Different from the conventional scientific calculations association rule mining one of the best known problems in data mining has a peculiar usage of main memory It allocates a lot of small data on main memory and the number of those areas multiplies to be enormous during the execution Thus the requirement of memory space changes dynamically and becomes extremely large Contents of memory must be swapped out if the requirement exceeds the real memory size However because the size of each data element is rather small and all the elements are accessed almost at random swapping out to a secondary storage system is likely to cause severe performance degradation We are investigating the feasibility of using available memory in remote nodes as a swap area when application execution nodes need to swap out their memory contents In this paper we report our experimental results in which 0-7695-0574-0/2000 $10.00 001 2000 IEEE 


nodes executing an application acquire extra memory dynamically from several remote nodes in the ATM-connected PC cluster Moreover a method using distant node's memory with remote update operations which is expected to prevent a thrashing problem is proposed and evaluated The rest of paper is organized as follows In Section 2 the data mining application and its parallelization are explained In Section 3 an overview of our PC cluster is presented and an implementation of parallelized data mining on our PC cluster is described The method of dynamic remote memory utilization for parallel data mining is explained in Section 4 In Section 5 performance results of the evaluation of proposed mechanisms are shown and analyzed Final remarks are made in Section 6 2 Data mining application and its parallelization 2.1 Mining of association rules Data mining has attracted a lot of attention from both research and commercial community for finding interesting trends hidden in large transaction logs Data mining is a method for the efficient discovery of useful information such as rules and previously unknown patterns existing among data items in large databases thus allowing for more effective utilization of existing data Large transaction processing system logs have been accumulated because of the progress of bar-code technology Such data was just archived and not used efficiently until recently The advance of microprocessor and secondary storage technologies allows us to analyze vast amount of transaction log data to extract interesting customer behaviors For very large mining operations however parallel processing is required to supply the necessary computational power One of the best known problems in data mining is mining of association rules from a database so called basket    Basket type transactions typically consist of a transaction identification and items bought per transaction An example of an association rule is if customers buy A and B then 90 of them also buy C The best known algorithm for association rule mining is the Apriori algorithm proposed by R Agrawal of IBM Almaden Research[6][7][8 Apriori first generates so-called candidate itemsets groups consisting of one or more items then scans the transaction database to determine whether the candidates have the user-specified minimum support In the first pass pass 1 support for each item is counted by scanning the transaction database and all items that achieve the minimum support are picked out These items are called large 1-itemsets In the second pass pass 2 2-itemsets pairs of two items are generated using the large 1-itemsets These 2-itemsets are called the candidate 2-itemsets Support for the candidate 2-itemsets is then counted by scanning the transaction database The large 2-itemsets that achieve the minimum support are determined The algorithm goes on to find the large 3-itemsets the large 4-itemsets and so on This iterative procedure terminates when a large itemset or a candidate itemset becomes empty Association rules that satisfy user-specified minimum confidence can be derived from these large itemsets 2.2 Parallelization of association rule mining In order to improve the quality of the rule we have to analyze very large amounts of transaction data which requires considerable computation time We have previously studied several parallel algorithms for mining association based on Apriori One of these algorithms called Hash Partitioned Apriori HPA is implemented and evaluated on the PC cluster HPA partitions the candidate itemsets among processors using a hash function like the hash join in relational databases HPA effectively utilizes the whole memory space of all the processors hence it works well for large scale data mining The steps of the algorithm are as follows 1 Generate candidate k itemsets All processors have all the large  k 000 1  itemsets in memory when pass k starts Each processor generates candidate k itemsets using large  k 000 1  itemsets applies a hash function and determines a destination processor ID If the ID is the processor's own the itemset is inserted into the hash table otherwise it is discarded 2 Scan the transaction database and count the support value Each processor reads the transaction database from its local disk It generates k itemsets from those transactions and applies the same hash function used in phase 1 The processor then determines the destination processor ID and sends the k itemsets to it When a processor receives these itemsets it searches the hash table for a match and increments the match count 3 Determine large k itemsets Each processor checks all the itemsets it has and determines large itemsets locally then broadcasts them to the other processors When this phase is finished at all processors large itemsets are determined globally The algorithm terminates if no large itemset is obtained 0-7695-0574-0/2000 $10.00 001 2000 IEEE 


3 ATM-connected PC cluster 3.1 The issues of PC clusters Components of today's high performance parallel computers are evolving from proprietary parts e.g CPUs disks and memories into commodity parts This is because technologies for such commodity parts have matured enough to be used for high-end computer systems While an interconnection network has not yet been commoditized thus far ATM technology is one of the strong candidates as a de facto standard of high speed communication networks With the progress of high performance networks such as ATM future parallel computer systems will undoubtedly employ commodity networks as well Thus PC/WS clusters using high performance commodity networks have become an exciting research topic in the field of parallel and distributed computing They are expected to play an important role as large-scale parallel computers in the next generation because of their good scalability and cost performance ratio Initially the processing nodes and/or networks of clusters were built from customized designs since it was difficult to achieve good performance using only off-the-shelf   Such systems are interesting as research prototypes but most failed to be accepted as common platforms However because of advances in workstation and high-speed network technologies reasonably high performance WS clusters can be built using off-the-shelf workstations and high speed Until recently workstations were overwhelmingly superior to personal computers in terms of performance as well as sophisticated software environments However the latest PC technology has dramatically increased CPU main memory and cache memory performance While RISC processors used in today's WSs provide better floating point performance than the microprocessors used in PCs some applications such as database processing primarily require good integer performance Since integer performance of latest PCs is better than that of WSs e.g SPECint95 of 800MHz PentiumIII is 38.3 while SPECint95 of 450MHz UltraSPARC-II is 19.7 PCs have better cost performance ratios than WSs for database operations High-speed bus architectures such as the PCI bus have also improved I/O performance of PCs Because the size of the PC market is much larger than the WS market further improvements in the cost performance ratio are expected for PC clusters Various research projects which develop and examine PC/WS clusters have been performed until       However most of them only measured basic characteristics of PCs and networks and/or some small benchmark programs were examined We believe that data intensive applications such as data mining e  Each node of the PC cluster  CPU Intel 200MHz Pentium Pro Chipset Intel 440FX Main memory 64Mbytes IDE hard disk WesternDigital Caviar32500 2.5Gbytes SCSI hard disk Seagate Barracuda 4.3Gbytes OS Solaris 2.5.1 for x86 ATM NIC Interphase 5515 PCI Adapter and ad hoc query processing in databases are quite important for future high performance computers in addition to the conventional scientific applications We have developed a pilot system of ATM-connected PC cluster consists of 100 Pentium Pro PCs and evaluated it with database applications including the TPC-D benchmark and a data mining application[1][2 3.2 An overview of our PC cluster pilot system 100 nodes of 200MHz Pentium Pro PCs are connected with an ATM switch in our PC cluster pilot system Each node consists of the components shown in Table 1 HITACHI's AN1000-20 which has 128 port 155Mbps UTP-5 is used as an ATM switch Since this switch has 128 port all nodes can be connected directly with each other forming a star topology rather than a cascade configuration All nodes of the cluster are connected by a 155Mbps ATM LAN as well as by an Ethernet An RFC1483 PVC driver which supports LLC/SNAP encapsulation for IP over  is used Only UBR traffic class is supported in this driver TCP/IP is used as a communication protocol An overview of the PC cluster is shown in Figure 1 3.3 Implementation of parallelized association rule mining HPA program described in Section 2 was implemented on the PC cluster pilot system Each node of the cluster has a transaction data file on its own hard disk Transaction data was produced using a data generation program developed by Agrawal designating some parameters such as the number of the transaction the number of different items and so  The produced data was divided by the number of nodes and copied to each node's hard disk WesternDigital Caviar32500 IDE disks are used for this purpose At each node two processes are created and executed One process makes candidate itemsets from previous large itemsets and sends them to the other process which puts 0-7695-0574-0/2000 $10.00 001 2000 IEEE 


    UTP-5 155Mbps Ethernet hub 10Base-T Ethernet Personal Computer Pentium Pro 200MHz 128port ATM switch HITACHI AN1000-20 100 PCs Figure 1 An overview of the PC cluster the data into a hash table In the data counting phase one process generates itemsets by scanning the transaction data file and sends them to the other processes on the nodes selected by the hash function The target processes check and increment their hash table values accordingly Solaris Transport Layer Interface TLI system calls are used for the inter-process communication All processes are connected with each other by TLI transport endpoints thus forming a mesh topology dev/tcp is used as a transport layer protocol It is a two-way connection based byte stream On the ATM level Permanent Virtual Channel PVC switching is used since data is transferred continuously between all processes During the execution of HPA itemsets are kept in memory as linked structures that are classified by a hash function That is to say all itemsets having the same hash value are assigned to the same hash line on the same node and their structures are connected with each other to form a list As an example a result of HPA is shown in Table 2 In this execution the number of transactions is 10,000,000 the number of different items is 5,000 and the minimum support is 0.7 When the PC cluster using 100 PCs is employed for this problem reasonably good performance improvement is Several characteristics such as CPU usage and network performance of the cluster during the execution of HPA are analyzed and discussed in It is known that the number of candidate itemsets in pass 2 is very much larger than in other passes as can be seen in the table This often happens in association rule mining e  The number of candidate and large itemsets at each step C Number of candidate itemsets L Number of large itemsets pass C L pass 1 1023 pass 2 522753 32 pass 3 19 19 pass 4 7 7 pass 5 1 0 4 Using available remote memory dynamically on the PC cluster 4.1 Swapping operation during the execution of data mining As mentioned in Section 3 the number of candidate itemsets in pass 2 is very much larger than other passes in association rule mining The number of itemsets is strongly dependent on user-specified conditions such as minimum support value and it is difficult to predict before execution how large the number will be before execution Therefore it may happen that the number of candidate itemsets increases dramatically in this step so that the requirement of memory becomes extremely large When the required memory is larger than the real memory size part of the memory contents must be swapped out However because the size of each data is rather small and all the data is accessed almost at random swapping out to a storage device is expected to degrade performance severely in this case In the case of large scale clusters a large fraction of the memory of total nodes is not in active use on Therefore borrowing available memory from other nodes in case of necessity seems to be a good idea In the rest of this paper several methods are discussed in which available memory in remote nodes is used as a swap area when huge memory is dynamically required during the execution of parallel data mining on PC clusters 4.2 Dynamic decision mechanism for remote memory availability Remote nodes whose memories are available for application execution nodes are found dynamically during the execution We call them memory available nodes The mechanism to decide the availability of remote nodes is shown in Figure 2 On memory available nodes a pro0-7695-0574-0/2000 $10.00 001 2000 IEEE 


cess is running to monitor the amount of available memory periodically netstat k command provided by Solaris operating system is used to get memory information from the kernel statistics structure 1 Each time the process gets the information the process broadcasts it to all application execution nodes On application execution nodes a client process is running and waiting for the information sent from the memory monitoring processes running on memory available nodes The client process has a memory area which can be shared with application processes and the received information about the amount of memory at each node is written on the shared memory The application processes can read this information at anytime to decide the policy of swapping operations For example when a memory available node does not have enough memory space the shortage of memory is detected by application processes so that another node is chosen as a swapping destination afterward On the other hand if some other processes begin their execution on a memory available node which already accepted swapping operations the swapped out data must be migrated to other memory available nodes to make space on its memory for the new processes This mechanism works as follows First the memory shortage is monitored on a memory available node then this information is broadcast to client processes of all application execution nodes When the application execution nodes detect this memory shortage they check a memory management table which shows where each entry currently exists If they find that part of their memory contents has been swapped out to this memory available node they sends a migration direction to this node to tell to which node these entries should be migrated The memory available node migrates its contents to other memory available nodes according to the direction 4.3 Dynamic remote memory acquisition As a method of experiments a limit value for memory usage of candidate itemsets is set at each node When the amount of memory usage exceeds this value during the execution of HPA program part of contents is swapped out to available remote nodes memory That is to say the application execution node acquires memory area dynamically from one of memory available nodes when it is needed When the number of candidate itemsets becomes extremely large in pass 2 and the amount of memory usage exceeds a specified value the node sends some of its memory contents to destination memory available nodes The unit of swapping operation is a hash line which is a listed structures as explained in Section 3 The hash line swapped out is selected using a LRU algorithm At the memory avail 1 The k option to netstat command is not documented on the manual pages See for more information Monitor Client App. Proc Mem. Monitor Mem. Manage App. Proc App. Proc App. Proc Monitor Client Mem. Monitor Application Execution Nodes Memory Available Nodes  HPA program  Shared Memory memory availability information swapping operation Mem. Manage data migration Figure 2 Dynamic decision mechanism r remote memory availability able node the received contents are allocated and written in its main memory Each memory available node may receive swapped out data from several application execution nodes A pagefault occurs on the other hand when an application execution node accesses an item that had been swapped out It sends a request to a memory available node which holds the data and the memory available node sends back the requested hash line After the application execution node receives the contents they are allocated and written on main memory again then the execution of application resumes Replacements of data are decided by LRU manner The maximum number of nodes used as memory available nodes can be changed as required from one to many in the experiment The basic behavior of this approach has something in common with distributed shared memory memory management system in distributed operating  or cache mechanism in client-server database  For example if data structures inside applications are considered in distributed shared memory almost the same effect can be expected That is to say it is possible to program almost the same mechanism using some types of distributed shared memory systems Thus our mechanism might be regarded as equivalent to a case of distributed shared memory optimized for a particular application 4.4 Remote memory update operation Because most of memory contents are accessed repeatedly the number of pagefaults is considered to become very high when the memory usage limit is small A kind of thrashing may happen in such a case In order to prevent this phenomenon we investigate a method to restrict swapping 0-7695-0574-0/2000 $10.00 001 2000 IEEE 


operations When usage of memory reaches to the limit value at an application execution node it acquires remote memory and swaps out part of its memory contents The contents will be swapped in again if this data is accessed later Instead of swapping it is sometimes better to send update information to the remote memory when a pagefault occurs That is to say once some contents are swapped out to memory in a remote node they are fixed at there and accessed only through a remote memory access interface provided by library functions We apply this policy to the itemsets counting phase in which the memory access operation is simple to compare itemsets with the contents of the hash table and update the table 5 Performance analysis of dynamic remote memory utilization 5.1 Implementation of the mechanism on PC cluster The proposed mechanism has been implemented on the PC cluster pilot system The parameters used in the experiment are as follows The number of transactions is 1,000,000 the number of different items is 5000 and the minimum support is 0.1 The size of the transaction data is about 80Mbytes in total The message block size is set to be 4Kbytes and the disk I/O block size is 64Kbytes The number of application execution node is 8 in this evaluation The number of hash line for candidate itemsets is 800,000 in total hence about 100,000 hash lines are assigned to each node during the execution The unit of swapping operation is a hash line which could be contained in one message block in this experiment With the above conditions the number of candidate itemsets in pass 2 was 4,871,881 in total These candidate 2-itemsets are assigned to each node using a hash function The numbers of candidate 2-itemsets at each node are shown in Table 3 Although the itemsets are assigned using a hash function the numbers at each node are not equal This frequently happens in the execution of HPA because some amount of skew usually exists in transaction data in association rule mining We have also developed a method to treat it which can be found in another Since each candidate itemset occupies 24bytes in total\(structure area  data area approximately 14-15Mbytes of memory are filled with these candidate itemsets at each node The interval of monitoring the amount of available memory is 3sec which is considered frequent enough for monitoring and not too heavy for application execution nodes This value will be discussed later e  The number of candidate 2-itemsets at each node  node 1 node 2 node 3 node 4 602559 641243 582149 614412 node 5 node 6 node 7 node 8 604851 596359 622679 607629 5.2 Dynamic remote memory acquisition with simple swapping First a method using available remote nodes memory with simple swapping is examined The maximum number of nodes used as memory available nodes is changed from 1 o 16 In this experiment all memory available nodes are assumed to have enough memory space to accept requests of swapping operations In such a case all the memory available nodes are used for swapping operations throughout the execution of the program and therefore the number of memory available nodes is constant during the experiment The execution time of pass 2 n HPA program when the number of memory available nodes changes from 1 to 16 is shown in Figure 3 In this figure the result of 5 different cases are shown The upper 4 lines are the cases of memory usage for candidate itemsets being limited as 12Mbytes 13Mbytes 14Mbytes and 15Mbytes respectively The lowest line is the case with no memory usage limit in which application execution nodes are assumed to have enough memory for candidate itemsets so that no swapping occurs Other mechanisms and conditions are the same with memory limited cases Memory monitor mechanism is running in this case also for comparison When the number of memory available nodes is small the execution time is quite long especially when the memory usage limit size is smaller Apparently memory available node\(s become bottleneck in these cases This bottleneck is resolved when the number of memory available nodes is 8 16 in this experiment The execution time becomes longer as the memory usage limit size becomes small since the number of swap out increases in such cases When memory usage is limited the execution time is quite longer than that of the no memory limit case This is because the number of swap out is extremely large We can calculate the execution time of each pagefault as follows We will focus on the case when the number of memory available nodes is 16 in which memory available nodes are not considered to be bottleneck In the case of memory usage limit being 13Mbytes for example the execution time of the program is 4674.0sec and the difference 0-7695-0574-0/2000 $10.00 001 2000 IEEE 


  0   5000   10000   15000   20000   25000   30000   1                   10 Execution Time [s Number of Memor y Available Node Memory usage limit = 12[MB  Memory usage limit = 13[MB   Memory usage limit = 14[MB   Memory usage limit = 15[MB   No Memory usage limit  Figure 3 Execution time of HPA rogram pass 2 of the execution time between this and the no memory limit case is 4427.0sec The total execution time is decided by the busiest node that does the most swapping operations In this case the maximum number of pagefaults in the busiest node was 1,896,226 Thus the execution time of each pagefault can be obtained by dividing the difference in execution times by the maximum number of pagefaults 2.33msec in this example The other cases are also calculated in Table 4 The execution time of each pagefault consists of round trip delay time data transmission time and memory allocation and/or search time at memory available nodes The point-to-point round trip time on our PC cluster is approximately 0.5msec as shown in Since the point-to-point throughput is about 120Mbps on our cluster and each pagefault data is contained in one message block 4Kbytes the data transmission time can be calculated from these values as approximately 0.3msec The rest of time is considered to be swapping operations cost in memory available nodes We can compare the pagefault execution time with the access time of hard disks According to a state-of-art specification of SCSI hard disks Seagate Barracuda 7,200rpm disks for example the average seek time for read is about 8.8msec and the average rotation waiting time is about 4.2msec In the case of latest fast hard disks such as HITACHI DK3E1T 12,000rpm disks the average seek time for read is about 5msec and the average rotation waiting time is about 2.5msec Therefore it takes at least 13.0msec in average to read data from 7,200rpm hard disks and 7.5msec even with the fastest 12,000rpm hard disks e  The execution time r each pagefault The number of memory vailable nodes is 16 E xec execution time of pass 2 n HPA Dif f difference in execution time between this and the no memory limit case Max maximum number of pagefaults PF execution time of each pagefault Usage limit E xec Dif f Max PF 12MB 7183.1 6936.1 2925243 2.37 13MB 4674.0 4427.0 1896226 2.33 14MB 2489.7 2242.7 1003757 2.22 15MB 757.3 510.3 268093 1.90 5.3 Using remote memory with remote update operation The access interface function is developed to realize the remote update operations In this experiment also all memory available nodes are assumed to have enough memory space to accept swapping requests so that the number of memory available nodes is constant during the experiment The execution time using this method is shown in Figure 4 This figure shows the execution time of pass 2 f HPA program when the number of memory available nodes is 16 The execution times of dynamic remote memory acquisition using remote update operations and using simple swapping are compared in the figure The execution time using hard disks as a swapping device is also shown for comparison Seagate Barracuda 7,200rpm SCSI hard disk is used for this purpose In this case memory contents are swapped out to hard disks when the memory usage of candidate itemsets exceeds the limit value Other conditions and implementations are the same with the case of dynamic remote memory acquisition The execution time using hard disks as swapping devices is very long especially when the memory usage limit is small because each access time to a hard disk is much longer than that of remote memory through the network The execution time of dynamic remote memory acquisition with simple swapping is better than for swapping out to hard disks It increases however when the memory usage limit is small since the number of pagefaults becomes extremely large in such a case The execution time of dynamic remote memory acquisition with remote update operations is quite short compared to these results even when the memory usage limit is small It seems to be effective to provide a simple remote access in0-7695-0574-0/2000 $10.00 001 2000 IEEE 


  0   2000   4000   6000   8000   10000   12000   14000   12   12.5   13   13.5   14   14.5   15 Execution Time [s Memor y Usa g e Limit Swapping out to hard disks  Dynamic remote memory acquisition with simple swapping   Dynamic remote memory acquisition with remote update  Figure 4 Comparison of proposed methods terface for the itemsets counting phase because the number of swapping operations during this phase is extremely large According to these results performance of the proposed remote memory utilization with remote update operations is considerably better than other methods 5.4 Dynamic memory migration on memory available nodes In the previous experiments all memory available nodes are assumed to have enough memory space to accept requests of swapping operations In such cases the number of memory available nodes is constant during the experiment while the amount of memory is monitored periodically and their availability is checked when they are accessed As explained in Section 4 when a destination memory available node does not have enough memory space and the shortage is detected on the application execution node another node is chosen as a swapping destination If some other processes begin to run on a memory available node which already accepted swapped out data and therefore it must make space on its memory for the new processes the swapped out data is migrated to another memory available node The following experiment is performed to evaluate the memory migration mechanism First the HPA program starts using the mechanism of the dynamic memory acquisition with remote update operation During the execution of the program a signal is sent to the amount of memory monitoring process on one of memory available nodes After the process receives this signal it begins to pretend to have no available memory space anymore as if other new processes begin to use the whole memory on this machine and broadcasts the information to its client processes on application execution nodes When application execution nodes detect the memory shortage on the memory available node they sends a migration direction to this memory available node for entries which was belonging to them to which node the entries should be migrated Then the memory available node migrates its contents to other memory available nodes After this procedure the application program resumes while the number of memory available nodes reduces by one It is possible to send a signal more than two memory available nodes and migrate memory contents from them The execution time of pass 2 in HPA program in this experiment is shown in Figure 5 The maximum number of nodes used as memory available nodes is 16 and the interval of monitoring the amount of available memory is 3sec This figure shows three lines The lower line is the case in which all 16 memory available nodes are used for swapping operations throughout the execution of the program The middle line is the case in which one of memory available nodes receives a signal during the execution of the program and it migrates contents of its memory to other memory available nodes The upper line is the case in which two of memory available nodes migrate their memory contents to other nodes As shown on the figure the execution time did not change significantly from case to case According to the result the overhead of memory contents migration is almost negligible in this experiment The results are not significantly changed either when the interval of monitoring the amount of available memory is a little shorter e.g 1sec Too short interval such as shorter than 1sec degrades the system performance because of the monitoring and communication overhead Such a short interval is expected to be unnecessary in most cases 6 Conclusion At present judging by the number of installation sites high performance parallel computers are becoming more popular in business applications than in scientific research Data mining and ad hoc query processing are considered two of the most important applications for parallel processing Since the PC cluster is a promising platform for future high performance computers from the cost/performance aspect the feasibility of implementing parallel data mining application over a C cluster was examined In contrast to conventional scientific calculations association rule mining one of the best-known problems in data mining has a peculiar feature in its usage of main memory it needs many small data elements in main memory at each node and the numbers of those areas suddenly increase greatly during execution In this paper we show and discuss experimental results in which application execution nodes acquire extra memory 0-7695-0574-0/2000 $10.00 001 2000 IEEE 


  0   100   200   300   400   500   12   12.5   13   13.5   14   14.5   15 Execution Time [s Memor y Usa g e Limit 2 memory nodes unavailable  1 memory node unavailable   all memory nodes available  Figure 5 Dynamic memory migration on memory available nodes dynamically from available remote nodes in an ATMconnected PC cluster The experimental results show this method is considerably better than using hard disks as a swapping device A method of updating remote memory in order to prevent thrashing was proposed and examined This method achieves the best performance The dynamic decision mechanism for remote memory availability and the migration operations were also evaluated The overhead of memory contents migration is almost negligible in this experiment unless the interval of monitoring the amount of available memory is too short Acknowledgment This project is partly supported by the Japan Society for the Promotion of Science JSPS New Energy and Industrial Technology Development Organization NEDO and the Grant-in-Aid for Creative Basic Research 09NP1401 by the Ministry of Education Science Sports and Culture HITACHI Ltd gave us extensive technical help with ATM-related issues We would like to thank Prof Spaniol in Aachen University of Technology for supporting our research References  T Tamura M Oguchi and M Kitsuregawa Parallel Database Processing on a 100 Node PC Cluster Cases for Decision Support Query Processing and Data Mining Proceedings of SC97 High Performance Networking and Computing SuperComputing 97  November 1997  M Oguchi T Shintani T Tamura and M Kitsuregawa Optimizing Protocol Parameters to Large Scale PC Cluster and Evaluation of its Effectiveness with Parallel Data Mining Proceedings of the Seventh IEEE International Symposium on High Performance Distributed Computing  pp.34-41 July 1998  U M Fayyad G P Shapiro P Smyth and R Uthurusamy Advances in Knowledge Discovery and Data Mining The MIT Press  1996  V Ganti J Gehrke and R Ramakrishnan Mining Very Large Databases IEEE Computer  Vol.32 No.8 pp.38-45 August 1999  M J Zaki Parallel and Distributed Association Mining A Survey IEEE Concurrency  Vol.7 No.4 pp.14-25 1999  R Agrawal T Imielinski and A Swami Mining Association Rules between Sets of Items in Large Databases Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data  pp.207-216 May 1993  R Agrawal T Imielinski and A Swami Database Mining A Performance Perspective IEEE Transactions on Knowledge and Data Engineering  Vol.5 No.6 pp.914-925 December 1993  R Agrawal and R Srikant Fast Algorithms for Mining Association Rules Proceedings of the Twentieth International Conference on Very Large Data Bases  pp.487-499 September 1994  T Shintani and M Kitsuregawa Hash Based Parallel Algorithms for Mining Association Rules Proceedings of the Fourth IEEE International Conference on Parallel and Distributed Information Systems  pp.1930 December 1996  R S Nikhil G M Papadopoulos and Arvind T A Multithreaded Massively Parallel Architecture Proceedings of the Nineteenth International Symposium on Computer Architecture  pp.156-167 May 1992  M Blumrich K Li R Alpert C Dubnicki E Felten and J Sandberg Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer Proceedings of the Twenty-First International Symposium on Computer Architecture  pp.142-153 April 1994  C Huang and P K McKinley Communication Issues in Parallel Computing Across ATM Networks 0-7695-0574-0/2000 $10.00 001 2000 IEEE 


IEEE Parallel and Distributed Technology  Vol.2 No.4 pp.73-86 1994  T Sterling D Saverese D J Becker B Fryxell and K Olson Communication Overhead for Space Science Applications on the Beowulf Parallel Workstation Proceedings of the Fourth IEEE International Symposium on High Performance Distributed Computing  pp.23-30 August 1995  R Carter and J Laroco Commodity Clusters Performance Comparison Between PC's and Workstations Proceedings of the Fifth IEEE International Symposium on High Performance Distributed Computing  pp.292-304 August 1996  D E Culler A A Dusseau R A Dusseau B Chun S Lumetta A Mainwaring R Martin C Yoshikawa and F Wong Parallel Computing on the Berkeley NOW Proceedings of the 1997 Joint Symposium on Parallel Processing\(JSPP 97  pp.237-247 May 1997  A Barak and O La'adan Performance of the MOSIX Parallel System for a Cluster of PC's Proceedings of the HPCN Europe 1997  pp.624-635 April 1997  H Tezuka A Hori Y Ishikawa and M Sato PM An Operating System Coordinated High Performance Communication Library Proceedings of the HPCN Europe 1997  pp.708-717 April 1997  M Oguchi T Shintani T Tamura and M Kitsuregawa Characteristics of a Parallel Data Mining Application Implemented on an ATM Connected PC Cluster Proceedings of the HPCN Europe 1997  pp.303-317 April 1997  J Heinanen Multiprotocol Encapsulation over ATM Adaptation Layer 5 RFC1483  July 1993  M Laubach Classical IP and ARP over ATM RFC1577  January 1994  M Oguchi T Tamura T Shintani and M Kitsuregawa Implementation of Parallel Data Mining on an ATM Connected PC Cluster and Performance Analysis of TCP Retransmission Mechanisms The Transactions of the Institute of Electronics Information and Communication Engineers  Vol.J81-B-I No.8 pp.461-472 August 1998  A Acharya and S Setia Availability and Utility of Idle Memory in Workstation Clusters Proceedings of the 1999 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems  pp.207-216 May 1993  A Cockcroft How Much RAM is Enough Sun World Online http://www.sunworld.com/sunworldonline/swol05-1996/swol-05-perf.html May 1996  C Amza A L Cox S Dwarkadas P Keleher H Lu R Rajamony W Yu and W Zwaenepoel TreadMarks Shared Memory Computing on Networks of Workstations IEEE Computer  Vol.29 No.2 pp.1828 February 1996  M J Feeley W E Morgan E P Pighin A R Karlin H M Levy and C A Thekkath Implementing Global Memory Management in a Workstation Cluster Proceedings of the ACM Symposium on Operating Systems Principles  pp.201-212 December 1995  S Dar M J Franklin B P Jonsson D Srivastava and M Tan Semantic Data Caching and Replacement Proceedings of the Twenty-second International Conference on Very Large Data Bases  pp.330-329 September 1996  T Shintani and M Kitsuregawa Parallel Mining Algorithms for Generalized Association Rules with Classification Hierarchy Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data  pp.25-36 June 1998 0-7695-0574-0/2000 $10.00 001 2000 IEEE 


CMP A Fast Decision Tree Classifier Using Multivariate Predictions  449 H Wang and C Zaniolo Mining Recurrent Items in Multimedia with Progressive Resolution Refinement  461 0 Zai'ane J Hun and H Zhu Panel Session 22 Is E-Commerce a New Wave for Database Research Moderator Anant Jhingran IBM T.J Watson Research Center USA Panelists Sesh Murthy IBM T.J Watson Research Center USA Sham Navathe, Georgia Institute of Technology USA Hamid Pirahesh IBM Almaden Research Center USA Krithi Ramamrithan University of Massachusetts-Amherst USA Industrial Session 23 Java and Databases Pure Java Databases for Deployed Applications  477 N Wyatt Database Technology for Internet Applications  700 A Nori Session 24 Association Rules and Correlations Finding Interesting Associations without Support Pruning  489 E Cohen M Datar S Fujiwara A Gionis P Indyk R Motwani J Ullman and C. Yang Dynamic Miss-Counting Algorithms Finding Implication and Similarity Rules with Confidence Pruning  501 S Fujiwara J Ullman and R Motwani Efficient Mining of Constrained Correlated Sets  512 G Grahne L Lakshmanan and X Wang Session 25 Spatial and Temporal Data Analyzing Range Queries on Spatial Data  525 J Jin N An and A Sivasubramaniam Data Redundancy and Duplicate Detection in Spatial Join Processing  535 J.-P Dittrich and B Seeger Query Plans for Conventional and Temporal Queries Involving Duplicates and Ordering  547 G Slivinskas C Jensen, and R Snodgrass xi 


Industrial Session 26 XML and Databases Oracle  The XML Enabled Data Management System  561 S Banerjee V Krishnamurthy M Krishnaprasad, and R Murthy XML and DB2  569 J Cheng and J Xu Session 27 High-Dimensional Data Independent Quantization An Index Compression Technique for High-Dimensional Data Spaces  577 S Berchtold, C Bohm H Jagadish H.-P. Kriegel and J Sander Deflating the Dimensionality Curse Using Multiple Fractal Dimensions  589 B.-U Pagel F Korn and C. Faloutsos Similarity Search for Multidimensional Data Sequences  599 S.-L Lee S.-J Chun D.-H Kim, J.-H Lee and C.-W Chung Session 28 Web-Based Systems WRAP An XML-Enabled Wrapper Construction System for Web Information Sources  611 L Liu C Pu and W. Hun Self-Adaptive User Profiles for Large-scale Data Delivery  622 U Cetintemel M Franklin and C. Giles Industrial Session 29 Main Memory and Small Footprint Databases In-Memory Data Management in the Application Tier  637 The TimesTen Team SQLServer for Windows CE -A Database Engine for Mobile and Embedded Platforms  642 P Seshadri and P. Garrett Join Enumeration in a Memory-Constrained Environment  645 I Bowman and G Paulley xii 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


