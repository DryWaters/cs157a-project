Mining Associations by Pattern Structure in Large Relational Tables Haixun Wang Chang-Shing Pemg Sheng Ma Philip S Yu IBM T J Watson Research Center Yorktown Heights NY 10598 haixun perng shengma, psyu}@us.ibm.com Abstract Associarion rule mining aims ar discovering parrerns whose supporr is beyond a given rhreshold Mining par rerns composed of irems described by an arbitrary subser of arrribures in LI forge relarional table represents a new chal lenge and has various pracrical applicarions. including 
rhe evenr management sysrems rhar morivared rhis work The anribure combinarions that define rhe irems in a parrempm vide rhe srrucrural information of rhe parrern Currenr as sociarion algorirhms do nor make full use ofrhe srrucrural informarion of rhe parrerns rhe informarion is either lost er ir is encoded wirh arrribure values or is consrroined by a given hierarchy or raxonomy Pattern srrucrures con vey imporrant knowledge abour rhe parrerns In rhis papes we present a novel archirecrure that organizes rhe mining space 
based on parfem srrucrures By exploiting rhe inrer relarionships among parrern structures execurion rimes for mining can be reduced significanrly This advanrage is demonsrrared by our experiments using both synrheric and mal-fife darosers 1 Introduction The problem of mining frequent itemsets in a set of transactions wherein each transaction is a set of items was first introduced by Agrawal et al I There is a press ing need for algorithms to support relational data min ing 141 as a majority of datasets in the real world are stored in the relational form or 
generated on the fly by other query tools e.g OLAP tools in the relational form An item can be described by a set of attributes 5 101 For instance, a system event can be described by attributes such as EVENTTYPE CATEGORY SOURCE APPLICA TION HOST and SEVERITY The dataset shown in Fig ure 1 contains five events that occurred rogerher occurred during the same time interval according to their TIDs One association rule or pattern supported by the dataset is 223TCPConnectionClose EVENT from Vesuvio 
0-7695-1754-4/02 517.00 Q 2002 IEEE 482 HOST occurs together with Security CATEGORY problems during Authorization APPLICATION In teresting rules or patterns often relate to items each de scribed by a subser of these attributes We call such items structured-items Rules or patterns composed of structured-items are often more interesting and informative because their varied com position enables them to represent concepts at all possible granularity levels The structures of the items in a pattern combine to form the pattern structure Pattern 
structure conveys important knowledge about the patterns Our work in mining such patterns in event data brings forth some in teresting issues and challenges EXPONENTIAL SEARCH SPACE Multi-attribute items incur a huge search space The item in RI is defined by two attributes EVENTTYPE and HOST Indeed any subset of attributes can be used to define a structured-item This in troduces a combinatorial challenge in a dataset with N at tributes, items can be defined in as many as ZN  1 different ways, in other words they have as 
many as ZN  1 different structures Furthermore they can be combined differently to form different patterns This introduces another comhi natorial challenge patterns containing k items can have as many as 2Ny-2 different pattern structures Lemma 1 RELATIONAL SENSITIVE The patterns embedded in the event data unlike supermarket purchasing patterns that are more or less stable over the time are constantly evolving as old problems in the system being solved and new types of problems being generated The mining algorithms should focus on newly generated data instead 
of the entire history archive Furthermore we often need to mine data streams generated on the fly by other query tools such as OLAP which requires our mining algorithms to be relational sensi tive 141 as it is often inefficient and unnecessay to first con vert the data to a mining format then discover all frequent itemsets among them and finally filter out the answers to the queries The dynamic nature of the data prevents us from running a mining algorithm once and for all and sav ing the results for future analysis 


Rec TID EVENTTYPE CATEGORY SOURCE APPLICATION HOST SEVERITY I 1001 TCPConnectionClose Network 10 System Vesuvio Low 3 1001 AuditFailure Security Software Authorization Magna High 4 1001 CoreDump Memory Exception Kernel 2.4 Stromboli High 5 1001 IRQConflict Device PCI Bus System Vulcano High 2 1001 CiscoDCDLinkUp Network DHCP Routing Etna Low Figure 1 Event Database the 5 records form one transaction, according to their TIDs USER PREFERENCES Traditional association rule algo rithms return all frequent itemsets However the user may only he interested in some specific pattern structures ac cording to his domain knowledge and the current focus of study for a given dataset For instance, a user wants to mine all patterns where EVENTTYPE and SOURCE are involved in describing the patterns At the same time he wants to ig nore patterns which contain elements described by both the SOURCE and CATEGORY attributes Note that such user preferences can not be enforced by preprocessing rhe dura ro exclude certain arrribures or values as these attributes and values may combine in different ways to form patterns that are interesting to the user Thus we need a mecha nism that is conducive to incorporate user preferences into the mining process. Although user preferences and mining constraints are beyond the scope of this paper the frame work proposed in this work makes such incorporation pos sible I I 2 Related Works Agrawal and Srikant 13 81 led the pioneering work of mining sequential patterns Each item in a pattern is repre sented by a set of literals and patterns on level k patterns with a total number of k literals aTe generated by joining patterns on level k  1 To map our problem into that of mining sequential patterns we convert each structured-item to a set of literals by encoding its attribute information for instance for each value a of attribute A we create an item  However, the pattern structure is lost through such en coding patterns on the same level take part in candidate generation and pruning as a whole, regardless of their pat tern struclure Le what combination of attributes are used in describing each item in the patterns As a result user preferences can not be incorporated in the mining process The algorithm mines unnecessarily all frequent patterns before the answers conformed to user\222s mining targets can he filtered out during the final step It will apparently slow down the mining process when the dataset is large and high dimensional, and it also creates difficulty in understanding the mining results since patterns are all mixed together in stead of partitioned by their pattern structures Srikant et al 7 and Han et al SI consider multi-level association rules based on item taxonomy and hierarchy These approaches are further extended 6.9 to handle more general constraints In Table I we compare them with HlFl With a pre-specified hierarchy or taxonomy, the min ing space is severely restricted The ability to discover pat terns RI   R5 e.g totally depends on whether their pattern structures happen to fit into the given taxonomy or hierarchy In contrast our framework enables us to mine patterns without a pre-specified item hierarchy HlFl offers the maximum flexibility in defining patterns patterns con taining k items can have zNy-z different pattern struc tures The search space of hierarchy and taxonomy is much smaller and their discoveries are limited by the taxonomy or the fixed hierarchy they use 3 Definitions and Notations Let 2 be a set of transactions where each transaction contains a set of records and each record is defined by a set of attributes A Our task is to find frequent k-itemsets how ever, the fundamental concept of irem is defined differently Definition 1 Ler 7  tl  tr be a subser of A A structured-item I is a ser of attribute-value pairs tl  VI  tk  vk where vi is LI value in rhe domin of at rribure t We call 7 rhe item structure of I Based on the event data in Figure I an example of a structured-item is CATEGORY=Security SEVER lTY=High and its item Structure is CATEGORY SEVER ITY We also use security, high todenote the structured item when no confusion arises Definition 2 A record R is an instance ofa structured-irem r  Itl  U  tk  v ifmi  vi,v ti E tl  tk Obviously if R is an instance of structured-item I then VI\222 C I R is an instance of It Definition 3 A pattern is a set ofsrrucrured-irems A k itemset pattern is a parrern conraining k srrucrured-items Let il and iz be two structured-items Pattern il,iz is different from pattern 21 Ui2 the former being a 2-itemset pattern, the lattera I-itemset pattern 483 


of different t of different item structures patternstructures taxonomy limitedbytaxonomy limitedby taxonomy hierarchy N N k 2\224fk--2 HlFl 2N  1 Table 1 A comparison of the three approaches N is the number of attributes of the dataset 222?\222 means the discovery of corresponding rules depends on the particular taxonomy or hierarchy in use discovers RI Rz RB R4 Rg     no   no  no yes yes yes yes yes Definition 4 A transaction S supports pattern X if each structured-item I E X maps to a unique record R E S such that R is an instance ofI Pattern X has supports in datasetD ifs of the transactions in 222D support X We continue our example in Figure 1 Record 2 is an instance of structured-item il SOURCE=DHCP and an instance of iz HosT=Etna which makes it an in stance of is  i U iz SOURCE=DHCP as well Also we can see that record 3 is an instance of ip  HosT=Magna Thus we derive some of the 2-itemset patterns supported by transaction 1001 the first 5 records i1,i4 iz,i4},and is,ia However transaction 1001 does not support 2-itemset pattern ilriz This is because il and az are supported by one record and one record only in transaction IOOI Ac cording to the definition in order to support il,ia the transaction needs to have at least two records that are in stances of il and ia respectively Definition 5 We define pattern structure as a multi-set c  7  Ti where each 7 is a non-empty set of attributes A pattern of structure c has the form p  I  Ik where the item structure of I is 7 For presentation simplicity we use parenthesis to sep arate each 7 in a pattern structure For instance as suming A B and C are 3 of the attributes in a dataset we use A AB C to represent pattern structure c  221E 72,731 where 7  A 72  A B 73  C This notation is used in Figure 2 for example 4 The HIFI Framework If there is only one attribute then all pattern structures are of the form A A  A and the problem degenerates to traditional association rule mining However as shown by Lemma 1 the mining space grows exponentially when the number of attributes increases Proposition 1 Given a dataset with N different attributes there me a total of zN~k-z diferentpattern structures for k-itemset patterns Proof An equivalent problem is how many different out comes are there after a throw of k dice each having m sides The answer is 224\222+[-\222 In our case we have k items, each can be defined in m  2\224  1 ways Thus there are a total of zNy-z different pattern structures for k-itemset patterns 0 To handle such a huge mining space we need to explore the relationships among different pattern structures. For in stance pattern structure AB is a specification of structure A and structure E The HlFl framework explore these links by defining the successorlpredecessor relationships Definition 6 Given LI pattern structure c  TI T and an attribute t E A c\222s immediate successors are in one of the following forms 1 7  Tm,Tm+l where Tm+l  t 2 E  7;U{t   7;\224},t<7 Figure 2 depicts a graph of pattern structures tightly coupled by the predecessorlsuccessor relationships for two attributes A and B Note that a pattern structure c  T  Tm on level L has no more than L predecessors where level L is defined as L  Czl TI the total num ber of attributes that appear in the relevant attributes of c The benefits of structuring the search space in the level wise tightly coupled form are the following The framework reveals all the relationships among pat tern structures These relationships are essential for candidate generation and pruning s Instead of joining the patterns on level K to derive can didate patterns on level K f 1 and then using the pat terns on level K again for pruning we can localize the candidate generation and pruning procedure to each pattern structure. For instance in order to find frequent patterns of structure A A B and A E B only 4 nodes their predecessors in Figure 2 need to be explored The improvement in performance is most significant in mining high dimensional data since the search space grows exponentially as the number of at tributes increases 484 


Figure 2 Mining space of the HlFl framework a graph of tightly coupled pattern structures The framework requires no data encoding. Each struc ture represents a mining target and more importantly the generated patterns are structured in the same form Following the links among the pattern structures users can choose to have a more general or a more specific view of the patterns It helps users to overcome the difficulty in interpreting and analyzing the results 4.1 Downward Closure Properties The extended Downward Closure Property which holds between each pair of parent and child pattern Structures en ables us to eliminate candidate patterns Proposition 2 Downward Closure Properry Let p  II  Ih beaparternofsrrucrurec andrhesupporrofp is less rhan min-sup then 1 rhe support ofparrern p  b  I Ia where I is an mbirraty srrucrured-item is less rhan min-sup 2 rhe supporr of pattern pa  II  Ik-1 la where 0 The organization of the search space in Figure 2 is justi fied by Lemma 2 The level-wise Apriori property has been broken down to a much finer granularity i.e to the pattern structure level If enables us to localize candidate genera tion and pruning for specific pattern structures 4.2 Join Properties I 3 In is less rhan minuup Proof Similar to the Apriori property 121 In this section we explore the most efficient way of pat tern generation. First, we show that the patterns of a pattern structure can he generated by joining the patterns of any nvo of its predecessor structures However although any pair of predecessors can be used the computational cost can be very different We hence identify a specific type of join referred to as simple join which can be implemented effi ciently if the patterns are sorted in certain order Finally we present the conditions that lead to simple joins without pre-sorting Unique Representation of Items and Pattern Structures To facilitate further discussions, we assume that there exists an order \(e.g., alphabetical order\among _he attributes in A Given a set of attributes 7 2 A we use 7 to denoLe the of dered sequence of the same attrib_utes Assuming 5 and 72 are two such sequences wesay 7 j 72 if 71 holds lexico graphical precedence over 72 Thus we can uniguely rep resent a structure c by its ordered version   71  A where 5 j  j E Figure 2 shows each pattern struc ture using the ordered representation assuming alphabetical order among the attributes i.e 4 3 B Now,-given a pattern structure Z on level L Z  3 ___ fi the patterns of c can be represented by a table with L columns tll _ tZ1 _ tkl  where tzl  is the ordered\attributes of  We use ti 10 denote the column defined by the j-th attribute of E and we use CI fo denore a parenr srrucrure of c resulred from raking our the k-rh column from Z For instance the patterns of structure c  A BC are represented by a table of three columns as shown in Figure 3\(a while the patterns of its three parent structures are represented by tables of two columns The Join Operation We show how patterns of parent structures can be joined to produce patterns of child structures To generate the candi date patterns of structure A BC we can join the patterns of its parents ca and c1 as follows Example 1 Join CQ  A B and c1  BC Io derive c  4 BC SELECTc3.111 c3.1n 1.112 FROM e ci WHEREe3.h  ci.111 However, not all join operations can he expressed as suc cinctly For example to generate all possible patterns of 485 


Figure 3 Representing patterns using relational schema Notation c.ti is the column defined by the j-th attribute of 7 and ck is a parent structure of c resulted from taking out the k-th column of C A AC by joiningcg  4 A andcl  AC we will have to use the following SQL Example 2 Join c3  A A and c1  AC ro derive c  4 ac SELECT CASE WHEN c~.lil  ez.1~1 THEN 3.12 ELSE 3.11 END CASE Cl.11 The reason of the complexity is because of the follow ing if p  ao al is a pattern of A A then patterns derived fromp for A AC can have two alternative forms ao al  and al ao Thus in thejoin condition we need to compare attribute 4 of cl to both attributes of CI We show later that such join operations can be avoided The Completeness of the Join Operation We show that the above join operations are complete mean ing that the result of the join contains all the patterns that have a support greater than min-sup Proposition 3 Join Properry I Given a pattern srructure c on level L L 2 2 the candidate parrerns derived by joining the parrerns ofany ofirs nvop'edecessorsrrucrures cunroin all rhe frequenrpatterns of e Proof Let p be a pattern of structure c and let ci and cj be any two parents of c Removing the i-th column and then the j-th column of p we get two subpatterns pi and pj Since p is a pattern of c, according to the anti-monotonicity property pi and pj must he patterns of c and cj respec tively Thus pattern p can be derived by joining pi and pj on their common columns 0 Join Property I also implies that joining the patterns of a structure's immediate predecessors generates fewer candi dates than joining its ancestors This is because the patterns of its predecessors are a subset of thejoin results of the pre decessors predecessors According to Join Property I for any pattern structure on level L L 2 2 we can generate its candidate patterns by joining the patterns of any of its 2 predecessors Other predecessors can be used to further prune the candidate pat terns according to the anti-monotonicity property Joins in the Simple Form Our goal is to generate candidates efficiently through a join operation such as Example 1 without indexing or sorting We denote join operations in Example 1 as joins in the sim ple form while join operations in Example 2 with a disjun tive WHERE condition in the non-simple form Joins in the simple form can he implemented efficiently if the patterns are stored in a certain order For instance, Example 1 can be implemented efficiently if patterns in cg and c1 are ordered by attribute B It is harder to implement the join operation in Example 2 effciently because rwo indices on table cg are required one on column tu the other on tn otherwise we have to do a linear scan on table CQ Since patterns are generated on the fly, maintaining extra index is expensive Given a pattern structure c and any of its two parents ci and cj it is easy to check if patterns of c can be derived by joining ci with cj in the simple form Assume c  X  G  7 and parent ci  171  z  7 where Tk  7 U 4 We call e a simple parent of e if7  z,VZ E c Proposition 4 Parterns of strucrure c con be derived by joining c and cj in the simple form, $both ci and cj are simple pards ofc 0 Proposition 5 Join Property U Given aparrern srrucrure c on level L there exisr at least two predecessor trucru~s c and cj rhar can be joined in rhe simpleform to generare parrerns for c Proof LetZ 3   1711  m and 1x1  n We show that cm and CL-"+I can be joined in the simple form c is the pent structure of c after removal of the last at tribute of  Assuming the removal of the last attribute of 71 results in a new sequznce of attripes T we have ei ther 7  0 or 7  5 5  5 5 which means there can be no 7 E c such that 7  7 Similar reasoning applies to CL-,,+I which is the parent structure of c after Proof Omitted for lack of space 486 


removing of the first attribute of x Since m  n 5 L we know m  L  n  1 according to Lemma 4 cm and 0 CL-~+I can be joined in the simple form Efficient Candidate Generation Merge-Join without Pre-sorting Let\222s assume patterns of the parent structures are ordered by their attribute values i.e patterns of AB CD are or dered by their values of 4 then B C and D Then candi date patterns of a child structure derived by merge-joining the ordered patterns of its parent structures maintain the or der Thus the new patterns can be used to merge-join with other patterns to derive patterns on the next level which still maintain the order We can repeat this process to gen erate patterns on all levels through merge-joining without re-sorting the data For instance say we want to derive the candidate patterns for structure AB CD EF on level 6 These can be generated by joining the patterns of C6  AB CD E and cj  AB CD F We can merge-join C6 and cj because they sharethesame prefix AB CD of length4 Furthermore the results of the join are still ordered by the attributes which makes them ready to generate patterns on the next level without sorting The question is can patterns of every structure be derived by merge-joining two of its parents using their existing order Apparently the parents that can be merge-joined to pro duce the patterns of c must have the same first L  2 at tributes Thus the only two parents that can qualify are CL and cL--l the two structures resulted by the removal of the last and the next-to-last column of c respectively However sometimes CL and CL do not have the same first L  2 at tributes Take B  AB AB on level L  4 for example Parent structure Z4 does not exist in the form of dB 4 but rather 4 4B since A  4B Thus patterns of B4 are not ordered by the same first L  2  2 attributes as 23  4B H On the other hand even if CL and CL-I do share the first L  2 attributes, they are not merge-joinable if they can not even be joined in the simple form An exam ple of such a case is A AB Proposition 6 Join Properry Ill The candidare parrerns of a srrucfure c on level L 2 3 can be derived by merge joining the parrerns of CL and CL-I if the following are sarisjed i CL and CL can be joined in the simple form andii CL and CL share rhe samefirsf L  2 afrribures Proof i guarantees only a single ordering of the patterns is required and ii guarantees they can be merge-joined 0 For instance, candidate patterns of 5 out of the 6 pattern structures on level 3 in Figure 2 can be derived by merge joining the patterns of their parents without re-sorting Overall around 80 of the structures can be derived by merge-join 5 Algorithm The main procedure for mining the HlFl framework is outlined in Algorithm I We start by generating frequent itemsets on the first level, where each pattern structure has one attribute The resulted frequent patterns are paired to generate candidate patterns on the second level Then on line 6 we scan the datnset to count the occurrences of each pattern on the current level, and on 7 we eliminate infre quent patterns. Next we generate all possible pattern struc tures for the next level Algorithm 2 and populate each pat tern structure by candidate patterns by the method described in the previous section Algorithm 3 We repeat the process until no more patterns can be generated Algorithm 1 HIFl\(Set0fAttributes 4 Dataset D Min Support minAup I generate frequent patterns of structures on the 1st level 2 L t 2 3 StructureL t structures on the 2nd level 4 CandL t join patterns on the 1 st level to generate can 5 while CandL  B do 6 countSupport\(D CandL 7 8 LtL+l 9 StructureL c StructureGen\(StructureL-1 A to CandidateCen\(StructureL 11 end while 12 return t.patternslt E StructureL didate patterns on the 2nd level eliminate candidates whose support are lower than min-sup Algorithm 2 StructureGen\(Set0fStructures parents I s t 0 2 for each p E parents do 3 4 5 end for 6 end for 7 returns for each child structure c ofp do S c S U e if all of e\222s parent structures exist and have non-empty pattern set Given all candidate patterns on a certain level the countsupport procedurescans the dataset once to count the occurrences of each pattern The counting itself is not a trivial problem, especially when 223exc1usive\224concepts are to be supported Efficient access to all valid items are essential for this purpose In HIFI we build an item free forthis task 


Algorithm 3 CandidateGen\(Set0fStructures Structures I for each c E Structures do 2 3 4 c.patterns t mergeloin\(cL CL-I 5 else 6 sort the patterns in c and cj 7 c.patterns t mergeJoin\(c cj 8 sort the patterns in c.patterns 9 end if 10 I I I2 Let cz and cj be parents of c with fewest patterns if joining CL and CL is less costly then for each parent ck of e and c does not take part in the join operation do for each pattern p E c.patterns do remove p from c.patterns if the sub-pattern of p with regard to cx does not exists in ck patterns 13 end for IJ endfor 15 end for The details of countsupport and the item tree structure canbefoundin Ill Based on the structures on the previous level a naive way of generating all the current structures is shown in Al gorithm 2 A child structure is to be generated only if all of its parents exist and have non-empty patterns Algorithm 2 is not optimal since each structure can have multiple succes sors and they are generated and tested multiple times The implementation of HIFI us~s an efficient algorithm Based on a parent structure TI  Tk we create a subset of its child structures using the following meth_ods iLadding a single attribute item TI to p\222such that Tx 5 TI ii adding a new attrib to an existing item to create a new item T\221 such that Tk 5 T\222 We prove that each structure on the new level is generated once and only once I I The core of HIFI is the candidate generation procedure shown in Algorithm 3 The rationale and the correctness are discussed in the previous section 6 Experimental Results Experiments on both synthetic datasets and real life datasets were camed out on a Pentium 111 machine run ning Linux OS 2.2.1 with a 166 MHz CPU 256M memory We implemented two other approaches Hierarchy and Tax onomy Hierarchy is based on the ML.T algorithms 5 Given a dataset with N attributes Hierarchy enumerates all N possible hierarchies of N levels and for each hierarchy we use ML.T to find frequent itemsets Furthermore we also find \223cross-level\224 patterns by combining frequent item sets on different hierarchy levels 5 Taxonomy is based on EstMerge IO A record with N attributes are encoded into 2N  1 items, such that a transaction with k records contains as many as k\(2N  1 items Taxonomy then performs an Apriori search in the extended transactions The synthetic datagenerator is parameterized by D num ber of records A number of attributes I average records per transaction N average number of distinct values for each attribute and P number of maximum patterns Both I and N are Poisson distribution parameters The number of items in a maximum pattern is decided by the Poisson distribution with X  112 and the number of attributes in an item is de cided by the Poisson distribution with X  A/2 We set N  30 and P  100 We use D?.A?.I as a name tem plate for instance, D100K.A5.18 is the dataset generated by setting D  100K A  5 and I  8 Figure 4 shows the scalability of the HIFI Algorithm Figure4\(a\showsthe execution time increases linearly with the size of the dataset Figure 4\(b however shows that the performance is heavily dependent on the number of at tributes since more attributes brings a combinatorial growth of the number of structures Shown in Figure 4\(c\the exe cution time also increases significantly as we increase I the average number of records in a transaction This is because when I becomes larger patterns will contain more items and require more passes of data scans to discover In Fig ure 4\(d,e,f\we compared the performance of HIFI against the other two approaches, namely Hierarchy and Taxonomy Despite the fact that these two approaches only discover a subset of the patterns discovered by HIFI HIFI is much more efficient because a large amount of candidate item sets are pruned by taking advantage of the anti-monotonic relationships in the tightly coupled mining space We also applied the HlFl algorithm on a real life dataset NETVIEW which is generated by a production network at a financial service company Events in the dataset are grouped into transactions by their timestamps based on a 30-second interval and on average each transaction contains about 10 events Each event has multiple at tributes which include Event Host Severity and Category Overall there are 241 event types 2526 hosts 6 severity levels and 17 event categories The patterns discovered by HIFI offers valuable insights into the under standing of the operational environment 7 Conclusion We present the search space of frequent patterns in a novel architecture where pattern structures are tightly cou pled in the anti-monotonic relationships Using such rela tionships and an efficient candidate generation algorithm based on merge-join, our approach is able to prune away a large amount of candidate patterns thus greatly improves the mining performance Unlike the level-wise mining algorithms used in Apriori 488 


7c4 sm HlFl  1 a D*.AS.IS minsupco.M d Scalabilily on DM1K.A4.15 e Scalability on DZOOK.A6.IS Figure 4 Execution Time and its extensions 3 81 our algorithm localizes the candi date generation and pruning procedure to each pattern Struc ture Given a set of query structures, we are able to find their frequent itemsets by exploring a much smaller search space than the other approaches. Furthermore our algorithm is re lational and attribute sensitive in that we do not encode the attribute information of a relational table into items The organization of the search space is also conducive to the in terpretation and analysis of the resulting patterns References I R Agrawal T Imielinski and A Swami Min ing asscxiation rules between sets of items in large databases In VLDB pages 207-216.1993 121 R Agrawal and R. Srikant Fast algorithms for mining association rules. In VLDB 1994 3 R Agrawal and R Srikant Mining sequential pat terns In ICDE 1995 4 Surajit Chaudhuri Data miningand database systems Where is the intersection In Bullelin of the 1EEE Computer Society Technical Committee on Data En gineering 1998 SI J Han and Y Fu Discovery of multiple-level associ ation rules from large databases In VLDB 1995 6 R Ng L Lakshmanan 1 Han and A Pang Ex ploratory mining and pruning optimizations of con strained associations rules In SICMOD pages 13-24 1998 7 R Srikant and R Agrawal Mining generalized asso ciation rules In VLDB pages 407-419 1995 SI R Srikant and R Agrawal Mining sequential pat terns generalization and performance improvements In EDBT 1996 9 R Srikant Q Vu and R Agrawal. Mining association rules with item constraints In SIGKDD pages 67-93 1997 lo Ramakrishnan Srikant and Rakesh Agrawal Min ing generalized association rules In VLDB Zurich Switzerland September 1995 I I Haixun Wang Chang-Shing Perng Sheng Ma and Philip S Yu Mining associations by pattern structure in large relational tables Technical report, IBM T J Watson 2002 489 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


