A way to improve personalized recommendation system efficiency Hualing  Liu Shanghai Institute of Foreign Trade Shanghai, China liuhl@shift.edu.cn Liu huarui Customer care department Alcatel-lucent shanghai bell Shanghai, China   Hanyu Shanghai Institute of Foreign Trade Shanghai, China    Abstract In this paper, we analyze the current situation and existing problems of web log format. Besides, we study the web log mining algorithm in order to determine the log format of the data and an approach to custom this log format. With the custom logging, we can gain users’ preferred information accurately reduce the complexity and workload of data preprocessing, thus improve the precision and efficiency of data mining Consequently, custom logging will improve personalized recommendation system efficiency in terms of data quality Keywords- web mining; custom web log; data quality recommendation system I   I NTRODUCTION  With the development of technologies of information management and data storage, it’s more convenient for sorts of web application systems to record large quantities of log data The data recorded differ in every application fields. Currently there are two ways of recording log data: the standardized log format and the customized log format. With the information gained by means of web technology and data mining algorithm we can promote website structure optimization. The information can also be used in the methods analyst of data mining and help discover knowledge. In this way, it assists managers and decision layer to judge and analyze. What’s more, it can provide personalized recommended service for users in line with their preferences. Despite of the simplicity convenience and generality of standardized log format, it also has disadvantages of less information and data redundancy. At the same time, customized log format shows specific features in accordance with demands of different application fields However, with changes in demands of data mining, these customized data are not enough to meet the needs in the future At the same time, the un-normalization of data also makes it more difficult to store and manage The data source needed by data mining of personalized recommendation is becoming increasingly diversified including log data, historical transaction records, user collection, user registration information and personal assessment and feedback information. Some researchers focus their study on improving the algorithm optimization of recommendation system in order to improve the precision of personalized recommendation system. However, it will radically promote to improve the precision if the customization of collected user data is targeted. These data show users preferences correctly. And, the more accurate these data are the easier to improve the precision of recommendation system At the same time, these data will also reduce the computing redundancy of recommendation system, thus increase the overall performance of personalized system II  W EB L OG  There are 3 key aspects in terms of personalization technology: recommendation system, user modeling and assessment. Recommendation system is the core of personalization, good system matters, which will have effects on a successful personalized application. The key factor to assess the performance of system is recommendation precision which is influenced by two things: recommended algorithm and data quality in this algorithm Web log is the main source of data in personalized recommendation. Consequently, the data quality of web log will have effects on the precision of recommendation and then influence the performance of personalized system A  Log Format Log format records log data, which can be used to follow the tracks of websites and service information, and make analysts. Currently, there are two kinds: Apache NCSA log format and IIS W3C log format. NCSA log format has four kinds: common log format \(CLF\ expand common log format ECLF\, wrong log format and customized log format. IIS W3C log format has 7 kinds: W3C expanded log format focused- recorded log format, NCSA common log format, IIS log format, ODBC-recorded log format, focused-recorded binary log format, wrong log format and customized log format. No matter Apache or IIS, expanded log format and customized log format are the most widely used. Compare the two and we can find that often used key words include: client IP, client user name, process-time of server, resources of client request, action of client request, server status, server bytes and the client browser Using these standardized log formats will bring advantages as follows   Generality. This log format can be used in different web application Identify applicable sponsor/s here sponsors  
2011 International Conference on Computational and Information Sciences 978-0-7695-4501-1/11 $26.00 © 2011 IEEE DOI 10.1109/ICCIS.2011.316 349 


  Simplicity. When the web application programs are deployed, the system will default setting log recording format. And the setting log recording format can be modified simply   Analyzability. The tools of log analysts can be applied in different situation with a little modification However, these formats also have disadvantages like   Poor adaptability. Different application situations demand recording different log information   The disorder of data. Due to the involvement of request information like CSS, pictures and script files in these standardized log formats, the data preprocessing becomes more complicated   Missing data. The diversification of user action may cause deletion of key information in recorded log B  Existing Problems Currently, the personalized recommendation research mainly focuses on improving algorithmic so as to improve the precision of personalized recommendation. However, people pay little attention to improve the data quality of web log when they are trying to promote the research on personalized recommendation precision based on web log. There are two main aspects 1  the research of data mining is often limited to standardized log format and its insufficient   Poor usability. Due to the problems like deletion redundancy and imperfection of standardized log data generated automatically by the system, it doesn 000 t work well to meet demands of personalized recommendation   Poor expandability. The constant log format makes it difficult to gain more information in the future 2  the log is customized in line with different needs in sorts of applications. There are some insufficient in customizing web log   Poor adaptability. The customized log format can only meet temporarily needs, but not the demands of sorts of information mining expansion   Non-standardization. The customized web log data has the problem of non-standardization and disunity. Due to the differences of application situation, the customized web logs are different to a great extent which is not a good thing for managing and further research in the future C  Resolution Customized log format is a remedy for insufficient above The basis of web using data mining in personalized recommendation is user modeling. The mining is in line with user modeling. So, we can discover the interests and preferences of users and predict their visit action in the future We may also recommend personalized information for users The collection of web using data in personalized user modeling should be in line with the demands of recommendation system If data collected can reflect users’ interests and preferences accurately, it will do good to improve the performance of personalized recommendation system. Or, the information recommended by the personalized system will not meet users demands. Then users may lost interest in personalized recommendation or even be enraged by some horrible recommendations. Finally, users will refuse to use this personalized recommendation system The user modeling is related to the data about their preferences and interests. So the collection should not excessively emphasize on users’ own data. There are two methods of collecting data   Ask about their preferences in an explicit way   Infer their preferences by inexplicit monitor In terms of different needs, the customized web log information can be categorized into 7 aspects   Users’ personal information like user name, gender birth date address and so on   Users’ historical transaction records   Users’ browsing history   Users’ favorites   Updates customized by users   Search criteria saved by users   Other information that can be used in expansion These fact data collected by application programs are recorded into web log files. People can clean and process the data and then save them in the data warehouse in case of any further data mining III  A LGORITHM  Web log algorithm often has four main types: association rules, classification, and cluster and hybrid algorithm A  Association rules The aim of association rules in data mining is to discover all the frequent patterns and strong association rules in dataset It has characteristics of discovering users’ interests and preferences about related goods and predicting users’ action in the future by using association rules and regression algorithm Apriori, one of the most influential algorithm which mines Boolean association rules of frequent item sets put forward by Agrawal, has the core ideas based on recursive algorithm of two-stage frequent item sets. In the first stage, discover all the candidate frequent item sets by means of multiple scans of datasets. In the second stage, generate association rules needed on the basis of candidate frequent item sets. The core codes are as figure 1 shows 
350 


 Figure 1  Apriori algorithm Agriori-gen function, the way to generate candidate item sets has L k-1 as its parameter, iterate all 002 k-1 002 set, and get back to supersets including all k terms. First of all, in the stage of inserting item sets, the program should use L k-1 set to add in L k-1 codes, just like figure 2  Figure 2  Insert the codes of item sets secondly, in the stage of clip, delete all the item sets c 001\031 C k  So, there will be no \(k-1\ subset of c. codes are shown in figure 3  Figure 3  Clip the codes of item sets Apriori algorithm does work in discovering frequent visit pattern even when the web log mining is modified. However there are some problems. When sorts of access modes are generated. Large quantities of candidate ones may be generated, too. This is due to several reasons   excessive length of single item set URL character string in log database transaction record   database needs multiple scans   it’s consuming to calculate access modes composed by URL character string Peng et al put forward a tf-idf weighting method of using apriori algorithm and being used for statistics in personalized recommendation model, which include three parts: resource description, User preferences extraction and personalized recommendation. User preferences can be gained from dinning users’ web access log. Then we can make recommendations about system resource on the basis of recommended model B  Classification In web mining, the most widely used grader is the method of concluding based on decision tree, which is determined by its features   The construction of decision tree doesn’t need guidance of domain knowledge, so it’s fit for detectknowledge discovery   It can deal with data of high dimensions   The form of knowledge expressed by decision tree is easy to understand   The classification of decision tree is simple and quick   Accuracy rate is high In later 1970s and earlier 1980s, J. Ross Quinlan studied and developed decision tree algorithm, which is called as ID3\(Iterative Dichotomiser, Iteration of the two differentiator then he expanded C4.5  and C5.0  algorithm, whose core idea is to construct a ED3 decision tree, and then make classifications of data on the basis of the decision tree. Dataset used to construct decision tree is called as training set. Dataset used to make classifications is called as test set. The datasets required to train focusing have been marked with classifier The disadvantages of ID3 algorithm are   Classified decision attributes may not be optimal   The result of classification may be effected by excessive branches   It fails to deal with consecutive numerical attributes and attributes with missing value C4.5 algorithm keeps the advantages of ID3 algorithm, and makes some progresses   It uses information gain ratio to choose attributes avoiding the preference to choose attributes with more values   It cuts branches during the process of making trees so that trees will not grow into a limitless height   It is able to fulfill the discretization of consecutive attributes   It is able to deal with missing the whole data. The core codes of C4.5 are showed as figure 4 C  Cluster Cluster is a kind of process in which categorizes users into similar classes without consideration of tabs of data classes but taking generating tabs of new classes through clusters into account. As described in the study of web log mining, the cluster algorithm can be categorized into 7 types   partitioning methods   hierarchical methods   density-based methods   model-based methods   Clustering high dimensional data   constraint-based clustering   conceptual clustering forall itemsets c 001\031 C k  do  forall k-1\-subsets s of c do  if s not 001\031 Lk-1 \ then delete c from C k  Insert into C k  select p.item 1 p.item 2 p.item k-1 q.item k-1  from L k-1 p, L k-1 q where p.item 1 q.item 1 p.item k-2 q.item k-2 p.item k-1 q.item k-1  1 L 1 large 1-itemsets 2 for k = 2; L k-1 000 k do begin  3 k  apriori-gen L k-1 New candidates 4 forall transactions t 001\031 D do begin  5\ C t subset\(C k t\; // Candidates contained in t 6 forall candidates c 001\031 C t do 7\       c.count 8 end  9\L k c 001\031 C k c.count 000 minsup 10 end   11  Answer U k  L k  
351 


 Figure 4  decision trees generated by C4.5 algorithm COBWEB is a popular and simple Incremental concept clustering method. Input objects are expressed by attribute value, and categorized into hierarchical methods as the form of classification trees. Classification trees are different from decision trees. Every node of classification trees corresponds to a concept, including the probability description of the concept and information like subtotal of objects under the node. The core steps are showed as figure 5  Figure 5  COBWEB algorithm The disadvantages of COBWEB are   The probability distribution of each attribute is not always independent from each other as assumed   The description about features of groups may increase the cost of updating and storage   The unbalancedness may give rise to more complexity of time and space   It does not adapt for huge database The CLASSIT algorithm of Gennari et al can be seen as an improvement of COBWEB algorithm and an Incremental clustering of dealing with consecutive data. There is a consecutive normal distribution stored in each attribute of each node. And it uses an improved classification utility to measure This metric is the integration of consecutive attributes. But COBWEB is a method to compute the sum of classification utility of discrete attributes D  Hybrid Algorithm Currently, the hot spot of recommendation system research is to use hybrid algorithm, which aims at improving the precision of recommendation. Burke put forward a way to achieve this by using classification and clustering algorithm in hybrid recommendation systems. Some of hybrid recommendation systems, such as the user modeling and technological research of personalized recommendation put forward by Ardissono et al, operate with colleting sorts of information about user preferences and using many kinds of heterogeneous recommend technology. Mobasher et al put forward a frame to separate conversation in webs of online users from online data preparation and mining in order to do processing. The technology based on clustering uses the same expression of the classification model of users’ action on webs as the model of content, thus provides a unified way of visiting for recommendation engine, so that the efficiency of personalized recommendation system is improved IV  C USTOMIZED L OG  In order to improve the performance and efficiency of recommendation system and meet the demands of personalized application, customized log data is in line with the algorithm However, with the increasing demands for information about recording logs in different applications, the form of customized log has not come into standardization, which gives rise to the difficulties of analyzing large quantities of log data. At the same time, the non-standardization of log form also leads to the difficulties of managing log data. Therefore, we should further study the standardization of customized log form so as to meet the demands of log analyzing and mining. There are two principles when customizing web log   Maximize the information   Minimize the redundancy Maximizing the information refers that the collected data which will be written into web log should be as much as possible so as to meet the sorts of demands of data in the future to the largest extent. Minimizing the redundancy refers that the generated web log should be the least on conditions that the content extended to users by personalized recommendation can meet their demands so as to reduce the consumption of performance of server and improve the efficiency of web data mining Having analyzed, there are following principles to be considered when customizing log data format   Structuralization; the standardization of log data format generated; it should be in line with the features of definition 002  T 002 test set 002  C 002 classification attribute 002  S 002 train set 002  Function C4.5 Algorithm 002 T,C,S 002 return to a decision tree 1 begin  2 if S = null then  3\return to false 002  4 if S i S& S i 001\031 C then  5\return to S i 002  6 if T= null then  7\return to a max {Probability{S i S i 001\031 C, n S 002  8 for T i T do  9 if T i 001\031 real then  10 begin  11\       B 1 min{T i T i T 002  12\       B k max{T i T i T 002  13\       for j = 2 to k-1 do 14\         B j B 1 j*\(A 1 A k k 002  15\         B = max{{GainRation\(S,T i T i 0010 B j GainRation\(S,T i T i B j  16 end  17\    A = max{GainRation\(S,T i T i T}; // the attribute with the largest information gain among test set 18\    a i A i A i 001\031 A },i=1 001\002 m;  //give sorts of values of attribute A to a i  19\    S i a i a i T},i=1,…,m;  //record a i which should be at a consists of S i  20\    A={a i a i T,i=1,…,m};  //return to a decision tree 21\    C4.5 Algorithm\(T-{A},C,S1\;C4.5 Algorithm\(T-{A},C,S2 C4.5 Algorithm\(T-{A},C,Sm  22  end   input 002 Object, Root print 002 a decision tree Function COBWEB \(Object, Root 1\ compute the roots again 2\f Root is a leaf then return to the leaf of adding a new object 002   else find the best host sub root for root and execute the next 3\ establish a new class 002  4\ put the roots together, and execute COBWEB \(Object, Merged node 002  Merged node is the root needed to be merged 5\ split roots, execute COBWEB \(Object, Root 002  6\     if the conditions of\(3 001 4 001 5\cannot be met 002 execute COBWEB Object, Best child of Root 002  7\eturn to Root 
352 


structuring data, easy to do statistical analysis and be visited by users as well as related actions classifications and clustering. The demands of recommending similar content should also be met   Standardization. The format of the log should be standard and normalized so as to share, cluster and manage data in the future   Consistency. When some information is changed, the related information recorded later should keep consistent   Simplicity. The log data recorded should be as few as possible on the premises that information is enough and without redundancy   Expandability. Enough consideration should be given in order to meet more demands for functions. Enough space should be ensuring to expand   Handle ability. Try to reduce the effect on server as least as possible. For instance, we can put the program about collecting log information in the client. At the same time, put the log data recorded to the rear of database table so that we don’t need to execute requiring database which will reduce the effect on performance of database and will not influence the efficiency of server programs On the basis of log system established on the principles above, we try to achieve the integrity, accuracy and consistency of customized log data without obviously influencing the performance and efficiency of system V  E PILOGUES  This paper analyzes some common mining algorithm and focus on discussing some classical algorithms, facing current studies about web log mining technology. Although present studies of web log mining often using standardized web log format, which has advantages of simplicity and normalization there are geneogenous disadvantages of this standardized format: little information, poor adaptability and short of expandability. This paper discusses the methods of improving the efficiency of personalized recommendation with customized web log format, and put forward some principles when we are customize weblog. With the way of customizing web log, we can gain user preferences accurately and reduce the complexity and workload of data preprocessing so that the precision and efficiency of data mining are improved Therefore, the method of customizing web log is able to improve the efficiency of personalized recommendation system in terms of data quality R EFERENCES   1  YANG DonfYuan 9 Shared Information Platform for Expressway 2  Management Information System[J 9 Journal of TongJI University 9 2002 9 28\(6 9 664~ 669 3  GUAN ji zhen,System Framework and Integration of ITS C 000R mmon Information PlatformsEJ 9 Journal of Transportation Systems Engineering and Information Technology 9 2002 \(11 002×\000\024\000\024  l6 4  SHI Mei 003\005 Lin 9 An Agent 003\005 based Framework for Cross 003\005 domain Cooperation of Virtual EnterpriseEJ 9 Computer Integrated Man 003\005  ufacturing Systems 9 2003 000x 9\(12 9 1072~ 1077 5  Jiawei Han, Micheline Kamber 9Ô\015\0379Õ*º9Ø*\004\030ÿ9Ø\022C\022Ê\023D  A  030™\027j\027 027‡\033N\025\(\012\010\026 035\003 M  015|\012i9æ\031Í\032¾\023 003 014´ \003 2006 6  J.Ross Quinlan. Induction of Decision Trees[J Ma ch i n e L e ar ning  1986,1: 81-106 7  J. Ross Quinlan. C4.5:Programs for machine learning[M   Mo r g an  Kaufmann Publishers,1993 8  Ross Quinlan[OL 010-10-29  h t t p   w w w r ule que s t co m P e r s o nal    9  John H. Gennari, Pat Langley, Doug Fisher. Models of incremental concept formation[J  A r t i fic i a l  I n t e lli gen c e 19 89 40 1  61   Burke, R.: Hybrid Web recommender systems I n Br us il o v s ky P  Kobsa, A., Nejdl, W., eds.: The Adaptive Web: Methods and Strategies of Web Personalization, Lecture Notes in Computer Science 2007,4321:377-408   Ardissono, L., Gena, C., Torasso, P., Bellifemine, F., Difino, A., Negro B. User modeling and recommendation techniques for personalized Electronic Program Guides[C  I n  Pers on a l i zed Di gi t a l T e l e vi s i on  Targeting Programs to Individual Users. Kluwer Academic Publishers 2004, 1:3-26   Bamshad Mobasher, Honghua Dai, Tao Luo, Yuqing Sun, and Jiang Zhu. Integrating Web Usage and Content Mining for More Effective Personalization. Lecture Notes in Computer Science , Electronic Commerce and Web Technologies, 2000, 165-176  
353 


ct t c with respect to minimum support gives a super set of all interesting frequent itemsets Figure 1 shows an example of interesting and noninteresting frequent itemsets. The frequent itemsets {b, d and {a, b, c} are not interesting because the median is outside the time interval\f c  ctt It is also clear that the frequent itemset {a, b, c} is too old to be interesting. On the other hand, {a, e, f} is interesting because the median of its time series is within the determined interval.  A necessary condition for interesting frequent itemset A is    pQuantile 1 2 with p A   minsup sup Where is the  of the time series of set A The  of the ordered set of values is a number puantileQ x such that a proportion p of the set are less than or equal to x  Figure 1. Interesting and non-interesting frequent itemsets For example, the  \(called also the quartile an ordered set of values is a value 0.25Quantile x  such that 25% of the values of the set are below that value, and 75% of the values are  above that value. The Quantile  \(same as the median values are less than or equal to it and half are greater than or 


equal to it. The usage of the above condition means finding all frequent itemsets from the transactions within the time interval 0.5 f ct t minsup c 2 with respect to minimum support minsup , gives a super set of all frequent itemsets.  This is because when the frequent itemset A is interesting, then and because the median is the central values of the frequent itemset, then we need at least half of the interval\f c ct t  If the itemset is interesting, then it should definitely be frequent. Therefore, supporting transactions should be in the interval 2minsup f ct ct . This condition can be used either as a preprocessing step to search for frequent itemsets within the determined interval, or as an extension to the Apriori algorithm to prune non-interesting frequent itemsets. Figure 2 is an example of applying this condition In this example, we have the minsup = 10. When using this condition, the search for frequent itemsets will be within the interval \f c ct t with minsup = 10/2 = 5 From that, we get a super set of all interesting frequent itemsets. The 3-candidate frequent itemset with minsup = 5 is {a,b,e}. On the other hand, the 3-candidate frequent itemsets with minsup =10 are {a,b,c} and {a,b,e Candidates for interesting frequent itemsets are \(interesting frequent itemsets are underlined a}, {b}, {c}, {e a,b}, {a,c}, {a,e}, {b,e 


a,b,e On the other hand, all frequent itemsets are a}, {b}, {c}, {e a,b}, {a,c}, {a,e}, {b,c}, {b,e a,b,c}, {a,b,e  Figure 2. An example of using the necessary condition Using this method, we reduced the cost of searching for frequent itemsets. This method can be used to improve the search strategy implemented by the Apriori algorithm. A time series fs  of a frequent itemset f  is an ordered sequence of time stamps of the covering transactions. This set will be helpful in finding out what kinds of changes are occurring in which time periods. This can give an indication of the behavior of users with respect to time. We can find why some pages are frequently visited and why others not Through that, we can get a better view about our pages, and also about the users visiting our website. We can predict the future behavior of users depending on the periodical behavior of the users that we have already extracted V. EXPERIMENTAL WORK For the experiments we used a dataset that contains the preprocessed and   filtered sessionized data for the main DePaul CTI Web server \(http://www.cs.depaul.edu data is   based on a random sample of users visiting this site for a 2 week period during April of 2002. Each session begins with a line of the form SESSION #n \(USER_ID = k Where n in the session number and k is the USER_ID There may be multiple   consecutive sessions corresponding to the same USER_ID \(repeated users with a dashed line. Within a given session, each line corresponds to one page view access. Each line in a session is a tab delimited sequence of 3 fields: time stamp, page view accessed, and the referrer. The time stamp represents the number of seconds relative to January 1, 2002 In order to illustrate what we made in our experimental work, let us take a sample of three sessions that could be found in the web log file SESSION #1 \(USER_ID = 11 9374553 /news/default.asp /news 9374590 /people/search.asp?sort=pt /news/default.asp 


9374610 /people/facultyinfo.asp people/search.asp?sort=pt 9374685 /news/default.asp /people/facultyinfo.asp 9374720 /courses/ /news/default.asp  SESSION #2 \(USER_ID = 22 9185108 /admissions/ /programs 9185138 /news/default.asp /news/default.asp  SESSION #3 \(USER_ID = 33 9226945 /people/search.asp?sort=pt /people 9226975 /people/facultyinfo.asp people/search.asp?sort=pt 9227072 /advising/ /courses 9227098 /people/search.asp?sort=pt /news/default.asp  After preprocessing which includes the removal of redundant URLs within every session, considering the time stamp of the first request in every session as the time stamp of the session, and the removal of the referrer we get SESSION #1 \(USER_ID = 11 9374553 /people/search.asp?sort=pt people/facultyinfo.asp news/default.asp courses  SESSION #2 \(USER_ID = 22 9185108 /admissions news/default.asp  SESSION #3 \(USER_ID = 33 9226945 /people/search.asp?sort=pt people/facultyinfo.asp advising news/default.asp Then we build the set of items which consists of the set of unique URLs of all sessions Set of items I = {/people/search.asp?sort=pt people/facultyinfo.asp news/default.asp, /courses/, /admissions/, /advising We then used the Apriori algorithm to mine for frequent itemsets. We set the minimum support to be 20% which is 


equivalent to minimum support count equal to 2 Candidate 1-itemset news/default.asp \(support count =3 people/search.asp?sort=pt\( support count =2 people/facultyinfo.asp\(support count =2 Candidate 2-Itemset news/default.asp, /people/search.asp?sort=pt}\(support count = 2 news/default.asp, /people/facultyinfo.asp}\(support count 2 people/search.asp?sort=pt, /people/facultyinfo.asp support count= 2 Candidate 3-Itemset news/default.asp, /people/search.asp?sort=pt people/facultyinfo.asp\(support count= 2 and because the available dataset was not suitable to run our experiments, the main goal of our future work is to find a real life representative dataset that fits our experimental needs or use one of the available dataset generators to generate a suitable one Candidate 3-Itemset represents the frequent itemset f . The set of covering transactions for the frequent itemset f  are in session #1 and session #3 which have the time  stamps 9374553 and 9226945 respectively. The time stamps are then ordered to get the time series corresponding to the frequent itemset f :  REFERENCES 1] Agrawal, R., Imieliski, T., and Swami, A. Mining association rules between sets of items in large databases. In: SIGMOD NY, USA, ACM Press \(1993 207-216 S = {9226945, 9374553 The experimental results we have got were not representative enough to reflect the applicability of our approach. Such bad results were expected because the dataset we used was not the one we need. On one hand because the time interval of the transactions is too small \(2 weeks other hand, the transactions represent user sessions in a university website, and usually the transactions recorded in the web log file of university websites are not more than course registration, search for an assignment, etc.  The dataset we need to run an effective experiment should have 


a long time interval for example a dataset that represents customers transactions in a retail website within two years As far as we know, such dataset does not exist especially when we talk about time stamped datasets. This lack of such datasets amplifies the need to develop time stamped transactional datasets generators which is discussed in [2 2]  Asem Omari, Regina Langer, and Stefan Conrad. TARtool: A Temporal Dataset Generator for Market Basket Analysis. In Proceedings of the 4th International Conference on Advanced Data Mining and Applications \(ADMA 2008 2008. Springer Lecture Notes in Artificial Intelligence \(LNAI August 2008 3] Asem Omari. Data Mining for Improved Website Design and Enhanced Marketing.  In Yukio Ohsawa and Katsutoshi Yada editors, Data Mining for Design and Marketing,  volume 5 of Chapman Hall/CRC Data Mining and Knowledge Discovery Series, chapter  6. Chapman Hall/CRC, First edition, November 2008 4] Asem Omari, Alexander Hinneburg, and Stefan Conrad Temporal Frequent Itemset Mining. In Proceedings of the Knowledge Discovery, Data Mining and Machine  Learning workshop \(KDML 2007 5]  B. Ozden, S. Ramaswamy and A. Silberschatz: Cyclic Association Rules. In: ICDE  98: Proceedings of the  Fourteenth International Conference on Data Engineering,  Washington, DC USA, IEEE Computer Society \(1998 412421 6] C. Antunes and A. Oliveira: Temporal Data Mining: An Overview. In: Proceedings of the Workshop on Temporal Data Mining, of Knowledge Discovery and Data Mining KDD01, San Francisco, USA \(2001 VI. APPLICATION FIELDS This method can be applied in different fields. One application field is in search engine log files for example to find out the most frequently searched keywords in the last time period. Another application field is in web usage mining for example to find out the most visited web pages in the last 3 months in some website. It can also be applied to a transaction dataset in a physical store or business to find out the most frequently bought products or used services in the last time period. Any other problem that needs to study the behavior of some items with respect to time can be a good application field 


7] Ding-An Chiang, Shao-Lun Lee, Chun-Chi Chen, and MingHua Wang. Mining Interval Sequential Patterns. International Journal of Intelligent Systems, 20\(3 359373 8] J. Han and M. Kamber: Data Mining Concepts and Techniques Morgan Kaufmann Publishers, San Francisco \(2001 9] Kaidi Zhao and Bing Liu. Visual Analysis of the Behavior of Discovered Rules. In Workshop Notes in ACM SIGKDD-2001 Workshop on Visual Data Mining, San  Francisco, CA, 2001 10] Mannila, H., Toivonen, H.: Multiple uses of frequent sets and condensed representations. In: KDD, Portland, USA \(1996 pp\(189-194 11] Q. Yang and X. Wu: 10 Challenging Problems in Data Mining Research. Volume 5, World Scientific Publishing Company International Journal of Information Technology and Decision Making \(2006 597604  VII. SUMMARY AND FUTURE WORK In this paper, we presented a new measure to mine for interesting frequent itemsets. This measure is based on the idea that interesting frequent itemsets are mainly covered by many recent transactions.  This measure reduces the cost of searching for frequent itemsets by minimizing the search interval. Furthermore, it can be used to improve the search strategy implemented by the Apriori algorithm 12] Sheikh, L.M. Tanveer, B. Hamdani, M.A. Interesting Measures for Mining Association Rules. In proceedings of the 8th International Multi-topic Conference INMIC, 2004. pp \(641 644 13] W. Lin, M. A. Orgun and G. Williams: An Overview of Temporal Data Mining. In:  Proceedings of the 1st Australian Data Mining Workshop, Canberra, Australia,   University of Technology, Sydney \(2002 8390  


processed, until all attributes in A have been exhausted and we get the final fuzzy version of the dataset E. At the end, all attributes would have categorical values for each record in the fuzzy dataset E. Thus, by applying the aforementioned pre-processing, given any dataset D with initial crisp attributes \(set A fuzzy records. And each of these is further iteratively converted to generate more fuzzy records, until each crisp attribute has been taken into account and we get our final fuzzy dataset E  VI. COUNTING IN FUZZY ASSOCIATION RULES Crisp ARM algorithms calculate support of itemsets in various ways Record-by-record counting; as in Apriori Counting using tidlists; for example, ARMOR Tree-style counting; as in FPGrowth In this section, we describe how counting is done in various fuzzy ARM algorithms using membership functions and how our pre-processing technique can be used to generate fuzzy datasets which can be used by any fuzzy ARM algorithm Table I. t-norms in Fuzzy sets  t-norm TM\(x, y x, y TP\(x, y TW\(x, y x + y ? 1, 0  A. Counting in Fuzzy Apriori The first pass of Apriori counts item occurrences to determine the large 1-itemsets. Any subsequent pass k consists of two phases. First, the large itemsets found in the k-1 the kth pass. Next, the database is scanned and the supports of candidate itemsets are counted. In any pass k, each record is selected in a sequential manner and the supports for the candidate itemsets, occurring in that particular record, are increased by one. Thus, the counting in Apriori is done in a record-by-record manner Fuzzy Apriori is a modified version of the original Apriori algorithm, and can deal with fuzzy records. Fuzzy 


Apriori counts the support of each itemset in a manner similar to the counting in Apriori; the only difference is that it calculates sum of the membership function corresponding to each record where the itemset exists. Thus the support for any itemset is its sum of membership functions over the whole fuzzy dataset. This calculation is done with the help of a suitable t-norm \(see Table I We generated the fuzzy dataset required for Fuzzy Apriori using our pre-processing methodology. The crisp dataset \(FAM95 sections 4 and 5, and the resultant fuzzy dataset was used as input to the Fuzzy Apriori algorithm. More details of how FPrep was used for pre-processing before Fuzzy Apriori can be found in [21] \(though the pre-processing methodology used in [21] is not explicitly names as FPrep  B. Counting in Fuzzy ARMOR Each record in the dataset is marked by a unique number called transaction id \(tid order. A tid-list of an itemset X is an ordered list of TIDs of transactions that contain X. ARMOR is based on the Oracle algorithm and is totally different from Apriori in that it calculates the support of each itemset by creating its tidlist and counting the number of tids in the tidlist. The count of any itemset is equal to the length of its corresponding tidlist The tidlist of an itemset can be obtained as the intersection of the tidlists of its mother and father \(for example, ABC is generated by intersecting AB and BC started off using the tidlists of frequent 1-itemsets In a similar manner, Fuzzy ARMOR also creates the tidlist for each itemset by intersecting the tidlists of its mother and father itemsets. And for each tid in the tidlist, it calculates the membership function  \(again using a suitable t-norm support for an itemset is thus the sum of the membership functions associated with each tid in its tidlist We have also developed an initial implementation of Fuzzy ARMOR [21]. This algorithm uses the same fuzzy dataset as input as that was used for Fuzzy Apriori. There is no change, whatsoever, made to this fuzzy dataset after it was generated initially \(for Fuzzy Apriori processing technique. Even though Fuzzy Apriori and Fuzzy 


ARMOR operate in different ways and process data differently, the fuzzy dataset created using our preprocessing technique can be used as input for both the algorithms. This is because the fuzzy dataset is generated in a standard manner of fuzzy data representation \(as described in section 5 ARM algorithm. More details of how FPrep was used for pre-processing before Fuzzy ARMOR can be found in [21 C. Counting in Fuzzy FPGrowth FPGrowth uses a compact data structure, called frequent pattern tree \(FP-tree structure and stores quantitative information about frequent patterns. Only frequent length-1 items will have nodes in the tree, and the tree nodes are arranged in such a way that more frequently occurring nodes will have better chances of sharing nodes than less frequently occurring ones. FP-treebased pattern fragment growth mining starts from a frequent length-1 pattern, examines only its conditional pattern base constructs its \(conditional recursively with such a tree. The support of any itemset can be calculated from its conditional pattern base and from the nodes in the FP-tree, which correspond to the itemset Fuzzy FPGrowth also works in a similar manner by constructing an FP-tree, with each node in the tree corresponding to a 1-itemset. In addition, each node also has a fuzzy membership function  corresponding to the 1itemset contained in the node. The membership function for each 1-itemset is retrieved from the fuzzy dataset while constructing the FP-tree, and the sum of all membership function values for the 1-itemset is its support. The support for a k-itemset \(where k ? 2 corresponding to the itemset by using a suitable t-norm  VII. RELATED WORK 3] describes the current status and future prospects of applying fuzzy logic to data mining applications. In [4] and 5], the authors discuss two facets of fuzzy association rules namely positive rules and negative rules, and describe briefly a few rule quality measures, especially for negative rules. The authors in [6] take this discussion further by describing in detail the theoretical basis for various rule quality measures using various t-norms, t-conorms, S 


implicators, and residual implicators. [8] and [9] illustrate quality measures for fuzzy association rules and also show how fuzzy partitioning can be done using various t-norms, tconorms, and implicators. The authors in [8] go a step further and do a detailed analysis of how implicators can be used in the context of fuzzy association rules Last, [7] and [10] take diametrically opposing stands on the usefulness of fuzzy association rules. The authors of [7 do a data-driven empirical study of fuzzy association rules and conclude that fuzzy association rules, after all, might not be as useful as thought to be. But the authors of [10 defended the usefulness of fuzzy association rules, by doing more experimental work, and then corroborating their stand through the successful results of their empirical research In addition to the fuzzy clustering based methodology briefly mentioned in [7], [19] and [20] describe methodologies for generating fuzzy partitions \(using nonfuzzy hard clustering original dataset into a fuzzy form. [19] uses k-Medoids CLARANS  CURE for the same. The hard clusterings so generated are then used to derive the fuzzy partitions. In such cases, where hard clustering is used, typically the middle point of each fuzzy partition is taken as reference \(membership  = 1 with respect to which the memberships for other values belonging to that partitions are calculated. [22] goes even a step further, and uses Multi-Objective Genetic Algorithms in the process for finding fuzzy partitions. Such methodologies which use hard clustering, or non-fuzzy methods are one way to obtain fuzzy versions of original datasets before any fuzzy ARM can ensue. But, with FPrep we use only fuzzy methods, fuzzy clustering to be more specific, in order to ensure consistency, and to have the notion of fuzziness maintained throughout. The main motive behind doing so is to ensure that any processing preceding the actual fuzzy ARM process, also involves fuzzy methods. Thus, the whole end-to-end process, right from the moment the processing of original crisp dataset starts till the time the final frequent itemsets are generated, involves only fuzzy methods and is holistic in nature  VIII. EXPERIMENTAL RESULTS 


The experimental results of FPrep as compared to other such non-fuzzy methods, on the basis of various parameters, are described below  A. Results from First Dataset We have tested FPrep against the automated methods for generating fuzzy partitions proposed in [19], [20]. These use hard clustering algorithms CLARANS \(k-Medoids CURE respectively. The main tangible metric to compare our approach to the ones proposed in [19], [20] is the time taken for execution. And, the dataset used for doing so is the USCensus1990raw dataset http://kdd.ics.uci.edu/databases/census1990 has around 2.5M transactions, and we have used nine attributes present in the dataset, of which five are quantitative and the rest are binary. The attributes, with their respective number of unique values, on which the evaluation was done, are as follows Age - 91 unique values Hours  100 unique values Income1  55089 unique values Income2  13707 unique values Income3  4949 unique values  Using each of the three methodologies being evaluated three fuzzy partitions were generated for each of these attributes. The results are illustrated in fig. 7, which has the y-axis in log10 form for ease of perusal.  The same are also available in Table II. As far as speed is concerned, for attributes having very low number of unique values \(~ 100 there is no big difference among the three methods. FPrep and CURE perform five times better than CLARANS for the attributes Age and Hours, both of which have around 100 unique values. But, the real differences become apparent for higher number of unique values. For attribute Income3, with 4949 unique values, we see that FPrep is nearly nine times faster than CURE, and nearly 2672 times faster than CLARANS, and for attribute Income2, with 13707 unique values, it is 27 times faster than CURE, and 13005 times faster than CLARANS. For attribute Income1, having 55089 unique values, FPrep is 46 times faster than CURE. No comparison was done with CLARANS for this attribute, as 


the time needed for execution exceeded 100000 seconds Thus, from this analysis we see that FPrep, which uses FCM clustering, clearly outperforms the CLARANS and CURE based methods on the basis of speed. The execution times for CLARANS and CURE mentioned in fig. 7 and Table II do not include the time required to create fuzzy sets, and calculate the membership value  for each numerical data point in every fuzzy set for the numerical attribute under consideration. These times also do not take into account the time required to transform crisp numerical attributes to fuzzy attributes, and derive the fuzzy dataset from the original crisp dataset The fuzzy partitions generated for each of the five numerical attributes for the USCensus1990raw dataset are shown in Table III. Coincidentally, generating three fuzzy partitions for each numerical attribute seemed a perfect fit In addition to the superior speeds achieved by FPrep, as illustrated in fig. 7 and Table II, Table III indicates the semantics and the quality of the fuzzy partitions generated by FPrep. Moreover, the number of frequent itemsets generated by a fuzzy ARM algorithm \(like fuzzy ARMOR and fuzzy Apriori minimum support threshold, is illustrated in fig. 8   Fig. 7. Algorithm, numerical attribute comparison based on speed \(log10 seconds   Fig. 8. Number of frequent itemsets for various minimum support values  B. Results from Second Dataset We have also applied FPrep on the FAM95 dataset http://www.stat.ucla.edu/data/fpp transactions. Of the 23 attributes in the dataset, we have used the first 18, of which six are quantitative and the rest are binary. For each of the six quantitative attributes, we have generated fuzzy partitions using FPrep. A thorough analysis with respect to execution times, has already been performed on the USCensus1990raw dataset \(which is manifolds bigger in size than the FAM95 dataset both on the basis of number of transactions and number of unique values for numerical 


attributes dataset has been done solely to provide further evidence of the quality and semantics of the fuzzy partitions generated by FPrep. The details of the same are in Table IV. In this case, the number of fuzzy partitions is different for different numerical attributes. Thus, the number and type of fuzzy partitions to be generated is totally dependent on the attribute under consideration. A graphical representation of the fuzzy partitions generated for the attribute Age has already been provided in fig. 5, and clearly shows the Gaussian nature of the fuzzy partitions. The nature and shapes of fuzzy partitions for the rest of the attributes are also similar. Last, the number of frequent itemsets generated for different minimum support values is illustrated in fig. 8  C. Analysis of Results With FPrep, we can analyze and zero in on the number and type of partitions required based on the semantics of the numerical attributes, which the methods detailed in [19 20] do not necessarily facilitate. Then, FPrep, backed by FCM clustering, takes care of the creating the fuzzy partitions, especially assigning membership values for each numerical data point in each fuzzy partition. In section 8.A we have already shown that FPrep is nearly 9 to 44 times faster than the CURE-based method, and 2672 to 13005 times faster than the CLARANS-based method. FPrep is not only much faster than other related methods, but also generates very high quality fuzzy partitions \(Table III and IV much user-intervention. We have created a standard way of representing any fuzzy dataset \(converted from any type of crisp dataset efficacy of the same is corroborated by the successful implementation of Fuzzy Apriori and Fuzzy ARMOR on the fuzzy dataset \(converted from crisp version of FAM95 dataset an initial implementation of Fuzzy ARMOR, are very encouraging. FPrep, when used in conjunction with these fuzzy ARM algorithms, generates a pretty good number of high-quality frequent itemsets \(fig. 8 frequent itemsets generated for a particular minimum support is same, irrespective of the fuzzy ARM algorithm 


used IX. CONCLUSIONS In this paper we have highlighted our methodology, called FPrep, for ARM in a fuzzy scenario. FPrep is meant for seamlessly and holistically transforming a crisp dataset into a fuzzy dataset such that it can drive a subsequent fuzzy ARM process. It does not rely on any non-fuzzy techniques and is thus more straightforward, fast, and consistent. It facilitates user-friendly automation of fuzzy dataset 1 0 1 2 3 4 5 Age - 91 Hours - 100 Income3 4949 Income2 13707 Income1 55089 Ti m e lo g1 0 se co nd s Numerical Attribute - Number of Unique Values FCM CURE CLARANS 0 500 1000 1500 2000 2500 


3000 0.075 0.1 0.15 0.2 0.25 0.3 0.35 0.4 N o o f F re qu en t I te m se ts Minimum Support USCensus1990 FAM95 generation through FCM, and subsequent steps in preprocessing with very less manual intervention and as simple and straightforward manner as possible. This methodology involves two distinct steps, namely creation of appropriate fuzzy partitions using fuzzy clustering and creation of fuzzy records, using these partitions, to get the fuzzy dataset from the original crisp dataset FPrep has been compared with other such techniques, and has been found to better on the basis of speed. We also illustrate its efficacy on the basis of quality of fuzzy partitions generated and the number of itemsets mined by a fuzzy ARM algorithm which is preceded by FPrep. This preprocessing technique provides us with a standard method of fuzzy data \(record that it is useful for any kind of fuzzy ARM algorithm irrespective of how the algorithm works. Furthermore, this pre-processing methodology has been adequately tested with two disparate fuzzy ARM algorithms, Fuzzy Apriori and Fuzzy ARMOR, and would also work fine with other fuzzy ARM algorithm REFERENCES 1] Zadeh, L. A.: Fuzzy sets. Inf. Control, 8, 338358 \(1965 2] Chen G., Yan P., Kerre E.E.: Computationally Efficient Mining for Fuzzy Implication-Based Association Rules in Quantitative Databases. International Journal of General Systems, 33, 163-182 


2004 3] Hllermeier, E.: Fuzzy methods in machine learning and data mining Status and prospects. Fuzzy Sets and Systems. 156, 387-406 \(2005 4] De Cock, M., Cornelis, C., Kerre, E.E.: Fuzzy Association Rules: A Two-Sided Approach. In: FIP, pp 385-390 \(2003 5] Yan, P., Chen, G., Cornelis, C., De Cock, M., Kerre, E.E.: Mining Positive and Negative Fuzzy Association Rules. In: KES, pp. 270-276 Springer \(2004 6] De Cock, M., Cornelis, C., Kerre, E.E.: Elicitation of fuzzy association rules from positive and negative examples. Fuzzy Sets and Systems, 149, 7385 \(2005 7] Verlinde, H., De Cock, M., Boute, R.: Fuzzy Versus Quantitative Association Rules: A Fair Data-Driven Comparison. IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics 36, 679-683 \(2006 8] Dubois, D., Hllermeier, E., Prade, H.: A systematic approach to the assessment of fuzzy association rules. Data Min. Knowl. Discov., 13 167-192 \(2006 9] Dubois, D., Hllermeier, E., Prade, H.: A Note on Quality Measures for Fuzzy Association Rules. In: IFSA, pp. 346-353. Springer-Verlag 2003 10] Hllermeier, E., Yi, Y.: In Defense of Fuzzy Association Analysis IEEE Transactions on Systems, Man, and Cybernetics - Part B Cybernetics, 37, 1039-1043 \(2007 11] Agrawal, R., Imielinski, T., Swami, A.N.: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Record, 22, 207216 \(1993 12]  Agrawal, R., Srikant, R.: Fast Algorithms for Mining Association Rules. In: VLDB, pp. 487-99. Morgan Kaufmann \(1994 13] Han, J., Pei, J., Yin, Y.: Mining Frequent Patterns without Candidate Generation. In: SIGMOD Conference, pp. 1-12. ACM Press \(2000 14] Han, J., Pei, J., Yin, Y., Mao, R.: Mining Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree Approach. Data Mining and Knowledge Discovery, 8, 5387 \(2004 15] Pudi V., Haritsa J.R.: ARMOR: Association Rule Mining based on Oracle. CEUR Workshop Proceedings, 90 \(2003 16] Dunn, J. C.: A Fuzzy Relative of the ISODATA Process and its Use in Detecting Compact, Well Separated Clusters. J. Cyber., 3, 32-57 1974 17] Hoppner, F., Klawonn, F., Kruse, R, Runkler, T.: Fuzzy Cluster Analysis, Methods for Classification, Data Analysis and Image Recognition. Wiley, New York \(1999 


18] Bezdek J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms. Kluwer Academic Publishers, Norwell, MA \(1981 19] Fu, A.W., Wong, M.H., Sze, S.C., Wong, W.C., Wong, W.L., Yu W.K. Finding Fuzzy Sets for the Mining of Fuzzy Association Rules for Numerical Attributes. In: IDEAL, pp. 263-268. Springer \(1998 20] Kaya, M., Alhajj, R., Polat, F., Arslan, A: Efficient Automated Mining of Fuzzy Association Rules. In: DEXA, pp. 133-142. Springer \(2002 21] Mangalampalli, A., Pudi, V. Fuzzy Association Rule Mining Algorithm for Fast and Efficient Performance on Very Large Datasets In FUZZ-IEEE, pp. 1163-1168. IEEE \(2009 22] Kaya, M., Alhajj. Integrating Multi-Objective Genetic Algorithms into Clustering for Fuzzy Association Rules Mining. In ICDM, pp. 431434. IEEE \(2004  Table II. Algorithm, numerical attribute comparison based on speed \(seconds  Algorithm Age - 91 Hours - 100 Income3 - 4949 Income2 - 13707 Income1 - 55089 FCM 0.27 0.3 3.13 6.28 79.4 CURE 0.25 0.25 28.67 163.19 3614.13 CLARANS 1.3 1.34 8363.53 78030.3 Table III. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions Age Old Middle Aged Young Hours More Average Less Income1 High Medium Low Income2 High Medium Low Income3 High Medium Low  Table IV. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions AGE Very old Around 25 Around 50 Around 65 Around 35 HOURS Very High Zero Around 40 Around 25 INCHEAD Very less Around 30K Around 50K Around 100K INCFAM Around 60K Around 152K Around 96K Around 31K Around 8K TAXINC Around 50K Around 95K Around 20K Very less FTAX Around 15K Very less Around 6K Very high Around 33K  


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


