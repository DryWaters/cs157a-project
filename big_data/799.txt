Mining Negative Relevance Feedback for Information Filtering Yuefeng Li Abdulmohsen algarni School of Information Technology Queensland University of Technology Brisbane Australia f y2.li,a1.algarni g qut.edu.au Sheng-Tang Wu Dept of Information Science and Applications Asia University Dublin Taiwan swu@asia.edu.tw Yue Xue School of Information Technology Queensland University of Technology Brisbane Australia yue.xu@qut.edu.au 
Abstract It is a big challenge to clearly identify the boundary between positive and negative streams Several attempts have used negative feedback to solve this challenge however there are two issues for using negative relevance feedback to improve the effectiveness of information 002ltering The 002rst one is how to select constructive negative samples in order to reduce the space of negative documents The second issue is how to decide noisy extracted features that should be updated based on the selected negative samples This paper proposes a pattern mining based approach to select some offenders from the negative documents where an offender can be used 
to reduce the side effects of noisy features It also classi\002es extracted features i.e terms into three categories positive speci\002c terms general terms and negative speci\002c terms In this way multiple revising strategies can be used to update extracted features An iterative learning algorithm is also proposed to implement this approach on RCV1 and substantial experiments show that the proposed approach achieves encouraging performance 1 Introduction Traditional IF models were developed based on a termbased user pro\002le approach see 13 8 The a dv antage of term-based pro\002les is ef\002cient computational performance as well as mature theories for term weighting which have 
emerged over the last couple of decades from the information retrieval IR and machine learning communities However term-based pro\002les suffer from the problems of polysemy and synonymy As IF systems are sensitive to data sets it is still a challenging issue to signi\002cantly improve the effectiveness of IF systems To overcome the limitations of term based approaches data mining pattern mining based techniques have been used for information 002ltering since patterns are more discriminative and arguably carry more semantics than terms One special 002ltering task was to extract usage patterns from Web logs  Another promis ing techniques were pattern taxonomy 
models PTM 21 that disco v ered closed sequent ial patterns in text documents where a pattern was a set of terms that frequently appeared in paragraph Pattern based approaches have shown encouraging improvements on effectiveness However two challenging issues have arisen when pattern mining techniques were introduced for IF systems The 002rst one is how to deal with low frequency patterns because the measures used from data mining e.g support and con\002dences to learn the patterns turn out be not suitable in the 002ltering stage The second issue is ho w to effectively use negative feedback to revise extracted features including patterns and terms for information 002ltering 
Many people believe that there are plenty negative information available and negative documents are very useful because they can help users to search for accurate information However whether negative feedback can indeed largely improve 002ltering accuracy is still an open question The existing methods of using negative feedback for IF can be grouped into two approaches The 002rst approach is to revise terms that appear in both positive samples and negative samples e.g Rocchio based models and SVM based 002ltering m odels This heuristics is obvious when people assume that terms are isolated atoms The second approach is based on how often 
terms appear or do not appear in positive samples and negative samples e.g probabilistic models a nd BM25 13 Ho wever usually people view terms in multiple perspectives when they attempt to 002nd what they want They normally use two dimensions speci\002city and exhaustivity for deciding the relevance of documents paragraphes or terms For example JDK is a speci\002c term for Java Language and LIB is more general than JDK because it is also frequently used for C and C Based on this observation this paper proposes a pattern mining based approach for using negative feedback It 002rstly extracts an initial list of terms from positive documents 
and selects some constructive negative documents or called offenders It then extracts terms from negative patterns in selected negative documents It also classi\002es all terms into three categories the positive speci\002c terms general terms and negative speci\002c terms In this way multiple revising strategies are used for terms in different categories In the implementation it recommends to increment positive speci\002c terms weights only and declines negative speci\002c terms weights based on their occurrences in discovered negative patterns Substantial experiments show that the proposed approach achieves exciting performance 
2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology 978-0-7695-3801-3/09 $26.00 © 2009 IEEE DOI 10.1109/WI-IAT.2009.103 606 
2009 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Workshops 978-0-7695-3801-3/09 $26.00 © 2009 IEEE DOI 10.1109/WI-IAT.2009.103 606 


The remainder of this paper is organized as follows Section 2 introduces a detailed overview of the related works Section 3 reviews the concepts of pattern taxonomy mining Section 4 introduces the equations for evaluating term weights based on discovered patterns Section 5 describes the proposed method of using negative feedback The empirical results and discussion are reported in Section 6 and the last section describes concluding remarks 2 Related Work Different from IR systems IF systems were commonly personalized to support long-term information needs of users The main distinction between IR and IF was that IR systems used queries but IF systems used user pro\002les The tasks of the 002ltering included adaptive 002ltering and batch and routing 002ltering Adaptive 002ltering involves feedback to dynamically adapt IF systems 23 In this paper  the focus is on the breakthrough for batch and routing 002ltering Normally IF systems tended to learn a map rank  D  R such that rank  d  corresponded to the relevance of a document d  where D denoted a set of documents R was the set of real numbers In rank was divided into two functions such that rank  f 1 016 f 2  where f 1  f 1  D  f C 1      C m g   and f 2  f 2  f C 1      C m g  R  were maps respectively and C 1  C 2      C m were clusters This method used a set of clusters based on a kind of classi\002cation method e.g the neural network The aim of the 002ltering track in TREC w as to measure the ability of IF systems to build pro\002les using sets of training documents to separate relevant and non-relevant documents The basic term-based IF models used in TREC 2002 were SVM Rocchio's algorithm probabilistic models and BM25 Term-based IF models have been developed recently which took into consideration more constraints in relation to the labeled data in training sets for instance Rocchio-style classi\002ers ra nking SVM 12 and BM 25 for str uctured documents Ho we v er  the research on term-based IF models has arguably hit somewhat of a wall in terms of effectiveness improvement possibly due to the ambiguity problem mentioned earlier To overcome the disadvantages of term-based approaches sequential patterns and closed patterns have been developed in pattern taxonomy models PTM 21 These approaches introduced data mining techniques to information 002ltering however too many noisy patterns adversely affect PTM systems The major research issue is ho w to use negative feedback to signi\002cantly reduce the effects of noisy patterns Traditional data mining techniques can only achieve a little progress for the effectiveness because they can only discuss this problem at the pattern level This paper starts to consider human being's perspective about relevance and uses a two dimension concept to classify terms into three groups positive speci\002c terms general terms and negative speci\002c terms by using the two classes of training documents In this Table 1 A set of paragraphs P arapgraph T erms dp 1 t 1 t 2 dp 2 t 3 t 4 t 6 dp 3 t 3 t 4 t 5 t 6 dp 4 t 3 t 4 t 5 t 6 dp 5 t 1 t 2 t 6 t 7 dp 6 t 1 t 2 t 6 t 7 perspective term weights can be evaluated accurately based on their appearances in both positive patterns and negative patterns 3 Pattern Taxonomy Mining In this paper we assume that all documents are split in paragraphs So a given document d yields a set of paragraphs P S  d   Let D be a training set of documents which consists of a set of positive documents D   and a set of negative documents D 000  Let T  f t 1  t 2      t m g be a set of terms or keywords which are extracted from the set of positive documents D   3.1 Frequent and Closed Patterns Given a termset X  a set of terms in document d  p X q is used to denote the covering set of X for d  which includes all paragraphs dp 2 P S  d  such that X 022 dp  i.e p X q  f dp j dp 2 P S  d   X 022 dp g  Its absolute support is the number of occurrences of X in P S  d   that is sup a  X   j p X q j  Its relative support is the fraction of the paragraphs that contain the pattern that is sup r  X   j p X q j j P S  d  j  A termset X is called frequent pattern if its sup a or sup r  025 min sup  a minimum support Table 1 lists a set of paragraphs for a given document d  where P S  d   f dp 1  dp 2      dp 6 g  and duplicate terms are removed Let min sup  3 giving rise to ten frequent patterns which are illustrated in Table 2 Normally not all frequent patterns are useful 22 F or e xample pattern f t 3  t 4 g always occurs with term t 6 in paragraphs see Table 1 therefore we want to keep the larger pattern only Given a termset X  its covering set p X q is a subset of paragraphs Similarly given a set of paragraphs Y 022 P S  d   we can de\002ne its termset  which satis\002es termset  Y   f t j8 dp 2 Y  t 2 dp g  The closure of X is de\002ned as follows Cls  X   termset  p X q   A pattern X also a termset is called closed if and only if X  Cls  X   Let X be a closed pattern We have sup a  X 1   sup a  X  1 for all pattern X 1 033 X  
607 
607 


Table 2 Frequent patterns and covering sets F requent P attern Covering Set f t 3  t 4  t 6 g f dp 2  dp 3  dp 4 g f t 3  t 4 g f dp 2  dp 3  dp 4 g f t 3  t 6 g f dp 2  dp 3  dp 4 g f t 4  t 6 g f dp 2  dp 3  dp 4 g f t 3 g f dp 2  dp 3  dp 4 g f t 4 g f dp 2  dp 3  dp 4 g f t 1  t 2 g f dp 1  dp 5  dp 6 g f t 1 g f dp 1  dp 5  dp 6 g f t 2 g f dp 1  dp 5  dp 6 g f t 6 g f dp 2  dp 3  dp 4  dp 5  dp 6 g 3.2 Pattern Taxonomy Patterns can be structured into a taxonomy by using the is-a or subset  relation and closed patterns For example Table 2 contains ten frequent patterns however it includes only three closed patterns  t 3  t 4  t 6    t 1  t 2   and  t 6   Simply a pattern taxonomy is described as a set of pattern-absolute support pairs for example PT fh t 3  t 4  t 6 i 3  h t 1  t 2 i 3  h t 6 i 5 g  where non-closed patterns are pruned After pruning some direct is-a retaliations may be changed for example pattern f t 6 g would become a direct sub-pattern of f t 3  t 4  t 6 g after pruning non-closed patterns  t 3  t 6  and  t 4  t 6   Smaller patterns in the taxonomy for example pattern f t 6 g  are usually more general because they could be used frequently in both positive and negative documents and larger patterns for example pattern f t 3  t 4  t 6 g  in the taxonomy are usually more speci\002c since they may only used in positive documents 3.3 Closed Sequential Patterns A sequential pattern s   t 1      t r   t i 2 T  is an ordered list of terms A sequence s 1   x 1      x i  is a sub-sequence of another sequence s 2   y 1      y j   denoted by s 1 v s 2  iff 9 j 1      j i such that 1 024 j 1  j 2     j i 024 j and x 1  y j 1  x 2  y j 2      x i  y j i  Given s 1 v s 2  we usually say s 1 is a sub-pattern of s 2  and s 2 is a super-pattern of s 1  In the following we simply say patterns for sequential patterns Given a pattern an ordered termset  X in document d  p X q is still used to denote the covering set of X  which includes all paragraphs ps 2 P S  d  such that X v ps  i.e p X q  f ps j ps 2 P S  d   X v ps g  Its absolute support and relative support are de\002ned as the same as for the normal patterns A sequential pattern X is called frequent pattern if its relative support 025 min sup  a minimum support The property of closed patterns see Eq 1 can be used to de\002ne closed sequential patterns A frequent sequential pattern X is called closed if not 9 any super-pattern X 1 of X such that sup a  X 1   sup a  X   4 Deploying Patterns on Terms The evaluation of term supports weights in this paper is different from the term-based approaches For a term based approach the evaluation of a given term's weight is based on its appearance in documents For pattern mining terms are weighted according to their appearance in discovered patterns To improve the ef\002ciency of the pattern taxonomy mining an algorithm SPMining  D   min sup   w as proposed also used in 8 to 002nd closed sequential pat terns for all document 2 D   which used the well-known Apriori property in order to reduce the searching space For all positive document d 2 D   the SPMining algorithm discovered all closed sequential patterns based on a given min sup  Let SP 1  SP 2   SP j D  j be the sets of discovered closed sequential patterns for all document d i 2 D   i  1  001 001 001  j D  j   For a given term its support in these discovered patterns can be described as follows support  t D    j D  j X i 1 jf p j p 2 SP i  t 2 p gj P p 2 SP i j p j After the supports of terms have been computed from the training set the following rank will be assigned to an incoming document d that can be used to decide its relevance rank  d   X t 2 T weight  t  034  t d  where weight  t   support  t D    and 034  t d   1 if t 2 d  otherwise 034  t d   0  5 Mining Negative Feedback In general the concept of relevance is subjective and normally people can describe the relevance of a topic or document in two dimensions the speci\002city and exhaustivity where speci\002city describes the extent to which the topic focuses on what users want and exhaustivity describes the extent to which the topic discusses what users want It is easy for human being to do so However it is very dif\002cult to use the two dimensions for IF systems In this section we 002rst discuss how to use the two dimensions for understanding the different roles of the selected terms We also presents an algorithm for both negative document selection and term weight revision 5.1 Speci\002c and General Terms Formally let DP  be the union of all discovered patterns of pattern taxonomies of D   and DP 000 be the union of all discovered negative patterns of pattern taxonomies of D 000  where a closed sequential pattern of D 000 is called negative pattern Given a term t 2 T  its exhaustivity is the number of discovered patterns in both DP  and DP 000 that contain t  and its speci\002city is the number of discovered patterns in DP  but not in DP 000 that contain t  Based on this understanding 
608 
608 


in this paper we classify terms into three groups We call a term a general term if it appear in both patterns discovered in positive documents and negative patterns discovered in negative documents We also call terms positive or negative speci\002c terms if they appear only in patterns discovered in positive or negative documents only Based on the above discussion we have the following de\002nitions for the set of general terms GT  the set of positive speci\002c terms T   and the set of negative speci\002c terms T 000  GT  f t j  9 p 1 2 DP     9  p 2 2 DP 000   t 2  p 1  p 2  g  T   f t j t  2 GT 9  p 2 DP    t 2 p g  and T 000  f t j t  2 GT 9  p 2 DP 000   t 2 p g  It is easy to verify that GT  T   T 000    Therefore GT  T  and T 000 is a partition of all terms in discovered patterns and negative patterns To present user pro\002les for a given topic normally we believe that speci\002c terms are very useful for the topic in order to distinguish to other topics However some experimental result show that using only speci\002c terms are not enough to improve the performance of information 002ltering because user information needs cannot simply be covered by documents that only contain the speci\002c terms Therefore the best way is to use the speci\002c terms mixed with some of the general terms 5.2 Strategies of Revision After we can classify terms into three categories we 002rstly show the basic process of revising discovered features in the training set This process can help readers to understand the proposed strategies for revising discovered features in different categories The process 002rst extracts initial features in the positive documents in the training set which include terms and patterns It then selects some negative samples or called offenders in the set of negative documents in the training set It also extracts negative features including both terms and negative patterns from the selected negative documents using the same pattern mining technique as used for the feature extraction in positive documents In addition it revises the initial features and obtains revised features The process can be repeated for several times as follows selecting negative documents extracting negative features and revising revised features Algorithm NFMining  D  describes the details of the strategies of the revision where we assume that the number of negative documents is greater than the number of positive documents For a given training set D  f D   D 000 g  we assume that the initial features  T DP   DP 000   have been extracted from positive documents D  before we start the algorithm where we let DP 000    We also let the experimental parameter 013  000 1 that will be used for calculating weights of terms in negative patterns Step 1 initializes the set of general terms GT  the set of positive speci\002c terms T  and the set of negative speci\002c terms T 000  where loop is used to control the times of the revision Step 2 and 3 calculate terms weights for all term in T  Step 4 and 5 rank documents in the set of negative documents where if t is a negative speci\002c term its weight is the revising weight calculating in step 10 and 11 The weight function can be described as follows weight  t   8   its revising weight  if t 2 T 000 support  t D    otherwise Step 6 and 7 sort the negative documents based on documents rank values and select offenders some negative documents If a document's rank less than or equals to 0 that means this document is clearly negative to the system A document has hight rank that means the document is an offender because it forces the system make mistake The offenders are normally de\002ned as the topK negative documents in sorted D 000  In this paper we let K  d j D  j 3 e  In the 002rst revision  loop  0  we ignore the topj negative documents for offender selection since the initial features only coming from positive documents and we believe that positive features are more important than negative features in the beginning where j  b j D 000 j j D  j c  the largest integer that less than or equals to j D 000 j j D  j  Step 8 and 9 extract negative features  DP 000  T 0  from selected negative documents D 000 3  where it calls algorithm SPMining  D 000 3  min sup  to discover negative patterns DP 000  and T 0 that includes all terms in patterns in DP 000  Step 10 to 12 revise negative speci\002c terms weights These steps will go through a loop for three times and the iteration is controlled by step 13 In each loop when a speci\002c negative term is extracted in the 002rst time the algorithm simply negatives its support obtained from the selected negative documents otherwise the algorithm cumulates its weight as follows weight  t   013 002 support  t D 000 3   weight  t   After three loops the algorithm participates T into general terms GT and positive speci\002c terms T  in step 14 and 15 It also revises positive speci\002c terms weights using the following equation in step 16 and 17 weight  t   weight  t  weight  t  003  jf d j d 2 D   t 2 d gj j D  j  At last it updates T to include negative speci\002c terms in step 18 NFMining calls three times SPMining and the total negative documents used in the three times is O  j D  j   therefore it takes the same computation time for mining patterns in selected negative documents as the SPMining does for mining patterns in positive documents NFMining also takes times for sorting D 000  assigning weights to terms and partitioning terms into groups The time complexity for these operations is O  j D 000 j  log  j D 000 j   j T j   j T j 2   
609 
609 


NFMining  D  Input A training set f D   D 000 g  parameter 013  000 1  extracted features  T DP   DP 000   DP 000    support function and minimum support min sup  Output Updated term set T and function weight  Method 1 GT    T     T 000    loop  0  2 foreach t 2 T do 3 weight  t   support  t D   4 foreach d 2 D 000 do 5 rank  d   006 t 2 d   T  T 000  weight  t   6 let D 000  f d 0  d 1   d j D 000 j\000 1 g in descendent ranking order let j  b j D 000 j j D  j c if loop  0  otherwise j  0  7 D 000 3  f d i j d i 2 D 000  j 024 i  d j D  j 3 e  j g  8 DP 000  SPMining  D 000 3  min sup   002nd negative patterns 9 T 0  f t 2 p j p 2 DP 000 g   all terms in negative patterns 10 foreach t 2  T 0 000 T  do 11 if  loop  0  then weight  t   013 002 support  t D 000 3  else weight  t   013 002 support  t D 000 3   weight  t   12 T 000  T 000   T 0 000 T   loop    13 if loop  3 then goto step 4 14 foreach t 2 T do term partition 15 if  t 2 T 000  then GT  GT  f t g else T   T   f t g  16 foreach t 2 T  do 17 weight  t   weight  t   weight  t  003  jf d j d 2 D  t 2 d gj j D  j   18 T  T  T 000  6 Evaluation In this section we 002rst discuss the data collection used for our experiments We also describe the baseline models and their implementation In addition we present the experimental results and the discussion 6.1 Data Reuters Corpus Volume 1 RCV1 was used to test the effectiveness of the proposed model RCV1 corpus consists of all and only English language stories produced by Reuter's journalists between August 20 1996 and August 19 1997 with total 806,791 documents The document collection is divided into training sets and test sets TREC 2002 has developed and provided 100 topics for the 002ltering track aiming at building a robust 002ltering system The topics are of two types 1 A 002rst set of 50 topics are developed by the assessors of the National Institute of Standards and Technology i.e assessor topics The relevance judgements have been made by assessor of NIST 2 A second set of 50 topics have been constructed arti\002cially from intersections of pairs of Reuters categories i.e intersection topics For that reason we use the 50 assessor topics in this paper where the result is more reliable RCV1 collection is marked in XML To avoid bias in experiments all of the meta-data information in the collection have been ignored The documents are treated as plain text documents by preprocessing the documents The tasks of removing stop-words according to a given stop-words list and stemming term by applying the Porter Stemming algorithm are conducted 6.2 Baseline Models and Setting Four baseline models are used the classic Rocchio model a BM25 based IF model a SVM based model and PTM model In this paper our new model is called Negative PaTtern Mining model N-PTM The Rocchio algorithm has been widely adopt ed in the areas of text categorization and information 002ltering It can be used to build the pro\002le for representing the concept of a topic which consists of a set of relevant positive and irrelevant negative documents The Centroid c of a topic can be generated as follows 013 1 j D  j X 000 d 2 D  000 d jj 000 d jj 000 014 1 j D 000 j X 000 d 2 D 000 000 d jj 000 d jj where we set 013  014  1  0 in this paper BM25 14 is the one of state-of-the-art re trie v al functions used in document retrieval The term weights are estimated using the following BM25 based equation W  t   tf 001  k 1  1 k 1 001 1 000 b   b DL AVDL   tf 001 log  r 0  5  n 000 r 0  5  R 000 r 0  5  N 000 n 000 R  r 0  5 where N is the total number of documents in the training set R is the number of positive documents in the training set n is the number of documents which contain term t  r is the number of positive documents which contain term t  tf is the term frequency DL and AVDL are the document length and average document length respectively and k 1 and b are the experimental parameters the values of k 1 and b are set as 1.2 and 0.75 respectively in this paper Information 002ltering can also be regarded as a special instance of text classi\002cation SVM is a statistical m ethod that can be used to 002nd a hyperplane that best separates two classes SVM achieved the best performance on the Reuters21578 data collection for document classi\002cation The decision function in SVM is de\002ned as h  x   sin w 001 x  b   8   1 if  w 001 x  b   0 000 1 otherwise where x is the input object b in  is a threshold and w  P l i 1 y i 013 i x i for the given training data  x i  y i     x l  y l   where x i   n and y i equals 1  000 1  if document x i is labeled positive negative 013 i 017  is the weight of the training example x i and satis\002es the following constraints 8 i  013 i  0 and l X i 1 013 i y i  0 2 To compare with other baseline models we tried to use SVM to rank documents rather than to make binary decisions 
610 
610 


Table 3 Results in all assessor topics for PTM and N-PTM PTM N-PTM  chg p value b=p 0.4299932 0.4684968 8.95 0.001974031 M AP 0.4435398 0.4871910 9.84 0.001415044 IAP 0.4641946 0.50668984 9.15 0.002250558 F 014 1 0.439174956 0.4637 5.58 0.000829231 For this purpose threshold b can be ignored We also believe that the positive documents in the training set should have the same importance to user information needs because the training set was only simply divided into positive documents and negative documents So we assign the same 013 i value i.e 1 to each positive document 002rst and then determine the same 013 i i.e 023 013  value to each negative document based on Eq 2 Therefore we use the following weighting function to estimate the similarity between a testing document and a given topic weight  d   w 001 d where 001 means inner product  d is the term vector of the testing document and w   X d i 2 D  d i    X d j 2 D 000 d j 023 013   For each topic we also choose 150 terms in the positive documents based on tf*idf values for all term-based baseline models PTM model is also selected as one of the baselines models because we want to verify that mining negative feedback can signi\002cantly improve the performance of PTM The size of the term set T is 4000 for PTM We also set min sup  0  2 relative support for both PTM and N-PTM 6.3 Results The effectiveness was measured by four different means The F-beta  F 014  measure Mean Average Precision MAP the break-even point  b=p  and Interpolated Average Precision IAP on 11-points  F 014 is calculated by the following function F 014   014 2  1 P R 014 2 P  R The parameter 014  1 is used in our study which means that recall and precision is weighed equally Mean Average precision is calculated by measuring precision at each relevant document 002rst and averaging precision over all topics The b/p break-even point indicates the value at which precision equals recall The larger a b/p  MAP IAP or F 014 measure score is the better the system performs 11-points measure is also used to compare the performance of different systems by averaging precisions at 11 standard recall levels i.e recall=0.0 0.1  1.0 Statistical method is also used to analyze the experimental results The t-test assesses whether the means of two groups Figure 1 comparison between the proposed method and other approachs are statistically different from each other The paired twotailed t-test is used in this paper If DIF represents the difference between observations the hypotheses are Ho  DIF  0 the difference between the two observations is 0 Ha  DIF 6  0 the difference is not 0 N is the sample size of group The test statistic is t with N 000 1 degrees of freedom  df  If the p value associated with t is low   0.05 there is evidence to reject the null hypothesis Thus there is evidence that the difference in means across the paired observations is signi\002cant The N-PTM model is compared with PTM Rocchio BM25 and SVM models for each variable b=p  M AP  IAP  F 014 1 over all the 50 topics respectively 6.3.1 PTM\(Positive vs N-PTM\(Positive and Negative In order to see the effectiveness of using both positive and negative feedback in pattern mining approach in this section we compare the proposed mining negative feedback model N-PTM to PTM model which uses positive documents only The results on the 50 topics for the comparison between PTM and N-PTM over the standard measures are shown in Tables 3 The results of 11-points on 50 topics are reported in Figure 1 As shown in Table 3 and Figure 1 the performance of NPTM model is extremely better than use positive feedback only Table 3 also shows the percentage changes and the p vales The statistic results indicate that the proposed mining negative feedback model is extremely statistically signi\002cant Therefore we conclude that mining negative relevance feedback for information 002ltering is an exciting achievement for pattern based approaches 6.3.2 N-PTM vs Term-Based State-of-the-Art Models The proposed method is also compared with term-based baseline models including Rocchio BM25 and SVM The experimental results on all assessor topics are reported in Table 5 The results of 11-points on all assessor topics are reported in Figure 1 As shown in Table 5 and Figure 1 the proposed new model N-PTM has achieved the best performance results for the 
611 
611 


Table 4 Extracted Features in First Ten Topics where min sup  0  2  Topic Number of Documents in Training Sets Number of Extracted Terms Weight of Extracted Terms Positive Negative Offenders T  T 000 GT T w  T   w  T 000  w  GT  total weight 101 7 16 5 64 142 35 241 20  2 000 34  2 16  6 2  6 102 135 64 21 604 79 221 904 250  4 000 152  3 663  2 761  3 103 14 50 7 106 434 51 591 24  8 000 47  2 25  3 2  9 104 120 74 25 425 123 297 845 131  7 000 179  7 678  0 630  0 105 16 21 9 145 144 43 332 76  1 000 109  7 50  5 16  9 106 4 40 2 91 48 12 151 22  1 000 24  4 11  8 9  5 107 3 58 3 54 544 35 633 10  4 000 11  0 7  5 6  9 108 3 50 3 57 77 21 155 16  0 000 14  6 10  1 11  4 109 20 20 6 215 53 87 355 63  0 000 78  6 99  7 84  2 110 5 86 3 40 103 17 160 11  3 000 17  2 19  2 13  4 Total 327 479 84 1801 1747 819 4367 626  0 000 668  8 1581  9 1539  2 Avg 33 48 8  4 180 175 82 437 62  6 000 66  9 158  2 153  9 Table 5 Results in all assessor topicss where  chg is the percentage change over the best term based model Rocchio BM25 SVM N-PTM  chg b=p 0.420 0.403 0.409 0.468 11.52 M AP 0.430 0.417 0.409 0.487 13.17 IAP 0.452 0.439 0.434 0.507 12.03 F 014 1 0.430 0.421 0.421 0.464 7.88 assessor topics The improvements are consistent and very signi\002cant on the all above measures 6.4 Discussion The main process of the proposed approach consists of two steps offender selection and the revision of term weights It is obvious that not all negative feedback are suitable to be selected as offenders where offenders are the most useful negative documents that can help to balance the percentages of general terms and speci\002c terms in the extracted features Informally the documents that have high weight are called offenders Figure 2 shows the difference between using all negative documents and using offenders in all assessor topics This 002gure illustrates that the proposed method for offender selection meets the design objectives Table 4 shows the numbers of positive documents negative documents and offenders in the training sets of the 002rst ten documents because of the limitation of the paper length we have not shown all assessor topics here It also illustrates that only 17 negative documents are selected as offenders that is the proposed method is much ef\002cient for reducing the space of negative documents For the revision of term weights the proposed method 002rst classi\002es extracted terms into general terms and speci\002c terms that is a distinguish advantage comparing with others  The normal belief is that speci\002c terms are more interesting than general terms for a given topic Therefore the proposed method only increases the weights of speci\002c terms when it conduces the revision using negative documents General terms are not only frequently appear in positive documents but also frequently appear in some negative documents because negative documents may describe some extent Figure 2 comparison between used all negative documents and used the offender one to which the topic discusses what users want To reduce the side effects of using general terms in the extracted features the proposed method adds negative speci\002c terms into the extracted features Table 4 also shows the numbers of extracted general terms speci\002c terms and negative speci\002c terms and their weights Before revision it can be seen that more than 72  158  2 158  2+62  6 weights are distributed to general terms although the percentage of general terms is 31  82 82+180 for all extracted terms in positive documents After revision 1747 negative speci\002c terms are added into T for the ten topics in Table 4 and they are assigned weight 000 66  9 in average In this way these negative speci\002c terms could reduce the side effects of general terms if both general terms and negative speci\002c terms appear in negative documents because now only 59 weights could be distributed to general terms considering positive speci\002c terms get weight 62  2 in average and general terms get 158  2 000 66  9 in average Comparing with the best model of the state-of-the-art models the proposed approach achieves excellent performance with 11  15 max 13  17 and min 7  88  average percentage change for all 4 measures 7 Conclusions Negative documents are very useful for information 002ltering However whether negative feedback can largely improve 
612 
612 


002ltering accuracy is still an open question This paper presents a pattern mining based approach for this open question It introduces a method to select negative documents or called offenders that are close to the extracted features It also proposes an approach to classify extracted terms into three groups positive speci\002c terms general terms and negative speci\002c terms In this perspective it presents an iterative algorithm to revise extracted features Compared with the state-of-the-art models the results of experiments on RCV1 collection demonstrate that the effectiveness of information 002ltering can be signi\002cantly improved by the proposed new approach This research provides a promising methodology for evaluating term weights based on discovered patterns rather than documents in both positive and negative documents References  R Baeza-Y ates and B Ribeiro-Neto Modern Information Retrieval  Addison Wesley 1999  N J Belkin and W  B Croft Information 002ltering and information retrieval two sides of the same coin Commun ACM  35\(12 1992  X Fu J Budzik and K J Hammond Mining na vig ation history for recommendation In IUI 00 Proceedings of the 5th international conference on Intelligent user interfaces  pages 106112 New York NY USA 2000 ACM  K S Jones S W alk er  and S E Robertson A probabilistic model of information retrieval development and comparative experiments part 1 Inf Process Manage  36\(6 2000  R Y  K Lau P  Bruza and D Song Belief re vision for adaptive information retrieval In SIGIR  pages 130137 2004  X Li and B Liu Learning to classify te xts using positi v e and unlabeled data In IJCAI  pages 587594 2003  Y  Li and N Zhong Mining ontology for automatically acquiring web user information needs IEEE Transactions on Knowledge and Data Engineering  18\(4 2006  Y  Li X Zhou P  Bruza Y  Xu and R Y  Lau A tw ostage text mining model for information 002ltering In CIKM 08 Proceeding of the 17th ACM conference on Information and knowledge management  pages 10231032 Napa Valley California USA 2008  B Liu Web Data Mining Exploring Hyperlinks Contents and Usage Data Data-Centric Systems and Applications  Springer January 2007  J Mosta f a and W  Lam Automatic classi\002cation using supervised learning in a medical document 002ltering application Inf Process Manage  36\(3 2000  J Mostaf a S Mukhopadh yay  W  Lam and M J P alakal A multilevel approach to intelligent information 002ltering Model system and evaluation ACM Trans Inf Syst  15\(4 1997  T  Qin X.-D Zhang D.-S W ang T Y  Liu W  Lai and H Li Ranking with multiple hyperplanes In SIGIR  pages 279286 2007  S E Robertson and I Soborof f The trec 2002 002ltering track report In TREC  2002  S E Robertson S W al k er  and M Hancock-Beaulieu Okapi at trec-7 Automatic ad hoc 002ltering vlc and interactive In TREC  pages 199210 1998  S E Robertson H Zaragoza and M J T aylor  Simple bm25 extension to multiple weighted 002elds In CIKM  pages 4249 2004  J Rocchio Relevance feedback in information retrieval  volume In The SMART Retrieval System Experiments in Automatic Document Processing Prentice Hall 1971  F  Sebastiani Machine learning in automated te xt cate gorization ACM Comput Surv  34\(1 2002  I Soborof f and S Robertson Building a 002ltering test collection for trec 2002 In SIGIR  pages 243250 New York NY USA 2003 ACM  S.T W u Y Li Y  Xu B Pham and P Chen Automatic pattern-taxonomy extraction for web mining In the IEEE/WIC/ACM International Conference on Web Intelligence  pages 242  248 China 2004  X W ang H F ang and C Zhai A study of methods for negative relevance feedback In SIGIR 08 Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval  pages 219226 New York NY USA 2008 ACM  S.-T  W u Y  Li and Y  Xu Deplo ying approaches for pattern re\002nement in text mining In ICDM  pages 11571161 2006  Y  Xu and Y  Li Generating concise ass ociation rules In CIKM  pages 781790 2007  Y  Y ang A Lad N Lao A Harpale B Kisiel and M Rogati Utility-based information distillation over temporally sequenced documents In SIGIR  pages 3138 2007  Y  Y ang and X Liu A re-e xamination of te xt cate gorization methods In SIGIR  pages 4249 1999  Y  Zhang Using bayesian priors to combine classi\002ers for adaptive 002ltering In SIGIR 04 Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval  pages 345352 New York NY USA 2004 ACM  Y  Zhang and J Callan Combining multiple forms of evidence while 002ltering In HLT 05 Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing  pages 587595 Morristown NJ USA 2005 Association for Computational Linguistics 
613 
613 


standard preferred for am airport surface wireless communications network Frequency spectrum has been identified in the MLS Extension Band \(5091-5150 MHz airport surface wireless application that meets a majority of the bandwidth requirement.  Characterization of this frequency band at several operational airports has been completed, and has led to the development of channel models.  Simulations based on these models indicates that good performance of the IEEE 802.16e standard in an airport environment in the MLS Extension Band can be expected The NextGen CNS Testbed has been developed at Cleveland Hopkins International Airport and two smaller nearby airports. The testbed is based upon multilateration surveillance equipment installed by Sensis and an airport surface wireless communications network.  The current network is based upon 802.11e equipment, however the installation of 802.16e equipment operating within the MLS Extension Band is imminent.  The 802.16E equipment will make possible a range of system test which will enable the development and validation of an 802.16e  aviation profile   that will become a blueprint for a national and international standard for airport surface wireless communicators networks.  The testbed thus equipped will then be able to test and demonstrate a wide range of current and future applications that will support the advances in airport surface management and improvements in airport surface safety required to enable the future growth in air traffic envisioned by NextGen and SESAR REFERENCES 1] Robert J. Kerczewski,, James M. Budinger, and Tricia J. Gilbert  Technology Assessment Results of the Eurocontrol/FAA  Future Communications Study  2008 IEEE Aerospace Conference, March, 2008 BIOGRAPHIES 2] Rafael D. Apaza  Wireless Communications for Airport Surface: An Evaluation of Requirements  2005 IEEE Aerospace Conference, March, 2005 3] Izabela Gheorghisor  Spectral Requirements of ANLE Networks for the Airport Surface  Mitre Report MP080109R1, July, 2008 4] Izabela Gheorghisor, Yan-Shek Hoh, and Frank Box  Compatibility Analysis of Airport Wireless Local Area Networks and Satellite Feeder Links in the 5091-5150 MHz Band  2006 ICNS Conference, May, 2006 5] Dana Hall and James M. Budinger  NextGen ATS Communications, Navigation, and Surveillance Test Bed   26th Digital Avionics System Conference, October, 2007 6] David W. Matolak, Lawrence R. Foore, and Rafael D. Apaza  Channel Characterization in the 5 GHz Microwave Landing System Extension Band for Future Airport Surface Communications  5th ICNA Conference and Workshop, May, 2005 7  Wireless Channel Characterization in the 5 GHz Microwave Landing System Extension Band for Airport Surface Areas  Final Project Report for NASA ACAST Project, Ohio University, May 2006 8] Glen Dyer, Tricia J. Gilbert, and James M Budinger  FCS Phase II Results Paper 3 - Detailed Technology Investigations  ICAO Aeronautical Communications Panel Working Group C Meeting  #11 Working Paper 9, September, 2006 9] David Brooks, Ryan Wilkins, and Douglas Hoder  Mobile IP Communications for Aeronautical Applications  23rd Digital Avionics System Conference October, 2004 8 BIOGRAPHIES  


 Robert J. Kerczewski has been involved with research and development of satellite communications systems and applications since for the Analex Corporation \(19821986 1986present degree from Cleveland State University \(1982 MSEE degree from Case Western Reserve University \(1987 Project Manager for the NASA  s Advanced CNS Architectures and System Technologies \(ACAST  James. M. Budinger is a senior engineer in the Communications Systems Integration Branch at NASA Glenn Research Center GRC Over nearly 30 years of service at NASA, Mr Budinger has lead research and advanced technology development for both space and aeronautical communications as a researcher, supervisor, and technology project manager.  He holds MSEE and BEE degrees from Cleveland State University.  Mr. Budinger received a NASA Exceptional Service Medal in 2003 for outstanding project and technical achievements in the development of advanced communications technology. Most recently, he led NASA  s technology assessment for the Eurocontrol/FAA-NASA Future Communications Study to determine an internationally harmonized solution for the next generation of aeronautical voice and data communications.  Currently, he leads GRC  s research in airport surface wireless communications via a cooperative agreement with Sensis Corporation for the NextGen CNS Test Bed at GRC and three nearby regional airports  David E. Brooks has over 14 years of advanced IP networking and advanced RF communication system integration experience on various projects at Glenn Research Center at Lewis Field as a Support Service Contractor\(SSC for Sterling Software, Inc 1994-1999 Global Infrastructures, LLC 1999-2006 MorganFranklin Corporation \(2006Present have included Advanced Communication Technology Satellite\(ACTS high data terminal experiments, Ku-band networking link experiments using mobile and air assets under Advanced Air Transportation Technologies\(AATT wireless/wired airport surface NexGen research activities He holds a BEE degree from Cleveland State University 1992 1997  Robert P. Dimond is currently employed by Verizon Business working on several projects at NASA 


several projects at NASA Glenn Research Center in Cleveland Ohio.  His career spans nearly twenty years involving the research implementation, and troubleshooting of network protocols and applications within satellite, mobile, and aeronautical environments pre></body></html 


common concern was compliance with the Healthcare Insurance Portability and Accountability Act \(HIPAA information privacy. Organizations were unsure whether DP products would be HIPAA compliant and even if the products themselves were secure whether their onboard information could be kept secure when the pens are small and easily lost  3.4.3. Implications for DP. Although there was substantial interest in DP technology among professional services and white collar organizations the ethnographic findings suggest that delivering appropriate value from DP technology would be difficult. The first barrier would be providing a platform that either conforms to or changes current behaviors around pen and paper: free availability of pens and unpatterned paper in the workplace behaviors of writing on environmental paper, and the need to keep up with a personal pen with one?s data The second barrier would be to meet expectations around text; much of the value was seen as being in retention and searching of textual information, but this would be made difficult by the frequent inclusion of acronyms, abbreviations symbols, and foreign languages. Many organizations saw value in centralized repositories that would manage DP information across many employees; this would require an IT infrastructure that was not available \(e.g., to manage the DP pattern space might be difficult and expensive to develop and maintain. It also poses questions about privacy, since employees might not wish the archive to retain personal information captured alongside work notes Finally, because we noted real or perceived barriers that were highly specific to various industries it appeared that DP solutions would be most likely to succeed if they were tailored to very specific settings Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 and use cases. This does not argue against DP usage and its value, but it suggests that delivering value to organizations may require substantially more design and development effort than a single general solution or platform would provide  4. Discussion  Our findings show that the interaction model of DP technology diverges substantially from common pen and paper usage. This divergence leads to unclear utility of DP for consumer note-taking purposes, and in a field trial, resulted in high abandonment rate of a consumer DP product. Among the most significant limitations were the low value seen in review of facsimile notes on a PC, the requirement with DP to use a special pen paired with patterned paper, and the low perceived value of capturing only notes taken on blank paper as opposed to handouts and other materials. In the enterprise space, the environmental issues were even more prominent due to the common infrastructure of freely available, communal pens and paper and the lack of IT infrastructure for DP document retention The note-taking research reported here suggests that writing behavior comprises many important areas besides capture, retention, and review of specific data. For instance, a single blank piece of paper or a one-word reminder may adequately serve as a reminder to do something that one intends, even though there may be little or no content as such Likewise, in interpersonal contexts such as attending 


Likewise, in interpersonal contexts such as attending a lecture, it may be socially desirable to take notes even if they are later simply disposed. Taking notes may also serve purposes of memory consolidation even when the content is never reviewed [8][9  4.1 DP and Writing Acts  Linguistic acts in social context have been described using a model of performative behavior commonly known as ?speech acts? theory [15 Chapman [2] extended that model to encompass writing, suggesting that many aspects of ?writing acts are unique and separate from spoken language.  In the extended model, writing acts may be described with a multidimensional taxonomy encompassing a writer?s context, aspects of the process, type of content, and linguistic features of the content Table 3 summarizes the dimensions of writing acts \(from [2 exemplify each dimension. For instance, ?separability denotes the extent to which items in a document are logically independent of one another; in a contract nothing is separable because the document is a single piece, while on a to-do list, many or all items may be independent of one another [2 DP technology may benefit from attention to the dimensions that are exemplified by writing acts embedded in social contexts. DP systems may be able to perform some kinds of writing acts quite well, but in other cases, DP may be inconvenient, unnecessary or inappropriate. Models such as the writing acts framework can be used both to understand user behavior broadly and systematically to explore the applicability of DP products across the general space of writing behavior A key problem for DP in note taking applications is that note taking can involve nearly every possible dimension of writing acts. Notes may be separable or not; they may serve as functional content or as contextual reminders; they may present just the facts? or be more interpretive; they may be transient or might be intended to be archival documents; and so forth. In short, notes are able to present a vast array of writing styles that pose substantially different value propositions and technological implications for DP products Delivering a general DP solution for note taking may therefore be expected to be difficult  Table 3: Dimensions of Writing Acts [2  Dimension Exemplars Separability a contract vs. a to-do list Function a typed inventory list vs calligraphy Emotionality a love letter vs. a packing slip Spatiality a transcript of speech vs. a diagram depicting concept relationships Associativity notes from class vs. doodling Linearity chronological notes vs. notes placed in a spatial ordering scheme Originality an essay vs. feedback on a manuscript Prescriptivity a signature vs. general notes Finality a document that will be archival vs. one that is a draft Structure a grocery list vs. concepts from a brainstorm Personality a letter to someone vs. a 


Personality a letter to someone vs. a journalistic essay Formality a business letter vs. a greeting card to a close friend  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 For product development purposes, the writing acts framework can be used to ensure coverage of use cases by generating possible scenarios combining various attributes of written documents. These potential combinations may then be explored evaluated, and prioritized for research attention development effort, or testing  4.2 Possible Directions for DP Technology  4.2.1. Note taking. What would make DP technology more popular for note taking? Our research suggests that this is a complex question because there are numerous behavioral barriers. The largest single problem for DP may be that it is a closed system comprising a special pen and patterned paper and does not function with the wide array of writing utensils and paper products that people use especially for environmentally available pens, paper and documents. If this problem could be solved or mitigated through an expanded DP technology platform, DP would avoid the large behavioral block posed by the current need for users to change their existing pen and paper habits DP effectively focuses on notes as data neglecting many aspects of the embedded social nature and process function of notes. If DP notes were easily integrated into a wider range of behavioral processes, adoption should increase. For example, if notes could be automatically handled for content such as phone numbers, appointment times reminders, temporary content, and the like, then the DP platform would come closer to matching current pen and paper usage. However, in many cases, there is a separate and larger issue: computer technology today also is not integrated into such processes. To take a simple example, consider a written grocery list Even if the problem could be solved to recognize extract, and transfer the list to a PC, it would be of little use because, for most people, the PC itself is not integrated into the grocery shopping process. Much information of this kind is transient; there is no need to manage or retain it once the paper has been used An example of potentially closer workflow integration is shown in the recently released Livescribe Pulse Smartpen [11], which couples the Anoto DP platform with audio recording such that note takers can review the audio of a meeting or lecture at the exact point that a note was written. We believe that these kinds of additions to base DP functionality are likely to appeal to specific niches of users, but as more use cases like these are enabled over time, DP may successively attain value for larger numbers of users In our field trials and organizational visits, one of the most common customer expectations was that DP notes should be converted from handwriting to text; respondents commonly noted that PC data is of little use unless it is transcribed to text. To meet customer demands, a DP product will need to address this expectation: the DP must either deliver text recognition with very high accuracy, which is a difficult problem, or it should manage the expectation in some other way that preserves customer perception of value from PC integration 


of value from PC integration The high cost of DP products \(approximately US 100 for a pen, plus the need for specially patterned paper demonstrable additional benefit. Transferring notes from paper to a PC today merely involves typing Unless the need for automation is great and the DP function is nearly perfect, users may simply prefer to type or to carry paper rather than to change behavior to use expensive and less flexible technology  4.2.2. Structured input. As noted in the Introduction above, another use case for DP technology is structured input of information. In particular, DP technology may be useful for form-based input into database and workflow systems, where information is initially recorded on paper forms and then automatically transferred from the pen to a database application. Although the present research report is concerned primarily with note taking applications, in the course of our enterprise research we discussed potential applications for forms-based DP usage We noted possible use cases that fell into five general areas: \(1 easier input of information by customers, such as clients filling out deposit or withdrawal slips at a bank; \(2 information from paper to database without needing to rekey or type the information, such as factory inspection and quality assurance logs, traffic tickets shipping manifests, and so forth; \(3 environments that were not suitable for handheld computing devices, such as construction sites and some kinds of manufacturing facilities; \(4 in which paper-based records are desirable for either employee compliance or customer comfort, such as medical settings; and \(5 based records are necessary, e.g., for legal reasons but a DP product could support faster turnaround and error correction. An example of such application involved financial forms that undergo offsite optical character recognition; a DP system might allow immediate recognition and error correction We plan to report this line of research fully in the future. For now, we note that each of those areas has various benefits and potential limitations with regard to DP technology. Forms-based use cases are more precisely defined and structured than note taking, in Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 terms of environment, workflow, and content; the information context, form design, and potential for systems integration with rapid feedback may mitigate issues with handwriting recognition; and enterprise customers may be less price sensitive than consumers Thus, structured input appears to be a more promising near-future application of DP technology than note taking  4.3 Research Discussion  As we noted above, the present research was primarily behavioral and qualitative. Thus, despite the strength and consistency of our results across multiple samples, contexts, and locations, we present no specific metrics or tested hypotheses on user behavior with DP products. Our findings could be used to inform directional hypotheses for future depth or quantitative research. For instance, one might formulate and test hypotheses about cultural differences, interest levels in DP between various groups such as professional and non-professional 


office workers, market research metrics such as price sensitivity, and the like It is important to underscore the value of ethnographic fieldwork in the present research. It would be possible to conduct design research that explores how to make DP technology better, e.g., in terms of usability and function, without investigating exactly what people would do with such products and why. It was only when we investigated behavior in depth that we discovered the divergence of DP products? limited current value for note taking, as opposed to the high value that one might presume in the absence of depth research  5. Conclusion  In our research, initial trials of digital pens in a controlled setting \(Research Series 2 above suggested potentially good fit between digital pen functionality and consumer note taking needs However, when we explored real world behavior in note taking \(Research Series 1, 3, and 4 above found many potential barriers to adoption of digital pens for note taking. In particular, traditional pen and paper offer advantages in terms of cost, widespread and ad hoc availability, flexibility to work with multiple sources seamlessly, behavioral workflow integration, and manageability of content The value of using digital pens will increase if manufacturers are able to expand their platform technology progressively to enable broader coverage of behavioral scenarios and habits, focusing on the broad range of writing behaviors rather than just needs for facsimile replication on a computer Alternatively, digital pen technology may be more easily applied to tasks involving structured input rather than unstructured note taking We suggest that DP development efforts should use existing linguistic frameworks \(e.g., [2 the space of writing acts of interest. This should allow DP products to target behavioral needs in a more focused manner, leading to higher customer adoption  6. Acknowledgements  We thank many people who participated in this research: first and foremost, the numerous research participants, firms, and organizations who so generously shared their time and insights, but who must remain confidential; and many colleagues and research partners at Microsoft who collaborated on the technology investigation and research efforts especially Jeff Staiman, Vince Ball, Brian Williams Yoshiyuki Moriya, Stephen Cooper, Dave Shen Setsuko Arimatsu, Benjamin Babcock, Dennis Meinhardt, John Chiloyan, Glen Larsen, Mehrdad Basseri, Lori Birtley, Ken Hinckley, and Jian Wang  7. References  1] Anoto Group. Digital pen and paper. Web page http://www.anoto.com/?id=158, last retrieved May 28 2008  2] Chapman, C. N. Writing acts: taxonomy and technological implications. Paper presented at North American Computing and Philosophy \(CAP Corvallis, OR, August 2005  3] Despont-Gros, C., B?uf, C., Geissbuhler, A., and Lovis C. \(2005 


C. \(2005 Implementation and Use in an Existing Clinical Information System. In Connecting Medical Informatics and Bio-Informatics: Proceedings of MIE2005 - The XIXth International Congress of the European Federation for Medical Informatics, 328-333. IOS Press, 2005  4] Fetterman, D. M. Ethnography, 2nd ed. Thousand Oaks CA: Sage, 1997  5] Furukawa, N.  Ikeda, H.  Kato, Y.  Sako, H. D-Pen: a digital pen system for public and business enterprises. In Frontiers in Handwriting Recognition 2004: Proceedings of the Ninth International Workshop on Frontiers in Handwriting Recognition \(IWFHR-9 2004  6] Guimbreti  re, F. 2003. Paper augmented digital documents. In Proceedings of the 16th Annual ACM Symposium on User interface Software and Technology Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 UIST ?03 November 2003  7] Holman, D., Vertegaal, R., Altosaar, M., Troje, N., and Johns, D. 2005. Paper windows: interaction techniques for digital paper. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems \(CHI ?05 Portland, Oregon. New York: ACM, 2005  8] Kiewra, K., DuBois, N., Christian, D., McShane, A Meyerhoffer, M., &amp; Roskelley, D. Note-taking functions and techniques. Journal of Educational Psychology, 83 240-245, 1991  9] Kobayashi, K. Combined effects of note-taking/reviewing on learning and enhancements through interventions: a meta-analytic review. Educational Psychology, 26, 459-477, 2006  10] Liao, C., Guimbreti  re, F., and Hinckley, K. 2005 PapierCraft: a command system for interactive paper. In Proceedings of the 18th Annual ACM Symposium on User interface Software and Technology \(UIST ?05 Seattle, WA. New York: ACM, 2005  11] Livescribe. Pulse Smartpen [electronic device http://www.livescribe.com/, last retrieved May 28, 2008  12] Logitech. io2 Digital Pen [electronic device http://www.logitech.com/index.cfm/mice_pointers/digital_ pen/devices/408&amp;cl=us,en, last retrieved May 28, 2008  13] Norrie, M. C., Signer, B., and Weibel, N. Print-n-link weaving the paper web. In Proceedings of the 2006 ACM Symposium on Document Engineering \(DocEng '06 New York: ACM, 2006  14] Randall, D., Harper, R., and Rouncefield, M Fieldwork for Design: Theory and Practice. London Springer-Verlag, 2007  15] Searle, J. R. Speech Acts: An Essay in the Philosophy of Language. Cambridge: Cambridge Univ. Press, 1969  16] Sellen, A. J. and Harper, R. H. The Myth of the Paperless Office. Cambridge, MA: MIT Press, 2003  17] Signer, B. and Norrie, M. C. 2007. PaperPoint: a paper-based presentation and interactive paper prototyping 


paper-based presentation and interactive paper prototyping tool. In Proceedings of the 1st international Conference on Tangible and Embedded interaction \(TEI ?07 Baton Rouge, Louisiana. New York: ACM, 2007  18] Tanabe, K., Yoshihara, M., Kameya, H., Mori, S Omata, S., Ito, T., Automatic Signature Verification Based on the Dynamic Feature of Pressure. Proceedings of the Sixth International Conference on Document Analysis and Recognition \(ICDAR ?01 Computer Society, 2001   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


The HyspIRI mission utilizes innovative techniques to both reduce the amount of data that must be transmitted to the ground and accommodate the required data volume on the ground The infrastructure and techniques developed by this mission will open the door to future high data volume science missions The designs presented here are the work of the authors and may differ from the current HyspIRI mission baseline A CKNOWLEDGMENTS This research was carried out at the Jet Propulsion Laboratory California Institute of Technology and was sponsored by the Space Grant program and the National Aeronautics and Space Administration R EFERENCES  K W ar\002eld T  V  Houten C Hee g V  Smith S Mobasser B Cox Y He R Jolly C Baker S Barry K Klassen A Nash M Vick S Kondos M Wallace J Wertz Chen R Cowley W Smythe S Klein L Cin-Young D Morabito M Pugh and R Miyake 223Hyspiri-tir mission study 2007-07 002nal report internal jpl document,\224 TeamX 923 Jet Propulsion Laboratory California Institute of Technology 4800 Oak Grove Drive Pasadena CA 91109 July 2007  R O Green 223Hyspiri summer 2008 o v ervie w  224 2008 Information exchanged during presentation  S Hook 2008 Information e xchanged during meeting discussion July 16th  R O Green 223Measuring the earth wi th imaging spectroscopy,\224 2008  223Moore s la w Made real by intel inno v ation 224 http://www.intel.com/technology/mooreslaw/index.htm  T  Doggett R Greele y  S Chein R Castano and B Cichy 223Autonomous detection of cryospheric change with hyperion on-board earth observing-1,\224 Remote Sensing of Environment  vol 101 pp 447\226462 2006  R Castano D Mazzoni N T ang and T  Dogget 223Learning classi\002ers for science event detection in remote sensing imagery,\224 in Proceedings of the ISAIRAS 2005 Conference  2005  S Shif fman 223Cloud detection from satellite imagery A comparison of expert-generated and autmatically-generated decision trees.\224 ti.arc.nasa.gov/m/pub/917/0917 Shiffman  M Griggin H Burk e D Mandl and J Miller  223Cloud cover detection algorithm for eo-1 hyperion imagery,\224 Geoscience and Remote Sensing Symposium 2003 IGARSS 03 Proceedings 2003 IEEE International  vol 1 pp 86\22689 July 2003  V  V apnik Advances in Kernel Methods Support Vector Learning  MIT Press 1999  C Bur ges 223 A tutorial on support v ector machines for pattern recognition,\224 Data Mining and Knowledge Discovery  vol 2 pp 121\226167 1998  M Klemish 223F ast lossless compression of multispectral imagery internal jpl document,\224 October 2007  F  Rizzo 223Lo w-comple xity lossless compression of h yperspectral imagery via linear prediction,\224 p 2 IEEE Signal Processing Letters IEEE 2005  R Roosta 223Nasa jpl Nasa electronic parts and packaging program.\224 http://nepp.nasa.gov/docuploads/3C8F70A32452-4336-B70CDF1C1B08F805/JPL%20RadTolerant%20FPGAs%20for%20Space%20Applications.pdf December 2004  I Xilinx 223Xilinx  Radiation-hardened virtex-4 qpro-v family overview.\224 http://www.xilinx.com/support/documentation data sheets/ds653.pdf March 2008  G S F  Center  223Tdrss o v ervie w  224 http://msp.gsfc.nasa.gov/tdrss/oview.html 7  H Hemmati 07 2008 Information e xchanged during meeting about LaserComm  223W orldvie w-1 224 http://www digitalglobe.com/inde x.php 86/WorldView-1 2008  223Sv albard ground station nor way.\224 http://www.aerospacetechnology.com/projects/svalbard 7 2008  223Satellite tracking ground station 224 http://www.asf.alaska.edu/stgs 2008  R Flaherty  223Sn/gn systems o v ervie w  224 tech rep Goddard Space Flight Center NASA 7 2002  223Geoe ye-1 f act sheet 224 http://launch.geoeye.com/launchsite/about/fact sheet.aspx 2008  L-3 Communications/Cincinnati Electronics Space Electronics 7500 Innovation Way Mason OH 45040 T-720 Transmitter  5 2007 PDF Spec Sheet for the T720 Ku-Band TDRSS Transmitter  L-3 Communications/Cincinnati Electronics Space Electronics 7500 Innovation Way Mason OH 45040 T-722 X-Band  7 2007 PDF Spec Sheet for the T-722  J Smith 07 2008 Information e xchanged during meeting about GDS  J Carpena-Nunez L Graham C Hartzell D Racek T Tao and C Taylor 223End-to-end data system design for hyspiri mission.\224 Jet Propulsion Laboratory Education Of\002ce 2008  J Behnk e T  W atts B K obler  D Lo we S F ox and R Meyer 223Eosdis petabyte archives Tenth anniversary,\224 Mass Storage Systems and Technologies 2005 Proceedings 22nd IEEE  13th NASA Goddard Conference on  pp 81\22693 April 2005 19 


 M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Evolving a ten year old data system,\224 Space Mission Challenges for Information Technology 2006 SMC-IT 2006 Second IEEE International Conference on  pp 8 pp.\226 July 2006  S Marle y  M Moore and B Clark 223Building costeffective remote data storage capabilities for nasa's eosdis,\224 Mass Storage Systems and Technologies 2003 MSST 2003 Proceedings 20th IEEE/11th NASA Goddard Conference on  pp 28\22639 April 2003  223Earth science data and information system esdis project.\224 http://esdis.eosdis.nasa.gov/index.html  M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Evolution of the earth observing system eos data and information system eosdis\\224 Geoscience and Remote Sensing Symposium 2006 IGARSS 2006 IEEE International Conference on  pp 309\226312 31 2006Aug 4 2006  223Earth science mission operations esmo 224 http://eos.gsfc.nasa.gov/esmo  E Masuoka and M T eague 223Science in v estig ator led global processing for the modis instrument,\224 Geoscience and Remote Sensing Symposium 2001 IGARSS 01 IEEE 2001 International  vol 1 pp 384\226386 vol.1 2001  M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Earth observing system eos data and information system eosdis 227 evolution update and future,\224 Geoscience and Remote Sensing Symposium 2007 IGARSS 2007 IEEE International  pp 4005\2264008 July 2007  D McAdam 223The e v olving role of tape in the data center,\224 The Clipper Group Explorer  December 2006  223Sun microsystems announces w orld s 002rst one terabyte tape storage drive.\224 http://www.sun.com/aboutsun/pr/200807/sun\003ash.20080714.2.xml July 2008  223P anasas 227 welcome 224 http://www panasas.com  R Domikis J Douglas and L Bisson 223Impacts of data format variability on environmental visual analysis systems.\224 http://ams.confex.com/ams/pdfpapers/119728.pdf  223Wh y did nasa choose hdf-eos as the format for data products from the earth observing system eos instruments?.\224 http://hdfeos.net/reference/Info docs/SESDA docs/NASA chooses HDFEOS.php July 2001  R E Ullman 223Status and plans for hdfeos nasa's format for eos standard products.\224 http://www.hdfeos.net/hdfeos status HDFEOSStatus.htm July 2001  223Hdf esdis project.\224 http://hdf.ncsa.uiuc.edu/projects/esdis/index.html August 2007  223W elcome to the ogc website 224 http://www.opengeospatial.org 2008  223Open gis Gis lounge geographic information systems.\224 http://gislounge.com/open-gis Christine M Hartzell received her B.S in Aerospace Engineering for Georgia Institute of Technology with Highest Honors in 2008 She is currently a PhD student at the University of Colorado at Boulder where she is researching the impact of solar radiation pressure on the dynamics of dust around asteroids She has spent two summers working at JPL on the data handling system for the HyspIRI mission with particular emphasis on the cloud detection algorithm development and instrument design Jennifer Carpena-Nunez received her B.S in physics in 2008 from the University of Puerto Rico where she is currently a PhD student in Chemical Physics Her research involves 002eld emission studies of nanostructures and she is currently developing a 002eld emission setup for further studies on nano\002eld emitters The summer of 2008 she worked at JPL on the HyspIRI mission There she was responsible for the science analysis of the data handling system speci\002cally de\002ning the data level and processing and determining potential mission collaborations Lindley C Graham is currently a junior at the Massachusetts Institute of Technology where she is working towards a B.S in Aerospace Engineering She spent last summer working at JPL on the data handling system for the HyspIRI mission focusing on developing a data storage and distribution strategy 20 


David M Racek is a senior working toward a B.S in Computer Engineering at Montana State University He works in the Montana State Space Science and Engineering Laboratory where he specializes in particle detector instruments and circuits He spent last summer working at JPL on compression algorithms for the HyspIRI mission Tony S Tao is currently a junior honor student at the Pennsylvania State University working towards a B.S in Aerospace Engineering and a Space Systems Engineering Certi\002cation Tony works in the PSU Student Space Programs Laboratory as the project manager of the OSIRIS Cube Satellite and as a systems engineer on the NittanySat nanosatellite both of which aim to study the ionosphere During his work at JPL in the summer of 2008 Tony worked on the communication and broadcast system of the HyspIRI satellite as well as a prototype Google Earth module for science product distribution Christianna E Taylor received her B.S from Boston University in 2005 and her M.S at Georgia Institute of Technology in 2008 She is currently pursing her PhD at the Georgia Institute of Technology and plans to pursue her MBA and Public Policy Certi\002cate in the near future She worked on the ground station selection for the HyspIRI mission during the summer of 2008 and looks forward to working at JPL in the coming year as a NASA GSRP fellow Hannah R Goldberg received her M.S.E.E and B.S.E from the Department of Electrical Engineering and Computer Science at the University of Michigan in 2004 and 2003 respectively She has been employed at the Jet Propulsion Laboratory California Institute of Technology since 2004 as a member of the technical staff in the Precision Motion Control and Celestial Sensors group Her research interests include the development of nano-class spacecraft and microsystems Charles D Norton is a Principal Member of Technical Staff at the Jet Propulsion Laboratory California Institute of Technology He received his Ph.D in Computer Science from Rensselaer and his B.S.E in Electrical Engineering and Computer Science from Princeton University Prior to joining JPL he was a National Research Council resident scientist His work covers advanced scienti\002c software for Earth and space science modeling with an emphasis on high performance computing and 002nite element adaptive methods Additionally he is leading efforts in development of smart payload instrument concepts He has given 32 national and international keynote/invited talks published in numerous journals conference proceedings and book chapters He is a member of the editorial board of the journal Scienti\002c Programming the IEEE Technical Committee on Scalable Computing a Senior Member of IEEE recipient of the JPL Lew Allen Award and a NASA Exceptional Service Medal 21 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





