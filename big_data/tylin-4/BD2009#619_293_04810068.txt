Interface Selection for Power Management in UMTS/WLAN Overlaying Network  Mostafa Zaman Chowdhury 206   Yeong Min Jang 206   Choong Sub Ji 206 Sunwoong Choi 206  Hongseok Jeon 206\206 Junghoon Jee 206\206 and Changmin Park 206\206  206 Kookmin University, Korea  206\206 Electronic and Telecommunications Research Institute \(ETRI\, Korea   mzceee@yahoo.com   yjang@kookmin.ac.kr    Abstract 
002 The multiple choices of access networks offer different opportunities and overcome the limitations of other technologies. Optimal selection of interface is a big challenge for multiple interfaces supported mobile terminals to make a seamless handover and to optimize the power consumption Seamless handover, resource management, and CAC to support QoS and multiple interface management to reduce power consumption in mobile terminal are the most important issues for the UMTS/WLAN overlaying network. The access of both interfaces simultaneously can reduce the handover latency and data loss in heterogeneous handover. The MN may maintain one interface connection while other interface can be used for handover process. But the access of both interfaces increases the consumption of power in MN. In this paper we present an efficient interface selection scheme including interface selection algorithms, interface selection mechanism and CAC considering battery power consumption for overlaying networks. MN\222s battery power level and provision of QoS/QoE in the target interface are considered as important parameters for our interface selection algorithm. The MIH is also introduced for interface selection Keywords 002 Battery power, interface selection, CAC overlaying network 1. Introduction  The use of wireless communication has been increased tremendously in the recent years and it will continue in the future.  Due to these huge demands, varieties of user types and varieties of user\222s requirement, different wireless technologies have been developed. These technologies vary widely in terms of bandwidths, QoS provisioning, security mechanisms, price coverage area and etc. The complementary characteristics of WLANs and Universal Mobile Telecommunications System 
UMTS\ased cellular networks make them attractive for integration. This integration offers the best of both technologies. Thus a mobile node \(MN\ with multiple wireless interfaces has become increasingly popular in recent years In heterogeneous overlay network, the MN can select one interface that is best or suitable in terms of price, Quality of Service \(QoS\ality of Experience \(QoE\ throughput or other parameters as required. During connection, due to changes in the availability or characteristics of an access network may result in a situation where already established connections should be moved from one interface to another But for a MN, especially a battery-operated device with multiple wireless interfaces, power consumption is one of the critical problems g u r e 1 s h o w s t h at t h e u s e s o f  m u l tipl e interfaces consume more power than that of use just single interface. Also the access of different network interface causes different level of power consumption  
Figure 1.  Battery power consumption for the use of different interfaces  Traditionally for horizontal handovers, only signal strength and available bandwidth are used as handover decision parameters. Also for traditional overlay network, the handover decision depends on several parameters like signal strength available bandwidth, price of the link, security level, and coverage radius s pow er of t h e bat t e ry i s v e r y i m port a n t  issue for a MN with multiple interfaces, battery power level of a MN should be considered as important parameter for suitable interface selection. Thus power management issue can be added to IEEE 802.21 Media Independent Handover \(MIH  3 o r in terf ace s e lectio n in o v erla y  n e t w ork  For ou r  proposed interface selection algorithm, MN\222s battery level and provision of QoS/QoE in the target interface have been considered as important parameters for our interface selection The power consumption of a MN depends on received signal strength and type of access network. So, the proper design of call admission control \(CAC\re is also essential to reduce the power consumption of the MN This paper is organized as follows. Section 2 provides the brief description about the power management issues in IEEE 802.21 MIH. Relative works as well as our proposed algorithm for interface selection are presente d in Section 3. In Section 4 we propose interface selection steps, interface selection functional architecture and a CAC for efficient interface selection. The numerical results for the proposed algorithm are presented in Section 5. We give our conclusion in Section 6  2. MIH and Power Management  The purpose of 802.21 is to fa cilitate the handover between different interfaces \(such as 802.11, 802.16, 3GPP, 3GPP2 ISBN 978-89-5519-139-4 795Feb. 15-18, 2009 ICACT 2009 


  In Equation \(8 L p 1 for battery power level greater than a threshold value; otherwise L p K Here, K is the rank of the interface according to power consumption. For lowest power ISBN 978-89-5519-139-4 796Feb. 15-18, 2009 ICACT 2009 003 Each property may have different intervals for its values t j,l r and 802.15\ and provide a handover management scheme in such a way that is independent from particular access network features   T h e MIH in cl u d es th ree di f f e re n t  m a j o r functions. The event service initiates handover from the lower layer by means of low-layer trigger events, the command service initiates handover from the upper layer to control connectivity and the information service controls communications of basic static information The introduction of power management functionalities in MIH can improve the network selection performance. Figure 2 shows the power management functionalities in MIHF    Figure 2. Power management in MIHF  3. Interface Selection Algorithms  3.1 Related Works and Background There are several parameters those are used for handover decision. The interface selection procedure is a Multiple Attribute Decision Making \(MADM\em where alternative options are presented by multiple number of links interfaces\t handover decision depends on how the parameters are selected and how these parameters are used for interface selection algorithm. There are several works already done for this area. Different researchers 8] as s u me  different parameters for their interface selection algorithm but no one assume the status of MN\222s battery or battery profile and provision of QoS/QoE in the target interface as their interface selection algorithm. We consider these two parameters for our proposed interface selection procedure The authors in s ed s i x p ara m eters  f o r each in ter f ace  signal strength, bit rate, power consumption, price, coverage and security. They made weight vector or profile for the interface selection algorithm. The Simple Additive Weighting SAW\d Weighted Product proposed in o r t h e measurement of each property  005   8 where 1 p N and 1s 7 1m m 002 002    003  p 1j j p 1j t j,lj t l t saw W r*w RS 1 002 002 004  003  p 1j j w t j,l t l r wp r\(RS 2   7 Current level or status of MN\222s battery condition should be considered as well for interface selection algorithm with other traditional parameters. For our proposed algorithm we consider battery power level and other seven parameters M=7 signal strength \(1\, throughput \(2\, power consumption \(3\t \(4 ll coverage \(5\S/QoE level 6\d security \(7\ation \(8\easures the weight for each interface  L1\(10log Sw W p 7 1m mm p value Authors in opos ed cos t f u n c t i o n bas e d m odel f o r  interface selection algorithm. Signal strength \(s\t of using the network access technology \(c\d client power consumed for the particular access technology \(p\sed as input parameters for their algorithm. They use score function \(SF for interface selection decision  SF i w s f s,i w p f p,i w c f c,i  3  According e po w e r  consumption for a specific application in WLAN C w and power consumption in UMTS C u  are given by  C W P tw C tw P rw C rw P lw C lw P sw C sw T 4 C U P tu C tu P ru C ru P su C su P pu C pu  5 In equation \(4\d \(5 C tw  C rw C lw  and C sw  represent the power consumption in transmit, receive, idle and sleep state respectively, while P tw P rw P lw  and P sw  are the probabilities of being in any of the respective communication state C tu C ru C su  and C pu  represent the power consumption in transmit, receive signaling and power-saving state respectively, while P tu P ru P su  and P pu  are the probabilities of being in any of the respective communication state. Hence power consumption also depends on different mode of operations  3.2 Proposed Algorithm In our proposed algorithm we divide all the interface selection parameters w i nto two groups. One group takes more priority than another group for interface selection decision. We have N number of available interfaces. Suppose battery power level and other M parameters for interface selection and among them q parameters have more priority than other remaining M-q parameters, then the weight W of the measurement is presented as  W = f 1 w 1 w 2 205, w q  f 2 w q+1 w q+2 205, w M  6  Suppose S m  indicates the scaling factor of m th  parameter and L p indicates the battery power level of the MN, then the weight of p th interface among N interfaces is  L\(f S,w\(f W p mm p represents the scaled t j,l r 


consuming interface K=1 and highest power consuming interface K=N  Equation \(7\d \(8\troduce battery power level condition in the interface selection. This parameter is only effective whenever the power level goes down a threshold battery power level. Lower than threshold level means, the battery power level is going to be worst condition and thus the MN should select an interface that consumes lower power The impact of L p in the interface selection algorithm may be changed as operator desired  4. Interface Selection Procedure  Figure 3 shows the steps for interface selection. The cross layer information for different interfaces are collected and then these information and   some pre-defined policies for interface selection are checked using appropriate algorithm to make a best interface selection decision. The algorithm calculates total weight for each interface. According to result of the algorithm, all the available N interfaces are ranked. For example best interface is ranked as 1 and the worst one is ranked as N Thus MN will try to handover to best selected network. If resources are available in the best selected interface, then the MN handover to that interface otherwise it will try for the next ranked interface. This process will continue until N-1 ranked interface  Figure 3. Interface selection steps Figure 4 shows the interface selection functional architecture. The decision engine collect information from user interface, battery profile, policy engine, MIHF and link information engine. User interface provides information about the type of application, access technology, user\222s QoS/QoE requirement and etc. Battery profile provides the information about battery power level. Policy engine provides the pre-defined policies. The link information engine observes different layers condition and combine these information using cross layer optimization and then forward these information to decision engine. The decision engine selects a best interface and forwards the decision to handoff module The decision engine also makes a rank for the available interfaces according to the total weight. The handover module executes the handover     Figure 4. Interface selection functional architecture Figure 5 shows an example of a CAC for power management based interface selection procedure in a UMTS/WLAN interworking. Handover from UMTS to WLAN and WLAN to UMTS both th e cases are considered in this CAC. For UMTS networks, MN is located far away from the base station \(BS\eans lower signal level. Thus received signal level became low and MN needs more power consumption. Hence distance of a MN from BS is also considered in the CAC. If distance of the MN is larger than a threshold distance or battery level higher than a threshold level then MN will try for WLAN interface. Whenever a MN is connected to WLAN, it will remain with WLAN until battery level does not go down a threshold level and distance of the MN from the BS is not less then the threshold distance ISBN 978-89-5519-139-4 797Feb. 15-18, 2009 ICACT 2009 


Figure 5. CAC for interface selection  5. Numerical Analysis  This section provides the results of our proposed interface selection algorithm. We consider Okumura-Hata model [10  for path loss calculation in our numerical analysis. Table 1 shows the basic assumption for Okumura-Hata model. We also assume that the distance of MN from every WLAN AP is 10m whatever the distance of MN from macrocell BS. Table 2 shows the basic assumption for the parameters those are considered for interface selec tion algorithm. The assumed scaling factor of each parameter is given here. In our numerical analysis we assume requested bandwidth is available in both the interfaces  Table 1. Parameters for path loss model Access network Parameter Assumption UMTS BS transmit signal power 1.5 KW Path loss model Okumura-Hata model for macrocell L p 69.55 + 26.16logf c 226 13.82 logh b 226a\(h m 44.9 226 6.55 logh b  l ogd Height of BS 100m Height of MN 2m Receiver sensitivity 100 dB WLAN AP transmit signal power 100 mW Path loss model Okumura-Hata model for microcell L p 135.41 + 12.49logf c 226 4.99 logh b 46.84 \226 2.34logh b  o g d  Height of AP 2m Coverage area 15 m Table 2.  Assumption for weight parameters Parameter Scaling factor Weight ratio Cost 0.4 UMTS\(1\AN\(10 Throughput 0.2 UMTS\(1\AN\(10 QoS/QoE 0.09 UMTS\(1\AN\(4 Cell coverage 0.05 UMTS\(100\AN\(1 Security level 0.08 UMTS\(4\AN\(1 Signal strength 0.08 Depends on the distance of MN from UMTS BS Power Consumption 0.1 Depends on the distance of MN from UMTS BS  In our numerical analysis we calculate that the battery power consumption of a MN for UMTS interface is less than the WLAN interface if the distance of MN from UMTS BS is less than 920m. So for power saving mode, MN can select UMTS interface when the distance is less than 920m and for a distance more than 920m, the MN can select WLAN interface to connect the user for longer lime As we assume that the distance of MN from every WLAN AP is 10m whatever the distance of MN from macrocell BS the received signal strength from the WLAN AP can be considered as same value for every WLAN environment Hence the weight of WLAN interface is almost constant for normal operating condition. But with the increase in distance between UMTS BS and MN, the received signal level decrease and also battery power consumption increase. So, the total weight of UMTS interface decreases with the increase of distance. Figure 6 shows the total weight of each interface whenever MN has sufficient battery power level. It shows that WLAN is better due to low cost and higher throughput  Sufficient battery power level \(more than 50 0 0.2 0.4 0.6 0.8 1 0.1 0.3 0.5 0.7 0.9 1.1 1.3 Distance of MN from UMTS BS [km Total weight UMTS WLAN Figure 6. Total weight of each interface whenever MN has sufficient battery power level Figure 7 shows the total weight of interface selection whenever battery power level is not sufficient. At this moment saving of power is more important. Thus MN can connect with the wireless link for longer time using power saving mode Figure 7 also shows that MN should select UMTS interface ISBN 978-89-5519-139-4 798Feb. 15-18, 2009 ICACT 2009 


when distance is less than 600m by considering all the parameters. For the distance more than 600m, the MN should select WLAN interface to reduce the power consumption if both the interfaces are available Battery power level of MN is not sufficientl \(less than 50 0 0.2 0.4 0.6 0.8 1 0.1 0.3 0.5 0.7 0.9 1.1 1.3 Distance of MN from UMTS BS [km Total weight UMTS WLAN Figure 7. Total weight of each interface whenever MN has insufficient battery power level  6. Conclusion  Multiple choice of interface selection is a good opportunity to access multiple access networks with the suitable price and better QoS/QoE level as required. The main problems with the multi-mode operated MN are very high battery consumption and difficulties in the selection of best interface. In this paper we proposed some new functionality in MIH to save power The current battery power level has been considered as the interface selection parameter for the interface selection algorithm. Thus for lower battery level environment, the MN will select an interface that consume less power. We also considered QoS and Qo E level that can be provided by target network in the interface selection algorithm. The proposed functional architecture for interface selection and CAC can provide a best handover decision for overlaying network. The numerical results show that the proposed algorithm is capable to select appropriate interface in both the normal operating mode and power saving mode. The MN will support the seamless services for longer time by the proposed power saving mode operation             Acknowledgement  This research was supported by the MKE \(Ministry of Knowledge and Economy\, Korea, under the ITRC Information Technology Research Center\pport program supervised by the IITA \(Institute of Information Technology Assessment\ \(IITA-2008-C1090-0801-0019\. This research was also supported by Electronic and Telecommunications Research Institute \(ETRI\rea  R EFERENCES 1  George Lampropoulos, Apostolis K. Salkintzis, and Nikos Passas 223Media-Independent Handover for Seamless Service Provision in Heterogeneous Networks,\224 IEEE Communications Magazine January 2008  2  Liu Li, Hong Peilin, and Lu Hancheng, \223Mobile IPv6 with Multiple Interfaces in Heterogeneous Networks,\224 IEEE Conference on Communications and Networking China, 2006  3  Ching-Lun, Lin Jen-Yi Pan, and Chih-Hsiang Ho, \223Hand-Off Evolution with Multiple Hand-Off Evolution with Multiple Interfaces,\224 IEEE Computer Society September/October 2008 4  Jani Puttonen and G\341bor Fekete, \223Interface Selection for Multi-homed Mobile Hosts,\224 IEEE International Symposium on PIMRC September 2006 5  Jukka Ylitalo, Tony Jokikyyny, Tero Kauppinen, and Antti J. Tuominen Jaakko Laine, \223Dynamic Network Interface Selection in Multihomed Mobile Hosts,\224 Proceedings of the  Hawaii International Conference on System Sciences Finland, 2003 6  Shiao-Li Tsao and E-Cheng Cheng, \223Energy-Conserving Always-On Schemes for A Mobile Node with Multiple Interfaces in All-IP Network,\224 IEEE  PIMRC September 2007 7  SuKyoung Lee and Nada Golmie, \223Power-Efficient Interface Selection Scheme using Paging of WWAN for WLAN in Heterogeneous Wireless Networks,\224 IEEE International Conference on Communication Korea June 2006 8  George Lampropoulos, Alexandros Kaloxylos, Nikos Passas, and Lazaros Merakos, \223A Power Consumption Analysis of Tight-Coupled WLAN/UMTS Networks,\224 IEEE PIMRC September 2007 9  Sowmia Devi M.K. and Prathima Agrawal, \223Dynamic Interface Selection in Portable Multi-interface Terminals,\224 IEEE International Conference on Portable Information Devices Aburn, May 2007   Kaveg Pahlavan and Prasant Krishnamurthy, Principles of Wireless Networks, Prentice Hall PTR, New Jersey, 2002 ISBN 978-89-5519-139-4 799Feb. 15-18, 2009 ICACT 2009 


b  004 004 11b  where 212 004 b 4 004 and inversely proportional to the average number of photons passing through the receiver aperture  A MATLAB simulation of the maximum likelihood estimation algorithm was used to evaluate performance Figures 5a and b show the performance of the centroid algorithm for two different PSF distributions. The large dashed \(green\circle represents the standard deviation of the PSF, whereas the smaller solid \(red Cram\351r-Rao bound error ellipse representing the twodimensional standard deviation of the lower bound on estimation error.  Twenty independent estimates were made for each figure, shown by bl ue diamonds. Note that the cluster of centroid estimates in Figure 5b are biased slightly towards the center of the arra y, because the PSF is larger and closer to the edge of the detector array, hence the large array assumption is not strictly valid for this case  Next, consider the modifications needed to accommodate a finite detector array, that is, one for which the sum of counts from all detector elements does not accurately represent the total number of photons passing through the receiver aperture x offset xoffset y offset y offset  Figure  5.  Centroid estimates at offset coordinates \(10 8\ith 100 photons/pulse average: a 1 004   In Figs. 6 the offset in the x direction has been increased to 15, bringing the PSF close to the simulated detector edge at x 16. Both centroid \(red diamond\rst-order corrected blue asterisk\ates are di splayed. Note that the firstorder corrected estimates are mu ch closer to the true center of the PSF than the uncorrect ed centroid estimates, although it can be perceived that further improvements are possible to reduce the remaining bias  Maximum Likelihood Solution, Without Background Even without the presence of significant background radiation, the exact maximum likelihood solution that takes into account the effects of realistic PSF dimensions over finite detector arrays beco mes somewhat complicated to describe and evaluate. The method of solution requires the estimator to substitute trial values for the estimates in a predetermined manner, and converge on those values that  s  6 inequalities must be satisfied by the estimate of each offset coordinate   1 2 0 00 2 00 ln  210 var n n 013\212 y yxp Eyy r  10b  where E is the expectation operator. The Cram\351r-Rao bound is computed by first expressing the log-likelihood function explicitly in terms of the parameters of interest. Taking the expected value of the counts and inverting yields the Cram\351r-Rao bound for the variance of the x and y coordinate estimates   s s ij sij kExx 004 004 11a   s s ij sij kEyy n n 013\212 x yxp Exx r 10a   1 2 0 00 2 00 ln  210 var  s 212\212\013\212 212 2 1 2 00   210 var 212\212\013\212 212 2 1 2 00   210 var 004   The modifications needed to accommodate a finite detector array, that is, one for which the sum of counts from all detector elements does not accurately represent the total number of photons passing thr ough the receiver aperture even after accounting for system transmission losses and detector quantum efficiencies\s modification is a first order correction applied to the centroid algorithm attempting to remove the bias resulting from the \223edge effect\224 of an insufficiently la rge detector array. It is described in  how a t w o-st ep approach based on t h e maximum-likelihood solution n early succeeds in correcting all the bias from this edge-effect  x offset xoffset y offset y offset  Figure 6. Centroid \(red diamonds corrected \(blue stars\\(15 8\ith 100 photons/pulse average: a 2 b ii ij s kE is the total signal energy collected by the detector array. It is a pparent that the variance of the coordinate estimate for each dimension is directly proportional to 2 s 004 b 4 b   s 212  s 


016   006 006 which again results in an iterative solution whereby that value of r 006 r\f f\006 r r r r b b  004 is the effective spread of the time-pulse, and r meaning that there are 4 samples per slot, and further consider M 4 PPM in the rest of this paper: therefore there are 16 slots per PPM symbol in the following analysis and simulations  When the signal pulse is assumed to be Gaussian, the algorithm again reduces to a centr oid estimator for delay, as shown in [3 b s titu tin g d elay fo r sp atial o ffset, th e CRB for delay can be obtained by inspection from equation \(11 as st K  210 var 2  7 achieve equality. As described in [3 th is ap p r o ach  was implemented in MATLAB to generate Figs. 7a and b, where the red diamonds are the centroid estimates, and the black dots are the true ML estimates of PSF offset, near the edge of a finite detector array. No te that the exact ML estimates are unbiased, clustering around the true coordinates for both small and large PSF distributions near the edge of the array However, this improved performance requires greater computational complexity than either the centroid or firstorder corrected centroid estimates  xoffset x offset y offset y offset  Figure 7. Centroid \(red diamonds\d exact ML \(black points nates \(15, 8\ith an average of 100 photons/pulse: a 2 t NT 004 b 4 212 212   In our model the sample-duration is  s 006 006 006  before solving for the delay f is the offset we want to estimate. It is assumed that all of the signal photons collected by the focal-plane detector array are available to the temporal acquisition algorithm therefore, the average signal and background photon-counts for a time-sample are obtained by integrating power over the duration of each sample   TdtPi TdtiPi b i i b b s i i s s    1 1 r seconds, hence there are r  T samples per T second slot. In the simulations, we assume that 4 f as in For t h e case of st rong background, we can assum e  that ii sb f is selected that 004   5  T EMPORAL A CQUISITION OF THE C ENTERED PSF  W ITH A  T IMING O FFSET  After successful spatial acquisition, a closed-loop tracking system generally takes over using the array outputs for continuous spatial tracking, keeping the PSF centered over the detector array.  Since by design there is sufficient signal energy to enable communication at high data rate \(1 GBPS is the goal\it can be argued that the SNR within a much narrower closed-loop bandwidth \(typically 1 Hz \226 3 Hz in deep-space applications be sufficient to continue spatial tracking of the PSF under nominal operating conditions. Therefore, we will not pursue the details of the spatial tracking system design here, concentrating instead on the analysis of the temporal acquisition problem  Temporal acquisition is inherently a one-dimensional problem, requiring only the estimation of a single delay error instead of two orthogonal spatial offsets. This simplification makes it easier to start considering the effects of background radiation, which is now included in the model as the constant average power b P in Fig. 8  We model the laser pulse as a one-dimensional Gaussian distribution matched to the slot-duration, to facilitate analysis paralleling the spatial acquisition problem   Figure 8.  Temporal acquisition via known pattern of PPM symbols, binned modulo NM to create a single pulse to simplify processing  The temporal distribution is assumed to be of the form   2/\[\(exp{\2 22 2/12 t ts s t PtP 004\f 004\005\f 212\212  212 12  where 2 t  s 004\f\f 212 where s K is the signal count over the entire detector array, per signal pulse. As expected from our previous results, the bound on the variance of the delay estimation error is directly proportional to pulse-spread, and inversely proportional to signal energy  Strong Background, No Edge Effects When strong background is present, we have to include the background intensity in the likelihood function, and replace     with   b s s ii 


  f b s KK consecutive estimate consecutive estimate offset offset 15,11,8offset 40,100    f b b s K KK 15,11,8offset 40 210 40,100  8 simultaneously satisfied both sides of an equation derived from the log-likelihood function. In the following, the average background count generated by the detector array per PPM slot is b K and the average signal count per pulse is defined as s K   Although the structure was motivated by the strong background assumption, the algorithm for estimating  s K signal photons per pulse and 40 f both estimators are accurate; for 11   b K b 40    f b b s KKK consecutive estimate consecutive estimate offset offset a b  Figure 10  Near-optimum delay estimator \(solid blue centroid estimator \(dashed red\erformance for offsets of 8 11, and 15 samples, with 100 017\017   b K a\xact background level subtracted; b background estimate with 10% error subtracted  Returning to the Centroid Estimator of Spatial Offset in the Presence of Background Although the true  ML estimator of spatial offset in the presence of background is not considered here due to its complexity, we observe from the temporal acquisition problem that subtracting the mean background from each detector element helps to remove the bias caused by uniformly di stributed background intensity over the detector array. Becau se the background energy is uniformly distributed over the detector-plane, it tends to bias the centroid estimate toward s the center of the array, as illustrated in Figs. 11a and b: w ith true offset coordinates of  b K   When the center of the interval is at NM 8, at greatest distance from both edges where edge effects are the least significant, we expect and obtain best performance. Twenty independent estimates were made using the centroid algorithm \(dashed red lines\he near-optimum solution solid blue line\in Figs. 9 and 10. With a total signal energy of 100 photons and background of 8 photons per slot, both the centroid and the near-optim um estimates are of similar quality when the true offset is 8  b K  subtracted out before processing, however the scatter in the estimates also increased signi ficantly. When the estimate of the background level is not know with perfect accuracy, as is usually the case, the sca tter of the centroid estimator increases still further, while the near-optimum estimator begins to suffer from outliers even for offsets near the center of the interval as shown in Fig. 10b, where a 10 error was introduced into the mean background estimate, so that 36 210 f this nearoptimum algorithm tends to chose between 0 and 16 with equal probability  15,11,8offset 8,100  f  both estimates are biased towards the center  Next, we ask if subtracting the mean background from each slot could reduce the bias in the centroid and near-optimum estimators, leaving only zero-mean fluctuations. Since perfect estimates of the background are generally not available, errors in the estim ate of the mean were also incorporated into the simulation. The results are shown in Figures 10a and b. It is immediately apparent in Fig. 10a that the mean of the estimates for both the centroid and the near-optimum estimators improved to 8, 11 and 14 when a perfect estimate of the background level 40 210 f and 15 in Fig. 9a. As before, the reason is that the center-of-mass of the constant background distribution is at the center the interval, hence the centroid estim ator is biased towards the center in the presence of background  The near-optimum estimator mitigates this problem by weighting the observed counts according to the Gaussian pulse model, which tends to suppress the background on the tails of the distribution, but emphasizes the signal near the true delay to compute a more accurate estimate. Only when edge effects begin to dominate near the edge of the interval does the near-optimum algorithm begin to under-estimate the delay, since edge-effects have not been incorporated into the model. With a high background level of 40 photons per slot as shown in Fig. 9b, both estimators fail to overcome the background and underestimate the true delay near the edge of the PPM symbol: for 8 f however the centroid estimator under-estimates the offset when 11 f hen 16   b K   15,11,8offset 36 210 40,100  b K  was used  instead of its true value of  40 f the estimates are closer to 10 somewhat lower for the centroid estimator 15  s K photons per pulse: a 8    f b s KK a b  Figure 9.  Near-optimum delay estimator \(solid blue\and centroid estimator \(dashed red\erformance over 20 consecutive estimates, for offsets of 8, 11, and 15 samples, with 100 f  was found to work best with moderate background improving significantly over the centroid algorithm which was shown to be optimum only in the absence of background. Wrap-around effects were incorporated into the simulation at the upper boundary by adding in the counts that would have occurred in slots past the upper boundary, into the corresponding slots at the beginning of the observation interval, modulo NM Since wrap-around was implemented only at the upper boundary, the algorithm works properly only for offsets at and above the center of the interval \(that is 158 


004 004 004 004 004 005 005\004 212\212 212 212\212 212   Note that in the limit as R approaches infinity all of the signal energy is captured according to the Gaussian PSF model. Therefore we define s s R KR R ij k b b s s R RR KR  9 13, 13\and with a small spotsize so that edge effects are not significant, the addition of background counts shifts the centroid estimates towards \(8,8 which is the center of the detector array. This shift can be eliminated by subtracting out the average background count from the array observable, leading to nearly unbiased estimates as shown in Fig. 11a \(blue dots ever the variance of the estimates increase significantly due to the large random distribution of counts around th eir mean value across the array. Note that any error in the estimate of the average background leads to both increased variance and bias, as shown in Fig. 11b, where the mean background counts subtracted from each detector are in error by 10%. The resulting estimation variance can be further reduced by averaging a large number of independent estimates however the bias term is not reduced by averaging Therefore it is important to obt ain accurate estimates of the average background before subtr action to avoid steering the receiver pointing away from the true source direction due to the induced bias  x offset y offset x offset y offset  Fig. 11  Centroid estimates without removing average background \(red diamonds background level removed \(blue asterisks coordinates \(13, 13\ith an average of 100 signal photons and 1 background photon per detector element 2 004  i j b where b 004 a background level underestimated by 10   6  O PTIMIZED FOV  M AXIMUM L IKELIHOOD D ETECTION OF PPM  S YMBOLS  Following successful spatial acqui sition, we assume that the PSF remains centered over the detector array by means of a closed-loop tracking circuit Temporal acquisition and tracking is also assumed, meaning that the receiver clock has been synchronized with the PPM slot-boundaries Under these conditions, the receiver FOV can be optimized to achieve best detection performance by increasing the acceptance-angle of the receive r in small increments, and computing the PPM symbol error probability for each increasing radius R in the detector-plane, proportional to linear FOV. This is the scenario depicted in Fig. 12, where the PSF and hence the two-dimensional FOV are assumed to be circular. The signal distri bution is taken to be circular Gaussian as before, with st andard deviation equal to s 006\005 2  lim 006 is the average background photon-count per square centimeter in the detector-plane  Consider the probability of corr ect symbol detection, with PPM signaling. For any radius R the probability of correct symbol detection is at least as great as the probability that the photon-count in the correct slot containing the signalpulse exceeds the count in every other slot.  To be more precise, tying equalities in r 1 1 r  M    ty pically resolved by  tossing a fair r sided die, but with high average signal and background counts these events have extremely small probabilities, hence can be neglected.  With this approximation, the lower bound on the probability of correct decision  CP l M as a function of R is given by  006\005 2  b 020\021 lim where s K is the total signal energy per pul se collected by the receiver aperture, measured in terms of photon energy. However, the background photon distribu tion is assumed to be uniform in the detector-plane, hence th e collected background energy increases with R in proportion to the area of the circular FOV, hence there is no limit to the amount of background energy that can be collected by the receiver according to this model b b RR  s b b 020\021 Focal-plane Detector radius R  Figure 12.  Optimization of receiver FOV by adjusting the radius of the acceptance-disk in the detector plane to minimize the probability of symbol error  The amount of signal energy collected by a circular FOV of radius R is   2  exp 1  2  exp  2  exp  2  exp 2 2 1 22 0 22 22 0 2 22 0 2 s R s s R s s R s R r drr r drrr 


 PSF  b 004 Detector array radius R cm Average number of phot ons  Fig. 13a.  Average signal and background energies as a function of acceptance radius \(cm\ the detector-plane, for 2 3 and 4 cm signal spot \(1.25 AU  cm 1 006 Detector array radius R cm Uncoded symbol-error probability, PSE  Fig. 13b. Example of symbol error probability optimization with 2, 3 and 4 cm signal spot \(1.25 AU  It can be seen in Fig. 14 that while uncoded performance may be marginally acceptable at closest approach, the symbol error probability decr eases rapidly at greater distances, since it is inversely proportional to the square of the distance. At the nominal distance of 1.25 AU the symbol error probability is no longer acceptable for data reception having degraded to approximately PSE 0.02. Therefore, at all but the closest distance considered, some form of errorcorrection coding will be necessary to achieve the required error probabilities with the 26 meter design   PSF  PSF  10 b\212 b 327 b+\b\212 b+\b 013 212 212  020  13  The corresponding symbol error probability  EP M is actually somewhat less than pr edicted by this approach hence, receiver performance is actually somewhat better but the difference is not significant  Performance of the 26 meter Receiver We first evaluate the performance of the 26 mete r receiver option described in Section II.  The performance optimization, which is actually a minimization of th e symbol error probability PSE  R  R was carried out numerically using an accurate Gaussian approximation to the Poisson probabilities given in \(13\described in The radius was increased in small increments, and signal and background energies as well as PSE  R puted for each radius. The varia tion of signal and background energies for PSF spreads of approximately 2, 3 and 4 cm is shown in Fig. 13a \(corresponding to cm\ 2 and 1.5  ,1  PSF  PSF 004  with the corresponding symbol-error probabilities shown in Fig. 13b. As expected, performance is best for the smallest PSF, since this allows collecti on of the signal energy with minimal background. Performance deteriorates by more than an order of magnitude as the PSF spread is increased from 1 to 2 cm \(corresponding to 220 and 440 rad 265  respectively when the antenna\222s effective focal-length is 91 meters, as is the case for the DSN 34 meter research antenna at Goldstone, CA\phasizi ng the importance of reducing the PSF hence the receiver field-of-view  Performance of a hypothetical 26 meter optical photoncounting receiver utilizing the inner solid panels of an operational 34 meter antenna, is shown in Fig. 14 for the following system parameters:  50 cm transmitting optics, 10 watts of transmitted laser power \(assuming 50% efficiency through the transmitting optics\eter receiver corresponding to 531 square mete rs of aperture, background radiation evaluated at SEP = 10 degrees \(this value is used only to provide a constant scattered background level for the numerical examples, and does not necessarily account for orbital geometry constraints  In addition, PSF sta ndard deviation of 1  004 cm 5.1 004 cm 2      1 exp   exp    1 1 0 1 EPEPCPEP R k R RR k RR CP M M l M u M M k j b j b b s k k b s l M 002\013\212\t 004 cm, and optical system throughput of 0.324 was assumed, which includes main reflector and subreflector losses \(90 reflectivity assumed on each surface transmission losses \(assumed to be 80% transmission for a 2 angstrom optical filter at 1550 nm  quantum efficiency. Perform ance was determined as a function of R for the nominal range of 1.25 AU, as well as 0.87, 1.8 and 2.5 AU. The inset shows the increase of collected signal and background energies as a function of R  note that optimum performance occurs at 1.75 cm acceptance radius for all cases considered in Fig 14  Background photons increase with R Signal photons saturate at s K cm 1  PSF  PSF 004 cm 5.1 004 cm 2 004 2 cmslot,/photons 7.16  PSF 


 s K AU\ \(2.5 photons 15  s K 60  s K AU\ \(1.25 photons 60  PSF  s K 30  11 Det ect or array radius R cm Uncoded symbol-error probability, PSE Dat ar at e = 1 GBPS M = 4 PPM, 0.5 ns slots cm 1 b  R s b 120  s K AU\ \(1.8 photons 30 212  BER With only about 15 signal photons received at the greatest distance of 2.5 AU as shown in Fig. 14, the required BER of approximately 10 6 for communications links cannot be achieved with this high-rate code.  In this case, a lower code rate could be used, but at the cost of additional overhead The threshold of the simulated rate \275 code is less than 14 signal photons per pulse on the average in this highbackground environment \(152 background photons per slot at 10 degree SEP angle could achieve the required BER with a 26 meter receiver even at the greatest distances considered  Bound rate \275 Bound rate 7/8 Simulation rate \275 code Simulation rate 7/8 code Uncoded BER PSE = 2 M 1\ER M  Figure 15. Coded performance of M = 4 PPM with background level corresponding to optimized acceptance angle for 2 cm spot, rate \275 and 7/8 codes  Performance of the 10 meter Receiver Finally, we consider the 10 meter receiver option described in Section II.  The salient characteristics of this receiver design are smaller turbulence-limited FOV due to higher-quality optics, but also a much smaller collecting area of only 78.5 square meters. Therefore the 10 meter receiver collects a fraction 78.5/531 = 0.148 of the signal power collected by the 26 meter antenna, corresponding to 8.3 dB less signal energy.  However, due to higher quality optics the FOV can be reduced to 55 micro-radians \(roughly the turbulenceinduced limit at 10 degree SEP angle\yielding an effective spotsize of 0.5 cm at the Casse grain focus. The distribution of background intensities in th e focal-plane have been scaled by the ratio of the apertures for the two options enabling direct comparison of optimized performance  Uncoded performance of the 10 meter receiver design is shown in Fig. 16, for spacecraft distances of 0.87, 1.25, 1.8 and 2.5 AU respectively.  At the nominal distance of 1.25 AU, three performance curves with slightly different spot sizes are plotted on the same graph: red corresponds to 0.3 green to and 25.0  toblue,2.0 004 cm.  It is noteworthy that not much background is co llected with any of these spots to begin with, hence performance cannot be improved significantly by reducing th e spotsize for the 10 meter receiver option   s K  Figure 14.  Uncoded symbol-error probability performance of 26 meter receiver as a function of detector-plane acceptance angle, at distances of 0.87, 1 25, 1.8 and 2.5 AU: approximately 2 cm spot  Examples of the performance improvement afforded by coding are shown in Fig. 15, we re a 2 cm spot was assumed and uncoded BER bit error rate\the BER for two coded systems are shown: one a rate \275 code, the other a rate 7/8 code.  The rate \275 code is a \(15120, 7558\serially concatenated convolutional PPM code, whereas the rate 7/8  code is a \(8176, 7154\ code currently proposed as a CCSDS standard high-rate c ode.  Each of these codes utilizes iterative demodulation of the PPM symbols providing an additional gain of approximately 0.2 dB. Both of these codes have been implemented in FPGA, hence represent practical implementati ons that can be used with confidence to predict coded performance of the proposed optical receivers  It is evident that the more pow erful rate \275 code attains 7 dB coding gain over uncoded BER at error probabilities of interest to missions 6 10  PSF 004 AU\ \(0.87 photons 120 212 002 BER  7/8  code provides 4 dB of coding gain.  Also illustrated are bounds \(sometimes referred to as capacity bounds\on the BER derived from the converse to the coding theorem for rates \275 and 7/8 These provide a lower bound on achievable error probability for any codi ng scheme at a given rate Note that a simulated rate 7/8 code would require an average of more than 16 photons to reach 6 10  s K 15  s K R cm Average phot ons  R b 


b  R s  s K 88.8  12 Det ect or array radius R cm R cm Average phot ons Uncoded symbol-error probability, PSE  R b  s K AU\ \(2.5 photons 22.2  s K AU\ \(1.25 photons 88.8 b 8.17  PSF  s K Dat a-r at e = 1 GBPS M = 4 PPM, 0.5 ns slots cm 25.0  s K 004 0.2  0.25  0.3 cm  Figure 16.  Uncoded symbol-error probability performance of 10 meter receiver as a function of detector-plane acceptance angle, at distances of 0.87 1.25, 1.8 and 2.5 AU: approximately 0.4, 0.5 and 0.6 cm spots  It should also be noted that the uncoded error probabilities achieved by the 10 meter receiver are not quite as good as those of the 26 meter receiver at any distance, despite much better optical surface quality:  this is a direct consequence of the 8.3 dB loss in relative signal energy.  However, since the background energy at minimum symbol error probability is only about 2 phot ons on the average per slot coded performance nevertheless remains acceptable at moderate distances, as shown in Fig. 17  For the 10 meter receiver, the threshold for the rate \275 code is approximately 5 signal photons on the average and a little more than 8 photons for the rate 7/8 code, which means that acceptable performance can be achieved at intermediate distances of 1.25 AU or less with either code  However, if the less powerful rate 7/8 code was employed then the required performance could not be achieved at distances much greater than 1.25 AU, since more than 8 photons would be required to reach the coding threshold which however is not feasible with the 10 meter receiver design as can be seen in Fig. 16  7  S UMMARY AND C ONCLUSIONS  We have presented two different system concepts for hybrid RF/Optical receivers based on the DSN 34-meter antennas currently employed for deep space communications Although significantly di fferent in terms of opto-mechanical design,  the key features of these two concepts can be characterized as follows: th e 10 meter option utilizes highquality glass mirrors added to the main reflector of the 34 meter antenna to achieve an e quivalent collecting aperture of a 10 meter diameter optical receiver, with a narrow fieldof-view consistent with day time turbulence when pointing close to the sun \(approximately 55 micro-radians meter option makes use of the existing solid aluminum panels of the inner 26 meter of the antenna polished to optical smoothness, and is assu med to achieve a wider fieldof-view of  200-400 micro-radians due to larger surface figure errors Simulation rate \275 code Bound rate \275 Bound rate 7/8 Simulation rate 7/8 code Uncoded BER PSE = 2 M 1\ER M  Figure 17. Coded performance of M = 4 PPM with background level corresponding to optimized acceptance angle for 0.5 cm spot, rate \275 and 7/8 codes  A mathematical model of the optical communications system applicable to both concepts was developed consisting of a focal-plane array of photon-counting detectors, optical filter to limit background, and pointspread functions in the focal-p lane incorporating the surface figure errors of the two concep ts. Both spatial and temporal acquisition algorithms were considered, and it was shown that in the absence of bac kground radiation and with large detector arrays the maximum likelihood estimators took the form of simple centroiding ope rations. When the detector array was comparable in size to the PSF then edge-effects had to be considered, and several modifications were derived that ameliorated the bias caused by these edge effects  It was also found that unifo rmly distributed background introduced large biases into the centroid estimates for both pointing offset and temporal delay, requiring modifications to the original algorithms to overcome these effects  Finally, detector FOV was optimized for both design concepts to minimize PPM symbol-detection error probability in the presence of strong background characteristic of daytime operation when pointing close to the sun. Uncoded performance was evaluated assuming M 4 PPM and 1 GBPS data-rate, and it was shown that the 26 meter collecting aperture outperformed the 10 meter aperture despite its narrower F OV, since the larger aperture collected much more signal photons. Coded performance was also considered with op timized FOV, demonstrating that acceptable coded performance could be achieved with  s K 44.4  PSF 004 AU\ \(1.8 photons 44.4  s K 22.2 AU\ \(0.87 photons 8.17 


 13 either design for communications distances characteristic of nearby planets  Acknowledgment The research described in this paper was carried out at the Jet Pr opulsion Laboratory, California Institute of Technology, under contract with the National Aeronautics and Space Administration  The authors would like to thank Sabino Piazzolla of JPL for determining the effective sky radiance at 10 degrees SEP angle due to atmospheri c scattering and dust on the optics, used in the calculations in Section 6. The authors would also like to thank Philip Tsao for generating the SEP angle predicts for Mars in Figure 2    R EFERENCES   R  Gagliardi and S. Karp Optical Communications  John Wiley and Sons, New York, 1976 2] V. Vilnrotter, M Srinivasan, \223Adaptive Detector Arrays for Optical Communications R eceivers,\224 IEEE Transactions on Communications, Vol. 50, Issue 7, July 2002  V. Vilnrotter, \223Hy b rid R F Optical Communications via 34-m DSN Antennas,\224 IPN Progress Report 42-181, Jet Propulsion Laboratory, February  15, 2009     B IOGRAPHIES   Victor A. Vilnrotter  M'79, SM\22202 received his Ph.D. in electrical engineering and communications theory from the University of Southern California in 1978.  He joined Jet Propulsion Laboratory Pasadena, CA in 1979, where he is a Principal Engineer in the Communications Architectures and Research section. His interests include electronic compensation of large antennas with focal-plane arrays adaptive combining algorithms for large antenna arrays optical communications thr ough atmospheric turbulence the application of quantum communications to deep-space optical links, and the devel opment of uplink array calibration and tracking tec hnologies. He has published extensively in conferences and refereed journals, and has received numerous NASA awards for technical innovations         Daniel J. Hoppe M'79, SM\22200  received the B.S. and M.S. degrees in electrical engineering from the University of Wisconsin Madison in 1982 and 1983, and the Ph.D. degree also in electrical engineering, from the University of California Los Angeles in 1994. He is currently at the Jet Propulsion Laboratory where he is a principal engineer His efforts there involve electromagnetic analysis and design of microwave and optical devices for a number of ground-based and space-based applications. Dr. Hoppe has also been a visiting lecturer at UCLA since 1995. He was awarded the NASA Exceptional Service Medal in 1993, and in 1994 he was recognized as the outstanding Ph.D. student in the School of Engineering and Applied Science at UCLA Dr. Hoppe is a senior member of the IEEE, and has been awarded several U.S. patents  Bruce Moision received the B.S degree in Engineering from Harvey Mudd College in 1991. From 1991 to 1994 he was with Datatape, Inc. in Pasadena, CA. He received the Ph.D degree in Electrical and Computer Engineering from the University of California, San Diego, in 1999 During 2000 he held a postdoctoral position with the Mathematics of Communications Research Group at Lucent Technologies, Murray Hill, NJ, and has been a member of the technical staff at JPL since 2001. His work covers the areas of e rror control coding and signal processing for free-space optic al communications channels  Jeffrey R. Charles founded Versacorp\231 in 1983, while becoming self-educated in di sciplines required to design patented products and propr ietary technology, and participate in related produc t prototyping, production and distribution. Products incl uded multiple function flip mirrors for telescopes and surveillance instruments, and technology for 360 degree panoramic and full sphere imaging.  He presented many lectures, published several technical papers, and authored a Springer-Verlag book Jeff worked as a contractor at the Jet Propulsion Laboratory \(JPL\ from 1992 to 1995, then joined JPL as an employee in 2000, having operated Versacorp\231 in the interim.  He has been a memb er of two JPL flight project teams \(MER and COSMIC\.  His work interests include conceptual design, systems architecture, and innovations that reduce the cost of larg e aperture optical systems for ground based, airborne, and s pace based applications Since joining JPL, he has authored New Technology Reports and co-authored  papers published in conference proceedings and journals  


University Working Paper #15 18] DiMaggio P., Hargittai, Eszter, Celeste, Coral and Shafer, Steven. 2004. Digital Inequality From Unequal Access to Differentiated Use. In K Neckerman \(Ed Russell Sage Foundation: New York 19] Dixit, A. The Making of Economic Policy: A Transactions Cost Perspective. MIT Press Cambridge, MA, 1996 20] Downs, A. An Economic Theory of Democracy Harper Brothers: New York, 1957 21] Evans, P., Rueschmeyer, D., and T. Skocpol, eds Bringing the State Back In. Cambridge University Press, New York, 1985 22] Graham, C. ?The Economics of Happiness Insights on Globalization from a Novel Approach,? World Economics, v.6, No.3, JulySeptember 2005 23] __ ?Insights on Development from the Economics of Happiness,? World Bank Research Observer, 20\(2 24] Grindle, M. Going Local: Decentralization Democratization, and the Promise of Good Governance. Princeton University Press Princeton, 2007 25] Groves, R., Dillman, D., Eltidge, J., and R. Little Survey Nonresponse. John Wiley and Sons, New York, 2001 26] Hargittai E. 1999. Weaving the Western Web Explaining Differences in Internet Connectivity Among OECD Countries. Telecommunications Policy 23\(10/11 27] Hall, Peter. Cities of Tomorrow. Blackwell Publishing, Boston, 2002 28] Hargittai E. 2002. Second-Level Digital Divide Differences in People's Online Skills.First Monday 7\(4 http://firstmonday.org/issues/issue7_4/hargittai 29] Hayek, F. ?The Use of Knowledge in Society American Economic Review, XXXV, No. 4 September, 1945, pp. 519-30 30] Hayek, F. ?The Pretence of Knowledge,? in Lindbeck, A., ed Nobel Lectures, Economics 1969-1980, World Scientific Publishing Co Singapore, 1992 31] Heeks, R. 1999. Information and communication technologies, poverty, and development Development Informatics Working Paper Series Institute for Development Policy and Management. University of Manchester 32] Herrera, Y. Imagined Economies: The Sources of Russian Regionalism. Cambridge University Press, New York, 2005 33] Herz, B., &amp; G. Sperling. Evidence and policies from the developing world: What works in girls education. Executive summary. Council on Foreign Relations, New York, 2004 34] Hj  rland, Birger \(2007 Knowledge Organization. Annual Review of Information Science and Technology 41:367 405 35] Hj  rland, Birger \(2004 organization and the feasibility of universal solutions. Presented at the Eighth International ISKO Conference, London, July 13-16, 2004 36] Jenkins, R. Democratic Politics and Economic Reform in India. Cambridge University Press New York, 1999 37] Kanungo, S. 2004. On the emancipatory role of rural information systems. Information Technology and People. 17\(4 


Technology and People. 17\(4 38] Loegelin, M. 1992. Interformation et development: etude synthetique des lignes de force du discourse universitaire. Institut Universitaire de Technologie B, Department Carrieres de l?information et del al communication, Universite de Bourdeaux 39] Mai, J-E. "Likeness: A Pragmatic Approach" In Dynamism and Stability in Knowledge Organization.   Proceedings of the Sixth International ISKO Conference. Advances in Knowledge Organization, 7: 23-27, 2000 40] Mai, J-E. "The Concept of Subject: On Problems in Indexing" In Knowledge Organization for Information Retrieval.  Proceedings of the 6th Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 International Study Conference on Classification Research. 6: 60-67, 1997 41] Mas-Colell, A., Whinston,M., and J.Green Microeconomic Theory. Oxford University Press New York, 1995 42] Madon, S. and Sahay, S. 2002. An Informationbased Model of NGO-mediation for the Empowerment of Slum Dwellers in Bangalore The Information Society:18\(1 43] McCubbins, M., and T. Schwartz. ?Congressional Oversight Overlooked: Police Patrols vs. Fire Alarms,? American Journal of Political Science February 1984 44] McKenzie, D., and D. Mookherjee. ?Distributive Impact of Privatization in Latin America: An Overview of Evidence from Four Countries Economia, vol. 3, no. 2, Spring 2003, pp. 161218 45] Menou, M.J. 1985. An overview of social measures of information. Journal of the American Society of Information Science and Technology 36\(3 46] Oates, W. ?An Essay on Fiscal Federalism Journal of Economic Literature, 1999, 37, \(3 1120-1149 47] Olken, B. ?Revealed Community Equivalence Scales,? Journal of Public Economics 89 \(2005 545-566 48] Neuwirth, R., Shadow Cities: A Billion Squatters Routledge, London, 2006 49] Perlman, M., ?Political Purpose and National Accounts,? in Alonso, W. and P. Starr, eds. The Politics of Numbers, Russel Sage Foundation New York, 1987 50] Persson, T. ?Do Institutions Shape Economic Policy?? Econometrica, v.70, no.3, 2002 51] Puri, S.K. and Sahay, S. 2003. Participation through communicative action: A case study of GIS for addressing land/water management in India. Information Technology for Development vol 10: 179-199 52] Scott, J. Seeing Like a State: How Certain Schemes to Improve the Human Condition Have Failed. Yale University Press, New Haven, 1998 53] Seely-Brown, John. and S. Duguid. The Social Life of Information. Harvard Business Press Cambridge, 2002 54] Sen, A. Development as Freedom. Oxford University Press, New York, 1999 55] Somasundaram, L. personal correspondence Chennai, India, December 20, 2007 56] Srinivasan, R., Pepe, A., and Rodriguez, M Eliciting Cultural Ontologies: A comparison between Hierarchical Clustering Methods and Participatory Design Processes.?, under review 


57] Srinivasan, Ramesh, and Jeffrey Huang. "Fluid Ontologies for Digital Museums." International Journal of Digital Libraries.5 \(2005 58] Srinivasan, Ramesh. "Ethnomethodological Architectures: Information Systems Driven by Cultural and Community Visions." Journal of the American Society for Information Science and Technology 5.5 \(2007 59] ---. "Indigenous, Ethnic and Cultural Articulations of New Media." International Journal of Cultural Studies 9.4 \(2006 60] ---. "Where Community Voice and Information Society Intersect." The Information Society 22 2006 61] Star, Susan Leigh, and James R. Griesemer Institutional Ecology, 'Translations' and Boundary Objects: Amateurs and Professionals in Berkeley's Museum of Vertebrate Zoology, 190739." Social Studies of Science 19 \(1989 420 62] Starr, P, ?The Sociology of Official Statistics 63] Suchman, Lucy A. Human-Machine Reconfigurations: Plans and Situated Actions 2nd ed. ed. Cambridge: Cambridge University Press, 2006 64] ---. "Located Accountabilities in Technology Production." Scandanavian Journal of Information Systems 14.2 \(2002 65] Stepan, A. State and Society: Peru in Comparative Perspective. Princeton University Press, Princeton, NJ, 1978 66] Turban, E., Aronson, J., Lian, T., Sharda, R Decision Support and Business Intelligence Prentice-Hall, New Jersey, 2006 \(8th Edition 67] Ulman, J. Future Tendencies in Computer Science, Control and Applied Mathematics Springer-Verlag, Heidelberg, 1992 68] United Nations Human Development Report http://hdr.undp.org/en 69] Vernon, R., ?The Politics of Comparative Economic Statistics: Three Cultures and Three Cases,? in Alonso, W. and P. Starr, eds. The Politics of Numbers, New York, 1987 70] Warschauer, M. ?Reconceptualizing the Digital Divide,? First Monday, v.7, no.7, July 2002 71] ___. Demystifying the digital divide. Scientific American 289\(2 72] Watson-Verran, Helen, and Leon White. "Issues of Knowledge in the Policy of SelfDetermination for Aboriginal Australian Communities 73] Wallack, J.S., and N.K. Singh. ?Of Demolition Drives and Creative Destruction,? Indian Express, October 15, 2006 74] Wallack, J.S. ?Fire Alarms in Action: Making Sense of User Feedback on City Services in Karnataka, India,? under review, 2008 75] Wallack, J.S, and S. Nadhamuni, ?User Innovation and eGovernance Design forthcoming in Hidden Successes: Urban Reforms in India, MIT Press, Cambridge, MA 76] Ziegler, E., and J.S. Wallack, ?State Capacity m/s, UCSD, San Diego, 2008 77] Wallack, J.S., and T.N. Srinivasan. Federalism and Economic Reform. Cambridge University Press. Cambridge, 2006 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


especially of computer networks, is often characterized by the multi-state performance degradations before reaching the complete failure. This multi-state nature of reliability has been increasingly recognized in recent years, and the most significant advances have been in the field of multistate reliability modeling \(Lisnianski and Levitin 2003                                                 4. STRATEGIC MODELING AND DYNAMIC   Box 3.3. Multivariate Survival Analysis  Dependence Mechanisms  Dependence is often the central focus of multivariate survival analysis, and together with censoring, they complicate survival analysis most significantly. Hougaard \(2000 three mechanisms of dependence: \(i ii iii Common events are equivalent to common mode failures in reliability analysis, for example, failure events due to accidents or natural disasters. The common risks mechanism describes the scenario that the individuals are dependent due to some common unobserved risks such as common genes in siblings or a bug in the operating system that may affect all software running on 


operating system that may affect all software running on it. The third mechanism, event-related mechanism refers to the phenomena that the actual event itself changes the risk, such as virus infection of a computer node Shared Frailty Modeling  Shared frailty theory occupies the central focus of multivariate survival analysis. The dominance is not accidental because frailty captures crucial but often directly unobservable \(latent random variables, which are attributable for dependence and variations. To some extent, dependence, variation randomness, and frailty all address some facets of multivariate failure events. The frailty theory provides a unique set of approaches to address the theoretical and practical issues related to those four concepts.  Similar to the fact that censoring is the trademark feature of survival analysis \(both univariate and multivariate frailty theory is the trademark of multivariate survival analysis. These two trademark properties, censoringhandling and frailty modeling, have made multivariate survival analysis a unique and indispensable mathematical tool for biomedical and public health research Multi-State Modeling  Multi-state has been developed concurrently in several fields in the last decade, including survival analysis \(Hougaard 2000 reliability \(Lisnianski and Levitin 2003 Commenges 1999 Huzurbazar 2006 multi-state modeling has been adopted to solve the domain specific problems, and the flow graph modeling offers a flexible and powerful approach to analyze multi-state models in general. Multi-State model is often formulated as Markov Process models. The transition probabilities in multi-states modeling are well defined by survivor functions \(Hougaard 2000 multi-state model is to estimate the state transition probabilities from hazards functions. multivariate system. According to Lisnianski and Levitin's \(2003 any systems that can have a finite number of performance rates are multi-state systems. Furthermore a system may consists of components which themselves could be multi-state subsystems 10 4. STRATEGIC LEVEL MODELING AND DYNAMIC HYBRID FAULT MODELS  Hybrid fault models are derived from agreement algorithms Lynch 1997 redundancy management in fault tolerance.  Therefore, the choice of hybrid fault models directly affects reliability analysis and fault tolerance. Despite the close ties between reliability and fault tolerance fields, Ma &amp; Krings \(2008e Ma 2008a with regard to the quantitative relationship between reliability analysis and hybrid fault models. To address the unrealistic assumption, they proposed a new concept  dynamic hybrid fault \(DHF mathematical tools \(survival analysis and evolutionary game theory modeling we first briefly describe the issues involved and summarize some of their key findings in Box 4.1  Agreement algorithms started out with the introduction of the Byzantine general problem by Pease et al. \(1980 Lamport \(1982 system are abstracted as generals of an army.  Loyal generals \(non-faulty components algorithm e.g., to attack or retreat while traitors \(or bad components others by sending conflicting messages. Because the focus of agreement algorithms is to reach agreement in the presence of faults and not on issues related to fail rates, the 


presence of faults and not on issues related to fail rates, the failure rate is often ignored or is implicitly assumed to be constant. In other words, the agreement algorithm, together with the assumptions about faults and their cardinality determines if agreement can be reached.  However, there is no direct link to the notion of real time in agreement algorithms or the associated hybrid fault models. This implies that agreement algorithms only specify whether or not an agreement can be reached, given a certain number of traitors, but they do not keep track of when the generals committed treason. Ignoring the notion of real time is appropriate in the study of agreement algorithms because they are only concerned with provable agreement even if the voting is dynamic \(multiple rounds of voting time is abstracted as rounds which are discrete and do not bear real-time values  The key issue is that the time \(in terms of rounds agreement algorithm or the hybrid fault models does not capture the process dynamics in terms of real time as needed in the analysis of reliability. When fault models are applied in analyzing a real-world system consisting of multiple components \(generals must be considered. Some generals may be loyal for their entire lifetimes; some may quickly become "corrupted;" still others may be loyal for long time but ultimately become  corrupted  Each of the generals may have different inhomogeneous not-constant function, ?i\(t generals  In most cases, the failure rate is completely ignored, and then the hybrid fault models are treated as static. For example, an engineer may claim that a system he designed will be able to tolerate 2 Byzantine faults, because the design has 7 redundant parts \(3m+1, m is the number of faults notion. In general, if we replace m with a time-dependent function m\(t t t-1 t t function of parts, then the statement becomes a timedependent predicate  In other cases, the consequences of different faults or failure modes can be hugely different. If a benign fault is replaced by a malicious fault, the consequence could be disastrous Without real-time notion, the dynamics of failure modes are never captured  From the above discussion, it is obvious that the lack of real-time notion in agreement algorithms and associated hybrid fault models is a critical issue of significant theoretic and practical implications, and the issue should not be ignored when agreement algorithms and hybrid fault models are applied to reliability analysis. To address this issue, Ma amp; Krings \(2008e, Ma 2008a fault \(DHF dynamic in two senses: the time-dependent failure rate and time-dependent failure modes  Actually, the issue is much more complex than what is briefly described above because the problem has two essential aspects.  The above brief introduction only reveals one aspect of the problem, i.e., the lack of real time notion This aspect is relatively straightforward to address and the proposed extension is to introduce time and covariate dependent failure hazard or survivor function \(see Box 3.1 The other aspect of the same problem arises when one tries to apply the new extension for reliability analysis. In other words, the extension with time and covariate dependent hazard function is necessary but not sufficient.  To integrate hybrid fault models or dynamic hybrid fault models reliability analysis requires the capability to simultaneously 


reliability analysis requires the capability to simultaneously process multiple failure behaviors with very different qualitative and quantitative properties. The term qualitative refers to the fault types, such as benign, symmetric asymmetric, or the transmissive and omissive Unfortunately, traditional reliability analysis cannot handle the above requirements. To achieve the flexibilities and capabilities to simultaneously quantify several types of failure behaviors required by hybrid fault models, and still to be capable of incorporating complex time-dependent failure rate represented with survival analysis models, Ma and Krings \(2008e, Ma 2008a game theory model to integrate dynamic hybrid fault models and reliability models \(represented with survival functions  Box 4.1 and 4.2 briefly describe the concepts involved in two aspects of dynamic hybrid fault \(DHF Detailed contents are referred to Ma &amp; Krings \(2008e, Ma 2008a 11                                                     


                                                Box 4.1. Agreement Algorithm Aspect of Dynamic Hybrid Fault Models: Time and Covariate Dependent Hazard Function Ma and Krings \(2008e, Ma 2008a the first aspect \(i.e., the lack of real time notion, or the agreement-algorithm side of the problem introduction of survival analysis. Specifically, time and covariate dependent survivor functions or hazard functions are utilized. In the following, I use the oral message version of the Byzantine general problem Lamport 1982 extension. The constraint of Byzantine general problem under oral message assumption \(Table 1 be replaced with the following model in the dynamic version 1 3 tmtN Further assuming that the survivor function of generals is S\(t|z    ztStNtN    ztStmtm m where N\(t t treacherous generals \(traitors S\(t|z t|z survivor functions for the total number of generals and 


survivor functions for the total number of generals and traitors, respectively.  z is the vector of covariates, and the conditional survivor functions can adopt parametric or semi-parametric covariate models such as Cox models. Obviously, the hybrid models now are not only time-dependent, but also covariate dependent.  The covariates can be any factors that affect the failures of generals. The above scheme conveniently transforms traditional hybrid fault models and their corresponding mathematical constraints for reaching agreement, into time and covariate dependent  The above models use Byzantine general problem with oral messages as an example, and the other constraints to reach agreement in hybrid fault models can be extended in the same manner.  In addition, the above models adopted discrete form, or difference equations Continuous model or differential equation can be adopted. Models similar to population dynamics can be utilized \(Ma and Bechinski 2008 field that studies the change of population numbers from generation to generation. The concept of population usually refers to animal, plant or human population Hallam &amp; Levin. 1986, Kingsland 1995, Kot 2001 Lande &amp; Engen, 2003 mathematical models such as expressed with survival analysis, populations of animals, Byzantine generals, or wireless sensor nodes, are essentially the same Therefore, the models in population dynamics theory can be used to replace above models to represent more complex systems. The population dynamics approach is particular convenient for the utilization of evolutionary game modeling, which is necessary for addressing the second aspect of the dynamic hybrid fault models Box 4.2. Reliability Aspect of Dynamic Hybrid Fault Models: Byzantine Generals Playing Evolutionary Games The solution to the first aspect \(Box 4.1 not sufficient to apply the DHF models to reliability or survivability analysis.  Ma and Krings \(2008e, Ma 2008a introduced evolutionary game theory \(EGT the second aspect of the problem  reliability aspect of dynamic hybrid fault models. With the Byzantine general problem as an example and they term the approach Byzantine Generals Playing Evolutionary Games  Basic Concepts in EGT  In EGT, game players are population individuals, game payoffs are the fitness of individuals, and strategies are evolved dynamically.  Similar to the Nash equilibrium in traditional game theory evolutionary stable strategy \(ESS both internal mutation and external perturbation.  The ESS can be mapped to the sustainable or survivable strategies. When mapping to the Byzantine general problem, players are the generals, and fitness or payoff of a game is mapped to reliability \(survivability sub-population of players, which can be represented with the hazard function or survival function introduced in the previous section.  The sub-populations can refer to groups of generals \(such as loyal general group or treacherous general group behaviors of generals, such as malicious vs. benign symmetric vs. asymmetric  Basic Mathematical Models for EGT  In literature mathematical models for evolutionary games can be as simple as simple algebraic equations or as complex as differential equation systems. For example, the so termed replicator dynamics model is actually the adaptation of population dynamics modeling for describing evolutionary games. Replicator dynamics describes evolution of the frequencies of strategies in a population. In evolutionary 


game theory, replicator dynamics is described with differential equations. For example, if a population consists of n types nEEE ,...,, 21 with frequencies 21 nxxx . The fitness xfi of iE will be a function of the population structure, or the vector 21 nxxxx Following the basic tenet of Darwinism, one may define the success as the difference between the fitness xfi of iE  and the average fitness   xfxxf ii of the population. The simplest replicator model can be defined as   xfxfxdtdx iii for .,...,2,1 ni =  The population nStx ?   where nS is a simplex, which is the space for population composition, is similar to mixed strategies in traditional games \(Hofbauer amp; Sigmund 1998, Vincent &amp; Brown 2005, Nowak 2006  12                                                  Box 4.3 included some additional comments on DHF models. The following is a summary for the potential benefits of introducing DHF models to reliability and PHM  i 


i models into time and covariate dependent dynamic counterparts  ii models and reliability analysis in a more realistic manner  iii level  whether or not functional components \(loyal generals diagnose correctly and take proper actions such as fault mask of failed components \(traitors asymmetric  iv survivability analysis. Evolutionary game modeling can derive sustainable or survivable strategies \(mapped from the ESS in EGT such as node failures such as security compromise level modeling in the so-called three-layer survivability analysis developed in Ma \(2008a this article  v offer an integrated architecture that unite reliability survivability, and fault tolerance, and the modeling approaches with survival analysis and evolutionary game theory implement this architecture. Finally, the dynamic hybrid fault models, when utilized to describe the survival of players in EGT, enhance the EGT's flexibility and power in modeling the survival and behaviors of the game players which should also be applicable to other problem domains where EGT is applicable  5. OPERATIONAL LEVEL MODELING AND DECISION-MAKING  5.1. Highlights of the Tactical and Strategic Levels  Let's first summarize what are obtainable at both tactical and strategic levels. The results at both tactical and strategic levels are precisely obtainable either via analytic or simulation optimization. With the term precisely, we mean that there is no need to assign subjective probabilities to UUUR events. This is possible because we try to assess the consequences of UUUR events \(tactical level ESS strategies \(strategic level time prediction of survivability. The following is a list of specific points. I use an assumed Wireless Sensor Network WSN  i of UUUR events: \(a actions which can be treated as censored events; \(b Cont' of Box 4.2 It can be shown that the replicator differential equations are equivalent to the classical population dynamics models such as Logistic differential equation and LotkaVolterra equation \(e.g., Kot 2001 Logistic equation, or the limited per capital growth rate is similar to the change rate of the fitness  xfxfi which can be represented with the hazard function or survivor functions introduced in the previous section on survival analysis.  This essentially connects the previous survival analysis modeling for lifetime and reliability with the EGT modeling. However, EGT provides additional modeling power beyond population dynamics or survival analysis approaches introduced in the previous section. The introduction of evolutionary theory makes the games played by a population evolvable. In other words, each player \(individual 


other words, each player \(individual agent and players interact with each other to evolve an optimized system Box 4.3. Additional Comments on DHF Models  The above introduced EGT models are very general given they are the system of ordinary differential equations. Furthermore, the choice of fitness function f\(x complexity to the differential equation system.  The system can easily be turned into system of nonlinear differential equations. The analytical solution to the models may be unobtainable when nonlinear differential equations are involved and simulation and/or numerical computation are often required  In the EGT modeling, Byzantine generals are the game players, and hybrid fault models are conveniently expressed as the strategies of players; the players may have different failure or communication behaviors Furthermore, players can be further divided into groups or subpopulations to formulate more complex network organizations. In the EGT modeling, reliability can be represented as the payoff \(fitness, the native term in EGT of the game. Because reliability function can be replaced by survivor function, survival analysis is seamlessly integrated into the EGT modeling. That is, let Byzantine generals play evolutionary games and their fitness reliability function  The evolutionary stable strategy \(ESS counterpart of Nash equilibrium in traditional games ESS corresponds to sustainable strategies, which are resistant to both internal mutations \(such as turning into treason generals or nodes such as security compromises represent survivable strategies and survivability in survivability analysis. Therefore, dynamic hybrid fault models, after the extension with EGT modeling, can be used to study both reliability and survivability 13 risks such as competing risks which can be described with CRA; \(c captured with the shard frailty.  We believe that these UUUR events are sufficiently general to capture the major factors/events in reliability, security and survivability whose occurrence probabilities are hard or impossible to obtain  Instead of trying to obtain the probabilities for these events which are infeasible in most occasions, we focus on analyzing the consequences of the events.  With survival analysis, it is possible to analyze the effects of these types of events on survivor functions. In addition, spatial frailty modeling can be utilized to capture the heterogeneity of risks in space, or the spatial distribution of risks \(Ma 2008a d UUUR events introduced previously. These approaches and models that deal with the effects of UUUR events form the core of tactical level modeling  To take advantage of the tactical level modeling approaches it is obviously necessary to stick to the survivor functions or hazard functions models. In other words, survival analysis can deal with UUUR events and offer every features reliability function provides, but reliability function cannot deal with UUUR events although survivor function and reliability function have the exactly same mathematical definition. This is the junction that survival analysis plays critical role in survivability analysis at tactical level. However, we 


recognize that it is infeasible to get a simple metric for survivability similar to reliability with tactical level modeling alone. Actually, up to this point, we are still vague for the measurement of survivability or a metric for survivability. We have not answered the question: what is our metric for survivability? We think that a precise or rigorous definition of survivability at tactical level is not feasible, due to the same reason we cited previously  the inability to determine the probabilities of UUUR events However, we consider it is very helpful to define a work definition for survivability at the tactical level  We therefore define the survivability at tactical level as a metric, Su\(t t function or reliability function with UUUR events considered. In the framework of three-layer survivability analysis, this metric is what we mean with the term survivability. The "metric" per se is not the focus of the three-layer survivability analysis. It is not very informative without the supports from the next two levels  strategic and operational models.  However, it is obvious that this metric sets a foundation to incorporate UUUR effects in the modeling at the next two levels  Due to the inadequacy of tactical level modeling, we proposed the next level approach  strategic level modeling for survivability. As expected, the tactical level is one foundation of strategic level modeling ii objectives: \(a affect survivability which survival analysis alone is not adequate to deal with; \(b survivability at tactical level is necessary but not sufficient for modeling survivability, we need to define what is meant with the term survivability at strategic level  With regard to \(a behaviors or modes which have very different consequences. These failure behaviors can be captured with hybrid fault models. However, the existing hybrid fault models in fault tolerance field are not adequate for applying to survivability analysis. There are two issues involved: one is the lack of real time notion in the constraints for hybrid fault models \(e.g., N&gt;3m+1 for Byzantine Generals problem synthesize the models after the real-time notions are introduced. The solution we proposed for the first issue is the dynamic hybrid fault models, which integrate survivor functions with traditional hybrid fault models. The solution we proposed for the second issue is the introduction of EGT modeling  With regard to \(b modeling our problem at strategic level, EGT modeling is essentially a powerful optimization algorithm.  One of the most important results from EGT modeling is the so-called evolutionary stable strategies \(ESS We map the ESS in EGT to survivable strategies in survivability analysis.   Therefore, at the strategic level, our work definition for survivability refers to the survivable strategies or sustainable strategies in the native term of EGT, which can be quantified with ESS  In addition to integrating dynamic hybrid fault models another advantage for introducing EGT modeling at strategic level is the flexibility for incorporating other node behaviors \(such as cooperative vs. non-cooperative those behaviors specified in standard hybrid fault models, as well as anthropocentric factors such as costs constraints  Without UUUR events, both tactical and strategic level 


Without UUUR events, both tactical and strategic level models default to regular reliability models. This implies that, in the absence of UUUR events, reliable strategies are sustainable or survivable.  This also implies that three-layer survivability analysis defaults to reliability analysis however, the three-layer approach does offer some significant advantages over traditional reliability analysis, as discussed in previous sections. Nevertheless, when UUUR events exist, reliable strategies and survivable strategies are different. This necessitates the next operational level modeling  5.2. Operational Level Modeling and Decision-Making  When UUUR events are involved, we cannot make real time predictions of survivability at tactical and strategic levels This implies that the implementations of survivable 14 strategies need additional measures that we develop in this section.  Box 5.1 explains the ideas involved with possibly the simplest example  Figure 4 is a diagram showing a simplified relationship between action threshold survivability \(TS survivability \(ES view since both TS and ES are multidimensional and dynamic in practice. Therefore, the sole purpose of the diagram is to illustrate the major concepts discussed above The blue curve is the survivability when survivable strategies specified by ESS are implemented at some point before time s.  The system is then guaranteed to hold survivability above ES. In contrary, if no ESS implemented before time s, then the system quickly falls below to the survivable level at around 40 time units  T i m e 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 1 0 0 Su rv iv ab ili ty M et ric S u t 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 E S S  i s  I m p lm e n t e d N o  E S S  is  I m p lm e n t e d ts E S T S  Figure 4. A Diagram Showing the Relationship Between TS and ES, as well as timing of s and t, with s &lt; t  6. SUMMARY  The previous sections discussed the major building blocks 


The previous sections discussed the major building blocks for the new life-system inspired PHM architecture. This section first identifies a few minor aspects that have not been discussed explicitly but are necessary for the implementation of the architecture, and then we summarize the major building blocks in a diagram  6.1. Missing Components and Links  Optimization Objectives  Lifetime, reliability, fault tolerance, and survivability, especially the latter two, are application dependent. Generally, the optimization of reliability and survivability are consistent; in that maximization of reliability also implies maximization of survivability. However, when application detail is considered, optimization of lifetime is not necessarily consistent with the optimization of reliability. Consider the case of the monitoring sensor network as an example. The network reliability is also dependent on connectivity coverage, etc, besides network lifetime. What may be further complicated is the time factor. All of the network metrics are time-dependent. A paradoxical situation between lifetime and reliability could be that nodes never 'sleep                                                   


          Box 5.1 Operational Level Modeling  Assuming that the ESS solution for a monitoring sensor network can be expressed with the following simple algebraic conditions: survivability metric at tactical level SU = 0.7, Router-Nodes in the WSN &gt; 10%, Selfish Nodes &lt; 40%. Even with this extremely simplified scenario, the ESS strategies cannot be implemented because we do not know when the actions should be taken to warrant a sustainable system.  These conditions lack a correlation with real time  The inability to implement ESS is rooted in our inability to assign definite probabilities to UUUR events, which implies that we cannot predict when something sufficiently bad will jeopardize the system survivability What we need at the operational level is a scheme to ensure ESS strategy is in place in advance  The fundamental idea we use to implement the ESS strategy is to hedge against the UUUR events. The similar idea has been used in financial engineering and also in integrated pest management in entomology. This can be implemented with the following scheme  Let us define a pair of survivability metrics: one is the expected survivability \(ES threshold survivability or simply threshold survivability \(TS ES is equivalent to the survivability metric at tactical level. ES corresponds to ESS at strategic level, but they are not equivalent since ESS is strategy and ES is survivability. TS is the survivability metric value \(at tactical level and TS can be obtained from strategic level models. For example, TS = SU\(s t condition for the implementation of ESS. In other words, the implementation of strategies that ensures TS at time s will guarantee the future ES level at time t.  To make the implementation more reliable and convenient multiple dynamic TSs can be computed at time s1, s2 sk, with si &lt; t for all i.  These TS at times s1, s2, ..., sk should be monitored by some evaluation systems  Unlike tactical and strategic levels, the operational level modeling is approximate. The term "approximate means that we cannot predict the real time survivability or we do not know the exact time an action should be taken. Instead, the action is triggered when the monitored survivability metric SU\(r survivability \(TS scheme of TS and ES, we ensure the ES by taking preventative actions \(prescribed by ESS and triggered by the TS consequences of UUUR events  Figure 4 is a diagram showing the above concepts and the decision-making process involved 15 This wakefulness \(never 'sleep short period but at the expense of network lifetime. Of course, when the network is running out of lifetime, network reliability ultimately crashes. This example reminds us that 


reliability ultimately crashes. This example reminds us that multi-objective optimization should be the norm rather than exception  Constraints and Extensions  Many application specific factors and constraints are ignored in this article. For example, we mentioned about spatial heterogeneity of environment, but never present a mathematical description The spatial heterogeneity can be modeled with the so-called spatial frailty in multivariate survival analysis \(Ma 2008a  Evolutionary Algorithm  Evolutionary game modeling when implemented in simulation, can be conveniently implemented with an algorithm similar to Genetic Algorithms \(GA ESS in the evolutionary game model with simulation is very similar to GA. Dynamic populations, in which population size varies from generation to generation \(Ma &amp; Krings 2008f of node failures. Another issue to be addressed is the synchronous vs. asynchronous updating when topology is considered in the simulation. This update scheme can have profound influences on the results of the simulation. Results from cellular automata computing should be very useful for getting insights on the update issue  6.2. Summary and Perspective  To recapture the major points of the article, let us revisit Figure 3, which summarizes the principal modules of the proposed life-system inspired PHM architecture. The main inspiration from life systems is the notion of individuals and their assemblage, the population. Population is an emergent entity at the next level and it has emergent properties which we are often more concerned with. Survival analysis, which has become a de facto standard in biomedicine, is particularly suitable for modeling population, although it is equally appropriate at individual level. Therefore, survival analysis \(including competing risks analysis and multivariate survival analysis comprehensively in the context of PHM in a series of four papers presented at IEEE AeroSpace 2008 \(Ma &amp; Krings 2008a, b, c, &amp; d proposed architecture. Survival analysis constitutes the major mathematical tools for analyzing lifetime and reliability, and also forms the tactical level of the three-layer survivability analysis  Besides lifetime and reliability, two other major modules in Figure 3 are fault tolerance and survivability. To integrate fault tolerance into the PHM system, Dynamic Hybrid Fault DHF 2008e, Ma 2008a make real-time prediction of reliability more realistic and make real-time prediction of fault tolerance level possible DHF models also unite lifetime, reliability and fault tolerance under a unified modeling framework that consists of survival analysis and evolutionary game theory modeling  DHG models also form the partial foundation, or strategic level, for the three-layer survivability analysis. At the strategic level, the Evolutionary Stable Strategies \(ESS which is mapped to survivable or sustainable strategies, can be obtained from the evolutionary game theory based DHF models. When there is not any UUUR event involved reliability and survivability are consistent, and reliable strategies are survivable. In this case, the strategic level modeling up to this point is sufficient for the whole PHM system modeling, and there is no need for the next level  operational level modeling  When there are UUUR events in a PHM system, the 


When there are UUUR events in a PHM system, the inability to determine the occurrence probabilities of UUUR events makes the operational level modeling necessary Then the principle of hedging must be utilized to deal with the "hanging" uncertainty from UUUR events. In this case reliability strategies are not necessarily survivable strategies At the operational level modeling, a duo of survivability metrics, expected survivability \(ES survivability \(TS the survivable strategies \(ESS level are promptly implemented based on the decisionmaking rules specified with the duo of survivability metrics then the PHM system should be able to endure the consequences of potentially catastrophic UUUR events. Of course, to endure such catastrophic events, the cost may be prohibitively high, but the PHM system will, at least, warn decision-makers for the potentially huge costs.  It might be cheap to just let it fail  Figure 3 also shows several other modules, such as security safety, application systems \(such as Automatic Logistics CBM+, RCM, Life cycle cost management, Real-time warning and alert systems architectures, but we do not discuss in this paper. Generally the new architecture should be fully compatible with existing ones in incorporating these additional modules. One point we stressed is that PHM system can be an ideal place to enforce security policies. Enforcing security policies can be mandatory for PHM systems that demand high security and safety such as weapon systems or nuclear plant facilities.  This is because maintenance, even without human-initiated security breaches, can break the security policies if the maintenance is not planned and performed properly  In perspective, although I did not discuss software issues in this paper, the introduced approaches and models should provide sufficient tools for modeling software reliability and survivability with some additional extension. Given the critical importance of software to modern PHM systems, we present the following discussion on the potential extension to software domain. Specifically, two points should be noted: \(1 architecture to software should be a metric which can 16 replace the time notion in software reliability; I suggest that the Kolmogorov complexity \(e.g., Li and Vitanyi 1997 be a promising candidate \(Ma 2008a change is because software does not wear and calendar time for software reliability usually does not make much sense 2 software reliability modeling.  Extending to general survivability analysis is not a problem either. In this article I implicitly assume that reliability and survivability are positively correlated, or reliability is the foundation of survivability. This positive correlation does not have to be the case. A simplified example that illustrates this point is the 'limit order' in online stock trading, in which limit order can be used in either direction: that stock price is rising or falling.  The solution to allow negative or uncorrelated relationships between reliability and survivability are very straightforward, and the solutions are already identified in previous discussions. Specifically, multiple G-functions and multi-stage G-functions by Vincent and Brown \(2005 very feasible solution, because lifetime, reliability and survivability may simply be represented with multiple Gfunctions. Another potential solution is the accommodation of the potential conflicts between reliability and survivability with multi-objective GA algorithms, which I previously suggested to be used as updating algorithms in the optimization of evolutionary games  


 The integration of dynamic hybrid fault models with evolutionary game modeling allows one to incorporate more realistic and detailed failure \(or survival individual players in an evolutionary game. This is because dynamic hybrid fault models are supported by survival analysis modeling, e.g., time and covariate dependent hazard or survivor functions for individual players. If necessary, more complex survival analysis modeling including competing risks analysis and multivariate survival analysis, can be introduced.  Therefore, any field to which evolutionary game theory is applicable may benefit from the increased flexibility in modeling individual players.  Two particularly interesting fields are system biology and ecological modeling.  In the former field, dynamic hybrid fault models may find important applications in the study of biological networks \(such as gene, molecular, and cell networks 2008g conjecture that explains the redundancy in the universal genetic code with Byzantine general algorithm. In addition they conducted a comparative analysis of bio-robustness with engineering fault tolerance, for example, the strong similarity between network survivability and ecological stability \(Ma &amp; Krings 2008g survivability analysis can be applied for the study of survivals or extinctions of biological species under global climate changes \(Ma 2008b  In this paper, I have to ignore much of the details related to the implementation issues to present the overall architecture and major approaches clearly and concisely. To deal with the potential devils in the implementation details, a well funded research and development team is necessary to take advantages of the ideas presented here. On the positive side I do see the great potential to build an enterprise PHM software product if there is sufficient resource to complete the implementation. Given the enormous complexity associated with the PHM practice in modern engineering fields, it is nearly impossible to realize or even demonstrate the benefits of the architecture without the software implementation. The critical importance of PHM to mission critical engineering fields such as aerospace engineering, in turn, dictates the great value of such kind software product  6.3. Beyond PHM  Finally, I would like to raise two questions that may be interested in by researchers and engineers beyond PHM community. The first question is: what can PHM offer to other engineering disciplines? The second question is: what kinds of engineering fields benefit most from PHM? Here, I use the term PHM with the definition proposed by IEEE which is quoted in the introduction section of the paper  As to the first question, I suggest software engineering and survivability analysis are two fields where PHM can play significant roles. With software engineering, I refer to applying PHM principles and approaches for dealing with software reliability, quality assurance, and even software process management, rather than building PHM software mentioned in the previous subsection. For survivability analysis, borrowing the procedures and practices of PHM should be particularly helpful for expanding its role beyond its originating domain \(network systems that control critical national infrastructures is a strong advocate for the expansion of survivability analysis to PHM. Therefore, the interaction between PHM and survivability analysis should be bidirectional. Indeed, I see the close relationships between PHM, software engineering, and survivability as well-justified because they all share some critical issues including reliability survivability, security, and dependability  


 The answer to the second question is much more elusive and I cannot present a full answer without comparative analysis of several engineering fields where PHM has been actively practiced. Of course, it is obvious that fields which demand mission critical reliability and dependability also demand better PHM solutions. One additional observation I would like to make is that PHM seems to play more crucial roles for engineering practices that depend on the systematic records of 'historical' data, such as reliability data in airplane engine manufacturing, rather than on the information from ad hoc events.  This may explain the critical importance of PHM in aerospace engineering particularly in commercial airplane design and manufacturing.  For example, comparing the tasks to design and build a space shuttle vs. to design and manufacture commercial jumbo jets, PHM should be more critical in the latter task  17    Figure 2. States of a monitoring sensor node and its failure modes \(after Ma &amp; Krings 2008e     Figure 3. Core Modules and their Relationships of the Life System Inspired PHM Architecture    REFERENCES  Adamides, E. D., Y. A. Stamboulis, A. G. Varelis. 2004 Model-Based Assessment of Military Aircraft Engine Maintenance Systems Model-Based Assessment of Military Aircraft Engine Maintenance Systems. Journal of the Operational Research Society, Vol. 55, No. 9:957-967  Anderson, R. 2001. Security Engineering. Wiley  Anderson, R. 2008. Security Engineering. 2nd ed. Wiley  Bird, J. W., Hess, A. 2007.   Propulsion System Prognostics R&amp;D Through the Technical Cooperation Program Aerospace Conference, 2007 IEEE, 3-10 March 2007, 8pp  Bock, J. R., Brotherton, T., W., Gass, D. 2005. Ontogenetic reasoning system for autonomic logistics. Aerospace Conference, 2005 IEEE 5-12 March 2005.Digital Object Identifier 10.1109/AERO.2005.1559677  Brotherton, T., P. Grabill, D. Wroblewski, R. Friend, B Sotomayer, and J. Berry. 2002. A Testbed for Data Fusion for Engine Diagnostics and Prognostics. Proceedings of the 2002 IEEE Aerospace Conference  Brotherton, T.; Grabill, P.; Friend, R.; Sotomayer, B.; Berry J. 2003. A testbed for data fusion for helicopter diagnostics and prognostics. Aerospace Conference, 2003. Proceedings 2003 IEEE  Brown, E. R., N. N. McCollom, E-E. Moore, A. Hess. 2007 Prognostics and Health Management A Data-Driven Approach to Supporting the F-35 Lightning II. 2007 IEEE AeroSpace Conference  Byington, C.S.; Watson, M.J.; Bharadwaj, S.P. 2008 Automated Health Management for Gas Turbine Engine Accessory System Components. Aerospace Conference 2008 IEEE, DOI:10.1109/AERO.2008.4526610 


2008 IEEE, DOI:10.1109/AERO.2008.4526610 Environment Covariates &amp; Spatial Frailty Applications: AL; Life Cycle Mgmt; Real-Time Alerts CBM+, RCM, TLCSM; Secret Sharing and Shared Control 18 Chen, Y. Q., S. Cheng. 2005. Semi-parametric regression analysis of mean residual life with censored survival data Biometrika \(2005  29  Commenges, D. 1999. Multi-state models in Epidemiology Lifetime Data Analysis. 5:315-327  Cook, J. 2004. Contrasting Approaches to the Validation of Helicopter HUMS  A Military User  s Perspective Aerospace Conference, 2004 IEEE  Cook, J. 2007. Reducing Military Helicopter Maintenance Through Prognostics. Aerospace Conference, 2007 IEEE Digital Object Identifier 10.1109/AERO.2007.352830  Cox, D. R. 1972. Regression models and life tables.  J. R Stat. Soc. Ser. B. 34:184-220  Crowder, M. J.  2001. Classical Competing Risks. Chapman amp; Hall. 200pp  David, H. A. &amp; M. L. Moeschberger. 1978. The theory of competing risks. Macmillan Publishing, 103pp  Ellison, E., L. Linger, and M. Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013  Hanski, I. 1999. Metapopulation Ecology. Oxford University Press  Hallam, T. G. and S. A. Levin. 1986. Mathematical Ecology. Biomathematics. Volume 17. Springer. 457pp  Hess, A., Fila, L. 2002.  The Joint Strike Fighter \(JSF concept: Potential impact on aging aircraft problems Aerospace Conference Proceedings, 2002. IEEE. Digital Object Identifier: 10.1109/AERO.2002.1036144  Hess, A., Calvello, G., T. Dabney. 2004. PHM a Key Enabler for the JSF Autonomic Logistics Support Concept. Aerospace Conference Proceedings, 2004. IEEE  Hofbauer, J. and K. Sigmund. 1998. Evolutionary Games and Population Dynamics. Cambridge University Press 323pp  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Huzurbazar, A. V. 2006. Flow-graph model for multi-state time-to-event data. Wiley InterScience  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis. Springer. 481pp  Kacprzynski, G. J., Roemer, M. J., Hess, A. J. 2002. Health management system design: Development, simulation and cost/benefit optimization. IEEE Aerospace Conference Proceedings, 2002. DOI:10.1109/AERO.2002.1036148  Kalbfleisch, J. D., and R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data. Wiley-InterScience, 2nd ed  Kalgren, P. W., Byington, C. S.   Roemer, M. J.  2006 Defining PHM, A Lexical Evolution of Maintenance and Logistics. Systems Readiness Technology Conference 


Logistics. Systems Readiness Technology Conference IEEE. DOI: 10.1109/AUTEST.2006.283685  Keller, K.; Baldwin, A.; Ofsthun, S.; Swearingen, K.; Vian J.; Wilmering, T.; Williams, Z. 2007. Health Management Engineering Environment and Open Integration Platform Aerospace Conference, 2007 IEEE, Digital Object Identifier 10.1109/AERO.2007.352919  Keller, K.; Sheahan, J.; Roach, J.; Casey, L.; Davis, G Flynn, F.; Perkinson, J.; Prestero, M. 2008. Power Conversion Prognostic Controller Implementation for Aeronautical Motor Drives. Aerospace Conference, 2008 IEEE. DOI:10.1109/AERO.2008.4526630  Klein, J. P. and M. L. Moeschberger. 2003. Survival analysis techniques for censored and truncated data Springer  Kingsland, S. E. 1995. Modeling Nature: Episodes in the History of Population Ecology. 2nd ed., University of Chicago Press, 315pp  Kot, M. 2001. Elements of Mathematical Ecology Cambridge University Press. 453pp  Krings, A. W. and Z. S. Ma. 2006. Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks Military Communications Conference, 23-25 October, 7 pages, 2006  Lamport, L., R. Shostak and M. Pease. 1982. The Byzantine Generals Problem. ACM Transactions on Programming Languages and Systems, 4\(3  Lawless, J. F. 2003. Statistical models and methods for lifetime data. John Wiley &amp; Sons. 2nd ed  Line, J. K., Iyer, A. 2007. Electronic Prognostics Through Advanced Modeling Techniques. Aerospace Conference 2007 IEEE. DOI:10.1109/AERO.2007.352906  Lisnianski, A., Levitin, G. 2003. Multi-State System Reliability: Assessment, Optimization and Applications World Scientific  Liu, Y., and K. S. Trivedi. 2006. Survivability Quantification: The Analytical Modeling Approach, Int. J of Performability Engineering, Vol. 2, No 1, pp. 29-44  19 Luchinsky, D.G.; Osipov, V.V.; Smelyanskiy, V.N Timucin, D.A.; Uckun, S. 2008. Model Based IVHM System for the Solid Rocket Booster. Aerospace Conference, 2008 IEEE.DOI:10.1109/AERO.2008.4526644  Lynch, N. 1997. Distributed Algorithms. Morgan Kaufmann Press  Ma, Z. S. 1997. Demography and survival analysis of Russian wheat aphid. Ph.D. dissertation, Univ. of Idaho 306pp  Ma, Z. S. 2008a. New Approaches to Reliability and Survivability with Survival Analysis, Dynamic Hybrid Fault Models, and Evolutionary  Game Theory. Ph.D. dissertation Univ. of Idaho. 177pp  Ma, Z. S. 2008b. Survivability Analysis of Biological Species under Global Climate Changes: A New Distributed and Agent-based Simulation Architecture with Survival Analysis and Evolutionary Game Theory. The Sixth 


International Conference on Ecological Informatics. Dec 25, 2008. Cancun, Mexico  Ma, Z. S. and E. J. Bechinski. 2008. A Survival-Analysis based  Simulation Model for Russian Wheat Aphid Population Dynamics. Ecological Modeling, 216\(2 332  Ma, Z. S. and A. W. Krings. 2008a.  Survival Analysis Approach to Reliability Analysis and Prognostics and Health Management \(PHM  AIAA AeroSpace Conference, March 1-8, 2008, Big Sky, MT, 20pp  Ma, Z. S. and A. W. Krings. 2008b. Competing Risks Analysis of Reliability, Survivability, and Prognostics and Health Management \(PHM  AIAA AeroSpace Conference, March 1-8, 2008.  Big Sky, MT. 20pp  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(I Dependence Modeling", Proc. IEEE  AIAA AeroSpace Conference, March 1-8, 2008, Big Sky, MT. 21pp  Ma, Z. S. and A. W. Krings., R. E. Hiromoto. 2008d Multivariate Survival Analysis \(II State Models in Biomedicine and Engineering Reliability IEEE International Conference of Biomedical Engineering and Informatics, BMEI 2008.  6 Pages  Ma, Z. S. and A. W. Krings. 2008e. Dynamic Hybrid Fault Models and their Applications to Wireless Sensor Networks WSNs Modeling, Analysis and Simulation of Wireless and Mobile Systems. \(ACM MSWiM 2008 Vancouver, Canada  Ma, Z. S. &amp; A. W. Krings. 2008f. Dynamic Populations in Genetic Algorithms. SIGAPP, the 23rd Annual ACM Symposium on Applied Computing, Ceara, Brazil, March 16-20, 2008. 5 Pages  Ma, Z. S. &amp; A. W. Krings. 2008g. Bio-Robustness and Fault Tolerance: A New Perspective on Reliable, Survivable and Evolvable Network Systems, Proc. IEEE  AIAA AeroSpace Conference, March 1-8, Big Sky, MT, 2008. 20 Pages  Ma, Z. S.  and A. W. Krings. 2009. Insect Sensory Systems Inspired Computing and Communications.  Ad Hoc Networks 7\(4  MacConnell, J.H. 2008. Structural Health Management and Structural Design: An Unbridgeable Gap? 2008 IEEE Aerospace Conference, DOI:10.1109/AERO.2008.4526613  MacConnell, J.H. 2007. ISHM &amp; Design: A review of the benefits of the ideal ISHM system. Aerospace Conference 2007 IEEE. DOI:10.1109/AERO.2007.352834  Marshall A. W., I. Olkin. 1967. A Multivariate Exponential Distribution. Journal of the American Statistical Association, 62\(317 Mar., 1967  Martinussen, T. and T. H. Scheike. 2006. Dynamic Regression Models for Survival Data. Springer. 466pp  Mazzuchi, T. A., R. Soyer., and R. V. Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Millar, R.C., Mazzuchi, T.A. &amp; Sarkani, S., 2007. A Survey of Advanced Methods for Analysis and Modeling of 


of Advanced Methods for Analysis and Modeling of Propulsion System", GT2007-27218, ASME Turbo Expo 2007, May 14-17, Montreal, Canada  Millar, Richard C., "Non-parametric Analysis of a Complex Propulsion System Data Base", Ph.D. Dissertation, George Washington University, June 2007  Millar, R. C. 2007. A Systems Engineering Approach to PHM for Military Aircraft Propulsion Systems. Aerospace Conference, 2007 IEEE. DOI:10.1109/AERO.2007.352840  Millar, R. C. 2008.  The Role of Reliability Data Bases in Deploying CBM+, RCM and PHM with TLCSM Aerospace Conference, 2008 IEEE, 1-8 March 2008. Digital Object Identifier: 10.1109/AERO.2008.4526633  Nowak, M. 2006. Evolutionary Dynamics: Exploring the Equations of Life. Harvard University Press. 363pp  Oakes, D. &amp; Dasu, T. 1990. A note on residual life Biometrika 77, 409  10  Pintilie, M. 2006. Competing Risks: A Practical Perspective.  Wiley. 224pp  20 Smith, M. J., C. S. Byington. 2006. Layered Classification for Improved Diagnostic Isolation in Drivetrain Components. 2006 IEEE AeroSpace Conference  Therneau, T. and P. Grambsch. 2000. Modeling Survival Data: Extending the Cox Model. Springer  Vincent, T. L. and J. L. Brown. 2005. Evolutionary Game Theory, Natural Selection and Darwinian Dynamics Cambridge University Press. 382pp  Wang. J., T. Yu, W. Wang. 2008. Research on Prognostic Health Management \(PHM on Flight Data. 2008 Int. Conf. on Condition Monitoring and Diagnosis, Beijing, China, April 21-24, 2008. 5pp  Zhang, S., R. Kang, X. He, and M. G. Pecht. 2008. China  s Efforts in Prognostics and Health Management. IEEE Trans. on Components and Packaging Technologies 31\(2             BIOGRAPHY  Zhanshan \(Sam scientist and earned the terminal degrees in both fields in 1997 and 2008, respectively. He has published more than 60 peer-refereed journal and conference papers, among which approximately 40 are journal papers and more than a third are in computer science.  Prior to his recent return to academia, he worked as senior network/software engineers in semiconductor and software industry. His current research interests include: reliability, dependability and fault tolerance of distributed and software systems behavioral and cognitive ecology inspired pervasive and 


behavioral and cognitive ecology inspired pervasive and resilient computing; evolutionary &amp; rendezvous search games; evolutionary computation &amp; machine learning bioinformatics &amp; ecoinformatics                 pre></body></html 


