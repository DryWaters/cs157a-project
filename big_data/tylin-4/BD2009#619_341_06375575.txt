Adaptive and Scalable Metadata Management to Support A Trillion Files Jing Xing 1  3  Jin Xiong 1  Ninghui Sun 2  Jie Ma 1 1 National Research Center for Intelligent Computing Systems Institute of Computing Technology Chinese Academy of Sciences 2 Key Laboratory of Computer System and Architecture Chinese Academy of Sciences 3 Graduate University of Chinese Academy of Sciences xingjing xj snh majie}@ncic.ac.cn ABSTRACT Nowadays more and more applications require le systems to eciently maintain million or more les How to provide high access performance with such a huge number of 
les and such large directories is a big challenge for cluster le systems Limited by static directory structures existing le systems will be prohibitively inecient for this use To address this problem we present a scalable and adaptive metadata management system which aims to maintain a trillion les eciently Firstly our system exploits an adaptive two-level directory partitioning based on extendible hashing to manage very large directories Secondly our system utilizes ne-grained parallel processing within a directory and greatly improves performance of le creation or deletion Thirdly our system uses multiple-layered metadata cache management which improves memory utilization on the servers And nally our system uses a dynamic loadbalance mechanism based on consistent hashing which en 
ables our system to scale up and down easily Our performance results on 32 metadata servers show that our user-level prototype implementation can create more than 74 thousand les per second and can get more than 270 thousand les attributes per second in a single directory with 100 million les Moreover it delivers a peak throughput of more than 60 thousand le creates/second in a single directory with 1 billion les 1 INTRODUCTION In recent years cluster storage has become the dominant architecture to meet the need of large-scale data storage In this architecture decoupling metadata processing from le data accessing is widely used The metadata servers MDS maintain the le system namespace and le attributes while the storage servers process the read and write requests Clus 
ter storage can provide hundreds of gigabyte per secondês I/O throughput and tens of thousands operations per secPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proìt or commercial advantage and that copies bear this notice and the full citation on the rst page To copy otherwise to republish to post on servers or to redistribute to lists requires prior speciìc permission and/or a fee SC09 November 14-20 2009 Portland Oregon USA c 2009 ACM 978-1-60558-744-8/09/11 10.00 ondês metadata processing power which makes them a suitable solution to address lots of applications I/O needs However with the popularization of internet applications and extensive applications of high-end scientiìc computing cluster storage is now facing with the big challenge caused 
by many more les and data to be stored in the le system The metadata management of cluster storage needs to deal with the following three major challenges The rst challenge is how to eciently organize and maintain very large directories each of which contains billions of les Internet applications like facebook  and ickr 10 already have to manage tens of billions of photos As there are millions of new les uploaded by users every day the total number of les increases very rapidly and will soon be more than one trillion Geographic information systems GIS are now managing billions of satellite photos and they need to manage trillions of satellite photos or more for higher-precision maps in the near future In the eld of 
exploration sciences like the large synoptic survey telescope LSST t here are m ore t han n ine q uin t illion les 10 18  The directory scale will be billions or more of les when managing such a large number of les However existing cluster le systems generally aim at managing directories with millions of les Their metadata organization is not scalable as a result their metadata performance is very poor if there are tens of millions of les in a directory A scalable directory organization method is required for cluster le systems which maintain billions or trillions of les The second challenge is how to provide high metadata performance for a large-scale le system with billions or trillions of les For 
facebook  there are 550,000 images served per second at the peak  The ickr also need to support processing 38,000 images per second I n t he eld of scientiìc computing applications on petaîop machines literally open hundreds of thousands of les a time and that number keeps growing Supporting hundreds of thousands of ops/sec of metadata processing will be an important requirement for the future metadata management Ho w e v e r the sync h ronization mechanism of existing cluster le systems greatly restricts the parallelization of metadata modiìcations leading to poor metadata performance The improvement of concurrent processing needs to be considered to achieve high performance of metadata processing The third challenge is how to provide high metadata per 
formance for mixed workloads generated by a large number of concurrent users The concurrent accesses from millions of users to large-scale cluster storage will cause two serious Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee SC09 November 14-20, 2009 Portland, Oregon, USA Copyright 2009 ACM 978-1-60558-744-8/09/11 ...$10.00 


problems inecient use of metadata cache and imbalance loads among metadata servers Large numbers of concurrent accesses from dierent users result in a vast amount of random accesses from the perspective of each metadata server Lack of locality for accesses to metadata servers causes frequent replacement in the metadata cache Furthermore random accesses will cause uneven access loads among metadata servers Some servers are heavily loaded while the others are lightly loaded If large-scale metadata management has a way to utilize memory well and balance access loads eectively metadata performance will be improved greatly To address the above challenges inspired by GIGA  we propose a scalable and adaptive metadata management system which aims to maintain a trillion les eciently In our system an adaptive two-level directory partitioning based on extendible hashing i s u sed t o m anage v e ry large directories In the rst level each directory is divided into multiple partitions to distribute on multiple servers The rst level partitioning controls the distribution of partitions among metadata servers And in the second level each partition is divided into a certain number of metadata chunks The second level partitioning controls the size of each partition So any le can be located within two I/O accesses This metadata organization is very scalable and can maintain a billion-scale le system eciently Our system uses small data structures such as partition and chunk as the metadata control unit for metadata modiìcations Therefore update operations such as le creations or deletions in the same directory can be processed concurrently Our system also exploits the dierent importance of dierent kinds of metadata and partitions the metadata cache into multiple layers with dierent replacement priorities according to the kinds of metadata This way the most frequently accessed metadata such as the directory information and the partition information which are shared by many directory entries will be cached in the memory for the longest time Meanwhile a dynamic load balance approach is used to rebalance the workload among metadata servers when new metadata servers are added to the system The rest of this paper is organized as follows Section 2 discusses related work in metadata management Section 3 presents the design of our proposed metadata management techniques Section 4 introduces some critical implementation details Section 5 provides the performance results based on microbenchmarks Finally Section 6 concludes the paper 2 RELATED WORK The organization of metadata is critical for distributed metadata management It plays an important role in the scalability of metadata servers concurrency of metadata processing and load balance among metadata servers Metadata in the le system can be divided into two categories the dentry whichisusedtomaintainthenamespace information and the inode whichisusedtomanagetheìleês information including attributes and block locations Since an inode is information on one le it is relatively independent of other inodes  However namespace information has more complex relationships than le information It needs to keep the relationship between each directory and the entries it contains and the relationship between each entry and its inode  Traditionally each directory is a special le which is a mapping table of entry-ino pairs And the entries of a directory are sometimes organized into a B tree  o r a hash table  Previously metadata management technologies were categorized by their partitioning method    H o w ev er the partitioning method emphasizes too much implementation details and neglects the characteristics of metadata organization We believe that to classify metadata management technologies by partition granularity is more helpful to understand metadata management in depth including the features of metadata structures scalability of organization concurrent processing memory utilizing and load balance According to partition granularity distributed metadata management can be classiìed into the following four categories 2.1 No Partitioning This type of metadata management does not partition the namespace The whole namespace will be held in one metadata server or duplicated on multiple metadata servers The Google le system i s a represen t ativ e o f t his t yp e This approach is suitable when the amount of metadata is small especially there is not more than millions of les in the le system As operations need not to be interleaved between metadata servers network overhead is low and metadata consistency is easy to maintain However the metadata management can not scale as all the metadata is maintained by a single server Moreover this approach does not support concurrent modiìcation operations among metadata servers because all the modiìcations need to be processed by the master server When there is a burst of modiìcations the master server will replace its metadata cache frequently when it can not hold all the metadata in the memory Readonly load can be balanced among read/shadow servers but modiìcation load can not 2.2 Subtree Partitioning In subtree partitioning the whole namespace is partitioned into several subtrees the minimal subtree is a single directory which are assigned to a speciìc metadata server There are two types of subtree partitioning static subtree partitioning and dynamic subtree partitioning In the static subtree partitioning the namespace is partitioned at system conìguration stage like NFS and S prite  14 This approach can achieve concurrent processing among dierent subtrees but the granularity is very large and not less than a subtree Apparently the disadvantage of static subtree partitioning is its load imbalance The dynamic subtree partition can compensate for the inadequacy of static subtree partition Based on the workload it transfers subtrees among servers and scalability and load balance is better than in the static case However when the subtree grows to a certain large size moving the subtree to anywhere will cause another imbalance Ceph adopted a hash approach to hash the contents of a large directory and distributed them among multiple metadata servers As the sub-directory number is limited by the number of metadata servers this method may not be sucient when subdirectories grow too large to scale well 2.3 Partitioning within a Single Directory Since the subtree partitioning can not scale well for large directories Lustre s plit directory b ased on hash H o w  


ever it use static policy to split directory on xed number of servers GPFS utilizes extendible h ashing to organize directory buckets It stores directory buckets in disk blocks and builds a distributed implementation across all the nodes through a distributed lock managers and cache consistency protocols In case of concurrent creation the GPFS lock manager need to initiate token transfers between the clients before anyone of them can create les in the shared directory The lock acquire and release phase will cause multiple disk I/Os which limits the performance of concurrent creation GIGA 16 presen t ed a m etadata m anagemen t sc heme that partitions a single directory into xed-sized parts And a partition will split when it is too full to insert any new metadata A large number of partitions are distributed among multiple metadata servers In GIGA a partition is the lower level processing unit for some update operations which increased the concurrency of metadata updates within a directory Since the number of partitions increases when the directory grows GIGA can provide higher scalability and concurrent processing However this two-level metadata hierarchy will become a bottle-neck when the directory contains more than billions of les The directory with billions of les will be split into millions of partitions which may lead to inecient partition management Although increasing the partition size does reduce the number of partitions but also lead to inecient memory utilization as big partition may increase internal fragmentation and inecient location inside partition 2.4 File Partitioning In le partitioning a feature of each le like name or identiìer is used to locate its metadata server The most appealing character of the le partitioning is that it can distribute all the les in the le system equally among all metadata servers which can achieve load balance if the probability of each le to be accessed is the same There are also two types of le partitioning the static one and the dynamic one The static le partitioning like the one used by Vesta  I n t ermezzo  2 R AMA  13 a nd zFS  19 a ssigns les to metadata servers by hashing their name and/or some other unique identiìer This simple method has a number of advantages Client can locate and contact the responsible metadata server directly For average workloads and well-behaved hash functions requests are evenly distributed across the cluster However changing the conìguration of metadata servers will cause re-hashing of all of the les Dynamic le partitioning like H and G HBA  main tains a dditional information o n e ac h s erv e r t o s upport changing the conìguration of metadata servers As a result it can achieve better load balance but it can hardly scale up to support a single large directory because the amount of maintained information increases linearly with the growth of the directory Besides it is dicult to achieve high concurrent processing as update operations need to be processed synchronized on the granularity of directory 2.5 Summary As we discussed the granularity of metadata partition has an important impact on many aspects of metadata processing such as scalability concurrent processing memory utilization and load balance Based on the analysis on related work we have concluded that the sub-directory granularity by which a directory is divided into partitions can achieve better scalability and concurrency than other partition granularity However a static one-level partitioning can achieve limited scalability We present a metadata management with an adaptive two-level partitioning It dose not only partitions a single directory into multiple partitions and distributes the partitions among metadata servers but also divides a partition into chunks within a metadata server According to the directory size the granularity of each level can be adjusted to leverage the concurrency and memory utilization For load balance we adopt the strategy to separate metadata storage and metadata processing and partitions can be migrated among the servers to achieve a relative balanced state 3 AN ADAPTIVE AND SCALABLE METADATA MANAGEMENT METHOD More and more users are asking cluster le systems to maintain billions or trillions of les eciently we propose a novel adaptive and scalable metadata management method The core of this method consists of four techniques adaptive and scalable directory partitioning ne-grained intradirectory parallel processing multi-layered metadata cache management and consistent-hashing-based load-balancing 3.1 Adaptive and Scalable Directory Partitioning Directories are used to organize the namespace inside the le system The lookup operations and update operations such as le creation and deletion need to manipulate the entries in a directory So they need to be synchronized by some sort of lock of the directory structure Since the directory structure aects the eciency of lookup operations and the concurrency of update operations it determines the scalability of metadata processing The concurrent control granularity is determined by this directory structure When a directory grows to consist of billions or more entries searching a le name lookup in such a large directory will be costly Concurrent updating of entries in a large directory will be restricted by obtaining a lock in the directory structure GIGA divides each directory into xed sized directory partitions and distributes the partitions among the metadata servers to improve metadata processing performance and maintain much larger directories eciently When a partition reaches a certain size it will be split into two partitions automatically And metadata servers only need to hold a mapping information to indicate the split status of each partition Updates to a directory can be processed in parallel if they manipulate dierent directory partitions Therefore this method can achieve higher concurrency and scalability at the same time than other existing methods However this method may have scalability and concurrency limits If each partition contains thousands of entries a directory with a billion les needs to be split into a million partitions Fast growing directories will experience frequent split which will cause frequent updates of partition bitmap Moreover each partition is stored as a separate le in order that modiìcation of dierent partitions can be processed in parallel and hence the performance of partition modiìcation will be limited by the local le system on metadata servers such as ext3 which is inecient to manage such a largenumberofìles 


                             Figure 1 Directory Structure If the size of directory partitions is increased to contain tens of thousands les the number of directory partition splitting will decline to an ideal level However memory utilization will be inecient because a large number of unused entries may be be loaded into the cache with each lookup operation Since the size of partition is related to the speed of locating metadata in the partition a large partition could lead to inecient lookup inside a partition Ecient metadata management should trade o between the directory size and the partition size The term directory size refers to the total number of entries in a directory Similarly the term partition size refers to the total number of entries in a partition In our system partition size is limited by a value which is the maximum size of each partition while directory size is unlimited On one hand a directory is automatically split into multiple partitions in our method even if its size is small so its partitions can be distributed across multiple metadata servers This results in higher concurrent processing than a single partition without splitting On the other hand the size of a partition is adaptable to restrict the number of partitions of a very large directory to a certain scope Initially each partition only contains one metadata chunk When the size of a directory reaches a threshold value its partitions will automatically enlarge to contain more chunks When the size of a partition doubles each partition doubles the number of metadata chunks The directory structure is showninFigure1 3.1.1 The Two-Level Directory Partitioning Algorithm The adaptive and scalable directory partitioning algorithm is shown in Figure 2 When locating a le through the lookup operation the algorithm rst calculates a 32-bit hash value from its name The hash value is divided into partition bits and chunk bits to locate partition and chunk respectively Each portion of the hash the partition bits and chunk bits uses extendible hashing When a directory is small it contains only one metadata chunk The corresponding partition bits length and chunk bits length are both zero which means the directory has only one partition and the partition has only one chunk The partition bits length is increased rst when one partition is not enough When the partition bits length reaches    related to the processing power of single server and the number of metadata servers the partition bits length stops          Figure 2 Two-Level Directory Partitioning growing and the chunk bits length begins to increase When the chunk bits length grows to   which means every partition contains 2  chunks the algorithm stops enlarging the partition and continues to split more partitions Through increasing the partition bits length and the chunks bits length alternately our method can achieve high performance and high scalability with the growing of a directory When a directory is small the algorithm tries to improve performance by distributing entries of a directory across all metadata servers and manipulating them concurrently When a directory is very large the algorithm tries to improve scalability by making each metadata server maintain more partitions For example suppose we need to locate a le whose hash value is 0101101001 The hash value is divided into 010110\(the partition bits and 1001\(the chunk bits to locate its partition and chunk respectively As shown in Figure 2 partition bits length is 2 and chunk bits length is 1 we can educe the le belongs to partition 2 as the last 2 bits of the partition bits 010110 is 10 Similarly we can educe the le belongs to chunk 1 because the last bit of the chunk bits 1001 is 1 Then we can locate the le at partition 2 chunk 1 3.1.2 The Partition Bit-Map Like GIGA as le access operations are issued on clients each client maintains a partition bit-map to indicate which partitions are available in order to reduce redirections in locating a le To support fast locating le in a directory with billion les we use the lower 30 bits of ino to locate partition and chunk The higher 32 bits of ino is used to locate the directory Among the lower 30 bits 20 bits is used to locate partition and 10 bits is used to locate chunk Since the number of directory partitions will be no more than 2 20  the bit-map for each directory will be less than 128 KB By the help of the partition bit-map and the partition bits length to locate a partition is very ecient and only needs several memory accesses on clients If the partition bit-map of a client is stale the metadata servers will inform the client to refresh its bit-map The partition bit-map is updated once a partition is split As the update just aects the existing partition and the new created partition only the directoryês primary metadata server and the two metadata servers maintaining the related two partitions need to update their bit-maps The directoryês primary metadata server has the uptodate partition bit-map The server with an obsolete bit-map can ask the primary server for the freshest bit-map GIGA use a static hash function to distribute partitions among multiple metadata servers Suppose there is a partition i  the server holds the partition is i mod number of servers  This mapping scheme will cause a large amount of overhead when metadata server conìguration changes Unlike GIGA we use consistent hashing to distribute partitions 


among multiple metadata servers Each metadata server will be assigned with a range of hash function values and a partition is assigned to a metadata server by the hash function value of its ID The range of hash function values for each metadata servers can be changed by our load balance mechanism which will be discussed in Section 3.4 to accommodate to the change of access workloads and system conìguration 3.2 Parallel Processing Within a Directory Metadata operations usually need to be synchronized because they will access or modify shared data structures This kind of synchronization is typically implemented by locks which greatly restricts the concurrency of metadata processing For example in traditional le systems any namespace update operation to a directory should rst get the lock of this directory which results in all the updates for the same directory to be processed in sequence A large number of updates competing for a single directory structure is one of the causes of poor metadata performance Motivated by two common ways to improve concurrency which are to reduce the granularity of each lock and to reduce the processing inside each lock our system divides a directory into many smaller-sized partitions Operations to the same directory no matter they are reads or modiìcations can be processed concurrently on dierent servers if they operate on dierent partitions Furthermore we can also beneìt from the smaller granularity of partition lock Without directory partitioning each update operation will lock the whole directory which not only prevents any other updates from being processed but also prevents metadata cache replacement of this directory However with directory partitioning each update operation will lock a partition which is much smaller than the directory and operations on other partitions can proceed on 3.2.1 Fine-Grained Parallel Metadata Processing In our metadata management system the metadata structures can be classiìed into 4 types according to the data size they manipulate Directories are on the highest level which manipulate the whole directory Partitions are the second largest structures and metadata chunks are the third largest structures The metadata of each le is the smallest structure and is on the lowest level To achieve high concurrency dierent types of operations are processed in parallel at different levels Both the directory cache replacement and partition bitmap update need to be synchronized by directory lock Because in directory cache replacement all the content of the evicted directory including all partitions and all chunks should be written back to the disk before the directory structure is written back And in partition bit-map update multiple servers may require to modify the bit-map simultaneously Although le or directory creation operations need to be synchronized by partition lock other operations which only read the partition information can be processed concurrently As metadata chunks are the unit to transfer metadata between memory and disk when it is being written back to or read from the disk both modiìcation and access to it should be blocked And setattr operations need to be synchronized by metadata lock Moreover in order to trade o between metadata con                              Figure 3 Cache Hierarchy A partition cache and its chunk caches are in the same metadata server The distribution of partition caches is based on the hash range of metadata servers A directory cache and its partition attribute caches are in the same metadata server A directory has only one directory cache which is in the same metadata server with partition 0 cache sistency and high processing concurrency our system uses read-write locks to allow high concurrency for read operations Especially speciìc data structure is locked by writtenlock only when the operations modify the data structure otherwise it is locked by read-lock Compared with other systems our system can achieve lelevel concurrency for le attribute updates partition-level concurrency for creation operations which is smaller than the directory-level concurrency in traditional le systems To achieve the equal concurrency with GIGA our system splits the partition as GIGA does when the directory is small After the partition number is larger than the number of CPU cores our system enlarges partition instead of splitting partition when the partition has no space to store new metadata Moreover our system can achieve better concurrency because some operations achieve chunk-level concurrency Through two-level directory partitioning our system greatly reduced the granularity of locks and can achieve better concurrency in metadata processing than existing le systems 3.3 Multiple-Layered Metadata Cache Management Both in data centers and large-scale application environments thousands or more users make a large number of concurrent le access or modiìcation requests As a result the metadata accesses on each server exhibit weak locality and they are very random from the perspective of each server Therefore the metadata cache on the servers will be replaced frequently resulting in frequent disk accesses and underutilized memory To improve the memory utilization and reduce disk accesses we present a multiple-layered cache management for metadata servers which can keep the most frequently accessed metadata in the memory As mentioned in previous subsection there are 4 types of metadata in our system and they have dierent access frequency Among them directories are the most frequently used data structure Many operations need to access directory information And if a directory is evicted out of the cache all its content includ 


ing all partitions and chunks should also be clear out of the memory Partitions are the second most frequently used data structure as every operations need to access partition information or attain partition lock rst Chunks are the least frequently accessed data structure Moreover the replacement cost of dierent types of metadata is not uniform The cost of replacing a chunk is the lowest which only need to write back the chunk itself to disk The cost of replacing a partition is much higher than chunk replacement because it needs to write back all chunks in the partition The cost of replacing a directory is the highest which need to write back all chunks in all partitions of the directory As shown in Figure 3 our system takes both the dierent access frequency and replacement cost of dierent types of metadata into account and partitions the metadata cache on each server into 3 variable-sized parts which is directory cache partition cache and chunk cache Appropriate size of both directory cache and partition cache is important for good memory utilization because if these caches are too large chunks being accessed will have little room in the memory Therefore our system restricts the size of directory cache and partition cache by a percentage of the the total size of metadata cache When directory cache reaches the maximum size loading a new directory will have to replace an old directory which contains the least number of partitions in the cache Similarly when partition cache reaches the maximum size loading a new partition will have to replace an old partition which contains the least number of chunks in the cache When a directory cache is evicted all its partitions are also evicted together To achieve this in the distributed setting we use an agency mechanism to record the partition cache information at its directoryês primary metadata server When a partition is cached an agency of the partition will be created and inserted into the directoryês partition hash table When a partition is released from cache the agency of the partition will be removed from the directoryês partition hash table So when a directory is evicted the processing thread will know all the cached partitions of the directory from the directoryês partition hash table Then it can release all the partition agencies and send requests to related servers to release the cached partitions 3.4 Dynamic Load Balance Mechanism A large number of concurrent users of a large-scale storage system usually generate irregular and mixed workloads on the metadata servers A load balance mechanism is needed to avoid certain individual servers to be over-loaded An over-loaded server not only slows down the users accessing metadata on it but also slows down other servers which are interacting with it In order to facilitate load migration our system divorces the metadata processing from metadata storage Metadata servers only process metadata access requests while metadata storage servers provide a shared metadata storage for all metadata servers By this separation access loads migration will not cause metadata storage migration which will be very costly Therefore our system can easily migrate access loads among metadata servers with low cost if the loads are imbalance For the sake of scalability and load balance our system assigns partitions to metadata servers through consistent hashing As described in Sec 3.1.2 each server is assigned with a range of values by the hash function value of its ID And each partition is assigned to a server if its hash function value is in the range assigned to the server Therefore the size of each range is one of the factors that determines the load on each server Theoretically each metadata server can be assigned almost equal-sized range of hash values through consistent hashing when there are a large number of metadata servers However for cluster le systems the number of metadata servers is from several to hundreds For such numbers of metadata servers the size of ranges may be very dierent Moreover the access load on each server is also determined by the user workloads itself which is very dynamic and changing with time So some servers may overload by vast amount of access requests on metadata assigned to these servers even if each range is equal-sized Since consistent hashing is focus on balancing metadata placement among a large number of servers and it does not balance workload dynamically our system also employs a dynamic load balance mechanism in which some loads will migrate from overloaded servers to light-loaded servers automatically when necessary There is a master which is one of the metadata servers in our system to take the charge of load balancing In our system the master needs not to collect the resource usage on all the servers periodically An overloaded server is detected by its resource usage including CPU usage and memory usage When the resource usage exceeds a threshold value the server will make a rebalance request to the master In order to lter the burst workload within a short period of time the master will collect the resource usage of all servers after a period of time If the resource usage on the requested server is still higher than the threshold the master will trigger the rebalance process immediately Otherwise the rebalance requestisjustignored In the rebalance process the master will choose a target server to accept the loads moved out of the overloaded server rst The range of the target server must not be larger than a threshold and such a server with lowest resource usage is chosen Thenthemasterwillchoosewhichvaluesonthe overloaded server need to be assigned to the target server This results in migration of the partitions whose hash function values equal to selected values Generally no more than 10 partition processing range is chosen to migrate The rebalance process will also be triggered when a new metadata server is added in the system or a metadata server crashes By using consistent hashing it is easy for our system to determine which range is split into two ranges when adding a server or which ranges need to be merged into one range when a server crashes And partition cache migrates accordingly Therefore our system is highly scalable either scale up or scale down 4 IMPLEMENTATION 4.1 System Architecture We have implemented a prototype cluster le system called skyFS It contains four components which are metadata servers data storage servers FUSE-based clients and an asynchronous message passing library AMPL To avoid the plague of kernel debugging all of our components are implemented in the user space The metadata servers are the core of our system They 


    Figure 4 Ino structure If the type of metadata is le the lower 32 bits of ino is a hash value of the le name or a conîict Id when the hash value conîicts with others If the type of metadata is directory the lower 32 bits of ino is a directory ID manage metadata process request from client and make decision about when to split or enlarge a partition To facilitate metadata processing scalability and load balance metadata is stored in shared storage servers Each metadata partition is stored as an object le in storage severs The distribution of metadata partitions to storage servers is similar as distribution partitions to metadata servers Each storage server will be assigned a range of values according to the hash function value of its ID And partitions are mapped to the storage servers through the function value of its ID Partition split needs several disk I/O In order not to interfere with metadata servers partition split is implemented on storage servers The FUSE-base client is actually an interface between FUSE and our servers After FUSE captures le system operations from the kernel it delivers these requests to our client The client packages the requests into messages and sends them to servers through AMPL AMPL is a package of TCP/IP operations with the support of send messages and data in synchronous and asynchronous mode As it uses multiple threads to send and receive messages and data servers and clients need not to use additional threads to transfer requests or data 4.2 Metadata Organization and Location File attributes such as inode in Linux and its entry in the directory such as dentry in Linux are correlative Under most conditions a request to access a dentry will be followed by a request to access its attributes However they are separately stored on disk in traditional le systems which results in many small-sized random accesses for metadata and poor metadata performance Some previous works  ha v e studied the method of storing le attributes with its dentry and achieved great performance improvement Motivated by these works our system uses a single data structure called cmeta short for combined metadata which is the combination of the inode structure and the dentry structure As inode and dentry are collocated in the same place they should be located either by le name or by a unique ID e.g ino  Since cmetaês le name needs to be used to identify its belonging directory partition ID and then locate its processing server by hashing the partition ID we need to adjust ino accordingly to facilitate locating the cmeta by ino  To address this problem the 64-bit ino for each le or directory is divided into 3 segments in our system as shown in Figure 4 The rst 2 bits are the type ag and the conîiction ag respectively The type ag is used to indicate whether the object is a le or a directory And the conîiction ag is used to indicate if the hash value of the le conîicts with other les The following 30 bits of the ino are the parent directory ID of this le And the last 32 bits are assigned dierently for les and directories If it is a le the last 32 bits are the hash value of the le name while if it is a directory the last 32 bits are the unique directory ID generated by the metadata server when the directory is created If the name hash conîicts with the existed les the third part will be a conîict number instead The conîict number is increased every time a name hash conîict occurred for this hash value And an appendix eld is used to hold the leês name hash The appendix eld will be fetched together with the ino once inode operation occurs When locating a cmeta by its ino  our system checks the conîiction ag rst If the ag indicates no conîict our system then uses the parent directory ID and 32-bit hash value in the ino to locate it If the ag indicates conîict occurs our system will use the appendix eld as les hash value to locate the metadata As we used directory ID to represent directory we support rename directory easily Cmeta location is relied on its hash value and directory id rename the directory would not change the directoryês ID so rename a directory will not inîuence the location of its containing metadata 4.3 File System Semantics In traditional le systems each update operation such as creation or removal needs the directory attributes to be updated simultaneously However this strict consistency requirement is harmful to metadata processing performance especially for multiple metadata servers because the coordination among them is costly In fact directory attributes only need to be consistent with its content when content related operations like readdir and getattr  need to be processed Therefore our system delays the update of some directory attributes such as nlink size modiìcation time etc until their precise values are needed When directory attributes need to be refreshed the metadata server who maintains the directory will send messages to the other servers who maintain the partitions of the directory to collect directory information and merged them into the attributes In Linux readdir is implemented by reading the directory blocks of the directory in sequence As directory block does not exist in our system we need to locate the start point of asked directory block and read metadata from chunks to compose directory blocks To improve the eciency of readdir  we set all the cmeta with the same length As a matter of this we can get the partition ID and chunk ID of a cmeta with speciìed oset by the status information of the asked directory Then we can locate the start reading point of a readdir request without much overhead 5 EVALUATION Our experiment platform is the Dawning5000 which is a very large-scale cluster-architecture supercomputer consisting of 1400 nodes Each node is composed of 4-way QuadCore AMD Opteron TM 8347 HE Processors and 64GB memory The nodes are interconnected by Inìniband DDR Each node also has a 150GB 7200RPM SATA hard drive Our experiments only use 232 nodes among which 32 nodes are used as metadata servers and storage servers and 200 nodes are conìgured as clients For the experiment below our prototype is conìgured with a chunk size of 128 cmetas per chunk by default the 


 0 10000 20000 30000 40000 50000 60000 70000 80000 1000 10000 100000 1e+006 1e+007 1e+008 1e+009 File Creation Throughput \(ops/sec Directory size adaptive partition single partition with 1024 cmeta single partition with 128 cmeta single partition with 8192 cmeta a Compare with single-level partitioning 0 10000 20000 30000 40000 50000 60000 70000 80000 1000 10000 100000 1e+006 1e+007 1e+008 1e+009 File Creation Throughput \(ops/sec Directory size adaptive partition static 8 chunk/partition static 64 chunk/partition b Compare with xed sized partition 0 10000 20000 30000 40000 50000 60000 70000 80000 1000 10000 100000 1e+006 1e+007 1e+008 1e+009 File Creation Throughput \(ops/sec Directory size adaptive partition 128 cmeta/chunk adaptive partition 64 cmeta/chunk adaptive partition 1024 cmeta/chunk c Eect of dierent chunk size Figure 6 Comparison of dierent Directory Structures 0 1000 2000 3000 4000 5000 6000 7000 8000 0 0.5 1 1.5 2 2.5 3 3.5 0 1000 2000 3000 4000 5000 File Creation Throughput \(ops/sec Number of times Directory size\(million adaptive partition performance split partition times enlarge partition times Figure 5 Directory Adaptability number is chosen based on its best performance in the tests among various conìgurations 5.1 Workload To evaluate the performance of large directory processing we use the mdtest  b e nc hmark Mdtest can produce an arbitrary number of concurrent metadata requests from multiple clients and we believe it is a good benchmark for simulating multiple access streams to multiple metadata servers Although each access stream of a client produced by mdtest is regular it has the same eect on the metadata performance of our system as the random access streams because each le is mapped to a metadata server through the hash value of its name and the accesses on each server is mixed and random 5.2 Adaptability and Scalability We evaluated metadata adaptability through building a very large directory from zero and monitoring the le creation performance of each stage to show the eect of adaptive splitting scheme Since the adaptability can be shown in a small scale system only one server node and eight client nodes are used in this evaluation As shown in Figure 5 although the performance is uctuant slightly during splitting directory or partitions the performance of our metadata management is constant in general as the directory gets larger which validates the adaptability of our scheme To evaluate the scalability of our metadata system we tested the metadata performance of our system both under dierent scale of workloads and with dierent numbers of servers As metadata performance is aected by the size of a partition and the depth of its splitting the metadata performance is tested by changing the two values In each test mdtest was started on 200 clients with 5 test threads on each client Each test thread created or stated or deleted 100 to 100,000 les in the same directory respectively Distributing partitions by consistent hashing a fresh system needs an adjusting stage to achieve the balance of metadata placement among all the servers However peak performance of metadata processing is achieved after loads are balanced among all the servers Therefore in these tests partitions were evenly distributed among all the servers by manual which also resulted in equal loads among the servers The partition structure with a single chunk in each partition is similar to GIGA Dierent with this single-level partition our adaptive partitioning method allows multiple chunks in each partition As showed in Figure 6\(a single-level partitioning with small partition speciìcally 128 cmetas per partition here has good performance at small directory size However with the growth of a directory it soon reached its peak performance and its performance declined rapidly Single-level partitioning with large partition speciìcally 8192 cmetas per partition performed better at large directory size but still did not perform as well as our adaptive partitioning method The performance of the single-level partitioning method fell down when the directory size is beyond 1 million because with the increase of directory scale partition splits frequently to inîuence the performance On the contrary our adaptive two-level partitioning method has higher performance than the single-level one when the directory size is beyond 10 million The reason is that the adaptive two-level partitioning method will enlarge partition to reduce split operation to inîuence the normal process The static two-level partitioning method can reduce the number of partition split However it performs poor at small directory size because a fewer number of partitions leads to low concurrency in processing As shown in Figure 6\(b partition with 64 chunks has the lowest performance compared with the others at small directory size and its performance is increased with the number of partition increased Since our adaptive partitioning method has single 


 0 10000 20000 30000 40000 50000 60000 70000 4 8 16 32 File creation Throughput \(ops/sec Server number 10k files 100k files 1m files 10m files 100m files a File creation 0 100000 200000 300000 400000 500000 600000 4 8 16 32 File stat Throughput \(ops/sec Server number 100k files 1m files 10m files 100m files b File stat Figure 8 Metadata server scalability In le creation multiple servers can have enough work to achieve peak performance with large directories However the conìguration of 32 servers can not have enough work with small directories to perform well   0   20000   40000   60000   80000   100000   120000   140000 10k 100k 1m 10m 100m 1g File creation Thoughput \(ops/sec Directory size Create with Split Create without Split Figure 7 Directory Scalability Create with split means create a directory with partition directory over multiple servers Create without split means the directory has already been partition over multiple servers and no splitting happens during creating les chunk for each partition when the directory is small and enlarges each partition to contain multiple chunks when the directory becames very large it can achieve high performance at any scale of directories Larger-sized chunks lead to fewer number of partitions splits and reduce the overhead for le creation However large chunks also lead to ineciency of search within a chunk As Figure 6\(c shows the lookup performance of larger-sized chunks is lower than that of smaller-sized chunks This means the beneìts of reducing the number of partitions can be counteracted by the cost of locating a cmeta within a chunk However small chunks with few cmeta are also inefìcient because it can result in frequent partition splitting as well as inecient partition search This indicates that small chunks can impede scaling up Therefore suitable chunk size is the trade-o between partition splitting and intra-chunk search Our experiment results show that the preferable chunk size is the one containing 128 cmetas The directory scalability test result is shown in Figure 7 We can see that with 32 metadata servers the create performance is stable even in the directory with 1 billion les The create without split mode means there were no partition split or enlarge during processing as the partition structures and the chunk structures had been constructed in the creation with split mode To show the processing scalability with the increase of the number of servers we run the same workload on dierent number of servers The Figure 8\(a and b shows both the le creation performance and lookup performance indicated by stat performance increase with the number of metadata servers Since the creation operations can be processed in parallel at the granularity of partitions the small directory can not fully use the power of multiple servers as it contains few partitions On the other hand the lookup operations can be processed in parallel at the granularity of cmeta its performance is increased linearly with the size of the directory Since the partitions of a small directory can be cached in the memory the performance of small directories is much higher than that of large directories We have evaluated our system at the directory size of 1 billion les It needs about 1.2TB to hold all the metadata Since the size of cmeta is 368B 1 billion cmetas occupies 368GB We waste about three times as much as the metadata storage needed To support a le system with a trillion les we need to reduce holes in directory partition to improve the utilization of storage 5.3 Load Balance Consistent hashing is used to assign partitions to servers in order to automatically balance load among servers when server conìguration is changed However this method may lead to imbalanced load as a result of imbalanced distribution of partitions among a small number of servers Furthermore the accesses to contents of each partition may also vary distinctly To deal with these problems our system exploits a dynamic load balancing method as described in section 3.4 In our dynamic load-balance method both the load threshold which triggers the rebalancing of loads and the amount of partitions that need to be migrated are two key factors for actual load distribution To investigate their inîuence on metadata performance we used mdtest to produce workloads to 32 metadata servers from 200 clients In each test mdtest created 10 million les in the same directory and 


 20 21 22 23 24 25 26 27 28 60 65 70 75 80 85 90 95 Avarage CPU time per Server \(Minute Balance Threshold \(CPU Idle Percentage Migrate 5 Migrate 10 Migrate 15 a Average CPU Time 0 0.5 1 1.5 2 2.5 3 3.5 4 60 65 70 75 80 85 90 95 Standard Deviation Balance Threshold \(CPU Idle Percentage Migrate 5 Migrate 10 Migrate 15 b Mean Square Division 100000 120000 140000 160000 180000 200000 220000 240000 60 65 70 75 80 85 90 95 File Creationg Throughput \(ops/sec Balance Threshold \(CPU Idle Percentage Migrate 5 Migrate 10 Migrate 15 c File Creation Throughput 300000 400000 500000 600000 700000 800000 60 65 70 75 80 85 90 95 File stat Throughput \(ops/sec Balance Threshold \(CPU Idle Percentage Migrate 5 Migrate 10 Migrate 15 d File Stat Throughput Figure 9 Load Balance Eect on dierent Parameters The migrate percentage means the amount of hash range which need to migrate between busy server and idle server on each rebalance operation The number of partitions to be migrated is determined by the migrate percentage and the range of hash function values of the busy server stated them afterwards and deleted them nally And this process was repeated 3 times CPU idle percentage is used to measure load threshold High CPU idle percentage means low load threshold Figure 9\(a and b show that aggressive load balancing with lower load threshold is better for load balance The reason is that each le is accessed the same times in the test workloads produced by mdtest  The imbalance of load among the servers is due to the imbalance distribution of partitions among servers On this situation low load threshold results in frequent rebalancing and the load among the servers can keep balance most of the time We guess that for more dynamic and changing workloads aggressive load balancing may not always be good These two gures have also shown lower percentage migration will be good for low load threshold while higher percentage migration will be good for high load threshold Thatês because frequent rebalance need small balance granularity to avoid disturbance with normal processing and large balance granularity will be ne for less frequent rebalance Figure 9\(c and d show that 10 percent of partitions are migrated to a light-loaded server is best on such situation which each le is equally accessed Less percentage resulted in long time to reach balance while more percentage aected normal processing of current working set and decreased the overall performance As the amount of partitions to be migrated is a trade o between the time cost to reach balance and the interference with current processing we guess that around 10 percent is preferable 6 CONCLUSION Directory organization and partitioning are the key factors that determine the metadata performance of large le systems or directories with billions or trillions of les To address the eciency of managing trillions of les in a single cluster le system which is increasingly required by more and more applications including both Internet services applications and high-end computing applications we present an adaptive and scalable and metadata management system which dynamically partitions each directory by an extendiblehashing-based method It can achieve high metadata performance by splitting directory into partitions when the size of directory is small and by enlarge partitions with more chunks when the size of directory is large And these two levels of splitting are done automatically by our system to adapt to dierent size of directories and the growth of directories Further more our system also exploits ne-grained parallel processing with a single directory multiple-layered metadata cache management and a dynamic load balancing mechanism based on consistent hashing These methods further improve metadata processing performance The experiment results show that our adaptive and scalable directory partitioning method can provide the best performance compared with other static method The performance tests on 32 metadata servers show that our system can create more than 74,000 les per second and can lookup more than 270,000 les per second in a single directory with 100 million les The prototype implementation delivers a peak throughput of more than 60 thousand le creates/second in a single directory with 1 billion les which 


proves that it can maintain billions of les eciently Moreover multiple metadata servers scale well and the dynamic load balance mechanism can reach balance among all the servers quickly 7 ACKNOWLEDGMENTS This work is supported in part by the National High-Tech Research and Development Program of China under grant numbered 2006AA01A102 The authors gratefully acknowledge the support of K.C Wong Education Foundation Hong Kong The authors would like to thank Professor Weisong Shi of Wayne State University for his discussion and helpful advices We would also like to thank Panyong Zhang and Zhigang Huo for managing the Dawning5000 testbed The nal version has beneìted greatly from the many detailed comments and suggestions from the anonymous reviewers and our shepherd Professor Garth Gibson and his student Swapnil Patil from CMU 8 REFERENCES  L arge synoptic s urv ey telescop e http://www.lsst.org/lsst  2008  P  B raam M  C allahan and P  S c h w a n The intermezzo le system In In Proceedings of the 3rd of the Perl Conference O  a rReilly Open Source Convention Monterey  1999  P eter J Braam The l ustre s torage arc h itecture 2 004  P eter F Corb ett a nd Dror G F e itelson The v esta parallel le system ACM Trans Comput Syst  14\(3 1996  D ARP A IPTO Exascale c omputing s tudy Technology challenges in achieving exascale systems 2008  J ohn R  D ouceur and J on Ho w e ll D istributed directory service in the farsite le system In OSDI 06 Proceedings of the 7th symposium on Operating systems design and implementation  pages 321Ö334 Berkeley CA USA 2006 USENIX Association  R onald F agin Jurg Niev ergelt N ic holas P ipp e nger and H Raymond Strong Extendible hashingÑa fast access method for dynamic les ACM Trans Database Syst  4\(3 1979  G regory R  G anger a nd M F r ans K aasho e k Embedded inodes and explicit grouping exploiting disk bandwidth for small les In ATEC 97 Proceedings of the annual conference on USENIX Annual Technical Conference  pages 1Ö17 Berkeley CA USA 1997 USENIX Association  S anja y G hema w a t Ho w a rd Gobio a nd Sh un-T a k Leung The google le system In SOSP 03 Proceedings of the nineteenth ACM symposium on Operating systems principles  pages 29Ö43 New York NY USA 2003 ACM  T o dd Ho Flic kr arc h itecture http://highscalability.com/flickr-architecture  2007  Y u Hua Yifeng Zh u Hong Jiang Dan F eng and L ei Tian Scalable and adaptive metadata management in ultra large-scale le systems In ICDCS 08 Proceedings of the 2008 The 28th International Conference on Distributed Computing Systems  pages 403Ö410 Washington DC USA 2008 IEEE Computer Society  La wrence Liv e rmore N ational L ab oratory  mdtest-1.7.4 http://sourceforge.net/projects/mdtest  2007  E L Miller and R  H  K atz Rama A n e asy-to-use high-performance parallel le system In Parallel Computing  pages 419Ö446 1997  John K Ousterhout A ndrew R Cherenson F r ederic k Douglis Michael N Nelson and Brent B Welch The sprite network operating system Computer  21\(2 1988  Sw apnil V  P atil and G arth Gibson Giga s calable directories for shared le systems http://highscalability.com/flickr-architecture  2008  Sw apnil V  P atil G arth Gibson Sam L ang and M ilo Polte Giga scalable directories for shared le systems In PDSW 07 Proceedings of the 2nd international workshop on Petascale data storage  pages 26Ö29 2007  Brian P a w lo wski C het J uszczak P eter Staubac h  C arl Smith Diane Lebel and David Hitz Nfs version 3 design and implementation In In Proceedings of the Summer USENIX Conference  pages 137Ö152 1994  Daniel Phillips A directory i ndex f or ext2 I n ALS 01 Proceedings of the 5th annual Linux Showcase  Conference  pages 20Ö20 Berkeley CA USA 2001 USENIX Association  O Ro deh a nd A T e p e rman z fs a scalable distributed le system using object disks In In Proceedings of the 20th IEEE  11th NASA Goddard Conference on Mass Storage Systems and Technologies  pages 207Ö218 2003  F r ank S c h m u c k and R oger Haskin G pfs A shared-disk le system for large computing clusters In FAST 02 Proceedings of the 1st USENIX Conference on File and Storage Technologies  pages 231Ö244 Berkeley CA USA 2002 USENIX Association  Adam Sw eeney  D oug D oucette W e i H u Curtis Anderson Mike Nishimoto and Geo Peck Scalability in the xfs le system Proceedings of the USENIX 1996 Annual Technical Conference  1996  P e ter V a j gel Needle in a h a y stac k:ecien t s torage of billions of photos http://www.facebook.com/note php?note_id=76191543919  2009  Sage A W e il K ristal T P o llac k Scott A  B randt a nd Ethan L Miller Dynamic metadata management for petabyte-scale le systems In SC 04 Proceedings of the 2004 ACM/IEEE conference on Supercomputing  page 4 Washington DC USA 2004 IEEE Computer Society 24 Yifeng Zhu Hong Jiang Jun Wang and Feng Xian Hba Distributed metadata management for large cluster-based storage systems IEEE Trans Parallel Distrib Syst  19\(6 2008 


block We demonstrate the effectiveness of the proposed method through attack emulation  where we inject certain types of attacks or other anomalous activities into a block with otherwise normal activities We have performed this study using a range of anomalies with a variety of activity patterns including outside scanning  back-door trojan activities and ddos attacks  We emulate the outside scanning activity by injecting with 3 response ows from TCP srcPort25 to all active hosts in the network To emulate the back-door trojan activities we inject 1000 response ows associated with TCP srcPort 443 to one client machine in the network We also emulate the ddos attacks by injecting 1000 echo reply ows ICMP srcPort 0 to one server machine in the network When an anomaly occurs if it is signiﬁcant either involving a large number of ows e.g ddos attacks or involving a large number of hosts e.g scanning attacks the pLSA decomposition result is expected to contain a new activity pattern corresponding to the anomaly behavior Therefore our goal is to identify the activity pattern which after attack injection deviates the most from all activity patterns before injection Hence we deﬁne an anomaly score AS as follows   v 1 v K  and  w 1 w L  represents the sets of activity patterns before and after attack injection respectively AS  max 1 005 j 005 L JSD  v 004 j w j  8 where JSD  v 004 j w j  is the minimum JSD between w j and any of the activity patterns or the best matching peer before injection The AS score ranges from 0 to 1 with 1 indicating the new activity is completely different from all activities before injection We select three different types of blocks for the attack emulation 1 a departmental block which contains mostly Email servers and HTTP servers 2 a residential hall block consisting mostly of client machines and 3 a very dynamic wireless block We conduct the attack emulation on all these three blocks As a baseline for comparison we compute the AS scores for block activities between 02/08/2006 and 02/09/2006 which can be considered as the maximum possible normal activity change in the block We summarize the results in Table V-B The second column shows the normal change measured by the AS scores We observe that for all the blocks the most signiﬁcant change of the normal activities only results in an AS score of 0.3-0.5 However after injecting a signiﬁcant anomaly activity which is different from all activity patterns in the network the AS scores rise to 0.65-0.80 This suggests that our pLSA decomposition results indeed capture the emerging anomaly activity patterns in the network Though the administrator needs to apply more complicated methods e.g deep packet inspection and other source of information e.g port numbers or domain names our method due to its light-weight and high efﬁcacy can be used as a real-time tool for tracking the network block behaviors and identify potential anomaly activities in the network VI C ONCLUSIONS In this paper we developed HEAPs a scalable and effective methodology which employs a local-global two-stage process Description Normal Scanning DDoS Back-door Departmental 0.4626 0.6553 0.6873 0.5835 Residential Hall 0.5280 0.7685 0.7616 0.6785 Wireless 0.3023 0.8312 0.8588 0.8013 TABLE V A TTACK EMULATION AND ANOMALY DETECTION RESULT to automatically extract activity patterns from massive network data In the rst process we illustrated the use of pLSA to extract locally signiﬁcant activity patterns using network data from individual network subnets Using the signiﬁcant activity patterns extracted from individual subnets in the second stage we performed global classiﬁcation of activity patterns by analyzing the similarities of locally extracted activity patterns and grouping them accordingly using a hierarchical clustering algorithm We also demonstrated how HEAPs can be used to proﬁle and track what are running on subnets To further illustrate the utility of HEAPs we artiﬁcially injected attack trafﬁc into the real network data and showed how they could be detected through extracted anomalous activity patterns on individual subnets Our method is general and scalable and can be applied to any other campus networks or enterprise network regardless of its size R EFERENCES  T homas Hofmann Unsupervised learning from dyadic d ata pages 466 472 MIT Press 1998  T homas Hofmann Unsupervised learning by probabilistic latent s emantic analysis In Machine Learning  page 2001 2001  T homas Landauer  P  W  F o ltz and D  L aham I ntroduction t o latent semantic analysis Discourse Processes  25 1998  F  D e l a T orre and M  J  B lack A frame w o rk for rob ust s ubspace learning International Journal of Computer Vision  54\(1-3 2003  I  D hillon Co-clustering documents and w ords using b ipartite spectral graph partitioning In Proc of KDD  2001  D  Lee and H  S eung Learning the p arts of objects b y non-ne gati v e matrix factorization In Nature  1999  W  X u X Liu and Y  G ong Document clustering b ased on non-ne gati v e matrix factorization In Proc of SIGIR  2003  C  D ing T  Li W  P eng and H  P ark Orthogonal nonne gati v e matrix t-factorizations for clustering In Proc of ACM KDD  2006  D a v id M Blei Andre w Y  Ng a nd Michael I J ordan Latent d irichlet allocation Journal of Machine Learning Research  3 2003  P  Haf f ner  S Sen O Spatscheck a nd D W a ng A C AS Automated construction of application signatures In Proc of MineNet  2005  T  Karagiannis A Broido M F aloutsos and K  claf fy  T ransport layer identiﬁcation of p2p trafﬁc In Proc of ACM IMC  2004  A Moore a nd D Zue v  I nternet t raf  c classiﬁcation u sing bayesian analysis techniques In Proc of ACM SIGMETRICS  2005  Jef fre y Erman Martin Arlitt and A nirban Mahanti T r af c classiﬁcation using clustering algorithms In Proc of the ACM SIGCOMM workshop on Mining network data MineNet’06  2006  E Sharafuddin Y  Jin N Jiang and Z L Zhang Kno w your enemy  know yourself Block-level network behavior proﬁling and tracking In Preprint  2009  Y  Jin E Sharafuddin and Z L Z hang Un v eiling c ore n etw orkwide communication patterns through application trafﬁc activity graph decomposition In Proc of SIGMETRICS 09  pages 49–60 2009  K Xu Z L Zhang a nd S Bhattacharyya P roﬁling I nternet b ackbone trafﬁc behavior models and applications In Proc of ACM SIGCOMM  August 2005  R Duda P  H art and D  S tork Pattern Classiﬁcation 2nd Edition  Wiley-Interscience 2000  Ian H  W itten and E ibe F rank Data Mining Practical Machine Learning Tools and Techniques with Java Implementations  1999 
696 
696 


  13 R EFERENCES  1 th o n y J. Hayter, Pro b ab ility an d Statistics fo r En g i n eers and Scientists, Third Edition, Belmont, CA, Duxbury 2007 2 C. J. Clo p p er  E S Pear son, \223The Use of Confidence or Fiducial Limits Illustrated in the Case of the Binomial,\224 Biometrika, 26, 404-413, 1934   Jam e s O. Berger, Statis tical Decision Theory and Bayesian Analysis, Second Edition. Springer-Verlag, New York, 1980 4 A. Po well, \223Op tim al an d  Ad ap tab l e Reliab ility Test Planning Using Conditional Methods,\224 Proceedings from the 14 th Annual International Symposium, International Council on Systems Engineering, Toulouse, FR, June 20\226 24, 2004  Harold Jeffreys, Theory of Probability. Oxford University Press, Oxford, 1939  Edward Jaynes, \223Prior Pr obabilities\224. IEEE Transactions Systems, Science and Cybernetics, 4, 227-291, 1968   Jose Bernardo and Adrian Smith, Bayesian Theory. John Wiley & Sons, LTD, New York, 1994   Howard Raifa and Robert Sc hlaifer, Applied Statistical Decision Theory. John Wiley & Sons, Inc. New York 1960 9 Brian Mu irh ead 223Co n stella tion Architecture and System Margins Strategy,\224 Proceedin gs from the International Astronautical Congress, Glasgo w, Scotland, September 29 226 October 3, 2008 10  Sco tt \223Do c\224 Ho ro witz, \223Cu rren t  Co n stellatio n  Planning,\224 presented to the House Science and Technology Committee Staff, March 13, 2007   M a rk A. Powell, Safety and M i ssion Assurance Special Assessments, CxP Orion LAS Verification Planning Evaluation Report, Repository Number: JS-2008-002 November 12, 2007      B IOGRAPHY  Mark Powell has practiced Systems Engineering for over 35 years in a wide range of technical environments including DoD, NASA, DOE, and commercial. More than 25 of those years have been in the aerospace arena. His roles in these environments have included project manager, engineering manager, chief systems engineer, and research scientist. He is currently an adjunct member of the Stevens Institute of Technology Syst ems Engineering Faculty, and of the University of Houston, Clear Lake Systems Engineering Faculty. Mr. Powell maintains an active engineering and management c onsulting practice \(currently in affiliation with SAIC\ North America, Europe, and Asia. Beyond consulting, he is sought frequently as a symposium and conference speaker and for training workshops, and tutorials on various topics in Systems Engineering, Project Management, and Risk Management Mr. Powell is an active member of AIAA, Sigma Xi, the International Society for Bayesian Analysis, and the International Council on Systems Engineering, where he serves as Assistant Direct or for Systems Engineering Processes  


  14  


2008, vol. 278/2008. Boston: Springer, July 2008, pp. 477  492 24] T. Neubauer, C. Stummer, and E. Weippl  Workshop-based Multiobjective Security Safeguard Selection  in Proceedings of the First International Conference on Availability, Reliability and Security ARES IEEE Computer Society, 2006, pp. 366  373 25] T. Neubauer and C. Stummer  Interactive Decision Support for multiobjective COTS Selection  in Proceedings of the 40th Annual Hawaii International Conference on System Sciences, no. 01, 2007 26    Extending Business Process Management to Determine Ef?cient IT Investments  in Proceedings of the 2007 ACM Symposium on Applied Computing, 2007, pp. 1250  1256 27] W3C  OWL - web ontology language  http://www.w3.org/TR/owlfeatures/, February 2004 28] J. Burtles, Principles and Practice of Business Continuity: Tools and Techniques. Rothstein Associates Inc., 2007 29] T. R. Peltier, Information Security Risk Analysis, 2nd ed. Auerbach Publications, 2005 30] S. Kairab and L. Kelly, A Practical Guide to Security Assessments Boston, MA, USA: Auerbach Publications, 2004 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


well as from practitioners, is available, IL strategy including DWH/BI strategy IL strategies are addressed at all, either actual artifacts/documents are proposed without an integrating meta model/methodology, or the strategy making process is described without proposing specific and consistent result templates/structures Since it consumes a significant amount of resources and may constitute significant potentials for business, IL needs strategy. IL strategy must not be limited to hardware/software selection and architectural considerations, but should address the entire business scope of sourcing services, integrating acquired and self-made services into customer-oriented IL solutions, and delivering such solutions to create customer value Our survey of the state of IL strategy in practice reveals that IL sourcing, IL delivery and IL portfolio strategies are regarded as important strategy components. The larger companies are, the more international their focus is, and the more their IL is organized according to the CC model, the more components of a supply-chain oriented explicit IL strategy they are likely to have deployed The IIM model provides a suitable conceptual foundation for structuring such strategy components and also provides best practices from IT management which often can be easily adapted to IL. Regarding IL product/service development and maintenance certain functional oriented strategy sub-components are differentiated in our framework. These strategy components are adapted from an established data management functional framework in order to reflect IL specifics. While traditional, more technically oriented sub-components such as system and data architecture are covered in most companies, business oriented components like change management and project/business requirements management are covered less frequently. Additional research is necessary to develop appropriate solution components based on existing fragments and experiences Based on a more complete comprehension of IL strategy and its components, the strategy development and update process needs to be addressed in future research as well. Instead of developing and updating business strategies, IT strategies and IL strategies in independent processes, dependencies and cycles need to be addressed. A comprehensive understanding of IL strategy and respective processes may also serve as a foundation for establishing maturity models, reference models and best practices  References 1] Arnott, D. and G. Pervan, Eight key issues for the decision support systems discipline. Decision Support Systems 44\(3  2] Baum  l, U., Strategic Agility through Situational Method Construction. Proceedings of the European Academy of Management Annual Conference 2005, 2005  3] Burton, B., et al., Activity Cycle Overview: Business Intelligence and Information Management. Gartner Research G00138711, 2006  4] Chan, J.O., Optimizing Data Warehousing Strategies Communications of the IIMA, 5\(1  5] Earl, M., Management Strategies for Information Technology, Prentice Hall, New York et al., 1989  6] Eckerson, W.W., Data Quality and the Bottom Line 


6] Eckerson, W.W., Data Quality and the Bottom Line Achieving Business Success through a Commitment to High Quality Data. TDWI, Chatsworth, 2002  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 7] Elliott, T., Implementing Business Intelligence Standards. BusinessObjects, 2004  8] English, L.P., Improving Data Warehouse and Business Information Quality: Methods for Reducing Costs and Increasing Profits, Wiley Computer, New York et al., 1999  9] Foshay, N., Best Practices in Business Intelligence Strategy. Blue Hammock, 2006  10] Friedman, T. and B. Hostmann, Management Update The Cornerstones of Business Intelligence Excellence Gartner Research G00120819, 2004  11] Gonzales, M., Creating a BI Stragey Document. DM Review, 2004\(November  12] Henderson, J.C. and N. Venkatraman, Strategic alignment: Leveraging information technology for transforming organizations. IBM Systems Journal, 32\(1  13] Hoffmann, O., Performance Management - Systeme und Implementierungsans  tze. 3 ed, Haupt, Bern et al 2002  14] Klesse, M. and R. Winter, Organizational Forms of Data Warehousing: An Explorative Analysis. in: IEEE Computer Society, Proceedings of the 40th Hawaii International Conference on System Sciences \(HICSS-40 Alamitos, 2007  15] Laudon, J. and K. Laudon, Management Information Systems: Managing the Digital Firm. 10 ed, Prentice Hall 2006  16] Losey, R., Enterprise Data Warehouse Strategy: Articulating the Vision. Dm Review, 2003\(January  17] Luftman, J.N. and R. Kempaiah, Key Issues For IT Executives 2007. MISQ Executive, 7\(2  18] MAIS and AIMS, A Business Intelligence Strategy Proposal for The University of Michigan. 2005  19] Melchert, F., Metadatenmanagement im Data Warehousing. Ergebnisse einer empirischen Studie. Institut f  r Wirtschaftsinformatik, Universit  t St. Gallen, 2004  20] Mosley, M., DAMA-DMBOK Functional Framework Version 3. DAMA International, 2008  21] Olszak, C.M. and E. Ziemba, Business Intelligence as a Key to Management of an Enterprise. in: Informing Science Institute, Informing Science + Information Technology Education, Pori, Finland, 2003  22] R  egg-St  rm, J., The New St. Gallen Management Model: Basic Categories of an Approach to Integrated Management, Palgrave Macmillan, Basingstoke, NY, 2005  23] Sommer, T., et al., Business Intelligence-Strategie bei der Volkswagen AG. in: Integrierte Informationslogistik B. Dinter and R. Winter, Editors, 2008, Springer, Berlin Heidelberg. pp. 261-284  


 24] Subramaniam, A., et al., Strategic planning for Data warehousing. Information &amp; Management, 33, 1997, pp 99-113  25] Totok, A., Entwicklung einer Business-IntelligenceStrategie. in: Analytische Informationssysteme - Business Intelligence-Technologien und -Anwendungen, P. Chamoni and P. Gluchowski, Editors, 2006, Springer, Berlin et al pp. 51-70  26] Vaduva, A. and T. Vetterli, Metadata Management for Data Warehousing: An Overview. International Journal of Cooperative Information Systems, 10\(3 298  27] Watson, H.J., D.L. Goodhue, and B.H. Wixom, The benefits of data warehousing: why some organizations realize exceptional payoffs. Information &amp; Management 39\(6  28] Watson, H.J., C. Fuller, and T. Ariyachandra, Data warehouse governance: best practices at Blue Cross and Blue Shield of North Carolina. Decision Support Systems 38\(3  29] Winter, R. and M. Meyer, Organization Of Data Warehousing In Large Service Companies: A Matrix Approach Based On Data Ownership and Competence Centers. Proceedings of the Seventh Americas Conference on Information Systems \(AMCIS 2001  30] Winter, R., Enterprise-wide Information Logistics Conceptual Foundations, Technology Enablers, and Management Challenges. ITI2008, 2008  31] Zarnekow, R., W. Brenner, and U. Pilgram, Integrated Information Management. Applying Successful Industrial Concepts in IT. 1 ed, Springer, Berlin, 2006  32] Zeid, A., Your BI Competency Center: A Blueprint for Successful Deployment. Business Intelligence Journal 11\(3    Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


The HyspIRI mission utilizes innovative techniques to both reduce the amount of data that must be transmitted to the ground and accommodate the required data volume on the ground The infrastructure and techniques developed by this mission will open the door to future high data volume science missions The designs presented here are the work of the authors and may differ from the current HyspIRI mission baseline A CKNOWLEDGMENTS This research was carried out at the Jet Propulsion Laboratory California Institute of Technology and was sponsored by the Space Grant program and the National Aeronautics and Space Administration R EFERENCES  K W ar\002eld T  V  Houten C Hee g V  Smith S Mobasser B Cox Y He R Jolly C Baker S Barry K Klassen A Nash M Vick S Kondos M Wallace J Wertz Chen R Cowley W Smythe S Klein L Cin-Young D Morabito M Pugh and R Miyake 223Hyspiri-tir mission study 2007-07 002nal report internal jpl document,\224 TeamX 923 Jet Propulsion Laboratory California Institute of Technology 4800 Oak Grove Drive Pasadena CA 91109 July 2007  R O Green 223Hyspiri summer 2008 o v ervie w  224 2008 Information exchanged during presentation  S Hook 2008 Information e xchanged during meeting discussion July 16th  R O Green 223Measuring the earth wi th imaging spectroscopy,\224 2008  223Moore s la w Made real by intel inno v ation 224 http://www.intel.com/technology/mooreslaw/index.htm  T  Doggett R Greele y  S Chein R Castano and B Cichy 223Autonomous detection of cryospheric change with hyperion on-board earth observing-1,\224 Remote Sensing of Environment  vol 101 pp 447\226462 2006  R Castano D Mazzoni N T ang and T  Dogget 223Learning classi\002ers for science event detection in remote sensing imagery,\224 in Proceedings of the ISAIRAS 2005 Conference  2005  S Shif fman 223Cloud detection from satellite imagery A comparison of expert-generated and autmatically-generated decision trees.\224 ti.arc.nasa.gov/m/pub/917/0917 Shiffman  M Griggin H Burk e D Mandl and J Miller  223Cloud cover detection algorithm for eo-1 hyperion imagery,\224 Geoscience and Remote Sensing Symposium 2003 IGARSS 03 Proceedings 2003 IEEE International  vol 1 pp 86\22689 July 2003  V  V apnik Advances in Kernel Methods Support Vector Learning  MIT Press 1999  C Bur ges 223 A tutorial on support v ector machines for pattern recognition,\224 Data Mining and Knowledge Discovery  vol 2 pp 121\226167 1998  M Klemish 223F ast lossless compression of multispectral imagery internal jpl document,\224 October 2007  F  Rizzo 223Lo w-comple xity lossless compression of h yperspectral imagery via linear prediction,\224 p 2 IEEE Signal Processing Letters IEEE 2005  R Roosta 223Nasa jpl Nasa electronic parts and packaging program.\224 http://nepp.nasa.gov/docuploads/3C8F70A32452-4336-B70CDF1C1B08F805/JPL%20RadTolerant%20FPGAs%20for%20Space%20Applications.pdf December 2004  I Xilinx 223Xilinx  Radiation-hardened virtex-4 qpro-v family overview.\224 http://www.xilinx.com/support/documentation data sheets/ds653.pdf March 2008  G S F  Center  223Tdrss o v ervie w  224 http://msp.gsfc.nasa.gov/tdrss/oview.html 7  H Hemmati 07 2008 Information e xchanged during meeting about LaserComm  223W orldvie w-1 224 http://www digitalglobe.com/inde x.php 86/WorldView-1 2008  223Sv albard ground station nor way.\224 http://www.aerospacetechnology.com/projects/svalbard 7 2008  223Satellite tracking ground station 224 http://www.asf.alaska.edu/stgs 2008  R Flaherty  223Sn/gn systems o v ervie w  224 tech rep Goddard Space Flight Center NASA 7 2002  223Geoe ye-1 f act sheet 224 http://launch.geoeye.com/launchsite/about/fact sheet.aspx 2008  L-3 Communications/Cincinnati Electronics Space Electronics 7500 Innovation Way Mason OH 45040 T-720 Transmitter  5 2007 PDF Spec Sheet for the T720 Ku-Band TDRSS Transmitter  L-3 Communications/Cincinnati Electronics Space Electronics 7500 Innovation Way Mason OH 45040 T-722 X-Band  7 2007 PDF Spec Sheet for the T-722  J Smith 07 2008 Information e xchanged during meeting about GDS  J Carpena-Nunez L Graham C Hartzell D Racek T Tao and C Taylor 223End-to-end data system design for hyspiri mission.\224 Jet Propulsion Laboratory Education Of\002ce 2008  J Behnk e T  W atts B K obler  D Lo we S F ox and R Meyer 223Eosdis petabyte archives Tenth anniversary,\224 Mass Storage Systems and Technologies 2005 Proceedings 22nd IEEE  13th NASA Goddard Conference on  pp 81\22693 April 2005 19 


 M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Evolving a ten year old data system,\224 Space Mission Challenges for Information Technology 2006 SMC-IT 2006 Second IEEE International Conference on  pp 8 pp.\226 July 2006  S Marle y  M Moore and B Clark 223Building costeffective remote data storage capabilities for nasa's eosdis,\224 Mass Storage Systems and Technologies 2003 MSST 2003 Proceedings 20th IEEE/11th NASA Goddard Conference on  pp 28\22639 April 2003  223Earth science data and information system esdis project.\224 http://esdis.eosdis.nasa.gov/index.html  M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Evolution of the earth observing system eos data and information system eosdis\\224 Geoscience and Remote Sensing Symposium 2006 IGARSS 2006 IEEE International Conference on  pp 309\226312 31 2006Aug 4 2006  223Earth science mission operations esmo 224 http://eos.gsfc.nasa.gov/esmo  E Masuoka and M T eague 223Science in v estig ator led global processing for the modis instrument,\224 Geoscience and Remote Sensing Symposium 2001 IGARSS 01 IEEE 2001 International  vol 1 pp 384\226386 vol.1 2001  M Esf andiari H Ramapriyan J Behnk e and E So\002nowski 223Earth observing system eos data and information system eosdis 227 evolution update and future,\224 Geoscience and Remote Sensing Symposium 2007 IGARSS 2007 IEEE International  pp 4005\2264008 July 2007  D McAdam 223The e v olving role of tape in the data center,\224 The Clipper Group Explorer  December 2006  223Sun microsystems announces w orld s 002rst one terabyte tape storage drive.\224 http://www.sun.com/aboutsun/pr/200807/sun\003ash.20080714.2.xml July 2008  223P anasas 227 welcome 224 http://www panasas.com  R Domikis J Douglas and L Bisson 223Impacts of data format variability on environmental visual analysis systems.\224 http://ams.confex.com/ams/pdfpapers/119728.pdf  223Wh y did nasa choose hdf-eos as the format for data products from the earth observing system eos instruments?.\224 http://hdfeos.net/reference/Info docs/SESDA docs/NASA chooses HDFEOS.php July 2001  R E Ullman 223Status and plans for hdfeos nasa's format for eos standard products.\224 http://www.hdfeos.net/hdfeos status HDFEOSStatus.htm July 2001  223Hdf esdis project.\224 http://hdf.ncsa.uiuc.edu/projects/esdis/index.html August 2007  223W elcome to the ogc website 224 http://www.opengeospatial.org 2008  223Open gis Gis lounge geographic information systems.\224 http://gislounge.com/open-gis Christine M Hartzell received her B.S in Aerospace Engineering for Georgia Institute of Technology with Highest Honors in 2008 She is currently a PhD student at the University of Colorado at Boulder where she is researching the impact of solar radiation pressure on the dynamics of dust around asteroids She has spent two summers working at JPL on the data handling system for the HyspIRI mission with particular emphasis on the cloud detection algorithm development and instrument design Jennifer Carpena-Nunez received her B.S in physics in 2008 from the University of Puerto Rico where she is currently a PhD student in Chemical Physics Her research involves 002eld emission studies of nanostructures and she is currently developing a 002eld emission setup for further studies on nano\002eld emitters The summer of 2008 she worked at JPL on the HyspIRI mission There she was responsible for the science analysis of the data handling system speci\002cally de\002ning the data level and processing and determining potential mission collaborations Lindley C Graham is currently a junior at the Massachusetts Institute of Technology where she is working towards a B.S in Aerospace Engineering She spent last summer working at JPL on the data handling system for the HyspIRI mission focusing on developing a data storage and distribution strategy 20 


David M Racek is a senior working toward a B.S in Computer Engineering at Montana State University He works in the Montana State Space Science and Engineering Laboratory where he specializes in particle detector instruments and circuits He spent last summer working at JPL on compression algorithms for the HyspIRI mission Tony S Tao is currently a junior honor student at the Pennsylvania State University working towards a B.S in Aerospace Engineering and a Space Systems Engineering Certi\002cation Tony works in the PSU Student Space Programs Laboratory as the project manager of the OSIRIS Cube Satellite and as a systems engineer on the NittanySat nanosatellite both of which aim to study the ionosphere During his work at JPL in the summer of 2008 Tony worked on the communication and broadcast system of the HyspIRI satellite as well as a prototype Google Earth module for science product distribution Christianna E Taylor received her B.S from Boston University in 2005 and her M.S at Georgia Institute of Technology in 2008 She is currently pursing her PhD at the Georgia Institute of Technology and plans to pursue her MBA and Public Policy Certi\002cate in the near future She worked on the ground station selection for the HyspIRI mission during the summer of 2008 and looks forward to working at JPL in the coming year as a NASA GSRP fellow Hannah R Goldberg received her M.S.E.E and B.S.E from the Department of Electrical Engineering and Computer Science at the University of Michigan in 2004 and 2003 respectively She has been employed at the Jet Propulsion Laboratory California Institute of Technology since 2004 as a member of the technical staff in the Precision Motion Control and Celestial Sensors group Her research interests include the development of nano-class spacecraft and microsystems Charles D Norton is a Principal Member of Technical Staff at the Jet Propulsion Laboratory California Institute of Technology He received his Ph.D in Computer Science from Rensselaer and his B.S.E in Electrical Engineering and Computer Science from Princeton University Prior to joining JPL he was a National Research Council resident scientist His work covers advanced scienti\002c software for Earth and space science modeling with an emphasis on high performance computing and 002nite element adaptive methods Additionally he is leading efforts in development of smart payload instrument concepts He has given 32 national and international keynote/invited talks published in numerous journals conference proceedings and book chapters He is a member of the editorial board of the journal Scienti\002c Programming the IEEE Technical Committee on Scalable Computing a Senior Member of IEEE recipient of the JPL Lew Allen Award and a NASA Exceptional Service Medal 21 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207ñ216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intíl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intíl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





