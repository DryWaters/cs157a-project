Comparison between Adaptive and NonAdaptive HRBF Neural Network in Multiple Steps Time Series Forecasting Mazlina Mamat  Institute of Microengineering and Nanotechnology Universiti Kebangsaan Malaysia 43600 UKM Bangi Selangor E-mail: mazlina@eng.ukm.my   Salina Abdul Samad  Faculty of Engineering and Built Environment Universiti Kebangsaan Malaysia 43600 UKM Bangi Selangor E-mail: salina@eng.ukm.my   Abstract  This paper compares the performance of adaptive and non-adaptive learning approaches of the Hybrid Radial 
Basis Function \(HRBF\l network in multiple steps time series forecasting. The HRBF was trained by using the Adaptive Fuzzy C-Means Clustering \(AFCMC\ and Exponential Weighted Recursive Least Square \(e-WRLS algorithms. Both approaches were set to produce up to 25 steps ahead forecasting on two time series data: Mackey Glass and Set A Data from Santa Fe Competition. The performance of both approaches in multiple steps ahead forecasting was measured using Mean Square Er ror Test and Coefficient of Determination Test between the actual and forecasted data for the 25 steps ahead forecasting. Results show that both approaches perform comparatively equal for shorter forecasting distance. However for longer forecasting distance 10 steps ahead onwards\e adaptive approach performs 
significantly better to compare with non-adaptive approach Keywords - adaptive; non-adaptive; HRBF; time series forecasting I   I NTRODUCTION  The study on time series forecasting has started long time ago. Yet until today, this field still captures researches interest to explore its possibiliti es. Research in time series forecasting has develop impressively since the emergence of Autoregressive Integrated Moving Average \(ARIMA model by Box and Jenkins. To date, various methods were introduced from Regression an  Networks \(ANN u zzy Lo gic \(FL\ [3 and Genetic 
Algorithms  h em the com p utational intelligence technique such as ANN, FL and GA are getting more attention in time series forecasting because they are non-linear in nature and able to approximate easily complex dynamically system. ANN for instance, demonstrates promising performance in variou s time series forecasting 5  7  Studies show most ANN training algorithms use to solve time series forecasting probl em are implemented in nonadaptive approach. In non-adaptive approach, ANN is trained by using the actual data from the system to be forecasted. Throughout training, the ANN will update its 
parameters according to the training data. Once the ANN reaches optimum condition, the training process is stopped Then the optimized ANN will be used to estimate the forthcoming output based on the current received inputs The main drawback of non-adaptive approach is that the ANN parameters need to be re estimated from time to time This is essential in order to sustain the forecaster s reliability especially if the sy stem to be forecasted is nonstationary and displays different pattern over times. The problem is how to determine when the retraining should be conducted: before or after the forecaster produces 
unacceptable error?  To solve this, the adaptive learning approach is proposed. In adaptive approach, a continuous learning is imposed, meaning that the ANN is constantly adjusting its parameters based on the recent input and output. Therefore the forecaster is always been supplied with the current information of the system to be forecasted without the need for retraining  This paper compares the performance of adaptive and non-adaptive learning approach es of HRBF neural network on two time series data: Mackey Glass and Set A Data from Santa Fe Competition. This paper is organized as follows. A brief description about the H ybrid Radial Basis Function 
HRBF\ neural network and th e learning algorithms used for training are given in Section II. Section II also describes the time series data and the evaluation tests used to measure the performance of the HRBF forecaster. The results and discussion were presented in Section III. Section IV concludes the findings  II  A PPROACH AND M ETHODS  The HRBF Neural Network is a modification of the Radial Basis Function \(RBF It differs from the RBF by the linear connections that exist between input nodes and 
output nodes. Similarly to RBF, the HRBF neural network has three layers: an input layer, a hidden layer with a nonlinear RBF activation function and a linear output layer Each layer has its own nodes where the nodes in input layer are connected to the nodes in hidden layer and nodes in hidden layer are connected to the nodes in output layer via linear weight. There are two parameters: the HRBF centres in hidden nodes and weights between the hidden nodes and 
Second International Conference on Computer Research and Development 978-0-7695-4043-6/10 $26.00 © 2010 IEEE DOI 10.1109/ICCRD.2010.177 817 


the output nodes, that must be updated during training to produce the optimize network. Two algorithms: Adaptive Fuzzy C-Means Clustering \(AFCMC\nd Exponential Weighted Recursive Least Square \(e-WRLS\used to update the centre and weight of the HRBF neural network The AFCMC algorithm is a modification of Fuzzy C-Means Clustering Algorithm to overcome the dead centre, local minima and centre redundancy: common problem in the Fuzzy C-Means Clustering Algorithm [9  To esti m a te th e weight between the hidden nodes and output node, the Exponential Weighted Recursive Least Square \(e-WRLS\s used [1   To compare the forecasting performance of adaptive and non-adaptive approaches, two HRBF networks are constructed using Borland C++ Builder 6. The adaptive approach is constructed in such a way employs online learning whereas the non-adaptive approach employs offline learning. The multiple steps ahead \(MSA\forecasting were attained by using iterative tech nique. In iterative technique the network forecasts one step ahead \(OSA\nd then uses that forecast to forecast another step ahead, and so on. For example, consider an ANN receives input vectors m  t=1  m  t=2  m  t=3 nd m  t=4 produces OSA forecasting at m\(t=5 The MSA prediction at t=6 is achieved by feeding the OSA forecasting at m\(t=5 as input to the ANN while removing the output m\(t=1 from the input vector Similarly, to obtain the MSA forecasting at t=7 the forecasted output at t=6 will be used as input to the ANN while removing the oldest data from the input vector that is m\(t=2 This procedures is repeats to obtain the MSA forecasting for t=8, 9,10 and so on A  Data The forecasting performance of the RBF trained with two algorithms described above is evaluated by using two time series: Mackey-Glass nonlinear time series and Set A Data from the Santa Fe Competition. The forecasting based on time series produced by the Mackey-Glass equation is regarded as a criterion for comparing the ability of different predicting method and is used in many time series forecasting researches [1 12]. The Mackey-Gla ss equation is a time-delayed differential equation proposed as a model of white blood cel l production by Mackey and Glass. The Mackey-Glass equation is given by  10 1  1  ts tts ts x xx x 000D 000E 000\020 000\020 000\020 000 \000\016\000\020 000\016   1 Where 001 0.2 002\025 0.1, and s is delayed time. In this research s is set to 17 in which the equation exhibits chaotic behavior with a fractal dimension. A total of 1000 data were used and display in Fig. 1. The Set A Data from the Santa Fe Competition were recorded from a Far-Infrared-Laser in a chaotic state. These data were chosen because they are a good example of the complicated behavior that can be seen in a clean, stationary, low-dimensional non-trivial physical system for which the underlying governing equations dynamics are well understood. The Set A Data from the Santa Fe Competition are displayed in Fig. 2  B  Evaluation Tests The Root Mean Square Error \(RMSE\nd Coefficients of Determination \(R 2 e used as measurement of derivation between actual and forecasted values. The R 2  value denotes the close degree between actual and forecasted value and are varies, which 1 indicates that the predicted data and the actual data are identical. The RMSE and R 2 tests are defined in the following respectively   2 1    d a n tn RMSE y t y t n 000 000 \000\020 000  2   2 2 2    1   d a d a n tn n tn yt yt R yt y 000 000 000\020 000 \000\020 000\020 000 000   3   where n a   and n d  are the first and the last measurement data n is the number of data used in the measurement n = \(n d  n a 1  y\(t and t are actual and forecasted value at time t and y is the average actual value, respectively              Figure 1 Mackey Glass Data              Figure 2 Set A Data from Santa Fe Competition  
818 


III  R ESULTS AND D ISCUSSIONS  For the reason that the selection of input lag and the number of HRBF center have strong impact on the forecaster performance, the forecaster undergoes two analyses, first to determine the best input lag and second to determine the correct number of HRBF center. The analysis to determine the best input lags for all data is conducted by fixing the HRBF centers at 10 and varies the input lag from t-1 to t1\\(t-2\\(t3\…\(t x where the maximum number of x is 10 out of the total available data. The input lags which produces the highest R 2 values for all steps ahead then undergoes the analysis to determine the best number of HRBF center. This analysis is conducted by in creasing the number of HRBF center one by one until it reaches 10% out of the total available data. For each approach, the structure that produces the best forecasting performance \(lowest RMSE values and highest R 2 values\r all steps ahead forecasting is selected as the best structure. From these analyses, the Mackey Glass data requires t-1\\(t-2\\(t3\…\(t 40\\(t-41\\(t-42 and 22 centres while SantaFe data requires t-1\\(t-2\\(t3\…\(t 14\\(t-15\\(t-16  and 10 centres  The purpose of the experiments in this study is to compare the performance of adaptive and non-adaptive approaches in multiple steps ahead forecasting. Therefore both approaches are set to produce up to 25 steps ahead forecasting. The analysis bega n by setting the HRBF with the best input lags and centre obtained in the optimization analysis before. For adaptive learning, as the new data available, the HRBF was allowed to update its parameters At the same time it uses the updated parameters together with the recent input data to generate forecasting output Slightly different, for non-adap tive learning, the data were divided into two parts: 500 for training and another 500 for testing. The HRBF was trained using the training data repeatedly and the training is stop when it produces the optimum performance. The opti mized HRBF then was used to forecast the 500 testing data by using the parameters obtained during training and the recent input data   To examine the performance of adaptive and nonadaptive approaches, the MSE and R 2 measurements of the real and forecasted data from 501 to 900 \(400 data\ were conducted.  Table 1 shows the MSE and R 2 obtained by both approaches on Mackey Glass and Set A Data. It can be said that for both data, the performance of adaptive approach is comparable or superior to compare with non-adaptive approach for all steps ahead forecasting. Particularly in long distance forecasting \(10 steps ahead onwards\ the adaptive approach shows better performance. The MSE plots in Fig. 3 supports these observations, in which when the forecasting distance increases, significant different in MSE reading between adaptive and non-adaptive approaches for both data were recorded which MSE values obtained by adaptive approach are smaller  By taking 0.6 as the lowest acceptable R 2 value, we can conclude that both adaptive and non-adaptive approaches are able to perform good forecas ting for all 25 steps ahead for Mackey Glass data. However for Set A Data, both approaches can only perform up to 5 steps ahead forecasting Fig. 4 displays the actual and forecasted values for 1, 5, 10 15, 20 and 25 steps ahead forecasting on 500 Mackey Glass Data. Fig. 5 displays the actual and forecasted values for 1 and 5 steps ahead forecasting on 500 Set A Data from SantaFe Competition IV  C ONCLUSION  This paper compares the performance of adaptive learning and non-adaptive lear ning of HRBF neural network in multiple steps ahead time series forecasting. The analyses on two time series data: Mackey Glass and Set A Data from Santa-Fe competition show that the adaptive approach outperform the non-adaptive approach especially in long distance forecasting. Besides that, the continuous learning of the adaptive approach produces a forecaster which has the recent information on the system to be forecasted. This will make the adaptive forecaster be implemented in real live easily without the need to retraining    TABLE 1. THE MSE AND R 2 VALUES OBTAINED BY THE ADAPTIVE AND NON-ADAPTIVE APPROACHES FOR 1 TO 25 STEPS AHEAD FORECASTING OF MACKEY GLASS AND SET A DATA FROM SANTA FE COMPETITION  Data Forecasting step MSE R 2  Adaptive Non-adaptive Adaptive Non-adaptive Mackey Glass Data 1 step ahead 0.000676 0.000685 1.000 1.000 5 steps ahead 0.030327 0.030284 0.989 0.989 10 steps ahead 0.107158 0.111728 0.857 0.844 15 steps ahead 0.130875 0.154965 0.786 0.700 20 steps ahead 0.123475 0.158490 0.810 0.686 25 steps ahead 0.131619 0.159002 0.783 0.684 Set A Data  1 step ahead 21.47787 21.23511 0.745 0.751 5 steps ahead 24.74253 25.16814 0.662 0.650 10 steps ahead 29.10646 29.75616 0.532 0.511 15 steps ahead 30.09125 31.98429 0.500 0.435 20 steps ahead 33.96234 36.76090 0.363 0.253 25 steps ahead 35.18539 39.24122 0.316 0.149 
819 


                                     Figure 4. The actual and forecasted values of 500 Mackey Glass Data using adaptive an d non-adaptive approaches: \(a d forecasting \(b\ 5 steps ahead forecasting \(c sting \(d\teps ahead forecasting \(e steps ahead forecasting \(f\he ad forecasting Figure 3. MSE values obtained by Adaptive and Non-adaptive approaches on \(a\ Mack ey Glass Data \(b\m Santa-Fe Com petition a   b   c   d   e   f   a b 
820 


               a               b  Figure 5. The actual and forecasted values of 500 Set A Data using adaptive and non-adaptive approaches: \(a recasting \(b\ 5 steps ahead forecasting  R EFERENCES    E Montañés, J. R. Quevedo, M. M  Prieto, and C. O Menéndez  Forecasting Time Series Combining Machine Learning and Box-Jenkins Time Series   Advances in Artificial Intelligence  IBERAMIA  2002, vol. 2527, Jan 2002, pp. 491-499, doi 10.1007/3-540-36131-6_50   K. Lin, Q. Lin C. Zhou, and J. Yao  Time Series Prediction Based on Linear Regression and SVR  Proc. of the Third International Conference on Natural Computation, Nov 2007 pp. 688-691, doi: 10.1109/ICNC.2007.780   M. Boznar, M. Les jak, and P. Mlakar  A Neural NetworkBased Method for Short-Term Pr edictions of Ambient SO2 Concentrations in Highly Polluted Industrial Areas of Complex Terrain  Atmospheric Environment, vol. 27\(2 Jun 1993, pp. 221-230, do i: 10.1016/0957-1272\(93 S  S. M. Ch en, and N. Y. C hung  Forecasting Enrollments using High-Order Fuzzy T ime Series and Genetic Algorithms  International Journal of Intelligent Systems vol. 21\(5\, May 2006, pp. 485-501, doi: 10.1002/int.v21:5  Q.S. Li, D.K. Liu, J.Q. Fa ng, A.P. Jeary and C.K. Wong  Damping in buildings: its neural network model and AR model  Engineering Structures, vol 22, May 2000, pp 1216  1223, doi: 10.1016/S0141-0296\(99  H.H. Ngu y en and C.W. Ch an  Multiple neural networks for a long term time series forecast  Neural Computing Applications  vol 13,  April 2004, pp. 90  98, doi 10.1007/s00521-003-0390-z   K. Kalaitzakis, G.S. St avra kakis and E.M. Anagnostakis  Short-term load forecasting based on artificial neural networks parallel implementation  Electric Power System Research, vol 63, October 2002, pp. 185  196, doi 10.1016/S0378-7796\(02  Y  Mashor, “Modification of the RBF Network Architecture ASEAN Journal of Sciences Technol. Dev vol 17\(1  M.Y. Mashor A daptive Fuzzy C-Means Clustering Algorithm for A Radial Basis Function Network   International Journal System Sc iences, vol 32\(1\, 2001, pp 53-63  J. A. Karl and W. Bjorm Computer Controlled Systems Theory & Design, 3 rd edn. New Jersey : Prentice Hall 1997   C F Gao  T. L Ch en and T S  Nan    Discussion of Some Problems About Nonlinear Time-Series Prediction Using vSupport vector Machine  Communication Theory Physics vol 48, July 2007, pp. 117-124  K. R. Muller, A. J  Sm ola G. Ratsch, B. Scholkopf, and J Kohlmorgen  Using Support Vector Machines for Time series Prediction  in  Advances in kernel methods: support vector learning  MA, USA. MIT Press Cambridge, 1999  
821 


familiarize them with the Center and this particular research being done Invitation letters are a common component of any survey research project.  However, delivering them online via e-mail is a major difference between other methods and creates additional challenges.  An individual who receives an unwanted piece of printed mail can throw it out.  Similarly, unwanted e-mail  spam be deleted after its read, deleted prior to being read, or deleted ever before it reaches a user s inbox.  Thus it was essential that we take steps to ensure that members of our sample actually received our initial invitation e-mail and entice them to read it and choose to participate. We also sent all contact emails from an academic server \(.edu domain\ it would be less likely to be deleted by government spam filters than e-mails coming from a .com or .net address  4.5. Survey administration process  We were very cognizant of the importance of both thoroughly testing the instrument and the administration process prio r to releasing the full survey.  As such, the survey went through three complete stages from start to finish: pre-testing, pilot testing, and full administration.  These three stages took approximately seven months \(October 2007  April 2008\rom start to finish; this does not count the time spent on sampling, instrument design invitation letter design, and building the survey into the online tool  4.5.1. Pre-test The goal of our pre-testing process was to get early feedback about the first implementation of the web-based instrument  The survey was sent to 23 individuals to pre-test including 3 members of the research team, and we received comments back from 9 of them. We discussed the comments and made some revisions to the survey questionnaire  4.5.2. Pilot Pilot tests are primarily used to ensure that survey questions operate well but also that the instrument as a whole is functioning as design we also wanted to examine, using a small group of government employees, if the full administration process planned would be feasible with a large sample.   The survey was piloted using 80 contacts approximately 10% of our sample at the time of pilot administration.  After eliminating individuals whom we were unable to contact \(mainly due to missing or incorrect e-mail addresses, or because they left their position during the course of the pilot administration period\, we had 70 contacts remaining.  Five chose to opt-out between the time of sending out the initial survey link and sending the final reminder 5/70=7.1%\eople completed the survey in full for a response rate of 28.6%.  We also had 3 partial responses, so our rate for any responses was 32.9% or almost one-third.  This became our goal for the full administration.  Unfortunately our n from the pilot testing was too low to do any kind of statistical analysis, but we did use descriptive statistics to examine these results for basic trends. Again, our communication with public health and justice professional associations throughout the survey design and sampling processes was critical. This relationship helped us to refine our message to appeal to the government professionals we were surveying One of the major determinations made from this was that the Likert item questions were generally skewed towards the right \(i.e., the higher end\ On a strongly agree we consistently had means of 5 or 6.  We thought that the wording used for the scale itself might be causing a response bias that would make respondents answer favorably and so we modified it during the post-pilot tweak period to being 1 \(not at all\ 7 \(to a great extent\his decision allowed us to retain the wording on nearly all of the individual question items and still use a single measurement scale for the entire instrument.  Similarly, there seemed to be a selfselection bias, as respondents were mostly choosing initiatives that had succeeded, and this could also skew scale responses.  To remedy this, we revised some of the language in the survey instruction text to encourage participants to think about ALL kinds of initiatives in which they were involved and not only ones with positive outcomes  4.5.3. Full administration The full administration of the survey began by e-mailing invitations to 815 contacts, 361 individuals in criminal justice agencies and 454 individuals in public health agencies. It contained a description of the survey project and background about the previous research leading up to the survey.   Members of our sample were informed that the link to the survey itself would be mailed the following week and gave them the opportunity to opt-out if they so chose; similarly they could offer names of contacts who would replace them or suggest additional contacts to add. Thus this invitation e-mail served two purposes 1  Provide the opportunity for sampled individuals to opt-out of the survey prior to receiving the link Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 


2  Check for working e-mail addresses: if the initial e-mail message delivery failed, we searched for an updated address and resent the invitation A follow-up e-mail containing an individual s unique survey link was sent approximately one week later.  This ensured that a single survey would be tied to a single respondent.  Reminders were e-mailed to non-completers two weeks, four weeks, five weeks and six weeks after the survey link was first sent. In total, our final sample size was 617 government professionals. We had 71 opt-outs without replacements, for a rate of 11.5%.  We had 173 completed surveys, for a rate of 28.0%. The remaining 373 individuals did not complete the survey, for a 60.5% non-response rate; this also included four individuals who started the survey but did not finish/submit it for whatever reason We were also generally satisfied with the final representation concerning the sample make-up \(see Figure 1\omparing the respondents who completed the survey with the non-respondents, the percentages were very similar by policy domain; this was also true when we compared the respondents who completed the survey with the entire sample However, for both comparisons, we found that the percentages were very different by level of government.  Accordingly, we can say that our final results will be representative among our sample as a whole, but not necessarily for the entire population of public health and criminal justice government employees at the state and local levels   Sample Statistics by Policy Domain and Level of Government 0 20 40 60 80 100 Health-Local Health-State Justice-Local Justice-State Non-Respondents Completes Opt-Outs  Figure 1. Respondents by Policy Domain  4.6. Increasing survey response rate  A challenge with any survey is finding ways to increase the response rate as much as possible.  With the MIII survey, as with many other digital government projects, this was especially necessary given that we were essentially cold calling our potential respondents.  At every contact point beginning with the initial invitation letter, to the letter sent with the survey link, to the letter sent with each survey reminder, as well as to the instruction text for the survey itself our goals were to draw these individuals further into the survey process by convincing them not to opt-out and to have them move deeper into the instrument itself once they clicked on a survey link.  Ultimately we wanted them complete the survey and contribute to high response rates As part of our early survey planning, we had looked at published journal articles to see what other researchers were reporting by way of their response rates and survey techniques.  This allowed us to have our own internal target for what we hoped the response rate would be, for both the pilot and then the full administration.  We used four major steps prior to and during the administration of the full survey to maximize the number of completed responses 1  Pre-test After the initial survey instrument was created, a small group of University at Albany faculty, Center for Technology in Government staff members, and public health/criminal justice practitioners reviewed \(pre-tested\or content and clarity.  Recommendations from these participants were used to revise the instrument This included making edits to our wording for clarification purposes, breaking single questions into multiple questions, and changing response options 2  Pilot test Next, we engaged in pilot test administration. We selected 10% of individuals n=80\rom the contact list available in midOctober 2007 to develop our pilot sample.  This was done by sorting our contact list in alphabetical order by first name, then selecting every 10 th person from both the criminal justice and public health lists.  The administration process was designed to mimic what we planned to do for the full survey.  Invitations were sent out asking these individuals if they wished to participate.  The following week a survey link was e-mailed.  Reminders were sent at the start of weeks 3 and 4, then a final reminder the day before the survey was set to close.  The survey instrument was revised based on comments from respondents, as well as on results from basic survey analysis that was done. We also learned that reminders were very useful 3  Full administration This was begun by sending out 815 invitations asking sampled individuals to complete the survey.  The use of a survey invitation allowed us to eliminate members of the sample who wanted to opt-out and not complete the survey, as well as update our Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


contact list with either replacements \(if someone else from an agency would be completing it instead\mendations for new contacts We were also able to remove individuals we were unable to contact \(i.e., non-contacts 4  Reminders Finally, the use of reminders was key in increasing response rate.  Reminders were sent at the start of weeks 2, 4, and 5, then a final reminder sent two days before the official survey close date.  \(However we kept the survey open one extra week since a number of agencies were closed on the end date; this produced four additional completes.\After each point of contact with sampled individuals \(the initial survey link plus the four reminders\, we had a spike in the number of completed surveys on either the day of contact itself or on the following day \(see Figure 2   Number of Completes by Day 0 5 10 15 20 25 2  7 20 0 8 2  1 4  2 00 8 2  2 1  2 008 2  2 8 2008 3 6 2008 3/13/20 0 8 3  2 0/2 00 8 3  2 7  2 008  Figure 2. Effect of Reminders  4.7. Academic-Practitioner Collaboration  For our entire project, the partnership between academic researchers and government practitioners was critical.  Engaging practitioners at the early project stages, from design onwards, ensured that key pieces were not missed when developing the models or in designing the survey items and companion text Both parties have expertise and viewpoints that can be shared with each other and that each other can learn from  Buy in from top government practitioners in a certain domain is important for encouraging the participation of others, both in survey research and in research projects more generally.  As highlighted earlier in this paper, we spoke with the directors or heads of various national-level criminal justice and public health professional associations, some of whom agreed to pre-test the survey.  Our hope was to obtain names and contact information for their members \(to add to our sample list\nd to have them publicize the survey through their channels \(e.g mailing lists, national conferences, etc.\his also took away some of the cold calling aspect once the invitations went out, since many potential respondents did have some advance knowledge These organizational leaders played a role in legitimizing the survey.  If they did not feel that it was an effort worthy of their time, they would not have agreed to offer pre-test feedback or to share its existence with the members of their organization Therefore it was critical to obtain their support However, like with any collaborative endeavor, there were also some costs [6    5. Conclusions  As the area of digital government grows, it becomes incumbent upon academics and practitioners to work together to empirically examine this complex and multifaceted phenomenon.  Surveys are excellent ways to gather essential data for empirical analysis and Web-based surveys in particular are powerful alternatives to mail or telephone ones. However, in order to overcome the challenges of conducting Webbased surveys, researchers should consider the disadvantages of Web-based surveys mentioned above when designing and administering their own surveys. There are several general recommendations to maintain the quality of Web-based surveys in the literature \(see, for exam  2 4 all the target population of a survey should be analyzed carefully in order to assess their ability to access and participate in online surveys d th e desig n  of a Web survey should be respondent-friendly  This includes considering participants technological competencies and the congruence between the logic of how computers operate and respondents  expectations for completing survey T h ird, in  order to increase the response rate, personalized email cover letters, pre-notification to invite participating a survey, and follow-up reminders are importan a st b u t n o t least, to e n s u re confidentiality and privacy of respondents and to have less sampling error the survey should not be open to the general public and passwords and PIN numbers are highly suggested  The MIII project, although extensive and timeconsuming from start to finish, was an extremely worthwhile and unique initiative.  We can pave the way for future digital government studies through our demonstration of how Web-based surveys using an unbounded population of government employees can be conducted in this field of research.  Below, we offer a list of five lessons learned specifically from this digital government project Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


1\inate with professional organizations and government agencies This is one of our most important lessons learned. Given the growth of email communication, problems with spam and the resulting fact that government practitioners are less inclined to read \(let alone respond to\mail from unknown senders, being able to get support from professional associations and government agencies is critical. These organizations do not need to endorse the survey, as none of them did for our project However, if they feel the survey is relevant to their members or employees and that results will provide value to them as well, they are more willing to communicate the existence and purpose of your survey  2\ interdisciplinary and diverse research team Since digital government is an interdisciplinary field, it is extremely valuable to have a project staff with a wide variety of disciplinary backgrounds and skills.  Include individuals with strengths in research methods statistics, IT, project management, and writing for wide audiences.  Even if practitioners are not on the team itself, get their feedback starting at early stages of the research process was very positive.  Our work would not have been as successful as it was without having a varied mix of people involved  3\a flexible plan and timeline Although we created a timeline of work very early in the research process, doing Web-based surveys with government practitioners continually modified our initial timeline It is important to be aware of both internal and external factors that may cause you to need extra time. For our project, at certain steps we needed more time to tweak our survey instrument or handle feedback from practitioners.  Once we began building the instrument in the online tool, sometimes we hit technical difficulties.  During pilot testing we ran into both Thanksgiving and Christmas on the calendar which influenced our administration period Although we had hoped to start the full administration by early to mid-January 2008, it was not until the end of January/early February 2008 that we could begin 4\ Have patience and understand the government context Similarly, it s important to recognize that this is a long process.  It takes time to engage in new research that involves primary data collection with government practitioners.  Team members may come and go, or other projects may pull resources away Members of the sample may be in their position during one point of contact and gone to another government agency the next. Surveys can be a slow frustrating, and often tedious line of work; while the outcome will ideally be the production of a rich data set that can provide any number of contributions to both academic and practitioner audiences, it can be hard to see the light at the end of the tunnel while you re going through it 5\ Keep backups and have a contingency plan  This is not only a good practice for digital government research, but a good IT practice in general.  You never know when the important file you need is discovered to have a mistake, been deleted, or have gotten corrupted, especially with Web-based surveys.  Having backup versions for redundancy ensures that if something goes wrong, it can be fixed quickly.  Because we handled general administration tasks manually, instead of relying solely on Survey Gizmo s invitation manager, the project consisted of numerous data \(spreadsheet\files to keep track of contacts at each stage of the process If there was an error, backup files were key to sort out corrections In conclusion, the use of online surveys offers a powerful yet risky research mechanism. The ability to survey a large population of geographically politically \(i.e., multiple levels of government\d professionally diverse government professionals within a quick turnaround timeframe and with relatively little cost provides a lot of value. The risk comes in part from the ease and prevalence of the communication method: e-mail. While a Web-based survey may be easier to administer than a paper one the fact is that the response rate and quality of responses may be significantly lower than from a paper version. Government professionals, like many others, are inundated with e-mail communication; a significant amount of it which folks consider to be spam or generally a nuisance for other reasons This forces digital government researchers first to build a relationship with key members of the relevant government professional communities. This also requires making the case for your research and why anyone should open another unsolicited e-mail and participate in another survey they do not have time for.  Therefore, if possible an e-mailed survey link should not be the first time that the government professional learns about your organization and the research for which you re collecting. Overall, Webbased surveys are valuable for digital government researchers, but potential challenges should be considered carefully  6. Acknowledgments  This work was partially supported by the National Science Foundation under Grant No. ITR-0205152 Any opinions, findings, and conclusions or recommendations expressed in this material are those Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


of the authors and do not necessarily reflect the views of the National Science Foundation  7. References  1 Boy e r, K  K O l s o n, J  R   J a c k s on, E. C Electronic surveys: Advantages and disadvantages over traditional print survey  Decision Line 32\(4\, 2001, 4-7 2 B ry man A  Social Research Methods \(2 nd ed Oxford University Press, New York, 2004 3 Caf f r e y  L  Ed Information Sharing Between Within Governments Commonwealth Secretariat, London 1998 4 Curra ll S C Ju dg e  T  A  Measuring trust between organizational boundary role persons  Organization Behavior and Human Decision Processes, 64 2\95 151-170 5 D a w e s  S. S Interagency Information Sharing Expected Benefits, Manageable Risks  Journal of Policy Analysis and Management, 15 3\, 1996, 377-394  6 D a w e s  S. S H e lbig N   Building a ResearchPractice Partnership: Lessons from a Government IT Workforce Study Proceedings of the 40th Hawaii International Conference on System Sciences 2007 7 D a w e s  S. S  P a rd o, T  A  200 2  Building Collaborative Digital Government Systems: Systematic Constraints and Effective Practices In W. J. McIver & A K. Elmagarmid, \(Eds Advances in Digital Government Technology, Human Factors, and Policy pp. 259-273 Kluwer Academic Publishers, Norwell, MA, 2002  8 D a w e s  S. S  P r  f onta i ne   L  Understanding New Models of Collaboration for Delivering Government Services  Communications of the ACM, 46 1\03, 4042 9 Dillm a n D.A T o rtora R.D  Bow ke r, D P r inc i ple s  for constructing web surveys. Working paper. Retrieved August 20, 2008 from http://www.sesrc.wsu.edu/dillman/papers/websurveyppr.pd f 10 Eg le ne  O D a w e s  S. S  Sc hne ide r  C. A  Authority and Leadership Patterns in Public Sector Knowledge Networks  The American Review of Public Administration, 37 1\2007, 91-113  11 G il-G a rc a J  R P a r do T  A  Multi-Method Approaches to Digital Government Research: Value Lessons and Implementation Challenges Proceedings of the 39th Hawaii International Conference on System Sciences 2006. \(Creswell, J. W Research Design Qualitative, Quantitative, and Mixed Methods Approaches  SAGE Publications, Thousand Oaks, CA, 2003 12 G il-G a rc a J  R., Che ng a l urSm ith, I D u c h e s s i  P Collaborative E-Government: Impediments and Benefits of Information Sharing Projects in the Public Sector  European Journal of Information Systems, 16 2\, 2007 121-133 13 G il-G a rc a J  R., P a r do T  A  Burk e  G  B Government Leadership in Multi-Sector IT-Enabled Networks: Lessons from the Response to the West Nile Virus Outbreak Proceedings of the Leading the Future of the Public Sector: The Third Transatlantic Dialogue Workshop \(May 31 June 2, 2007\niversity of Delaware, Newark, Delaware, USA  14 G il-G a rc a J  R., Sc hne ide r   C P a rd o, T  A   Cresswell, A. M Interorganizational Information Integration in the Criminal Justice Enterprise: Preliminary Lessons from State and County Initiatives Proceedings of the 38th Hawaii International Conference on System Sciences 2005 15 G l a s e r B. G  Basics of Grounded Theory Analysis Emergence vs. Forcing Sociology Press, Mill Valley, CA 1992 16 G l a s e r B. G  Stra us s  A  L  Discovery of Grounded Theory: Strategies for Qualitative Research  Aldine Transaction, Chicago, IL, 1967 17 G r a n e llo, D  H   W h e a t on J  E  Online  Data Collection: Strategies for Research  Journal of Counseling Development  82 4\, 2004, 387-393 18 G un H  Web-based Surveys: Changing the Survey Process  First Monday 7\(2\, 2002 1 Jarven p aa S  L   Ives B   Executive involvement and participation in the management of information technology  MIS Quarterly, 15 2\991, 205-227 20 L a n f r e d C. W  The Paradox of Self-Management Individual and Group Autonomy in Work Groups  Journal of Organization Behavior, 21 5\, 2000, 563-585 21 L e f e v e r, S., D a l, M  Ma tt hia s d ttir, A Online Data Collection in Academic Research: Advantages and Limitations  British Journal of Educational Technology 38 4\, 2007, 574-582 22  P a r d o  T  A G ilG a rc  a J  R Burk e  G  B Sustainable Cross-Boundary Information Sharing In H Chen et al. \(Eds Digital Government: E-Government Research, Case Studies, and Implementation pp. 421-438 Springer, New York, 2008  23  Se g a rs A   G r ov e r V  Strategic Information Systems Planning and Success: An Investigation of the Construct and its Measurement  MIS Quarterly, 22 2 1998, 139-163 24 n n o n D  M J o hns on T  E Se a r c y S., L o tt, A  Using electronic surveys: Advice from survey professionals  Practical Assessment, Research Evaluation 8 \(1 25 us s  A  Cor bin  J    Basics of Qualitative Research. Techniques and Procedures for Developing Grounded Theory Sage Publications, Thousand Oaks, CA 1998 26 U m ba c h  P  D  Web surveys: Best practices  New Directions for Institutional Research 121, 2004, 23-38 27  V a n Se lm M J a nk ow s k i N  W  Conducting online surveys  Quality & Quantity 40\(3\, 2006, 435456  2 W r i g h t  K B   Researching Internet-based populations: Advantages and disadvantages of online survey research, online questionnaire authoring software packages, and web survey services  Journal of ComputerMediated Communication 10\(3\, 2005, 1-19  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


11 nI KK t nn nn nd nd nd nd X yz yz y y z z a      13 The heading estimate  can then be calculated geometrically using the relative displacement of the lateral components of the intersection line in the navigation frame x = East and y = North 14        min,max min,max,atan2  xx yy II II?  \(14 4. THEORETICAL PERFORMANCE USING SYNTHESIZED FEATURES Simulation Procedure This section will describe a simulation that was developed to assess the theoretical heading accuracy possible from ALS data and a perfect synthesized feature.  The first step of the simulation was to define the scanner parameters that reflect those likely to be used in a flight test: 30 lines per second, 10,000 pulses per second, and a  30 deg field of view.  ALS angles, times, and ranges were then simulated using these parameters.  A ground feature was synthesized by replacing the simulated ranges at the correct ground positions with those calculated according to the desired feature parameters \(i.e. plane equations synthesis and plane intersection procedure described in this section is summarized in the block diagram shown in Figure 8  Figure 8: Feature Synthesis and Plane Intersection Block Diagram Table 1: Simulation Parameters Parameter Value Aircraft Altitude 350, m Points in Surface 1 127 Points in Surface 2 248 Feature Length 25.79, m Feature Width 51.12, m The synthesized feature for this paper was based on airborne measurements of the size, location, and roof slope parameters of the AEC hangar at the Ohio University Airport.  For an aircraft at ~350m altitude in the simulation the synthesized hangar would be illuminated by 127 points 


on surface 1 and 248 points on surface 2 contained within a feature footprint with dimensions of 25.79 m long x 51.12 m wide as summarized in Table 1.  The flight profile from a January 2005 flight test [7] was used to establish the aircraft position and orientation over the two second flight duration needed to extract the simulated feature.  For the data collection phase, the hangar building was over flown while collecting GPS, INS, and ALS data.  In the synthesis phase actual GPS and INS information were used, but the ALS data was replaced with simulated data so that the ALS measurement errors could be controlled.  The heading derived from the measured ALS flight data will be compared with the simulation in the next section Two existing simulators were used to generate the ALS scan angle and ranges measurements [9].  An angle simulator was used to produce an array of angles and times that correspond to the laser scanner parameters.  A range simulator was used to produce range estimates by calculating the geometric difference between the ALS height and the terrain crossing \(simulated using a Digital Terrain Elevation Data \(DTED terrain crossing was found by increasing the range until it intersected the terrain and then iterated until the height accuracy was within some threshold \(1 ?m in this case The simulated range accuracy was determined from the height accuracy using the geometric relationship described in \(15 7 cos dHdR =  \(15 where dR = Range accuracy dH = Height accuracy ALS scan angle The top subplot of Figure 9 illustrates the range accuracy threshold used by the simulator \(determined from the height error and \(15 can be seen in the bottom subplot of Figure 9 because of the range iteration technique.  A 1 ?m height accuracy \(iteration threshold sufficiently small \(i.e. approximating true range influence the standard deviation of the final simulated range measurement \(i.e. true range + sensor noise better represent the capabilities of the true ALS, 25 mm of Additive White Gaussian Noise \(AWGN range measurement as shown in the top subplot of Figure 10 following with the Gaussian distribution function shown in the bottom subplot.  The DTED accuracy was not important for this analysis since only the range measurements to the simulated feature were required for the orientation calculations.  The range to the DTED merely provides terrain measurements for height contrast in the feature extraction algorithms  Figure 9: Range Error Distribution from the Simulator   Figure 10: Range Error + Noise Distribution with 25 mm Noise The feature was synthesized by calculating its height using the planar equations for surface 1 and surface 2 of the AEC hangar roof and the illuminated ground pulse positions as shown in \(16 2 22 11   BH BH   


 Surface Surface V V 16 where V = Vertical height H = Horizontal position vector   B = Plane parameters \(Surface 1 or Surface 2 Pulses that fall within a predetermined space \(represented by white space in Figure 11 points.  The DTED terrain heights at these positions were replaced with feature heights from a feature database prior to range simulation. The synthesized feature is shown in Figure 12  Figure 11: Synthesized Feature Footprint   Figure 12: Synthesized Feature Extraction Simulation Results for Typical System Performance Once a feature has been simulated, the feature extraction and plane intersection algorithms described in the previous section are used to determine the feature orientation.  The result of the least squares plane fit is shown in Figure 13 Notice that a gap is present near the ridgeline in Figure 13 This is due to the exclusion of the actual ridge point Because the peak of the roof contains a ridge cap, inclusion of these points in surface 1 or surface 2 would distort the results.  By using the plane intersection method, these few points \(16 hundred other points would remain in the plane.  The height residuals after subtracting the plane fit are shown in Figure 14.  As expected, these height residuals reflect the same accuracy  25 mm 1 shown previously in Figure 10.  The plane residuals shown in Figure 14 can be used to measure the range accuracy directly from the data.  The next section will describe how to use this information to predict the heading accuracy 8  Figure 13: Data Point Segregation and Plane Fit   Figure 14: Plane Fit Residuals The plane parameters, 1B  and 2B  are then used to find the equation of the intersection line as is shown in Figure 15 The intersection line contains heading information for comparison with some reference data.  In this case, the reference was derived from the same range simulation without noise  Figure 15: Plane / Plane Intersection Line Multiple repetitions of the synthesized feature extraction and plane intersection algorithm were used to determine the repeatable accuracy of the technique over 5000 Monte Carlo runs as shown by the block diagram in Figure 8.  The resulting 1? heading accuracy is shown in Figure 16 for  25 mm 1? ALS range accuracy  Figure 16: Heading Accuracy for 25 mm Range Error The error performance is summarized in Table 2.  The bias term is statistically zero \(as would be expected from the zero mean AWGN added to the simulated range heading error is better than a tenth of a degree Table 2: Heading Error Summary Error Type Magnitude Heading Error Mean 4.0778 x 10-4, deg 1? Heading Standard Deviation 0.0395, deg 0.69, mrad 


1? Heading Standard Deviation 0.0395, deg 0.69, mrad Sensitivity Analysis for Varying System Performance In order to understand how the heading error varies with parameters such as laser ranging accuracy, position accuracy, and heading bias, a sensitivity analysis was conducted.  For computational efficiency the number of repetitions at each parameter setting was reduced to 1000 For each set of 1000 repetitions of the plane intersection algorithm, the one parameter was linearly increased and its affect on the heading accuracy was assessed.  The range accuracy will be considered first, whereby the simulated range noise was increase in 46 steps from 0.001m until 0.31m.  The heading error is computed by comparing the noisy simulated feature intersection with a noise-free feature intersection.  Figure 17 shows the increase in heading standard deviation as the range standard deviation increases while Figure 18 shows the rate of change of the standard deviation per unit of range noise.  Although not shown here the heading resolution bias is zero-mean up to 15 cm and increases to almost 0.6 deg for 30 cm of range noise 9  Figure 17: Heading Error Standard Deviation  Figure 18: Heading Error Standard Deviation Growth Rate per Unit of Range Noise The results shown in Figure 17 illustrate a fairly linear increase in heading error growth for small range errors \(less than 12 cm rate of error growth is constant \(~1.53 deg/m range error.  The results shown in the previous section can be directly estimated using these figures.  If a range noise standard deviation of 25 mm is multiplied by a 1.53 deg/m error growth rate, the predicted heading accuracy would be 0.668 mrad.  This predicted accuracy is approximately the same as the accuracy found from simulation summarized in Table 2 The next parameter to be considered was a constant heading bias.  For the algorithm presented in this paper, the heading bias must be consistent with the position error as would be the case for an IMU operating in a dead reckoning mode Consequently, a heading error was simulated from 0 to 2 degrees in the inertial measurements and then the ENU position was corrupted using the erroneous heading in the direction cosines matrix.  Figure 19 illustrates the residual heading resolution error bias after the simulated heading bias has been removed.  Figure 20 illustrates the standard deviation of the heading resolution error  Figure 19: Heading Resolution Error Bias as a Function of Injected Heading Bias   Figure 20: Heading Resolution Error Standard Deviation as a Function of Injected Heading Bias Up to 2 deg of injected heading bias, there is not discernable effect on the heading resolution accuracy in either the mean or the standard deviation.  This illustrates that the algorithm can detect a constant heading bias without sensitivity to its magnitude The final parameter to be considered was position noise The position noise was varied linearly from 0 to 10 cm in 48 steps.  The response of the error mean to position noise is shown in Figure 21 while the response of the error standard deviation to position noise is shown in Figure 22 10  Figure 21: Heading Resolution Error Bias as a Function of Injected Position Noise   Figure 22: Heading Resolution Error Standard 


Figure 22: Heading Resolution Error Standard Deviation as a Function of Injected Position Noise As the injected position noise increases, the heading error bias remained nearly constant at the mm-level as would be expected since the injected position noise was AWGN Similarly, the heading error standard deviation increases as the injected position noise increases.  As with the laser range noise, the position noise will also determine the heading determination standard deviation.  Inertial positions are generally low noise \(better than 1 cm source is not felt to be as significant of a contributor \(&lt; 0.04 deg As an aside, it is interesting to note that this error will increase slightly as the aircraft height increases.  This is thought to be due to the reduced number of laser pulses that comprise each plane in the intersection equation.  For example, if the aircraft height increases from 350m to 400 m, the pulse count decreases to 68 pulses in surface 1 and 141 pulses in surface 2.  The heading accuracy will decrease by approximately 0.01 degrees 5. FLIGHT TEST PROOF OF CONCEPT  Figure 23: Flight Path over AEC Hangar The flight test data was collected in January 2005 and provided ALS measurements containing several features at the Ohio University Airport including the AEC hangar [7 The flight path over the hangar is shown in Figure 23.  The ALS settings during this flight test were more optimal for this application than for mapping since the gaps between scan lines was large compared to the gap between pulses in a line.  In September 2005, a static GPS survey was conducted \(relative to the KUNI GPS Continuously Operating Reference Station the AEC hangar.  The GPS survey provided an absolute reference to compare with the ALS measurements.  The lateral measurement accuracy of this survey was thought to be on the order of  20 cm because of antenna placement uncertainty Planes were fit to the ALS data from each side of the hangar roof as described previously as shown in Figure 24.  The plane fits are shown in lighter colors \(cyan and magenta than the measured data \(blue and red  Figure 24: Two Planes fit to the Data  Figure 24 is only intended to provide an overview since the plane fit residuals contain more information as shown in Figure 25.  The plane fit residuals indicate 9 cm of height accuracy \(1 11  Figure 25: Plane Fit Residuals The resulting plane intersection line was overlaid on the ALS measurements as shown in Figure 26.  The line was a close visual match, but when comparing the calculated line with the surveyed line, slight errors can be observed  Figure 26: Line Formed by Plane Intersection A closer examination of the two lines in the horizontal plane reveals their differences in the heading as shown in Figure 27  Figure 27: Intersection Line Comparison As mentioned previously, the measured plane-fit residuals provide an indication of the heading accuracy that could be expected from the real data if the survey was perfect.  Since the flight test residuals were within the linear region of the empirical error curves shown in Figure 17, the expected heading accuracy should be predictable.  Since the measured range accuracy is 9 cm \(from the plane-ft residuals heading accuracy is expected to be 0.1377 deg \(at 1.53 


deg/m to 2.4 mrad.  This accuracy is better than the typical heading alignment accuracy of a commercial navigation grade INS The actual heading angle of the two vectors is summarized in Table 3 Table 3: Hangar Ridge Vector Comparison Heading Survey 60.9436, deg Intersection 61.8772, deg Difference 0.9336, deg 16.29, mrad The importance of this proof of concept was to demonstrate the technique and to show that the accuracy can be predicted.  In this case there are several sources of uncertainty that might explain part of the performance degradation The reference survey introduced lateral antenna placement errors of  20cm.  The antenna placement uncertainty is expected to be the dominant error source in the measured data presented here.  Over a 25 m baseline, a  20 cm placement error will become a 0.916 deg pointing error worst case prediction using flight test data Further effort is needed to make a more accurate ground feature survey, but the concept of heading determination from ALS plane intersections has been demonstrated 6. CONCLUSIONS This paper presented results that leverage the accuracy and high number of \(i.e., oversampled measurements along with a priori surveyed features to determine airborne platform heading.  Plane fitting has the affect of averaging the oversampled measurements and can deliver mrad-level heading observations.  The ALS-derived heading information shown in this paper could be used to periodically calibrate tactical grade IMU heading biases while in flight, to perform an ALS / IMU calibration prior to a terrain aided landing, or simply to stabilize the inertial heading measurements when GPS is unavailable.  A simulation was presented to show the theoretical performance of heading determination from plane intersections using typical ALS parameters.  With a perfect survey, the heading accuracy was better than 1 mrad.  Using the same simulation software, a theoretical sensitivity analysis was conducted to determine the effect of range noise, heading bias, and position noise on the heading accuracy.  The heading standard deviation was shown to be a function of the range measurement and position standard deviations.  Inertial positions are generally low noise, so this error source was not deemed to be as significant of a contributor as the range noise.  The heading error magnitude did not change the performance in a noticeable way. This theoretical performance assessment was then compared with 12 flight test data.  While the flight test performance was not as good as expected, the error sources were justified.  Even with large survey uncertainties, the heading measurement performance was still within 1 deg.  If a better survey was available, the flight test results were expected to be accurate to within a few mrad Operationally, there are many things than can be done to improve these results.  This paper discussed flight over a single building.  Accuracy can potentially be improved by flying over multiple buildings of larger sizes and at lower altitudes or other types of features.  Additionally, the heading error growth rates could be predicted by tracking the heading change while flying over a building or between two buildings.  With careful attention to calibration and measurement accuracy, these heading alignments can potentially be made at a higher accuracy than a navigation grade INS can align its heading REFERENCES 1] Schenk, T  Modeling and recovering systematic 


1] Schenk, T  Modeling and recovering systematic errors in airborne laser scanners  Proceedings of the OEEPE workshop on Airborne Laserscanning and Interferometric SAR for Detailed Digital Elevation Models, OEEPE Publication no. 40, 2001, pp. 40-48  2] Crombaghs, M.J.E., Brugelmann, R., and d Min, E.J  On the Adjustment of Overlapping Strips of Laseraltimeter Height Data  International Archives of Photogrammetry and Remote Sensing, 2000, 33 B3/1   3] Dickman, J., and Uijt de Haag, M  Aircraft Heading Measurement Potential from an Airborne Laser Scanner Using Edge Extraction  Proceedings of the IEEE Aerospace Conference, March 3-10, 2007  4] Kraus, K., Pfeifer, N  Determination of Terrain Models in Wooded Areas with Airborne Laser Scanner Data  ISPRS Journal of Photogrammetry and Remote Sensing, 1998, 53, pp. 193-203  5] Maas, H., Vosselman, G  Two Algorithms for Extracting Building Models from Raw Laser Altimetry Data  ISPRS Journal of Photogrammetry and Remote Sensing, 1999, 54, pp. 193-203  6] Venable, D., Campbell, J., and Uijt de Haag, M  Feature Extraction and Separation in Airborne Laser Scanner Terrain Integrity Monitors  Digital Avionics Systems Conference, 2005  7] Campbell, J. L., M. Uijt de Haag, van Graas, F  Terrain Referenced Precision Approach Guidance   Proceedings of the ION National Technical Meeting 2005, San Diego, CA, January 24-26, 2005, pp. 643653  8] http://www.geom.umn.edu/software/download/COPYI NG.html, accessed May 2007  9] V. Nguyen, et. al  A Comparison of Line Extraction Algorithms Using 2D Laser Rangefinder for Indoor Mobile Robotics  Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, Aug. 2-6, 2005, pp. 1929-1934  10] Vadlamani, A  Preliminary Design and Analysis of a LIDAR Based Obstacle Detection System   Proceedings of the 24th Digital Avionics Systems Conference  13 BIOGRAPHY Jeff Dickman graduated from Ohio University in 2008 with a Ph.D in Electrical Engineering His research emphasizes navigation system integration and sensor stabilization.  He has also been involved with GPS landing system research and antenna design and measurement.  He is presently working on vision-aided navigation systems.  He is a member of the IEEE ION, AIAA, Eta Kappa Nu, and Tau Beta Pi  Maarten Uijt de Haag is an Associate Professor of Electrical Engineering at Ohio University and a Principal Investigator with the Ohio University 


Investigator with the Ohio University Avionics Engineering Center.  He earned his Ph.D. from Ohio University and holds a B.S.E.E. and M.S.E.E from Delft University of Technology located in the Netherlands.  He has been involved with GPS landing systems  research, advanced signal processing techniques for GPS receivers, GPS/INS integrated systems, and terrain-referenced navigation systems.  The latter includes the development of terrain data base integrity monitors as an enabling technology for Synthetic Vision Systems and autonomous aircraft operation      pre></body></html 


           7 C ur re nt ly n ot w or ki ng 2  0 0 5  0 0 8 0 04 0 04 0 0 1 0 16  0 16             8 C ur re nt ly in e du ca tio n2   0 6 


6 7  0 01 0 1 9 0 08  0 03 0 6 8 0 0 7 0 3 2           9 C ur re nt ly w or ki ng 2  0 2 8  0 03 0 18  0 1 1 0 0 3 0 64  0 00 0 1 4 0 8 9   


        10 E du ca tio n ac hi ev ed 3  3 5 7 1 5 2  0 04 0 02 0 2 1 0 1 2 0 16  0 02 0 1 6 0 13  0 0 6         11 D is pe ns ab le in co m e   


  21 0 9 2 72 7  0 14  0 0 1 0 09  0 08  0 2 0 0 00 0 0 4 0 18  0 1 6 0 0 1        In te rn et u sa ge                     


  12 A ct iv e in te rn et u sa ge 1  0 0 2 0 9 6 0 2 1 0 25  0 11  0 12  0 10  0 0 4 0 05  0 0 8 0 0 5 0 0 1 0 12        13 H ou rs o nl in e h ou rs 


rs   2 6 5 3 0 3  0 04 0 12  0 1 1 0 0 3 0 40  0 0 7 0 0 7 0 4 7 0 5 3 0 07  0 1 1 0 07       14 W illi ng ne ss to p ay 1  1 8 3 0 6 3  0 03 0 10 


10  0 07  0 08  0 0 2 0 0 4 0 0 1 0 01  0 00 0 0 5 0 14  0 04 0 05      G am e sp ec ifi c va ria bl es                      15 T en 


ur e w ee ks   2 8 2 3 5 2 0 2 6 0 31  0 0 9 0 01 0 12  0 0 4 0 02 0 0 9 0 0 9 0 07  0 02 0 13  0 08  0 0 4    16 C ro ss o ve r on o ffl in e 4  0 1 5 


5 1 1 1 0 1 9 0 11  0 13  0 18  0 2 0 0 1 4 0 0 7 0 14  0 1 1 0 0 4 0 08  0 15  0 0 5 0 01 0 07    17 S at is fa ct io n1   18 7 5 1 3 16  0 18  0 00 


00 0 44  0 52  0 1 4 0 0 3 0 02 0 07  0 0 9 0 1 4 0 10  0 08  0 0 6 0 09  0 0 1 0 13   18 C om m itm en t1  0 6 2 0 8 3 0 3 1 0 13  0 37  0 39  0 0 7 


7 0 0 6 0 02 0 03  0 0 4 0 1 3 0 14  0 17  0 0 5 0 09  0 07  0 19  0 58  S ou rc e O w n ca lc ul at io n N ot e N  1 3 89 o bs er va tio ns S ig ni fic an ce le ve ls 


ls  p  0 05 S D  S ta nd ar d de vi at io n 1 5 po in t L ik er t s ca le ra ng in g fro m 2 to 2  2 du m m y va ria bl e 3 o rd in al v ar ia bl e ra ng in g fro m v oc at io na l e du ca 


tio n to P h D 4 n um be r o f c on ta ct s   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


