Hybrid Detector based Nega tive Selection Algorithm  Bin Xu Department of Computer Science & Engineering Nanjing University of Aeronautics & Astronautics Nanjing, China Xb_xxxt@yahoo.com.cn Yi Zhuang Department of Computer Science & Engineering Nanjing University of Aeronautics & Astronautics Nanjing, China Zhuangyi@263.net  Abstract 227A new algorithm of detector generation for negative selection algorithm is introduced by adding a big detector to reach a high coverage of non-self space. While the big detector can be variable in different shape using this concept, the paper 
puts forward an algorithm using a ring-hyper-sphere shaped detector as a big detector. The algorithm is tested using different self set\(or training set\d real-word dataset. Preliminary results demonstrate that the new approach enhances the negative selection algorithm in efficiency and reliability without significant increase in complexity Keywords-negative selection; anomaly detection; one calss calssification; sensor network security I   I NTRODUCTION  Along with the better understanding of nature and society the main research object has been changed from linear system into a complex nonlinear system. Soft computing is a general 
term for a class of nonlinear methods, including fuzzy logical control, neural network, genetic algorithms and artificial immune system\(AIS et c A I S ha s suc c e ssful l y a pp l i e d t o anomaly detectio  f a ul t  d i a g n o si s[3 op t i m a l  combination[5  etc   M o s t ex itin g A I S alg o r i t h m s i m itate on e  of the following mechanisms of the immune system: negative selection, clonal selection, immune network or danger theory The inspiration of negative selection-based algorithms come from the immune cell maturation process without the participation of non-selves in thymus and bone marrow, thus these algorithms have the feature of one class classification The main research topics of Negative Selection-based 
Algorithms\(NSAs\ are focused on the following aspects: the presentation of self and detector, the detector shape, and the matching rule, etc.. Almost all of NSAs use binary presentation in early studies because of its simplicity of analysing and using suc h  a s nat i v e N S A  6    l i n ea r d e t e c t or ge n e r a t i ng algorithm gre e d y d e t e c t or ge n e r a t i ng a l gor i t hm  7  and non-hole detector-generating algorithm  e t c   T h e  st ud y  o f  real valued NSAs are promoted because the real-world problems are often more difficult to expressed in binary representation. Instead of binary string, selves and detectors are represented in multi-dimensional vectors, the affinity may 
reflected by the distance of vector points in vector space. In addition, the detector shape is more flexible. There exist much work about real valued NSAs like constant-sized detectorgenerating algorithm var i ab l e si z e d  d e t e c t or ge n e r a t i ng algorithm hy p e r  e l l i p s o i d de t e c t or ge ne r a t i ng algorithm[10  et c C o m p are d t o  oth er NSA s v a ri ab les i ze d  NSA has a balance in detection rate and false alarm rate and has a wide range of applications. A hybrid detector based negative selection algorithm is proposed in this paper by introducing a big detector which is a ring-hyper-sphere shaped The experimental results demonstrate that the algorithm enhances the performance of variable-sized NSA with fewer 
detectors and comparable detection and false alarm rate, and improves the detection efficiency and reduces the detection of energy consumption II  A LGORITHM AND A NALYSIS   Definition 1 Self Set SE Non-self Set NS ppose that the problem space is T  0,1 0,1        nnnn TRC 
002\002 1 where P is a set of representation, also known as a coding 
002 Self Set is defined in \(1       SE p s p P s S 
set S is a set of shape. The self space is defined in \(2          SE S x x f ps ps SE 
002 002 2 where  f SE T 
003 represents the space covered by self sample p,s in problem space. The non-self space is defined by   NS SE SxxTS   SE NS SS 
 002 212 3 SE NS SST 
004 thus the non-self set is defined by 
   NS NS x x S 978-1-4244-3693-4/09/$25.00 \2512009 IEEE 
 002 002 002 5 where M is a set of matching rules. The detector space is defined in \(6       D Sxxgpsm 
 002 6 
 002 4 Definition 2 Detector Set D Detector Set is defined in \(5         D psm p Ps Sm M 
 


where  g DT  which represents the covered region in problem space decided by a detector’s position, shape and matching rule Definition 3 Coverage C The coverage is defined by 12   ChSS  7 0,1 C  where  0,1 hT T R  which represents the percentage of the number of the elements in 1 S 2 S in that in 2 S set. It is also recorded as 12  CSS  Thus the coverage of detector set  DDNS CSS   Definition 4 Big detector det B ector  is defined by det     B ector p s m   8 det B ector D    det B ector SE SS   and det max   SE B ector SS   where max function denotes maximum of the coverage A hybrid detector based negative selection algorithm\(HDNSA\ is put forward by introducing the det B ector  In HDNSA, the T is n R Self and detectors are defined in \(9       n ss SE s r s R r R    9 where s is the center of a self sample s r is the radius measured by Euclidean distance. A self sample is a hypersphere shape with center of s and radius of s r in n R space  det  D BectorD  10 det      bl B ector d r r    11 where   n lb dRrr R  is a matching rule which represents the vectors lie in the hyper-sphere region  with the center of d and radius of l r or lie out of hyper-sphere region with the same center and the different radius of b r are anomaly         n D dr d R r R     12 where  is a matching rule which represents the vectors lie in the hyper-sphere shape are abnormal points HD-NSA consists of two phases: detector generating phase and detection phase. The detector generating phase, which is the core of HD-NSA, mainly includes three parts: the generation of det B ector  the prediction of the coverage of detector set and the generation of variable-sized detectors One of the important feature of HD-NSA is the det B ector  which is actually a ring-hyper-sphere shape. The generation of big detector is described in steps 1 through 4 of Fig. 2. As is shown in Fig. 2, first, the algorithm generates the hyper-sphere\(HS1\in which all self sample be wrapped with the maximal coverage 1  SE HS SS Obviously, the coverage is high when the distribution of self sample is compact or concentrate in the central of vector space. The HS1 space will evolve into the entire space in the worst case. Then, the algorithm finds another hyper-sphere\(HS2\o cover the nonself space in HS1. This step ensure det B ector  formed by HS1 and HS2 has a high coverage det  SE B ector SS  when self samples are distributed evenly in the edge of entire space  Figure 1  Monte Carlo Coverage Assessment Algorithm Another important feature of HD-NSA is the using of Monte Carlo method[11  t o p r e d i c t t h e c o vera ge o f  de t e c t or se t  in real time. The method is described in Fig. 1. Using the Chebyshev’s inequality and specifying a confidence level 1  one can determine the sample size N  2 1  4 Inf   1 th a t  guarantees an error no larger than   2 1  4 Inf   rounds the elements of 2 1 4  to the nearest integers towards infinity. As is described in step 6 of Fig. 2, the algorithm predict the coverage of detector set when there is a det B ector  or the number of detectors is multiples of cp the artificial threshold to ensure the efficiency of detector generation The last part of this algorithm in step 7 of Fig. 2 shows variable-sized detectors are generated randomly in B-detector’s ring-hyper-sphere-shaped region to cover the non-self space and holes further more. Because of the reduced search space this process, compared to the original variable-sized detector generation algorithm, has faster convergence speed This algorithm normally converges in one of the two ways Type 1 convergence is when the estimated coverage is reached in step 6 of Fig. 2 where shows the B-detector’s power in controlling detector number. Type 2 convergence is when the limit of detector number is reached in step 5.The computation MCCA A Monte Carlo Coverage Assessmen t Algorith m input D set of Detectors  absolute error of the estimated coverage  confidence level output C coverage of the set of D  Step 1. randomly generating N points x from n SE R S   where N is the worst-case sample size 2 1  4  NInf    Step 2. calculating the value of inside which is the number of points that are in the D s coverage space with the corresponding matching rule when 1\d s matching rule is * and dist\(d.d, x  b r d  or dist\(d.d, x  l r d   d s matching rule is  and dist\(d.d, x    d.r where d D and dist\(d.d,x\ is the the Euclidean distance between vector d.d  and x the point x is covered by D so 1 inside inside   Step 3  CinsideN  return C 
 


complexity of hybrid detector generating algorithm is O\(\(|D|+1\|SE|\, where |D| and |SE| are the number of detector and self sample data respectively. It is easy to see that the complexity is further improved when |D| is reduced. Similarly the complexity of detection algorithm and the space complexity is also decided by number of detectors. It should be noted that the B-detector is ring-hyper-sphere-shaped in this algorithm while in another cases, it can be ring-hyper-polyhedron-shaped or ring-hyper-ellipsoid-shaped and so on. Assume there are m candidate shapes of big detector, the complexity of detector generation is O\(\(|D|-1+2m\|SE  Figure 2  Hybrid Detector Generating Algorithm III  E XPERIMENTS AND R ESULTS  A  the algorithm’s basic propert y on different self region shape A synthetic 2-dimensional datasets  a r e used  t o  demonstrate the properties of this algorithm. Figure 3 shows a intersection-shaped self region and its complementary shaped self region composed by the blue points with its self radius over entire space 2 0,1   Th e y e ll ow s h aded a r e a in Fig  3  shows the coverage achieved by hybrid detector set generated using this algorithm. Two red circles composed the ring-hypersphere shaped region in 2-D space. Comparing \(a\nd \(b\ is easy to see the effect of self radius on the results. The smaller self radius would result in high detection rate but high false alarm rate too, so it is suitable for the scenario when detecting all or most anomalies is very important. On the other hand larger self radius would result low detection rate and low false alarm rate, thus suitable when we need to try best to avoid false alarm. Comparing \(c\and \(d\, it is clear that when self samples suffuse the entire space, the B-detector covers the entire space There is no detector generated when the self  space is the entire space by set self radius to 0.1. The  and 1   are set to 0.1 and 0.9 respectively  Figure 3  Intersection shaped space and its complementary shaped space  Figure 4  a\etection rate and false alarm rate \(b\mber of detectors Fig. 4 shows the complete trend of self radius’ effect on the results for self radius from 0.01 up to 0.2. The results using two different values of estimate coverage, 96% and 99%, are presented together to show that parameter’s influence. All the results shown in this figure are of 100 repeated experiments Detection rate and false alarm rate[6 e d e fi ned a s  DR=TP/\(TP+FN\, FA=FP/\(FP+TN\, respectively B  Comparison with Similar Methods on Real-World Data To study the property and possible advantages of hybrid detector based negative selection algorithm, experiments were also carried out to compare with the results obtained using other anomaly detection methods that only use normal data to train as this algorithm. Two such AIS methods are A \(constantsized detector-generating algorithm[10 a nd B  var i a b l e si zed detector-generating algorithm[6 H e re w e c a l l t h e a l g o ri t h m put forward in this paper C for convenience Table I shows the comparison using the famous benchmark Fisher’s Iris Data\(self radius 0.1, estimated coverage 99 The results shown are the summary of 100 repeated tests for     a\            \(b\                  \(c\            \(d c\elf radius = 0.01              \(b\ \(d\  self radius = 0.1   a    b   HDGA, Hybri d De t ector Genera t ing Algorith m  input SE set of self samples N the max number of detectors o C estimated coverage s r radius of self point in SE;cp check point of the number of D set    the same meanings as in MCCAA function output D set of hybrid detectors Step 1. set the D to  randomly sampling two self samples i s e and j s e  from SE and then generating the initial hypersphere d of det B ector  as the following equations  2 ij d d se se     2 bijisjs d r dist se se se r se r   Step 2. for all other s e in SE if the Euclidean distance between  dd and  s es is greater than  b dr then adjusting the d as the following equations    2 bs dd dd dr ses ser     2 bb s dr dr dist dd ses ser    Step 3. calculating the minimum distance  l dr between  dd  and s e in SE if  l dr is less than  s s er then set it to zero Step 4. adding the det B ector    bl dd dr dr to D  Step 5. if the number of detectors in D set is greater than N  return D otherwise go to the next step Step 6. if there is a det B ector  in D set or  0 Dcp   the function MCCAA  D  lled to compute the coverage, if MCCAA  D   o C then return D  otherwise go to the next step Step 7. randomly generating the variable-sized detectors   xr  in det Bector S  and adding them to D go to 5 
 


each method and parameter setting. One of the three types of iris is considered as normal data, while the other two are considered abnormal. The normal data are either completely or partially used to train the system. Although the partial training set may seems small in this case, it is necessary to demonstrate the system’s capability to recognize unknown normal data. The detector size in algorithm A is set to 0.1 as self radius. The problem space is 4-dimensional as Iris data’s is 4-dimensional so the distance is defined in 4-dimensional space. The maximum detector is set to be 1000. The  and 1   are set to 0.1 and 0.9 respectively. Table I shows algorithm C has comparable detection rate and false alarm rate but small numbers of detectors in most cases, especially when fewer training data were used. The self radius and estimated coverage parameters can be used to balance between high detection rate and low false alarm rate  Figure 5  a\etection rate and false alarm rate \(b\mber of detectors TABLE I  C OMPARISON BETWEEN THREE DIFFERENT METHODS  Training Data Algorithm Detection Rate False Alarm Rate Number of Detectors  Mean SD Mean SD Mean SD Setosa 100 A 42.08 10.51 0.00 0.00 1000 0 B 100.00 0.00 0.00 0.00 1000 0 C 100.00 0.00 0.00 0.00 436 72 Setosa 50 A 34.17 3.69 0.00 0.00 1000 0 B 100.00 0.00 0.00 0.00 1000 0 C 100.00 0.00 0.00 0.00 221 40 Vesicolor 100 A 37.26 8.64 0.00 0.00 1000 0 B 74.15 8.37 0.00 0.00 1000 0 C 83.05 1.56 0.00 0.00 1000 0 Vesicolor 50 A 34.20 6.82 0.00 0.00 1000 0 B 83.85 6.48 0.50 0.87 1000 0 C 87.25 1.09 0.10 0.44 770 135 Virginica 100 A 39.60 7.63 0.00 0.00 1000 0 B 85.30 1.23 0.00 0.00 1000 0 C 82.50 3.09 0.00 0.00 1000 0 Virginica 50 A 33.41 7.62 0.00 0.00 1000 0 B 86.05 1.02 1.90 0.44 1000 0 C 81.35 2.99 2.00 0.50 1000 0 Fig. 5 shows the balance over a whole range of self radius using Vesicolor 50% for training when estimated coverage and self radius are set to 96% and 0.1 respectively. The results show that algorithm C’s performance is better than B if we consider detection rate, false alarm issue and especially the number of detectors. It further confirms algorithm C’s advantage in balancing the goals IV  C ONCLUSION  The paper proposed an extension of negative selection algorithm with a hybrid detector generation scheme Experimental results demonstrated that this algorithm is more effective in using smaller number of detectors because of the big detector. The following are some advantages of this algorithm: 1\The number of detectors is reduced by B-detector to cover a large space when self distribution is compact 2\Time to generate detectors is saved by reduce the searching space by B-detector. 3\Keeping the high coverage of non-self space and holes  by conserving the variable sized detectors when needed. 4\The coverage estimate for prediction is useful when the self sample is few but sufficient A CKNOWLEDGMENT  The authors gratefully acknowledge Department of Computer Science & Engineering of Nanjing University of Aeronautics & Astronautics for supporting the  AIS research that produced this paper here. Support was provided by National Aviation Science Fundation \(Grant No.  05F2037\ and the National Defense Foundation of China\(Grant No.  Q07 2006C 002-1 R EFERENCES  1  E. Hart and J. Timmis, Application Areas of AIS: The Past, Present and the Future. Journal of Applied Soft Computing 8\(1\. 2008 2  Zhengbing, Hu, Ji, Zhou, etc., A novel anomaly detection algorithm based on real-valued Negative Selection system. Proceedings - 1st International Workshop on Knowledge Discovery and Data Mining WKDD, p 499-502, 2008 3  R. de Lemos, J. Timmis, S. Forrest and M. Ayara, Immune-Inspired Adaptable Error Detection for Automated Teller Machines. IEEE SMC Part C: Applications and Reviews. 37\(5\.pp. 873-886, 2007 4  Luo, Wen-Jian, Zhang, Yi-Guo, Wang, Xin, Wang, Xu-Fa, Experimental analyses of evolutionary negative selection algorithm for function optimization, Journal of Harbin Engineering University, v 27, n SUPPL p 158-163, July, 2006 5  Ji, Z., Dasgupta, D, Real-valued negative selection algorithm with variable size detectors. In: Deb, K., et al. \(eds.\ GECCO 2004. LNCS vol. 3102, pp. 287–298, Springer, Heidelberg, 2004 6  Forrest, S., A. Perelson, L. Allen, and R. Cherukuri, ‘Self-nonself discrimination in a computer’. In: Proc. IEEE Symp. on Research in Security and Privacy. pp. 202–212, 1994 7  D'haeseleer, Forrest, Helman, An immunological approach to change detection: Algorithms, analysis, and implications. In: Proc of the 1996 IEEE Symp on Computer Security and Privacy, Los Alamitos, CA IEEE Computer Society Press, 110-119, 1996 8  HE Shen,LUO Wen-Jian,WANG Xu-Fa, A Negative Selection Algorithm with the Variable Length Detector,Journal of Software,vol 18, Nol 6, pp.1361-1368, June 2007 9  Gonzalez, F.D. Dasgupta, Anomaly Detection Using Real-Valued Negative Selection, Genetic Programming and Evolvable Machine, vol 4. pp. 383-403, 2003   J. M. Shapiro, G. B. Lamont, and G. L. Peterson, An evolutionary algorithm to generate hyper-ellipsoid detectors for negative selection GECCO 2005, volume 1, pages 337–344,Washington DC, USA, 25-29 June, 2005, ACM Press   Thomas Stibor, Jonathan Timmis, Claudia Eckert, On the Use of Hyperspheres in Artificial Immune Systems as Antibody Recognition Regions. ICARIS-2006, Oeiras, Portugal Lecture Notes in Computer Science, Vol.4163, pp.215-228, Springer-Verlag, 2006   Fishman, G.S.: Monte Carlo Concepts, Algorithms, and Applications Springer, 1995   V-detector: a one-class classification algorithm http://www.zhouji.net/prof/vdetector.html   a    b   
 


   u v v i v V u i u u v v V Sim r r P r Sim       2  Where V represents the set of k similar users. While calculating prediction only those users in set V who have rated item i are considered  3.2 Item-based collaborative filtering  In item-based collaborative filtering [7 similarities between the various items are computed From the set of items rated by the target user, k items most similar to the target item are selected. For computing the prediction for the target item weighted average is taken of the target user?s ratings on the k similar items earlier selected. Weightage used is the similarity coefficient value between the target item and target user items To compute item-tem similarity we used adjusted cosine similarity. Let the set of users who rated both items i and j be denoted by U, then similarity coefficient \( ,i jSim    2 2      u i u u j u u U i j u i u u j u u U u U r r r r Sim r r r r          3  Here ,u ir  denotes the rating of user u for item i and  ur  is the average rating given by user u calculated over all items rated by u. Similarly, ,u jr denotes the rating of user u for item j To compute the predicted rating for a target item i for target user u, we use the following formula      


  i j u j j I u i i j j I Sim r P Sim      4  In equation 4, I represent the set of k most similar items to target item i that have already been rated by the target user u. As earlier mentioned, ,u jr denotes the rating of user u for item j  3.3  Prediction shift  For the purpose of measuring the effectiveness of the attack we use the widely used metric called prediction shift. Prediction shift of a target item is the difference of average predicted rate of the target item after and before the attack, for all target users Average Prediction shift of an attack is the average Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 3 change in prediction for all target items. We use the same formula as in [3], which is defined as follows  Let U and I be the sets of target users and target items. Let ,u i?  denote the prediction shift for user u on item i. ,u i?  can be measured as , , ,'u i u i u ip p where ,'u ip  is the prediction value after the attack and  ,u ip before the attack. The average prediction shift for an item i  over all users can be computed as     u i u U i U    5  The prediction shift for an attack model is the average prediction shift for all items tested. It can be computed as   i i I I     6  4. Target item classification  Our attack strategies are dependent on target item rating distribution. Target item is categorized into TL or TH category on the basis of the target item rating distribution. In a scale of 1 to 5, higher scale implies rating of 4 or 5, lower scale implies a rating 


implies rating of 4 or 5, lower scale implies a rating of 1 or 2 TL: Target item with majority of their ratings at lower end of the rating scale fall into this category. In our experiment we grouped items with 60 % or more of their ratings as 1 or 2 in this category TH: Target item with majority of their ratings at higher end of the rating scale fall into this category In our experiment we grouped items with 60 % or more of their ratings as 4 or 5 in this category Once category of the target item and the recommendation system algorithm to be attacked are known, an appropriate strategy based on filler items is then used to construct attack profiles  5. Filler item strategies for user-based collaborative filtering systems  Known attack models like average attack, bandwagon attack, and segmented attack are focused at constructing attack profiles that will result in creation of malicious users that have greater chance of having high similarity with as many genuine users as possible. Because a malicious user with high similarity with a genuine user increases the chance of it being selected in top k similar neighbors of the genuine user, thereby influencing the rating of the target item. Bandwagon attack and segmented attack have tried to achieve this objective by selecting popular items as part of their attack profiles. An attack profile consisting of the popular items will have similarity with higher number of users, which should finally result in an effective attack. However it has been found that average attack is more effective compared to bandwagon and segmented attack when effectiveness is measured across all users [3 Segment attack performs better than average attack only in in-segment attacks. One possible reason for this unexpected result could be the presence of other factors which also affect the effectiveness of an attack Our proposed approach for user-based CF systems considers rating distribution of the target item as a critical factor that can affect the effectiveness of an attack. Unlike previous attack models which focus on creating malicious users that are similar to as many genuine users as possible from the set of all users; the objective of our approach is to create malicious users that increase similarity with as many of those genuine users who have rated the target item. Our approach proposes two strategies that are based on the rating distribution of the target item. Both strategies improve similarity by assigning appropriate values to filler items. As we show below, our proposed approach performs substantially better than best available attack model i.e., average attack model. We define below the two strategies  5.1 Strategy UL  This strategy is followed when the target item falls in TL category. As majority of the users have rated the target item at the lower end of the rating scale, to improve effectiveness of the attack, we need to create malicious users that are similar to the genuine users who have rated the target item with a lower value So, to improve similarity, a randomly selected filler item is assigned the average rating given to the filler item by those users who have rated the target item at the lower scale Let UA be the set of all users who have rated the 


Let UA be the set of all users who have rated the randomly selected filler item IF. Let UL be the set of all users who have rated the target item at a lower scale of rating and have also rated the randomly selected filler item. So, in this strategy, filler item IF is assigned the average rating given to it by the set of Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 4 users UL. This approach differs from average attack in the way filler items are assigned values, as in an average attack item IF was assigned the average rating given to it by the set of users UA  5.2  Strategy UH  This strategy is followed when the target item falls in TH category. As a large majority of the users have rated the target item at the higher end of the rating scale, to improve effectiveness of the attack, we need to create malicious users  that are similar to the genuine users who have rated the target item with a higher value. So, to improve similarity, a randomly selected filler item is assigned the average rating given to the filler item by those users who have rated the target item at the higher scale Let UA be the set of all users who have rated the randomly selected filler item IF. Let UH be the set of all users who have rated the target item at a higher scale of rating and have also rated the randomly selected filler item. So, in this strategy, filler item IF is assigned the average rating given to it by the set of users UH. This approach differs from average attack in the way filler items are assigned values, as in average attack item IF was assigned the average rating given to it by the set of users UA  6. Filler item strategies for item-based collaborative filtering systems  It has also been found that attack models are not as effective against item-based CF systems as they are against user-based CF systems [3]. One explanation of this may be because most attack models are designed to improve similarity among users than items Our proposed approach for item-based CF system is designed to improve similarity among items and considers rating distribution of the target item as a critical factor that can affect the effectiveness of an attack. Our approach proposes two strategies that are based on the rating distribution of the target item Both strategies improve similarity by selecting appropriate filler items.  While in filler item strategies against user-based CF systems we had modified an existing attack model i.e., average attack model by intelligently assigning values to filler items the strategies elaborated in this section is a new approach that is focused on selection of appropriate filler items and is specifically built for attack against item-based collaborative filtering systems. As we show below, our proposed approach performs substantially better than average attack model. We define below the two strategies  6.1 Strategy IL  This strategy is followed when the target item falls in TL category. As majority of the users have rated the target item at the lower end of the rating scale, it is most likely that items similar to the target item will also be rated at the lower end of the scale. To improve effectiveness of the attack, we need to create 


improve effectiveness of the attack, we need to create malicious users that increase the similarity of the target item with items which are rated at the higher end of the rating scale. So, to improve similarity, we select filler items from the set of items which are highly rated by those users who have rated target item at a lower scale. In a scale of 1 to 5, higher scale implies rating of 4 or 5, lower scale implies a rating of 1 or 2 Let UL be the set of all users who have rated the target item at a lower scale of rating i.e., 1 or 2. Let IF be the set of items that have been rated 4 or 5 by the set of users UL. So, in this strategy, filler items are selected from this set of items IF. To select filler items, frequency count of all items which are rated 5 in set IF is computed and those with higher frequency count are given preference while selecting filler items. In case that the number of filler items required for creating an attack profile can?t be fulfilled by all the 5 rated items in the set IF then a frequency count of all 4 rated items in set IF is computed. As in earlier case, during selection of filler items, items with higher count are given preference. It is also taken into consideration that filler items selected should be distinct. All items selected as filler items are assigned the maximum rating of the rating scale i.e., 5 in a scale of 1 to 5.  In case number of filler items required are more than the number of items present in set IF then remaining filler items are selected randomly from the set of all items as done in an average attack. In an average attack, filler items are randomly selected and rating assigned to a filler item is its average rating  6.2  Strategy IH  This strategy is followed when the target item falls in TH category. As majority of the users have rated the target item at the higher end of the rating scale, it is most likely that items similar to the target item will also be rated at the higher end of the scale. In this strategy we try to further increase the association of Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 5 target item with more highly rated items. So, to improve similarity, we select filler items from the set of items which are highly rated by those users who have rated the target item at a higher scale. This will help further strengthen the similarity of target item with highly rated items Let UH be the set of all users who have rated the target item at a higher scale of rating i.e., 4 or 5. Let IF be the set of items that have been rated 4 or 5 by the set of users UH. So, in this strategy, filler items are selected from this set of items IF. The process of selecting filler items from the set IF is exactly similar to that of Strategy IL The two strategies Strategy IL and Strategy IH differ in the way members of UL and UH   are selected  7. Filler item strategies for segment-based attack  In [3] it has been shown that segmented attack is the most effective attack model against in-segment users. It performs much better than average attack model, the most effective model when effectiveness is measured across all users. A segmented attack is mounted with the intent of pushing a target item towards a specific group of users i.e., a segment of users that are grouped on the basis of their similarity in preferences for a category of items. The selection 


in preferences for a category of items. The selection of segment is done by selecting highly rated popular items with similar characteristics. Segmented attack is focused on those users who have highly rated majority of the selected items present in the segment For example, group of users who have highly rated at least any three of the five most popular animation movies form a segment of users interested in animation movies. An attacker with intent to promote a new animation movie will find it beneficial to mount a segmented attack against a segment of user interested in animation movies In our work, we study the effect of our filler item strategies in further improving the effectiveness of insegment attacks against both user-based and itembased attacks. In segment attack, filler items are randomly selected and assigned the minimum value in the rating scale. The reasoning behind assignment of minimum value to the filler item has not been explained in detail in any of the literature on segmented attack.  We provide below details of our filler item strategies for segment attack  7.1 Strategy SUL  This strategy is followed when a segment attack is mounted against a user-based CF system and the target item falls in TL category. In our approach to improve effectiveness of the attack, we need to create malicious users that are similar to those genuine users who have rated the target item with a lower value and also belong to the segment of users targeted by the segmented attack. So, to improve similarity, a randomly selected filler item is assigned the average rating given to it by those users who have rated any of the items that define the segment and have also rated the target item at a lower scale. For example for an attack against a segment of users interested in animation movies i.e., those users who have rated highly at least any three of the five most popular movies in animation genre, a filler item is assigned the average rating given to it by those users who have rated the target item at a lower rating and have rated highly at least one of the five animation movies that define the segment  7.2 Strategy SUH This strategy is followed when a segment attack is mounted against a user-based CF system and the target item falls in TH category. To improve effectiveness of a segment attack, a randomly selected filler item is assigned the average rating given to it by those users who have rated any of the items that define the segment and have also rated the target item at a higher scale  7.3 Strategy SIL  This strategy is followed when a segment attack is mounted against an item-based CF system and the target item falls in TL category. To improve effectiveness of a segment attack, we select filler items from the set of items which are highly rated by those users who have rated target item at a lower scale. The strategy used is similar to Strategy IL Filler items are selected the way explained in section 6.1  7.4 Strategy SIH  This strategy is followed when a segment attack is mounted against an item-based CF system and the 


target item falls in TH category. To improve effectiveness of a segment attack, we select filler items from the set of items which are highly rated by those users who have rated target item at a higher Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 scale. The strategy used is similar to Strategy IH Filler items are selected the way explained in section 6.2  8. Experimental evaluation and discussion  We performed the experimental evaluation of our strategies on the publicly available MovieLens data set [8]. This is the most widely used dataset in recommender systems research. MovieLens consists of 100,000 ratings made by 943 users on 1682 movies. Each user in the data set has rated at least 20 movies and each movie has been rated at least once A timestamp value is associated with each user movie, and rating combination. The data set also contains information on the demographic detail \(age sex, occupation, and zip code information \(genre and release date The ratings are made in a scale of 1 to 5, where 5 indicate extreme likeness for an item and 1 dislike We evaluated effectiveness of the proposed strategies on user-based and item-based collaborative algorithm. For similarity calculation and prediction in user-based CF algorithm, equations 1 and 2 stated in section 3 were used. Similarly, equations 3 and 4 stated in section 3 were used for computing similarity and prediction value for item-based CF algorithm We used a neighborhood size of k = 20 for prediction calculation. Case amplification value of 10 was used while calculating correlation and only positive correlations values were considered for computing predictions To conduct our evaluation, we selected a sample 20 items. Out of the 20 items, 10 items belonged to TL  category while remaining 10 items to TH category All the 20 items were selected randomly from a larger set of items belonging to each category. We also randomly selected a sample of 50 target users Target users selected were those who have never rated any of the 20 test items. Each of the target items was attacked individually and the prediction shift was calculated by averaging the prediction shift observed for each user. The final prediction shift for the attack is the average prediction over all items used in the test. Equation 6 was used to calculate the metric For implementation of segmented attack we followed the same guidelines as stated in [3]. Horror segment was selected as the target segment. Five of the most popular horror movies were selected to represent the segment. These five movies selected formed the selected item set in the attack profiles constructed. The five movies are Alien, Psycho, The Shining, Jaws, and The Birds. Users who have given a rating of 4 or 5 to at least any 3 of the five movies were identified as the target segment against which the attack was focused. For calculating prediction shift we selected 50 of the users from this target segment to form the test user set. While implementing the segment attack, selected items were given a rating of 5 and the randomly selected filler items were assigned a value 1 All experiments were conducted for ?Size of attack? values 1%, 3%, 6%, 12%, and 15%.  ?Size of attack? represents number of attack profiles added as a percentage of pre-attack profiles. 1% ?Size of attack? implies 10 attack profiles were added to a 


attack? implies 10 attack profiles were added to a system of 1000 genuine users. On the basis of the results reported in [4] that best results are reported when a filler size of 3% is used in an average attack we used a filler size of 3% for all our tests i.e., 3 % of 1682 items which is approximately 50 filler items For attacks against user-based collaborative filtering systems we used six strategies: Strategy UL, Strategy UH, Strategy SUL, Strategy SUH, segment attack and average attack. Similarly, for attacks against item based collaborative filtering systems we used six strategies: Strategy IL, Strategy IH Strategy SIL Strategy SIH, segment attack and average attack. For average attack, filler item strategy used was the same as in an average attack i.e., the mean of the filler item was assigned to it. Segment attack was implemented as explained earlier. Category TL, Category TH Strategy UL, Strategy UH, Strategy IL, Strategy IH Strategy SUL, Strategy SUH, Strategy SIL and Strategy SIH were implemented the way explained earlier in section 4, section 5, section 6 and section 7 respectively. For attacks against item-based CF while selecting filler items from set IF, only items with minimum frequency count of 10 were considered Figure 2 and Figure 3 show the effectiveness of our attacks when calculated for all users against systems using user-based collaborative filtering for recommendations.  Figure 2 shows the prediction shift values of attacks Strategy UL and average attack for items belonging to TL category. From the graph it?s obvious that for items in TL category, Strategy UL outperforms average attack model for all values of attack size. Similarly, Figure 3 shows the prediction shift values for the attack strategies Strategy UH and average attack for items belonging to TH category From the graph it can be concluded that for items belonging to TH category, Strategy UH performs much better than average attack over lower values of attack size. At attack size of 12 % and 15% both attack have similar effectiveness Figure 4 and Figure 5 show the effectiveness of our attacks when calculated for all users against systems using item-based collaborative filtering for recommendations.  Figure 4 shows the prediction shift values of Strategy IL and average attack for Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 items belonging to TL category. Similarly, Figure 5 shows the prediction shift values for the Strategy IH and average attack for items belonging to TH category. From Figure 4 and 5 it can be concluded that both Strategy IL and Strategy IH perform substantially better than average attack over all values of attack size. It can also be observed that our attack strategies are more effective against itembased systems than user-based systems Figure 6, 7, 8, and 9 show the effectiveness of our filler based attack strategies for in-segment users. We observe that for attacks against both user-based and item-based CF systems the effectiveness of our filler based strategies is comparable to the best available attack model for in-segment attacks i.e., segment attack. However, in Figure 8 we observe that filler item strategy SIL performs better than segment attack. Because of the low knowledge cost involved in segment attack, we can conclude that for most scenarios segment attack is a better attack model for in-segment attacks than filler based attack models Experimental results clearly show that our approach of selecting a strategy based on target item rating distribution outperforms the best available attack model i.e., average model. One drawback of 


attack model i.e., average model. One drawback of our attack strategies is its high knowledge cost However, automated software agents can help diminish the cost. One approach that can be used to decrease the cost is to use a subset of users while selecting filler items. For example, in attacks against item-based systems, while implementing Strategy IH instead of selecting all users who have rated target item as 4 or 5 as members of the set UH. , we only select 20 users. Selection of items for set IF will then be performed using the data of the 20 users in set UH Similarly, in case of attacks against user-based systems, while implementing Strategy UH instead of assigning a filler item IF the average rating given to it by the set of users UH. , we assign IF the average rating given to it by a subset of 5 randomly selected users from UH. In future work we plan to experimentally verify the effectiveness of these cost reduction approaches    Figure 2:   Attack on TL category of items against user-based collaborative filtering system   Figure 3:   Attack on TH   category of items against user-based collaborative filtering system   Figure 4:   Attack on TL category of items against item-based collaborative filtering system  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8  Figure 5:   Attack on TH category of items against item-based collaborative filtering system   Figure 6:   Attack on TL category of items against user-based collaborative filtering system   Figure 7:   Attack on TH   category of items against user-based collaborative filtering system    Figure 8:   Attack on TL category of items against item-based collaborative filtering system   Figure 9:   Attack on TH category of items against item-based collaborative filtering system  9. Conclusion  This paper provides an effective approach towards constructing attack models. We show the importance of target item and filler items in construction of successful attack strategies. Through experiments we show that our approach of intelligent selection of filler items based on target item rating distribution results in substantial improvement over the baseline average attack. We also compare our approach with the well known in-segment approach and conclude that our approach gives slightly improved results. In future, we plan to examine the filler items strategies for other attack models, and also create algorithms to improve robustness and stability of recommender systems against shilling attacks 


systems against shilling attacks  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9  10. References  1] Lam, S., and Riedl, J. 2004. Shilling Recommender Systems for Fun and Profit, In Proceedings of the 13th International WWW Conference 2] Mehta, B., Hofmann, T., and Nejdl, W. 2007. Robust Collaborative Filtering, In Proceedings of the 2007 ACM Conference on Recommender Systems, 49-56 3] Mobasher, B., Burke, R., Bhaumik, R., and Williams C. 2007. Towards Trustworthy Recommender Systems: An Analysis of Attack Models and Algorithm Robustness, ACM Transactions on Internet Technology, 7\(2007 4] Burke, R., Mobasher, B., and Bhaumik, R. 2005 Limited Knowledge Shilling Attacks in Collaborative Filtering Systems, In Proceedings of Workshop on Intelligent Techniques for Web Personalization 5] Konstan, J., Miller, B., Maltz, D., Herlocker, J Gordon, L., and Riedl, J. 1997.  GroupLens: Applying Collaborative Filtering to Usenet News Communications of the ACM, 40, 3\(1997 6] Herlocker, J., Konstan, J., Borchers, A., and Riedl J.1999. An Algorithm Framework for Performing Collaborative Filtering, In Proceedings of  SIGIR ACM, 77-87 7] Sarwar, B., Karypis, G., Konstan, J., and Riedl, J 2001. Item-based Collaborative Filtering of Recommendation Algorithms. In Proceedings of the 10th International WWW Conference 8] MovieLens data set,www.grouplens.org  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


 Current Time \(min  Healthy Failure Expected Just-in-time line Actual Remaining Life  Figure 17. Results of failure prognosis 0 100 200 300 400 500 600 700 800 900 1000 0 0.02 0.04 0.06 0.08 0.1 0.12 Time \(min Sp al l S iz e  m m 2 Interpolation of spall growth according to feature values 0 100 200 300 400 500 600 700 800 900 1000 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Time \(min Fe a tu re V a lu e    Interpolation of feature value with noise Interpolation of feature vlaue snapshot with ground truth data 11 CONCLUSIONS This paper shows that enhancements to diagnostic techniques are desirable as well as attainable additions to Health and Usage Monitoring Systems \(HUMS particularly in the case of rotorcraft component monitoring Enhancements like those presented support CBM efforts primarily in two ways: reduce the sensitivity of diagnostic processes to both signal noise and variations in environmental and operating conditions, and improve the performance of detection systems as well as the task of fault identification \(e.g., severity quantification instantiation of reliable prognostics Representative examples, motivated by the interest of the U.S. Army in transitioning from time-based \(using TBO definitions drive train bearing, illustrates the potential benefits of 


pursuing an integrated approach to diagnostics and prognostics, combining technologies for enhanced data preprocessing, advanced diagnostic-support algorithms, fusion at the sensor/feature levels, and an adequate framework for false alarm mitigation and uncertainty management. An architecture for achieving such integration is presented, with emphasis on supporting a robust performance of diagnostics operations, even in the presence of such kinds of disturbances as those observed in data acquired by HUMS vibration sensors. The present study also gives relevance to seeded fault because the technologies discussed can integrate knowledge about damage mechanism interactions or physics-of-failure models, as well as make use of multiple-sensor and multiple-feature data sets representative of known fault conditions For this reason, the team behind this project is evaluating a potential opportunity to perform a series of tests on rotorcraft drive train bearings with varying fault severities and under multiple, though realistic, operating conditions Such tests are being planned to provide algorithm/model validations, as well as diagnostic/prognostic performance assessments, in support of providing the U.S. Army with technologies that make detection systems more robust allow for the implementation of prognostics, and extend the useful life of drive train components. Component degradation testing thus remains as future, follow-up work to the research reported in this document ACKNOWLEDGMENTS This work has been partially supported with a cooperative agreement by the Army Research Laboratory under contract number W911NF-07-2-0075. In addition to the primary authors, we would like to thank government and contractor representatives from organizations supporting the Army Utility \(Blackhawk Estes, Mr. Carlos Rivera, and Dr. Jon Keller. This work has also benefitted greatly from consultations with other Army Research Laboratory and NASA Glenn researchers such as Dr. Timothy Krantz, Dr. David Lewicki, Dr. Harry Decker Dr. Hiralal Khatri, Mr. Ken Ranney and Mr. Kwok Tom REFERENCES 1] Branhof, R.W., Grabill, P., Grant, L., and Keller, J.A  Application of Automated Rotor Smoothing Using Continuous Vibration Measurements  American Helicopter Society 61st annual forum, Grapevine, Texas June 1  3, 2005 2] Dora, R., Wright, J., Hess, R., and Boydstun, B  Utility of the IMD HUMS in an Operational Setting on the UH60L Blackhawk  American Helicopter Society 60th annual forum, Baltimore, Maryland, May 7  10, 2004 3] Zakrajsek, J.J., Dempsey, P.J., et al  Rotorcraft Health Management Issues and Challenges  NASA report TM  2006-214022. February, 2006 4] Suggs, D.T., and Wade, D.R  Vibration Based Maintenance Credits for the UH-60 Oil Cooler Fan Assembly  American Helicopter Society, CBM Specialists Meeting, Huntsville, Alabama, February 13 2008 5] Baker, C., Marble, S., Morton, B.P., and Smith, B.J  Failure Modes and Prognostic Techniques for H-60 Tail Rotor Drive System Bearings  IEEEAC paper #1122 IEEE, 2007 6] Keller, J.A., Branhof, R., Dunaway, D., and Grabill, P  Examples of Condition Based Maintenance with the Vibration Management Enhancement Program   American Helicopter Society 61st Annual Forum Grapevine, Texas, June 1  3, 2005 7] Zhang, B., Sconyers, C., Byington, C.S., Patrick, R Orchard, M.E., and Vachtsevanos, G.J  Anomaly Detection: A Robust Approach to Detection of 


Detection: A Robust Approach to Detection of Unanticipated Faults  International Conference on Prognostics and Health Management, Denver, Colorado October 6-9, 2008 8] Byington, C.S., Watson, M., Lee, H., and Hollins, M  Sensor-level Fusion to Enhance Health and Usage Monitoring Systems  American Helicopter Society, 64th Annual Forum, Montreal, Canada, April 29-May 1, 2008 9] Engel, S.J., Gilmartin, B.J., Bongort, K., and Hess, A  Prognostics, the Real Issues Involved With Predicting Life Remaining  Proceedings of the IEEE Aerospace Conference, Big Sky, Montana, March 18-25, 2000 12 BIOGRAPHY Romano Patrick is a Project Manager at Impact Technologies. He received a Ph.D. in Electrical Engineering from the Georgia Institute of Technology specializing in model-based machine health diagnostics and prognostics. He also holds an MBA from Georgia Tech and degrees from U Texas, Arlington and U. Panamericana, Mexico. With career focus on interdisciplinary integration of technologies, his recent work involves practicable diagnostics/prognostics design for complex systems, such as rotorcraft drive trains Past experience includes automation and design for a variety of industrial and government sponsors \(DARPA, Lockheed Martin, Northrop Grumman, etc and program coordination at U. Panamericana, and some entrepreneurial R&amp;D Matthew J. Smith is a Senior Project Engineer at Impact Technologies. During his tenure with Impact, Matthew has performed multiple efforts pertaining to bearing vibration analysis, diagnostic and prognostic system development, and experimental study of faulted system reponse and fault progression. Previously, as a research assistant at Penn State and the NASA Glenn Research Center, Matthew performed experimental and analytical oil-free bearing analyses Matthew received his B.S. and M.S. degrees in Mechanical Engineering from The Pennsylvania State University. His research interests include: prognostic health assessment for bearing and actuator systems, grease degradation modeling and fault classifier development Bin Zhang received his Ph.D. degree from Nanyang Technological University, Singapore in 2007. He received his BE and MSE degrees from Nanjing University of Science and Technology, China, in 1993 and 1999, respectively. He is a senior member of IEEE. From 2005 to present, he has been a Post-Doc with the School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta GA His current research interests are fault diagnosis and failure prognosis, systems and control, digital signal processing learning control, intelligent systems and their applications to robotics, power electronics and various mechanical systems Carl S. Byington is a Professional Engineer and the Director of Systems Engineering at Impact Technologies. He directs R&amp;D in pursuit of advanced, automated systems health management for land-based, shipboard, and airborne machinery for military and commercial customers. He is Chairman of the Machinery Diagnostics &amp; Prognostics Committee of ASME and a member of IEEE, AIAA, SAE and AHS. He has a BS degree in Mechanical Engineering from the University of Pennsylvania and an MS in Aeronautical Engineering from George Washington University, and has published over 60 papers, book chapters, magazine and journal articles related to diagnostics and prognostics technologies George Vachtsevanos is Professor Emeritus at the Georgia Institute of Technology and also serves as the Chief Scientist at Impact Technologies, LLC. He directed the Intelligent Control Systems laboratory at Georgia Tech for the past 28 years where faculty and students are conducting research in fault diagnosis/prognosis and fault-tolerant control of engineering systems, intelligent control of industrial 


engineering systems, intelligent control of industrial processes, neurotechnology and cardiotechnology, and unmanned systems. His research work has been sponsored by government and industry and has published over 250 technical papers in his area of expertise. He is the lead author of a book on "Intelligent Fault Diagnosis and Prognosis of Engineering Systems" published by Wiley in 2006. He is the recipient of the Georgia Tech Interdisciplinary Activities award and the ECE Distinguished Professor award Romeo de la Cruz del Rosario, Jr. is the Chief of the Electronics Technology Branch at the U.S. Army Research Laboratory. He also serves as the Army Technology Objective ATO P&amp;D Operational Readiness and Condition Based Maintenance He received the B.E.E. degree from the Catholic University of America, Washington, D.C., and the M.S.E. and Ph.D degrees in Electrical and Computer Engineering from the Johns Hopkins University, Baltimore, MD. Since 1991 he has been an engineer at the Harry Diamond Laboratory then U.S Army Research Laboratory working in several areas including high power microwave technology characterization &amp; modeling of heterostructure RF devices and fabrication and failure analysis of electron devices and circuits  pre></body></html 


movies. In the case of the volume of critical reviews however, there was no big difference between mainstream and non-mainstream movies. WOM and critical reviews were usually positive H1 and H2 tested the relationship between WOM and weekly box office revenue, and the results supported the hypotheses. The volume of WOM was positively related to weekly box office revenue, while the valence of WOM had no significant effect. H3 H4a, and H4b tested the impact of critical reviews, and the results also supported the hypotheses except H4b The volume and valence of critical reviews had no consistent significances to weekly box office revenue H3 H4b. Table 7 showed that the number of critical reviews was statistically significant to aggregate box office revenue \(H4a for the attitude of critical reviews \(H4b the result more detail, an additional test was performed using only those factors related to critical reviews as independent variables. The result of the additional test supported H4, but the signs were reversed, i.e. positive critical reviews had minus signs, and negative critical reviews had plus signs. This reversed signs imply that the preference of critical reviewers is very similar to that of normal moviegoers. H5s and H6s tested the different effects of WOM and critical reviews on mainstream and non-mainstream movies. The result failed to determine that WOM give different impact on mainstream and non-mainstream movies, so H5a and H5b were rejected. H6, however, was supported, i.e the effects of critical reviews were different for mainstream and non-mainstream movies. There were no significant relationships between critical reviews and aggregate box office revenue in mainstream movies. For non-mainstream movies, however, the volume of critical reviews and the percentage of negative critical reviews were significant. Nonmainstream movies have fewer sources from which consumers can get information, and this might explain the results The above findings lead to several managerial implications. First, producers and distributors of movies could forecast weekly box office revenue by looking at previous weeks? volume of WOM. It does not matter what attitude people have when they spread WOM, the important factor is its volume. Therefore producers and distributors need to develop an appropriate strategy to manage WOM for their movies For example, the terms related to WOM marketing such as buzz and viral marketing are easily found Second, for the distributors who usually distribute less commercial and more artistic movies, and consequently have a smaller market compared to the major distributors, critical reviews can impact their movies box office revenues in a significant way. There are usually fewer sources for information for nonmainstream movies than mainstream movies, and so small efforts could leverage the outcomes. Finally, for those who are dealing with mainstream movies, the finding that the valence of WOM and critical reviews do not have significant relationship with box office revenue can have certain implications. Particularly, the attitude of critical reviews showed reversed effects Therefore, they may need to concentrate on other features rather than attitude of moviegoers or critical reviews, such as encouraging moviegoers to spread WOM This study contributes to the understanding of the motion picture industry, especially the relationship between box office revenue and WOM including critical reviews. There are existing studies that already 


critical reviews. There are existing studies that already dealt with similar issues, but this study has some differentiated features compare to prior studies. First the data used in this study was collected from South Korea, while most of the relevant studies usually focus on the North American market. This helps to provide the opportunity to understand the international market especially the Asian market, even though South Korea is a small part of it in terms of the motion picture industry. Second, movies were categorized to two groups, i.e. mainstream and non-mainstream and this study attempted to determine how WOM impacts these categories differently by testing several hypotheses In this study, there are also several limitations that could be dealt with in future research. First, using box office revenue as a dependent variable is more meaningful for distributers rather than producers. Due to there is close correlation between box office revenue and number of screens, one of producers? main concerns is how many screens their movies can be played on. Moreover, DVD sales are also important measurement for success of movies these days, and so it also could be a dependent variable. Therefore, it could be possible to give more fruitful managerial implications to various players in the motion picture industry by taking some other dependent variables Second, in this study, movies were categorized simply as mainstream and non-mainstream movies, but there could be further studies with diverse techniques of movie categorizations. For example, it would be possible to study the varying influence of WOM or critical reviews on different genres or movie budgets Third, an interesting finding of this study is that positive critical reviews could have negative relationship with box office revenue while negative critical reviews could have positive relationship. This study tried to provide a reasonable discussion on the issue, but more studies could be elaborate on it  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 References  1] Dellarocas, C., The Digitization of Word of Mouth Promise and Challenges of Online Feedback Mechanisms Management Science, 2003. 49\(10 2] Bone, P.F., Word-of-mouth effects on short-term and long-term product judgments. Journal of Business Research 1995. 32\(3 3] Swanson, S.R. and S.W. Kelley, Service recovery attributions and word-of-mouth intentions. European Journal of Marketing, 2001. 35\(1 4] Hennig-Thurau, F., et al., Electronic word-of-mouth via consumer-opinion platforms: What motivates consumers to articulate themselves on the internet? Journal of Interactive Marketing, 2004. 18\(1 5] Fong, J. and S. Burton, Electronic Word-of-Mouth: A Comparison of Stated and Revealed Behavior on Electronic Discussion Boards. Journal of Interactive Advertising, 2006 6\(2 6] Gruen, T.W., T. Osmonbekov, and A.J. Czaplewski eWOM: The impact of customer-to-customer online knowhow exchange on customer value and loyalty. Journal of Business Research, 2006. 59\(4 7] Garbarino, E. and M. Strahilevitz, Gender differences in the perceived risk of buying online and the effects of receiving a site recommendation. Journal of Business Research, 2004. 57\(7 8] Ward, J.C. and A.L. Ostrom, The Internet as information minefield: An analysis of the source and content of brand information yielded by net searches. Journal of Business Research, 2003. 56\(11 


9] Goldsmith, R.E. and D. Horowitz, Measuring Motivations for Online Opinion Seeking. Journal of Interactive Advertising, 2006. 6\(2 10] Eliashberg, J., A. Elberse, and M. Leenders, The motion picture industry: critical issues in practice, current research amp; new research directions. HBS Working Paper, 2005 11] S&amp;P, Industry surveys: Movies and home entertainment 2004 12] KNSO, Revenue of Motion Picture Industry 2004 Ministry of Culture, Sports, and Tourism, 2004 13] Duan, W., B. Gu, and A.B. Whinston, Do Online Reviews Matter? - An Empirical Investigation of Panel Data 2005, UT Austin 14] Zhang, X., C. Dellarocas, and N.F. Awad, Estimating word-of-mouth for movies: The impact of online movie reviews on box office performance, in Workshop on Information Systems and Economics \(WISE Park, MD 15] Mahajan, V., E. Muller, and R.A. Kerin, Introduction Strategy For New Products With Positive And Negative Word-Of-Mouth. Management Science, 1984. 30\(12 1389-1404 16] Moul, C.C., Measuring Word of Mouth's Impact on Theatrical Movie Admissions. Journal of Economics &amp Management Strategy, 2007. 16\(4 17] Liu, Y., Word of Mouth for Movies: Its Dynamics and Impact on Box Office Revenue. Journal of Marketing, 2006 70\(3 18] Austin, B.A., Immediate Seating: A Look at Movie Audiences. 1989, Wadsworth Publishing Company 19] Bayus, B.L., Word of Mouth: The Indirect Effects of Marketing Efforts. Journal of Advertising Research, 1985 25\(3 20] Faber, R.J., Effect of Media Advertising and Other Sources on Movie Selection. Journalism Quarterly, 1984 61\(2 21] Eliashberg, J. and S.M. Shugan, Film critics: Influencers or predictors? Journal of Marketing, 1997. 61\(2 22] Reinstein, D.A. and C.M. Snyder, The Influence Of Expert Reviews On Consumer Demand For Experience Goods: A Case Study Of Movie Critics. Journal of Industrial Economics, 2005. 53\(1 23] Gemser, G., M. Van Oostrum, and M. Leenders, The impact of film reviews on the box office performance of art house versus mainstream motion pictures. Journal of Cultural Economics, 2007. 31\(1 24] Wijnberg, N.M. and G. Gemser, Adding Value to Innovation: Impressionism and the Transformation of the Selection System in Visual Arts. Organization Science, 2000 11\(3 25] De Vany, A. and W.D. Walls, Bose-Einstein Dynamics and Adaptive Contracting in the Motion Picture Industry Economic Journal, 1996. 106\(439 26] Bagella, M. and L. Becchetti, The Determinants of Motion Picture Box Office Performance: Evidence from Movies Produced in Italy. Journal of Cultural Economics 1999. 23\(4 27] Basuroy, S., K.K. Desai, and D. Talukdar, An Empirical Investigation of Signaling in the Motion Picture Industry Journal of Marketing Research \(JMR 2 295 28] Neelamegham, R. and D. Jain, Consumer Choice Process for Experience Goods: An Econometric Model and Analysis. Journal of Marketing Research \(JMR 3 p. 373-386 29] Lovell, G., Movies and manipulation: How studios punish critics. Columbia Journalism Review, 1997. 35\(5 30] Thompson, K., Film Art: An Introduction. 2001 McGraw Hill, New York 31] Zuckerman, E.W. and T.Y. Kim, The critical trade-off identity assignment and box-office success in the feature film industry. Industrial and Corporate Change, 2003. 12\(1 


industry. Industrial and Corporate Change, 2003. 12\(1 27-67 32] KOFIC, Annual Report of Film Industry in Korea 2006 Korean Film Council, 2006 33] Sutton, S., Predicting and Explaining Intentions and Behavior: How Well Are We Doing? Journal of Applied Social Psychology, 1998. 28\(15 34] Basuroy, S., S. Chatterjee, and S.A. Ravid, How Critical Are Critical Reviews? The Box Office Effects of Film Critics Star Power, and Budgets. Journal of Marketing, 2003. 67\(4 p. 103-117  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 





