Privacy Preserving Decision Tree Mining from Perturbed Data Li Liu Global Information Security eBay Inc liiliu@ebay.com Murat Kantarcioglu and Bhavani Thuraisingham Computer Science Department University of Texas at Dallas muratk bhavani.thuraisingham@utdallas.edu Abstract Privacy preserving data mining has been investigated extensively The previous works mainly fall into two categories perturbation and randomization based approaches and secure multi-party computation based approaches The 
earlier perturbation and randomization approaches have a step to reconstruct the original data distribution The new research in this area adopts different data distortion methods or modi“es the data mining techniques to make it more suitable to the perturbation scenario Secure multi-party computation approaches which employ cryptographic tools to build data mining models face high communication and computation costs especially when the number of parties participating in the computation is large In this paper we propose a new perturbation based technique In our solution we modify the data min 
ing algorithms so that they can be directly used on the perturbed data In other words we directly build a classi“er for the original data set from the perturbed training data set 1 Introduction Due to increasing concerns related to privacy various privacy-preserving data mining techniques have been developed to address different privacy issues These techniques usually operate under various assumptions and employ different methods In this paper we will focus on the perturbation method that is extensively used in privacy pre 
serving data mining In this paper we propose a new method that we build data mining models directly from the perturbed data without trying to solve the general data distribution reconstruction as an intermediate step More precisely we propose a modi“ed C4.5 decision tree classi“er that can deal with perturbed numeric continuous attributes Our privacy preserving decision tree C4.5 PPDTC4.5 classi“er uses perturbed training data and builds a decision tree model which could be used to classify the original or perturbed data sets Our experiments have shown that our PPDTC4.5 classi“er 
can obtain a high degree of accuracy when used to classify the original data set The paper is organized as follows Section 2 describes related work Section 3 introduce a privacy metric system used to measure privacy in our work Section 4 shows the construction of the decision tree Section 5 describes how to build Naive Bayesian models from the perturbed data sets In section 6 we explain our PPDTC4.5 in detail Section 7 presents our experimental results In section 8 we conclude with a discussion of future work 2 Related Work and Motivation 
Previous work in privacy-preserving data mining has addressed two issues In one the aim is to preserve customer privacy by perturbing the data values In this scheme random noise data is introduced to distort sensitive values and the distribution of the random data is used to generate a new data distribution which is close to the original data distribution without revealing the original data values The estimated original data distribution is used to reconstruct the data and data mining techniques such as classi“ers and association rules are applied to the reconstructed data set 
Later re“nement of this approach has tightened estimation of original values based on the distorted data The data distortion approach has also been applied to Boolean values in research work 7 16 Perturbation methods and their privacy protection have been criticized because some methods may derive private information from the reconstruction step Dif ferent to the original noise additive method in man y distincti v e perturbation methods have been proposed One important category is multiplicative perturbation method In the view of geometric property of the data multiplying the original 
data values with a random noise matrix is to rotate the original data matrix so it is also called rotated based perturbation In authors ha v e gi v e n a sound proof of Rotationinvariant Classi“ers to show some data mining tools can Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 1 978-0-7695-3450-3/09 $25.00 © 2009 IEEE 
 


be directly applied to the rotation based perturbed data In the later work Liu et al ha v e proposed multiplicati v e random projection which provided more enhanced privacy protection There are some other interesting techniques such as condensation based approach matrix decomposition and so on As pointed out in 12 these recently research on perturbation based approaches apply the data mining techniques directly on the perturbed data skipping the reconstruction step Choosing the suitable data mining techniques is determined by the method which noise has been introduce To our knowledge very few works focus on mapping or modifying the data mining techniques to meet the perturbation data needs The other approach uses cryptographic tools to build data mining models For example in the goal is to securely build an ID3 decision tree where the training set is distributed between two parties Different solutions were given to address different data mining problems using cryptographic techniques e.g 5 8 This approach treats privacy-preserving data mining as a special case of secure multi-party computation and not only aims for preserving individual privacy but also tries to preserve leakage of any information other than the nal result But when the numbers of parties become bigger the communication and computation cost grow exponentially Our proposed approach is a modi“ed C4.5 decision tree algorithm and adopt noise additi v e method Our approach is suitable for the scenarios where many parties want to perform data mining but each of them only has a small portion of the data To get the global data mining patterns the various parties must share their data but each party has its privacy and security concerns Our approach is the solution for such a situation In our approach each party perturbs its data according to the distribution of a pre-set random noise and sends its perturbed data to the data miner The data miner collects the perturbed data sets from each party and also knows the distribution used to perturb the data Based on this information the data miner builds a classi“er and returns the classi“er to every participating party Then each party can use this classi“er to classify its data In this case each party only knows its own data and the classi“er and it does not have any knowledge about the data of others The data miner only has access to the perturbed data and the distribution of the noise data This way privacy is preserved and the communication and computation costs for each party are minimized 3 Privacy Metrics In the work Agra w a l and Agg arw al ha v e proposed a privacy measure based on differential entropy We brie”y repeat the ideas here The differential entropy h  A  of a random variable A is de“ned as follows h  A     A f A  a  log 2 f A  a  da 1 where  A is the domain of A  Actually h  A  is a measure of uncertainty inherent in the value of A in the statistics Agrawal and Aggarwal based on this proposed that the privacy measure inherent in the random variable A as  A    A  h  A  2 For example a random variable U distributed uniformly between 0 and a has privacy  U  log 2  a   a  Thus if  A   then A has as much privacy as a random variable distributed uniformly in an interval of length 1  Furthermore if f B  x  f A 2 x   then B offers half as much privacy as A  This can be easily illustrated as a random variable uniformly distributed over 0  1 has half as much privacy as a random variable uniformly distributed over 0  2 In[2 Agrawal and Aggarwal have also de“ned conditional privacy and information loss For more detail please refer the original work We choose this privacy measure in our work to quantify the privacy in our experiments 4 Overview of Decision Tree Construction We propose a modi“ed C4.5 decision tree classi“er which builds the decision tree from perturbed data and can be used to classify both the original and the perturbed data The idea behind this approach is the following when we consider the splitting point of the attribute we consider the bias of the noise data set as well We calculate the bias whenever we try to nd the best attribute the best split point and partition the training data The C4.5 algorithm is an extension of the ID3 algorithm and proposed by Quinlan in After years of impro v e ment C4.5 algorithm is one of the best algorithms in handling numeric continuous attributes It nds the best splitting attribute and the best splitting point of the numeric continuous attributes 4.1 Splitting Criterion Splitting criterion is very important in building a decision tree It decides which attribute to use for the splitting and for the numeric continuous attribute and also determines which value is used for this splitting It determines whether or not a decision tree is ef“cient It dramatically affects the classi“cation accuracy ID3 uses information gain as splitting criterion C4.5 algorithm uses information gain ratio which takes the number of branches into account when examining an attribute The formulas of information gain and gain ratio are given as follows Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 2 
 


Info  S   k  j 1 freq  C j S    S   log 2  freq  C j S   S   3 Info Test A  S  n  i 1  S i   S   Info  S i  4 gain  Test A  Info  S   Info Test A  S  5 splitInf o  Test A   n  i 1  S i   S   log 2   S i   S   6 gainRatio  Test A  gain  Test A  splitInf o  Test A  7 Let S be the training set  S  is the number of instance in S and freq  C i S  is the number of instance that belongs to class i where i from 1 to n   S i  is the number of instance in the category S i  Test A is the attribute chosen 4.2 Discretizing Continuous Attributes Binary Split Approach C4.5 algorithm is also designed to handle numeric attributes Instead of using the x range C4.5 algorithm searches among possible split points to nd the best split point Let us assume that a numeric attribute A i has the values  x 1 x 2   x m  in increasing order C4.5 partitions the instances into two groups S 1 and S 2 where S 1 consists of the values up to and including x j  and S 2 consists those that have values greater than x j  For each of these partitions C4.5 computes the gain ratio and chooses the partition that maximizes the gain ratio 4.3 Stopping Criteria The stopping criteria decides when to stop growing a decision tree In C4.5 Algorithm the tree stops growing when one of the two criteria is met One is that all the instances at the node have the same class label C i  we say this node is pure Another is when the number of instances at the node is less than or equal to a pre-set threshold number we say this number is minimum instance number of the node We use the same stopping criteria in our privacy preserving decision tree C4.5 PPDTC4.5 too 5 Naive Bayes Classi“er Construction over Perturbed Data As stated in Nai v e Bayes classi“er can be applied directly on the perturbed data For the completeness we brie”y describe here 5.1 Naive Bayes Classi“er Overview The Naive Bayes classi“er labels a new instance by assigning the most probable class value Besides it assumes that attribute values are conditionally independent given the class value in order to simplify the estimation of the required probabilities Using the above assumptions Naive Bayes classi“er selects the most likely classi“cation C nb as[13 C nb  argmax C j  C P  C j   i P  X i  C j  8 where X  X 1 X 2   X n denotes the set of attributes C  C 1 C 2   C d denotes the nite set of possible class labels and C nb denotes the class label output by the Naive Bayes classi“er Clearly we need to calculate the probabilities P  X i  x  C j  used in the Equation 8 based on the training data In practice for numeric attributes P  X i  x  C j  is estimated by using Gaussian distribution N   ij  2 ij  The required parameters  ij  E  X i  c j  and  2 ij  Var  X i  C j  areestimated by using the training data In Section 5.2 we show how to estimate the parameters  ij and  2 ij  using the perturbed training data 5.2 Over Perturbed Numeric Data We need to estimate  ij and  2 ij for each attribute X i and for each class label C j using the perturbed numeric data to construct a Naive Bayes classi“er In the perturbed data case instead of the original attribute value X i  we only see the W i  X i  R values Let w t ij be the i th attribute value of the t th training data instance with class label C j  In addition we assume that there are n instances with class label C j  We also know that w t ij  x t ij  r t ij where r t ij is the randomly generated noise with mean zero and known variance  2 R  Using the above facts we can show that the expected value of  w ij  1 n   n t 1  w t ij  is equal to  ij  Since the sample variance S 2  1 n  1   n t 1  w t ij   w i j  2 has an expected value  2 ij   2 R  we can use S 2 and the known  2 R to estimate the  2 ij i.e use S 2   2 R to estimate  2 ij  As a result as long as we do not change the class labels we can directly construct Naive Bayes classi“er from the perturbed data Even more since the parameter estimations done by using the perturbed data and the original data have the same expected values we should be able to get similar classi“cation accuracy in both cases To verify the above intuition we have performed some experiments using the Naive Bayes classi“er from the WEKA machine learning Using the same data set with all six numeric continuous attributes we construct the Naive Bayes classi“er from the original data set and Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 3 
 


we get 79  7 classi“cation accuracy Similarly if we directly construct the Naive Bayes classi“er from the perturbed training data set and test it on the perturbed test set we get 78 classi“cation accuracy As expected the two classi“cation accuracy values are very close 6 Privacy Preserving Decision Tree C4.5 PPDTC4.5 In this section we will describe how to build a decision tree classi“er from the perturbed training data set We will show threshold and random path selection two different ways to build classi“ers The threshold algorithm gives reasonably good accuracy for classifying the original data set The random path selection algorithm uses the probability as weight and nds good splitting points for the attributes However as we will see that randomly selecting the path to partition the training data set does not build a good decision tree classi“er We include the random path selection algorithms mainly for comparison purposes as well as to provide some directions for future work to building decision tree classi“ers to classify perturbed data set We believe that with proper improvement on tight bounds of random variables R  the performance of random path selection algorithm can be improved The goal is to build a decision tree model from perturbed data which can classify the original data set or perturbed data set accurately In our case we do not know the original x j values due to added noise Instead we observe w j  x j  R where R is random noise which we know its distribution Clearly for a split point t for attribute A i  if we know the x j values we use the following rule to split instances into S 1 and S 2  split  x j t   S 1 x j  t S 2 x j t 9 Since we do not know the x j values therefore we can only calculate the probability of w j belongs to S 1 that for given split point t and w j  Note that Pr  w j  S 1   Pr  w j  R  t   Pr  w j  t  R   Since we know the cumulative distribution function of R  we can calculate the probability easily Let p S 1  w j  t  is the probability that w j belongs to S 1 given the split point t and w j  Similarly de“ne p S 2  w j  t   p S 1  w j  t   In general let us de“ne p S  w j  t  is the probability that w j belongs to set S Now instead of splitting according to equation  9 we can use the p S  w j  t  values to estimate the best split point and partition the w j just as the original data x j would have been partitioned 6.1 Splitting Criterion Using Threshold We can calculate the probability p S 1  w j  t   for given split point t for each w j value This p S 1  w j  t  value indicates the likelihood w j  S 1  We can set a threshold and count the number of w j having a class label C j and p S i  w j  value greater than the threshold This can be expressed in the form of equation as follows freq   C j S i   w j  S i  I w j  C j p S i  w j   threshold  10 In the above equation we calculate the frequency of a class value by using the p S i  w j   I w j  C j is an indicator function and returns 1 if w j has a class label C j  The equation  3 is changed to as follow Info  S   k  j 1 freq   C j S    S   log 2  freq   C j S   S   11 Using above equation  11 plus equation  4  5  6 and  7 we can nd the best splitting attribute and the best splitting point for the numeric continuous attribute by maximizing the gainRatio  6.2 Splitting Training Data by Threshold The splitting criteria for the training data is straightforward splitT hreshold  w j t   S 1 p S 1  w j  t   threshold S 2 p S 1  w j  t   threshold 12 Noticed that the condition p S 1  w j  t   threshold is equivalent to the condition p S 2  w j  t   1  threshold   vice versa For example condition p S 1  w j  t   0  2 is equivalent to the condition p S 2  w j  t   0  8  Pseudo-code is shown in the Algorithm 1 Using the threshold approach seems like a simple method but setting the appropriate threshold that will lead to a successful classi“er is not trivial By de“nition p S 1  w j  t  is the probability that w j belongs to S 1 given the split point t and w j  When the split point t and perturbed instance w j are given the only uncertainty is the random noise R  In another words the choice of threshold is related to the distribution of the random noise In our experiments we have used both the Uniform and Gaussian distributions for random noise The thresholds are different for these two kinds of distributions For Gaussian distribution when the threshold is set to 0  30  the classi“er gives higher accuracy for Uniform distribution when the threshold is set to 0  50  the classi“er gives higher accuracy The experimental results are discussed in next section Another good way to decide the threshold is to keep it exible just like what the C4.5 algorithm does to nd the best splitting point t for the numeric continuous attribute This would lead to good results but the computation costs are increased Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 4 
 


 Partition Node N  1 if Stopping Criteria is Met then 2 return 3 else 4 Using T hresholdSplittingCriterion 5 Compute the Best Attribute Best A and the Best Splitting Point t  for each Instance w j in Node N do 6 Calculate the p S 1  w j  t  p 1 7 Based on the best splitting attribute Best A 8 and the best splitting point t  9 if p 1  threshold then 10 addChild  N.leftChild w j  11 else 12 addChild  N.rightChild w j  13 end 14 end 15 end 16 Partition N.leftChild  17 Partition N.rightChild  18 Algorithm 1  Partition Training Instances Using Threshold 6.3 Classifying the Original Instance Using the algorithm described in the previous two subsections we choose the splitting attributes and splitting points that actually belong to the original data We also partition the perturbed training data as decided by our estimation of its original values This way we can build a decision tree classi“er for the original data Then when we use it to classify the original data we just classify the data based on whether an attribute value is less than or greater than the certain attribute splitting point The pseudo-code of classifying the original data is shown in Algorithm 2 6.4 Splitting Criterion Using Probability as Weight We use the p S 1  w j  t  and p S 2  w j  t  values as weight to rewrite all the equations as follows freq   C j S i S   w j  S  I w j  C j  p S i  w j   13 In the above equation we calculate the frequency of a class value by using the p S i  w j  as weights I w j  C j is an indicator function and returns 1 if w j has a class label C j  In order to normalize we need to calculate the sum of the total weights i.e sum of the all p S i  w j  values w  S i S   w j  S  p S i  w j  14 for each Instance x j in X from the root node do 1 Classify\(Node N Instance x j  2 if N is a leaf node then 3 use the rule given at the leaf 4 return class value 5 else 6 Based on the attribute A i used in N 7 and the split point t  8 if x j  A i   t then 9 return Classify N.leftChild x j  10 else 11 return Classify N.rightChild x j  12 end 13 end 14 end 15 Algorithm 2  Classify Original Instances Now using the above two de“nitions we are ready to rede“ne the conditional entropy using the p S i  w j values Prob  S i S  freq   C j S i S   w  S i S   15 Info   S i S   k  j 1 Prob  S i S   log 2  Prob  S i S  16 Prob  S i S  freq   C j S i S   w  S i S   17 Info   S i S   k  j 1 Prob  S i S   log 2  Prob  S i S  18 Similarly we need to update information gain of choosing an attribute A with split point t  Info  Test A  S  2  i 1  w  S i S    S   Info   S i S  19 Also in calculating the splitInf o  we need to use w  S i S   splitInf o   Test A   n  i 1  w  S i S    S   log 2   w  S i S    S   20 Now we can plug the above modi“ed de“nitions to original tests to choose the split point and attribute Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 5 
 


gain   Test A  Info  S   Info  Test A  S  21 gainRatio   Test A  gain   Test A   splitInf o   Test A  22 Now we can use the equation  22 to choose a split point and an attribute Please note that the main difference between the original equations used in C4.5 and ours is that we use the probability of being in a certain partition as a weight in the formulas The rational behind this modi“cation is the following If we write the original formulas in clear form we can see that there exists an implicit indicator function that assigns 0 or 1 based on the membership instances to set S 1 and set S 2  Since we can not be sure whether a certain instance is in S 1 or in S 2  we use the probability of being in S 1 or in S 2 as a weight 6.5 Splitting Training Data Set Using Random Path Selection This method is an alternative way to split the training data into two after nding a split point and an attribute Again in our implementation we use a random split based on the p S 1  w j  and p S 2  w j  values splitRandom  w j t   S 1  with prob p S 1  w j  S 2  with prob p S 2  w j  23 Pseudo-code is shown in Algorithm 3 6.6 Classifying the Perturbed Instance Using Random Path Selection For each test instance w j in the perturbed data set W and for each chosen split point in the constructed tree we calculate p S 1  w j  t  p 1  Next we place the instance to the left child of the node with prob p 1 and to the right child with prob 1  p 1  We continue with this until we reach a leaf node The Pseudo-code is shown in Algorithm 4 7 Experimental Results In our experiments we use the data extracted from the census database 1994 Census Income or Income which can be downloaded from University of California Irvine UCI machine learning database repository 1  This data set has fourteen attributes six continuous and eight nominal It altogether has 48842 instances separate as training data 32561 instances and testing data 16281 instances The data is used to predict whether the income exceeds 50 K annually We choose this data set to have fair 1 http://www.ics.uci.edu mlearn/MLSummary.html Partition Node N  1 if Stopping Criteria is Met then 2 return 3 else 4 Using RandomSplittingCriterion Compute 5 the Best Attribute Best A and the Best Splitting Point t  for each Instance w j in Node N do 6 Calculate the p S 1  w j  t  p 1 7 Based on attribute Best A and splitting 8 point t  Let R be a uniform random value between 9 0  1  if R  p 1 then 10 addChild  N.leftChild w j  11 else 12 addChild  N.rightChild w j  13 end 14 end 15 end 16 Partition N.leftChild  17 Partition N.rightChild  18 Algorithm 3  Partition Training Instances Using Random Criteria for each Instance w j in W from the root node do 1 Classify\(Node N Instance w j  2 if N is a leaf node then 3 use the rule given at the leaf 4 return class value 5 else 6 Calculate the p S 1  w j  t  p 1 7 Based on the attribute A i used in N 8 and the split point t  9 Let R be a uniform random value between 10 0  1  if R  p 1 then 11 return Classify N.leftChild w j  12 else 13 return Classify N.rightChild w j  14 end 15 end 16 end 17 Algorithm 4  Classify Noisy Instances Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 
 


comparison with reconstruction based techniques that require relatively large data sets Since in this paper we focus on the numeric continuous attributes we only keep the six numeric continuous attributes in our data set Also for ef“ciency purposes we randomly choose 10000 instances from the training data set and keep all the instances in the testing data set We use the noise addition frame work proposed in and add both Gaussian and Uniform random noise to each attribute When using Gaussian random noise we know that the variance  2 can dramatically affect the results We use four different Gaussian distribution noise data with different variance values To quantify the relative amount of noise added to actual data we used the Signal-to-Noise Ratio SNR that is the ratio of variance  2 of actual data to variance  2 of noise data W e also use the pri v a c y measure mentioned in section 3 to quantify the privacy loss The table 1 shows the ve perturbed data sets with their SNR values and privacy measures In our experiments we only use one data set which perturbed by uniform noise shown as data5 We can see from table 1 when the SNR value is higher the variance  2 of noise data is lower thus the perturbed data preserves less privacy The uniform distributed noise is generated by a given data range We can calculate the SNR for each attribute for uniform noise data in our experimental data set the SNR values for six attribute are 1.3 2.7 1.2 53.6 1.9 and 1.1 respectively Table 1 Privacy measure of different data sets  Data1 Data2 Data3 Data4 Data5 Noise Distribution Gaussian Gaussian Gaussian Gaussian Uniform SNR 1  7 1  3 1  0 0  5 N/A Privacy loss 0  2183 0  1909 0  1619 0  1026 0  2604 7.1 Local vs Global Data Mining First note that by local data mining each participant mines its own data By global data mining we mean that the participants share the data and mine to obtain global patterns As we have mentioned before our approach is suitable for the scenarios where many parties are participating to perform global data mining without compromising their privacy The data sets distributed among each party can be horizontally or vertically partitioned Horizontally partitioned data means the instances are split across the parties and vertically partitioned data means the attributes are split across the parties Experimental results show that for both types of partitioning local data mining results are less accurate compared with those obtained from global data mining This supports the fact that extracting information from globally shared data is better Table 2 C4.5 decision tree classi“er accuracy over horizontally partitioned data Accuracy data1 data2 data3 data4 data5 50 Instance 73.47 74.23 73.13 73.23 73.63 100 Instance 78.54 73.33 77.43 78.13 75.63 Accuracy data6 data7 data8 data9 data10 50 Instance 74.63 72.37 74.23 76.27 77.73 100 Instance 78.77 75.4 78.13 76.2 77.77 We use the data set described in the previous sub-section with six attributes We randomly choose instances to form small data sets with different sizes denoted as group 1 group 2 to group 10 We apply standard C4.5 classi“er on these data sets and the accuracy numbers are shown in Table 1 It is clear that when the number of instances are increased the C4.5 decision tree algorithm has better performance Table 3 C4.5 decision tree classi“er accuracy over vertically partitioned data Accuracy 2 Attributes 3 Attributes 4 Attributes 5K Instance 77.54 77.84 78.9 32K Instance 78.06 78.51 79.05 Similarly we have removed some attributes from the Income data set and then applied standard C4.5 decision tree classi“er on the new data sets and the accuracy of the classi“cation results are shown in Table 2 We can see that when the attribute number is increased the C4.5 decision tree algorithm performs better 7.2 Reconstruction Based Approaches Results For comparison purposes we report the data mining results obtained by using original data distribution reconstruction methods We apply two notable reconstruction techniques to the perturbed data set The rst technique is Bayesian inference estimation BE based approach proposed by Agrawal et al The second technique is the principal component analysis PCA based approach proposed by Kargupta et al Please refer to the original work for the algorithms details We apply the two techniques on the ve data sets We rst reconstruct the original distribution and then use this Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 
 


Table 4 Data Mining accuracy of applying data mining techniques directly on 10k perturbed training data set  Data mining on perturbed data set Decision Tree C4.5 Classi“er Accuracy  Data Set Data1 Data2 Data3 Data4 Data5 Test on Original 79  61 79  04 77  73 77  32 80  29 Test on Perturbed 78  56 78  14 77  45 77  05 80  47 Naive Bayes Classi“er Accuracy  Data Set Data1 Data2 Data3 Data4 Data5 Test on Original 78  45 78  21 78  17 77  99 80  45 Test on Perturbed 78  08 77  78 77  46 76  47 80  29 Table 5 Data Mining accuracy with BE based reconstruction technique BE based reconstruction technique Decision Tree C4.5 Classi“er Accuracy  Data Set Data1 Data2 Data3 Data4 Data5 Test on BE-recon 91  91 90  50 86  26 93  97 86  35 Test on Original 24  45 25  17 69  32 35  96 78  72 Test on Perturbed 38  29 36  47 58  45 43  63 74  06 Original Data Mining Accuracy  83  40 Naive Bayes Classi“er Accuracy  Data Set Data1 Data2 Data3 Data4 Data5 Test on BE-recon 88  68 87  09 84  32 94  59 77  53 Test on Original 26  30 21  66 23  01 23  08 76  46 Test on Perturbed 37  19 39  91 33  09 37  30 49  66 Original Data Mining Accuracy  79  87 estimated distribution to build the data mining models We perform three different tests to compare data mining accuracy In the rst case we test the classi“er on the reconstructed test data in the second case we test the classi“er on the original test data and in the third case we test the classi“er on the perturbed test data The data mining models prediction accuracy is shown in the table 5 and table 6 As comparison table 4 shows the data mining accuracy obtained directly from the perturbed data sets Our results indicate that both reconstruction techniques fail to produce good data mining models This result is not surprising since in general estimating data distributions on nite data is a very hard problem If we use this original data distribution reconstruction phase as a intermediate step to do privacy preserving data mining we may not always get good performance results In the work the authors have investigated three different real world data sets and Table 6 BE based reconstruction technique data mining accuracy PCA based reconstruction technique Decision Tree C4.5 Classi“er Accuracy  Data Set Data1 Data2 Data3 Data4 Data5 Test on PCA-recon 99  23 98  16 98  34 95  10 99  61 Test on Original 70  31 71  71 72  35 68  42 76  86 Test on Perturbed 54  76 63  49 59  17 61  08 61  49 Original Data Mining Accuracy  83  40 Naive Bayes Classi“er Accuracy  Data Set Data1 Data2 Data3 Data4 Data5 Test on PCA-recon 97  83 98  10 97  42 94  33 98  63 Test on Original 66  12 64  29 63  21 59  84 64  81 Test on Perturbed 46  08 41  71 39  07 28  05 51  08 Original Data Mining Accuracy  79  87 Table 7 Proposed PPDTC4.5 data mining accuracy Our Proposed PPDTC4.5 Classi“er Accuracy  PPDTC4.5 Threshold Method Data Set Data1 Data2 Data3 Data4 Data5 Test on Original 80  74 79  69 76  63 77  02 80  29 Test on Perturbed 76  09 76  14 74  41 76  03 80  52 PPDTC4.5 Random Path Selection Method Data Set Data1 Data2 Data3 Data4 Data5 Test on Original 78  72 77  21 77  67 78  01 80  31 Test on Perturbed 78  40 77  77 77  30 77  06 80  32 the reconstruction based approaches have failed on all those data sets These results support our motivation of nding direct ways to perform privacy preserving data mining from perturbed data 7.3 PPDTC4.5 Classi“er Accuracy Using the data sets described earlier we perform different experiments Applying WEKA C4.5 algorithm on the original training data set to build the decision tree and classify the original testing data set we get 83  40 accuracy table 4 shows the data mining accuracy when apply data mining tools directly on the perturbed data sets Table 7 shows data mining accuracy of our proposed PPDTC4.5 algorithms We can see when we use our proposed PPDTC4.5 Threshold Method on these ve data sets to build the decision tree and classify on the original data set We get higher accuracy than which classify on the perturbed data for data 1 and data 2 equivalent accuracy for Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 
 


Figure 1 Threshold Method of PPDTC4.5 Classi“er Accuracy on 10K Uniform Perturbed Training Data 16K Original Test Data Figure 2 Threshold Method of PPDTC4.5 Classi“er Accuracy on 32K Gaussian Perturbed Training Data 16K Original Test Data data 5 lower accuracy on data 3 and data4 The reason is SNR value plays an important role here Table 1 shows different perturbed data sets with different SNR values When SNR less than 1  0 means the variance  2 of noise data greater than the variance  2 of actual data In other words data 3 and data 4 are introduced more noise Our algorithm get good results when the SNR value is greater than 1  0  When The threshold method builds a decision tree classi“er which is not suitable to classify the perturbed data set because our algorithm estimates the splitting point and partition the training data as the original data would have To build a classi“er to classify the perturbed data we use the probability as weight to nd the best splitting point and use random path selection to partition the training data so far the classi“er accuracy has not been improved much compared with directly applying WEKA C4.5 algorithm The reason is when partitioning the training data set and classifying the test data set the random path selection method does not bound the random noise R well In the future we would like to nd a better way to bound the R value to build a better classi“er for classifying the perturbed data As we have seen in our experimental results our proposed PPDTC4.5 classi“ers may not get very excited high accuracy comparing with those obtained from directly applying data mining techniques to the perturbed data sets But comparing with reconstructed based approaches our methods obtain very good results We try to represent the message here is avoiding to solving the hard distribution problem in stead mapping the data mining functions to construct privacy preserving data mining methods This is a promising direction Furthermore our experimental results have also indicated that when huge data set is available white noise is no longer can prevent data mining tools to abstract patterns So directly mining the perturbed data set is also a good approach when the data set is big enough In the PPDTC4.5 threshold method we know that choosing different threshold values affect the data mining accuracy Choosing the threshold to get good data mining results is related to the distribution of the random noise added to the data and the data itself In our experiments when using Uniform distribution random noise to distort the data 0  5 is a good threshold to get a classi“er with high accuracy when using Gaussian distribution random noise to distort the data 0  3 is a good threshold to get a classi“er with high accuracy The relationship between data mining accuracy and threshold values are shown in gure 1 and gure 2 The best threshold should change from data to data In other words this is dependent on the data property 7.4 Algorithm Complexity Given n instances m attributes and p label values the number of potential splitting points t of numeric continuous attribute at most is n  1  The complexity C4.5 algorithm on training phase is O  nlgn  tmp   Our algorithm evaluates the probability for instance w j for every given potential splitting point t  which increases the complexity of algorithm in the worst case scenario to O  ntmp   Since our algorithm skips the steps of reconstruction the original distribution for each attribute the running time is very reasonable comparing with the BE reconstruction algorithm given in In BE reconstruction algorithm there is a stop parameter to determine when to stop the calculation of the estimated distribution The fact is more loops calculation more running time and better accuracy of the estimated distribution In our experiments based on different choice of the stop parameter the running time of the BE reconstruction algorithm is raged from three to ve times longer than our proposed algorithm running on the same con“guration computers Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 
 


8 Conclusion We have proposed a modi“ed C4.5 decision tree classi“er which is suitable for privacy preserving data mining The classi“er is built from the perturbed data set and the model can be used to classify the original data with high accuracy In the scenarios where many parties are participating to perform global data mining without compromising their privacy our algorithm decreases the costs of communication and computation compared with the cryptographybased approaches Our algorithm is based on the perturbation scheme but skips the steps of reconstructing the original data distribution The proposed technique has increased the privacy protection with less computation time In the future we will investigate various ways to build the classi“ers which can be used to classify the perturbed data set As we have mentioned before with a better bound of the random noise data R  using the probability as weighting is an approach that needs further investigation As pointed out before some data mining techniques can be directly applied to perturbed data due to the perturbation process still preserve some nature of the data Naive Bayes classi“er can be directly applied to the additive perturbation data and Euclidean based data mining tools e.g k Nearest Neighbor Classi“er Support Vector Machines and Perceptrons Neural Network can be applied to the multiplicative perturbation data But the data mining accuracy is reduced due to the information loss in the process and some data mining methods themselves may not have good performance As we know k Nearest Neighbor is a simple but poor performance classi“er Do we have the exibility to choose different data mining tools In this paper we provide a new direction which is modifying data mining functions to suit the perturbed data This absolutely enable us more choices Our proposed method skips the reconstructing the original data distribution from the perturbed data In this way the method performs privacy preserving data mining without solving the hard distribution problem Data mining techniques are used to derive patterns and high level information from data Data mining results do not cause the violation of privacy One thing bring to our notice is that when data set is big enough perform data mining techniques directly on the perturbed data sets can obtain good data mining accuracy For example applying decision tree classi“er to additive perturbation data can get good data mining accuracy This can be observed in our experimental results Privacy as a security issue in data mining area is still a challenge References  C C Aggarw al and P  S Y u  A condensation approach to privacy preserving data mining  D Agra w a l and C C Aggarw al On the design and quanti“cation of privacy preserving data mining algorithms In PODS  ACM 2001  R Agra w a l and R Srikant Pri v a c y-preserving data mining In SIGMOD Conference  pages 439…450 2000  K Chen and L Liu Pri v a c y preserving data classi“cation with rotation perturbation In ICDM  pages 589…592 2005  C Clifton M Kantarcioglu J V aidya X Lin and M Y  Zhu Tools for privacy preserving data mining SIGKDD Explorations  4\(2 2002  W  Du and Z Zhan Using randomized response techniques for privacy-preserving data mining In KDD  pages 505 510 2003  A V  Ev“mie vski R Srikant R Agra w al and J Gehrk e Privacy preserving mining of association rules In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  pages 217…228 2002  M Kantarcioglu and C Clifton Pri v ately computing a distributed k-nn classi“er In J.-F Boulicaut F Esposito F Giannotti and D Pedreschi editors PKDD  volume 3202 of Lecture Notes in Computer Science  pages 279…290 Springer 2004  H Kar gupta S Datta Q W ang and K Si v akumar  O n the privacy preserving properties of random data perturbation techniques In ICDM  pages 99…106 IEEE Computer Society 2003  Y  Lindell and B Pinkas Pri v a c y preserving data mining In M Bellare editor CRYPTO  volume 1880 of Lecture Notes in Computer Science  pages 36…54 Springer 2000  K Liu H Kar gupta and J Ryan Random Projection-Based Multiplicative Data Perturbation for Privacy Preserving Distributed Data Mining IEEE Transactions on Knowledge and Data Engineering TKDE  18\(1 January 2006  L Liu M Kantarcioglu and B Thuraisingham The applicability of the perturbation based privacy preserving data mining for real-world data Data and Knowledge Engineering Journal  2007  T  M Mitchell Machine Learning  mcgraw-hill 1997  J R Quinlan C4.5 Programs for Machine Learning  Morgan Kaufmann 1993  J R Quinlan Impro v e d use of continuous attrib utes in c4.5 J Artif Intell Res JAIR  4:77…90 1996  S Rizvi and J R Haritsa Maintaining data pri v a c y in association rule mining In VLDB  pages 682…693 Morgan Kaufmann 2002  B M Thuraisingham Pri v a c y constraint processing in a privacy-enhanced database management system Data Knowl Eng  55\(2 2005  J V aidya and C Clifton Pri v a c y-preserving means cluster ing over vertically partitioned data In KDD  pages 206…215 2003  I H W itten and E Frank Data Mining Practical Machine Learning Tools and Techniques  Morgan Kaufmann San Francisco 2 edition 2005  S Xu J Zhang D Han and J W ang Singular v alue decomposition based data distortion strategy for privacy protection Knowl Inf Syst  10\(3 2006 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 
 


target item falls in TH category. To improve effectiveness of a segment attack, we select filler items from the set of items which are highly rated by those users who have rated target item at a higher Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 scale. The strategy used is similar to Strategy IH Filler items are selected the way explained in section 6.2  8. Experimental evaluation and discussion  We performed the experimental evaluation of our strategies on the publicly available MovieLens data set [8]. This is the most widely used dataset in recommender systems research. MovieLens consists of 100,000 ratings made by 943 users on 1682 movies. Each user in the data set has rated at least 20 movies and each movie has been rated at least once A timestamp value is associated with each user movie, and rating combination. The data set also contains information on the demographic detail \(age sex, occupation, and zip code information \(genre and release date The ratings are made in a scale of 1 to 5, where 5 indicate extreme likeness for an item and 1 dislike We evaluated effectiveness of the proposed strategies on user-based and item-based collaborative algorithm. For similarity calculation and prediction in user-based CF algorithm, equations 1 and 2 stated in section 3 were used. Similarly, equations 3 and 4 stated in section 3 were used for computing similarity and prediction value for item-based CF algorithm We used a neighborhood size of k = 20 for prediction calculation. Case amplification value of 10 was used while calculating correlation and only positive correlations values were considered for computing predictions To conduct our evaluation, we selected a sample 20 items. Out of the 20 items, 10 items belonged to TL  category while remaining 10 items to TH category All the 20 items were selected randomly from a larger set of items belonging to each category. We also randomly selected a sample of 50 target users Target users selected were those who have never rated any of the 20 test items. Each of the target items was attacked individually and the prediction shift was calculated by averaging the prediction shift observed for each user. The final prediction shift for the attack is the average prediction over all items used in the test. Equation 6 was used to calculate the metric For implementation of segmented attack we followed the same guidelines as stated in [3]. Horror segment was selected as the target segment. Five of the most popular horror movies were selected to represent the segment. These five movies selected formed the selected item set in the attack profiles constructed. The five movies are Alien, Psycho, The Shining, Jaws, and The Birds. Users who have given a rating of 4 or 5 to at least any 3 of the five movies were identified as the target segment against which the attack was focused. For calculating prediction shift we selected 50 of the users from this target segment to form the test user set. While implementing the segment attack, selected items were given a rating of 5 and the randomly selected filler items were assigned a value 1 All experiments were conducted for ?Size of attack? values 1%, 3%, 6%, 12%, and 15%.  ?Size of attack? represents number of attack profiles added as a percentage of pre-attack profiles. 1% ?Size of attack? implies 10 attack profiles were added to a 


attack? implies 10 attack profiles were added to a system of 1000 genuine users. On the basis of the results reported in [4] that best results are reported when a filler size of 3% is used in an average attack we used a filler size of 3% for all our tests i.e., 3 % of 1682 items which is approximately 50 filler items For attacks against user-based collaborative filtering systems we used six strategies: Strategy UL, Strategy UH, Strategy SUL, Strategy SUH, segment attack and average attack. Similarly, for attacks against item based collaborative filtering systems we used six strategies: Strategy IL, Strategy IH Strategy SIL Strategy SIH, segment attack and average attack. For average attack, filler item strategy used was the same as in an average attack i.e., the mean of the filler item was assigned to it. Segment attack was implemented as explained earlier. Category TL, Category TH Strategy UL, Strategy UH, Strategy IL, Strategy IH Strategy SUL, Strategy SUH, Strategy SIL and Strategy SIH were implemented the way explained earlier in section 4, section 5, section 6 and section 7 respectively. For attacks against item-based CF while selecting filler items from set IF, only items with minimum frequency count of 10 were considered Figure 2 and Figure 3 show the effectiveness of our attacks when calculated for all users against systems using user-based collaborative filtering for recommendations.  Figure 2 shows the prediction shift values of attacks Strategy UL and average attack for items belonging to TL category. From the graph it?s obvious that for items in TL category, Strategy UL outperforms average attack model for all values of attack size. Similarly, Figure 3 shows the prediction shift values for the attack strategies Strategy UH and average attack for items belonging to TH category From the graph it can be concluded that for items belonging to TH category, Strategy UH performs much better than average attack over lower values of attack size. At attack size of 12 % and 15% both attack have similar effectiveness Figure 4 and Figure 5 show the effectiveness of our attacks when calculated for all users against systems using item-based collaborative filtering for recommendations.  Figure 4 shows the prediction shift values of Strategy IL and average attack for Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 items belonging to TL category. Similarly, Figure 5 shows the prediction shift values for the Strategy IH and average attack for items belonging to TH category. From Figure 4 and 5 it can be concluded that both Strategy IL and Strategy IH perform substantially better than average attack over all values of attack size. It can also be observed that our attack strategies are more effective against itembased systems than user-based systems Figure 6, 7, 8, and 9 show the effectiveness of our filler based attack strategies for in-segment users. We observe that for attacks against both user-based and item-based CF systems the effectiveness of our filler based strategies is comparable to the best available attack model for in-segment attacks i.e., segment attack. However, in Figure 8 we observe that filler item strategy SIL performs better than segment attack. Because of the low knowledge cost involved in segment attack, we can conclude that for most scenarios segment attack is a better attack model for in-segment attacks than filler based attack models Experimental results clearly show that our approach of selecting a strategy based on target item rating distribution outperforms the best available attack model i.e., average model. One drawback of 


attack model i.e., average model. One drawback of our attack strategies is its high knowledge cost However, automated software agents can help diminish the cost. One approach that can be used to decrease the cost is to use a subset of users while selecting filler items. For example, in attacks against item-based systems, while implementing Strategy IH instead of selecting all users who have rated target item as 4 or 5 as members of the set UH. , we only select 20 users. Selection of items for set IF will then be performed using the data of the 20 users in set UH Similarly, in case of attacks against user-based systems, while implementing Strategy UH instead of assigning a filler item IF the average rating given to it by the set of users UH. , we assign IF the average rating given to it by a subset of 5 randomly selected users from UH. In future work we plan to experimentally verify the effectiveness of these cost reduction approaches    Figure 2:   Attack on TL category of items against user-based collaborative filtering system   Figure 3:   Attack on TH   category of items against user-based collaborative filtering system   Figure 4:   Attack on TL category of items against item-based collaborative filtering system  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8  Figure 5:   Attack on TH category of items against item-based collaborative filtering system   Figure 6:   Attack on TL category of items against user-based collaborative filtering system   Figure 7:   Attack on TH   category of items against user-based collaborative filtering system    Figure 8:   Attack on TL category of items against item-based collaborative filtering system   Figure 9:   Attack on TH category of items against item-based collaborative filtering system  9. Conclusion  This paper provides an effective approach towards constructing attack models. We show the importance of target item and filler items in construction of successful attack strategies. Through experiments we show that our approach of intelligent selection of filler items based on target item rating distribution results in substantial improvement over the baseline average attack. We also compare our approach with the well known in-segment approach and conclude that our approach gives slightly improved results. In future, we plan to examine the filler items strategies for other attack models, and also create algorithms to improve robustness and stability of recommender systems against shilling attacks 


systems against shilling attacks  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9  10. References  1] Lam, S., and Riedl, J. 2004. Shilling Recommender Systems for Fun and Profit, In Proceedings of the 13th International WWW Conference 2] Mehta, B., Hofmann, T., and Nejdl, W. 2007. Robust Collaborative Filtering, In Proceedings of the 2007 ACM Conference on Recommender Systems, 49-56 3] Mobasher, B., Burke, R., Bhaumik, R., and Williams C. 2007. Towards Trustworthy Recommender Systems: An Analysis of Attack Models and Algorithm Robustness, ACM Transactions on Internet Technology, 7\(2007 4] Burke, R., Mobasher, B., and Bhaumik, R. 2005 Limited Knowledge Shilling Attacks in Collaborative Filtering Systems, In Proceedings of Workshop on Intelligent Techniques for Web Personalization 5] Konstan, J., Miller, B., Maltz, D., Herlocker, J Gordon, L., and Riedl, J. 1997.  GroupLens: Applying Collaborative Filtering to Usenet News Communications of the ACM, 40, 3\(1997 6] Herlocker, J., Konstan, J., Borchers, A., and Riedl J.1999. An Algorithm Framework for Performing Collaborative Filtering, In Proceedings of  SIGIR ACM, 77-87 7] Sarwar, B., Karypis, G., Konstan, J., and Riedl, J 2001. Item-based Collaborative Filtering of Recommendation Algorithms. In Proceedings of the 10th International WWW Conference 8] MovieLens data set,www.grouplens.org  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


 Current Time \(min  Healthy Failure Expected Just-in-time line Actual Remaining Life  Figure 17. Results of failure prognosis 0 100 200 300 400 500 600 700 800 900 1000 0 0.02 0.04 0.06 0.08 0.1 0.12 Time \(min Sp al l S iz e  m m 2 Interpolation of spall growth according to feature values 0 100 200 300 400 500 600 700 800 900 1000 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Time \(min Fe a tu re V a lu e    Interpolation of feature value with noise Interpolation of feature vlaue snapshot with ground truth data 11 CONCLUSIONS This paper shows that enhancements to diagnostic techniques are desirable as well as attainable additions to Health and Usage Monitoring Systems \(HUMS particularly in the case of rotorcraft component monitoring Enhancements like those presented support CBM efforts primarily in two ways: reduce the sensitivity of diagnostic processes to both signal noise and variations in environmental and operating conditions, and improve the performance of detection systems as well as the task of fault identification \(e.g., severity quantification instantiation of reliable prognostics Representative examples, motivated by the interest of the U.S. Army in transitioning from time-based \(using TBO definitions drive train bearing, illustrates the potential benefits of 


pursuing an integrated approach to diagnostics and prognostics, combining technologies for enhanced data preprocessing, advanced diagnostic-support algorithms, fusion at the sensor/feature levels, and an adequate framework for false alarm mitigation and uncertainty management. An architecture for achieving such integration is presented, with emphasis on supporting a robust performance of diagnostics operations, even in the presence of such kinds of disturbances as those observed in data acquired by HUMS vibration sensors. The present study also gives relevance to seeded fault because the technologies discussed can integrate knowledge about damage mechanism interactions or physics-of-failure models, as well as make use of multiple-sensor and multiple-feature data sets representative of known fault conditions For this reason, the team behind this project is evaluating a potential opportunity to perform a series of tests on rotorcraft drive train bearings with varying fault severities and under multiple, though realistic, operating conditions Such tests are being planned to provide algorithm/model validations, as well as diagnostic/prognostic performance assessments, in support of providing the U.S. Army with technologies that make detection systems more robust allow for the implementation of prognostics, and extend the useful life of drive train components. Component degradation testing thus remains as future, follow-up work to the research reported in this document ACKNOWLEDGMENTS This work has been partially supported with a cooperative agreement by the Army Research Laboratory under contract number W911NF-07-2-0075. In addition to the primary authors, we would like to thank government and contractor representatives from organizations supporting the Army Utility \(Blackhawk Estes, Mr. Carlos Rivera, and Dr. Jon Keller. This work has also benefitted greatly from consultations with other Army Research Laboratory and NASA Glenn researchers such as Dr. Timothy Krantz, Dr. David Lewicki, Dr. Harry Decker Dr. Hiralal Khatri, Mr. Ken Ranney and Mr. Kwok Tom REFERENCES 1] Branhof, R.W., Grabill, P., Grant, L., and Keller, J.A  Application of Automated Rotor Smoothing Using Continuous Vibration Measurements  American Helicopter Society 61st annual forum, Grapevine, Texas June 1  3, 2005 2] Dora, R., Wright, J., Hess, R., and Boydstun, B  Utility of the IMD HUMS in an Operational Setting on the UH60L Blackhawk  American Helicopter Society 60th annual forum, Baltimore, Maryland, May 7  10, 2004 3] Zakrajsek, J.J., Dempsey, P.J., et al  Rotorcraft Health Management Issues and Challenges  NASA report TM  2006-214022. February, 2006 4] Suggs, D.T., and Wade, D.R  Vibration Based Maintenance Credits for the UH-60 Oil Cooler Fan Assembly  American Helicopter Society, CBM Specialists Meeting, Huntsville, Alabama, February 13 2008 5] Baker, C., Marble, S., Morton, B.P., and Smith, B.J  Failure Modes and Prognostic Techniques for H-60 Tail Rotor Drive System Bearings  IEEEAC paper #1122 IEEE, 2007 6] Keller, J.A., Branhof, R., Dunaway, D., and Grabill, P  Examples of Condition Based Maintenance with the Vibration Management Enhancement Program   American Helicopter Society 61st Annual Forum Grapevine, Texas, June 1  3, 2005 7] Zhang, B., Sconyers, C., Byington, C.S., Patrick, R Orchard, M.E., and Vachtsevanos, G.J  Anomaly Detection: A Robust Approach to Detection of 


Detection: A Robust Approach to Detection of Unanticipated Faults  International Conference on Prognostics and Health Management, Denver, Colorado October 6-9, 2008 8] Byington, C.S., Watson, M., Lee, H., and Hollins, M  Sensor-level Fusion to Enhance Health and Usage Monitoring Systems  American Helicopter Society, 64th Annual Forum, Montreal, Canada, April 29-May 1, 2008 9] Engel, S.J., Gilmartin, B.J., Bongort, K., and Hess, A  Prognostics, the Real Issues Involved With Predicting Life Remaining  Proceedings of the IEEE Aerospace Conference, Big Sky, Montana, March 18-25, 2000 12 BIOGRAPHY Romano Patrick is a Project Manager at Impact Technologies. He received a Ph.D. in Electrical Engineering from the Georgia Institute of Technology specializing in model-based machine health diagnostics and prognostics. He also holds an MBA from Georgia Tech and degrees from U Texas, Arlington and U. Panamericana, Mexico. With career focus on interdisciplinary integration of technologies, his recent work involves practicable diagnostics/prognostics design for complex systems, such as rotorcraft drive trains Past experience includes automation and design for a variety of industrial and government sponsors \(DARPA, Lockheed Martin, Northrop Grumman, etc and program coordination at U. Panamericana, and some entrepreneurial R&amp;D Matthew J. Smith is a Senior Project Engineer at Impact Technologies. During his tenure with Impact, Matthew has performed multiple efforts pertaining to bearing vibration analysis, diagnostic and prognostic system development, and experimental study of faulted system reponse and fault progression. Previously, as a research assistant at Penn State and the NASA Glenn Research Center, Matthew performed experimental and analytical oil-free bearing analyses Matthew received his B.S. and M.S. degrees in Mechanical Engineering from The Pennsylvania State University. His research interests include: prognostic health assessment for bearing and actuator systems, grease degradation modeling and fault classifier development Bin Zhang received his Ph.D. degree from Nanyang Technological University, Singapore in 2007. He received his BE and MSE degrees from Nanjing University of Science and Technology, China, in 1993 and 1999, respectively. He is a senior member of IEEE. From 2005 to present, he has been a Post-Doc with the School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta GA His current research interests are fault diagnosis and failure prognosis, systems and control, digital signal processing learning control, intelligent systems and their applications to robotics, power electronics and various mechanical systems Carl S. Byington is a Professional Engineer and the Director of Systems Engineering at Impact Technologies. He directs R&amp;D in pursuit of advanced, automated systems health management for land-based, shipboard, and airborne machinery for military and commercial customers. He is Chairman of the Machinery Diagnostics &amp; Prognostics Committee of ASME and a member of IEEE, AIAA, SAE and AHS. He has a BS degree in Mechanical Engineering from the University of Pennsylvania and an MS in Aeronautical Engineering from George Washington University, and has published over 60 papers, book chapters, magazine and journal articles related to diagnostics and prognostics technologies George Vachtsevanos is Professor Emeritus at the Georgia Institute of Technology and also serves as the Chief Scientist at Impact Technologies, LLC. He directed the Intelligent Control Systems laboratory at Georgia Tech for the past 28 years where faculty and students are conducting research in fault diagnosis/prognosis and fault-tolerant control of engineering systems, intelligent control of industrial 


engineering systems, intelligent control of industrial processes, neurotechnology and cardiotechnology, and unmanned systems. His research work has been sponsored by government and industry and has published over 250 technical papers in his area of expertise. He is the lead author of a book on "Intelligent Fault Diagnosis and Prognosis of Engineering Systems" published by Wiley in 2006. He is the recipient of the Georgia Tech Interdisciplinary Activities award and the ECE Distinguished Professor award Romeo de la Cruz del Rosario, Jr. is the Chief of the Electronics Technology Branch at the U.S. Army Research Laboratory. He also serves as the Army Technology Objective ATO P&amp;D Operational Readiness and Condition Based Maintenance He received the B.E.E. degree from the Catholic University of America, Washington, D.C., and the M.S.E. and Ph.D degrees in Electrical and Computer Engineering from the Johns Hopkins University, Baltimore, MD. Since 1991 he has been an engineer at the Harry Diamond Laboratory then U.S Army Research Laboratory working in several areas including high power microwave technology characterization &amp; modeling of heterostructure RF devices and fabrication and failure analysis of electron devices and circuits  pre></body></html 


movies. In the case of the volume of critical reviews however, there was no big difference between mainstream and non-mainstream movies. WOM and critical reviews were usually positive H1 and H2 tested the relationship between WOM and weekly box office revenue, and the results supported the hypotheses. The volume of WOM was positively related to weekly box office revenue, while the valence of WOM had no significant effect. H3 H4a, and H4b tested the impact of critical reviews, and the results also supported the hypotheses except H4b The volume and valence of critical reviews had no consistent significances to weekly box office revenue H3 H4b. Table 7 showed that the number of critical reviews was statistically significant to aggregate box office revenue \(H4a for the attitude of critical reviews \(H4b the result more detail, an additional test was performed using only those factors related to critical reviews as independent variables. The result of the additional test supported H4, but the signs were reversed, i.e. positive critical reviews had minus signs, and negative critical reviews had plus signs. This reversed signs imply that the preference of critical reviewers is very similar to that of normal moviegoers. H5s and H6s tested the different effects of WOM and critical reviews on mainstream and non-mainstream movies. The result failed to determine that WOM give different impact on mainstream and non-mainstream movies, so H5a and H5b were rejected. H6, however, was supported, i.e the effects of critical reviews were different for mainstream and non-mainstream movies. There were no significant relationships between critical reviews and aggregate box office revenue in mainstream movies. For non-mainstream movies, however, the volume of critical reviews and the percentage of negative critical reviews were significant. Nonmainstream movies have fewer sources from which consumers can get information, and this might explain the results The above findings lead to several managerial implications. First, producers and distributors of movies could forecast weekly box office revenue by looking at previous weeks? volume of WOM. It does not matter what attitude people have when they spread WOM, the important factor is its volume. Therefore producers and distributors need to develop an appropriate strategy to manage WOM for their movies For example, the terms related to WOM marketing such as buzz and viral marketing are easily found Second, for the distributors who usually distribute less commercial and more artistic movies, and consequently have a smaller market compared to the major distributors, critical reviews can impact their movies box office revenues in a significant way. There are usually fewer sources for information for nonmainstream movies than mainstream movies, and so small efforts could leverage the outcomes. Finally, for those who are dealing with mainstream movies, the finding that the valence of WOM and critical reviews do not have significant relationship with box office revenue can have certain implications. Particularly, the attitude of critical reviews showed reversed effects Therefore, they may need to concentrate on other features rather than attitude of moviegoers or critical reviews, such as encouraging moviegoers to spread WOM This study contributes to the understanding of the motion picture industry, especially the relationship between box office revenue and WOM including critical reviews. There are existing studies that already 


critical reviews. There are existing studies that already dealt with similar issues, but this study has some differentiated features compare to prior studies. First the data used in this study was collected from South Korea, while most of the relevant studies usually focus on the North American market. This helps to provide the opportunity to understand the international market especially the Asian market, even though South Korea is a small part of it in terms of the motion picture industry. Second, movies were categorized to two groups, i.e. mainstream and non-mainstream and this study attempted to determine how WOM impacts these categories differently by testing several hypotheses In this study, there are also several limitations that could be dealt with in future research. First, using box office revenue as a dependent variable is more meaningful for distributers rather than producers. Due to there is close correlation between box office revenue and number of screens, one of producers? main concerns is how many screens their movies can be played on. Moreover, DVD sales are also important measurement for success of movies these days, and so it also could be a dependent variable. Therefore, it could be possible to give more fruitful managerial implications to various players in the motion picture industry by taking some other dependent variables Second, in this study, movies were categorized simply as mainstream and non-mainstream movies, but there could be further studies with diverse techniques of movie categorizations. For example, it would be possible to study the varying influence of WOM or critical reviews on different genres or movie budgets Third, an interesting finding of this study is that positive critical reviews could have negative relationship with box office revenue while negative critical reviews could have positive relationship. This study tried to provide a reasonable discussion on the issue, but more studies could be elaborate on it  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 References  1] Dellarocas, C., The Digitization of Word of Mouth Promise and Challenges of Online Feedback Mechanisms Management Science, 2003. 49\(10 2] Bone, P.F., Word-of-mouth effects on short-term and long-term product judgments. Journal of Business Research 1995. 32\(3 3] Swanson, S.R. and S.W. Kelley, Service recovery attributions and word-of-mouth intentions. European Journal of Marketing, 2001. 35\(1 4] Hennig-Thurau, F., et al., Electronic word-of-mouth via consumer-opinion platforms: What motivates consumers to articulate themselves on the internet? Journal of Interactive Marketing, 2004. 18\(1 5] Fong, J. and S. Burton, Electronic Word-of-Mouth: A Comparison of Stated and Revealed Behavior on Electronic Discussion Boards. Journal of Interactive Advertising, 2006 6\(2 6] Gruen, T.W., T. Osmonbekov, and A.J. Czaplewski eWOM: The impact of customer-to-customer online knowhow exchange on customer value and loyalty. Journal of Business Research, 2006. 59\(4 7] Garbarino, E. and M. Strahilevitz, Gender differences in the perceived risk of buying online and the effects of receiving a site recommendation. Journal of Business Research, 2004. 57\(7 8] Ward, J.C. and A.L. Ostrom, The Internet as information minefield: An analysis of the source and content of brand information yielded by net searches. Journal of Business Research, 2003. 56\(11 


9] Goldsmith, R.E. and D. Horowitz, Measuring Motivations for Online Opinion Seeking. Journal of Interactive Advertising, 2006. 6\(2 10] Eliashberg, J., A. Elberse, and M. Leenders, The motion picture industry: critical issues in practice, current research amp; new research directions. HBS Working Paper, 2005 11] S&amp;P, Industry surveys: Movies and home entertainment 2004 12] KNSO, Revenue of Motion Picture Industry 2004 Ministry of Culture, Sports, and Tourism, 2004 13] Duan, W., B. Gu, and A.B. Whinston, Do Online Reviews Matter? - An Empirical Investigation of Panel Data 2005, UT Austin 14] Zhang, X., C. Dellarocas, and N.F. Awad, Estimating word-of-mouth for movies: The impact of online movie reviews on box office performance, in Workshop on Information Systems and Economics \(WISE Park, MD 15] Mahajan, V., E. Muller, and R.A. Kerin, Introduction Strategy For New Products With Positive And Negative Word-Of-Mouth. Management Science, 1984. 30\(12 1389-1404 16] Moul, C.C., Measuring Word of Mouth's Impact on Theatrical Movie Admissions. Journal of Economics &amp Management Strategy, 2007. 16\(4 17] Liu, Y., Word of Mouth for Movies: Its Dynamics and Impact on Box Office Revenue. Journal of Marketing, 2006 70\(3 18] Austin, B.A., Immediate Seating: A Look at Movie Audiences. 1989, Wadsworth Publishing Company 19] Bayus, B.L., Word of Mouth: The Indirect Effects of Marketing Efforts. Journal of Advertising Research, 1985 25\(3 20] Faber, R.J., Effect of Media Advertising and Other Sources on Movie Selection. Journalism Quarterly, 1984 61\(2 21] Eliashberg, J. and S.M. Shugan, Film critics: Influencers or predictors? Journal of Marketing, 1997. 61\(2 22] Reinstein, D.A. and C.M. Snyder, The Influence Of Expert Reviews On Consumer Demand For Experience Goods: A Case Study Of Movie Critics. Journal of Industrial Economics, 2005. 53\(1 23] Gemser, G., M. Van Oostrum, and M. Leenders, The impact of film reviews on the box office performance of art house versus mainstream motion pictures. Journal of Cultural Economics, 2007. 31\(1 24] Wijnberg, N.M. and G. Gemser, Adding Value to Innovation: Impressionism and the Transformation of the Selection System in Visual Arts. Organization Science, 2000 11\(3 25] De Vany, A. and W.D. Walls, Bose-Einstein Dynamics and Adaptive Contracting in the Motion Picture Industry Economic Journal, 1996. 106\(439 26] Bagella, M. and L. Becchetti, The Determinants of Motion Picture Box Office Performance: Evidence from Movies Produced in Italy. Journal of Cultural Economics 1999. 23\(4 27] Basuroy, S., K.K. Desai, and D. Talukdar, An Empirical Investigation of Signaling in the Motion Picture Industry Journal of Marketing Research \(JMR 2 295 28] Neelamegham, R. and D. Jain, Consumer Choice Process for Experience Goods: An Econometric Model and Analysis. Journal of Marketing Research \(JMR 3 p. 373-386 29] Lovell, G., Movies and manipulation: How studios punish critics. Columbia Journalism Review, 1997. 35\(5 30] Thompson, K., Film Art: An Introduction. 2001 McGraw Hill, New York 31] Zuckerman, E.W. and T.Y. Kim, The critical trade-off identity assignment and box-office success in the feature film industry. Industrial and Corporate Change, 2003. 12\(1 


industry. Industrial and Corporate Change, 2003. 12\(1 27-67 32] KOFIC, Annual Report of Film Industry in Korea 2006 Korean Film Council, 2006 33] Sutton, S., Predicting and Explaining Intentions and Behavior: How Well Are We Doing? Journal of Applied Social Psychology, 1998. 28\(15 34] Basuroy, S., S. Chatterjee, and S.A. Ravid, How Critical Are Critical Reviews? The Box Office Effects of Film Critics Star Power, and Budgets. Journal of Marketing, 2003. 67\(4 p. 103-117  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 





