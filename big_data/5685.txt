Abstract 
Survey of Research on Big Data Storage 
  
With the development of cloud computing and mobile Internet, the issues related to big data have been concerned by both academe and industry. Based on the analysis of the existed work, the research progress of how to use distributed file system to meet the challenge in storing big 
  Xiaoxue Zhang College of Computer and Information Hohai University Nanjing, China zhang_zxx@163.com Feng Xu College of Computer and Information Hohai University Nanjing, China njxufeng@163.com   
data is expatiated, including four key techniques: storage of small files, load balancing, copy consistency, and deduplication. We also indicate some key issues need to concern in the future work 
Keywords-big data; storage;distributed file system 
 
I 
 I NTRODUCTION  According to IDC \(Internet Data Center\ the global big data will increase by 50 times in next decade. How to store these fast-growing, vast amounts of data? How to analysis and process these big data? A series of related problems become a common challenge faced by all enterprises, also become today's research focus. The reason why big data 
takes so much attention cab be summarized as follows: the network terminal changed from a single desktop to multiterminal such as desktop, tablet PCs, book-reader PCs mobile phones, television and so on, which greatly expands the content and scope of the network services, and improves the user's reliance on the Internet; the rapid growth of network users and the average time a user spends on the network, which makes a significant increase in the user network behavior data; the network services have changed from a single form of words to the multimedia form such as pictures, voice, video, leading to significant increase in the amount of data. All above lead to the increasing amount of 
data. Not just the Internet, the phenomenon of large data already exists in the fields of physics, biology, environmental ecology, automatic control and other scientific area. Whatês more, big data also exist in military, communications finance and other industries for some time. It is not a new problem, it is widespread. There is no in-depth study on it because of restricted by condition in the past. But now, with the hardware costs come down and more and more develop in science and technology, people can focus into the big data which implied the great value of II 
 
T HE C 
HARACTERISTICS OF B IG D ATA  The main sources of big data are data information within the organization, sensory information in the Internet of things and interactive information in the Internet world. It is difficult to form a unified concept of big data. IDC defines it this way: Big data technologies describe a new generation of technologies and architectures, designed to economically extract value from very large volumes of a wide variety of data, by enabling high-velocity capture, discovery, and analysis [1  Th is d e f i n i tio n in clu d es th r ee m a in  characteristics of big data .Two other characteristics seem 
relevant: value and complexity. Summarize as ç4V+C 
  
  
Volume: The data volume is huge, especially the huge amounts of data generated by a variety of devices, the size of the data is very large, much larger than the flow of information on the Internet PB level will be the norm. This implies the need for large storage space and computing power Velocity: The data related to perception transmission, decision making, control open loop have very high requirements on real-time data processing. The late result is of little value. It is essentially different with traditional data processing 
   
   
Variety: Big data may be blogs, videos, pictures location information, etc. Even with the same kind of data, encoding, data format or other aspects may be different Value: Big Data having a high commercial value. It is the reason why more and more people are concerned about the big data. But the density of the value is low. Take the video for example, during uninterrupted monitoring process, useful data may only few seconds 001  Complex: Big Data is not only complex and diverse 
in data types and representation. There is often a complex dynamic relationship between each data The change of one data may have an impact to a lot of data III 
 
B IG D ATA S TORAGE C HALLENGES  With the expansion of the application scale, the amount of data will show the trends of explosive growth. The conventional data storage system reaches a bottleneck 
2013 12th International Symposium on Distributed Computing and Applications to Business, Engineering & Science 978-0-7695-5060-2/13 $26.00 © 2013 IEEE DOI 10.1109/DCABES.2013.21 76 


 
 
 
       
  
Characteristics Social networks Challenges 
 Large amount of data Large amount of data and growing rapidly Real-time Intelligence Security Reliability Integrity Low consumption High concurrency High efficiency Many kinds of data Unstructured Semi-structured Structured High potential value Social relationship mining Personalized recommendation Large fluctuations 7-8 times unable to complete the operational task timely. Storage systems face the challenge of complex big data applications Take social networks for example, as shown in the TABLE I 
A Small Files Problem B Load Balancing 
Because the fluctuation of load is high and uncertain the storage system for big data needs to dynamically match the load characteristics Because of the need of high real-time in big data applications, the delay and the length of IO path in data processing should be reduced High accumulation of the amount of data, high concurrent user access and high data growth require the storage system large capacity, high aggregate bandwidth, high IOPS Currently, there are many storage technologies to deal with big data, such as distributed file system, new type of database based on MapReduce and so on. On one hand, the great value in big data promotes development on these technologies. On the other hand, these technologies provide technical support for big dat a storage and make big data becoming a research hotspot in recent years. Existing technology just can meet the demand of big data storage barely. So many improvements are needed and have high research value IV D ISTRIBUTED F ILE S YSTEM  File system is the foundation of upper layer application 2  W i t h t h e c o nt i nuo us de ve l o p m e n t o f i n t e r n e t  applications, data is growing rapidly. So the large-scale data storage became the arduous task of the companies and research focus. Because of the limit on the expansion of storage capacity, traditional storage system is difficult to meet the big data storage So have to use the distributed file system to transfer system load to multiple nodes. By grouping hard disks on multiple nodes into a global storage system, distributed file system provides polymeric storage capacity and I/O bandwidth, and easy to extension according to the system scale. Generally speaking, the mainstream distributed file systems, such as Lustre, GFS, HDFS, separately store metadata and application data because of the different characteristics of store and access. Divide and rule can improve the performance of the entire file system significantly. Despite these advantages, distributed file system still has many shortcomings when it faces explosive growth of data, complex variety of storage needs. Then these shortcomings are paid more and more attention, and become the research focus today The distributed file systems, such as GFS and HDFS, are designed for larger files. But most of the data on the Internet represent by the high frequency of small files, and more storage access are for small files in the application TABLE II shows the problems of traditional distributed file system in the small file management aspect Small file access frequency is higher, need to access the disk for many times, so the performance of the I/O is low Small files will form a large amount of metadata which can affect the management and retrieval performance of metadata server, and cause overall performance degradation Because the file is relatively small, easy to form file fragmentation resulting in a waste of disk space To create a link for each file can easily lead to network delays There are some common optimization methods [3-1 as  the TABLE II shown. Of course, this is not comprehensive In [11 th e f i les b e l o n g t o sa m e d i r e c t o r y w ill w r itten in t o  the same block as much as possible. This will help increase the task speed distributed by MapReduce in the future In order to make the multiple nodes can be a very good to complete the task together , a variety of load balancing algorithm is proposed to eliminate or avoid unbalanced load data traffic congestion and long reaction time The load balancing [10-12 n be  do ne b y t w o w a y s   One is to prevent. One is to prevent. The I/O request is equally distributed on each storage node by the right I/O scheduling policy, so each node can be a very good job, and the situation that a part of the nodes are overloaded, but another part of the nodes are light load will not be happened Another way is to adjust after happened. When load imbalance phenomenon has emerged, it can be eliminated effectively by migration or copy. In general, load balancing algorithm through many years research has been relatively mature, usually divided into two types: static load balancing algorithm and dynamic load balancing algorithm Static load balancing algorithm has nothing to do with the current state of the system. It assigns the task by experience or pre-established strategy. This algorithm is easy, and spends little. But it does not consider the dynamic changes of the system status information, so it has blindness It is mainly suitable for smaller and homogeneous service systems. Classical static load balancing algorithm includes polling algorithm, ratio algorithm, priority algorithm Dynamic load balancing algorithm assigns the I/O task or adjust the load between nodes according to the current load of the system. Classical load balancing algorithm includes minimum connection algorithm, weighted least connection algorithm, destination address hash algorithm, source address hash algorithm  
  TABLE I 
      
NEW CHALLENGES ON STORAGE  
 
77 


  
Each type of load balancing strategy above has advantages and disadvantage, so some mix strategy emerged In [1  k e e p th re e qu eu es l ig h t l o a d  o p t i m a l l o a d  ov erl o ad   in master server, use priority algorithm among these queues and weighting polling algorithm in the queues Because the centralized load balancing has the advantages of les s communi cation overhead that is particularly important for the large data storage, so a lot of research enhances its reliability by changing the system structure. A more common way is to decompose complex load balancing management tasks by layering or grading and accomplished by multiple servers. In [13  a h i e r a r ch ica l  dynamic load balancing strategy was proposed. An intermediate layer was added between the metadata server and the client, which was designed to collect load status information especially to reduce the load information acquisition cost, and by adding a backup server of load management to solve the problem of low reliability. In [1   a load balancing strategies of two-stage meta server cluster file system was proposed, using the idea that the high level manage the low level, the task allocation and task processing functions of the meta server were decentralized, which improves the parallel processing ability and extensibility of the system, and put forward a model of the heat reflecting accessing frequency of the storage node to determine the ideal location of the file to be saved The process of load balancing scheduling is complex especially in dynamic equilibrium. In addition to the scheduling policy, there are many other factors, such as how to collect information, what information should be collected when should migrate task, where the task should be migrated More attention is paid to the related research of load evaluation index which is used to judge when to migrate HDFSês equilibrium strategies use the single evaluation index of disk usage, which has considerable limitations. In 15  th e ev alu ati o n f u n ct i o n is det e rm in ed by m u ltipl e  attributes and the server's load condition is determined by double threshold value However, the method above still can't solve the limitations of threshold. Usually thresholds are calculated and set in advance, if the thresholds are set too high, when the average system load is lighter, the phenomenon that part of the nodes are idle while the other part are busy may appear; If the threshold is set too low, when the average 
C LASSIFICATION AND ANALYSIS OF ALGORITHMS  Metadata Management Metadata compression reduces metadata size Improve space utilization the metadata read performance was hurt due to extra steps of lookup file Performance Optimization Utilizing prefetching and caching technologies to improve access efficiency e.g. Hot Files Caching, Metadata Caching The improvement of the Cache hit ratio Just for particular application Small file merging A set of correlated files is combined into a single large file to reduce the file count An indexing mechanism has been built to access the individual files from the corresponding combined file Reduce the metadata Two indexes, affect the speed sequence files Form by a series of binary, where key is the name of the file, the value of the file content Free access for small files, nor restrict how much users and files P latform dependent Way to store Small files stored separately in separate areas Reduce disk fragmentation Complexity of the movement Dynamic load balancing algorithm can be divided into centralized and distributed strategy. TABLE III shows the advantages and disadvantages of them Load balancing algorithm in the distributed file system can also be divided into sender starting method and recipient starting method. Sender starting method starts load distribution activities by overload point, by which part of the load of the overloaded nodes is sent to light load nodes, is suitable for the system as a whole in light load condition because of more light load nodes in system, light load node is easy to found, so frequent migration wonêt happen Recipient starting method starts load distribution activities by light load nodes which apply for part of the load of the overloaded nodes, is relatively effective when the system as a whole is in a state of overload, the reasons are similar to the above TABLE III The function of the dynamic load balancing is concentrated in a special load management server. The server is responsible for the load information collection and maintenance of the whole system There is no specific load management server, so each node need to collect record and manage the load information of the surrounding nodes Advantages Simple and less communication cost; The best node for  migration or copy can be found Suitable for large system model High reliability, easy to extend; There is no single point failure, paralysis of any host will not affect the normal work of the whole system Disadvantages Low reliability, hard to extend Once the load management server fails the whole load balancing system will be paralyzed The larger the size of the system, the more complex the management When the system is very large, the communication cost of load information collection will geometrically growth Because each node only master a small amount of load information, and the load regulation is local, the result may be not satisfactory, and even have a chance to cause the load to move back 
method illustration advantage disadvantage Centralized strategy Distributed strategy  
  TABLE II 
COMPARISON BETWEEN CENTRALIZED AND DISTRIBUTED STRATEGY  Principle  
78 


  
  
C Replica Consistency D De-duplication 
In distributed file system, replication technology is widely used to improve the reliability and performance of system. Ensure the safety and reliability of the data and fault tolerance of the system by storing multiple copies, with that just using another copy can normally access the data when a copy is destroyed. In addition, when the system is larger, the use of replication technology on different servers to store a copy of the same file helps to achieve the load balancing of the system, thereby improving overall performance Nowadays the master - slave replica model is adopted by the most study of the Replica consistency, and it is the most original way for the salve replica to receive the update passively from the master replica. In w o co nsi st e ncy protocols based the tree are proposed: aggressive copy coherence and lazy copy coherence. With the root node regarded as the master copy, in the aggressive copy coherence, once the data of the root node update, the update will broadcast to each copy nodes, and the copy nodes will be updated; in the lazy copy coherence, the copy nodes are accessed by comparing timestamp to check their own data whether the latest version or not, if not, they will update Therefore, though the aggressive copy coherence can guarantee update from the copy in a timely manner, in the big data environment, the system will be given a great deal of load pressure; while though the lazy copy coherence can reduce the network load, but will have access latency, and when the master copy is damaged it may not be restored from a copy of which has not been updated However, in the master - slave replica model, as the master copy is the only update source, the efficiency will by reduced and it will be the bottleneck of the system performance improvement. In [17 a n o n cen t r al iz ed  c opy  update consistency model based on the replica identification of timestamp was proposed, which effectively solves the bottleneck problem of the original model by substituting the master copy nodes to the slave copy nodes to trigger the update process. In addition, it also referred to a strategy of creating copy dynamically when a request exceeds the threshold, but no extra copies of deletion policy was referred which makes a number of file copy used to be hot in the system resulting in a large number of redundant. In th e dynamic replication strategy was referred, dynamically changing the number of copies, including the creation and deletion In the actual application, load fluctuation of distributed file system of is very big, so if consistency detection can be done when the load is light, not only the data will be more reliable, but also the system resources will be fully used Extensive redundancy exists in the distributed file system, such as multi-user storing the same file, different versions of the unified file, similar file header of the same type file and so on. De-duplication is a very popular storage technology, effectively optimizing storage capacity. It deletes duplicate data in data set, leaving only one, thus eliminating redundant data The mainstream method to determine whether the data block is repeated is by the way of computing their hash value If the hash value of the data block matches with a value from the hash index table, it indicates that data block is repeated and substitute it with a pointer to the data storage. With the increase of the index, the memory performance will rapidly decline. So some researches focus on the approach by which more redundant data can be reduced, and some investigate how to do data de-duplication at high speed Another key technology is the division of the data. In file units to detect the speed will be very fast, but even if many of the same data exist in many different files, the duplicate data in the file canêt be deleted, so the file are needed to be divided, and are tested in block units. In fi ve representative chunking algorithms of data de-duplication are introduced and their performance on real data set is compared By de-duplication the rapid growth of data is effectively controlled, effective storage space is increased, storage efficiency is improved, and so on. However the reliability of data will be affected, and multiple files rely on the same data block, that is to say, multiple files are damaged, if the data block is damaged. Therefore, we can consider conducting the de-duplication firstly, and then backup appropriately for the data set with low access rate, by which storage space is saved, and the reliability of the data is ensured V C ONCLUSION  Despite the big data is popular recently, the study of it is not just started. There have been many countermeasures and related technologies to meet the challenge of big data. This paper summarizes and analyzes four technologies of distributed file system on big data storage, and gives some outlook for further study A CKNOWLEDGMENT  This paper is supported by National Natural Science Foundation of China: çResearch on Trusted Technologies for The Terminals in The Distributed Network Environment Grant No. 60903018\nd çResearch on the Security Technologies for Cloud Computing Platformé \(Grant No 61272543  R EFERENCES  1 
J. A. E. R. Gantz, "Extracting Value from Chaos," IDCês Digital Universe Study 2011 2 X. Ci and X. Meng, "Big Data Management:Concepts,Techniques and Challenges," journal of Computer Research and Development 2013 
  system load is heavier, not only the phenomenon of frequent migration will appear, but also that a migration triggers a new migration will happen. That will cause the system instability. Dynamic threshold, which determines the threshold by the current system load situation, has the potential to become one of the ways to solve this problem and has very high research value 
 
79 


X. Li, B. Dong, L. Xiao, L. Ruan, and Y. Ding, "Small files problem in parallel file system," in 2011 International Conference on Network Computing and Information Security, NCIS 2011, May 14, 2011 May 15, 2011, Guilin, Guangxi, China, 2011, pp. 227-232 4 B. Dong, Q. Zheng, F. Tian, K. Chao, R. Ma, and R. Anane, "An optimized approach for storing and accessing small files on cloud storage," Journal of Network and Computer Applications, vol. 35, pp 1847-1862, 2012 5 B. Dong, J. Qiu, Q. Zheng, X. Zhong, J. Li, and Y. Li, "A novel approach to improving the efficiency of storing and accessing small files on hadoop: A case study by PowerPoint files," in 2010 IEEE 7th International Conference on Services Computing, SCC 2010, July 5 2010 - July 10, 2010, Miami, FL, United states, 2010, pp. 65-72 6 G. MacKey, S. Sehrish and J. Wang, "Improving metadata management for small files in HDFS," in 2009 IEEE International Conference on Cluster Computing and Workshops, CLUSTER '09 August 31, 2009 - September 4, 2009, New Orleans, LA, United states, 2009 7 S. Chandrasekar, R. Dakshinamurthy, P. G. Seshakumar, B Prabavathy, and C. Babu, "A novel indexing scheme for efficient handling of small files in Hadoop Distributed File System," in 2013 3rd International Conference on Computer Communication and Informatics, ICCCI 2013, January 4, 2013 - January 6, 2013 Coimbatore, India, 2013, p. Gov. India, Dep. Sci. Technol., Minist Sci. Technol.; Council for Scientific and Industrial Research \(CSIR 8 Y. Zhang and D. Liu, "Improving the efficiency of storing for small files in hdfs," in 2012 International Conference on Computer Science and Service System, CSSS 2012, August 11, 2012 - August 13, 2012 Nanjing, China, 2012, pp. 2239-2242 9 9  X  L i B. D o ng L  X i ao an d  L  Rua n  P e r f o r m ance  optimization of small file I/O with adaptive migration strategy in cluster file system," in 2nd International Conference on HighPerformance Computing and Applications, HPCA 2009, August 10 2009 - August 12, 2009, Shanghai, China, 2010, pp. 242-249   N M oha nd a s  a n d S  M  T h amp i   I mp rovi n g h a d o o p  performance in handling small files," in 1st International Conference on Advances in Computing and Communications, ACC 2011, July 22, 2011 - July 24, 2011, Kochi, India, 2011, pp. 187-194  J. Liu, L. Bing and S. Meina, "The optimization of HDFS based on small files," in 2010 3rd IEEE International Conference on Broadband Network and Multimedia Technology, IC-BNMT2010 October 26, 2010 - October 28, 2010, Beijing, China, 2010, pp. 912915  C. Zhang and J. Yin, "Dynamic Load Balancing Algorithm of Distributed File System," Journal of Chinese Computer Systems, vol 32, pp. 1424-1426, 2011  W. Wu, "Research on mass storage metadata management,". vol. D Huazhong University of Science and  Technology, 2010  J. Tian, W. Song and H. Yu, "Load-Balance Policy in Two Levelcluster File System," Computer Engineering, vol. 33, pp. 77-79,82 2007  F. Gu, "Research on Distributed File System Load Balancing in Cloud Environment,". vol. D: Beijing Jiaotong University, 2011  Y. Sun and Z. Xu, "Grid replication coherence protocol," in Proceedings - 18th International Parallel and Distributed Processing Symposium, IPDPS 2004 \(Abstracts and CD-ROM\, April 26, 2004 April 30, 2004, Santa Fe, NM, United states, 2004, pp. 3197-3204  S. Zheng, M. Li and W. Sun, "DRCSM:a Novel Decentralized Replica Consistency Service Model," Journal of Chinese Computer Systems, vol. 32, pp. 1622-1627, 2011  S. Zheng, M. Li and W. Sun, "DRCSM:a Novel Decentralized Replica Consistency Service Model," Journal of Chinese Computer Systems, vol. 32, pp. 1622-1627, 2011  B. Cai, F. L. Zhang and C. Wang, "Research on chunking algorithms of data de-duplication," in International Conference on Communication, Electronics, and Automation Engineering, 2012 Xi'an, China, 2013, pp. 1019-1025  
  3 
                 
80 


     


    


       


        


  


Bottom Top A B 
Figure 15 Figure 16 
messages seen for all workers in a superstep \(Figures 10 and 13\. When looking at the messages sent by workers in a superstep for METIS, we see that there are message load imbalances within work ers in a superstep, caused due to concentration of vertices being traversed in that superstep in certain partitions This variability is much more pronounced in CP as compared to WG \(Figures 11 and 14\ E.g. in superstep 9 for CP, twice as many messages \(4M\ are generated by a worker compared to another \(2M\.  For Pregel BSP, the time taken in a superstep is determined by the slowest worker in that superstep. Hence increase d variability in CP causes even çgoodé partitioning strategies to cause an increase in total execution time wh en using the Pregel/BSP model VIII A NALYSIS OF E LASTIC C LOUD S CALING  Cloud environments offer elasticity Ö the ability to scale-out or scale-in VMs on-demand and only pay for what one uses [28   On th e f l i p s i de  on e en ds u p  paying for VMs that are acquired even if they are underutilized. We have already shown the high variation in compute/memory resources used by algorithms like BC and APSP across different supersteps. While our earlier swath initiation heuristics attempt to flatten these out by overlapping swath executions, one can consider leveraging the cloudês elasticity to, instead, scale up and down the concurrent workers \(and graph partitions\ allocated in each superstep The peak and trough nature of resource utilization combined with Pregel/BSPês synchronous barrier between supersteps offers a window for dynamic scaleout and Öin at superstep boundaries. Peak supersteps can greatly benefit from additional workers, while those same workers will contribute to added synchronization overhead for trough supersteps We offer an analysis of the potential benefits of elastic scaling by extrapolating from observed results for running BC on WG and CP graphs, using four and eight workers.  To provide a fair and focused comparison, we turned off swath heuristics in favor of fixed swath sizes and initiation intervals Figure 15 \(Bottom\ plots the speedup of BC running on eight workers when normalized to BC running on four workers, at corresponding supersteps.  The number of workers does not impact the number of supersteps We also plot the number of active vertices \(i.e. vertices still computing for a given swath\these supersteps which is a measure of how much work is required \(Fig 15 \(Top\. We find that we occasionally get superlinear speedup spikes \(i.e. >2x\ that shows a strong correlation with the peaks of active messages, for both WG and CP graphs. At other times, the sp eedup is sublinear or even a speed-down \(i.e. <1\responding to inactive vertices.  The superlinear speedup is attributable to the lower contention and reduced memory pressure for 8 workers when the active vertices peak \(similar to what we observed for the swath initiation heuristics Similarly, the below par speedup during periods of low activity is contributed by the increased overhead of barrier synchronization across 8 workers. Intuitively, by dynamically scaling up the number of workers for supersteps with peaking active vertices and scaling them down otherwise, we can leverage the superlinear speedup and get more value per worker Using a threshold of 50% active vertices as the threshold condition for between 4 and 8 workers in a superstep, we extrapolate the time per superstep and compared this to the fixed 4 and 8 worker runtimes. We also compute the best-case run time using an çoracleé approach to i.e. for each superstep, we pick the minimum of the 4 or 8 workerês time.  Note that these projections do not yet consider the overheads of scaling, but are rather used to estimate the potential upside if we had an ideal or an automated heuristic for scaling. The total time estimates for running BC on WG and CP graphs, normalized to  
 plot shows speedup of 8 workers relative to 4 workers, for each superstep, when running BC on WG and CP graphs plot shows the number of vertices active in that superstep Estimated time for BC using elastic scaling, normalized to time taken for 4 workers. Normalized cost is shown on secondary Y axis WG graph shown on left CP graph shown on right. Smaller is better 
022\011 022\010 022\007 022\002 006 002 007 006 002 007 010 011 012 013 014 015 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 027\031\030\037\020#@\020"\031\030\027\020\035 0201!2#\024$#\015#5\024",\020"#\017\003"\003\031\003#\011#5\024",\020"\035 024"'\033\026\0309\0201#\\031\020 2 035#\032\020"#+!\034 017\020\021\022\023\024\024\025\026\020 027\030\031\022\032\033\031\020\034\031\035 017\020\021\022\023\024\024\025\026\020#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 027\030\031\022\032\033\031\020\034\031\035#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 036\030\034\020\033"#\\0201!2 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 017\020\021\022\023\024\024\025\026\020#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035\031 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 002\003\011 002\003\013 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 033\026\030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 027\030\031\022\032\033\031\020\034\031\035#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035 031 
 
dynamically scaling ideal scaling 
Our hypothesis is that an intelligent adaptive scaling of workers can achieve a similar performance as a large, fixed number of workers, but with reduced cost 
213 


Nature Nature Ecological Applications Nature ACM International Conference on Management of Data \(SIGMOD In Parallel Object-Oriented Scientic Computing \(POOSC Science Communications of the ACM ACM Workshop on Mining and Learning with Graphs Communications of the ACM HotCloud Proceedings of the 19th ACM International Symposium on High PErformance Distributed Computing HPDC Knowledge and Information Systems KAIS International Conference on Computational Science IEEE International Conference on Cloud Computing Technology and Science ACM/IEEE Conference on Advances in Social Network Analysis and Mining \(ASONAM IEEE International Parallel and Distributed Processing Symposium \(IPDPS International Conference on Distributed Computing and Networking Journal of Mathematical Sociology International Conference on Parallel Processing Communications of the ACM 
 
observed time taken using 4 workers, are plotted in Figures 16\(A\ and 16\(B We see that our dynamic scaling heuristic using the percentage of active vertices achieves nearly the same CP\ or better \(WG\ performance as a fixed 8 worker approach. Clearly there is benefit of using fewer workers for low utilization su persteps to eliminate the barrier synchronization overhead. Also, the dynamic scaling heuristic performs almost as well as the ideal scaling. Finally, when we consider the monetary cost of the proposed approaches, assuming a pro-rata normalized cost per VM-second plotted on the secondary Y axis, we see that dynamic scaling is comparable \(CP\ or cheaper \(WG\ than a 4 worker scenario while offering the performance of an 8 worker deployment IX C ONCLUSION  In conclusion, we introduce optimization and heuristics for controlling memory utilization and show they are critical to performance.  By breaking computation into swaths of vertices and using our sizing heuristics we achieve up to 3.5x speedup over the maximum swath size that does not cause the a failure.  In addition overlapping swath executions can provide a 24% gain with automated heuristics and even greater speedup when a priori knowledge of the network characteristics is applied This evaluation offers help to eScience users to make framework selection and cost-performancescalability trade-offs. Our he uristics are generalizable and can be leveraged by other BSP and distributed graph frameworks, and for graph applications beyond BC. Our work uncovered an unexpected impact of partitioning and it would be worthwhile, in future, to examine the ability to pred ict, given certain graph properties, a suitable partitioning model for Pregel/BSP It may also be useful to perform such evaluations on larger graphs and more numbers of VMs. At the same time, it is also worth considering if non-linear graph algorithms are tractable in pr actice for large graphs in a distributed environment B IBLIOGRAPHY  1  F  L i lj er os C   Ed l i n g L  A m a r a l H  S t an ley   and Y    berg The web of human sexual contacts 
vol. 411, pp. 907908, 2001   H Je o n g  S   Ma so n A  L   B a ra b s i  a nd Z   Oltva i  L e t ha l i t y  and centrality in protein networks vol. 411, pp. 41-42 2001   O. B o din and E   E s t r ada    U s i n g n e t w ork c e nt r a l i t y  m e a s ures t o  manage landscape connectivity vol 18, no. 7, pp. 1810-1825, October 2008   D. W a ts s  and S  S t r ogat z  C olle c t i v e  d y nam i cs of  s m a ll-w orl d   networks vol. 393, no. 6684, pp. 440Ö442, June 1998   G  Ma lew i c z   M A u s t er n A   Bik  J   Dehn er t I  Hor n   N. L e i s er and G. Czajkowski, "Pregel: A system for large-scale graph processing," in 2010   D. G r egor  and A  L u m s dain e  T h e  pa r a llel  B G L  A gen e r i c  library for distributed graph computations," in 2005   B. S h a o  H. W a n g  and Y  L i T he T r init y G r aph E n g i n e    Microsoft Research, Technical Report MSR-TR-2012-30, 2012   A  F ox  C lo ud c o m putin g w h at  s  in it for m e  as  a  s c i e n tis t     vol. 331, pp. 406-407, 2011   S. G h e m a w a t  and J  De an   Map re duc e s i m p lifi e d data  processing on large clusters vol 51, no. 3, pp. 107-113, 2008   J  L i n and M. S c hat z   Des i g n  patt er n s  for eff i ci ent gr aph algorithms in MapReduce," in 2010   L   Va l i ant   A b r id g i n g m o d e l f or pa r a llel com putati o n  vol. 33, no. 8, pp. 103-111, 1990 12 a c h e  Ha ma    O n l i n e    http://hama.apache.org   13 Ap a c h e  Ha d o op    O n l i n e    http://hadoop.apache.org     M Z a h a r i a, M. Ch ow dhu ry M F r ank l in S  S h e n k e r, and I   Stoica, "Spark: Cluster Computing with Working Sets," in 2010   J  Ekana y ak e e t a l     T w i st er A  r untim e f o r it er ati v e  MapReduce," in Chicago, 2010, pp. 810-818   U. K a n g  C  T s o u rakakis   and C. F a l outs o s  Peg a s us   Minin g  Peta-scale Graphs," in 2010   M. P a c e  B S P vs  MapR e duc e    in vol. 103.2081, 2012   S. Seo  E  Yoo n, J  K i m  S  J i n  J-S. K i m   and S   Ma e n g HAMA: An Efficient matrix computation with the MapReduce framework," in 2010, pp. 721-726   S. S a l i h ogl u  and J  W i d o m  G PS A G r a ph P r oc e s s i n g Sy s t em    Stanford University, Technical Report 2011   R L i cht e n w a l t e r and N   Cha w la D is Ne t  A fr am ew ork for  distributed graph computation," in  2011   K  Maddu r i  D. E d i g er K   J i an g  D. Bad e r  and D  Cha v a r riaMiranda, "A faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets," in 2009   E  K r e p s k a, T  K i el m a nn, W  F o kkink, H   Ba l, "A  hi g h level framework for distributed processing of large-scale graphs," in 2011, pp. 155-166   L   Pa ge  S  B r in R. M o t w ani and T  W i nogr ad  T h e P a geRank citation ranking: Bringing order to the web," Stanford InfoLab Technical Report 1999-66, 1999   U  Brand  s  A f a s t er  a l gor ith m for  b e t w eenn e s s c e nt r a l i t y    vol. 25, no. 2, pp. 163-177 2001   Stan fo r d  Net w or k A na l y s is Pro j e c t  O n l in e    http://snap.stanford.edu    I  S t ant o n and G  K l i o t, "S t r e a m i n g G r aph P a rtiti o n in g  for L a rge Distributed Graphs," Microsoft Corp., Technical Report MSRTR-2011-121, 2011   G   K a ry pis and V   K um a r A fas t and hi g h qua l i t y m u l t i l evel scheme for partitioning irregular graphs," in 1995, pp. 113-122   M. A r m b r u s t e t  a l   A v i ew of  c l o u d  c o m putin g    vol. 53, no. 0001-0782, pp. 50-58 April 2010  
214 


  13  or gani c  c he m i s t r y  i n our  Sol ar  Sy s t e m       Xi a n g  L i r e c e i v e d h i s B  S   m is tr y  fr o m  th e  P e k in g  U n iv e r s ity  C h in a  in  2 0 0 3  and P h D   i n P hy s i c al  C he m i s t r y  f r om  t he  J ohns  H opk i ns  Un i v e r s i t y  i n  2 0 0 9   He  h a s  b e e n  a  R e s e a r c h  A s s o c i a t e  wi t h  a  j o i n t  a p p o i n t m e n t  a t  t h e  U n i v e r s i t y  o f  M a r y l a n d   Ba l t i m o r e  C o u n t y  a n d  N AS A G o d d a r d  S p a c e  Fl i  Ce n t e r  s i n c e  2 0 1 1   H i s  r e s e a r c h  f o c u s e s  o n  t h e  d e t e c t i o n  of  t r ac e  e l e m e nt  and as t r obi ol ogi c al l y  r e l e v ant  or gani c  mo l e c u l e s  i n  p l a n e t a r y  s y s t e ms   l i k e  M a r s   He  i s  es p eci a l l y i n t er es t ed  i n  t h e d evel o p m en t  o f  T i m e of  and I on T r ap m as s  s pe c t r om e t e r s w i t h v a r i o u s i o n i z a t i o n  ng te c h n iq u e s   Wi l l  B r i n c k e r h o f f  sp a c e  sc i e n t i st  i n  t h e  Pl a n e t a r y  En v i r o n m e n t s  La b  a t  N A S A  s  G o d d a r d  Spac e  F l i ght  C e nt e r  i n Gr e e n b e l t   M D w i t h  pr i m ar y  r e s pons i bi l i t y  f or  th e  d e v e lo p m e n t o f th e  L D TO F  m a s s  s p e c t r o  th is  p r o je c t H e  h a s  fo c u s e d  re c e n t l y  o n  t h e  d e v e l o p m e n t  o f  m i n i a t u re  l a se r d ma s s  s p e c t r o me t e r s  f o r  f u t u r e  p l a n e t a r y  mi s s i o n s  a l o n g  wi t h  b a s i c  e x p e r i m e n t a l  r e s e a r c h  i n  a s t r o b i o l o g y  a n d  p r e bi ot i c  s y nt he s i s   D r   B r i nc k e r hof f  i s  i nv ol v e d i n t he  de v e l opm e nt  of  m as s  s pe c t r om e t e r  f or  bot h t he  2011 Ma r s  S c i e n c e  L a b o r a t o r y  a n d  t h e  2 0 1 8  E x o Ma r s  mi s s i o n s   


  14   


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a ìkey, valueî list using an XSTL  Queries made against this list of ìkey, valueî pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


