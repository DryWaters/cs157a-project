Ke Wang  Yanjun Qi  Jeffrey J Fox  Mircea R Stan and Kevin Skadron 
Association Rule Mining with the Micron Automata Processor 
        
  
Dept of Comp Sci Dept of Mater Sci Dept of Elec  Comp Eng University of Virginia Charlottesville VA 22904 USA kewang yanjun jjf5x mircea skadron virginia.edu 
Abstract 
Association rule mining ARM is a widely used data mining technique for discovering sets of frequently associated items in large databases As datasets grow in size and real-time analysis becomes important the performance of ARM implementation can impede its applicability We accelerate ARM by using Micronês Automata Processor AP a hardware implementation of non-deterministic nite automata NFAs with additional features that signiìcantly expand the APs capabilities beyond those of traditional NFAs The Apriori algorithm that ARM uses for discovering itemsets maps naturally to the massive parallelism of the AP We implement the multipass pruning strategy used in the Apriori ARM through the APs symbol replacement capability a form of 
lightweight reconìgurability Up to 129X and 49X speedups are achieved by the AP-accelerated Apriori on seven synthetic and real-world datasets when compared with the Apriori singlecore CPU implementation and Eclat a more efìcient ARM algorithm 6-core multicore CPU implementation respectively The AP-accelerated Apriori solution also outperforms GPU implementations of Eclat especially for large datasets Technology scaling projections suggest even better speedups from future generations of AP 
Keywords 
Automata Processor association rule mining frequent set mining 
I I NTRODUCTION Association Rule Mining ARM also referred to as 
Frequent Set Mining FSM is a data-mining technique that identiìes strong and interesting relations between variables in databases using different measures of interestingness ARM has been the key module of many recommendation systems and has created many commercial opportunities for on-line retail stores In the past 10 years this technique has also been widely used in web usage mining trafìc accident analysis intrusion detection market basket analysis bioinformatics etc As modern databases continue to grow rapidly the execution efìciency of ARM becomes a bottleneck for its application in new domains Many previous studies have been devoted to improving the performance of sequential CPU-based ARM implementations Different data structures were proposed including horizontal representation vertical 
representation and matrix representation Multiple algorithms have been developed including 
  and  A number of parallel acceleration based solutions have also been developed on multi-core CPU GPU and FPGA 7 Recently Micron proposed a novel and powerful non-von Neumann architecture the Automata Processor AP The AP architecture demonstrates a massively parallel computing ability through a large number of state elements It also achieves ne-grained communication ability through its conìgurable routing mechanism These advantages make the AP 
Apriori Eclat FP-growth 
versus the 
suitable for pattern-matching centered tasks like ARM Very recently the AP has been successfully used to accelerate the tasks of regular expression matching and DN A motif searching In this paper we propose an AP-based acceleration solution for ARM A Non-deterministic Finite Automata NFA is designed to recognize the sets of frequent items Counter elements of the AP are used to count the frequencies of itemsets We also introduce a number of optimization strategies to improve the performance of AP-based ARM On multiple synthetic and real-world datasets we compare the performance of the proposed AP-accelerated Apriori Apriori 
single-core CPU implementation as well as multicore and GPU implementations of the 
algorithm The proposed solution achieves up to 129X speedups when compared with the single-core CPU implementation and up to 49X speedups over multicore implementation of  It also outperforms GPU implementations of in some cases especially on large datasets Overall this paper makes three principal contributions 1 We develop a CPU-AP computing infrastructure to improve the algorithm based ARM 2 We design a novel automaton structure for the match 
Eclat Apriori Eclat Eclat Apriori 
ing and counting operations in ARM This structure avoids routing reconìguration of the AP during the mining process 3 Our AP ARM solution shows performance improvement and broader capability over multi-core and GPU implementations of 
ARM on large datasets II A SSOCIATION R ULE M INING Association rule mining ARM among sets of items was rst described by Agrawal and Srikant The ARM problem was initially studied to nd regularities in the 
Eclat 
2015 IEEE 29th International Parallel and Distributed Processing Symposium 1530-2075/15 $31.00 © 2015 IEEE DOI 10.1109/IPDPS.2015.101 689 


1 2 1 2 1 2 12 
iff iff minsup minsup A Function elements B Speed and capacity C Input and output D Programming and reconìguration 
I i i   i T t t   t t I x i i   i I k k t x x t x Sup x 
     2 
shopping behavior of customers of supermarkets and has since been applied to very broad application domains In the ARM problem we deìne as a set of interesting items Let be a database of transactions each transaction is a subset of  Deìne be a set of items in  called an itemset The itemset with items is called itemset A transaction is said to cover the itemset  The support of   is the number of transactions that cover it An itemset is known as frequent its support is greater than a given threshold value called minimum support  The goal of association rule mining is to nd out all itemsets which supports are greater than  III A UTOMATA P ROCESSOR Micronês Automata Processor AP is a massively parallel non-von Neumann accelerator designed for high-throughput pattern mining The AP chip has three types of functional elements the state transition element STE the counters and the Boolean elements The state transition element is the central feature of the AP chip and is the element with the highest population density Counters and Boolean elements are designed to work with STEs to increase the space efìciency of automata implementations and to extend computational capabilities beyond NFAs Micronês current generation AP D480 chip is built on 45nm technology running at an input symbol 8-bit rate of 133 MHz The D480 chip has two half-cores and each half-core has 96 blocks Each block has 256 STEs 4 counters and 12 Boolean elements In total one D480 chip has 49,152 processing state elements 2,304 programmable Boolean elements and 768 counter elements Each AP board can have up to 48 AP chips that can perform matching in parallel Each AP chip has a w orst case po wer consumption of 4W The po wer consumption of a 48core AP board is similar to a high-end GPU card Each STE can be conìgured to match a set of any 8-bit symbols The counter element counts the occurrence of a pattern described by the NFA connected to it and activates other elements or reports when a given threshold is reached One counter can count up to which may not be enough for ARM counting in some cases In such a scenario two counters can be combined to handle a larger threshold Counter elements are a scarce resource of the AP currentgeneration chip and therefore are a main limiting factor of the capacity of the ARM automaton proposed in this work The AP takes input streams of 8-bit symbols Each AP chip is capable of processing up to 6 separate data streams concurrently although we do not use this feature for this work The data processing and data transfer are implicitly overlapped by using the input double-buffer of the AP chip Any STE can be conìgured to accept the rst symbol in the stream called start-of-data mode small 1 in the left-upper corner of STE in the following automaton illustrations to accept every symbol in the input stream called all-input mode small   in the left-upper corner of STE in the following automaton illustrations or to accept a symbol only upon activation The all-input mode will consume one extra STE Any type of element on the AP chip can be conìgured as a reporting element one reporting element generates a one-bit signal when the element matches the input symbol One AP chip has up to 6144 reporting elements If any reporting element reports at a cycle the chip will generate an output vector which contains signals of 1 corresponding to the elements that report at that cycle and 0és for reporting elements that do not report If too many output vectors are generated the output buffer can ll up and stall the chip Thus minimizing output vectors is an important consideration for performance optimization Automata Network Markup Language ANML is an XML language for describing the composition of automata networks ANML is the basic way to program automata on the AP chip Besides ANML Micron provides a graphical user interface tool called the AP Workbench for quick automaton designing and debugging A macro is a container of automata for encapsulating a given functionality similar to a function or subroutine in common programming languages A macro can be deìned with parameters of symbol sets of STEs and counter thresholds which can be instantiated with actual arguments Micronês AP SDK also provides C and Python interfaces to build automata create input streams parse output and manage computational tasks on the AP board Placing automata onto the AP fabric involves three steps compilation optimization routing conìguration and STE symbol set conìguration The initial compilation of automata onto the AP involves all these three steps while the pre-compiled automata only requires the last two steps The compilation optimization usually takes tens of seconds The routing conìguration of the whole board needs about 5 milliseconds The symbol set conìguration takes approximately 45 milliseconds for an entire board 
m n j i s s sl p q q p q q 
  002 003 
690 


IV R ELATED W ORK After describing the association rule mining problem Agrawal and Srikant proposed the algorithm The algorithm is a well known and widely used algorithm It prunes the search space of itemset candidates in a breadth-ìrst-search scheme the using property The Equivalent Class Clustering algorithm was developed by Zaki The typical implementation adopts a vertical bitset representation of transactions and depth-ìrst-search The low level operation e.g the bit-level intersection of two itemsets exposes more instruction-level parallelism which enables to outperform on conventional architectures Han  introduced another popular ARM algorithm  By utilizing a Frequent-Pattern tree data structure to avoid multi-pass database scanning has very good performance in many cases However the poor memory-size scaling of the Frequent-Pattern tree prevents the use of for very large databases Zaki  de v eloped a parallel v ersion of the algorithm for a shared memory SM multi-core platform This implementation achieved 8X speedup on a 12-processor SM platform for synthetic datasets Liu  proposed a parallel version of on a multi-core processor This work achieved 6X speedup on an 8-core processor Pramudiono and Kitsuregawa proposed a parallel algorithm of achieving 22.6X speedup on a 32node cluster Ansari  de v eloped an MPI v ersion of the algorithm and achieved 6X speedup on an 8-node cluster An FPGA-based solution was proposed to accelerate the algorithm by Zhang This solution achieved a speedup of 68X on a four-FPGA board with respect to the CPU sequential implementation of  Fang  designed a GPU-accelerated implementation of  2X-10X speedup is achieved with NVIDIA GeForce GTX 280 GPU when compared with CPU sequential implementation Zhang  proposed another GPU-accelerated implementation and achieved 6X30X speedup relative to the state-of-the-art sequential and implementations Zhang also proposed the  which hybridizes breadthìrst-search and depth-ìrst-search to expose more parallelism in this implementation This implementation also generalizes the parallel paradigm by a producer-consumer model that makes the implementation applicable to multicore CPU and multiple GPUs According to the previous cross-algorithm comparison there is no clear winner among the different sequential algorithms and implementations Ho we v er  to our kno wledge Zhangês  is the f astest parallel ARM implementation Thus we compare our AP-accelerated implementation with Zhangês parallel implementation on both multi-core CPU and GPU platforms However as more parallelism is exposed the vertical representation of many itemsets has to be kept in the memory main memory or GPU global memory simultaneously The tradeoff between memory and performance parallelism still exists particularly for large datasets on the GPU In contrast our AP-accelerated solution does not rely on local memory and therefore is less sensitive to the data size Further exploration is needed regarding how different algorithms scale with diverse multicore CPUs and specialized accelerators and the role of optimization techniques such as blocking and data-layout optimization This is an interesting area for future work V M APPING ARM PROBLEM ONTO THE AP Apriori The algorithm framework is adopted for the AP to reduce the search space as itemset size increases The algorithm is based on property all the subsets of a frequent itemset are also frequent and thus for an infrequent itemset all its supersets must also be infrequent In the framework candidates of itemsets are generated from known frequent itemsets by adding one more possible frequent item The mining begins at 1-itemset and the size of candidate itemsets increases by one at each level In each level the algorithm has two major operations 1 Generating candidates of frequent itemsets from known frequent itemsets 2 Counting support numbers of candidate itemsets and comparing these support numbers with The support counting step is the performance bottleneck of the algorithm particularly for the large databases The hardware features of the AP are well suited for matching and support-counting many itemsets in parallel Therefore we propose to use the AP to accelerate the support-counting step in each level Figure 1 shows the complete workîow of the APaccelerated ARM proposed in this paper The data preprocessing stage creates a data stream from the input transactional dataset and makes the data stream compatible with the AP interface Preprocessing consists of the following steps 1 Filter out infrequent items from transactions 2 Recode items into 8-bit or 16-bit symbols 3 Recode transactions 
A Sequential algorithms Apriori Apriori downward-closure Eclat Eclat Eclat Apriori et al FP-growth FP-growth FP-growth B Multi-thread  multi-process et al Apriori et al FP-growth FP-growth et al Apriori C Accelerators Eclat et al Eclat et al Apriori et al Eclat Eclat FP-Growth Frontier Expansion algorithm Eclat Eclat Apriori Eclat Apriori A Algorithm Apriori Apriori downward-closure Apriori Apriori minsup Apriori B Program infrastructure 
 1  1 
k k k k 
691 


002\003\004\003\005 006 007\006 002\003\004\003\005 006 010\006 002\003\004\003\005 006 011\006 002\003\004\003\005 006 012\006  002\003\004\003\005 006 025\006  013\014\015\016\017\016\020 006 006 021\014\022\023\014\016\003\016\024\006\006 
C Automaton for matching and counting 
a Automaton for itemset b Automaton for itemset 
       
freq item  freq item  k k 
 255 254  64516  1 
0 2 1 3 5 
4 Sort items in transactions 5 Connect transactions by a special separator symbol to form the input data stream for the AP Step 1 is a common step in almost all existing ARM implementations that helps to avoid unnecessary computing on infrequent items and reduces the number of items and transaction sizes Depending on the population of frequent items the items can be encoded by 8-bit   or 16-bit symbols   in step 2 Different encoding schemes lead to different automaton designs Step 3 deletes infrequent items from the transactions applies the codes of items to all transactions encodes transaction boundary markers and removes very short transactions less than two items Step 4 sorts items in each transaction in any given order to avoid needing to consider all permutations of a given itemset and therefore saves STE resources on the AP We adopt descending sorting according to item frequency proposed by Borgelt The data pre-processing is only executed once in the whole workîow Each iteration of the loop shown in Figure 1 explores all frequent itemsets from the candidates generated from itemsets The candidates are generated from the CPU and are compiled onto the AP by using the automaton structure designed in this paper The input data formulated in preprocessing is then streamed into the AP for counting Figure 2 shows the initial automaton design for ARM The items are coded as digital numbers in the range from 0 to 254 with the number 255 reserved as the separator of transactions Each automaton for ARM has two components matching and counting The matching component is implemented by an NFA the groups of STEs in Figure 2a and 2b to recognize a given itemset Note that unlike string matching the itemset matching in ARM needs to consider the cases of discontinuous patterns of items For example consider the itemset of 6,11  in transactions such as or 3,6,11,15 item 11 is ne xt to item 6 while in other cases such as or  there are an unkno wn number of items between 6 and 11 The NFA we designed can capture all possible continuous and discontinuous variants of a given itemset The only requirement is the order of items appearing in the transactions which is already guaranteed by sorting in data pre-processing As shown in Figure 2 the NFA for itemset matching can be divided into multiple levels Each level except Level 0 has two STEs the top STE holds the activation in this level and the bottom STE triggers the next level if one item in a given transaction matches it For each automaton corresponding to a given itemset activation begins at Level 0 and will move forward to the right to Level 1 when the transaction separator is seen in the input Each level will Figure 1 The workîow of AP-accelerated ARM    Figure 2 Initial design of automata for ARM itemset matching and support-counting Blue circles and black boxes are STEs and counters respectively The numbers on an STE represent the symbol set that STE can match 0:254 means any item ID in the range of 0-254 Symbol 255 is reserved as the transaction separator 
   
002\003\004\003\005 006 007\006 002\003\004\003\005 006 010\006 002\003\004\003\005 006 011\006 002\003\004\003\005 006 012\006  013\014\015\016\017\016\020 006 006 021\014\022\023\014\016\003\016\024\006\006 
692 


Figure 3 Optimization for minimizing the output The node with 254 is the reporter trigger the next level if the item represented by this level bottom STE is seen in the input If the item of the current level is not seen the activation of the current level will be held by the top symbol until the end of this transaction when separator symbol is seen The itemset matching is restarted in the beginning of each transaction by the Level 0 STE The counting component uses an on-chip counter element to calculate the frequency of a given itemset If the last level has been triggered the matching component waits for the separator symbol to indicate the end of a transaction The separator symbol then activates the counter incrementing it by one If the threshold which is set to  is reached in the counter this automaton produces a report signal at this cycle After processing the whole dataset on the AP the output vectors are retrieved Each itemset with a frequency above the minimum support will appear in the output Although the automata shown in Figure 2 already implement the basic functions of matching and counting for ARM there is still much room for performance optimization We will talk about the performance optimization in the next subsection We only show the automata for 8-bit encoding scheme in this paper The automata for 16-bit encoding scheme are designed in a similar way but use two connecting STEs to match an item In this paper we propose three optimization strategies to maximize the computation performance of the AP The rst strategy is to minimize the output from the AP In the initial automaton design shown in Section V-C the AP chip creates a report vector at each cycle whenever there is at least one counter report Each report vector carries the information about the cycle count for this report Therefore the AP chip creates many report vectors during the data processing These report vectors may ll up the output buffers and cause stalls during processing However solving the ARM problem only requires identifying the frequent itemsets the cycle at which a given itemset reaches the minimum support level is irrelevant We therefore modify the design of the reporting element and postpone all reports to the last cycle Figure 3 We utilize the latch property of the counter to keep activating another STE connected to this counter after the counter is reached We call this STE the reporter One symbol i.e 254 is reserved to indicate the end of a transaction stream and this end-of-stream symbol matches to the reporter STE and triggers the actual output Consequently the global set of items is 0-253 which ensures that the ending symbol 254 will not appear in the middle of the transaction stream With this modiìcation only one output vector will be produced in the end of data stream Another beneìt of this modiìcation is that it eliminates the need to merge multiple report vectors as a postprocessing step on the CPU Instead the counting results can be parsed from only one report vector As shown in Figure 2 when the mining of itemsets nishes the automata for itemset need to be compiled onto the AP to replace the automata for itemsets The automata reconìguration involves both routing reconìguration and symbol replacement steps because the NFAs that recognize itemsets of different sizes have different structures compare Figure 2a and Figure 2b On the other hand the AP also provides a mechanism to only replace the symbol set for each STE while the connections between AP elements are not modiìed The time of symbol replacement depends on how many AP chips are involved The max symbol replacement time is 45ms if all STEs update their symbol sets To remove the routing reconìguration step we propose a general automaton structure supporting itemsets with different sizes The idea is to add multiple entry paths to the NFA shown in Figure 2 To count the support of a given itemset only one of the entry paths is enabled by matching to the transaction separator symbol while the other entry paths are blocked by a reserved special symbol This special symbol can be the same as the data stream ending symbol i.e 254 discussed in Section V-D1 This structure is called multiple-entry NFA for variable-size itemset MENFA-VSI 10 total reconìguration time 5ms is saved by using the ME-NFA-VFI structure Figure 4 shows a small-scale example of an ME-NFAVSI structure that can count an itemset of size 2 to 4 Figure 4a shows the ANML macro of this ME-NFA-VSI structure leaving some parameters to be assigned for a speciìc itemset e01 e03 are symbols for three entries An entry can be conìgured as either 255 or 254 to present enabled and disable status Only one entry is enabled for a given itemset I represents the global set of items  i01 i04 are individual symbols of items in the itemset SP is the transaction separator and END is the ending symbol of the input stream To count a 2-itemset the rst two entries are blocked by 254 and the third entry is enabled by 255 Figure 4b Similarly this structure can be conìgured to counting a 3itemset and a 4-itemset by enabling a different entry point Figure 4c and 4d Another optimization has been made to reduce STE usage 
minsup D Performance optimization 1 Output optimization minsup 2 Avoid routing reconìguration 
k k k I 
 1 
693 


a AP macro of ME-NFA-VSI  b Automaton for itemset c Automaton for itemset d Automaton for itemset 
006 006 006 006 006 006 
3 Concurrent mining itemset and itemset A Capacity and Overhead Apriori 
k k k k k k k k k k k 
1 3 2 7 8 4 5 25 30 
002\003\004\005\006 007 002\003\004\005\006 010 002\003\004\005\006 011 
        
of ME-NFA-VSI structure by switching entry STEs from all-input mode to start-of-data mode with a bi-directional connection to I STE Figure 4a The max number of the optimized ME-NFA-VSI structures that can t on the AP chip is mainly limited by the number of counter elements Therefore it is possible to compile large ME-NFAVSI structures on the AP chip without sacriìcing capacity In the 8-bit symbol encoding scheme one block of the AP chip can support two ME-NFA-VSI structures that match itemsets of size 2 to 40 For the 16-bit-symbol encoding scheme we use an ME-NFA-VSI structure that matches itemset of size 2 to 24 24 is a reasonable upper bound of itemset size we discovered in our test cases At the very beginning  is small and the end  is large of mining the number of candidates could be too small to make full use of the AP board In these cases we predict the number candidates of the itemset by assuming all itemset candidates are frequent If the total number of itemset candidates and predicted itemset candidates can t onto the AP board we generate itemset candidates and concurrently mine frequent itemsets and itemsets in one round This optimization takes advantage of uniìed ME-NFA-VSI structure and saves about 5%-10 AP processing time in general VI E XPERIMENTAL R ESULTS The performance of our AP implementation is evaluated using CPU timers host codes and an AP simulator in the AP SDK AP codes assuming a 48-core D480 AP board In our experiments our AP-accelerated algorithm Apriori-AP switches between 8-bit and 16-bit encoding schemes automatically in the data preprocessing stage shown in the owchart Figure 1 In an 8-bit scheme the items are coded with symbols from 0 to 253 If more than 254 frequent items are represented after ltering two 8-bit symbols are used to represent one item 16-bit symbol scheme In both encoding schemes the symbol 255 is reserved for the transaction separator and the symbol 254 is reserved for both the input ending symbol and the entryblockers for the ME-NFA-VSI structure By using the MENFA-VSI structure one AP board can match and count 18,432 itemsets in parallel with sizes from 2 to 40 for 8bit encoding and 2 to 24 for 16-bit encoding In all our experiments 24 is a reasonable upper bound of the sizes of the itemsets If there are more than 18,432 candidate itemsets multiple passes are required Before each single pass a symbol replacement process is applied to reconìgure all ME-NFA-VSI structures on the board which takes 0.045 second Figure 4 A small example of multiple-entry NFA for variable-size itemset support counter for 2-itemset 3itemset and 4-itemset a is the macro of this ME-NFA-VSI with parameters 
 1  1  1  1  1 
694 


B Comparison with other implementations Apriori Apriori Eclat Eclat Eclat C Datasets Frequent Itemset Mining Dataset Repository D AP vs CPU Apriori 
We use the computation times from Borgeltês CPU sequential implementation Apriori-CPU as a baseline Because the AP accelerates the counting operation at each iteration we show the performance results of both the counting operation and the overall computation in this section We also compare a state-of-the-art CPU serial implementation of Eclat-1C a multi-threading implementation of Eclat-6C and a GPU-accelerated implementation of Eclat-1G All of the abo v e implementations are tested using the following hardware CPU Intel\(R Xeon\(R CPU E5-1650\(6 physical cores 3.20GHz Mem 32GB 1.333GHz GPU Nvidia Kepler K20C 706 MHz clock 2496 CUDA cores 4.8GB global memory For each benchmark we compare the performance of the above implementations over a range of minimum support values A lower support number requires a larger search space and more memory usage since fewer itemsets are ltered during mining To have all our experiments nished in a reasonable time we select minimum support numbers that produce computation times of the Apriori-CPU implementation that is in the range from 1 second to 5 hours for any dataset smaller than 1GB and from 1 second to 10 hours for larger datasets The relative minimum support number deìned as the ratio of minimum support number to the total number of transactions is used in the gures of this section Three commonly-used real-world datasets from the  three synthetic datasets and one real-world dataset generated by ourselves ENWiki are tested The details of these datasets are shown in Table I and II T40D500K and T10020M are obtained from the IBM Market-Basket Synthetic Data Generator Webdocs5X is generated by duplicating transactions of Webdocs 5 times The ENWiki is the English Wikipedia downloaded in December 2014 We have removed all paragraphs containing non-roman characters and all MediaWiki markups The resulting dataset contains about 1,461,281 articles 11,507,383 sentences deìned as transactions with 6,322,092 unique words We construct a dictionary by ranking the words using their frequencies Capital letters are all converted into lower case and numbers are replace with the special NUM word In natural language processing eld the idea that some aspects of word semantic meaning can be induced from patterns of word co-occurrence is becoming increasingly popular The association rule mining provides a suite of efìcient tools for computing such co-occurred word clusters Apriori Apriori Figure 5 shows the performance comparison between our Apriori-AP solution and the classic Apriori-CPU implementation on three real-world datasets The computation time of Apriori-CPU grows exponentially as minimum support number decreases for three datasets while Apriori-AP shows much less computation time and much slower growth of computation time as minimum support number decreases As a result the speedup of Apriori-AP over Apriori-CPU grows as support decreases and achieves up to 129X speedup The drop in the speedup at the relative minimum support of 0.1 for Webdocs is caused by switching from 8-bit encoding to 16-bit encoding which doubles the size of the input stream The speedup increases again after this point For small and dense datasets like Pumsb data processing time is relatively low while the symbol replacement takes up to 80 of the total computation time Though the symbol replacement is a light-weight reconìguration frequent symbol replacement decreases the AP hardware utilization Also the increasing CPU time of Apriori-AP on small and dense datasets leads to a smaller relative utilization of the AP when the minimum support decreases In contrast larger datasets like Accidents and Webdocs spend relatively more time on data processing and the portion of data processing time goes up as the support decreases This analysis indicates our Apriori-AP solution exhibits superior relative performance for large datasets and small minimum support values Figure 6 shows similar trends of Apriori-AP speedup over Apriori-CPU on three synthetic benchmarks Up to 94X speedups are achieved for the T100D20M dataset In all above the cases the difference between the counting speedup and overall speedup is due to the computation on the host CPU This difference will decrease as the total computation time increases for large datasets The symbol replacement latency can be quite important for small and dense datasets that require multiple passes in each iteration but this latency may be signiìcantly reduced in future generations of the AP Figure 7 shows Table I 
Real-World Datasets Synthetic Datasets 
Name Trans Aver Len Item Size MB Pumsb 49046 74 2113 16 Accidents 340183 33.8 468 34 Webdocs 1692082 177.2 5267656 1434 ENWiki 11507383 70.3 6322092 2997.5 Aver Len  Average number of items per transaction Table II Name Trans Aver Len Item ALMP Size MB T40D500K 500K 40 100 15 49 T100D20M 20M 100 200 25 6348.8 Webdocs5X 8460410 177.2 5267656 N/A 7168 Aver Len  Average number of items per transaction ALMP  Average length of maximal pattern 
   
695 


0.80 0.76 0.72 0.68 0.64 0.60 0.56 0.52 0.48 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0   Apriori-AP counting speedup Apriori-AP overall speedup  Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time 
0.1 1 10 100 3000 2000 1000 0 1000 2000 3000 4000 5000 
0.6 0.5 0.4 0.3 0.2 0.1 0.0 10 100 
1 10 100 6000 4000 2000 0 2000 4000 6000 8000 10000 
0.20 0.19 0.18 0.17 0.16 0.15 0.14 0.13 0.12 0.11 0.10 0.09 0.08 0.07 0.06 0.05 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0 Apriori-AP counting speedup Apriori-AP overall speedup  Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time  0 10000 5000 15000 c Webdocs Figure 5 The performance results of Apriori-AP on three real-world benchmarks DP time SR time and CPU time represent the data process time on AP symbol replacement time on AP and CPU time respectively Webdocs switches to 16-bit encoding when relative minimum support is less then 0.1 8-bit encoding is applied in other cases 
E vs Eclat Eclat et al Eclat Eclat 
0.80 0.75 0.70 0.65 0.60 0.55 0 100 200 300 400 Computation Time \(s Relative minimum support 1.0X Symbol replacement time 0.5X Symbol replacement time 0.1X Symbol replacement time Figure 7 The impact of symbol replacement time on Apriori-AP performance for Pumsb how symbol replacement time affects the total AprioriAP computation time A reduction of 90 in the symbol replacement time leads to 2.3X-3.4X speedups of the total computation time The reduction of symbol replacement latency will not affect the performance behavior of AprioriAP for large datasets since data processing dominates the total computation time 
Apriori Eclat Equivalent Class Clustering   is another algorithm based on Downward-closure uses a vertical representation of transactions and depth-ìrst-search strategy to minimize memory usage Zhang  proposed a h ybrid depth-ìrst/breadth-ìrst search scheme to expose more parallelism for both multi-thread and GPU versions of  However the trade-off between parallelism and memory usage still exists For large datasets the nite memory main or GPU global memory will become a limiting factor for performance and for very large datasets the algorithm fails While there is a parameter which can tune the trade-off between parallelism and memory occupancy we simply use the default setting of this parameter for better performance Figure 8 shows the speedups that the sequential 
0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0 AP counting speedup Apriori-AP overall speedup    Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time  b Accidents 
T40D500K Counting T40D500K Overall T100D20M Counting T100D20M Overall Webdocs5X Counting Webdocs5X Overall Speedup Relative Minimum Support Figure 6 The speedup of Apriori-AP over Apriori-CPU on three synthetic benchmarks 
1 10 100 
a Pumsb 
696 


0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.1 1 10 Pumsb Accidents T40D500K Webdocs Webdocs5X T100D20M ENWiki Speedup Relative Minimum Support 
Figure 8 The performance comparison of CPU sequential and algorithm achieved with respect to sequential Apriori-CPU Though has 8X performance advantage in average cases the vertical bitset representation become less efìcient for sparse and large dataset high trans and freq item ratio This situation becomes worse as the support number decreases The Apriori-CPU implementation usually achieves worse performance than  though the performance boost of counting operation makes Apriori-AP a competitive solution to parallelized  Three factors make a poor t for the AP though it has better performance on CPU 1 requires bit-level operations but the AP works on byte-level symbols 2 generates new vertical representations of transactions for each new itemset candidate while dynamically changing the values in the input stream is not efìcient using the AP 3 Even the hybrid search strategy cannot expose enough parallelism to make full use of the AP chips Figure 9 and 10 show the performance comparison between Apriori-AP 45nm for current generation of AP and sequential multi-core and GPU versions of  Generally Apriori-AP shows better performance than sequential and multi-core versions of  The GPU version of shows better performance in Pumsb Accidents and Webdocs when the minimum support number is small However because of the constraint of GPU global memory Eclat-1G fails at small support numbers for three large datasets ENWiki T100D20M and Webdocs5X ENWiki as a typical sparse dataset causes inefìcient storage of bitset representation in Eclat and leads to early failure of EclatGPU and up to 49X speedup of Apriori-AP over Eclat-6C In other benchmarks Apriori-AP shows up to 7.5X speedup over Eclat-6C and 3.6X speedup over Eclat-1G This gure also indicates that the performance advantage of AprioriAP over GPU/multi-core increases as the size of the dataset grows The AP D480 chip is based on 45nm technology while the Intel CPU Xeon E5-1650 and Nvidia Kepler K20C on which we test are based on 32nm and 28nm technologies respectively To compare the different architectures in the same semiconductor technology mode we show the performance of technology projections on 32nm and 28nm technologies in Figure 9 and 10 assuming linear scaling for clock frequency and square scaling for capacity The technology normalized performance of Apriori-AP shows better performance than multi-core and GPU versions of Eclat in almost all of the ranges of support that we investigated for all datasets with the exception of small support for Pumsb and T100D20M Apriori-AP achieves up to 112X speedup over Eclat-6C and 6.3X speedup over Eclat-1G The above results indicate that the size of the dataset could be a limiting factor for the parallel algorithms By varying the number of transactions but keeping other parameters xed we studied the behavior of Apriori-AP and Eclat as the size of the dataset increases Figure 11 For T100 the datasets with different sizes are obtained by the IBM synthetic data generator For Webdocs the different data sizes are obtained by randomly sampling the transactions or by concatenating duplicates of the whole dataset In the tested cases the GPU version of fails in the range from 2GB to 4GB because of the nite GPU global memory Comparing the results using different support numbers on the same dataset it is apparent that the smaller support number causes Eclat-1G to fail at a smaller dataset This failure is caused by the fact that the ARM with a smaller support will keep more items and transactions in the data preprocessing stage While not shown in this gure it is reasonable to predict that the multicore implementation would fail when the available physical memory is exhausted However Apriori-AP will still work well on much larger datasets assuming the data is streamed in from the hard drive assuming the hard drive bandwidth is not a bottleneck VII C ONCLUSIONS AND THE F UTURE W ORK We present a hardware-accelerated ARM solution using Micronês new AP architecture Our proposed solution includes a novel automaton design for matching and counting frequent itemsets for ARM The multiple-entry NFA based design was proposed to handle variable-size itemsets MENFA-VSI and avoid routing reconìguration The whole design makes full usage of the massive parallelism of the AP and can match and count up to itemsets in parallel on an AP D480 48-core board When compared with the based single-core CPU implementation the proposed solution shows up to 129X speedup in our experimental 
Apriori Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat F Normalizing for technology Eclat G Data size Eclat Eclat Eclat Apriori 
18 432 
 
697 


0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 1 10 100 1000 10000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm a Webdocs\(1.4GB 0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 10 100 1000 10000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails d Webdocs5X 7.1GB Figure 10 Performance comparison among Apriori-AP Eclat-1C Eclat-6C and Eclat-1G with technology normalization on four large datasets 
0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(second Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm b Accidents 34MB 0.40 0.36 0.32 0.28 0.24 0.20 0.16 0.12 0.08 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm c T40D500K\(49MB Figure 9 Performance comparison among Apriori-AP Eclat1C Eclat-6C and Eclat-1G with technology normalization on three small datasets results on seven real-world and synthetic datasets This APaccelerated solution also outperforms the multicore-based and GPU-based implementations of 0.60 0.55 0.50 0.45 0.40 0.35 0.30 0.25 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails c T100D20M 6.3GB 
ARM a more efìcient algorithm with up to 49X speedups especially on large datasets When performing technology projections on future generations of the AP our results suggest even better speedups relative to the equivalent-generation of CPUs and GPUs Furthermore by varying the size of the datasets from small to very large our results demonstrate the memory constraint of parallel ARM particularly for GPU 
Eclat Eclat 
0.80 0.76 0.72 0.68 0.64 0.60 0.56 0.52 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm 
0.20 0.15 0.10 0.05 0.00 1 10 100 1000 10000 100000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(second Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails b ENWiki\(3.0GB 
a Pumsb 16MB 
698 


 Frequent pattern mining Current status and future directions  2007  R Agra w al and R Srikant F ast algorithms for mining association rules in  1994  M J Zaki Scalable algorithms for association mining  vol 12 no 3 pp 372Ö390 2000  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation in  2000  Y  Zhang F  Zhang and J Bak os Frequent itemset mining on large-scale shared memory machines in  2011  F  Zhang Y  Zhang and J D Bak os  Accelerating frequent itemset mining on graphics processing units  vol 66 no 1 pp 94Ö117 2013  Y  Zhang  An fpga-based accelerator for frequent itemset mining  vol 6 no 1 pp 2:1Ö2:17 May 2013  P  Dlugosch  An efìcient and scalable semiconductor architecture for parallel automata processing  vol 25 no 12 2014  I Ro y and S Aluru Finding motifs in biological sequences using the micron automata processor in  2014  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases in  1993  H No yes Micronês automata processor architecture Reconìgurable and massively parallel automata processing in  June 2014 keynote presentation  M J Zaki  Parallel data mining for association rules on shared-memory multi-processors in  1996  L Liu  Optimization of frequent itemset mining on multiple-core processor in  2007  I Pramudiono and M Kitsure ga w a P arallel fp-gro wth on pc cluster in  2003  E Ansari  Distributed frequent itemset mining using trie data structure  vol 35 no 3 p 377 2008  W  F ang  Frequent itemset mining on graphics processors in  2009  B Goethals Surv e y on frequent pattern mining  Univ of Helsinki Tech Rep 2003  C Bor gelt Ef cient implementations of apriori and eclat in  2003 p 90  Frequent itemset mining dataset repository   http mi.ua.ac.be/data  J Rabae y  A Chandrakasan and B Nik oli  c  2nd ed Pearson Education 2003 
100 1000 10000 5 50 500 5000 re_sup = 0.12 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails for re_sup = 0.08 re_sup = 0.08 
GPU fails re_sup = 0.42 re_sup = 0.42 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm  Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails re_sup = 0.3 re_sup = 0.3 b T100 Figure 11 Performance prediction with technology normalization as a function of input size implementation In contrast the capability of our AP ARM solution scales nicely with the data size since the AP was designed for processing streaming data With the challenge of the big data era a number of other complex pattern mining tasks such as frequent sequential pattern mining and frequent episode mining have attracted great interests in both academia and industry We plan to extend the proposed CPU-AP infrastructure and automaton designs to address more complex pattern-mining problems A CKNOWLEDGMENT This work was supported in part by the Virginia CIT CRCF program under grant no MF14S-021-IT by C-FAR one of the six SRC STARnet Centers sponsored by MARCO and DARPA NSF grant EF-1124931 and a grant from Micron Technology R EFERENCES  J Han 
et al Data Min Knowl Discov Proc of VLDB 94 IEEE Trans on Knowl and Data Eng Proc of SIGMOD 00 Proc of CLUSTER 11 J Supercomput et al ACM Trans Reconìgurable Technol Syst et al IEEE TPDS Proc of IPDPSê14 Proc of SIGMOD 93 Proc of Fifth International Symposium on Highly-Efìcient Accelerators and Reconìgurable Technologies et al Proc of Supercomputing 96 et al Proc of VLDB 07 Proc of PAKDD 03 et al IAENG Intl J Comp Sci et al Proc of DaMoN 09 Proc FIMI 03 Digital Integrated Circuits 
1 10 100 1000 10000 0.1 1 10 100 1000 
a Webdocs 
699 


