The Application of Associat ion Rules Algorithm in The Prediction of Drilling Troubles Hongwu Luo 1,2,a Jinhuan Yin 3,b Xiaofei Jia 4,c  1.Chengdu University of Technology, Chengdu 2.China Research Institute of Petroleum Exploration & Development-Northwest, PetroChina, Lanzhou, China 3. China University of Petroleum \(East China\handong Dongying, China 4. Bohai Oilfield Exploration and Development Research Institute,Tianjin Branch of CNOOC Ltd., Tianjin a. luohw@petrochina.com.cn; b. yinjinhuan109@163.com; c. jiaxf@cnooc.com.cn   Abstract Because the drilling complications happen randomly 
and uncertainly, this article uses association rules algorithm to analyze and predict the connections among drilling troubles and its researching data is the complications happened around the oil well, then draw a conclusion whether one or more complications can cause others, so as to improve the forecasting accuracy of drilling troubles, and we can take some preventive measures in advance and reduce unnecessary losses in the end Keywords-association rules algo rithm; drilling troubles;pre diction I  I NTRODUCTION  In the drilling process, drilling troubles may occur at any 
time, such as well leakage, blowout and so on. If not detected early, they can lead to not only the waste of more human material resources and time, but also the well abandoned and personal injury. Whether we can predict the occurrence of one or more complications, whether one complication can cause other complications, so we can take measures to minimize the enormous losses that complications caused in advance. It is good practical guidance for drilling engineering At present, each big oilfield mainly rely on expert system[1 t o  p r e d ic t th e co m p licati o n  w h ich h a s str o n g  subjective initiative and the accuracy is not high. Judging from home and abroad research, in the field of computer 
technology, we mainly make use of neural network[2 t o  predict a kind of complex situations, but the studies of the connections among the complex situations are rare. Taking many years’ well history information as the researching data this paper studies the drilling troubles happened around and establishes model with association rules algorithm to improve the efficiency and accuracy of the drilling complex situation forecast and provide a reference for further research in this field II  T HE IDEA OF THE APPLICATION OF ASSOCIATION RULES ON DRILLING COMPLEX SITUATION FORECAST  The main characteristics of association rule mining is to 
discover the potential associations among items[5  In  th e  process of drilling complex situation forecast, we consider each oil well as a transaction, regard complications that have happened as items , so as to find out the potential associations among them The more similar the factors of geology and engineering are, the more similar complications that have happened are Therefore, this article regards the complication that have occurred around this oil well as the research object, and the coordinates of the research object depend on whether the factors of geology and engineering are similar, that is to say 
different oil may determine different research object. The next is to generate a library from association rule mining then, based on the library and the complications that have occurred in this oil well, it is to predict whether the complication that have occurred can cause other complications or not. The last is to analyze the forecast results. In addition, if this prediction is not in the library, it will be added into the library III  T HE APPLICATION PROCESS OF THE ASSOCIATION RULES IN OIL COMPLICATION  The data format of well history information is not well suited to the requirements of association rules, therefore, the 
first need is to preprocess the well history information; the following is the steps of association rules mining, including looking for frequent item sets and association rules; finally the result of analysis is to obtain the relevance of drilling complications; shown in Fig. 1     
   A   Data preparation stage Data preparation is a process after which we can obtain the data format that association rule mining process required through data pre-processing. therefore, the data preparation 


stage includes three steps, that is the original data’s selection cleaning and integration, conversion. To gain the data that association rules required ultimately, the specific process is shown in Fig. 2   1\ Select the useful information in raw data Well history information including well number, start time, end time, recording contents, recorder and other property were listed as TABL shows, of which only the well number and contents is the data that the association rule mining needs, therefore, the first is to select the necessary attributes for association rules algorithm from well history information TABLE I  ORIGINAL WELL HISTORY INFORMATION  well number start time end time recording contents recorder oil Well A  Drilling to well dept h  2181.53m,severe limp and jump drill suddenly appear, the weight of drill was from 850kN down to 650kN, pump pressure reduceed from 17MPa to15MPa.Thus judging from this, drilling down accident occurred. Drill wells to the well depth 1580m met difficulties, on 18 wipe eyes to well depth 2072.85m \(head position\ll out of drill down to well depth 169.14m sticking  2\ clean-up and integrate the raw data As the drilling situation is not unique and successive, the records for the same well in well history information is scattered too. That is, the same well in the database may correspond to multiple records spaced by the data records of other wells therefore, need to clean up the raw data and integration the specific implementation process is shown in Fig. 3  Knowing from Fig. 3, after clean-up and integration, the form of the data stored in the database is shown in TABLE  TABLE II  THE DATA TABLE OF WELL COMPLEX SITUATIONS  Well number Recording contents Oil well A Drilling to well depth 2181.53m,severe limp and jump dril l suddenly appear, the weight of drill was from 850kN down to 650kN, pump pressure reduced from 17MPa to 15MPa Thus,judging from this, drilling down accident occurred. Drill wells to the well depth 1580m met difficulties, on 18 wipe eyes to well depth 2072.85m \(head position\pull out of drill down to well depth 169.14m sticking 3\he conversion of raw data Data conversion uses the string matching algorithm specific steps are as follows 1\ First, establish a data dictionary, where 10 kinds of the complex case as an example are used to set up the data dictionary, as shown in TABLE in which each situation as a substring named T, and recording contents are the main string named S TABLE III  D ATA D ICTIONARY  number 1 2 3 4 5 6 7 8 9 10 String T Wells Bay well blowout Well fall Lost circulation Pipesticking Gas cut Barbed drill drill pipe falling Borehole plugging End bit 2\ Second, match each T and S by order, if the match succeeded, that is to say the complication has happened, set the value of the corresponding property of the transaction as 1, otherwise, match is unsuccessful, that is to say the complicated situation did not happen, put the value of the corresponding attribute of the transaction as 0. After data conversion, the form of data is shown in TABL  TABLE IV  DATA TABLE AFTER CONVERSION  Complicatio n  well number Well Bay well b lowou t Well fall Lost circulation Pipesticking Gas cut Barbed drill drill pipe falling Borehole plugging En d  bit Oil well A 0 0 0 0 1 0 0 1 0 0 A  Establish Forecasting Model for Association Rules 1\etermine the scope of the original data The first step to determine the scope of the original data is to determine the coordinates of center of oil wells, in this instance, the coordinates of A is the well center; the next step is to determine a radius R and ensure that similar geological 


conditions and other factors; the last is to draw a circle so that the well history information which wellhead coordinates are in this circle,is the original data used to build the association rule model 2\ Select the target data In mining Association Rules, only two or more kinds of complications happened, can this well have good values for data mining, thus, select the records in which the number of complications that have happened is more than two as target data. In this application example, after data preprocessing we finally collect the data of 1021 oil wells, that is to say there are 1021 records in transaction database, part records are shown in Fig. 4 Fig. 4 part records \(target data 3\Establish Association rules model for the association rules Application of association rules algorithm is the process of mining association rules, including two steps, that is to find frequent item sets and association rules. In the actual mining process, the determination of the minimum support and minimum confidence is very important, if the value is set too large, some rules we need may not come out; if the minimum support is set too small, it will generate a large number of frequent item sets and a large number of redundant association rules, and spend a long time Experienced designers can determine the appropriate minimum support and confidence as quickly as possible after using the prediction model in a long time. In this application example, we initially set the minimum support as 5%, set the minimum confidence as 40%, and use the association rules algorithm that is  mentioned in reference 6 to establish models, finally generate 86 rules after data mining, take some for example, as shown in TABL  TABLE V  SOME ASSOCIATION RULES  association rules support confidence Lost circulation => Well Bay 15.3 66.7 Wells Bay + Wells all => drill pipe falling 13.4 43.3 Wells Bay + Lost circulation drill pipe falling 21.6 83.2 Well Bay => well blowout 10.4 42.7 B  Application of association rules and result analysis Suppose in a oil well lost circulation has occurred knowing from the association rules that lost circulation is likely to lead to the occurrence of Wells Bay. Because when lost circulation happens, the drilling fluid liquid will drop liquid pressure become lower as well, then the groundborehole pressure balance system will lose when the drilling fluid pressure below the pore pressure, then fluid will flow into the well bore resulting in the occurrence of wells bay Wells bay without control will cause the emitted fluid or formation fluid increased over turntable after the above, what we call a blowout Thus, there are close association in all complications happened in drilling process. Some strong rules can be mined from the application of association rules, so that we can take preventive measures to reduce unnecessary losses IV  C ONCLUSION  By the way of studying the complications happened around the well, association rules can predict accurately whether one or more complications happened will lead to others, then connections among complications can be found in time, so staff can adjust the drilling fluid by the concentration of parameters in advance to prevent possible complications, minimizing their losses and impact, and even prevent it from happening. The next research is to combine the forecast from causes and the forecast from complications to achieve a comprehensive drilling forecast R EFERENCES  1  Wenjian Zhu, Xuezeng Guo. study on the expert system for real-time predicting kick and lost circulation[J   Mo de r n  Geology,1997.3,11\(1\:86-89.\(in Chinese 2  Jiangping Wang, Xiangqin Meng, Zefu Bao. Diagnosis of drilling faults using neural network technology[J J o ur n a l o f X i  a n S h iy o u  University\(Natural Science Edition\,2008.3,23\(2\:99-102. \(in Chinese 3  Xingli Han. The application of the neural network technology in forecasting and predicting drilling engineering accident[J  M u d  Logging Engineering,2007,18\(1\:28-32. \(in Chinese 4  Yusheng Shi,Shuyun Liang. real time diagnosis and modeling for optimization model using neural network-the second part of application of artificial intelligence to drilling engineering[J   geology and prospecting, 1999.5,35\(3\9-53. \(in Chinese 5  Agrawal R,Imielinske T,Swami A.Mining association rules between sets of items in large databases  P r o c eed i n g of t h e A C M S I C M O D  conference on management of data,1993.5:207-216 6  Wendong Zhang, Jinhuan Yin,and so on. Research of frequent itemsets mining algorithm based on vector[J o ur n a l o f S h an do ng  University\(Natural Science\,2010.11,45\(11\. \(in Chinese         


roulette and therefore they will be more likely selected In the rst evolutionary process all samples have the same probability to be selected Constraints to generate individuals are given by the number of attributes that belong to rule represented by an individual the number of attributes in the antecedents and consequents and the structure of the rule attributes xed or not xed in consequent D Genetic Operators The genetic operators implemented in the genetic algorithm proposed are Crossover and Mutation described in  In addition a n e w Mutation operator has been added Concretely the Antecedent  Consequent Mutation that works as follow If the type   of the selected attribute is antecedent 1 changed to consequent 2 else if the type   of the selected attribute is consequent 2 changed to antecedent 1 IV R ESULTS We applied our methodology to the microarray datasets of Spellman and Cho for the budding yeast Saccharomyces cerevisiae cell-cycle and 23 These data were synchronized by three different methods cdc15 cdc28 and alpha-factors Therefore these three gene expression data sets may be dened as statistically independent The same training experiments with cdc15 dataset used by Soinov et al in were analyzed to achie v e a comparison between the two methods We considered a set of welldescribed genes which encode proteins important for cellcycle regulation We selected these genes for the performance analysis of the proposed method in order to establish comparisons with the previous study A Parameters con“guration As the proposed algorithm is non-deterministic it has been executed ve times for the dataset The main parameters are as follows 100 for the number of the rules to obtain 50 for the size of the population 50 for the number of generations 0.1 for the mutation probability   of the individuals 0.2 for the mutation probability   015 of each gene in the individual B Discussion of Results In order to choose the best individual rule of each generation the individual with the highest support value in the rst Pareto front has been selected in order to cover the maximum number of examples by the obtained rules We have extracted the relationships between attributes belonging to the antecedent and attributes belonging to the consequent for each AR found by the proposed algorithm in each run For example if we have the following rule 015  0.2     0.3 015   0.5 the relationships or associations between the attributes of the antecedent and consequent of the rule are 015    and 015    Then we have built a graph with associations derived from the rules where each attribute that belongs to the rule is a graph node and each association obtained between attributes is an edge of the graph For the resulting graph we performed the intersection between the graphs obtained in each of the ve executions carried out by the algorithm in order to nd the frequent interrelations between genes Table I shows some of the QAR obtained by the algorithm resulting after performing the intersection of the graphs constructed for each algorithm execution The Sup Rule column shows the support of the rule that is the percentage of samples covered by the rule The Conf column indicates the probability that instances satisfying the antecedent also satisfy the consequent The Lev column presents the leverage of the rule and measures the proportion of additional cases covered by both antecedent and consequent above those expected if they were independent of each other The Acc column describes the accuracy of the rule and means the percentage success of the rule The CF column presents the Certainty Factor of the rule The interest of the rule is shown in column Lift and the Amp column presents the average amplitude of the intervals of the attributes belonging to each rule It is important that the values of all interestingness measures of the AR are as high as possible For better understanding Table I shows rules containing 2 attributes one attribute in the antecedent and one in the consequent Rules formed by 3 attributes are shown only for the relationships of genes that are not obtained in any rule of 2 attributes Because the format of the rules obtained by the algorithm is not xed that is any attribute may belong to the antecedent or the consequent rules have been obtained with the same attributes but the sense of the implication of the association is different For example rules 0 and 1 rules 3 and 4 which are represented as directed edges in the graph in Figure 3 We can see that the support value of all rules between 25  and 50  is good enough for the problem at hand Equally remarkable the values of condence certainty factor and accuracy for most of the rules is equal to 1 or very close to 1 which means that these measures have their highest value and indicates that the rule is totally accurate and the implication of the rule is perfect The lift and leverage values are quite high and this means that the rules are interesting and provides valuable information about antecedent and consequent occurring together in the dataset In addition the proportion of instances covered by both antecedent and consequent is greater than ones covered by antecedent and consequent separately Leverage is a lower bound for support so optimizing leverage guarantees 12 44 2011 11th International Conference on Inte lligent Systems Design and Applications 


Table I Q UANTITATIVE A SSOCIATION R ULES AND G ENE G ENE ASSOCIATIONS INFERRED BY THE PROPOSED ALGORITHM  ID Rule Sup Rule Conf Lev Acc CF Lift Amp Gene-Gene associations Soinov inferred by our method 0    0.23      0.84 0.292 1 0.207 1 1 3.429 0.26 CLN1 CLN2 1    0.61      0.2 0.333 1 0.222 1 1 3 0.296 CLN2 CLN1  2    0.23      1.34 0.5 0.857 0.184 0.875 0.688 1.582 0.332 CDC20 CLN1  3    1.37      1.74 0.5 1 0.25 1 1 2 0.496 CLB1 CLB2  4    1.74      1.37 0.458 1 0.229 0.958 1 2 0.483 CLB2 CLB1  5    0.92      0.58 0.375 1 0.219 0.958 1 2.4 0.285 CLB6 CLB5  6    0.58      0.92 0.333 1 0.208 0.958 1 2.667 0.254 CLB5 CLB6  7    0.61      0.25 0.333 1 0.222 1 1 3 0.352 CLN2 CLB5 8    0.42      1.02 0.458 1 0.172 0.833 1 1.6 0.399 CLB2 CLB5 9    0.24      0.56 0.542 1 0.226 0.958 1 1.714 0.418 CLB2 SW15  10  012  1.17      0.28 0.458 1 0.248 1 1 2.182 0.45 CDC34 MBP1  11    0.52    012  1.17 0.417 1 0.243 1 1 2.4 0.352 MBP1 CDC34  12    0.52      1.47 0.375 1 0.203 0.917 1 2.182 0.358 MBP1 SKP1  13    0.83      0.52 0.33 1 0.194 0.917 1 2.4 0.241 SKP1 MBP1 14    0.3      1.88 0.375 1 0.18 0.875 1 2 0.321 SW15 CLN2  15    0.07      1.88 0.458 0.917 0.208 0.917 0.833 1.833 0.469 CLB1 CLN2 16    1.37      1.46 0.333 1 0.194 0.917 1 2.4 0.349 CLB1 SW15  17    1.74   0.458 1 0.248 1 1 2.182 0.481 CLB2 CLN2     1.37     0 18    1.12     0.62   0.458 1 0.21 0.917 1 1.846 0.387 CDC53 SKP1    0.21 19  012  0.14   012  0.09   0.417 1 0.243 1 1 2.4 0.387 SW14 CDC34  012  0.06     0.59 a certain minimum support contrary to optimizing only condence or only lift C Biological Relevance The associations inferred by our approach are summarized in the tenth column of Table I The eleventh column of Table I indicates gene-gene associations that were also inferred by the proposed methods by Soinov in  using the same dataset The Gene Regulatory Network corresponding to the rules inferred by our approach and Soinov is shown in Figure 3 and 4 respectively CLB1 SKP1 MBP1 SW15 CLB2 CDC53 CLN2 CLB5 CLN1 SW14 CDC34 CLB6 CDC20 Figure 3 Directed graph obtained by the proposed algorithm In summary all rules inferred by the decision-tree-based method 13 in total were also inferred by our approach with the addition of new seven rules inferred only by our proposal The biological relevance of the rules inferred by our approach was veried by analyzing whether such rules reect functional properties relating to the different CLN1 SW15 CLN2 CLB1 CLB5 MBP1 CDC20 CDC34 CLB2 SKP1 CLB6 Figure 4 Directed graph obtained by Soinov cell-cycle phase The rules which are supported by the literature are 3 4 5 6 9 10 11 12 14 16 The rules 1 and 2 are consistent with the prior knowledge and are detected by Soinov The rules which are not supported by the literature i.e 0 y and 7 are new hypothesis to analyze in the laboratory V C ONCLUSION A multi-objective evolutionary algorithm for mining quantitative association rules has been proposed in this work The approach is based on the well-known NSGA-II and has determined the intervals that form the rules without discretizing the attributes as a rst step of the process In order to evaluate its performance the approach has been applied in a dataset and compared to other published results The results report the relevance and signicance in the group of genes found in the rules obtained for the problem studied in terms of support condence accuracy interest and leverage As a conclusion an advantage of network reconstruction using our approach is that the method is able to construct 2011 11th International Conference on Inte lligent Systems Design and Applications 12 45 


a network correctly i.e reproducing the logic of a network consistent with the data as The netw ork reconstructed from cell cycle yeast dataset is consistent with the knowledge store in the literature Furthermore the method can be improve by adding prior knowledge and more gene expression proles Our method constitute an interactive expert system for gene association networks where the expert decides when to stop adding new gene expression proles and what biological meaning represent the network A CKNOWLEDGMENT The nancial support from the Spanish Ministry of Science and Technology project TIN2007-68084-C-00 and from the Junta de Andaluca project P07-TIC-02611 is acknowledged R EFERENCES  P  Bro w n and D Botstein Exploring the ne w w orld of the genome with dna microarrays Nature Genet  vol 21 no Suppl pp 33–37 1999  F  Azuaje and Y  D  adn DR W agner  Coordinated modular functionality and prognostic potential of a heart failure biomarker-driven interaction network BMC Syst Biol  vol 4 p 60 2010  R Agra w a l and R Srikant F ast algorithms for mining association rules in large databases in Proceedings of the International Conference on Very Large Databases  1994 pp 478–499  M V annucci and V  Colla Meaningful discretization of continuous features for association rules mining by means of a som in Proceedings of the European Symposium on Arti“cial Neural Networks  2004 pp 489–494  E D Goldber g Genetic Algorithms in Search Optimization and Machine Learning  Addison-Wesley Publishing Company 1989  J Alcal  a-Fdez N Flugy-Pape A Bonarini and F Herrera Analysis of the effectiveness of the genetic algorithms based on extraction of association rules Fundamenta Informaticae  vol 98 no 1 pp 1001–1014 2010  J Mata J L  Alvarez and J C Riquelme Discovering numeric association rules via evolutionary algorithm Lecture Notes in Arti“cial Intelligence  vol 2336 pp 40–51 2002  X Y an C Zhang and S Zhang Genetic algorithm-based strategy for identifying association rules without specifying actual minimum support Expert Systems with Applications An International Journal  vol 36 no 2 pp 3066–3076 2009  K Deb Multi-Objective Optimization Using Evolutionary Algorithms  John Wiley  Sons Inc 2001  K Deb A Pratap S Agarw al and T  Me yari v an  A f ast and elitist multiobjective genetic algorithm Nsga-ii Evolutionary Computation IEEE Transactions on  vol 6 no 2 pp 182 197 2002  E Zitzler  M  Laumanns and L  Thiele Spea2 Impro ving the strength pareto evolutionary algorithm EUROGEN vol 3242 no 103 pp 95  100 2001  B Alatas E Akin and A Karci MODEN AR Multiobjective differential evolution algorithm for mining numeric association rules Applied Soft Computing  vol 8 no 1 pp 646–656 2008  H Qodmanan M Nasiri and B Minaei-Bidgoli Multi objective association rule mining with genetic algorithm without specifying minimum support and minimum condence Expert Systems with Applications  vol 38 no 1 pp 288–298 2011  M Mart  nez-Ballesteros F Mart  nez Alvarez A Troncoso and J C Riquelme Mining quantitative association rules based on evoluationary computation and its application to atmospheric pollution Integrated Computer-Aided Engineering  vol 17 pp 227–242 2010  M Mart  nez-Ballesteros F Mart  nez Alvarez A Troncoso and J Riquelme An evolutionary algorithm to discover quantitative association rules in multidimensional time series Soft Computing  vol 15 no 10 pp 2065–2084 2011  J Han and M Kamber  Data Mining Concepts and Techniques  Morgan Kaufmann 2006  R Agra w a l T  Imielinski and A Sw ami Mining association rules between sets of items in large databases in Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data  1993 pp 207–216  L Geng and H  Hamilton Interestingness measures for data mining A survey ACM Comput Surv  vol 38 no 3 p 9 2006  G Piatetsk y-Shapiro Disco v e ry  a nalysis and presentation of strong rules in Knowledge Discovery in Databases  1991 pp 229–248  B Miller and D  Goldber g Genetic algorithms tournament selection and the effects of noise Complex Systems vol.9 pp 193–212 1995  G V e nturini SIA A Supervised Inducti v e Algorithm with genetic search for learning attribute based concepts in Proceedings of the European Conference on Machine Learning  1993 pp 280–296  P  Spellman G Sherlock M Zhang V  Iyer  K  A nders M Eisen P Brown D Botstein and B Futcher Comprehensive identication of cell cycle-regulated genes of the yeast saccharomyces cerevisiae by microarray hybridization Mol Biol Cell 1998  vol 9 pp 3273–3297 1998  R Cho M Campbell E W inzeler  L  Steinmetz A Conw ay  L Wodicka T Wolfsberg A Gabrielian D Landsman D Lockhart and R Davis A genome-wide transcriptional analysis of the mitotic cell cycle Mol Cell  vol 2 pp 65–73 1998  L Soino v  M Krestyanino v a  a nd A Brazma T o w ards reconstruction of gene networks from expression data by supervised learning Genome Biology  vol 4 p R6 2003 12 4 6 2011 11th International Conference on Inte lligent Systems Design and Applications 


minutes or through a continuous trickle feed Once the load interval \(e.g. five minutes\is up, the freshly loaded tables are simply swapped into production and the tables with the now stale data are released from production 10  This can be accomplished through the dynamic updating of views by simple renaming tables or by swapping partitions The downside of this type of n minute cycle-loading process is that the data in the EventBase is not truly real time For applications where true real-time data is required, the best approach is to continuously trickle-feed the changing data from the source system directly into the EventBase. Obviously, near real-time data integration cannot be as fast as bulk load integration Therefore it is necessary to carefully identify critical data which needs to be integrated continuously with minimized latency In the following, an approach is introduced for integrating trickle-feed event data with event trace capture and loading ETCL\. A key difference of ETCL to traditional bulk loading of data warehouse systems can be summarized as follows Trace Files instead of Flat Files  Traditional ETL tools use flat files where records of the same type are collected and periodically loaded into the database. ETCL uses event traces which collect in memory the records resulting from the processing of a single event. Thereby, various types of records e.g records for event data metrics scores etc are created Finally, the event trace is applied to the database Size of Data Chunks  The data loads for ETCL handle smaller data sets and is performed after the completing the processing of an event. The size of the data to be loaded into the database depends on the event processing results Loading Data from Memory  During the event processing the SARI system collects event trace data in memory. When loading the data into the EventBase, the event trace data is directly transferred to the database without using intermediate storage \(e.g. a flat-file Consistent Event Data A trace file keeps a record of the result from event processing tasks and thereby maintains the temporal order the created records When bulk-loading data the records are split into flat files which are individually loaded into the database During the data loading simultaneous user queries must be turned off in order to avoid locked database records and inconsistent query results ETCL solves the problem by continuously applying consistent fragments of the event data trace during the loading process thereby keeping the stored event data consistent When capturing data for the event trace the event data is process ed  in multiple stages Each stage produces its own set of records which are added to the trace On the higher level we distinguish the following types of records 1 the attribute data for events which contain the elementary information of event objects, 2\ information on relationships between events and 3\ metrics and scores which are calculated during the data staging. The records are continuously created during the event processing and finally loaded in chunks into the EventBase database. Figure 7 illustrates this process The SARI system allows the user to configure which artifacts, i.e., which parts of the event trace, shall be stored to the EventBase This configuration is called event trace outline  Note that there are several dependencies between the various trace levels For instance tracing correlations of events by activating the trace for correlation sets requires that the correlated events are also captured in the trace. Otherwise, th e event trace contains incomplete records when writing the trace to the database en-GB A  Methods for Storing Event Traces In the following we discuss multiple methods for loading the records of event traces into a database system Transactional Inserts  The simplest way of loading the data into the database is to generate and execute INSERT SQL statements By using normal INSERT SQL statements the processing is fully transactional, similar to OLTP applications Since data is only added to the EventBase, a low isolation level can be used for the transactions A major shortcoming of this approach is a high number of transactions and database round trips for storing the event trace records in the EventBase Transactional Batch Processing  The processing of the transactional INSERT statements can be optimized by processing them in a batch mode i.e by sending multiple INSERT commands to the database in a batch Another approach is to use stored procedures which can consume the event trace information as input and perform inserts on database level. Using batch processing for the data inserts can significantly improve the performance However one shortcoming of this approach is that the database system still has the overhead for processing each record with a separat e SQL statement Transactional In Memory Bulk Loading This approach combines the transactional batch processing with bulk loading capabilities. Instead of loading the data from flat files, records from the event trace are directly loaded from memory into the de-AT  Figure 7   EventBase Data Staging  


database system However many   database system do not support large in-memory data-loads directly. In order to circumvent this problem, mechanisms can be used to load data from in memory documents For instance Microsoft  SQL Server or Oracle database systems allow preparing XML documents in memory which can then be used to efficiently query data items for inserting them in one step in to  the database The data inserts for a single XML document is performed in a single transaction Using Transactional Logs for Event Traces  In this approach a database-specific event trace format is used for storing record information When applying the event trace to the database system the trace can be applied directly by the database system thereby minimizing the processing and time for the data integration. The key advantage for this approach is that data is inserted in a highly efficiently manner A major shortcoming of  this approach however is that transactional logs for database system are propri etary and vary from system to system  en-GB B  Performance Experiments with ETCL Using an event processing application from the fraud domain 23 we conducted an experiment for storing the event processing results with event tracing. We ran SARI, using the transactional batch mode  on two nodes with four-CPU Intel servers  We used a separate machine with the same technical specification as the database server   running Microsoft SQL Server 2008 The following table summarizes the perfor mance results  TABLE I  E VENT D ATA S TAGING WITH ETCL   en-GB Input  en-GB Number of source events  en-GB 16 8 232 events  en-GB  en-GB Average  number of attributes  per event  en-GB 18 event attributes  en-GB  en-GB Number of event types   en-GB 14 event types  en-GB  en-GB Event tracing mode   en-GB Transactional batch  en-GB Output  en-GB Processing t ime \(total  en-GB 4 minutes  40 seconds  en-GB  en-GB Events processed / second  en-GB 601 events   sec  en-GB  en-GB Stored events   en-GB 171.327 events  en-GB  en-GB Stored correlations   en-GB 12.301 correlations  en-GB  en-GB Metrics and score updates   en-GB 31.323 updates  en-GB  en-GB Total number of inserted or  updated database record s   en-GB 412.420 records  en-GB  en-GB Database r ecords inserted or updated / second  en-GB 1472 records   sec   We were able to store the events with the full event trace at a rate of about 600 events per second Please note that the processing of an event potentially generates new events e.g alert events\ which are also stored as part of the event trace We conducted another experiment using in memory bulk loading. By using this storage mode with SQL Server 2008, we created XML documents which containing the full event trace information and sent these XML documents to a stored procedure for inserting the data With this storage mode we were able to store 424 events per second. The lower throughput is caused by the significant amount of XML processing which increases the CPU utilization on the SARI nodes as well as on the database node Finally, we did an experiment with the transactional insert storage mode, which allowed us to store 127 events per second The significantly lower throughput was caused by the higher number of database activity due to more database roundtrips VII  E VENT D RIVEN B USINESS I NTELLIGENCE  As yet we presented the concept of EDWH and showed how events, correlations and other artifacts are persisted in the EventBase In this section we focus on the analysis part of SARI: Event-driven BI means the creation of knowledge about underlying business environments from historic event data. As such knowledge can be used to adapt and further improve realtime event-processing logic applied event-driven BI is essential for upto date and continuously refining event-based systems In the following, we demonstrate event-driven BI through a real-world use-case from the fraud detection domain Fraud detection and prevention is a major issue across technologydriven business domains relying on online payment solutions and customer interactions Suntinger et al 23  showed that fraud analysis fits particularly well with the event-driven approach Fraud analysis requires root-cause and cause-chain analysis on the level of single user-actions both in near real time for the intime prevention of ongoing fraud and a posteriori for the continuous improvement of the knowledge base\. Event-based systems use a rule-based approach to continuously evaluate the stream of customer interactions. For the expert-driven analysis of past customer data relevant events are stored in the EventBase As an exemplary analysis framework we utilize the EventAnalyzer 24 The EventAnalyzer is the first grownto maturity BI tool built upon the EventBase. It offers a range of visualization opportunities tailored to the characteristics of event data. The key visualization techniques are derived from the Event Tunnel metaphor of seeing past events flowing through a 3D cylinder and providing different viewing on it Figure 8 above shows a screenshot of the analysis framework A detailed description of the depicted panels will be given throughout the below example In the following, we assume that a well-established onlinebetting provider uses a SARI-based fraud-detection system to continuously monitor ongoing customer interactions We furthermore assume that the system automatically generates alerts for users that match known fraud patterns After receiving two temporally close fraud alerts the business analyst decides to further investigate the affected user accounts In the analysis framework the analyst formulates a query selecting the corresponding accou nt history correlationsessions from the Event Base 


In the EventAnalyzer, the definition of queries is facilitated by the query builder \(Figure 8a\: Here, analysts can select those event types that are considered relevant from the overall list of event types defined in the given event-model the same is provided for correlation sets If required for their investigations analysts can define more sophisticated clauses as, for instance, on event attribute values The event-tunnel top-view in Figure 8c shows the results of the above query: Single events are plotted as glyphs, with userdefined mappings Figure 8b from event attributes to color shape and size Correlations between events are plotted as colored bands, connecting the correlated events by their times of occurrence As depicting similar-looking glyphs at similar points in time, the plot in Figure 8c unveils that the suspicious   sers  officials by the system From visualizations of the EventBase business analysts gain deep insight into the various business processes persisted therein Query-driven and tailored for a quick and step-wise navigation through the EventBase, analytical applications such as the Event Analyzer push it one step further: From an initial clue such as the above fraud alarms they allow the analyst detecting previously unconsidered facts and relationships Consider the business analyst formulating a new query on the event-data warehouse now selecting all events from the BetPlaced-event table that are a   occurred recently. The corresponding event-tunnel side-view in Fig ure 8d scatter of occurrence \(x axis\. It is easy to see that soon after the detected fraud attempts \(mapped to blue based on their account IDs an outlier showing a significantly higher bet amount occurs mapped to red as exceeding a threshold  For an experienced analyst this data point represents a valuable link to related and possibly conspicuous data. One possible interpretation could be that the outlier was placed by a so-called putteron  a fraudster that places bets for people who are prohibited to bet. The suspicion  available in the EventBase and depicted over time in Figure 8e The step chart shows that  inactive with sequences of cash-ins placing a bet winning a bet and cash-outs occurring straightly every few days The example shows that with tailored analysis tools such as the EventAnalyzer the EventBase serves as a valuable and easy to access source for knowledge discovery Thus far the visual analysis of event data was proven useful adoptions of well-known data-mining techniques such as similarity searching are however in current development For more information on the EventAnalyzer the Event Tunnel and its application in fraud detection, readers may refer to Suntinger et al 24  VIII  C ONCLUSION AND O UTLOOK  In this paper we have addressed several open issues regarding event persistence and analysis of existing CEP solutions In particular we have presented a solution for efficient event data repository management as an extension for CEP solutions in order to enable post analysis of events and break up current event processing limitations The presented solution is based on a formal and efficient data schema maintained by a common RDBMS It solves common problems to efficiently persist events their relationships the preservation of calculated event metrics and the representation of non-native database types including de-AT  de-AT   Figure 8  Event Driven Business Intelligence with the EventAnalyzer    


advanced concepts such as event inheritance The implementation and the evaluation have been conducted upon the existing event processing solution SARI The work presented in this paper is part of a larger, longterm research effort aiming at developing a comprehensive set of technologies and tools for event analysis Furthermore we plan to evaluate the integration of the Event Data Warehouse EDWH respectively the linking of EventBase entities to external DWH dimensions to bundle dynamic real-time event information with large historical information repositories for better situation detection and verification of  decision making processes A CKNOWLEDGEMENT  We want to thank the Senactive development team for their valuable discussions and for implementing this research work R EFERENCES  1  Aalst W Weijters A J M M and Maruster L 2004  mining: Discovering process models from event logs. IEEE Transactions on Knowledge and Data Engineering, 16, 9, 1128 1142  2  Abadi, D.J., Ahmad, Y., Balazinska, M., \307etintemel, U., Cherniack, M Hwang, J.H., Lindner, W., Maskey, A.S., Rasin, A., Ryvkina, E., Tatbul N Xing Y and Zdonik S 2005 The Design of the Borealis Stream Processing Engine In Proc of the Conf on Innovative Data Systems Research, Asilomar, CA, USA, 277 289 3  Abadi D J Carney D Cetintemel U Cherniack M Convey C Lee, S., Stonebraker, M., Tatbul, N. and Zdonik, S. 2003. Aurora: A new model and architecture for data stream management. VLDB Journal, 12 120 139 4  Adi A and Etzion O 2004  AMIT  the situation manager The VLDB Journal, 13, 2, 177-203 5  Brobst S and Ballinger C 2000 Active Data Warehousing Whitepaper EB 1327, NCR Corporation 6  Chen, S.-K., Jeng, J.-J. and Chang, H. 2006. Complex Event Processing using Simple Rule-based Event Correlation Engines for Business Performance Management. CEC/EEE 7  Esper, http://esper.codehaus.org/, 2009-0201  8  Hoppe A. and Gryz J. 2007. Stream Processing in a Relational Database a Case Study Database Engineering and Applications Symposium 2007. IDEAS 2007. 11th International Volume, 216 224 9  Inmon B., Imhoff C. and Sousa R. 2001. Corporate Information Factory Wiley, New York 10  Kimball R 1996 The Data Warehouse Toolkit Practical Techniques for Building Dimensional Data Warehouses. John Willey, 1996 11  Luckham, D. 2005. The Power Of Events. Addison Wesley 12  Mannila H and Moen P 1999 Similarity between event types in sequences, Proc. First Intl. Conf. on Data Warehousing and Knowledge Discovery  271 28 13  Moen P 2000 Attribute Event Sequence and Event Type Similarity Notions for Data Mining Ph.D thesis Department of Computer Science, University of Helsinki, Finland 14  Rozsnyai S 2006 Efficient indexing and searching in correlated business events. PhD thesis, Vienna University of Technology 15  Rozsn yai S., Schiefer J and Sch atten A  2007 Concepts and Models for Typing Events for Event Based Systems  International Conference on Distributed Event Based Systems   Toronto  Canada  DEBS  0 7   16  Rozsnyai, S., Vecera, R., Schiefer, J and Schatten, A. 2007 Event cloud  searching fo r correlated business events. In CEC/EEE, IEEE Computer Society  409 420  17  Schiefer, J. and Seufert, A. 2005. Management and controlling of timesensitive business processes with sense & respond. In CIMCA/IAWTIC IEEE Computer Society, 77 82  18  Schiefer J Rozsnyai S Saurer G and Rauscher C 2007 Event Driven Rules for Sensing and Responding to Business Situations International Conference on Distribut ed Event Based Systems, Toronto   19  Schiefer, J. and Seufert, A. 2005. Management and Controlling of TimeSensitive Business Processes with Sense  Respond International Conference on Computational Intelligence for Modelling Control and Automation \(CIMCA\, Vienna 20  Schrefl M and Thalhammer T 2000 On Making Data Warehouses Active In Proc of the 2nd Intl Conf on Data Warehousing and Knowledge Discovery DaWaK Springer LNCS 1874 London UK 34 46 21  Seirio M. and Berndtsson M. 2005 Design and Implementation of an ECA Rule Markup Language. RuleML, Springer Verlag, 98 112 22  Stonebraker M  and 307etintemel U 2005  One Size Fits All An Idea Whose Time Has Come and Gone, ICDE 2005, 2-11 23  Suntinger M Schiefer J Roth H and Obweger H 2008 Data Warehousing versus Event-Driven BI Data Management and Knowledge Discovery in Fraud Analysis  International Conference on Software Knowledge Information Management and Applications  Kathmandu, Nepal  08  24  Suntinger, M., Obweger, H., Schiefer, J and Groeller M. E  2007 The E vent T unnel  Interactive visualization of complex event streams for busin ess process pattern analysis Technical report Institute of Computer Graphics and Algorithms  Vienna University of Technology  25  Vecera R 2007 Efficient indexing Searching and Analysis of Event Streams. PhD thesis, Vienna University of Technology 26  Vecera R Rozsnyai S and Roth H 2007 Indexing and search of correlated business events The Second International Conference on Availability, Reliability and Security, Ares 2007, 1124 1134 27  Widom J  Ceri S and Dayal U 1994 Active Database Systems Triggers and Rules for Advanced Database Processing Morgan Kaufmann Publishers Inc., San Francisco, CA 28  Wu P Bhatnagar R Epshtein L Bhandaru M and Shi Z 1998 Alarm correlation engine \(ACE\, In Proceedings of the IEEE/IFIP 1998 Network Opera tions and Management Symposium NOMS New Orleans  29  Zdonik S Stonebraker M., Cherniack M. Cetintemel U Balazinska M. and Balakrishnan, H. 2003. The Aurora and Medusa Projects. IEEE Data Engineering Bulletin, 26  1  


4 Heart 270 13 2 5 Diabetes 768 8 2 6 Pima 768 8 2  For a classifier, classification accuracy is a basic performance measurement, which is the ratio of the number of cases truly predicted by the classifier over the total number of cases in the whole test dataset, e.g Number of cases truly predictedClassification accuracy= 100 Total number of cases  2 We compared BitTableAC with some associative classification algorithms on accuracy, such as CBA and CMAR. We also include the C4.5 and LIBSVM results on the same datasets as a reference. C4.5 is a well-known traditional classifier based on decision tree induction technique, and LIBSVM is an accurate support vector machine classifier For the fair of the comparison, we do not implement these algorithms, but their classification accuracies are obtained from their research literatures. For BitTableAC, the MinSup MinConf and NFP \(num of fuzzy partitions and 3, respectively The comparison results are shown in Table 4. As shown in this table, BitTableAC has the satisfactory classification accuracy. It achieves the highest accuracy in four of six datasets used in experiments and also outperforms other algorithms on average Table 4 Experiment Results Dataset C4.5 LIBSVM CBA CMAR BitTableAC 1 Glass 68.70 77.57 72.60 70.10 75.23 2 Iris 93.60 94.00 92.90 94.00 96.25 3 Breast 95.70 96.14 95.80 96.40 98.53 4 Heart 82.50 88.45 81.50 82.20 86.67 5 Diabetes 72.10 73.83 75.30 75.80 80.00 6 Pima 75.50 79.69 73.10 75.10 81.82 Average 81.35 84.95 81.87 82.27 86.42  IV. CONCLUSION In this paper, an accurate associative classifier BitTableAC is proposed. It employs BitTable to mine association rules 


efficiently, and fuzzy c-means \(FCM attributes. To evaluate the performance of the proposed algorithm, we compare BitTableAC with other well-known classifiers on accuracy including previous associative classifiers, C4.5 and LIBSVM on 6 test datasets from UCI Machine Learning Repository. The results show that, in terms of accuracy, BitTableAC outperforms others REFERENCES 1] G. Goulbourne, F. Coenen, and P. Leng, "Algorithms for computing association rules using a partial-support tree," Knowledge-Based Systems vol. 13, pp. 141-149, Apr 2000 2] M. J. Zaki, "Scalable algorithms for association mining," Ieee Transactions on Knowledge and Data Engineering, vol. 12, pp. 372-390 May-Jun 2000 3] F. Bonchi, F. Giannotti, A. Mazzanti, and D. Pedreschi, "Efficient breadth-first mining of frequent pattern with monotone constraints Knowledge and Information Systems, vol. 8, pp. 131-153, Aug 2005 4] G. Grahne and J. F. Zhu, "Fast algorithms for frequent itemset mining using FP-trees," Ieee Transactions on Knowledge and Data Engineering, vol 17, pp. 1347-1362, Oct 2005 5] I. I. Artamonova, G. Frishman, and D. Frishman, "Applying negative rule mining to improve genome annotation," Bmc Bioinformatics, vol. 8, pp. -, Jul 21 2007 6] J. W. Han, J. Pei, Y. W. Yin, and R. Y. Mao, "Mining frequent patterns without candidate generation: A frequent-pattern tree approach," Data Mining and Knowledge Discovery, vol. 8, pp. 53-87, Jan 2004 7] Y. C. Hu and G. H. Tzeng, "Elicitation of classification rules by fuzzy data mining," Engineering Applications of Artificial Intelligence, vol. 16, pp 709-716, Oct-Dec 2003 8] J. D. Holt and S. M. Chung, "Mining of association rules in text databases using Inverted Hashing and Pruning," Data Warehousing and Knowledge Discovery, Proceedings, vol. 1874, pp. 290-300, 2000 9] Y. J. Li, P. Ning, X. S. Wang, and S. Jajodia, "Discovering calendar-based temporal association rules," Data & Knowledge Engineering, vol. 44, pp 193-218, Feb 2003 10] Y. J. Tsay and J. Y. Chiang, "CBAR: an efficient method for mining association rules," Knowledge-Based Systems, vol. 18, pp. 99-105, Apr 2005 11] B. Liu, W. Hsu, and Y. Ma, "Integrating Classification and Association Rule Mining," in Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining, KDD'98, AAAI, New York, 1998, pp. 80-86 12] W. Li, J. Han, and J. Pei, "CMAR: accurate and efficient classification 


based on multiple class association rule," in Proceedings of the 2001 IEEE International Conference on Data Mining, ICDM'01, San Jose, CA, 2001 pp. 369-376 13] D. Janssens, G. Wets, T. Brijs, and K. Vanhoof, "Adapting the CBA algorithm by means of intensity of implication," Information Sciences, vol 173, pp. 305-318, Jun 23 2005 14] W. Song, B. R. Yang, and Z. Y. Xu, "Index-BitTableFI: An improved algorithm for mining frequent itemsets," Knowledge-Based Systems, vol. 21 pp. 507-513, Aug 2008 15] J. Dong and M. Han, "BitTableFI: An efficient mining frequent itemsets algorithm," Knowledge-Based Systems, vol. 20, pp. 329-335, May 2007  532 


 Table I.  Number of intervals Stage Interval No. Stage Interval No EP 7 ES 8 ED 10 EB 9 ET 8 EI 11  Table II.  Results using the proposed approach Stage Bias MMRE MdMRE ES -8.5% 27.0% 17.0 ED -33.1% 40.5% 13.7 EB -2.8% 9.3% 7.5 ET -11.6% 16.7% 7.23 EI -20% 91.0% 30.2  Table III.  Results using exponential regression Stage Bias MMRE MdMRE ES -24.3% 81.3% 49.7 ED -72.3% 120.4% 54.224 EB 0.7% 44.35% 37.6 ET -45.4% 81.1% 39.0 EI -179% 184% 104.0  Results shown in Table III revealed that most of predictions are under estimation which supports our approach findings. The best estimation accuracy was obtained in building stage, which also corroborates our findings that best estimation accuracy was in building stage. The negative values in Bias criterion show underestimation. It is acknowledged that MMRE is unbalanced in many validation circumstances and leads to overestimation more than underestimation. In our case, we found that MMRE leads to underestimation in most stages. This is may be related to the absence of systematic scheme between all prior effort records   253   Figure 1. Effort distribution of Planning stage 


Figure 2. Effort distribution of Specification stage Figure 3. Effort distribution of Design effort stage  Figure 4. Effort distribution of Building stage Figure 5. Effort distribution of Testing stage Figure 6. Effort distribution of Imp stage   Table IV. Statistical significance Stage sum rank Z-value p-Value ES 769 -4.31 <0.01 ED 713 -5.03 <0.01 EB 685 -5.4 <0.01 ET 595 -6.54 <0.01 EI 799 -3.93 <0.01  The comparison between our approach and exponential regression technique showed that there are considerable improvements in estimation accuracy on all phases of software development lifecycle. MMREs of our approach have been reduced by at least 35.05 and at most 93%. Biases have been reduced by at least 3.5% and at most 159%.We have to bear in mind that the length of interval plays important role in estimation accuracy, thus, when the universe of discourse is partitioned into several equal intervals, the distribution of data should be taken into account. Moreover, we should remove the extreme values because they affect interval partitioning, thus, estimation accuracy Figures 7 to 11 show comparison between proposed approach and exponential regression in each stage by using Boxplot. The Boxplot [17] offers a way to compare between estimation models based on their absolute residuals. The Boxplot is non-parametric statistics used to show the median as central tendency of distribution, interquartile range and the outliers of individual models [17]. The length of Boxplot from 


lower tail to upper tail shows the spread of the distribution. The length of box represents the interquartile range that contains 50% of observations The position of median inside the box and length of Boxplot indicate the skewness of distribution. A Boxplot with a small box and long tails represents a very peaked distribution while a Boxplot with long box represents a flatter distribution. The prominent and common characteristic among these figures is the spread of absolute residuals for our approach is less than spread of exponential regression which presents more accurate results. The larger interquartile of exponential regression indicates a high dispersion of the absolute residuals. The Boxplot revealed that the box length for our models is smaller than exponential regression which also indicates reduced variability of absolute residuals. The median of our model is smaller than median of exponential regression which revealed that at least half of the predictions of our model are more accurate than exponential regression 254  Figure 7. Boxplot of absolute residuals for the specification stage  Figure 8. Boxplot of absolute residuals for the design stage   Figure 9. Boxplot of absolute residuals for the building stage Figure 10. Boxplot of absolute residuals for the testing stage   The lower tails of our model is much smaller than upper tail which means the absolute residuals are skewed towards the smaller value Figure 11 illustrates the reason of why prediction of implementation stage in our approach produced the worst accuracy. The reason related to the existing of outlier. Although one project is considered as an outlier the MMRE is easily influenced with that project Based on the obtained results, we can observe that 


exponential regression gave bad accuracy. The reason may relate to the structure complexity of prior effort records. There is no correlation between all prior stages and target stage To ensure that the results obtained are not by chance we investigated the statistical significance of the proposed approach using Wilcoxon sum rank test for absolute residuals as shown in Table IV. In this test if the resulting p-value is small \(p<0.05 statistically significant difference can be accepted between the two samples median. The residuals obtained using the proposed approach were significantly different from those obtained by exponential regression Suggesting that, there is difference if the predications generated using the proposed approach or exponential regression and based on the accuracy comparison in Tables II and III we can safely conclude that our proposed method outperformed exponential regression for stage effort estimation Figure 11. Boxplot of absolute residuals for the implementation stage  VIII. CONCLUSIONS Some of software projects are failed due to the absence of re-estimation during software development which results in huge gap between initial plan and final outcome. Even with good estimate at first stage the project manager must keep update with project progress and should be able to re-estimate the project at any particular point of project in order to re-allocate the proper number of resources. The objective of this paper was to check whether the prior effort records can 255 be used to predict stage effort with reasonable accuracy or not. The obtained results revealed that using association rule and Fuzzy set theory lead to significant improvement in stage-effort estimation and give project manager an evolving picture about project progress. Comparing our approach with exponential regression showed that there is a considerable potential in estimation accuracy. As part of future plan, we 


intend to expand this work to involve some interesting features in each stage prediction and evaluate it on many datasets   REFERENCES  1] F. Ricardo, N. Ana, M. Paula, B. Gleidson, R. Fabiano ODE: Ontology-based software Development Environment, Proceedings of the IX Argentine Congress on Computer Science, pp. 1124-1135, 2003 2] E. Mendes, B. A. Kitchenham. Further comparison of cross-company and within-company effort estimation models for Web applications. In: Proc. 10th IEEE International Software Metrics Symposium, Chicago USA, pp.348-357, 2004 3] B. Boehm, R. Valerdi. Achievements and Challenges in Software Resource Estimation, Proceedings of ICSE 06 Shanghai, China, pp. 74-83,  2006 4] K. Molokken, M. Jorgensen. A review of software surveys on software effort estimation, Proceedings of International Symposium on Empirical Software Engineering \(ISESE 2003 5] M. Jorgensen, K. Molokken-Ostvold. How large are software cost overruns? A review of the 1994 CHAOS report, Information and Software Technology, Vol. 48 issue 4. PP. 297-301, 2006 6] X. Huanga, D. Hob, J. Rena, L. F. Capretz. Improving the COCOMO model using a neuro-Fuzzy approach Applied Soft Computing, Vol.7, issue 1, pp. 29-40, 2007 7] L. Briand, T. Langley, I. Wieczorek. A replicated assessment and comparison of common software cost modeling techniques, Proceedings of the 22nd international conference on Software Engineering, 2000 8] S.-J Huang, N. H. Chiu. Optimization of analogy weights by genetic algorithm for software effort estimation Information and Software Technology, Vol. 48, issue 11 pp. 1034-1045, 2006 9] Z. Xu, T. M. Khoshgoftaar. Identification of Fuzzy models of software cost estimation, Fuzzy Sets and Systems, Vol. 145, issue 1, pp. 141-163, 2004 10] R. Pressman. Software Engineering: practitioner 


approaches, McGraw Hill, London, 2004 11] M. Boraso, C. Montangero, H. Sedhi. Software cost estimation: an experimental study of model performance Universita di Pisa, Italy, 1996 12] Y. Wang, Q. Song, J. Shen., 2007. Grey Learning Based Software Stage-Effort Estimation. International Conference on Machine Learning and Cybernetics, pp 1470-1475, 2007 13] S. G. MacDonell, M. J. Shepperd. Using prior-phase effort records for re-estimation during software projects Ninth International, Software Metrics Symposium, pp 73- 86, 2003 14] M .C Ohlsson, C. Wohlin. An Empirical Study of Effort Estimation during Project Execution, Sixth International Software Metrics Symposium \(METRICS'99 1999 15] N. H. Chiu,,S. J. Huang.  The adjusted analogy-based software effort estimation based on similarity distances Journal of Systems and Software, Vol. 80, issue 4, pp 628-640, 2007 16] P. Sentas, L. Angelis, I. Stamelos, G.  Bleris. Software productivity and effort prediction with ordinal regression Information and Software Technology, Vol. 47, issue 1 pp. 17-29, 2005 17] E. Mendes, N. Mosley. Comparing effort prediction models for Web design and authoring using boxplots Australian Computer Science Communications,  Vol. 23 Issue 1, pp. 125-133, 2001 18] E. Mendes, N. Mosley, I. Watson. A comparison of casebased reasoning approaches, Proceedings of the 11th international conference on World Wide Web, pp. 272280, 2002 19] Q. Zhao, S. S. Bhowmick. Association Rule Mining: A Survey  http://citeseer.ist.psu.edu/734613.html, 2003 20] S. Morisak, A. Monden, H. Tamada. An Extension of association rule mining for software engineering data repositories, Information Science Technical Report NAIST, 2006 21] Q. Song, M. Shepperd.  M. Cartwright, C. Mair. Software defect association mining and defect correction effort prediction, IEEE transaction on software engineering Vol. 32, No.2, pp. 69-82, 2006 


22] R. Agrawal, T. Amielinski, A. Swami. Mining association rule between sets of items in large databases Proceedings of the ACM SIGMOD International Conference on Management of Data, pp. 207-216, 1993 23] M-J. Huang, Y-L. Tsou, S-C. Lee.  Integrating Fuzzy data mining and Fuzzy artificial neural networks for discovering implicit knowledge, J. Knowledge-Based Systems, Vol.19 \(6 24] ISBSG International Software Benchmarking standards Group, Data repository release 10, Site http://www.isbsg.org, 2007 25] L. Zadeh. Toward a theory of Fuzzy information granulation and its centrality in human reasoning and Fuzzy logic. J. Fuzzy sets and Systems 90, pp. 111-127 1997 26] I. H. Witten, E. Frank. Data Mining: Practical machine learning tools and techniques, 2nd Edition, Morgan Kaufmann, San Francisco, 2005   256 


encountering a related term, i.e. IC\(c e intuition behind the use of the negative likelihood is that the more probable a term to appear, the less information it conveys. All these features show that Jiangs measure tends to be more general and more appropriate for evaluating nontaxonomically related terms. Indeed, a high score of the relatedness measures suggests a strong relationship between terms Nevertheless, all relatedness measures have limitations because they assume that all the semantic content of a particular term is modeled by semantic links in WordNet Consequently, in many situations, truly related terms obtain a low scores even though their belongings to a certain category of tags, e.g., jargon tags Additionally, when measuring the quality of an automatically knowledge acquisition results, the typical measures used in Information Retrieval are Recall, Precision and F-Measure. However, computing Recall and F-Measure requires the availability of a Gold Standard. Hence, we will only compute the Precision which speci?es to which extent the non-taxonomic relationships is extracted correctly. In this case, the ratio between the correctly extracted relations i.e., their relatedness measures is greater than or equal to a minimum threshold, and the whole number of extracted ones is computed. Thus, we have Precision Total correctly selected entities Total selected entities 12 http://search.cpan.org/dist/WordNet-Similarity 13 A term refers to a tag subject or a tag object C. Evaluation of non-taxonomic relationships Only a percentage of the full set of non-taxonomic relationships \(89 is caused by the presence of non standard terms which are not contained in WordNet and, in consequence, cannot be evaluated using WordNet-based relatedness measures. Fig. 5 depicts the evaluation results of the extracted non-taxonomic relationships against their relatedness measures High relatedness score \(88 17% of the extracted relationships, as most of terms are strongly related with respect WordNet Null Scores were obtained for 5% of the extracted 


relationships. Analyzing this case in more detail, we have observed that the poor score is caused in many situations by the way in which Jiangs distance metric works. This latter completely depends on the distance between two terms based on the number of edges found on the path between them in WordNet. In consequence this measure returns a value that does not fully represent reality. For example, on the one hand, Jiangs distance metric returns a null value for the relationship between insurance and car, even though the ?rst is a commonly related to the second, i.e., car involved insurance Finally with a minimal Jiangs distance metric threshold, set to 46%, the computed precision of correctly extracted relationships candidates is equal to 68.8 An example of extracted non-taxonomic relationships is depicted in Table V where each relation describes the subject tag, e.g., tool, the predicate, e.g is being developed within, and the object tag, e.g mesh. Fig. 4 represent a fragment output of the extracted ontological structure where each concept de?nes a set of similar and synonym tags and labels, i.e., mentions has been, revealed, caused and is created with describe the predicates of the non-taxonomic relationships between terms Due to the limitations observed by the automatic evaluation procedure and the lack of gold standards containing non-taxonomic relationships, we have examined the extracted non-taxonomic relationships from a linguistic point 377 Top space      distance     quad great     groovy nifty caused address      addresses extension      quotation   reference  references extensions        referenz     source      refrence sources    rfrences    quotations research    search     searching searchs open-source     open_source 


opensource linux aim     design     designer      designers patern    project     patterns     projekte projects web+design    web_design webdesign internet       internetbs net          web network      networking networks      web discussion     news       password word      words community      communities is_created_with mentions revealed has_been Figure 4. A fragment output of the extracted ontological structure of view. This qualitative evaluation can bring some interesting insights about the kind of results one can expect Invalid relations are extracted: Even though a relation such as music cities skill is considered as correct one since tag subject, tag object and predicate are correctly extracted. From a semantic point of view, this relation has no meaning. Hence, a higher precision is expected Figure 5. Summary of non-taxonomic evaluation measure Table V EXAMPLES OF EXTRACTED NON TAXONOMIC RELATIONSHIPS Subject Predicate Object search has been reference reference mentions search tool is being developed within mesh security added encoding search revealed reference java provides library by performing the sense analysis on complete relations An ambiguity in the extracted predicates between terms is observed: Hence, same relations are redundant since they use a synonym predicates between terms, e.g java provides library and java yields library. Thus we expect that the redundancy removal within extracted relations will be of bene?t for the improvement of the 


obtained results VI. CONCLUSION AND FUTURE WORK The extraction of non-taxonomic relationships from folksonomies is to the best of our knowledge is the least tackled task within ontology building from folksonomy. This is why there is a need of novel and general purpose approaches covering the full process of learning relationships. In this paper, we introduced a new approach called NONTAXFOLKS that starts by pre-processing tags aiming at getting a set of frequent tagsets corresponding to an agreed representation Then, they are used to retrieve related tags using external resources such as WordNet. Thanks to the particular structure of triadic concepts, it allows grouping semantically related tags by considering the semantic relatedness embodied in the different frequencies of co-occurences among users, resources and tags in the folksonomy. Thereafter we introduced an algorithm called NTREXTRACTION for extracting non-taxonomic relationships between pair of tags picked from the triadic concepts. In summary, our approach uses several well known techniques \(such as formal concept analysis or association rule discovering the social bookmaring environnement in order to propose a new way of extracting labeled non-taxonomic relationships between tags. Currently, we are investigating the following topic concerning the discovered predicates between two terms. Indeed, in order to avoid relationships redundancy and thus a redundancy in the builded ontology. One can try to classify them into prede?ned semantic classes, detect synonyms, inverses, etc. A standard classi?cation of verbs could be used for this purpose, adding additional information about the semantic content, e.g., senses, verb types, thematic roles, etc., of predicates relationships 378 REFERENCES 1] J. Pan, S. Taylor, and E. Thomas, Reducing ambiguity in tagging systems with folksonomy search expansion, in Proceedings of the 6th Annual European Semantic Web Conference \(ESWC2009 2] V. S. M. Kavalec, A. Maedche, Discovery of lexical entries for non-taxonomic relations in ontology learning, in Proceedings of the SOFSEM 2004, LNCS, vol. 2932, 2004, pp 249256 


3] L. Specia and E. Motta, Integrating folksonomies with the semantic web, in Proceedings of the 4th European Semantic Web Conference \(ESWC 2007 Innsbruck, Austria, vol. 4519, June 2007, pp. 624639 4] P. Mika, Ontologies are us: A uni?ed model of social networks and semantics, in Proceedings of the 4th International Semantic Web Conference \(ISWC2005 3729, Galway, Ireland, June 2005, pp. 522536 5] P. Schmitz, Inducing ontology from ?ickr tags, in Proceedings of the Workshop on Collaborative Tagging \(WWW 2006 Edinburgh, Scotland, May 2006 6] M. Zhou, S. Bao, X. Wu, and Y. Yu, An unsupervised model for exploring hierarchical semantics from social annotations, in Proceedings of the 6th International Semantic Web Conference and 2nd Asian Semantic Web Conference ISWC/ASWC2007 Korea, vol. 4825, November 2006, pp. 673686 7] C. Schmitz, A. Hotho, R. Jaschke, and G. Stumme, Mining association rules in folksonomies, in Proceedings of the 10th IFCS Conference \(IFCS 2006 2006, pp. 261270 8] A. Hotho, A. Maedche, S. Staab, and V. Zacharias, On knowledgeable unsupervised text mining, in Proceedings of Text Mining Workshop, Physica-Verlag, 2003, pp. 131152 9] A. Hotho, R. Jaschke, C. Schmitz, and G. Stumme, Information retrieval in folksonomies: Search and ranking, in The Semantic Web: Research and Applications, vol. 4011 Springer, 2006, pp. 411426 10] F. Lehmann and R. Wille, A triadic approach to formal concept analysis, in Proceedings of the 3rd International Conference on Conceptual Structures: Applications, Implementation and Theory. Springer-Verlag, 1995, pp. 3243 11] R. Jaschke, A. Hotho, C. Schmitz, B. Ganter, and G.Stumme Discovering shared conceptualizations in folksonomies Web Semantics: Science, Services and Agents on the World Wide Web, vol. 6, pp. 3853, 2008 12] A. Mathes, Folksonomies - cooperative classi?cation and communication through shared metadata, Graduate School of Library and Information Science, University of Illinois Urbana-Champaign, Tech. Rep. LIS590CMC, December 2004 13] H. Lin, J. Davis, and Y. Zhou, An integrated approach 


to extracting ontological structures from folksonomies, in Proceedings of the 6th European Semantic Web Conference ESWC 2009 vol. 5554, 2009, pp. 654668 14] M. Szomszor, H. Alani, K. OHara, and N. Shadbolt, Semantic modelling of user interests based on cross-folksonomy, in Proceedings of the 7th International Semantic Web Conference \(ISWC 2008 15] G.Begelman, P. Keller, and F.Smadja, Automated tag clustering: Improving search and exploration in the tag space, in Proceedings of the the Collaborative Web Tagging Workshop WWW 2006 16] R. Jaschke, A. Hotho, C. Schmitz, B. Ganter, and G. Stumme TRIAS - an algorithm for mining iceberg tri-lattices, in Procedings of the 6th IEEE International Conference on Data Mining, \(ICDM 2006 2006, pp. 907911 17] C. Borgelt, Ef?cient implementation of APRIORI and ECLAT, in FIMI, COEUR Workshop Proceedings, COEURWS.org, vol. 126, 2003 18] J. Tang, H. Leung, Q. Luo, D. Chen, and J. Gong, Towards ontology learning from folksonomies, in Proceedings of the 21st international jont conference on Arti?cal intelligence IJCAI 2009 20892094 19] L. Ding, T. Finin, A. Joshi, R. Pan, R. Cost, Y. Peng P. Reddivari, V. Doshi, and J. Sachs, Swoogle: A search and metadata engine for the semantic web, in Proceedings of the 13th ACM Conference on Information and Knowledge Management, ACM Press, 2004, pp. 652659 20] A. Hliaoutakis, G. Varelas, E. Voutsakis, E. Petrakis, and E. E Milios, Information retrieval by semantic similarity, International Journal on Semantic Web and Information Systems IJSWIS 21] G. Pirro, M. Ruffolo, and D. Talia, Secco: On building semantic links in peer to peer networks, Journal on Data Semantics XII, LNCS 5480, pp. 136, 2009 22] C. Meilicke, H. Stuckenschmidt, and A. Tamilin, Repairing ontology mappings, in Proceedings of the International Conference AAAI 2007, Vancouver, British Columbia, Canada 2007, pp. 14081413 23] S. Ravi and M. Rada, Unsupervised graph-based word sense 


disambiguation using measures of word semantic similarity in Proceedings of the International Conference ICSC 2007 Irvine, California, USA, 2007 24] H. G. A. Budanitsky, Semantic distance in wordnet: an experimental application oriented evaluation of ?ve measures in Proceedings of the International Conference NACCL 2001 Pittsburgh, Pennsylvania, USA, 2007, pp. 2934 25] J. Jiang and D. Conrath, Semantic similarity based on corpus statistics and lexical taxonomy, in Proceedings of the International Conference ROCLING X, 1997 379 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


