An innovative algorithm for clustering retail items Pradip Kumar Bala Dept. of Operations Management & Decision Science Xavier Institute of Management, Bhubaneswar \(XIMB Bhubaneswar, India e-mail: p_k_bala@rediffmail.com Abstract  Dendrogram for clustering retail items is a tool used in customer relationship management \(CRM\ for implementing homogeneous schemes for all the items in one cluster. In agglomerative hierarchical clustering, dendrograms are developed based on the concept of èdistanceê between the entities or, groups of entiti es. These entities may be the customers, retail items, business units etc. as per the business problem. The present paper focuses on agglomerative hierarchical clustering of retail items based on the concept of similarityê of the items which is regarded as the inverse of distanceê \(or èdissimilarityê\imilarity is measured based on the strength of association rule/s containing the candidate items to be merged. An algorithm has been proposed for developing dendrogram and it has been explained with an example in retail sale. The algorithm overcomes the negative impact of negative association between the items on the quality of clustering Keywords association rule; clustering; CRM; dendrogram data mining I I NTRODUCTION In the recent years, the primary task in business analytics of customer relationship management \(CRM\s been to integrate èrelationship technology \(i.e. data consolidating and data mining\ê with èloyalty schemesê. CRM is expected to enhance value to customers through raising satisfaction levels on transactions. If customers appreciate the value provided by a scheme in CRM, they are expected to continuously enhance the relationship with the firm involved through loyalty to the products/brands, purchasing more advocating the firm to others, etc. With the advent of data mining technology, cluster analysis of items is frequently done in supermarkets and in other large-scale retail sectors Clustering of items has been a popular tool for identification of different groups of items where appropriate programs in CRM are designed for each group separately with maximum effectiveness and return. For example, items frequently purchased together are placed in one place in the shelf of a retail store. A discount-oriented promotional offer is designed for a group of items with incidence of high association in purchase. Such clustering of items is useful in operations other than CRM also. One such illustration is seen in clustering of retail items for the purpose of inventory control or, placing joint replenishment order Cluster analysis or clustering assigns a set of entities or observations or objects into subsets so that observations in the same cluster are similar in some sense. These subsets are called clusters. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis used in many fields, including machine learning data mining, pattern recognition, image analysis and bioinformatics. There are various algorithms used for clustering. Hierarchical algorithms find successive clusters using previously established clusters. These algorithms usually are either agglomerative \("bottom-up"\ or divisive top-down"\. Agglomerative algorithms begin with each element as a separate cluster and merge them into successively larger clusters. Agglomerative hierarchical clustering creates a hierarchy of clusters which may be represented in a tree structure called a dendrogram. The root of the tree consists of a single cluster containing all observations, and the leaves correspond to individual observations. Any valid metric may be used as a measure of similarity between pairs of observations. The choice of which clusters to merge or split is determined by a linkage criterion, which is a function of the pair-wise distances between observations. Different clusters are obtained at different levels of the tree diagram of dendrogram. This gives an opportunity to compare the performance of various clustering in different levels with respect to a selected performance criterion. In CRM, the performance of clustering may be evaluated based on total profit, customer retention, total sale etc. Inventory managers may choose total relevant inventory cost as a parameter for analyzing the performance of clustering at various levels An important step in any clustering is to select a distance measure, which will determine how the similarity of two elements is calculated. This will influence the shape of the clusters, as some elements may be close to one another according to one distance metric and farther away according to another distance metric. For example, in a 2-dimensional space, the distance between the point x 1 y 0\and the origin x 0 y 0\ always 1 according to the usual norms, but the distance between the point x 1 y 1\ and the origin can be 2 2 or 1 if you take respectively the 1norm \(Manhattan distance\norm \(Euclidean distance infinity-norm \(Maximum norm\tance. The Mahalanobis distance corrects data for different scales and correlations in the variables. Another approach is based on inner product or scalar product where the angle between two vectors can be used as a distance measure when clustering high 208 978-1-4244-7896-5/10/$26.00 c  2010 IEEE 


dimensional data. The Hamming distance measures the minimum number of substitutions required to change one member into another. Another important distinction is whether the clustering uses symmetric or asymmetric distances. Many of the distance functions discussed here have the property that distances are symmetric \(the distance from object A to B is the same as the distance from B to A  In other applications, e.g., sequence-alignment methods in 1 this is not the case. A matrix populated with both upper and lower triangles gives complete measures of distance The concept of èdistanceê for the purpose of clustering has been extended to various other aspects for measuring similarity or dissimilarity between the items. The work in the current research paper gives a novel algorithm for agglomerative hierarchical clustering based on the strength of association rule within the items II A GGLOMERATIVE HIERARCHICAL CLUSTERING AND DEDNDROGRAM Agglomerative hierarchical clustering method builds the hierarchy from the individual elements by progressively merging clusters that operates in a bottom-up fashion the example given in fig. 1, we have six elements {a}, {b c}, {d}, {e} and {f} in the Euclidean field. Clustering is to be done on the basis of Euclidean distance of similarity. The first step is to determine which elements to merge in a cluster. Usually, we want to take the two closest elements according to the chosen distance metric. Based on Euclidean distance metric, the dendrogram is given in fig. 2 which has been developed in four steps. In step-1, b and c form one cluster and similarly, d and e form one cluster and hence there are four clusters obtained, viz. {a} {b, c} {d, e} and f}. Similarly in the subsequent steps, merger happens till all fall in one cluster Fig.1: Objects for cluster ing in Euclidean Field Fig.2: Hierarchical clustering dendrogram III S TRENGTH OF ASSOCIATION RULE AS A BASIS OF CLUSTERING Association rule is a type of data mining rule or pattern that correlates one set of items or events with another set of items or events. An association rule must satisfy the minimum values of support, confidence and lift which are measures of strength of a rule. Use of data mining algorithm in [3 he l p s i n  fi nd i n g fr e q ue nt i t e mse t s e f fi c i e n t l y a n d  hence in mining association rules efficiently from sale transaction data or, order list data. There are other data mining algorithms also to find frequent itemsets for mining association rules [4  W i t h gi ve n mi ni mu m t h r e sho l d  va l u e s  of support, confidence and lift, all possible association rules can be mined. An association rule is represented in the form X => Y \(read as èX associates Yê\where X and Y are two mutually exclusive sets of items or events. X is known as antecedent and Y is known as consequent.  èSupportê of an itemset in a transaction database is defined as the percetntage of occurrence of the itemset, out of all the transactions.  X=>Y holds with support s%, if s% of the transactions in the database contain èXê and èYê both Confidenceê of an association rule X=>Y is defined as the percentage of transactions containing X and Y both, out of all the transactions containing X. X=>Y holds with confidence c%,   if c% of the transaction in the database that supports èXê also supports èYê. èLiftê of an association rule X=>Y is computed as the confidence of the rule divided by the confidence assuming independence of consequent from antecedent, i.e., lift = \(percentage confidence of the percentage support of Y\ lift ratio greater than 1.0 2010 International Conference of Soft Computing and Pattern Recognition 209 


suggests that the rule is positive and hence, X gives lift to Y Lift value less than 1.0 implies a negative association rule and hence, implication is that  purchase of X reduces purchase of Y. For most of the uses, association rules can not be directly applied in business decision-making The wo i v e s a n association clus teri n g  m e t h od f o r grouping of inventory items for designing an optimum canorder inventory policy. The method can be used to develop dendrogram for agglomerative hierarchical clustering with supportê of an itemset as the measure of èsimilarity \(seen as inverse of èdistanceê\ê between or amongst the items Support of an itemset, as defined earlier, is the percetntage of occurrence of the itemset, out of all the transactions of order list or in sale. The paper in [5 n s id ers a list o f ite m s  from order list for clustering and does not consider confidence and lift in the similarity measure. Lift gives a meaningful interpretation in terms of positive and negative association. If we do not address the issue of lift, there may come many items in one cluster with negative association effect which is not desirable for the clusters. Moreover confidence is an important measure of strength of association between two itemsets. Hence, it is expected that confidence as a measure of strength of association must find place while clustering the items IV A SSOCIATION RULE BASED CLUSTERING As we understand, strength of association is described with three relevant measures, viz., rule support, confidence and lift, due regard must be given to all three while clustering of items based on their association. Moreover negative association rule suggests that item associated are not purchased together. In view of this, items correlated by negative association rule are not to be clustered in one group if we are clustering items based on the coincidence of purchase in the same market basket. In the following subsection, implication of negative association rule has been explained A Implication of Negative Association Rule Positive association rules are generally mined with the threshold values of support and confidence . However, this may be misleading sometimes which is explained below with the example given in table 1. Table 1 shows the number of transactions with respect to presence and absence of two items, X and Y in a transaction.  For the association rule, èX Yê, where, X is called antecedent and Y is called consequent, support and confidence are computed as given below Support of the rule = \(8,000/20,000\100 % = 40 Confidence of the rule = \(8,000/12,000\100 66.67 Support value of 40% and confidence value of 66.67% are reasonably good for the rule, èX = Yê, to qualify for being called a valid association rule. An association rule of the form, èX => Yê, implies that purchase of X causes purchase of Y in many transactions and it happens in certain percentage of transactions which is given by the confidence value of the rule Table 1 Illustration of Negative Association Rule X is present X is absent row  Y is present 8,000 7,000 15,000 Y is absent 4,000 1,000 5,000 column 12,000 8,000 20,000 However, if we analyze further, it is observed that X in fact inhibits purchase of Y. The reality is that item, Y, is purchased on its own in most of the transactions with a very high support of 75% \(in 15,000 transactions out of total of 20,000 transactions\wever, when X is purchased, Y is purchased only in 66.67% of those transactions. As 66.67 is less than 75%, it is interpreted that X inhibits the purchase of Y. This fact is captured by èliftê. Lift of the association rule, èX => Yê is given by the expression, ç\(Confidence of Support of the consequent\or the example under discussion, lift is 0.89, i.e., \(66.67%\\(75%\ is evident that if the confidence of the rule is less than the support of Y, the lift value is less thanê1ê and it implies that X inhibits purchase of Y and it is a negative association rule Positive association rules are with lift value of more than 1ê and a lift value of less than è1ê implies a negative association rule [4 cco rd in g to 5  w h en  lift is less th a n  1, negating the rule produces a better rule. If the rule, çIF B an C THEN Aé, has a lift value less than è1ê, then the rule IF B an C THEN not Aé is a better rule B Details of the proposed Clustering Tehniques The present work in this paper complements the existing literature on the technique of association clustering. The proposed technique in this work considers only the positive associations for clustering. There are two steps in this technique. First step is mining association rules with threshold value of è1.00ê for lift. Suitable threshold values for support and confidence are also chosen. In the second step, these association rules along with the support confidence and lift are taken as input for clustering. Hence this step gives an algorithm for developing the dendrogram As the proposed algorithm for clustering makes use of èall strength measures of association rule/sê in stead of only 210 2010 International Conference of Soft Computing and Pattern Recognition 


support of itemsetê, the proposed algorithm has been named as èassociation rule based clustering As discussed, there are two phases involved in the proposed clustering technique as mentioned below Phase-I Mine association rules from the transaction data with some threshold values of rule support and confidence with lift more than 1.00. The threshold values of rule support and confidence are chosen with low values so that all the items under consideration find place at least in one rule. Amongst these rules, we will select only the positive rules, i.e., the rules with lift value of greater than 1.00 The input and output in this step are as given below Input I = {I 1 I 2 I m set o items D = {t 1 t 2 t n  a transaction database with t i as one transaction Threshold rule support = s Threshold rule confidence = c Threshold lift = 1.00 Output A = { {r 1 s 1 c 1 l 1 r 2 s 2 c 2 l 2 r n s n c n l n   set of positive association rules r i  with rule support \(s i onfidence \(c i d lift \(l i  Phase-II Make use of the output in phase-1 for developing the dendrogram. The algorithm is given in fig.3 Key steps involved in this phase are described below Step 1 Input to this phase is the output of phase-I, i.e., the set of association rules with the values of rule support confidence and lift, given by set A Step 2 Obtain the set of individual items present in at least one of the association rules in A by I = {I 1 I 2 I m  Step 3 Start with tree level s initiated as 1, the itemset similarity is defined as very high value \(tending to infinity and number of clusters \(itemsets\m \(the total number of itemsin I\ce, the set of clusters at level 1, L 1 contains all 1-item clusters in I. That is, L 1 I 1  I 2 I m  Step 4 To generate a set of candidate itemsets for next level C s+1 pair of itemsets in the previous level are joined Step 5 To evaluate itemset similarity, i.e., similarity amongst the items in a cluster, each of the association rules is checked if all the items in the candidate set exist in the rule \(either in the antecedent or in the consequent\ all the items exist in a rule and no other item is present in the rule then sum up rule support, confidence and lift for the rule Similarly, sums are obtained for all other rules where all the items are present in a rule. Sum of all such sums is taken as the measure of similarity Step 6 To generate L s+1 i.e.,  the set of itemsets in level s+1\e two itemsets are merged if their similarity is the highest value among all itemsets in C s+1 Hence, L s+1  L s L a s L b s U{L a s U L b s  Input A = { {r 1 s 1 c 1 l 1 r 2 s 2 c 2 l 2 r n s n  c n l n set of  positive association rules \(r i  with rule support \(s i confidence \(c i and lift \(l i  Output DE  / Dendrogram: a set of ordered tuples Association rule based clustering algorithm I = {I 1 I 2 I m set of individual items present in at least one of the association rules in A s =1;              // the tree level sim = Infinity;            // the itemset similarity \(inversely proportional to distance\ is very high k = m;         // the total number of itemsets \(clusters\ at tree level L L s I 1 I 2 I m the set of itemsets clusters\ at level s DE =  <s, sim, k, L s  for \(s=1; k=1; s++\/generate all candidate itemsets join step for each itemset L i s L s is for èbelongs to for each itemset L j s L s L i s  for each association rule r i where all items in L i s and L j s exist either in antecedent or in consequent of a rule and no other item is present in the rule  sim i rule support i confidence i lift i the itemset similarity equals the sum of rule support, confidence and lift sim i  sim{L i s L j s sim i the similarity for L i s and L j s L a s L b s arg max sim{L i s L j s  L i s L s L j s L s  the two clusters with the highest similarity L s+1 L s L a s L b s U{L a s U L b s the set of itemsets \(clusters\ in the next level sim = sim{L a s L b s k = k-1; L s L s+1 update the parameters DE = DE U <s, sim, k, L s   return DE  Fig.3  The pseudo-code of the association rule based clustering algorithm Step 7 The steps 4-6 are iterated with updating the dendrogram \(DE\ by adding the tuple <s, sim, k, L s into DE where s =s+1, sim = sim{L a s L b s k = k-1,  L s L  s+1 Itereation stops when there is no association rule with all items of any pair of combined clusters/itemsets in a level and this level is the last level of clustering. Hence, all items may not be merged in one cluster as per the proposed algotithm in most of the cases 2010 International Conference of Soft Computing and Pattern Recognition 211 


V A PPLYING THE PROPOSED CLUSTERING TECHNIQUE The proposed technique of clustering was applied on the sale transaction data of nine grocery items with 10,000 sales transactions \(at least one of the selected nine items is present in each transaction\. Moreover, the customers have been chosen who are loyal to a store and hence, not moving from one store to another. This is guaranteed to a great extent by the location of the stores chosen where no other store is located nearby. Association rules were mined from the transaction data with different threshold values of rule support and confidence. It was found that for maximum threshold values of 0.20 for rule support and 0.65 for confidence, all nine items found place. With threshold values above this, it was found that at least one item was missing For the chosen threshold values of rule support and confidence, only 15 association rules were found to qualify as given in table 2 Table 2 Association rules mined covering all the items Rule  Antecedent Consequent Rule Support Confidence Lift Sum  1 Noodles Ö Maggi 200 g\. and Tomato Sauce \(Maggi\ \(200 g MDH Chicken Masala 0.23 0.93 2.76 3.92 2 MDH Chicken Masala Noodles Ö Maggi 200 g\. and Tomato Sau 0.23 0.68 2.76 3.67 3 MDH Chicken Masala. and Noodles Ö Maggi 200 g Tomato Sauce Maggi\ \(200 g 0.23 0.87 2.57 3.67 4 Tomato Sauce Maggi\ \(200 g MDH Chicken Masala. and Noodles Ö Maggi 0.23 0.68 2.57 3.48 5 Kismis \(100 g Basmati \(1 kg 0.21 0.71 1.25 2.17 6 Tomato Sauce Maggi\ \(200 g MDH Chicken Masala 0.25 0.74 2.19 3.18 7 MDH Chicken Masala Tomato Sauce Maggi\ \(200 g 0.25 0.75 2.19 3.19 8 Coffee Ö Nescafe 50 g Bournvita Cadbury \(500 g 0.23 0.67 1.32 2.22 9 MDH Chicken Masala. and Tomato Sauce \(Maggi\ \(200 g Noodles Ö Maggi 200 g 0.23 0.92 1.57 2.72 10 1 kg\. and Sooji \(1 kg Noodles Ö Maggi 200 g 0.20 0.89 1.52 2.61 11 1 kg Noodles Ö Maggi 200 g 0.34 0.88 1.5 2.72 12 Sooji \(1 kg Noodles Ö Maggi 200 g 0.35 0.85 1.46 2.66 13 MDH Chicken Masala Noodles Ö Maggi 200 g 0.26 0.78 1.35 2.40 14 Tomato Sauce Maggi\ \(200 g Noodles Ö Maggi 200 g 0.25 0.73 1.25 2.23 15 Sooji \(1 kg Basmati \(1 kg 0.37 0.72 0.85 1.94 All nine items find place in one or more rules, either in the antecedent or in the consequent. The rules are given with the value of rule support, confidence and lift whereas the last column is the sum of these three measures of strength of association. Amongst these 15 rules, 14 rules were positive i.e., with lift of more than 1.00 and only one rule was negative, i.e., with lift value less than 1.00 For applying the proposed technique for clustering of the items, we consider only the positive association rules \(i.e with lift > 1.00\In 14 positive association rules, nine items as selected for the purpose of clustering\ are as given below Basmati \(1 kg\, Bournvita - Cadbury \(500 g\, Coffee Nescafe \(50 g\,  Kismis \(100 g\,  MDH Chicken Masala 1 kg\,  Sooji \(1 200 g As per the proposed technique, in level 1 of the dendrogram, all individual items will be differen clusters as given below Level 1 \(9 itemsets/clusters\asmati \(1 kg Bournvita - Cadbury \(500 g\}, {Coffee Ö Nescafe \(50 g Kismis \(100 g\}, {MDH Chicken Masala}, {Noodles 1 kg\}, {Sooji \(1 kg 200 g MDH Chicken Masala} and {Tomato Sauce \(Maggi 200 g\}are merged in level 2 as the similarity is highest for these two itemsets which is obtained from rules # 6 and #7 Each of these two rules contains only the two items, i.e 200 g\}. For rule#6, sum is 3.18 and for rule#7, the sum is 3.19 and total is \(3.18 +3.19\.e., 6.37. This similarity of 6.37 as given by the sum of sums is higher than the similarity between any two itemsets. Hence, these two items are merged in level 2 as given below Level 2 \(8 itemsets/clusters\ati \(1 kg Bournvita - Cadbury \(500 g\}, {Coffee Ö Nescafe \(50 g Kismis \(100 g MDH Chicken Masala, Tomato Sauce 200 g Noodles Ö Maggi \(200 g\}, {Poha 1 kg\}, {Sooji \(1 kg Adopting the similar logic, we obtain the clusters in different levels as given below Level 3 \(7 itemsets/clusters\asmati \(1 kg Bournvita - Cadbury \(500 g\}, {Coffee Ö Nescafe \(50 g Kismis \(100 g MDH Chicken Masala, Tomato Sauce 200 g\, Noodles Ö Maggi \(200 g Poha \(Chura 1 kg\}, {Sooji \(1 kg Level 4 \(6 itemsets/clusters\asmati \(1 kg Bournvita - Cadbury \(500 g\, Coffee Ö Nescafe \(50 g Kismis \(100 g MDH Chicken Masala, Tomato Sauce 200 g\, Noodles Ö Maggi \(200 g Poha \(Chura 1 kg\}, {Sooji \(1 kg Level 5 \(5 itemsets/clusters Basmati \(1 kg\is 100 g  Bournvita - Cadbury \(500 g\ffee Ö Nescafe 50 g MDH Chicken Masala, Tomato Sauce \(Maggi 200 g\ Noodles Ö Maggi \(200 g Poha \(Chura\ \(1 kg Sooji \(1 kg 212 2010 International Conference of Soft Computing and Pattern Recognition 


Level 5 of the dendrogram with 5 clusters is the last level as there is not possibility of further clustering as no association rule exists in the list for further clustering VI C ONCLUSION If we apply the algorithm of f o r clu s t ering  w h ich i s  based on only the strength of the rule support, in level 2 Basmati \(1 kg\ and Sooji \(1 kg\ are merged in one cluster as the distance \(= 1- rule support\een them is minimum if we compare it amongst the distances between all the pairs of items. However, these two items are correlated by a negative association rule with lift value of 0.85 \(which is less than 1.00\. As per the interpretation of negative association rule, the antecedent item inhibits the purchase of the consequent item. With the objective of clustering items which are purchased together, this pair should not find place in any level of clustering in the dendrogram. Moreover, if Basmati \(1 kg\ and Sooji \(1 kg\ are put in one cluster, the former item \(i.e., Basmati \(1 kg\no more gets an opportunity to be in one cluster with Kismis \(100 g whereas Kismis \(100 g\ and Basmati \(1 kg\ are positively associated as given in rule#5. The proposed algorithm in this paper is designed to avoi d such undesirable clustering in the dendrogram. As CRM makes an attempt to design homogeneous strategy for all the items in a cluster, items associated by negative rule in the same cluster will have a negative effect on the objectives of a program Further work on devising more appropriate utility based clustering may be done as an extension of this work. The dendrogram can be used in designing promotional offer of itemsets/clusters obtained in various levels where based on a performance indicator, the best level of clustering can be identified for implementation. The same can be used for designing joint inventory replenishment policies with minimum total relevant cost R EFERENCES 1 P r inz ie A P o e l D  V a n de n I nco r p o r ati n g  s e que n t i a l i n f o r m at io n  into traditional classification models by using an element/positionsensitive SAM Decision Support Systems 42 \(2\08-526, 2006 2 D un ham  M H D at aM in i n g  I ntr o ducto r y andA dv ance dT o p i cs    Pearson Education Inc.,NewJersey,USA, 2003 3 S ri k a n t  R  Agra w a l R   M in in g Gen era l i z ed A s s o c i a t i o n  R u les    Proceedings of the 21 st International Conference on Very Large Databases, Zurich, Switzerland, 407-419, 1995 4 H a n  J  K a m b er  M  Da t a Mi ning: C o n c ep t s and Tec h n i q u e s   Morgan Kaufmann Publishers, 2006 5 T s ai, C h ie hY u a n T s ai C h iY a ng H u a n g  P o W e n A n asso cia tio n  clustering algorithm for can-order policies in the joint replenishment problemé, International Journal of Production Economics, 117 \(1 pp. 30Ö41, 2009 6 B e r r y M.J.A an d L i no f f   G  S  D ata Mi n i ng T e chn iq ue s  pp 3 1 1   Wiley India, 2008 2010 International Conference of Soft Computing and Pattern Recognition 213 


34 P  D a y a n a n d T  S e j n o w s k i  223 T h e v a r i a n c e o f c o v a r i a n c e r u l e s for associative matrix memories and reinforcement learning.\224 Neural Computation  vol 5 pp 205\226209 1993 35 G  P a l m a n d F  S o m m e r  223 A s s o c i a t i v e d a t a s t o r a g e a n d r e t r i e v a l i n neural nets.\224 in Models of Neural Networks III  E Domany J van Hemmen and K Schulten Eds New York Springer-Verlag 1996 pp 79\226118 36 G  C h e c h i k  I  M e i l i j s o n  a n d E  R u p p i n  223 E f f e c t i v e n e u r o n a l l e a r n i n g with ineffective hebbian learning rules.\224 Neural Computation  vol 13 pp 817\226840 2001 37 D  S t e r r a t t a n d D  W i l l s h a w  223 I n h o m o g e n e i t i e s i n h e t e r o a s s o c i a t i v e memories with linear learning rules.\224 Neural Computation  vol 20 pp 311\226344 2008 38 A  L a n s n e r a n d O  E k e b e r g  223 A n a s s o c i a t i v e n e t w o r k s o l v i n g t h e 224 4 bit adder problem\224.\224 in Proceedings of the IEEE First International Conference on Neural Networks  M Caudill and C Butler Eds San Diego CA 1987 pp II\226549 39 227 227  223 A o n e l a y e r f e e d b a c k a r t i 002 c i a l n e u r a l n e t w o r k w i t h a B a y e s i a n learning rule.\224 International Journal of Neural Systems  vol 1\(1 pp 77\22687 1989 40 I  K o n o n e n k o  223 B a y e s i a n n e u r a l n e t w o r k s  224 Biological Cybernetics  vol 61\(5 pp 361\226370 1989 41 227 227  223 O n B a y e s i a n n e u r a l n e t w o r k s  224 Informatica Slovenia  vol 18\(2 pp 183\226195 1994 42 A  L a n s n e r a n d A  H o l s t  223 A h i g h e r o r d e r B a y e s i a n n e u r a l n e t w o r k with spiking units.\224 International Journal of Neural Systems  vol 7\(2 pp 115\226128 1996 43 A  S a n d b e r g  A  L a n s n e r  K  P e t e r s s o n  a n d O  E k e b e r g  223 A p a l i m p s e s t memory based on an incremental Bayesian learning rule.\224 Neurocomputing  vol 32-33 pp 987\226994 2000 44 A  K n o b l a u c h  223 N e u r a l a s s o c i a t i v e n e t w o r k s w i t h o p t i m a l b a y e s i a n learning.\224 Honda Research Institute Europe GmbH D-63073 Offenbach/Main Germany HRI-EU Report 09-02 May 2009 45 S  W a y d o  A  K r a s k o v  R  Q u i r o g a  I  F r i e d  a n d C  K o c h  223 S p a r s e representation in the human medial temporal lobe.\224 Journal of Neuroscience  vol 26\(40 pp 10 232\22610 234 2006 46 A  K n o b l a u c h  223 C o m p a r i s o n o f t h e l a n s n e r  e k e b e r g r u l e t o o p t i m a l bayesian learning in neural associative memory.\224 Honda Research Institute Europe GmbH D-63073 Offenbach/Main Germany HRI-EU Report 10-06 April 2010 47 227 227  223 N e u r a l a s s o c i a t i v e m e m o r y w i t h o p t i m a l b a y e s i a n l e a r n i n g  224 submitted  pp 226 2010 48 227 227  223 O n t h e c o m p u t a t i o n a l b e n e 002 t s o f i n h i b i t o r y n e u r a l a s s o c i a t i v e networks.\224 Honda Research Institute Europe GmbH D-63073 Offenbach/Main Germany HRI-EU Report 07-05 May 2007 49 A  K n o b l a u c h a n d G  P a l m  223 P a t t e r n s e p a r a t i o n a n d s y n c h r o n i z a t i o n in spiking associative memories and visual areas.\224 Neural Networks  vol 14 pp 763\226780 2001 50 T  S e j n o w s k i  223 S t o r i n g c o v a r i a n c e w i t h n o n l i n e a r l y i n t e r a c t i n g n e u rons.\224 Journal of Mathematical Biology  vol 4 pp 303\226321 1977 51 227 227  223 S t a t i s t i c a l c o n s t r a i n t s o n s y n a p t i c p l a s t i c i t y  224 Journal of Theoretical Biology  vol 69 pp 385\226389 1977 52 D  W i l l s h a w a n d P  D a y a n  223 O p t i m a l p l a s t i c i t y i n m a t r i x m e m o r i e s  what goes up must come down.\224 Neural Computation  vol 2 pp 85\226 93 1990 53 G  P a l m a n d F  S o m m e r  223 I n f o r m a t i o n c a p a c i t y i n r e c u r r e n t McCulloch-Pitts networks with sparsely coded memory states.\224 Network  vol 3 pp 177\226186 1992 54 B  G r a h a m a n d D  W i l l s h a w  223 I m p r o v i n g r e c a l l f r o m a n a s s o c i a t i v e memory.\224 Biological Cybernetics  vol 72 pp 337\226346 1995 55 H  M a r k r a m  M  T o l e d o R o d r i g u e z  Y  W a n g  A  G u p t a  G  S i l b e r b e r g  and C Wu 223Interneurons of the neocortical inhibitory system.\224 Nature Reviews Neuroscience  vol 5 pp 793\226807 2004 56 H  Z h a n g  223 T h e o p t i m a l i t y o f n a i v e b a y e s  224 i n Proceedings of the 17th Florida Arti\002cial Intelligence Research Society Conference  V Barr and Z Markov Eds AAAI Press 2004 pp 562\226567 57 P  D o m i n g o s a n d M  P a z z a n i  223 O n t h e o p t i m a l i t y o f t h e s i m p l e Bayesian classi\002er under zero-one loss.\224 Machine Learning  vol 29 pp 103\226130 1997 58 G  P a l m  223 L o c a l s y n a p t i c r u l e s w i t h m a x i m a l i n f o r m a t i o n s t o r a g e capacity.\224 in Neural and synergetic computers  ser Springer Series in Synergetics H Haken Ed Berlin Heidelberg New York Springer Verlag 1988 vol 42 pp 100\226110 59 D  G o l o m b  N  R u b i n  a n d H  S o m p o l i n s k y  223 W i l l s h a w m o d e l  A s s o ciative memory with sparse coding and low 002ring rates.\224 Phys Rev A  vol 41 pp 1843\2261854 1990 60 A  H o l t m a a t a n d K  S v o b o d a  223 E x p e r i e n c e d e p e n d e n t s t r u c t u r a l s y n a p tic plasticity in the mammalian brain.\224 Nature Reviews Neuroscience  vol 10 pp 647\226658 2009 61 A  K n o b l a u c h  223 S y n c h r o n i z a t i o n a n d p a t t e r n s e p a r a t i o n i n s p i k i n g associative memory and visual cortical areas.\224 PhD thesis Department of Neural Information Processing University of Ulm Germany  2003 62 227 227  223 O n c o m p r e s s i n g t h e m e m o r y s t r u c t u r e s o f b i n a r y n e u r a l a s s o ciative networks,\224 Honda Research Institute Europe GmbH D-63073 Offenbach/Main Germany HRI-EU Report 06-02 April 2006 63 227 227  223 N e u r a l a s s o c i a t i v e m e m o r y a n d t h e W i l l s h a w P a l m p r o b a b i l i t y distribution.\224 SIAM Journal on Applied Mathematics  vol 69\(1 pp 169\226196 2008 64 227 227  223 T h e r o l e o f s t r u c t u r a l p l a s t i c i t y a n d s y n a p t i c c o n s o l i d a t i o n f o r memory and amnesia in a model of cortico-hippocampal interplay.\224 in Connectionist Models of Behavior and Cognition II Proceedings of the 11th Neural Computation and Psychology Workshop  J Mayor N Ruh and K Plunkett Eds Singapore World Scienti\002c Publishing 2009 pp 79\22690 65 227 227  223 Z i p n e t s  E f 002 c i e n t a s s o c i a t i v e c o m p u t a t i o n w i t h b i n a r y synapses.\224 in Proceedings of the International Joint Conference on Neural Networks IJCNN 2010  2010 


   Table 4. Normalized Criteria Comparison Table In AHP   Reusability Meeting Operational Requirements Meeting project Deadline Reusability 0.157 0.148 0.272 Meeting Operational Requirements 0.789 0.744 0.636 Meeting Project Deadline 0.052 0.106 0.090  Table 3 and Table 4 show the weight values of the three criterions as compared to each other using the AHP process. These weights have been decided by the stakeholders after discussions among themselves Average weights can be derived from Table 4 as follows Reusability- 0.193 Meeting Operational Requirements- 0.724 Meeting Project Deadline- 0.083 These weights represent the priority of each criterion on a scale of 0 to 1  5.3. Argumentation Tree  We develop argumentation tree for each and every alternative separately. The ar guments are stated by stake holders and assembled under the alternative but they target a specific cr iterion. These arguments can either be supporting or attacking each other or their respective alternative nodes. We present three figures, where each figure represents the argumentation hierarchy for one alternative. Rectangular boxes represent the alternatives with the name of the alternative under it. Ovals represent the criteria with their descr iption. The arguments are specified by labels èAê, èBê, èCê for alternative çAdobe flashé, çAdobe Directoré and çOpen GLé respectively Along with the labels, the arguments also have indexes associated with them. Beneath the labels are two boxes The box on left shows the weight of the argument whereas the box on right shows the priority of the stakeholder who specifies the argument  Once the argument has been sp ecified, the user enters its weight. We first reassess the weights of the arguments using priority reassessment discussed in h e n us ing the techniques specified in [11 w e red u ce t h e arg u m e n t s  to a single level. Finally, the weighted summation of the arguments with the criteria weights helps us evaluate the final weights for the decision matrix. It is important to note here that, the aggregation method used for calculating the favorability is a weighted summation  The three argumentation hierarchies for the three alternatives are presented in the Figures 7, 8, and 9. The diagrams contain arguments, their weights and the stakeholderês priorities     Figure 7. Argumentation Tree For Adobe Flash   Figure 8. Argumentation Tree For Adobe Director 150 


     Figure 9. Argumentation Tree For Open GL  A1 The current system in flash does not have the functionality of dynamic allocation of particles like mine or clutter. It places them randomly  A1.1 That is not of much importance because it still gives a new position to mine and clutter particles A2 Current system in flash has faster response time as compared to system in Adobe Director A3 The current system doesnêt satisfy many of the features required for the new system like database A4 Adobe Flash cannot communicate with database A4.1 Flash doesnêt support database but database support is very important and critical A4.1.1 The system should be able to generate evaluation reports for trainee based on pr evious records stored in the database A5 Flash doesnêt create sound clips  A5.1 We donêt need sound creating features as the sys tem has to generate sound. We can play externally recorded sound files using Adobe Flash A6 Flash can provide good visual effects as compared to Adobe Director A7 The developer has good knowledge in development using Flash so the system can be developed quickly B1 We could reuse the system already developed for sound generation, as it is developed using Adobe Audition for analysis which is somehow related to Adobe Director B1.1 The current system is better synthesized in terms of sound production and the sound produced is also instantaneous rather than discrete B1.2 That current system has certain performance issues like slow response time B1.3 The current system in Adobe Director has the feature of producing dynamic coloring scheme on approaching a mine. This kind of scheme is highly preferable and is not present in Adobe Flash system B2 Adobe Director can provide more functionality as compared to the current flash system. E.g. Multiple sounds while detecting mines   B2.1 Adobe Director can provide better visual effects as compared to flash e.g. in case of GUIês   B2.2 A modified version of the current system in flash can also provide the same functionality B2.2.1 We cannot integrate code developed in other platforms with Flash, but Flash can be integrated in Adobe Director B3 The interface provided by flash is not professional enough. It is too simple and straight forward for doing more things in future   B4 Easily available plug-ins can help integrate the tracking system developed in C# with Adobe Director  B4.1 Code developed in Open GL/AL can also be integrated using Adobe Director using suitable stubs   B5 A new sound recognition algorithm is being developed in Adobe Audition which can be integrated with Adobe Director but not with Open GL or Flash Evidence supported B6 If the current system is reused; the project deadline can be met easily B7 The developer has very little experience in development using Adobe Director   B7.1 The developer can take help from the already developed system in Adobe Director C1 The tracking software already developed is coded in C#/NX5. We could reuse that and develop our system in Open GL/AL C1.1 Open GL has C# libraries which can be used to develop the system C2 Because the platform used is for high end application development, it can provide good GUI and database support C2.1 Open GL/AL can help us generate dynamic surfaces for mine detection and training which the original system in flash does not have C4 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C3 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C4 The time taken for developing the project using open GL will be comparatively more as the whole system would have to be developed from scratch C4.1 If Open GL has support for C# libraries, and then the system could be develope d faster as developer is quite familiar with programming languages like C 151 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





