Data mining Algorithm of Browsing Pattern Based on Web Log Li Yu-xia and Li Hong-yu Department of Computer Wenli College Harbin Normal University Heilongjiang Province, China Abstract An Web log contains a large number of user browsing information, so how to effectively mine it for user browsing pattern is an import ant research subject. Based on the analysis of the problems in the current mining algorithm of the user browsing pattern, and combining the characteristics of the existing fast association rules mining algorithm, this paper adds the sequential constraint and the time factor, and puts forward a browsing pattern mining algorithm TBPM which is based on the temporal constraint. It 
also designs incremental updating algorithm based on the temporal frequent item set algorithm TBPM. At last, it makes a comparison with the related work of the class Apriori algorithm, and the experimental results on the actual data have verified the effectiv eness of this algorithm Keywords-Web log mining, temporal constraint, browsing pattern, incremental updating I I NTRODUCTION With the constant developm ent of Internet, Web log mining has become an international emerging important research field. Web log mining is making use of the principles and ideas of data mining to expand and improve the traditional mining methods 
aiming to new features of Web logs, and to apply them in Web logs so as to be mined for frequent browsing patterns. The user-based browsing pattern can optimize the website structure, and improve the website performance Web log data is a kind of sequence composed of realtime, continuous and ordered data, which has a large amount of data, dynamicity, timeliness and other characteristics. But the exis ting browsing pattern mining algorithm \(class Apriori  eeds to frequently scan the database, with the low effici ency, and considers little about the timeliness of the browsing pattern. The browsing patterns obtained from mining in fact are all assumed to be 
permanently valid. Based on the characteristics of transactions and sessions for users, and combining the existing fast association rules mining  thinking, this paper puts forward a browsing pattern mining algorithm TBPM Time Browsing Pattern Mining\with the temporal constraint, which is suitable for Web logs. This algorithm simplifies the generating operatio ns of the candidate pattern in the process of mining. In this algorith m, scan the database once to obtain a continuous subsequence set of each transaction, and use set and crossover operations to obtain support. This algorithm increases the effective time domain and time-matched ite m sets, and merges time 
intervals. Compared with the class Aprior i algorithm, this algorithm has less time, good scalability, and timeliness II P ROBLEM STATEMENT Data mining based on Web logs requires first carrying out data pretreatment, including user identification and transaction identification. M.S Chen and others put forward the concept of MFR \(Maximal Forward Reference\as the basis for division of Web transactions. To divide Web transactions based on MFR means that the path formed from the first page the user forward browses to the previous page he backspaces each time in a user session is a transaction. For example, in a user session the requested page order is A-B-A-C-D \(capita 
l letters are used in this paper to mark pages\, and then the corresponding transactions are A-B and A-C-D. After pretreatment, data in Web logs is stored in th e database in the from of recording each transaction at a time, and the first page access time in the path is taken as the access time for each transaction. To facilitate th e discussion, there are the following definitions Definition 1 \(Subpath ath P={p 1 p 2  003ž\003 p n  003 the arbitrary P  p i p i+1  003ž\003 p i+j 
1 0010 i,i+j 0010 n is a subpath of P stating that P contains P    Definition 2 \(Temporal frequent browsing pattern\a Web access path P={p 1 p 2  003ž\003 p k  satisfies the following conditions 000\003 Then it states that the path P is the temporal frequent browsing pattern in the time in terval of TE. In the formula S min is the pre-defined minimum support by users 
sup\(P is P s support; TE is a time interval III B ROWSING PATTERN MINING WITH THE TEMPORAL CONSTRAINT A Temporal frequent browsing pattern mining algorithm This algorithm is mainly to scan the database once, and to obtain all item sets from each transaction in Web transaction flow which are stor ed in the temporary item sets most in memory; when reaching a threshold value, they are written into the physical databa se in batches\lso, this algorithm sets the counting device \(support\ appearing in each item set, and adds two time domains to respectively 


record start access time ts and end time te which support this item set. All the te mporary sub-sequence sets subit 1 subit 2  003 subit max_T in each transaction can match with each candidate item set candit 1 candit 2  003 candit max_T which are generated and stored in the candidate item set CanditSet DB of all transaction sets Each item set comes with the time frame which supports the frequent access item set, and then this algo rithm traverses candidate item sets to filter out the item set greater than the given minimum support, so as to form temporal frequent access path sets. The candidate item data sets after being reorganized can replace the original database, and thus can reduce the time to repeatedly search the database First, briefly describe the extension and merging technologies of time intervals   1\ Extension of the time interval Extension of the time interval refers to the outward extension of its two endpoints, in order to make the two time intervals meet or overlap, and then to combin e them into the same time interval. The degree of outw ard extension of the time interval’s two endpoints is determined by the promoting factor f which indicates the extension degree of the time interval. The value of the promoting factor f can be automatically set by the algorithm, and can also be designated when the algorithm is called. In a word, the value of the promoting factor f should be selected empirically based on specific application areas 2\ Merging of time intervals Supposing    1 1 1 002 003 004 I I I and    2 2 2 002 003 004 I I I are two time interval variables, whose temporal merging operation is defined as   Temporal merging 005\006 T 002 if 003 002 007 2 1 I I or 003 002 007 1 2 I I  then 010 004 2 1 I I T 002 otherwise    3 3 2 1 002 003 004 I I I I T 002 in which   min 2 1 3 003 003 003 004 I I I and   max 2 1 3 002 002 002 004 I I I  The following is an example of the algorithm implementation. Suppose there is a temporal transaction database DB as shown in Figur e 1, including 4 transactions and the support number is 2. In this algorithm, TID is the transaction identifier; Itemset is the url sequence set which users access \(which has corresponded to letters\e is the access time for a transaction; the promoting factor f is 1.5 The diagram 1 of the mining process is shown below Figure 1 The diagram of TBPM algorithm generating the browsing pattern Algorithm 1: Frequent browsing pattern mining algorithm based on the temporal constraint Input: the Web transaction da tabase DB in a sliding window, the minimum support S min the maximum transaction length max_T, and the promoting factor f Output: the temporal frequent item set L Algorithm TBPM \(DB S min max_T 003 f 1 L 010  2\CanditSet DB CanditSet\(DB,f\; //Generating temporal candidate item sets 3\ candit i 001\031 CanditSet DB  4 i c j 001\031 canditi | c j count 002 S min  5 i T i L L max_ 1 004 004 002  Among them, in order to improve the algorithm efficiency, the comparison and matching operations between all the non-empty subsets of a transaction and the generated candidate item sets adopt sets’ union intersection, and subtraction operations. All the temporary transaction subsets formed by i items are expressed by subit i and all the candidate item subsets formed by i items are expressed by candit i For each item in the intersections of temporary item sets and can didate item sets, determine whether the time is matched If so, support increases by 1 merging the effective time otherwise, add the item to candidate sets. Add each item in the transactions of temporary item sets and candida te item sets to candidate 


item sets. Set support as 1, and set the access time of the session Start and end time are the extension time for this session\. The generating operator CanditSet\(DB,f\ of the temporal candidate item set is shown below Algorithm CanditSet\(DB,f 1\dit 010  2\or each transaction T 011 DB 3 subit= subset\(T By definition 1, obtain all the non-empty continuous subsequence sets of each transaction At the same time, extend the effective time of all subsequences based on f 4\or each subit i 011 subit 5 N_set=subit i candit i  Subtraction operation is used for the i th subset and the i th candidate item set 6 002 For each n j 011 N_set 7\dd the item n j to candit i Add the item set n j to the i th candidate item set orderly inserting 8 j count=1 9 j ts=T.time-\(f-1\T.time; n j te=T.time*f When the subset appears in the candidate item set for the first time Start and end time are both the time after the transaction time extends 10 O_set=candit i 012 subit i  Union operation is used for the i th subset and the i th candidate item set 11\or each o j 011 O_set 12\or each c k 011 candit i 13 if 010 004 k T j c o 002 Time not matching 14\dd the item o j to candit i 15 j count=1 16 j ts=T.time-\(f-1\T.time 17 j te=T.time*f 18\    else  //Time matching, merging 19 c k count 20\\(o j ts< c k ts\ then c k ts= o j ts 21\\(o j te> c k te\en c k te= o j te 22 Return candit B Log incremental mining With the continuous generation of log data, it is necessary to carry out incremental updating mining. This part designs incremental updating algorithm based on temporal browsing pattern mining algorithm in the last part Aiming to the continuous generation of new logs, we need to make full use of the mine d results. On the basis of a known pattern, we need to mine the transaction sets which have been modified to discover a new pattern and get rid of the failure pattern, so as to get a real-time updated user browsing pattern 1\ Symbol definition in the algorithm The frequent set of the database DB is expressed by L, and the frequent set of DB 013 db is expressed by  L  D, d and UD respectively express the transaction number of the database DB, db, and DB 013 db; support counts X.sup D X.sup d and X.sup UD respectively express the transaction number containing X in DB, db, and DB 013 db 2\ Related concepts of the incremental algorithm In the database DB, add one more db database formed by d transactions. For the given minimum support S min the support of the item set X in the database DB 013 db is not less than S min namely X.sup 014 S min 015 D+d\, the item set X is frequent in the updated database DB 013 db The algorithm thinking: scan once the candidate item set CanditSet db generated by db’s use of TBPM algorithm. For the item sets in CanditSet db there are two cases, including belonging to the initial frequent set L and not belonging to the initial frequent set L For the item set belonging to the initial frequent set L and time matching\, recalculate the new support count and merge time. If the item set is a loser when it is less than the minimum support count of the updated database DB 013 db, only modify the support and merge time without other treat ments. Otherwise, it is a winner. Then reserve and add it to the new frequent set  L  For the item set not belonging to the initial frequent set L it is first pretreated. If the item set is less than the minimum support count of the updated database DB 013 db, pruning and marking are used \(not tr ansmitted to the new frequent set\itSet DB and add the item sets which meet the conditions of the support count to  L  The support counts is {I 1 I 2 I n sup D I 1 I 2 I n sup d At the same time, merge the item sets which are pruned and marked in CanditSet db into CanditSet db if it is the same item set \(time matching\, then update support and merge time; if it is a new item set, then it is added into For example, suppose D 004\010 4 003 d=3 003 S min 0.5 003 f=1.5 T 1 003 T 2 003 T 3 and T 4 are four transactio ns in the initial database DB, while T 5 003 T 6 003 T 7 are three transactions in the new database db, as shown in Table 1 Table 1 db transaction database TID Itemset time t5 ABCDE 10 t6 ABCD 12 t7 BCD 14 Scan once the candidate item set CanditSet db generated by db’s use of TBPM algorithm CanditSet db A B,[5,21],3 C   5,21],3 D  E  5,15],1 A BC  5,21],3  C D,[5,21],3 A BC BC D 5,21],3 C DE,[5,15],1 A BC D,[5,18  BC DE,[5,15],1 A BC DE  5,15],1 can  CanditSet db once, and for each item determine whether it is 


contained in L A,B,C,AB,BC,ABC is contained in L  and time matches, so modify support and merge time in CanditSet DB  For the item {A,B,C,AB,BC,ABC whose support is greater than 0.5*7, directly add it to  L  D,ECD,DE,BCD,CDE,ABCD,BCDE,ABCDE is not contained in L For the item {D CD existing in CanditSet DB modify the support and merge time. For the item {DE,BCD,CDE,ABCD,BCDE,ABCDE} which does not exist in CanditSet DB  add it to CanditSet DB  For the item D,CD} whose support is greater than 0.5*7, add it directly to  L  At last  L  003 A 003  003 C  003  003 CD 003”\003 AB 003 BC 003 ABC The advantage of this algorithm is that a new frequent set  L is generated while the initial candidate item set CanditSetDB is updated, without scanning the huge CanditSetDB once again to discover the frequent item  L till the candidate item set Cand itSetDB is updated, which effectively improves the efficiency of the algorithm IV A NALYSIS OF EXPERIMENTAL RESULTS The class Apriori algorithm used for mining user browsing pattern put forward by the literature [1 m e s from improvement of the Apriori algorithm  which discovers frequent item sets when mining association rules in the data mining field. The disadvantage of the class Apriori algorithm is its need to repeatedly scan the database, which may also generate huge candidate sets. The following is the comparison between the TBPM algorithm and the class Apriori algorithm, which indicates the speediness and effectiveness of the TBPM algorithm A Comparison of algorithm efficiency This part makes a performance comparison between the TBPM algorithm and the class Apriori algorithm when they are used for Web log transaction database mining. The following shows the comparison between the two algorithms, aiming to different Web transaction database minings. X-axis indicates the size of different Web transaction databases, and Y-axis indicates the time spent in running the algorithm. The results are shown in Figure 2 Figure 2 The efficiency comparison between the TBPM algorithm and the class Apriori algorithm Seen from the experimental results, as the database size increases, the TBPM algorithm uses less time than the class Apriori algorithm, which indicates that the TBPM algorithm has good adaptability in the large-scale database mining. This is because the TBPM algorithm only scans the database once, while the class Apriori algorithm belongs to the Apriori algorithm. So it is inevitable to bear the cost of the generation of a large number of candidate sets and frequently scanning the database. The generation of Lk each time requires scanning the database once. Due to the fact that the user access log data base stored in the server is usually very huge, the scanning process is inevitably very time-consuming. From this point of view, the TBPM algorithm is quite better than the class Apriori algorithm in efficiency. When mining a large-scale database, the corresponding mining time becomes critical, therefore, the TBPM algorithm has good efficiency B Analysis on incremental mining algorithm This part makes a performance comparison between TBPM-based incremental updating algorithm and Aprioribased FUP algorithm  when they are used for Web log transaction database mining. Suppose the maximum length of the session is 8; DB is 100k; db is 1k; the minimum support sup is 2%. When the potential frequent access path takes different values, the pe rformance ratio of the log incremental algorithm and the FUP algorithm is shown in Figure 3 Figure 3 Performance ratios of log incremental algorithm and FUP aiming to pattern length changes Seen from the experimental results, as the potential frequent access path length increases the performance ratio of log incremental algorithm to the FUP algorithm also increases proportionately, which indicates that log incremental algorith m has high efficiency in long pattern mining, related with the ch aracteristics of the TBPM algorithm. In the entire process of log incremental algorithm, there is a need to only scan DB once and db twice, unrelated with the maximum length of the frequent path. But the FUP algorithm is based on the Apriori algorithm, which requires scanning all the databases K 000\023 000\024 000\025 000\026 000\027 000\030 000\025\000\026\000\027\000\030\000\031 Potential fre q uent access p ath len g th performance ratio log incremental algorithm and FUP 


times. That is, when the greater is the frequent path length it requires scanning the databa se more times, which thus results in reduced efficiency of the algorithm C.  Comparison of mining results The following is a comparison result between the class Apriori algorithm and the TBPM algorithm, as shown in Table 2 and Table 3, which further illustrates the effectiveness of the TBPM algorithm Table 2 The patterns generated by the class Apriori algorithm No Web browsing patterns support 1 jxky/gzzd/kyb/kxyj.htm 0.42 2 xy/xyxw.html 0.68 3 zshjy/bzhzhsh/2008/123.html 0.45 4 Jxky/jxchg/23.html 0.82 5 Jxky/kybgsh/2008/122.html 0.38 Table 3 The patterns generated by the TBPM algorithm No 000\003 Web browsing pattern 000\003 time zone 000\003 suppor t 000\003 1 jxky/gzzd/kyb/kxyj.htm April-June 0.42 2 xy/xyxw.html March-December 0.68 3 zshjy/bzhzhsh/2008/123.html May-September 0.45 4 Jxky/jxchg/23.html April-June 0.42 5 Jxky/jxchg/23.html October-December 0.40 6 Jxky/kybgsh/2008/122.html SeptemberNovember 0.38 From the analysis on the tw o results, it sh ows that the No. 1 pattern discovered by the TBPM algorithm is effective only in April-June thus, the obtained result may be changes in Website link stru cture. With the incremental updating, this pattern may be eliminated. The No. 3 pattern is effective only in May-Sep tember, which can result in real-time decision support. The patterns mined by the class Apriori algorithm are considered to be always effective lacking fidelity, which indicates that the TBPM algorithm is reasonable and effective V C ONCLUSION Due to a large amount of Web log data constantly updated every day, and the discovery of Web logs with time mark from the analysis on Web log data, this paper puts forward a user browsing pattern mining algorithm, and makes a comparison between the TBPM algorithm and the class Apriori algorithm, which proves that the algorithm proposed by this paper is fast and effective, suitable for real-time Web log mining. Acco rding to the actual needs the appropriate time quantum is selected for mining using the algorithm in this pape r, and the cyclic pattern  can also be mined. In e-commerce, to select one-day logs for mining, you will find such rules as Milk 004\010\004\007 Bread 7:00AM 004\001 9:00AM\ practice, the author has designed and implemented a prototype system of Web log mining and proved that this algorithm is fast and effective R EFERENCES 1 M S C h e n  J S Pa rk  a n d P S Yu  Effi c i e n t  Da t a  M i n i n g for Pa t h  Traversal Patterns [C  I n   I EEE T r an sactio n s o n K n o w l e d g e an d D a ta Engineering,1998,10\(2\:209-221  C o o l e y  R., Mo bas h e r B., S r iv as tav a J   W e b Mining  I n f o r m atio n and Pattern Discovery on the World Wide Web”. Proceedings of the 9th IEEE International Conference on Tools with Artificial Intelligence\(ICTAI’97\,November 1997  O uy ang W e im in, Cai qing s h e n g  D i s c o v e r y o f A s s o ciatio n Rul e s  with Temporal Constraint in Databases [J o ur nal o f S o f t w a r e  1999, 10 \(5\: 527-532  M ao G u o j un, D u an L ijuan, W a ng S h i, S h i Y un. P r incipl e  and Algorithm of Data Mining ijing  T s ing hua U n iv e r s ity P r e s s   2005, 7. 66-70 5 A g r aw al R, S r ikant R. F a st al g o r ithm s f o r  m i ning  asso ciatio n r u l e s in large databases .In:Proc of the 20th International Conference on Very Large Databases.Santiago,chile,1994, 478-499  R  A g r a w a l   R. S r ikant  Mining S e que ntial P a tte r n s  P r o c e e d ing o f  the 11th International Conference on Data Engineering ICDE’95\,Taipei, Taiwan, March 1995  C he ung D W  e tc. Mainte nance o f D is c o v e r e d A s s o ciatio n Rul e s  in Large Databases :An Incremental Updating Technique , Proceedings of the 12th International Conference on Data Engineering , New Orleans , Louisana , 1996,106-114\(FUP  L iu X u e j un, X u H o ng bing e t c. Mining F r e que nt Cl o s e d  P a tte r n s  from a Sliding Window over Data Streams [J o ur nal o f Co m pute r  Research and Development, 2006, 43 \(10\: 1738-1743 


1] Stanley Y. W.Su? Chunbo Huang? Joachim Hammer. An Intemet-based negotiation server for e-commerce [J].The 298 VLDB Joumal? 2001\(1 2] Sunita Sarawagi. User-cognizant multidimensional analysis The VLDB Jouma? 2000\(?? 224-239 Al1en G.A.Unified Spproach to Automatic Indexing and Information Re?ieval. IEEE Expe?? 1993 3] Baetz-Yates. Modem Infonnation Re?ialNew York: Association for Computing [M].Machine Press 1999 4] Bal1esteros L.Dictionary -based Mtheds for Cross-Lingual Infonnation Retrial[C]. Proceeding of the 7th Intemational DEXA Conference on Database and Expert Systems Applications?1996?791-801 5] ???. ?????????[M 1999?123-152 Yimin Yan.Theory and practice of infonnation sestem[M]. Wuhan: Wuhan University Press.1999? 123-152 6] ??.?????????????[M 1999 Shan Wang. Technology of data mining and on-line analytical processing[M].Beijing: Science Press. 1999 7] ??????.?????[M].??????????1999 Jiaxun Ni?Wei Yuan.Applied statistics. [M].People's University of China Press? 1999 8] ??.SPSS?????????[M].???? ????2004 Wei Xue. SPSS statistical analysis technique and application[?1].Elec?onic Indus?y Press.2004 


B Results Below \(Table III are the results about Books@HPClab First column includes the query strings used to fetch data from Amazon and eBay. To give an insight of the reasoning load, we also give the number of total triples found in the data. The number of results includes only preferencematching books. All timings are given in seconds TABLE III B OOKS HPCL AB PERFORMANCE  Query Load Time Reasoning Time triples results Web 2.0 135 76 14808 96 Web 2.0 applications 139 85 15551 100 Programming in java 139 87 18959 100 Java 141 71 19808 97 The high load times observed are due to the increased number of requests made to the Web APIs: Amazon and eBay only allow paged fetch of their results, which enforces us to make multiple successive requests, each taking about one second. The poor reasoning performance is indicative of the inability of current DL-based reasoners to scale over a large number of triples, especi ally when rules are involved In Table IV we examine the measurements about Books@HPCLab_revised TABLE IV B OOKS HPCL AB _ REVISED PERFORMANCE  Query Load Time Reasoning Time triples results Web 2.0 80 12 69576 95 Web 2.0 applications 83 7 69576 99 Programming in java 108 9 69576 90 Java 113 16 69576 88 In this case, load times are found somewhat reduced \(Fig 8\: indeed, we now make fewer calls to the eBay API, by asking for multiple book offers in one request, thus resulting in fewer costly requests overall. The huge performance improvement is however in reasoning Fig. 9\otice a reduction in reasoning time by almost an order of magnitude, which cannot be justified by the different hardware: Clearly SPARQL queries are rapid when compared to the DL-based cl assification and the forward chaining algorithm of the triple store pays off 0 20 40 60 80 100 120 140 160 Web 2.0 Web 2.0 applications programming in java java Load Time \(sec Books@HPCLab Books@HPCLab_revised Figure 8 Load time comparison per query 0 10 20 30 40 50 60 70 80 90 100 Web 2.0 Web 2.0 applications programming in java java Reasoning Time \(sec Books@HPCLab Books@HPCLab_revised Figure 9 Reasoning time comparison per query In fact, the actual reasoning time for Books@HPCLab_revised has been included in the load time: Each time a new query dataset gets added to the store all the rules are fired and new triples are computed, based on total materialization. Besides, the most part of it includes the overhead to transfer the da taset to the store over HTTP In contrast, Books@HPCLab lo ads the ontologies directly from the disk. Therefore, ‘reasoning time’ would simply amount to the evaluation of the SPARQL query and results fetch Notice also that the number of triples for each query has increased. This is because we now reason over the whole dataset, i.e. the sum of triples produced by every query. To this, one should add the 450 triples found in the schema ontology \(BookShop\hat are loaded in the repository during initialization. Still, even though the triple count has quadrupled, the new implementation hugely outperforms its predecessor in terms of reasoning performance. To the DLbased reasoner’s advocacy we have to add OWLim’s inability to reason about datatypes. The latter can be easily integrated in SWRL with the use of built-ins [7 te also  the fact that the BookShop ontology falls into the ‘OWL Horst’ dialect, a proper subset of OWL 2-RL an d on l y  a small subset of OWL 2-DL, which the DL reasoner is able to provide sound and complete reasoning for VII C ONCLUSIONS AND F UTURE W ORK One of the greatest features Semantic Web has to offer is undoubtedly reasoning-based querying. Linked Data is another important dimension which enables opening linking and sharing information in machine understandable formats. However, it often appears to disregard the strong semantic underpinnings the Semantic Web comes with; and this due to the high complexity and stiff scalability of reasoning, especially for expressive languages. In this paper we have shown that a reasonable compromise can be achieved by relying on rule-based reasoning.  By combining linked data and rules we can produce meaningful addedvalue, oriented towards user recommendation, with controlled and scalable performance costs 
95 


At the same time, we have set up a prototype that can bootstrap semantic personalization services with Linked Data coming from legacy contexts. Our semantic mashup actually triplifies and links data from different sources that can be independently consumed later by other applications In addition, it can act as an independent framework for efficient management of rule-based ontologies; the component design is oblivious to the underlying business logic \(e.g. Java, servlets the front-end implementation e.g. PHP Having eliminated reasoning times, mashup creation requests are mainly responsible for lowering overall performance. An obvious improvement would be the asynchronous processing of these requests and loading the datasets to the triple store littl e-by-little or caching, in case of repeated queries, since datasets are already persistent More succinct data formats may also be necessary, since RDF/XML appears quite verbose and can put a considerable communication overhead. To this end, JSON http://json.org, http://json-ld.org e an a lternative for data serialization. Finally, the use of datatype operators inside rules comes in handy, although it can be mimicked by SPARQL queries. To this di rection the consideration of triple stores like OWLIM to express rules in SPARQL-like format appears promising R EFERENCES 1 B B i s h op A  K i r y a k ov  D O g n y a nof f  I  P e ik ov  Z. T a s h e v   and R. Velkov, “OWLIM: A Family of Scalable Semantic Repositories Semantic Web Journal, vol. 2 \(1\ pp. 33–42 2011  C  Bi zer T  Heat h and T  Bern e r s-L e e L i n ked Dat a T h e  Story So Far International Journal on Semantic Web and Information Systems, vol. 5 \(3\, pp. 1–22, 2009  J  Bro ekst ra A  Kam p man an d F  van Harm el en  S esa m e A  Generic Architecture for Storing and Querying RDF and RDF Schema”, Proc of 1st International Semantic Web Conference, LNCS, vol. 2342, pp. 54–68, 2002 4 P C o w l e s  E x pos ing W e b A pplic a tions Se m a ntic a l l y U s i ng  RAP \(RDF API for PHP PHP Architect, vol. 3 \(10\, pp 34–39 2004 5 J C r upi, a n d C  W a r n e r  E nte r pr is e Ma s hups T h e N e w  Face of Your SOA SOA World Magazine, http://soa.syscon.com/node/719917, 2009 6 S H a r r i s a n d A Se a bor ne  S P A R Q L 1.1 Q u e r y L a ng uag e   W3C Recommendation. http://www.w3.org/TR/sparql11query 2011 7 P H itz le r a nd B   P a r s ia  O nt ol og ie s a nd R u le s  S. Sta a b  and R. Studer, Eds. Handbook on Ontologies, pp. 111–132 Springer Verlag, 2nd Edition, 2009 8 A H o g a n I nte g r a ting  L i nk e d D a ta thoug h R D F S a n d OWL: Some Lessons Learnt Proc. of 5th International Conference on Web Reasoning and Rule Systems. LNCS vol. 6848 pp. 250–256, Springer, 2011  A  Ho gan  J P a n A  P o l l e res an d R Yu an  S cal ab l e OW L 2 Reasoning for Linked Data”, Proc. of Reasoning Web Semantic Technologies for the Web of Data LNCS vol 6848, pp. 250–325, Springer, 2011 10 H  J  te r H o r s t C om bining R D F a nd P a r t  of O W L  w ith Rules: Semantics, Decidability, Complexity Proc. of 4th International Semantic Web Conference, LNCS vol. 3729 pp. 668–684, 2005 11 A  K a lou, T   Pom onis  D   K outs o m itr op oul os a n d T   Papatheodorou, “Intelligent Book Mashup: Using Semantic Web Ontologies and Rules for User Personalisation”, Proc of 4th IEEE Int. Conference on Semantic Computing, pp 536–541, 2010 12 B  M o tik e t a l    O W L 2 W e b O n tol o g y La ng ua g e P r of ile s   W3C Recommendation http://www.w3.org/TR/owl2profiles/, 2009 
96 


0     0     1 0     0     0 0     0     0 153 target e-shopper for the past nT ?  periods prior to time T are given, it is important for marketer how to predict e-shoppers purchase behavior at timeT For solving the above problem, the following measures are taken. First, transaction clustering is conducted, so that all the transactions of e-shoppers are clustered. The SOM technique is used to cluster target e-shoppers transactions Then it is necessary to detect the evolving e-shopper purchase sequences as time passes. These e-shopper behaviors, which are derived from a change in the cluster number of each e-shopper, are kept in the purchase sequence database. Finally sequential purchase patterns over user-specified minimum support and confidence are extracted by using the association rule. The sequential purchase patterns are then stored in the association rule database Although SOM technique can obtains transaction clusters SOM clustering technique often breaks down when handling very high-dimensional data. So it is proposed that using product classes represents the hierarchical relationships among products. The method can make an effective dimensionality reduction while improving clustering results Assume that a product class set P  is classified into n different subclasses, and that each subclass consists of subclasses at a lower level, or eventual leaf products, as follows PPPPP nn ,, 121 ?= "                          \(1 Suppose that C is the set of the transactions of m e-shoppers during s periods before timeT . More specifically letC be composed as follows CCC kTmkTkTC ???= ,,2,1 ,, "                   \(2 11,0 ?= sk "  2?s WhereC kTj ?, ?C  is a non-empty subset of products Each C kTj ?,  represents the product class or classes from which e-shopper j purchased products at time kT EveryC kTj ?,  is transformed into an input matrix composed of a bit vector, and the matrix to be transformed is used in the transaction clustering. The time-ordered vectors for a particular e-shopper represent the purchasing history of the 


e-shopper; this input matrix can be thought of as the dynamic profile of the e-shopper. A dynamic e-shopper profile is defined as follows Let C _ be a dynamic e-shopper profile. Then, C _ is defined by the following matrix for n product classes and m e-shoppers over the course of s periods  3 mj "2,1 11,0 ?= sk " 2?s Where                         1 if Pi?C kTj   All the transactions of e-shoppers in the training e-shopper purchase database are transformed into dynamic e-shopper profiles based on their prior purchase behaviors. Then we use the SOM clustering technique to assign each transaction to a group. This transaction clustering facilitates the discovery of the dynamic cluster sequence of e-shopper The transaction clustering results in the following set of q clusters DDDD q"21 ,=                             \(4 Where each Di is a subset of C _ the given in \(3 A rearrangement of these clusters by  e-shopper  and by time  period is necessary for the identification of the dynamic behavior of each e-shopper. It is possible to learn the cluster sequence of a e-shopper by identifying the cluster to which each transaction of the e-shopper belongs, during each time period. To formalize this concept, we use the following terminology Let BP j be the behavior pattern of e-shopper j . Then, the behavior pattern BPi is identical to the changes in the cluster number of e-shopper j during s periods and is defined as follows  5 Where D kTj ?, ?D ,11,0 ?= sk "  2?s The  process  of searching for  a behavior path can 


be simply conducted through transaction clustering. All e-shoppers  have  a behavior path based on their prior transactions. The association rule technique is well suited for determining the most frequent pattern with confidence, since it provides automatic filtering capabilities. To discover the behavior path of a target e-shopper at time T based on his/her past behavior, the input data should be divided into a conditional part and a consequential part. Association rules are descriptive patterns of the form X?Y, where X and Y are statements regarding the values of attributes of an instance in a database. X is termed the left-hand-side, and is the conditional part of an association rule. Meanwhile, Y is called the right-hand-side, and is the consequent part. The conditional part is composed of the left-hand-side assigned to the consequential part R j represents the association rule about the user specified minimum support and confidence in the following form R j 6 A rule R j  indicates that, if the path of a e-shopper is DD TjsTj 1,1, , ?+? " , then the behavior cluster for that e-shopper is D Tj, at timeT It is necessary to know the degree to which the behavior path of a target e-shopper during 1?s periods beforeT  is similar to the association rule. The cluster path of a target e-shopper, transformed via the SOM, is compared with the association rules derived from other e-shoppers paths, and then the best-matching path is determined. Execution of this 0 if Otherwise    C kTj 154 process requires new measures for calculating the degree of correspondence between the association rules and the behavior path of a target e-shopper. This similarity measure is defined as follows     


1 1  s i i kTj i j SSD                            \(7 Where        1 if RD kTikTi   Si kTj 0 otherwise mj "2,1 11,0 ?= sk " 2?s ni "2,1 The above definition indicates that, if the behavior path of a target e-shopper i is equal to the conditional part of association rule j in the same period, then S j kTi ?,  is equal to one, otherwise is equal to zero. However, even if the similarity measure is high, a choice of the association rule suited to the prediction of the cluster of a target e-shopper at timeT is difficult, since such a rule is not general, given that the support and confidence of the association rule may be remarkably low. Therefore, to assure a good fit between the behavior path of a target e-shopper and the conditional part of the association rule, it is necessary to measure fitness. Fitness is defined as follows Suppose FDij be a degree of the goodness-of-fit between the behavior path i and the association rule j . Then, FDij is defined as follows ConfidenceSupportSDFD jjjiij = ,             \(8 Using the above definition, we can determine the cluster of a target e-shopper at timeT is a consequential part R Tj, of the association rule j with maximum FDij IV.  ILLUSTRATIVE EXAMPLE In this paper, we use Table 3 as example given to illustrate proposed method. The set of product classes given in Table 3 is P={Candy, Can, Milk, Bread, Biscuit}. The transactions of e-shopper CID006 are C June,006 {Candy}, =C May,006 {Can}, =C July,006 {Milk 


Therefore, the dynamic purchase profile of CID006 buying the set of products {Candy, Can, Milk Bread, Biscuit} from May to July may be represented as { }0,0,0,0,1,006 _ C June , { }0,0,0,1,0,006 _ C May and 0,0,1,0,0,006 _ C July . CID016 and CID006 are exactly same as both bought the same products during the same month Therefore, similarly, the transactions of target e-shopper CID016 are =C June,016 {Candy} and =C May,016 {Can The dynamic purchase profile of CID016 buying the products may be represented as 0,0,0,0,1,016 _ C June , { }0,0,0,1,0,016 _ C May TABLE IV. BEHAVIOR PATH OF E-SHOPPER CID 1+? sT  1?T  T 006 007 008  015 016 10 10 3   10 3 1 10   3 9 


3 4  9 9  Suppose 3=s , according to formula \(5 of CID006, BP006  is{ }9,3,10 , as shown in table 4,  which indicates  that  e-shopper  CID006 belonged to the tenth cluster in May and moved into the third cluster in June thereafter reaching the ninth cluster in July According to formula \(6 from e-shoppers path with regard to a minimum support of 0.1 and a minimum confidence of 0.5. The association rules are as shown in table 5. The similarities  between the path of e-shopper CID016  and the derived rules are 22016 =SD and 11016 =SD . Therefore, e-shopper CID016 belongs to the ninth cluster at timeT , since the fitness between CID016 and the rules are =FD2016 0.2 and =FD 1 016 0.2001. Therefore, we predict that the products which e-shopper CID016 is likely to buy are Bread, and Biscuit  TABLE V. THE DERIVED ASSOCIATE RULES Rule 1+? sT  1?T  T  Support Confidence 1 2 3  15 16 10 10 3   10 3 1 10  


 3 9 3 4  9 9 0.3 0.1 0.1 0.2 0.1 0.1 1.0 1.0 1.0 0.5 1.0 1.0 V. CONCLUSION The preferences of e-shopper change over time. In this study, we describe a new approach for mining the changes of e-shopper  purchase behavior over time and discuss solutions to several problems. For predicting e-shoppers purchase behavior, the following concepts are proposed: BP j SDij and FDij . The SOM technique is used to detect the evolving e-shopper purchase sequences as time passes. The purchase sequences are derived from the changes in the cluster number of e-shopper. The sequential purchase patterns over user-specified minimum support and confidence are extracted by using the association rule. Then the sequential purchase patterns are stored in the rule database Finally, we give the example to elaborate the new methodology. The research presented in this paper makes a 155 contribution to mining  e-shoppers purchase behavior basing on transaction data. E-retailer may be able to perform effective  one-to-one marketing campaigns by providing individual target e-shoppers with personalized Product basing on using purchase sequences In the future, some possible extensions to this work are as 


follows. From the results of this study, we know which products target e-shoppers are likely to buy, but we have not yet explored the times at which these purchases are likely to occur. Further research analyzing e-shoppers past purchasing patterns should likewise enable prediction of the most appropriate times. Furthermore, one interesting research extension would be the setting up of a real marketing campaign, in which e-shoppers would be targeted using this methodology, which could then be evaluated with regard to its performance REFERENCES 1]Dhond, Gupta, A., Vadhavkar, S. Data mining techniques for optimizing inventories for electronic commerce[C]. In the Proceeding of the ACM-SIGKDD International Conference on Knowledge Discovery and Data Mining, 2005:480-486 2] Kuo, R. J., Chen, J. H., Hwang, Y. C. An intelligent stock trading decision support system through integration of genetic algorithm based fuzzy neural network and artificial neural network[J]. Fuzzy Sets and Systems, 2001 118\(1 3] Agrawal, D., Schorling, C. Market share forecasting: An empirical comparison of artificial neural networks and multinomial logist model Journal of Retailing[J]. 1997, 72\(4 4] Weigen, A. S., Rumelhart, D. E.Generalization by weight-elimination with application to forecasting. Advances in Neural Information Processing Systems[J]. 1999, 3:875882 5] Chen, M, S, Han, J. Data mining: an overview from a database perspective[J]. IEEE Transactions on Knowledge and Data Engineering, 2006 8\(6 6] Schafer, J. B., Konstan. E-commerce recommendation application[J Journal of Data Mining and Knowledge Discovery, 2001, 16:125153 7] Giudici, P, Passerone, G. Data mining of association structures to model e-shopper behavior. Computational Statistics and Data Analysis[J]. 2002 38:533541 8]Changchien, S. Mining association rules procedures to support on-line recommendation by e-shoppers and products fragmentation[J]. Expert Systems with Applications, 2001, 20\(4 9] Song, H, Kim, J. Mining the change of e-shopper behavior in an Internet shopping mall[J]. Expert System with Applications, 2001, 21\(3 10] Anand, S, Patrick, A. A data mining methodology for cross-sales[J Knowledge-Based Systems, 2006, 10:449-461 11] G. Adomavicius, A. Tuzbilin. Using data mining methods to build e-shopper profiles[J]. IEEE Computer, 2006, 34 \(2 


12] Dhond, Gupta, A., Vadhavkar, S. Data mining techniques for optimizing inventories for electronic commerce[C]. In the Proceeding of the ACM-SIGKDD International Conference on Knowledge Discovery and Data Mining, 2005:480-486 13]Chui-Yu Chiu , Yi-Feng Chen. An intelligent market segmentation system using k-means and particle swarm optimization[J]. Expert Systems with Applications, 2009, 36: 45584565 14]Tzung-Shi Chen , Shih-Chun Hsu. Mining frequent tree-like patterns in large datasets[J]. Data & Knowledge Engineering, 2007,62:6583 15]H. Tsukimoto, Extracting rules from trained neural networks[J]. IEEE Trans.Neural Networks, 2000, 11 \(2 156 


http://datamining.buaa.edu.cn/TopKCos.pdf 14] M. Zaki, Scalable algorithms for association mining, TKDE, vol. 12, pp. 372390, 2000 


enhance item-based collaborative filtering, in 2nd IASTED International Conference on Information and Knowledge Sharing, Scottsdale, Arizona, 2003 476 2010 10th International Conference on Intelligent Systems Design and Applications 


Basi Association Rles Basi c  Association R u les Association is basically connecting or tying up occurrences of Association is basically connecting or tying up occurrences of events Ol dib t f ilt t O n l y d escr ib e se t s o f s i mu lt aneous even t s Cannot describe patterns that iterate over time e g  itemset a  0  b  0  g    Eg If you sense higher data rates on the downlink than normal AND New Route generated Implies high chances of Intrusion AND New Route generated Implies high chances of Intrusion Associative IDS for NextGen Frameworks Dr S Dua LA Tech 20 


Enhanced Inte r transaction Association Rules Enhanced Inter transaction Association Rules Enhanced Inter transaction Association Rules Extension of association rules Conditional relationships at multiple different time steps e.g itemset a\(0 0 1 2 You sense Higher data rate than normal AND You see New Route g enerated AND 1 minute a g o you detected checksum gg error packets AND 2 minutes ago your encountered wrong checksum   Implies High Chance of Intrusion Enhanced Rules and Confidence Associative IDS for NextGen Frameworks Dr S Dua LA Tech 21 


Complex Spatio temporal Association Complex Spatio temporal Association Rules Further extension of inter transaction association rules Describe event durations e.g itemset a\(0,X j,Y k,Z Eg  You sense high data rates for X seconds AND new route generated j minutes ago task completed in Y AND new route generated j minutes ago task completed in Y seconds AND checksum error packets received k minutes ago for Z seconds High Chance of Intrusion With highest confidence level in association rules  association rules  Associative IDS for NextGen Frameworks Dr S Dua LA Tech 22 


DMITAR Al ith ARD DMITAR Al gor ith m  ARD Problem Domain Problem Statement and Challenges Aiti Miig bd IDS A ssoc i a ti ve Mi n i n g b ase d IDS  Introduction to data mining Association rule in data mining DMITAR Algorithm  ARD New research Associative IDS for NextGen Frameworks Dr S Dua LA Tech 23 


DMITAR Algorithm DMITAR Difference Matrix Based Inter Transaction Association Rule Miner developed in DMRL Uses vertical data format Differences of the transaction IDs are used to generate extended itemsets Windowless mechanism Associative IDS for NextGen Frameworks Dr S Dua LA Tech 24 


Deep into the Mechanism The DMITAR algorithm is based on lhilii comp l ex mat h emat i ca l assoc i at i ve formulation and proofs Four major parts Four major parts Frequent 1 itemset generation Frequent 2 itemset generation Frequent k itemset generation k>2 Spatio temporal rule formation Associative IDS for NextGen Frameworks Dr S Dua LA Tech 25 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


