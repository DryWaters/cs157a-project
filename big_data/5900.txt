A Mashup Personalization Service based on Semantic Web Rules and Linked Data Georgia D. Solomou, Aikaterini K Kalou, Dimitrios A. Koutsomitropoulos and Theodore S. Papatheodorou Member, ACM, IEEE HPCLab, Computer Engineering and Informatics Dpt., University of Patras Patras, Greece solomou, kaloukat, kotsomit,tsp}@hpclab.ceid.upatras.gr Abstract The growing availability of Linked Data and other structured information on the Web does not keep pace with the rich semantic descriptions and conceptual associations that would be necessary for direct deployment of user-tailored services. In contrast, the more complex descriptions become 
the harder it is to reason about them. To show the efficacy of a potential compromise between the two, in this paper we propose an intelligent and scalable personalization service built upon the idea of combining Linked Data with Semantic Web rules. This service is mashing up information from different bookstores, and suggests users with personalized data according to their preferences, which in turn are modeled by a set of Semantic Web rules This information is made available as Linked Data, thus enabling third-party recipients to consume knowledge-enhanced information Keywords-Linked Data; ontologies; Semantic Web rules 
triple stores; mashups  I I NTRODUCTION The great proliferation of Linked Data paves the way for the deployment of robust applications, able to consume large datasets in a more effe ctive way 2 Ho w e ver, thei r limited semantics often lead to applications with poor knowledge discovery capabilities 8 9  O n t h e o t he r  ha nd   the use of full-fledged ontologies comes with an additional overhead, especially when inferencing is required In an attempt to achieve a trade-off between powerful reasoning and scalability we propose a web application that combines the basic principles of Semantic Web rules and Web 2.0 mashups [5 i n g to pro v id e a n intellige n t a n d  
scalable service for personalized querying over popular online bookstores. Our application \(‘Books@HPCLab provides users with the ability to search and find sell-offers for books that fit their preferences. These data are mashed up from different and heterogeneous sources on the Web like Amazon and Half Ebay. The descriptive metadata of retrieved books are finally triplified, thus producing a linking connection to their offe rs. All collected information is stored in a scalable rule-b ased triple store \(OWLIM    Gathered information is mapped to an OWL ontology that has been developed in order to describe users, books and offers \(the BookShop 
ontology\he BookShop ontology is kept in the triple store together with a number of rules, which reflect user preferences. This ontology- and rule- oriented approach renders the gathered information reusable and sharable. At the same time, we expose and make it available to the Linked and Open Data \(LOD world, through an appropriate interface Books@HPCLab application is a significantly enhanced version of a previous one presented in  T h i s n e w implementation actually serves as a proof of concept about the successful and efficient combination of Semantic Web Linked Data and rules in designing intelligent mashups The rest of this paper is organized as follows: Section II gives an overview of the overall application architecture and 
component interaction. Then, Section III puts focus on the BookShop ontology and the rules used for personalization The major architectural components are discussed in Section IV, where the data aggregation and triplication procedures are explained. Section V outlines the functionality of the application, which is then evaluated in section VI, by performing several timings in comparison to its earlier version. Conclusions and future work are presented in the last section VII II ARCHITECTURAL OVERVIEW In this section, we are sketching out the system’s architecture. We present its structural components designated to carry through the mashing and linking functionality that is finally provided to the end users. We 
also describe in what sense do we take advantage of Semantic Web technologies The full functionality of Books@HPCLab application is carried out by several distinct components that interact with a central management mechanism, the Core Unit An overview of the system’s architecture is given in Fig. 1. The Core Unit implements the app lication logic of the service and is responsible for communicating with the Mashup Component the Triple Store the User Interface \(UI  and finally with the Linked Data Component 
which carries out the proper exposure of retrieved information as Linked Data To implement communication and resource interchange among the central Core Unit and the other components, we exploit web services, programs, scripts, and other Web technologies. The implemented architecture is based on a RESTful communication strategy, where data transfer is implemented via HTTP requests. The semantic processing of gathered information is accomplished by designated Semantic Web technologies, like ontologies, rules, inference engines and a triple store mechanism 
2011 Seventh International Conference on Signal Image Technology & Internet-Based Systems 978-0-7695-4635-3/11 $26.00 © 2011 IEEE DOI 10.1109/SITIS.2011.12 89 


The UI component is the application front-end that provides users with all necessary facilities for mashing information that is in conformance with their preferences By interacting with the UI, a user can create his profile submit a search request and finally view and access appropriate results – available also as Linked Data through the Linked Data component. The user’s request, consisting of search keywords, is passed on to the Core Unit which in turn assigns it to the corresponding components The Mashup Component is responsible for establishing communication and interaction with the Amazon and Half EBay Web APIs. It aggregates data by mashing the user requests. Harvested data are then transformed to meaningful RDF descriptions and get stored in the triple store. The storage mechanism is built upon OWLI  w h i c h benefits from the scalable arch itecture and other features of the Sesame framework g is acco m p l i s h e d  through appropriate HTTP requests The triple store keeps the ontology schema and several rules, which reflect user preferences for personalization. The population of data as well as the reasoning process is performed on loading. Stored rules are fired immediately and produce a number of inferred statements. Results explicit and inferred\e available as RDF data and can be accessed via HTTP, by making SPARQL queries III O NTOLOGIES AND R ULES Here we describe our application’s core ontology, as well as the rules that come to augment our service with personalization capabilities. We first give an overview of the ontology’s main characteristics and proceed with a brief explanation about our custom rule set A The BookShop Ontology To design our book ontology, we took into account the kind of metadata offered by Amazon and Half Ebay responses. Our design process has resulted in the core ontology BookShop BookShop contains four main classes Book  Author  Offer and User Overall, ontology expressivity complies with OWL Horst [10  w ith t h e  exception of disjoint classes that are only partially supported under pD entailment OWL Horst in known to enjoy desirable computational properties, especially in comparison to the expressively richer OWL 2 where computational problems can be rather st  The class Book is enriched with relations about title publisher dimensions, ISBN, publication year, number of pages, format, rating, images in various sizes and a URL corresponding to the Amazon online bookstore – so as to describe the instances of this class. All these relations are captured in the ontology as datatype properties Items for sale on Amazon and Half Ebay can be sold by more than one seller for different prices and in different conditions \(‘New’ or ‘Used’\ Thus, any item – any book in this case – is associated with an offer. An offer is a combination of price, co ndition and vendor/seller Therefore, to find the price for a book, we have to get all the offers made by vendors selling this book on online bookstores. The concept of an offer is represented by the class Offer. Datatype properties such as Condition  Price and Origin express the price, the condition of the offered book and the seller URL in these bookstores, respectively The class User is meant to express user profiles. We capture the preferences of each user in this class, such as preferred condition, preferred rating, preferred publication year and preferred maximum price \(preference criteria\ll this data about users are represented as datatype properties Relationships between instances of the BookShop ontology are represented by object properties that express relations between instances of two classes. In this context, a Book must have at least one Author \(hasAuthor and inversely isAuthorOf\nd there is at least one Offer for a Book \(isOfferOf and inversely hasOffer Object properties that link an instance of the class Book to an instance of the class User and inversely, are defined to Figure 1 Architecture overview 
90 


express the user's preference for a book depending on which preference fields of the user's profile are covered. Then, a set of rules is responsible for actually populating these properties. For example, the object property prefersBookbyCondition would relate a user with books being in a condition the user prefers. There are also properties for the case where more than one preference criteria are met. For example prefersBook_byRate2 would hold in case exactly two criteria are satisfied. It is natural that whenever a prefersBookbyX relationship holds \(with X being condition, rating, etc\, then prefersBook_byRate1 would also be true for this particular user-book pair. Thus we define all prefersBookbyX properties as sub-properties of prefersBook_byRate1  B Personalization Rules Rules follow OWLIM’s inherently supported rule formalism. Based on what rule s dictate, a second matching process is performed upon retrieved results, taking into account user preferences \(user profile\These personalization rules” actually act as a filter to the result set, being able to distinguish among those books that satisfy user’s preferences and those that are irrelevant to the user’s profile TABLE I E XAMPLE RULES THAT WERE CREATED FOR MATCHING USERS  PREFERENCES  No Rule Body Rule Head 1 IF u prefersCondition z  AND b hasOffer o  AND o hasBookCondition z u prefersBookbyCondition b 2 IF u prefersMaxPrice z  AND b hasOffer o  AND o offerPrice z u prefersBookbyPrice b 3 IF u prefersRating z  AND b hasRating z u prefersBookbyRating b 4 IF u prefersPublicationDate z  AND b PublicationDate z u prefersBookbyPublicationDate b 5 IF upb AND u q b AND p, q subPropertyOf  prefersBook_ByRate1 AND p, q  prefersBook_ByRate1 AND p  q  u prefersBook_byRate2 b As a starting point, four rules were written in order to check the satisfiability of each preference criterion separately \(the ‘rule head’ in Table I\ Take for example the case of rule 1: It expresses the desired effect when correlating a certain book condition with a user’s particular preference about this characteristic. Along these lines, rules 2-4 match books that satisfy the remaining three user preferences For the case where two, three or more preference criteria are satisfied together, we wrote more rules so as to check the exact number of satisfied criteria. Rule 5 is such an example, for the case a book matches two criteria. Recall that prefersBook_byRate1 is a catch-all for rule heads 1-4 Then this rule can be read as follows match all user-book pairs that are related with two rule preferences p and q  but make sure that these pr eferences are unique, i.e different from each other Another couple of rules have been devised in a likewise fashion, so as to check all possible combinations IV D ATA A GGREGATION AND S EMANTIC S TORAGE A Mashup and Linked Data Components The Mashup component is the one that carries out the communication with Amazon and Half Ebay services, by interacting with their respective Web APIs. Whenever the user sends a searching call the searching pr ocess starts to query data from Amazon Web Services \(AWS\d especially from the US E-Commerce Service ECS In order to extract the appropriate data for our application, we choose the ItemSearch operation, among the set of available ECS operations Once our application completes the search process at Amazon, it starts searching Half Ebay: for each book returned by Amazon, we find additional offers that may be available at Half Ebay. We use the eBay Shopping Web Services and particularly, the FindHalfProducts operation The interaction with the eBay Shopping API is based also on the REST-protocol and the exchange of URL requests and XML files-responses Item ASIN 156484272X ASIN  DetailPageURL http://www.amazon.com/Web-2-0-How-Gwen-Solomon/dp 156484272X%3FSubscriptionId%3 D05QEM4HDNYGR2EDE91R2%26tag%3Dws%26linkCode%3Dxm2%26camp 3D2025%26creative%3D165953%26creativeASIN%3D156484272X DetailPageURL ItemAttributes Author Gwen Solomon Author ISBN 9781564842725 ISBN Title Web 2.0: How-To for Educators Title  ItemAttributes Offers Offer Merchant GlancePage http://www.amazon.com/gp/help/seller home.html?seller=ASWK2P83YYFWO GlancePage   Merchant  OfferAttributes Condition Used Condition   OfferAttributes  OfferListing Price FormattedPrice 20.27 FormattedPrice Price  OfferListing   Offer  Offers Item Figure 2 Sample Amazon response 
91 


FindHalfProductsResponse xmlns  urn:ebay:apis:eBLBaseComponents  Products Product Title Web 2.0: How-to for Educators by Lynne Schrum and Gwen Solomon 2010, Paperback Title DetailsURL http://syicatalogs.ebay.com/ws eBayISAPI.dll?PageSyiProductDetails&amp;IncludeAttributes=1&amp;Show AttributesTable=1&amp;ProductMementoString=111322:2:1055:3768915169 349003979:cd998ecc605cf041c490bb9931398786:1:1:1:1411329640 DetailsURL ProductID type  ISBN  156484272X ProductID ProductID type  ISBN  9781564842725 ProductID  ItemArray Item ViewItemURLForNaturalSearch http://product.half.ebay.com/Web-2-0        by-Gwen-Solomon-Lynne-Schrum-2010-Paperback-Gwen-Solomon        Lynne-Schrum-Paperback-2010_W0QQprZ 92445819QQt gZvidetailsQQitemZ342138964277 ViewItemURLForNaturalSearch CurrentPrice currencyID  USD  24.03 CurrentPrice HalfItemCondition BrandNew HalfItemCondition  Item ItemArray Product Products FindHalfProductsResponse Figure 3 Sample Half Ebay Response The application uses the RDF API for PHP [4 i n order t o  process aggregated data, to assi gn resolvable identifiers and ultimately to convert them to Linked Data in RDF format This conversion happens in both directions, i.e. when the data are collected as well as when a user requests RDF data in the book view page \(see next section\. In the latter case however, information are already available in the triple store and are fetched directly, with only a minimal intervention to tide them up. Fig. 2 and Fig. 3 present typical sample responses from Amazon and Half Ebay respectively. Fig. 4 shows how these responses are mapped to the BookShop ontology and translated in to RDF. Note finally that we assign book identifiers such that they can be resolved within our application \(see next section B Semantic Storage All information gathered by the Amazon and Half EBay web services - after a necessary transformation phase that ends up with its RDF representation - is stored in the application’s triple store in the form of RDF triples. The store is built upon OWLIM Lite v. 4.0 \(previously SwiftOWLIM which implements the Sesame SAIL interface, so that it can exploit all Sesame components. As far as reasoning is concerned, this is based on R-entailment defined by w h ere inf e ren ce r u les are applied directly to RDF triples. OWLIM is configurable as to what set of inference rules may support, thus choosing the level of supported semantics. Each rule is built of premises body\d conclusions \(head\h premises and conclusions are RDF statements, where the use of variables is allowable at any position An important feature of the recent version of Sesame Sesame v. 2.4.0\ is its compli ance with the latest version of SPARQL query language, namely SPARQL 1.1 [6 As a  consequence, OWLIM becomes able of supporting SPARQL 1.1 query functionality directly through the Sesame APIs. So, accessing and retrieving the data stored in the triple store is as easy as evaluating SPARQL 1.1 queries Given that Sesame uses an HTTP-based communication protocol, the aforementioned functionality is also feasible via the standard HTTP methods \(GET, POST, PUT, etc Sesame can also be coupled with a relational database, thus providing for fast indexing and evaluation of queries All RDF triples that answer a user’s specific search request are stored in the same OWLIM repository Nevertheless, the underlying Sesame architecture allows for storing triples in the form of quads’ where an additional context’ information piece can be attached. Context declares the provenance of data and is useful in order to distinguish among different users’ data BookShop:Book rdf:about  http://levantes.hpclab.ceid.upatras.gr:8000/BooksHPClab/book/156484272X  BookShop:Title rdf:datatype  http://www.w3.org/2001/XMLSchema#string  Web 2.0: How-To for Educators BookShop:Title BookShop:DetailPageURL rdf:datatype  http://www.w3.org/2001/XMLSchema#string  http://www.amazon.com/Web-2-0-How-Gwen-Solomon/dp/156484272X%3FSubscriptionId%3D05QEM4HDNYGR2EDE91R2 26tag%3Dws%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D156484272X BookShop:DetailPageURL BookShop:ISBN-10 rdf:datatype  http://www.w3.org/2001/XMLSchema#string  9781564842725 BookShop:ISBN-10    BookShop:hasAuthor rdf:Description rdf:about  Author_1_156484272X   foaf:name rdf:datatype  http://www.w3.org/2001/XMLSchema#Literal  Gwe foaf:name foaf:surname rdf:datatype  http://www.w3.org/2001/XMLSchema#Literal  Solomon foaf:surname rdf:Description BookShop:hasAuthor BookShop:hasOffer rdf:Description rdf:about  http://www.amazon.com/gp/help/seller/home.html?seller=ASWK2P83YYFWO  BookShop:BookCondition rdf:datatype  http://www.w3.org/2001/XMLSchema#string  Used BookShop:BookCondition BookShop:OfferPrice rdf:datatype  http://www.w 3.org/2001/XMLSchema#decimal  20.27 BookShop:OfferPrice rdf:Description BookShop:hasOffer BookShop:hasOffer rdf:Description rdf:about  http://product.half.ebay.com/Web-2-0-by-Gwen-Solomon-Lynne-Schrum2010-Paperback-Gwen-Solomon-Lynne-Schrum-Paperback2010_W0QQprZ92445819QQtgZvidetailsQ QitemZ342138964277  BookShop:BookCondition rdf:datatype  http://www.w3.org/2001/XMLSchema#string  New BookShop:BookCondition BookShop:OfferPrice rdf:datatype  http://www.w3.org/2001/XMLSchema#decimal  24.03 BookShop:OfferPrice rdf:Description BookShop:hasOffer BookShop:Book Figure 4 Data triplification and mapping to the BookShop ontology 
92 


V F UNCTIONALITY The first page of the application includes two choices for the visitor New User and Registered User New users are presented with a completion form, which includes fields such as Book Condition  Maximum Book Price  Publication Year and Maximum Book Rating thus forming the user profile. After successful authoriza tion, the user is directed to the main page of the application, which includes a search form. This consists only of a Search button and a text field, where the user types the keyword or the key-phrase to initiate the book searching process During searching, results are imported into the triple store and the rules, that would determine how user preferences are matched, are fired against the data. Search results are ranked based on the rules outcome, as shown in Fig. 5. In particular, the more criteria a Book satisfies \(the more preference rules it triggers the higher it appears in the results Each table’s row includes information, such as an autoincrementing number, the book’s title and a number of exclamation marks which express the number of satisfied criteria. Each book’s title is a link and clicking it, a page appears with all the available features of the specific book like Title, Author, ISBN \(Fig 6 The title and image of the book are links to its "official" page at Amazon. The offers for this book are also presented in a table. Following the Search link \(top right of Fig. 5\ the user is taken back to the main page in order to initiate a new query Finally, book information is made available as Linked Data in RDF, where books’ and offers’ links are resolvable within our application or within their original context Figure 5 Ranking of books about “Programming in Java” according to preferences of User_16 Figure 6 Information about the selected book and its offers 
93 


respectively \(Fig. 7\This is also true for the search results list, which is exposed as RDF along with the inferred book rankings, for other applications to be able to consume and reuse VI E VALUATION In order to identify the major points made by the design of this mashup, we evaluate it against the previous version of Books@HPCLab, originally introduced in n  summary, we notice a great improvement in query response time, which is mostly due to the utilization of a specialized triple store, as well as an overall performance improvement due to additional optimizations introduced First, we describe the methodology of the experiments, as well as their configuration. To clarify the comparison, we also summarize the different features between the two implementations. Then we pr esent the results for some typical queries made with each of the two applications respectively A Methodology and Configuration There have been measurements about the original mashup application Books@HPCLab d the new implementation Books@HPCLab_revised ll Books@HPCLab timings where made on a machine with a P4 HT processor, running at 3.0 Ghz and with 2.0 GB of RAM. Measurements about the revised implementation involve also a second machine hosting the triple store: Intel Core 2 6300@1.86 Ghz with 2.0 GB of RAM. However, the former computer is responsible for the mashup creation collecting and triplifying Amazon and eBay data\n both cases We measure the performance of both applications in respect to four queries posed by the same user \(i.e. same personalization preferences\We distinguish between load time which is how long it takes to fetch, triplify and store mashed-up data and reasoning time that is, the time it takes to evaluate the personalization rules against th e data set and get back the results. Full-text query evaluation is made independently by the Amazon API and therefore is included in the load time Results are comprised of the books that actually match at least one preference criterion. In principle, one would expect these results to be identical in both cases; however there may be slight differences in result count due to the fact that some eBay offers may have been added or withdrawn in the course of time between the two experiments Table II sums up the main differences between the two implementations Optimized means that we use a different algorithm to fetch the data from their sources resulting in fewer requests, which are actually responsible for most part of the load time TABLE II F EATURE COMPARISON FOR THE TWO IMPLEMENTATIONS  Books@HPCLab Books@HPCLab_revised 1\ Mashup Creation Web APIs Web APIs \(optimized 2\taset storage management File System Triple Store \(OWLim 3\le-based reasoning SWRL, Pellet Proprietary, OWLim 4\ommunication protocol fopen, fwrite OWLAPI HTTP REST, SPARQL Figure 7 Data triplification and mapping to the BookShop ontology 
94 


B Results Below \(Table III are the results about Books@HPClab First column includes the query strings used to fetch data from Amazon and eBay. To give an insight of the reasoning load, we also give the number of total triples found in the data. The number of results includes only preferencematching books. All timings are given in seconds TABLE III B OOKS HPCL AB PERFORMANCE  Query Load Time Reasoning Time triples results Web 2.0 135 76 14808 96 Web 2.0 applications 139 85 15551 100 Programming in java 139 87 18959 100 Java 141 71 19808 97 The high load times observed are due to the increased number of requests made to the Web APIs: Amazon and eBay only allow paged fetch of their results, which enforces us to make multiple successive requests, each taking about one second. The poor reasoning performance is indicative of the inability of current DL-based reasoners to scale over a large number of triples, especi ally when rules are involved In Table IV we examine the measurements about Books@HPCLab_revised TABLE IV B OOKS HPCL AB _ REVISED PERFORMANCE  Query Load Time Reasoning Time triples results Web 2.0 80 12 69576 95 Web 2.0 applications 83 7 69576 99 Programming in java 108 9 69576 90 Java 113 16 69576 88 In this case, load times are found somewhat reduced \(Fig 8\: indeed, we now make fewer calls to the eBay API, by asking for multiple book offers in one request, thus resulting in fewer costly requests overall. The huge performance improvement is however in reasoning Fig. 9\otice a reduction in reasoning time by almost an order of magnitude, which cannot be justified by the different hardware: Clearly SPARQL queries are rapid when compared to the DL-based cl assification and the forward chaining algorithm of the triple store pays off 0 20 40 60 80 100 120 140 160 Web 2.0 Web 2.0 applications programming in java java Load Time \(sec Books@HPCLab Books@HPCLab_revised Figure 8 Load time comparison per query 0 10 20 30 40 50 60 70 80 90 100 Web 2.0 Web 2.0 applications programming in java java Reasoning Time \(sec Books@HPCLab Books@HPCLab_revised Figure 9 Reasoning time comparison per query In fact, the actual reasoning time for Books@HPCLab_revised has been included in the load time: Each time a new query dataset gets added to the store all the rules are fired and new triples are computed, based on total materialization. Besides, the most part of it includes the overhead to transfer the da taset to the store over HTTP In contrast, Books@HPCLab lo ads the ontologies directly from the disk. Therefore, ‘reasoning time’ would simply amount to the evaluation of the SPARQL query and results fetch Notice also that the number of triples for each query has increased. This is because we now reason over the whole dataset, i.e. the sum of triples produced by every query. To this, one should add the 450 triples found in the schema ontology \(BookShop\hat are loaded in the repository during initialization. Still, even though the triple count has quadrupled, the new implementation hugely outperforms its predecessor in terms of reasoning performance. To the DLbased reasoner’s advocacy we have to add OWLim’s inability to reason about datatypes. The latter can be easily integrated in SWRL with the use of built-ins [7 te also  the fact that the BookShop ontology falls into the ‘OWL Horst’ dialect, a proper subset of OWL 2-RL an d on l y  a small subset of OWL 2-DL, which the DL reasoner is able to provide sound and complete reasoning for VII C ONCLUSIONS AND F UTURE W ORK One of the greatest features Semantic Web has to offer is undoubtedly reasoning-based querying. Linked Data is another important dimension which enables opening linking and sharing information in machine understandable formats. However, it often appears to disregard the strong semantic underpinnings the Semantic Web comes with; and this due to the high complexity and stiff scalability of reasoning, especially for expressive languages. In this paper we have shown that a reasonable compromise can be achieved by relying on rule-based reasoning.  By combining linked data and rules we can produce meaningful addedvalue, oriented towards user recommendation, with controlled and scalable performance costs 
95 


At the same time, we have set up a prototype that can bootstrap semantic personalization services with Linked Data coming from legacy contexts. Our semantic mashup actually triplifies and links data from different sources that can be independently consumed later by other applications In addition, it can act as an independent framework for efficient management of rule-based ontologies; the component design is oblivious to the underlying business logic \(e.g. Java, servlets the front-end implementation e.g. PHP Having eliminated reasoning times, mashup creation requests are mainly responsible for lowering overall performance. An obvious improvement would be the asynchronous processing of these requests and loading the datasets to the triple store littl e-by-little or caching, in case of repeated queries, since datasets are already persistent More succinct data formats may also be necessary, since RDF/XML appears quite verbose and can put a considerable communication overhead. To this end, JSON http://json.org, http://json-ld.org e an a lternative for data serialization. Finally, the use of datatype operators inside rules comes in handy, although it can be mimicked by SPARQL queries. To this di rection the consideration of triple stores like OWLIM to express rules in SPARQL-like format appears promising R EFERENCES 1 B B i s h op A  K i r y a k ov  D O g n y a nof f  I  P e ik ov  Z. T a s h e v   and R. Velkov, “OWLIM: A Family of Scalable Semantic Repositories Semantic Web Journal, vol. 2 \(1\ pp. 33–42 2011  C  Bi zer T  Heat h and T  Bern e r s-L e e L i n ked Dat a T h e  Story So Far International Journal on Semantic Web and Information Systems, vol. 5 \(3\, pp. 1–22, 2009  J  Bro ekst ra A  Kam p man an d F  van Harm el en  S esa m e A  Generic Architecture for Storing and Querying RDF and RDF Schema”, Proc of 1st International Semantic Web Conference, LNCS, vol. 2342, pp. 54–68, 2002 4 P C o w l e s  E x pos ing W e b A pplic a tions Se m a ntic a l l y U s i ng  RAP \(RDF API for PHP PHP Architect, vol. 3 \(10\, pp 34–39 2004 5 J C r upi, a n d C  W a r n e r  E nte r pr is e Ma s hups T h e N e w  Face of Your SOA SOA World Magazine, http://soa.syscon.com/node/719917, 2009 6 S H a r r i s a n d A Se a bor ne  S P A R Q L 1.1 Q u e r y L a ng uag e   W3C Recommendation. http://www.w3.org/TR/sparql11query 2011 7 P H itz le r a nd B   P a r s ia  O nt ol og ie s a nd R u le s  S. Sta a b  and R. Studer, Eds. Handbook on Ontologies, pp. 111–132 Springer Verlag, 2nd Edition, 2009 8 A H o g a n I nte g r a ting  L i nk e d D a ta thoug h R D F S a n d OWL: Some Lessons Learnt Proc. of 5th International Conference on Web Reasoning and Rule Systems. LNCS vol. 6848 pp. 250–256, Springer, 2011  A  Ho gan  J P a n A  P o l l e res an d R Yu an  S cal ab l e OW L 2 Reasoning for Linked Data”, Proc. of Reasoning Web Semantic Technologies for the Web of Data LNCS vol 6848, pp. 250–325, Springer, 2011 10 H  J  te r H o r s t C om bining R D F a nd P a r t  of O W L  w ith Rules: Semantics, Decidability, Complexity Proc. of 4th International Semantic Web Conference, LNCS vol. 3729 pp. 668–684, 2005 11 A  K a lou, T   Pom onis  D   K outs o m itr op oul os a n d T   Papatheodorou, “Intelligent Book Mashup: Using Semantic Web Ontologies and Rules for User Personalisation”, Proc of 4th IEEE Int. Conference on Semantic Computing, pp 536–541, 2010 12 B  M o tik e t a l    O W L 2 W e b O n tol o g y La ng ua g e P r of ile s   W3C Recommendation http://www.w3.org/TR/owl2profiles/, 2009 
96 


0     0     1 0     0     0 0     0     0 153 target e-shopper for the past nT ?  periods prior to time T are given, it is important for marketer how to predict e-shoppers purchase behavior at timeT For solving the above problem, the following measures are taken. First, transaction clustering is conducted, so that all the transactions of e-shoppers are clustered. The SOM technique is used to cluster target e-shoppers transactions Then it is necessary to detect the evolving e-shopper purchase sequences as time passes. These e-shopper behaviors, which are derived from a change in the cluster number of each e-shopper, are kept in the purchase sequence database. Finally sequential purchase patterns over user-specified minimum support and confidence are extracted by using the association rule. The sequential purchase patterns are then stored in the association rule database Although SOM technique can obtains transaction clusters SOM clustering technique often breaks down when handling very high-dimensional data. So it is proposed that using product classes represents the hierarchical relationships among products. The method can make an effective dimensionality reduction while improving clustering results Assume that a product class set P  is classified into n different subclasses, and that each subclass consists of subclasses at a lower level, or eventual leaf products, as follows PPPPP nn ,, 121 ?= "                          \(1 Suppose that C is the set of the transactions of m e-shoppers during s periods before timeT . More specifically letC be composed as follows CCC kTmkTkTC ???= ,,2,1 ,, "                   \(2 11,0 ?= sk "  2?s WhereC kTj ?, ?C  is a non-empty subset of products Each C kTj ?,  represents the product class or classes from which e-shopper j purchased products at time kT EveryC kTj ?,  is transformed into an input matrix composed of a bit vector, and the matrix to be transformed is used in the transaction clustering. The time-ordered vectors for a particular e-shopper represent the purchasing history of the 


e-shopper; this input matrix can be thought of as the dynamic profile of the e-shopper. A dynamic e-shopper profile is defined as follows Let C _ be a dynamic e-shopper profile. Then, C _ is defined by the following matrix for n product classes and m e-shoppers over the course of s periods  3 mj "2,1 11,0 ?= sk " 2?s Where                         1 if Pi?C kTj   All the transactions of e-shoppers in the training e-shopper purchase database are transformed into dynamic e-shopper profiles based on their prior purchase behaviors. Then we use the SOM clustering technique to assign each transaction to a group. This transaction clustering facilitates the discovery of the dynamic cluster sequence of e-shopper The transaction clustering results in the following set of q clusters DDDD q"21 ,=                             \(4 Where each Di is a subset of C _ the given in \(3 A rearrangement of these clusters by  e-shopper  and by time  period is necessary for the identification of the dynamic behavior of each e-shopper. It is possible to learn the cluster sequence of a e-shopper by identifying the cluster to which each transaction of the e-shopper belongs, during each time period. To formalize this concept, we use the following terminology Let BP j be the behavior pattern of e-shopper j . Then, the behavior pattern BPi is identical to the changes in the cluster number of e-shopper j during s periods and is defined as follows  5 Where D kTj ?, ?D ,11,0 ?= sk "  2?s The  process  of searching for  a behavior path can 


be simply conducted through transaction clustering. All e-shoppers  have  a behavior path based on their prior transactions. The association rule technique is well suited for determining the most frequent pattern with confidence, since it provides automatic filtering capabilities. To discover the behavior path of a target e-shopper at time T based on his/her past behavior, the input data should be divided into a conditional part and a consequential part. Association rules are descriptive patterns of the form X?Y, where X and Y are statements regarding the values of attributes of an instance in a database. X is termed the left-hand-side, and is the conditional part of an association rule. Meanwhile, Y is called the right-hand-side, and is the consequent part. The conditional part is composed of the left-hand-side assigned to the consequential part R j represents the association rule about the user specified minimum support and confidence in the following form R j 6 A rule R j  indicates that, if the path of a e-shopper is DD TjsTj 1,1, , ?+? " , then the behavior cluster for that e-shopper is D Tj, at timeT It is necessary to know the degree to which the behavior path of a target e-shopper during 1?s periods beforeT  is similar to the association rule. The cluster path of a target e-shopper, transformed via the SOM, is compared with the association rules derived from other e-shoppers paths, and then the best-matching path is determined. Execution of this 0 if Otherwise    C kTj 154 process requires new measures for calculating the degree of correspondence between the association rules and the behavior path of a target e-shopper. This similarity measure is defined as follows     


1 1  s i i kTj i j SSD                            \(7 Where        1 if RD kTikTi   Si kTj 0 otherwise mj "2,1 11,0 ?= sk " 2?s ni "2,1 The above definition indicates that, if the behavior path of a target e-shopper i is equal to the conditional part of association rule j in the same period, then S j kTi ?,  is equal to one, otherwise is equal to zero. However, even if the similarity measure is high, a choice of the association rule suited to the prediction of the cluster of a target e-shopper at timeT is difficult, since such a rule is not general, given that the support and confidence of the association rule may be remarkably low. Therefore, to assure a good fit between the behavior path of a target e-shopper and the conditional part of the association rule, it is necessary to measure fitness. Fitness is defined as follows Suppose FDij be a degree of the goodness-of-fit between the behavior path i and the association rule j . Then, FDij is defined as follows ConfidenceSupportSDFD jjjiij = ,             \(8 Using the above definition, we can determine the cluster of a target e-shopper at timeT is a consequential part R Tj, of the association rule j with maximum FDij IV.  ILLUSTRATIVE EXAMPLE In this paper, we use Table 3 as example given to illustrate proposed method. The set of product classes given in Table 3 is P={Candy, Can, Milk, Bread, Biscuit}. The transactions of e-shopper CID006 are C June,006 {Candy}, =C May,006 {Can}, =C July,006 {Milk 


Therefore, the dynamic purchase profile of CID006 buying the set of products {Candy, Can, Milk Bread, Biscuit} from May to July may be represented as { }0,0,0,0,1,006 _ C June , { }0,0,0,1,0,006 _ C May and 0,0,1,0,0,006 _ C July . CID016 and CID006 are exactly same as both bought the same products during the same month Therefore, similarly, the transactions of target e-shopper CID016 are =C June,016 {Candy} and =C May,016 {Can The dynamic purchase profile of CID016 buying the products may be represented as 0,0,0,0,1,016 _ C June , { }0,0,0,1,0,016 _ C May TABLE IV. BEHAVIOR PATH OF E-SHOPPER CID 1+? sT  1?T  T 006 007 008  015 016 10 10 3   10 3 1 10   3 9 


3 4  9 9  Suppose 3=s , according to formula \(5 of CID006, BP006  is{ }9,3,10 , as shown in table 4,  which indicates  that  e-shopper  CID006 belonged to the tenth cluster in May and moved into the third cluster in June thereafter reaching the ninth cluster in July According to formula \(6 from e-shoppers path with regard to a minimum support of 0.1 and a minimum confidence of 0.5. The association rules are as shown in table 5. The similarities  between the path of e-shopper CID016  and the derived rules are 22016 =SD and 11016 =SD . Therefore, e-shopper CID016 belongs to the ninth cluster at timeT , since the fitness between CID016 and the rules are =FD2016 0.2 and =FD 1 016 0.2001. Therefore, we predict that the products which e-shopper CID016 is likely to buy are Bread, and Biscuit  TABLE V. THE DERIVED ASSOCIATE RULES Rule 1+? sT  1?T  T  Support Confidence 1 2 3  15 16 10 10 3   10 3 1 10  


 3 9 3 4  9 9 0.3 0.1 0.1 0.2 0.1 0.1 1.0 1.0 1.0 0.5 1.0 1.0 V. CONCLUSION The preferences of e-shopper change over time. In this study, we describe a new approach for mining the changes of e-shopper  purchase behavior over time and discuss solutions to several problems. For predicting e-shoppers purchase behavior, the following concepts are proposed: BP j SDij and FDij . The SOM technique is used to detect the evolving e-shopper purchase sequences as time passes. The purchase sequences are derived from the changes in the cluster number of e-shopper. The sequential purchase patterns over user-specified minimum support and confidence are extracted by using the association rule. Then the sequential purchase patterns are stored in the rule database Finally, we give the example to elaborate the new methodology. The research presented in this paper makes a 155 contribution to mining  e-shoppers purchase behavior basing on transaction data. E-retailer may be able to perform effective  one-to-one marketing campaigns by providing individual target e-shoppers with personalized Product basing on using purchase sequences In the future, some possible extensions to this work are as 


follows. From the results of this study, we know which products target e-shoppers are likely to buy, but we have not yet explored the times at which these purchases are likely to occur. Further research analyzing e-shoppers past purchasing patterns should likewise enable prediction of the most appropriate times. Furthermore, one interesting research extension would be the setting up of a real marketing campaign, in which e-shoppers would be targeted using this methodology, which could then be evaluated with regard to its performance REFERENCES 1]Dhond, Gupta, A., Vadhavkar, S. Data mining techniques for optimizing inventories for electronic commerce[C]. In the Proceeding of the ACM-SIGKDD International Conference on Knowledge Discovery and Data Mining, 2005:480-486 2] Kuo, R. J., Chen, J. H., Hwang, Y. C. An intelligent stock trading decision support system through integration of genetic algorithm based fuzzy neural network and artificial neural network[J]. Fuzzy Sets and Systems, 2001 118\(1 3] Agrawal, D., Schorling, C. Market share forecasting: An empirical comparison of artificial neural networks and multinomial logist model Journal of Retailing[J]. 1997, 72\(4 4] Weigen, A. S., Rumelhart, D. E.Generalization by weight-elimination with application to forecasting. Advances in Neural Information Processing Systems[J]. 1999, 3:875882 5] Chen, M, S, Han, J. Data mining: an overview from a database perspective[J]. IEEE Transactions on Knowledge and Data Engineering, 2006 8\(6 6] Schafer, J. B., Konstan. E-commerce recommendation application[J Journal of Data Mining and Knowledge Discovery, 2001, 16:125153 7] Giudici, P, Passerone, G. Data mining of association structures to model e-shopper behavior. Computational Statistics and Data Analysis[J]. 2002 38:533541 8]Changchien, S. Mining association rules procedures to support on-line recommendation by e-shoppers and products fragmentation[J]. Expert Systems with Applications, 2001, 20\(4 9] Song, H, Kim, J. Mining the change of e-shopper behavior in an Internet shopping mall[J]. Expert System with Applications, 2001, 21\(3 10] Anand, S, Patrick, A. A data mining methodology for cross-sales[J Knowledge-Based Systems, 2006, 10:449-461 11] G. Adomavicius, A. Tuzbilin. Using data mining methods to build e-shopper profiles[J]. IEEE Computer, 2006, 34 \(2 


12] Dhond, Gupta, A., Vadhavkar, S. Data mining techniques for optimizing inventories for electronic commerce[C]. In the Proceeding of the ACM-SIGKDD International Conference on Knowledge Discovery and Data Mining, 2005:480-486 13]Chui-Yu Chiu , Yi-Feng Chen. An intelligent market segmentation system using k-means and particle swarm optimization[J]. Expert Systems with Applications, 2009, 36: 45584565 14]Tzung-Shi Chen , Shih-Chun Hsu. Mining frequent tree-like patterns in large datasets[J]. Data & Knowledge Engineering, 2007,62:6583 15]H. Tsukimoto, Extracting rules from trained neural networks[J]. IEEE Trans.Neural Networks, 2000, 11 \(2 156 


http://datamining.buaa.edu.cn/TopKCos.pdf 14] M. Zaki, Scalable algorithms for association mining, TKDE, vol. 12, pp. 372390, 2000 


enhance item-based collaborative filtering, in 2nd IASTED International Conference on Information and Knowledge Sharing, Scottsdale, Arizona, 2003 476 2010 10th International Conference on Intelligent Systems Design and Applications 


Basi Association Rles Basi c  Association R u les Association is basically connecting or tying up occurrences of Association is basically connecting or tying up occurrences of events Ol dib t f ilt t O n l y d escr ib e se t s o f s i mu lt aneous even t s Cannot describe patterns that iterate over time e g  itemset a  0  b  0  g    Eg If you sense higher data rates on the downlink than normal AND New Route generated Implies high chances of Intrusion AND New Route generated Implies high chances of Intrusion Associative IDS for NextGen Frameworks Dr S Dua LA Tech 20 


Enhanced Inte r transaction Association Rules Enhanced Inter transaction Association Rules Enhanced Inter transaction Association Rules Extension of association rules Conditional relationships at multiple different time steps e.g itemset a\(0 0 1 2 You sense Higher data rate than normal AND You see New Route g enerated AND 1 minute a g o you detected checksum gg error packets AND 2 minutes ago your encountered wrong checksum   Implies High Chance of Intrusion Enhanced Rules and Confidence Associative IDS for NextGen Frameworks Dr S Dua LA Tech 21 


Complex Spatio temporal Association Complex Spatio temporal Association Rules Further extension of inter transaction association rules Describe event durations e.g itemset a\(0,X j,Y k,Z Eg  You sense high data rates for X seconds AND new route generated j minutes ago task completed in Y AND new route generated j minutes ago task completed in Y seconds AND checksum error packets received k minutes ago for Z seconds High Chance of Intrusion With highest confidence level in association rules  association rules  Associative IDS for NextGen Frameworks Dr S Dua LA Tech 22 


DMITAR Al ith ARD DMITAR Al gor ith m  ARD Problem Domain Problem Statement and Challenges Aiti Miig bd IDS A ssoc i a ti ve Mi n i n g b ase d IDS  Introduction to data mining Association rule in data mining DMITAR Algorithm  ARD New research Associative IDS for NextGen Frameworks Dr S Dua LA Tech 23 


DMITAR Algorithm DMITAR Difference Matrix Based Inter Transaction Association Rule Miner developed in DMRL Uses vertical data format Differences of the transaction IDs are used to generate extended itemsets Windowless mechanism Associative IDS for NextGen Frameworks Dr S Dua LA Tech 24 


Deep into the Mechanism The DMITAR algorithm is based on lhilii comp l ex mat h emat i ca l assoc i at i ve formulation and proofs Four major parts Four major parts Frequent 1 itemset generation Frequent 2 itemset generation Frequent k itemset generation k>2 Spatio temporal rule formation Associative IDS for NextGen Frameworks Dr S Dua LA Tech 25 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


