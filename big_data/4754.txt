Optimal synaptic learning in non-linear associative memory Andre as Knoblauch Abstract 227 Neural associative memories are single layer perceptrons with fast synaptic learning typically storing discrete associations between pairs of neural activity patterns For linear learning such as employed in Hop\002eld-type networks it is well known that the so-called covariance rule is optimal resulting in minimal output noise and maximal storage capacity On the other hand numerical simulations suggest that nonlinear rules such as clipped Hebbian learning in Willshaw-type networks perform better at least for sparse neural activity and 002nite network size Here I show that the Willshaw and Hop\002eld models are only limit cases of a general optimal model where synaptic learning is determined by probabilistic Bayesian 
considerations Asymptotically for large networks and very sparse neuron activity the Bayesian model becomes identical to an inhibitory implementation of the Willshaw model Similarly for less sparse patterns the Bayesian model becomes identical to the Hop\002eld network employing the covariance rule For intermediate sparseness or 002nite networks the optimal Bayesian rule differs from both the Willshaw and Hop\002eld models and can signi\002cantly improve memory performance I I NTRODUCTION A neural associative memory is a single layer perceptron with fast typically 223one-shot\224 learning corresponding to the storage of M associations between pairs of binary memory vectors f  u 026  v 026   026  1   M g  They exist both in 
a hetero-associative feed-forward form storing the mapping between the address memories u 026 and the associated content memories v 026  but also in an auto-associative recurrent form where address and content memories are identical where typical tasks include denoising and pattern completion Neural associative memories have close relationships to Hebbian cell assemblies 1 a n d a r e w i d e l y u s e d i n n e u r o science as models of neural computation for various brain structures for example neocortex 2   3    4    5    6   hippocampus 7   8   p e r i r h i n a l c o r t e x  9   c e r e b e l l u m  1 0   11   1 2   a n d m u s h r o o m b o d y  1 3   I n a d d i t i o n  n e u r a l a s s o ciative memories are potentially useful in technical applications such as cluster analysis speech and object recognition or information retrieval in large databases 14   1 5    1 6   
17   1 8    1 9    2 0    2 1   Learning in neural associative memories is strongly constrained by the 223one-shot\224 property For example gradient descent methods such as error-backpropagation are not viable because they require repeated training of the whole pattern set Instead it is straight-forward to use simple Hebbianlike learning rules If during presentation of a single pattern pair both the presynaptic and postsynaptic neurons are active then the synaptic weight must be increased There have been numerous previous attempts to develop optimized learning Andreas Knoblauch is with the Honda Research Institute Europe CarlLegien-Strasse 30 D-63073 Offenbach/Main Germany phone 49 69 89011 750 email andreas.knoblauch@honda-ri.de models that maximize 223storage capacity\224 as de\002ned for 
example by the number of stored memories or the stored Shannon information per synapse e.g see 22   One of the earliest and simplest one-shot learning model is the so-called Steinbuch or Willshaw model with binary synapses and clipped Hebbian learning 23   2 4    2 5    2 6   27   2 8    2 9   H e r e a s i n g l e c o i n c i d e n c e o f p r e s y n a p t i c and postsynaptic activity increases the synaptic weight from 0 to 1 while additional coincidences do not cause further changes In contrast linear learning models of the Hop\002eldtype 30   3 1    3 2    3 3    3 4    3 5    3 6    3 7  a d d t h e contributions of each pattern pair For binary memory patterns the general linear learning rule can be described by four values 013  f  r  and 016 specifying the weight increments 
for the pre-/postsynaptic activations u 026 i v 026 j  0/0 0/1 1/0 and 1/1 A third model class is based on the 223Bayesian Con\002dence Propagation Neural Network\224 or BCPNN rule of Lansner and Ekeberg 38   3 9    4 0    4 1    4 2    4 3   6 a n d e m p l o y s B a y e s i a n m a x i m u m l i k e l i h o o d h e u r i s t i c s f o r synaptic learning and retrieval The 002rst two model classes are theoretically well investigated whereas the BCPNN-type models still lack a comparable consideration Surprisingly the maximal network storage capacity C in bits per synapse is almost identical for the binary Willshaw and linear Hop\002eld type models The Willshaw model can achieve up to C  0  69 bits per binary synapse whereas the linear learning model can 
achieve only a slightly higher capacity of C  0  72 bits per real-valued synapse However closer investigations reveal that the Willshaw model can achieve non-zero capacity only for very sparse patterns where the number of active units per pattern vector scales logarithmic with the vector size In contrast the linear model achieves the maximum C  0  72 for almost arbitrary sparseness Only for linearly or nonsparse patterns performance drops to the capacity of the original Hop\002eld network e.g C  0  14 bps 30   3 5  o r rather C  0  33 bps for 223hetero-associative\224 feed-forward networks considered here 44   I n a n y c a s e  t h e l i n e a r learning model achieves maximal storage capacity only for 
the optimal covariance learning rule e.g 33   3 5   w h i c h becomes equal to the Hebb rule for very sparse patterns and equal to the Hop\002eld rule for non-sparse patterns Moreover simulation experiments show that the capacity of the optimal linear model remains well below the capacity of the Willshaw model for any reasonable 002nite network size e.g C=0.2bps vs C=0.5bps for n  10 5 neurons 44   T h i s s u g g e s t s t h a t the linear covariance rule is not always optimal in particular not for 002nite networks and sparse memory representations as found in real brains 45  S i m i l a r l y  t h e B C P N N m o d e l has recently been shown to have a suboptimal performance 978-1-4244-8126-2/10/$26.00 ©2010 IEEE 


46   4 7   b a s i c a l l y e q u i v a l e n t t o t h e l i n e a r h o m o s y n a p t i c rul e which is well known to have a factor 1 000 p lower storage capacity than the optimal covariance rule here p is the fraction of active units in an address memory see rule R3 in 33 p 2 5 9   This paper develops the generally optimal associative memory that minimizes output noise and maximizes storage capacity by activating neurons based on Bayesian maximumlikelihood decisions The corresponding neural interpretation of the Bayesian associative memory corresponds in general to a novel non-linear learning rule A signal-to-noise analysis allows to compute the storage capacity and to compare optimal Bayesian learning to the previous model types Speci\002cally it turns out that the previous models are only special limit cases of the general optimal Bayesian model Asymptotically for large networks and very sparse memory patterns the Bayesian model becomes essentially identical to the binary Willshaw model but implemented with inhibitory rather than excitatory synapses 48   S i m i l a r l y  f o r l e s s sparse patterns the Bayesian model becomes identical to the linear model employing the optimal covariance rule For intermediate sparseness and 002nite networks the optimal Bayesian learning rule performs signi\002cantly better than the previous learning models II M EMORY STORAGE The task is to store M associations between address patterns u 026 and content patterns v 026 where 026  1    M  We assume that all memory patterns are binary vectors Address patterns u 026 have dimension m and content patterns v 026 dimension n  Then we assume that each address neuron i and each content neuron j can memorize its unit usage M 1  j    f 026  v 026 j  1 g 1 M 0  j    f 026  v 026 j  0 g  M 000 M 1  j  2 M 0 1  i    f 026  u 026 i  1 g 3 M 0 0  i    f 026  u 026 i  0 g  M 000 M 0 1  i   4 Similarly each synapse ij can memorize its synapse usage M 11  ij    f 026  u 026 i  1  v 026 j  1 g 5 M 01  ij    f 026  u 026 i  0  v 026 j  1 g  M 1 000 M 11 6 M 00  ij    f 026  u 026 i  0  v 026 j  0 g  M 0 0 000 M 01 7 M 10  ij    f 026  u 026 i  1  v 026 j  0 g  M 0 000 M 00 8 for i  1      m and j  1      n  Note that it is suf\002cient to memorize M  M 1  M 0 1  and M 11  Thus an implementation on a digital computer requires only about  mn  m  n  1\ld M memory bits Note also that the synaptic weights of the Willshaw network and the linear models can be expressed in terms of the synapse usages The weights of the Willshaw network are w ij  1 if M 11 025 1 and w ij  0 otherwise The weights of the linear models are w ij  013M 00  fM 01  rM 10  016M 11  III O PTIMAL RETRIEVAL Given a query pattern u the memory task is to 002nd the 223most similar\224 address pattern u 026 and return a reconstruction v of the associated content v 026  In general the query u is a noisy version of one of the address patterns u 026  For clarity the following analysis considers only zero query noise assuming u  u 026  For the general case see a technical report 44  Now the content neurons j have to decide independently of each other whether to be activated or to remain silent Given the query u  the optimal maximum-likelihood decision  v j   1  pr v 026 j 1 j u  pr v 026 j 0 j u  025 1 0  o therwise 9 minimizes the expected Hamming distance d H  v 026  v   P n j 1 j v 026 j 000  v j j between original and reconstructed content If the query pattern components are conditional independent given the activity of content neuron j  e.g assuming independently generated address pattern components we have for a 2 f 0  1 g pr u j v 026 j  a   m Y i 1 pr u i j v 026 j  a   m Y i 1 M  u i a  ij  M a  j   10  With pr v 026 j  a j u   p r   u j v 026 j  a  p r  v 026 j  a   pr u  the Bayes formula we obtain pr v 026 j  1 j u  pr v 026 j  0 j u   022 M 0  j  M 1  j  023 m 000 1 m Y i 1 M  u i 1  ij  M  u i 0  ij   1 1 For a neural formulation we can take logarithms of the probabilities and obtain synaptic weights w ij and dendritic potentials x j  log\(pr v 026 j  1 j u   pr v 026 j  0 j u    w ij  log M 11 M 00 M 10 M 01  12 x j   m 000 1 log M 0 M 1  m X i 1 lo g M 01 M 00  m X i 0 w i j  u i 13 such that pr v 026 j  1 j u   1  1  e 000 x j  writes as a sigmoid function of x j  and a content neuron 002res  v j  1  iff the dendritic potential is non-negative x j 025 0  Note that indices of M 0  j   M 1  j   M 00  ij   M 01  ij   M 10  ij   and M 11  ij  are skipped for brevity Thus the generally optimal Bayesian learning rule is non-linear and differs from both the Willshaw and linear models Evaluating eq 13 is much cheaper than eq 11 in particular for sparse queries having only a small number of active components with  u i  1  However the synaptic weights eq 12 may not yet satisfy Dale's law that a neuron is either excitatory or inhibitory To be more consistent with biology we may add a suf\002ciently large constant c  000 min ij w ij to each weight Then all synapses have non-negative weights w 0 ij  w ij  c and the dendritic potentials remain unchanged if we replace the last sum in eq 13 by m X i 0 w ij  u i  m X i 0 w 0 ij  u i 000 c m X i 0  u i  14 


Here the negative sum could be realized for example by fee dforward inhibition with a strength proportional to the query pattern activity e.g 49   IV A NALYSIS OF SIGNAL TO NOISE RATIO AND STORAGE CAPACITY To build a memory system with high retrieval quality we have to minimize the expected Hamming distance d H  v 026  v   The expected Hamming distance E  d H   nqp 10  n 1 000 q  p 01 can be computed from the component output error probabilities p 01  pr v j  1 j v 026 j  0  p r  x j 025 002 j j v 026 j  0 15 p 10  pr v j  0 j v 026 j  1  p r  x j  002 j j v 026 j  1 16 where nq is the mean activity of a content pattern and 002 j is the 002ring threshold e.g 002 j  0 for dendritic potentials x j as in eq 13 Intuitively retrieval quality will be high if the 223high potential distribution\224 pr x j j v 026 j  1 and the 223low potential distribution\224 pr x j j v 026 j  0 are well separated i.e if the signal-to-noise ratio SNR r  SNR 026 lo  033 lo  026 hi  033 hi   026 hi 000 026 lo max 033 lo  033 h i  17 is large see 31   3 3    3 5    3 7    H e r e 026 lo  E  x j j v 026 j  0 and 033 2 lo  Var x j j v 026 j  0 are expectation and variance of the low-potential distribution and 026 hi  E  x j j v 026 j  1 and 033 2 hi  Var x j j v 026 j  1 are expectation and variance of the high-potential distribution In the following we compute the SNR assuming that each address unit is one with the same probability p  pr u 026 i  1 independently of other units For the content patterns it is suf\002cient to assume q  pr v 026 j  1  Then let u  u 026 be a noise-free query pattern having k one-entries Without loss of generality we assume further that  u i is one for i  1      k and zero for i  k  1      m  Equivalently to eq 13 but replacing m 000 1 by m for brevity a content neuron j will be activated if the dendritic potential x j exceeds the threshold 002 j  log M 0 M 1  instead of 002 j  0  where x j  m log M 0 M 1  k X i 1 lo g M 11 M 10  m X i  k 1 l og M 01 M 00 1 8  m log M 0 M 1  k X i 1 lo g M 11 M 0 000 M 00  m X i  k 1 l og M 1 000 M 11 M 00  1 9 For given M 1  M 0 the remaining variables are binomially distributed M 00 030 B M 0  1 000 p and M 11 030 B M 1 p  where pr B N;P  z   000 N z 001 P z 1 000 P  N 000 z  For large N P 1 000 P  the binomial B N;P can be approximated by a Gaussian G 026;\033 with mean 026  N P and variance 033 2  N P 1 000 P   Given u 026 i and v 026 j we have then M 11 030 8           G M 1 p p M 1 p 1 000 p  u 026 i  v 026 j  0  0 G M 1 p p M 1 p 1 000 p  u 026 i  v 026 j  1  0 G  M 1 000 1 p p  M 1 000 1 p 1 000 p   u 026 i v 026 j  0  1 G 1 M 1 000 1 p p  M 1 000 1 p 1 000 p   u 026 i v 026 j  1  1 20 M 00 030 8           G 1 M 0 000 1\\(1 000 p   p  M 0 000 1 p 1 000 p   u 026 i v 026 j  0  0 G  M 0 000 1\\(1 000 p   p  M 0 000 1 p 1 000 p   u 026 i v 026 j  1  0 G M 0 1 000 p   p M 0 p 1 000 p  u 026 i  v 026 j  0  1 G M 0 1 000 p   p M 0 p 1 000 p  u 026 i  v 026 j  1  1 21 From this we can approximate the distribution of the potential x j for low units and high units respectively For large k and m 000 k the sums of logarithms are approximately Gaussian distributed For Gaussian random variables X 030 G 026;\033 it is E log X   log 026 000 1 X i 1 1 001 3 001 001 001 2 i 000 1 2 i  033=\026  2 i 031 log 026 22 Var\(log X  031  033 026  2 23  which can be proved using the Newton-Mercator series for a detailed proof see 44  a p p  C   H e r e a p p r o x i m a t i o n s a r e valid for 033 034 026 and tight for 033=\026  0 if 026 6 1  From this we can compute the exact mean dendritic potentials 026 lo  026 hi and variances 033 2 lo  033 2 hi for low-units and high-units Fortunately it turns out that the mean potential difference 001 026  026 hi 000 026 lo can be well approximated by using only the 002rst order term in eq 22 while all higherorder terms become virtually identical for 026 hi and 026 lo  for more details see 44  a p p s  D  F   T h e n t h e 002 r s t o r d e r approximations 026 0 lo  026 0 hi of 026 lo  026 hi are 026 0 lo  m log M 0 M 1  k log M 1 p M 0 000  M 0 000 1 1 000 p   m 000 k  log M 1 000 M 1 p 1   M 0 000 1 1 000 p   000 k log\(1  1 000 p M 0 p  000  m 000 k  lo g\(1  p M 0 1 000 p   031 000 k  1 000 p  M 0 p 000  m 000 k  p M 0 1 000 p   2 4 


026 0 hi  m lo g M 0 M 1  k l og 1   M 1 000 1 p M 0 000 M 0 1 000 p   m 000 k  log M 1 000  M 1 000 1 p M 0 1 000 p   l og 024\022  M 1 000 1 p  1 M 1 p 023 k 002 022 M 1 1 000 p   p M 1 1 000 p  023 m 000 k 025 031 k 1 000 p  M 1 p   m 000 k  p M 1 1 000 p  2 5 where the approximations are valid for large M 0 p M 1 p  1  Therefore the mean difference 001 026 between the highand low-distributions is 001 026  026 hi 000 026 lo 031 026 0 hi 000 026 0 lo 031 022 k 1 000 p  p   m 000 k  p 1 000 p 023 022 1 M 1  1 M 0 023 26  In order to get the signal-to-noise ratio eq 17 we still have to compute the variances 033 2 lo and 033 2 hi for x j in eq 19 Given the unit usages M 1  the random variables M 00  i j  and M 11  i j  are independent and thus the variances simply add Because each variance summand is positive for large M 1 p M 0 p  1 we can simply assume M 11 030 G M 1 p p M 1 p 1 000 p  an d M 00 030 G M 0 1 000 p   p M 0 p 1 000 p  in all cases cf eqs 20,21 With eq 23 we get Var\(log M 11  031 1 000 p M 1 p 27  Var\(log M 0 000 M 00  031 1 000 p M 0 p 28  Var\(log M 1 000 M 11  031 p M 1 1 000 p  2 9 Var\(log M 00  031 p M 0 1 000 p   3 0 Thus the variances 033 2  Var x j  031 033 2 lo 031 033 2 hi for the potentials of both low-units and high units are approximately 033 2 031 k Var\(log M 11   Var\(log M 0 000 M 00   m 000 k Var\(log M 1 000 M 11   Var\(log M 00  031 k 1 000 p  p 022 1 M 1  1 M 0 023   m 000 k  p 1 000 p 022 1 M 1  1 M 0 023  022 k 1 000 p  p   m 000 k  p 1 000 p 023 022 1 M 1  1 M 0 023 031 001 026 31  Therefore the signal-to-noise-ratio r  001 026=\033 eq 17 is given by r 2 031 001 026 031 033 2 031 022 k 1 000 p  p   m 000 k  p 1 000 p 023 022 1 M 1  1 M 0 023 031 m M q 1 000 q   3 2 where the last approximation is true because for large M M 1  M 0  k  1 the unit usage and pattern activity will be close to the mean values M 1 031 M q  M 0 031 M 1 000 q   and k 031 pm  In summary for M pq  1 the SNR r 031 m  M q 1 000 q  of optimal Bayesian learning is identical to the asymptotic SNR of linear learning with the optimal covariance rule e.g see 032 Covariance 3 in 33 p 2 5 9  o r e q  3  2 8 i n  3 5  p 9 5   Since the network storage capacity C can be written as a function of the SNR r e.g see 35   4 4    f o r M pq  1 the Bayesian learning model has also the same asymptotic network storage capacity as the linear covariance rule C  0  72 bps V R ELATION TO LINEAR LEARNING MODELS AND THE COVARIANCE RULE In general the synaptic weights of the Bayesian associative network eq 12 are a non-linear function of presynaptic and postsynaptic activity In the following we show that under some conditions the optimal Bayesian rule can be approximated by a linear learning rule For large networks the synapse usages will be close to its expectations i.e M 00 031 M 0 1 000 p   M 01 031 M 1 1 000 p   M 10 031 M 0 p  and M 11 031 M 1 p  In fact these approximations will make only a negligible relative error if the standard deviations are small compared to the expectations The most critical variable is M 11 having expectation M 1 p and standard deviation M 1 p 1 000 p   Thus the approximations are valid for M 1 p  1  or M pq  1  if we assume M 1 031 M q  Then the argument of the logarithm in eq 12 will be close to one A linear approximation of the logarithm around one yields w ij 031 f  M 00  M 01  M 10  M 11   M 11 M 00 M 10 M 01 000 1   33 Similarly the resulting function f can be linearized around the expectations of the synapse usages The partial derivatives are f M 00 j M xy  E  M x y   M 11 M 10 M 01 j M x y  E  M xy   1 M 0 1 000 p  3 4 f M 01 j M xy  E  M x y   000 M 11 M 00 M 10 M 2 01 j M x y  E  M xy   000 1 M 1 1 000 p  3 5 f M 10 j M xy  E  M x y   000 M 11 M 00 M 2 10 M 01 j M x y  E  M xy   000 1 M 0 p 36  f M 11 j M x y  E  M x y   M 00 M 10 M 0 1 j M x y  E  M xy   1 M 1 p 3 7  


                    4                                                       7                                                       10                                     22                   25                                     32                                     50                     10 5                   10 4                   10 3                   10 2                   10 1                   10 0    pattern activity k \(=l output noise e m=n=100     4                                                                                                                               12                                                                                                                               71                                                       100                                     292                                                                         595                                                                                                             1250                                     2500   10 7                   10 6                   10 5                   10 4                   10 3                   10 2                   10 1                   10 0    m=n=5000 pattern activity k \(=l output noise e L-Hebb L-Cov Willshaw Bayes L-Hebb L-Cov Willshaw Bayes Fig 1 Experimental retrieval output noise 017  d H  v 026  v   l as function of pattern activity k  mp  for the optimal Bayesian rule cyan solid Willshaw rule black dashed linear covariance rule red solid and the linear Hebb rule blue dashed Here 017 is de\002ned as the expected Hamming distance between retrieval output v and original content v 026 normalized by the mean content pattern activity l  nq  Queries u contained half the one-entries of the original address patterns u 026  For each data point the number of stored memories M  has been chosen maximal such that the Willshaw model does not exceed 017  0  01 data taken from 29   L e f t p a n e l  S m a l l n e t w o r k s w i t h m  n  100 and M  7  26  20  11  10  7  4 for k  l  4  7  10  22  25  32  50  Right panel Larger networks with m  n  5000 and M  3985  31481  7082  736  202  49  12 for k  l  4  12  71  292  595  1250  2500  Each data point averages over 10000 retrievals in 100 different networks For q  M 1 M the linear approximation writes 002nally w ij 031 M 00 M 1 000 p 1 000 q  000 M 0 1 M 1 000 p  q 000 M 10 M p 1 000 q   M 11 M pq 38  or for 021  1   M pq 1 000 p 1 000 q  w ij 021  pqM 0 0 000 p 1 000 q  M 01 000 1 000 p  qM 10  1 000 p 1 000 q  M 11  39 This is essentially up to factor 021  the covariance rule as discussed in many previous works e.g see 50   5 1   32   5 2    3 3    5 3    3 4    3 5    3 6    3 7    T h u s  i n t h e asymptotic limit M pq  1 optimal Bayesian learning becomes equivalent to the covariance rule of linear learning models see Fig 1 VI R ELATION TO THE W ILLSHAW MODEL AND INHIBITORY ASSOCIATIVE MEMORY The Willshaw or Steinbuch model is one of the simplest models for distributed associative memory employing synapses with binary weights w ij  min\(1  M 11  ij  2 f 0  1 g  40 The dendritic potentials of the content neurons are simply x j  P m i 0 w ij  u i  The Willshaw model works particular good for 223pattern part retrieval\224 i.e if the query pattern  u contains a subset of the one-entries of an address pattern u 026  Then the optimal threshold is maximal i.e equal to the query pattern activity 002 j  P m i 0  u i  This implies that a single 223missing\224 input i.e  u i  1 but w ij  0  excludes activation of content neuron j  This observation suggests that the Willshaw model should be interpreted as an essentially inhibitory network where zero weights become negative active weights zero and the optimal threshold zero In particular for a diluted network with low connectivity the inhibitory interpretation of the Willshaw model with optimal threshold control seems to be much more realistic than an excitatory interpretation e.g compare 48 v s   5 4    Under some conditions the optimal Bayesian model behaves quite similar Obviously the synaptic weight eq 12 becomes plus or minus in\002nity if one of the synapse usages M xy is zero Since memory patterns are typically sparse it is most likely that only the synapse usage M 11 030 B M;pq remains zero Then the optimal synaptic weight is w ij  0001 such that similar as for the Willshaw model with maximal threshold a single inhibitory input  u i  1 and w ij  0001  can silence the postsynaptic content neuron j  In fact for suf\002ciently sparse memory patterns with M pq 034 1 but still M p  1 and M q  1 the synapse usages M 00 031 M 1 000 p 1 000 q   M 01 031 M 1 000 p  q  and M 10 031 M p 1 000 q  will be close to their large mean values whereas the synapse usage M 11 remains small and occasionally even may assume zero Thus the synaptic weights up to an essentially constant offset are dominated by the in\002nitely negative weights due to the M 11  0 events Therefore in such a regime the Bayesian model becomes equivalent to the Willshaw model Fig 1 Note also that this regime offers novel functional interpretations for strongly inhibitory circuits for example involving basket or chandelier cells 55   4 8   


VII C ONCLUSIONS Th is paper develops the optimal neural associative memory based on Bayesian maximum-likelihood considerations assuming local synaptic learning section II and independent address attributes 56   5 7   I n g e n e r a l  t h e r e s u l t i n g optimal synaptic learning rule is non-linear and differs from previously investigated linear learning models of the Hop\002eld type 30   5 8    3 2    3 3    5 3    3 4    3 5    3 6    3 7   s i m p l e non-linear learning models of the Willshaw type 24   2 3   25   5 9    2 6    2 8    1 8    2 9   a n d B C P N N t y p e m o d e l s employing suboptimal Bayesian heuristics 38   3 9    4 0   41   4 2    4 3    6   The previous models become optimal only in the asymptotic limit of many stored memories M  1  depending on the pattern activity parameters p and q  In the limit M pq  1 of moderately sparse and non-sparse memory patterns the optimal Bayesian model becomes equivalent to the linear network employing the covariance rule and thus achieves a maximal network storage capacity of C  0  72 bits per synapse In the limit M pq 034 1 of very sparse patterns the optimal Bayesian model becomes equivalent to the Willshaw network and thus achieves C  0  69 bits per synapse For a large range of intermediate sparseness and 002nite network size the optimal Bayesian model can perform signi\002cantly better than the previous models These theoretical results have been veri\002ed by numerical experiments illustrated by Fig 1 see also a related technical report 44   4 7    Note that this paper shows that even the best method of Hebbian-type synaptic plasticity modifying synaptic weights based on presynaptic and postsynaptic activity can store signi\002cantly less than one bit per real-valued synapse By contrast we have shown elsewhere that storing memories in similar network models by structural plasticity meaning the elimination and generation of synapses 60  y i e l d s a m u c h higher diverging synaptic capacity where the information that a binary synapse can store scales with the logarithm of the network size n 61   6 2    6 3    6 4    2 9    6 5   A CKNOWLEDGMENT The author is grateful to Julian Eggert Marc-Oliver Gewaltig and Edgar K\250orner for providing the opportunity to do this work at the Honda Research Institute He is also grateful to them and to Ursula K\250orner Anders Lansner G\250unther Palm and Friedrich Sommer for helpful discussions and comments R EFERENCES 1 D  H e b b  The organization of behavior A neuropsychological theory New York Wiley 1949 2 V  B r a i t e n b e r g  223 C e l l a s s e m b l i e s i n t h e c e r e b r a l c o r t e x  224 i n Lecture notes in biomathematics 21 Theoretical approaches to complex systems  R Heim and G Palm Eds Berlin Heidelberg New York Springer-Verlag 1978 pp 171\226188 3 G  P a l m  Neural Assemblies An Alternative Approach to Arti\002cial Intelligence Berlin Springer 1982 4 227 227  223 C e l l a s s e m b l i e s a s a g u i d e l i n e f o r b r a i n r e s e a r c h  224 Concepts in Neuroscience  vol 1 pp 133\226148 1990 5 E  F r a n s e n a n d A  L a n s n e r  223 A m o d e l o f c o r t i c a l a s s o c i a t i v e m e m o r y based on a horizontal network of connected columns.\224 Network Computation in Neural Systems  vol 9 pp 235\226264 1998 6 A  L a n s n e r  223 A s s o c i a t i v e m e m o r y m o d e l s  f r o m t h e c e l l a s s e m b l y theory to biophysically detailed cortex simulations.\224 Trends in Neurosciences  vol 32\(3 pp 178\226186 2009 7 D  M a r r  223 S i m p l e m e m o r y  a t h e o r y f o r a r c h i c o r t e x  224 Philosophical Transactions of the Royal Society of London Series B  vol 262 pp 24\22681 1971 8 E  R o l l s  223 A t h e o r y o f h i p p o c a m p a l f u n c t i o n i n m e m o r y  224 Hippocampus  vol 6 pp 601\226620 1996 9 R  B o g a c z  M  B r o w n  a n d C  G i r a u d C a r r i e r  223 M o d e l o f f a m i l i a r i t y discrimination in the perirhinal cortex.\224 Journal of Computational Neuroscience  vol 10 pp 5\22623 2001 10 D  M a r r  223 A t h e o r y o f c e r e b e l l a r c o r t e x  224 Journal of Physiology  vol 202\(2 pp 437\226470 1969 11 J  A l b u s  223 A t h e o r y o f c e r e b e l l a r f u n c t i o n  224 Mathematical Biosciences  vol 10 pp 25\22661 1971 12 P  K a n e r v a  Sparse Distributed Memory Cambridge MA MIT Press 1988 13 G  L a u r e n t  223 O l f a c t o r y n e t w o r k d y n a m i c s a n d t h e c o d i n g o f m u l t i d i mensional signals.\224 Nature Reviews Neuroscience  vol 3 pp 884\226895 2002 14 T  K o h o n e n  Associative memory a system theoretic approach Berlin Springer 1977 15 H  B e n t z  M  H a g s t r o e m  a n d G  P a l m  223 I n f o r m a t i o n s t o r a g e a n d effective data retrieval in sparse matrices.\224 Neural Networks  vol 2 pp 289\226293 1989 16 R  P r a g e r a n d F  F a l l s i d e  223 T h e m o d i 002 e d K a n e r v a m o d e l f o r a u t o m a t i c speech recognition.\224 Computer Speech and Language  vol 3 pp 61\226 81 1989 17 D  G r e e n e  M  P a r n a s  a n d F  Y a o  223 M u l t i i n d e x h a s h i n g f o r i n f o r mation retrieval.\224 Proceedings of the 35th Annual Symposium on Foundations of Computer Science  pp 722\226731 1994 18 A  K n o b l a u c h  223 N e u r a l a s s o c i a t i v e m e m o r y f o r b r a i n m o d e l i n g a n d information retrieval.\224 Information Processing Letters  vol 95 pp 537\226544 2005 19 X  M u  M  A r t i k l a r  P  W a t t a  a n d M  H a s s o u n  223 A n R C E b a s e d associative memory with application to human face recognition.\224 Neural Processing Letters  vol 23 pp 257\226271 2006 20 A  W i c h e r t  223 C e l l a s s e m b l i e s f o r d i a g n o s t i c p r o b l e m s o l v i n g  224 Neurocomputing  vol 69 pp 810\226824 2006 21 M  R e h n a n d F  S o m m e r  223 S t o r i n g a n d r e s t o r i n g v i s u a l i n p u t w i t h collaborative rank coding and associative memory.\224 Neurocomputing  vol 69 pp 1219\2261223 2006 22 G  P a l m  223 M e m o r y c a p a c i t i e s o f l o c a l r u l e s f o r s y n a p t i c m o d i 002 c a t i o n  A comparative review.\224 Concepts in Neuroscience  vol 2 pp 97\226128 1991 23 K  S t e i n b u c h  223 D i e L e r n m a t r i x  224 Kybernetik  vol 1 pp 36\22645 1961 24 D  W i l l s h a w  O  B u n e m a n  a n d H  L o n g u e t H i g g i n s  223 N o n h o l o g r a p h i c associative memory.\224 Nature  vol 222 pp 960\226962 1969 25 G  P a l m  223 O n a s s o c i a t i v e m e m o r i e s  224 Biological Cybernetics  vol 36 pp 19\22631 1980 26 J  P  N a d a l  223 A s s o c i a t i v e m e m o r y  o n t h e  p u z z l i n g  s p a r s e c o d i n g limit.\224 J.Phys A Math Gen  vol 24 pp 1093\2261101 1991 27 F  S o m m e r a n d P  D a y a n  223 B a y e s i a n r e t r i e v a l i n a s s o c i a t i v e m e m o r i e s with storage errors.\224 IEEE Transactions on Neural Networks  vol 9 pp 705\226713 1998 28 F  S o m m e r a n d G  P a l m  223 I m p r o v e d b i d i r e c t i o n a l r e t r i e v a l o f s p a r s e patterns stored by Hebbian learning.\224 Neural Networks  vol 12 pp 281\226297 1999 29 A  K n o b l a u c h  G  P a l m  a n d F  S o m m e r  223 M e m o r y c a p a c i t i e s f o r synaptic and structural plasticity.\224 Neural Computation  vol 22\(2 pp 289\226341 2010 30 J  H o p 002 e l d  223 N e u r a l n e t w o r k s a n d p h y s i c a l s y s t e m s w i t h e m e r g e n t c o l lective computational abilities.\224 Proceedings of the National Academy of Science USA  vol 79 pp 2554\2262558 1982 31 G  P a l m  223 O n t h e a s y m p t o t i c i n f o r m a t i o n s t o r a g e c a p a c i t y o f n e u r a l networks.\224 in Neural Computers  ser NATO ASI Series F41 R Eckmiller and C von der Malsburg Eds Berlin Heidelberg New York Springer Verlag 1988 pp 271\226280 32 M  T s o d y k s a n d M  F e i g e l  m a n  223 T h e e n h a n c e d s t o r a g e c a p a c i t y i n neural networks with low activity level.\224 Europhysics Letters  vol 6 pp 101\226105 1988 33 P  D a y a n a n d D  W i l l s h a w  223 O p t i m i s i n g s y n a p t i c l e a r n i n g r u l e s i n linear associative memory.\224 Biological Cybernetics  vol 65 pp 253\226 265 1991 


34 P  D a y a n a n d T  S e j n o w s k i  223 T h e v a r i a n c e o f c o v a r i a n c e r u l e s for associative matrix memories and reinforcement learning.\224 Neural Computation  vol 5 pp 205\226209 1993 35 G  P a l m a n d F  S o m m e r  223 A s s o c i a t i v e d a t a s t o r a g e a n d r e t r i e v a l i n neural nets.\224 in Models of Neural Networks III  E Domany J van Hemmen and K Schulten Eds New York Springer-Verlag 1996 pp 79\226118 36 G  C h e c h i k  I  M e i l i j s o n  a n d E  R u p p i n  223 E f f e c t i v e n e u r o n a l l e a r n i n g with ineffective hebbian learning rules.\224 Neural Computation  vol 13 pp 817\226840 2001 37 D  S t e r r a t t a n d D  W i l l s h a w  223 I n h o m o g e n e i t i e s i n h e t e r o a s s o c i a t i v e memories with linear learning rules.\224 Neural Computation  vol 20 pp 311\226344 2008 38 A  L a n s n e r a n d O  E k e b e r g  223 A n a s s o c i a t i v e n e t w o r k s o l v i n g t h e 224 4 bit adder problem\224.\224 in Proceedings of the IEEE First International Conference on Neural Networks  M Caudill and C Butler Eds San Diego CA 1987 pp II\226549 39 227 227  223 A o n e l a y e r f e e d b a c k a r t i 002 c i a l n e u r a l n e t w o r k w i t h a B a y e s i a n learning rule.\224 International Journal of Neural Systems  vol 1\(1 pp 77\22687 1989 40 I  K o n o n e n k o  223 B a y e s i a n n e u r a l n e t w o r k s  224 Biological Cybernetics  vol 61\(5 pp 361\226370 1989 41 227 227  223 O n B a y e s i a n n e u r a l n e t w o r k s  224 Informatica Slovenia  vol 18\(2 pp 183\226195 1994 42 A  L a n s n e r a n d A  H o l s t  223 A h i g h e r o r d e r B a y e s i a n n e u r a l n e t w o r k with spiking units.\224 International Journal of Neural Systems  vol 7\(2 pp 115\226128 1996 43 A  S a n d b e r g  A  L a n s n e r  K  P e t e r s s o n  a n d O  E k e b e r g  223 A p a l i m p s e s t memory based on an incremental Bayesian learning rule.\224 Neurocomputing  vol 32-33 pp 987\226994 2000 44 A  K n o b l a u c h  223 N e u r a l a s s o c i a t i v e n e t w o r k s w i t h o p t i m a l b a y e s i a n learning.\224 Honda Research Institute Europe GmbH D-63073 Offenbach/Main Germany HRI-EU Report 09-02 May 2009 45 S  W a y d o  A  K r a s k o v  R  Q u i r o g a  I  F r i e d  a n d C  K o c h  223 S p a r s e representation in the human medial temporal lobe.\224 Journal of Neuroscience  vol 26\(40 pp 10 232\22610 234 2006 46 A  K n o b l a u c h  223 C o m p a r i s o n o f t h e l a n s n e r  e k e b e r g r u l e t o o p t i m a l bayesian learning in neural associative memory.\224 Honda Research Institute Europe GmbH D-63073 Offenbach/Main Germany HRI-EU Report 10-06 April 2010 47 227 227  223 N e u r a l a s s o c i a t i v e m e m o r y w i t h o p t i m a l b a y e s i a n l e a r n i n g  224 submitted  pp 226 2010 48 227 227  223 O n t h e c o m p u t a t i o n a l b e n e 002 t s o f i n h i b i t o r y n e u r a l a s s o c i a t i v e networks.\224 Honda Research Institute Europe GmbH D-63073 Offenbach/Main Germany HRI-EU Report 07-05 May 2007 49 A  K n o b l a u c h a n d G  P a l m  223 P a t t e r n s e p a r a t i o n a n d s y n c h r o n i z a t i o n in spiking associative memories and visual areas.\224 Neural Networks  vol 14 pp 763\226780 2001 50 T  S e j n o w s k i  223 S t o r i n g c o v a r i a n c e w i t h n o n l i n e a r l y i n t e r a c t i n g n e u rons.\224 Journal of Mathematical Biology  vol 4 pp 303\226321 1977 51 227 227  223 S t a t i s t i c a l c o n s t r a i n t s o n s y n a p t i c p l a s t i c i t y  224 Journal of Theoretical Biology  vol 69 pp 385\226389 1977 52 D  W i l l s h a w a n d P  D a y a n  223 O p t i m a l p l a s t i c i t y i n m a t r i x m e m o r i e s  what goes up must come down.\224 Neural Computation  vol 2 pp 85\226 93 1990 53 G  P a l m a n d F  S o m m e r  223 I n f o r m a t i o n c a p a c i t y i n r e c u r r e n t McCulloch-Pitts networks with sparsely coded memory states.\224 Network  vol 3 pp 177\226186 1992 54 B  G r a h a m a n d D  W i l l s h a w  223 I m p r o v i n g r e c a l l f r o m a n a s s o c i a t i v e memory.\224 Biological Cybernetics  vol 72 pp 337\226346 1995 55 H  M a r k r a m  M  T o l e d o R o d r i g u e z  Y  W a n g  A  G u p t a  G  S i l b e r b e r g  and C Wu 223Interneurons of the neocortical inhibitory system.\224 Nature Reviews Neuroscience  vol 5 pp 793\226807 2004 56 H  Z h a n g  223 T h e o p t i m a l i t y o f n a i v e b a y e s  224 i n Proceedings of the 17th Florida Arti\002cial Intelligence Research Society Conference  V Barr and Z Markov Eds AAAI Press 2004 pp 562\226567 57 P  D o m i n g o s a n d M  P a z z a n i  223 O n t h e o p t i m a l i t y o f t h e s i m p l e Bayesian classi\002er under zero-one loss.\224 Machine Learning  vol 29 pp 103\226130 1997 58 G  P a l m  223 L o c a l s y n a p t i c r u l e s w i t h m a x i m a l i n f o r m a t i o n s t o r a g e capacity.\224 in Neural and synergetic computers  ser Springer Series in Synergetics H Haken Ed Berlin Heidelberg New York Springer Verlag 1988 vol 42 pp 100\226110 59 D  G o l o m b  N  R u b i n  a n d H  S o m p o l i n s k y  223 W i l l s h a w m o d e l  A s s o ciative memory with sparse coding and low 002ring rates.\224 Phys Rev A  vol 41 pp 1843\2261854 1990 60 A  H o l t m a a t a n d K  S v o b o d a  223 E x p e r i e n c e d e p e n d e n t s t r u c t u r a l s y n a p tic plasticity in the mammalian brain.\224 Nature Reviews Neuroscience  vol 10 pp 647\226658 2009 61 A  K n o b l a u c h  223 S y n c h r o n i z a t i o n a n d p a t t e r n s e p a r a t i o n i n s p i k i n g associative memory and visual cortical areas.\224 PhD thesis Department of Neural Information Processing University of Ulm Germany  2003 62 227 227  223 O n c o m p r e s s i n g t h e m e m o r y s t r u c t u r e s o f b i n a r y n e u r a l a s s o ciative networks,\224 Honda Research Institute Europe GmbH D-63073 Offenbach/Main Germany HRI-EU Report 06-02 April 2006 63 227 227  223 N e u r a l a s s o c i a t i v e m e m o r y a n d t h e W i l l s h a w P a l m p r o b a b i l i t y distribution.\224 SIAM Journal on Applied Mathematics  vol 69\(1 pp 169\226196 2008 64 227 227  223 T h e r o l e o f s t r u c t u r a l p l a s t i c i t y a n d s y n a p t i c c o n s o l i d a t i o n f o r memory and amnesia in a model of cortico-hippocampal interplay.\224 in Connectionist Models of Behavior and Cognition II Proceedings of the 11th Neural Computation and Psychology Workshop  J Mayor N Ruh and K Plunkett Eds Singapore World Scienti\002c Publishing 2009 pp 79\22690 65 227 227  223 Z i p n e t s  E f 002 c i e n t a s s o c i a t i v e c o m p u t a t i o n w i t h b i n a r y synapses.\224 in Proceedings of the International Joint Conference on Neural Networks IJCNN 2010  2010 


   Table 4. Normalized Criteria Comparison Table In AHP   Reusability Meeting Operational Requirements Meeting project Deadline Reusability 0.157 0.148 0.272 Meeting Operational Requirements 0.789 0.744 0.636 Meeting Project Deadline 0.052 0.106 0.090  Table 3 and Table 4 show the weight values of the three criterions as compared to each other using the AHP process. These weights have been decided by the stakeholders after discussions among themselves Average weights can be derived from Table 4 as follows Reusability- 0.193 Meeting Operational Requirements- 0.724 Meeting Project Deadline- 0.083 These weights represent the priority of each criterion on a scale of 0 to 1  5.3. Argumentation Tree  We develop argumentation tree for each and every alternative separately. The ar guments are stated by stake holders and assembled under the alternative but they target a specific cr iterion. These arguments can either be supporting or attacking each other or their respective alternative nodes. We present three figures, where each figure represents the argumentation hierarchy for one alternative. Rectangular boxes represent the alternatives with the name of the alternative under it. Ovals represent the criteria with their descr iption. The arguments are specified by labels A, B, C for alternative Adobe flash, Adobe Director and Open GL respectively Along with the labels, the arguments also have indexes associated with them. Beneath the labels are two boxes The box on left shows the weight of the argument whereas the box on right shows the priority of the stakeholder who specifies the argument  Once the argument has been sp ecified, the user enters its weight. We first reassess the weights of the arguments using priority reassessment discussed in h e n us ing the techniques specified in [11 w e red u ce t h e arg u m e n t s  to a single level. Finally, the weighted summation of the arguments with the criteria weights helps us evaluate the final weights for the decision matrix. It is important to note here that, the aggregation method used for calculating the favorability is a weighted summation  The three argumentation hierarchies for the three alternatives are presented in the Figures 7, 8, and 9. The diagrams contain arguments, their weights and the stakeholders priorities     Figure 7. Argumentation Tree For Adobe Flash   Figure 8. Argumentation Tree For Adobe Director 150 


     Figure 9. Argumentation Tree For Open GL  A1 The current system in flash does not have the functionality of dynamic allocation of particles like mine or clutter. It places them randomly  A1.1 That is not of much importance because it still gives a new position to mine and clutter particles A2 Current system in flash has faster response time as compared to system in Adobe Director A3 The current system doesnt satisfy many of the features required for the new system like database A4 Adobe Flash cannot communicate with database A4.1 Flash doesnt support database but database support is very important and critical A4.1.1 The system should be able to generate evaluation reports for trainee based on pr evious records stored in the database A5 Flash doesnt create sound clips  A5.1 We dont need sound creating features as the sys tem has to generate sound. We can play externally recorded sound files using Adobe Flash A6 Flash can provide good visual effects as compared to Adobe Director A7 The developer has good knowledge in development using Flash so the system can be developed quickly B1 We could reuse the system already developed for sound generation, as it is developed using Adobe Audition for analysis which is somehow related to Adobe Director B1.1 The current system is better synthesized in terms of sound production and the sound produced is also instantaneous rather than discrete B1.2 That current system has certain performance issues like slow response time B1.3 The current system in Adobe Director has the feature of producing dynamic coloring scheme on approaching a mine. This kind of scheme is highly preferable and is not present in Adobe Flash system B2 Adobe Director can provide more functionality as compared to the current flash system. E.g. Multiple sounds while detecting mines   B2.1 Adobe Director can provide better visual effects as compared to flash e.g. in case of GUIs   B2.2 A modified version of the current system in flash can also provide the same functionality B2.2.1 We cannot integrate code developed in other platforms with Flash, but Flash can be integrated in Adobe Director B3 The interface provided by flash is not professional enough. It is too simple and straight forward for doing more things in future   B4 Easily available plug-ins can help integrate the tracking system developed in C# with Adobe Director  B4.1 Code developed in Open GL/AL can also be integrated using Adobe Director using suitable stubs   B5 A new sound recognition algorithm is being developed in Adobe Audition which can be integrated with Adobe Director but not with Open GL or Flash Evidence supported B6 If the current system is reused; the project deadline can be met easily B7 The developer has very little experience in development using Adobe Director   B7.1 The developer can take help from the already developed system in Adobe Director C1 The tracking software already developed is coded in C#/NX5. We could reuse that and develop our system in Open GL/AL C1.1 Open GL has C# libraries which can be used to develop the system C2 Because the platform used is for high end application development, it can provide good GUI and database support C2.1 Open GL/AL can help us generate dynamic surfaces for mine detection and training which the original system in flash does not have C4 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C3 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C4 The time taken for developing the project using open GL will be comparatively more as the whole system would have to be developed from scratch C4.1 If Open GL has support for C# libraries, and then the system could be develope d faster as developer is quite familiar with programming languages like C 151 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





