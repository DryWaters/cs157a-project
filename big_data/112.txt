Abstract Constraint-based rule miners find all rules in a given dataset meeting user-specified constraints such as minimum support and confidence. We describe a new algorithm that directly exploits all user-specified constraints including minimum support, minimum confidence, and a new constraint that ensures every mined rule offers a predictive advantage over any of its simplifications. Our algorithm maintains efficiency even at low supports on data that is dense \(e.g. relational data\. Previous approaches such as Apriori and its variants exploit only the minimum support constraint, and as a result are ineffective on dense data due to a combinatorial explosion of \223frequent itemsets\224 1.     Introduction Mining rules from data is a problem that has attracted considerable interest because a rule provides a concise statement of potentially useful information that is easily understood by end users. In the database literature, the focus has been on developing association rule [1 go ri t h ms t h at identify all conjunctive rules meeting user-specified constraints such as minimum support \(a statement of generality\ minimum confidence \(a statement of predictive ability\ These algorithms were initially developed to tackle data-sets primarily from the domain of market-basket analysis, though there has been recent interest in applying these algorithms to other domains including telecommunications data analysis cens u s d a t a an al y s is [6  an d clas sif i cation and predictive modeling tasks in general. Unlike data from market-basket analysis, these data-sets tend to be dense in that they have any or all of the following properties 1 225 many frequently occurring items \(e.g. sex=male 225 strong correlations between several items 225 many items in each record These data-sets can cause an exponential blow-up in the resource consumption of standard association rule mining algorithms including Apriori [2 d its m a ny v a rian ts The combinatorial explosion is a result of the fact that these algorithms effectively mine all rules that satisfy only the minimum support constraint, the number of which is exorbitant. Though other rule constraints are specifiable, they are typically enforced solely during a post-processing filter step. Our approach to mining on dense data-sets is to instead directly enforce all user-specified rule constraints during mining. For example, most association rule miners allow users to set a minimum on the predictive ability of any mined rule specified as either a minimum confid  o r an alternative measure such as lift [5,8 a l s o kno wn as interest [6 or con v icti on  6   Our algo rith m can  e x p l o i t such minimums on predictive ability during mining for vastly improved efficiency Even given strong minimums on support and predictive ability, the rules satisfying these constraints in a dense dataset are often too numerous to be mined efficiently or comprehended by the end user. To remedy this problem, our algorithm exploits another constraint that eliminates rules that are uninteresting because they contain conditions that do not \(strongly\ contribute to the predictive ability of the rule. To illustrate this useful concept, first consider the rule below Bread & Butter Milk \(Confidence = 80 This rule has a confidence of 80%, which says that 80 of the people who purchase bread and butter also purchase the item in the consequent of the rule, which is milk Because of its high confidence, one might be inclined to believe that this rule is an interesting finding if the goal is to, say, understand the population of likely milk buyers in order to make better stocking and discounting decisions However, if 85% of the population under examination purchased milk, this rule is actually quite uninteresting for this purpose since it characterizes a population that is even less likely to buy milk than the average shopper. This point has motivated additional measures for identifying interesting rules including lift and conviction. Both lift and conviction represent the predictive advantage a rule offers over simply guessing based on the frequency of the consequent. But both measures still exhibit another closely related problem illustrated by the next rule Eggs & Cereal Milk \(Confidence = 95 Because the confidence of this rule \(95%\is significantly higher than the frequency with which milk is purchased 85%\is rule will have lift and conviction values that could imply to the end-user that it is interesting for understanding likely milk buyers. But suppose the purchase of cereal alone implies that milk is purchased with 99% confidence. We then have that the above rule actually represents 1 Market-basket data is sometimes dense, particularly when it incorporates information culled from convenience card applications for mining rules that intermix personal attributes with items purchased 256 256 Constraint-Based Rule Mining in Large, Dense Databases Roberto J. Bayardo Jr IBM Almaden Research Center bayardo@alum.mit.edu Rakesh Agrawal IBM Almaden Research Center rakesh_agrawal@ieee.org Dimitrios Gunopulos IBM Almaden Research Center dg@cs.ucr.edu 


a significant decrease in predictive ability over a more concise rule which is more broadly applicable \(because there are more people who buy cereal than people who buy both cereal and eggs To address these problems, our algorithm allows the user to specify a minimum improvement constraint. The idea is to mine only those rules whose confidence is at least minimp greater than the confidence of any of its proper sub-rules where a proper sub-rule is a simplification of the rule formed by removing one or more conditions from its antecedent. Any positive setting of minimp would prevent the undesirable rules from the examples above from being generated by our algorithm. More generally, the minimum improvement constraint remedies the rule explosion problem resulting from the fact that in dense data-sets, the confidence of many rules can often be marginally improved upon in an overwhelming number of ways by adding additional conditions. For example, given the rule stating that cereal implies milk with 99% confidence, there may be hundreds of rules of the form below with a confidence between 99 and 99.1 Cereal     Milk By specifying a small positive value for minimp, one can trade away such marginal benefits in predictive ability for a far more concise set of rules, with the added property that every returned rule consists entirely of items that are strong contributors to its predictive ability. We feel this is a worthwhile trade-off in most situations where the mined rules are used for end-user understanding For rules to be comparable in the above-described context, they must have equivalent consequents. For this reason, our work is done in the setting where the consequent of the rules is fixed and specified in advance. This setting is quite natural in many applications where the goal is to discover properties of a specific class of interest. This task is sometimes referred to as partial-classification S o m e example domains where it is applicable include failure analysis, fraud detection, and targeted marketing among many others 1.1  Paper overview Section 2 summarizes related work. Section 3 formally defines and motivates the problem of mining rules from dense data subject to minimum support, confidence, and/or improvement constraints. The next three sections define our algorithm in a top-down manner. Section 4 begins with an overview of the general search strategy, and presents pseudo-code for the top level of our algorithm. Section 5 provides details and pseudo-code for the pruning functions invoked by the algorithm body. Section 6 details the itemreordering heuristic, and Section 7 describes the rule postprocessor. The algorithm is empirically evaluated in Section 8. Section 9 concludes with a summary of the contributions 2.     Related work Previous work on mining rules from data is extensive We will not review the numerous proposals for greedy or heuristic rule mining \(e.g. decision tree induction\ focus instead on algorithms that provide completeness guarantees We refer the reader interested in heuristic approaches for mining large data-sets to the scalable algorithms proposed in [7 and 12  There are numerous papers presenting improvements to the manner in which the Apriori algorithm [2 umerates all frequent itemsets \(e.g. [6 th ou g h no n e  ad dress  th e problem of combinatorial explosion in the number of frequent itemsets resulting from applying these techniques to dense data. Other works \(e.g. [4  sh o w ho w to id e n tify all maximal frequent itemsets in data-sets where the frequent itemsets are long and numerous. Unfortunately, all association rules cannot be efficiently extracted from maximal frequent itemsets alone, as this would require performing the intractable task of enumerating and computing the support of all their subsets Srikant et al. [1 N g et al. [1 0] ha v e i n v e s tig ated incorporating item constraints on the set of frequent itemsets for faster association rule mining. These constraints which restrict the items or combinations of items that are allowed to participate in mined rules, are orthogonal to those exploited by our approach. We believe both classes of constraints should be part of any rule-mining tool or application There is some work on ranking association rules using interest measures [6,8,9  th ou g h th is w o r k gi ves no in dication of how these measures could be exploited to make mining on dense data-sets feasible. Smythe and Goodman [1 describe a constraint-based rule miner that exploits an information theoretic constraint which heavily penalizes long rules in order to control model and search complexity. We incorporate constraints whose effects are more easily understood by the end user, and allow efficient mining of long rules should they satisfy these constraints There are several proposals for constraint-based rule mining with a machine-learning instead of data-mining focus that do not address the issue of efficiently dealing with large data-sets. Webb [1 r o v i d es a goo d s u rv e y o f this class of algorithms, and presents the OPUS framework which extends the set-enumeration search framework of Rymon [11  w ith  ad d itio nal g e neric p r u n i n g m e th od s. W e bb instantiates his framework to produce an algorithm for obtaining a single rule that is optimal with respect to the Laplace preference function. We borrow from this work the idea of exploiting an optimistic pruning function in the context of lattice-space search. However, instead of using a single pruning function for optimization, we use several for constraint enforcement. Also, because the itemset frequency information required for exploiting pruning functions is expensive to obtain from a large data-set, we frame our pruning functions so that they can accommodate restricted availability of such information 3.     Definitions and problem statement A transaction is a set of one or more items obtained from a finite item domain, and a data-set is a collection of transactions. A set of items will be referred to more succinctly as an itemset The support of an itemset denoted is I 1 I 2 274 I n 256 I sup I  


the number of transactions in the data-set to contain An association rule or just rule for short  consists of an itemset called the antecedent, and an itemset disjoint from the antecedent called the consequent A rule is denoted as where is the antecedent and the consequent. The support of an association rule is the support of the itemset formed by taking the union of the antecedent and consequent The confidence of an association rule is the probability with which the items in the antecedent appear together with items in the consequent in the given dataset. More specifically Other measures of predictive ability include lift 5,8 which is  also known as interest 6 d co n v i c t i o n 6 Th e conviction and lift of a rule can each be expressed as a function of the rule\222s confidence and the frequency of the consequent; further, both functions are monotone in confidence Though we frame the remainder of this work in terms of confidence, it can be easily recast in terms of any measure with this monotonicity property The association rule mining problem [1 s t o  pr odu ce al l association rules present in a data-set that meet specified minimums on support and confidence. In this paper, we restrict the problem in two ways in order to render it solvable given dense data 3.1  The consequent constraint We require mined rules to have a given consequent specified by the user. This restriction is an item constraint which can be exploited by other proposals [10, 14 b u t on ly to reduce the set of frequent itemsets considered. A frequent itemset is a set of items whose support exceeds the minimum support threshold. Frequent itemsets are too numerous in dense data even given this item constraint. Our algorithm instead leverages the consequent constraint through pruning functions for enforcing confidence, support, and improvement \(defined next\nstraints during the mining phase 3.2  The minimum improvement constraint While our algorithm runs efficiently on many dense datasets without further restriction, the end-result can easily be many thousands of rules, with no indication of which ones are \223good\224. On some highly dense data-sets, the number of rules returned explodes as support is decreased, resulting in unacceptable algorithm performance and a rule-set the enduser has no possibility of digesting. We address this problem by introducing an additional constraint Let the improvement of a rule be defined as the minimum difference between its confidence and the confidence of any proper sub-rule with the same consequent. More formally given a rule If the improvement of a rule is positive, then removing any non-empty combination of items from its antecedent will drop its confidence by at least its improvement. Thus every item and every combination of items present in the antecedent of a large-improvement rule is an important contributor to its predictive ability. A rule with negative improvement is typically undesirable because the rule can be simplified to yield a proper sub-rule that is more predictive, and applies to an equal or larger population due to the antecedent containment relationship. An improvement greater than 0 is thus a desirable constraint in almost any application of association rule mining. A larger minimum on improvement is also often justified because most rules in dense data-sets are not useful due to conditions or combinations of conditions that add only a marginal increase in confidence. Our algorithm allows the user to specify an arbitrary positive minimum on improvement 3.3  Problem statement We develop an algorithm for mining all association rules with consequent meeting user-specified minimums on support, confidence, and improvement. The algorithm parameter specifying the minimum confidence bound is known as minconf and the minimum support bound minsup We call the parameter specifying a minimum bound on improvement minimp A rule is said to be confident if its confidence is at least minconf, and frequent if its support is at least minsup. A rule is said to have a large improvement if its improvement is at least minimp 4.     Set-enumeration search in large data-sets From now on, we will represent a rule using only its antecedent itemset since the consequent is assumed to be fixed to itemset Let denote the set of all items present in the database except for those in the consequent. The rulemining problem is then one of searching through the power set of for rules which satisfy the minimum support, confidence, and improvement constraints. Rymon\222s set-enumeration tree framework pro v id es a s c h e me fo r representing a subset search problem as a tree search problem, allowing pruning rules to be defined in a straightforward manner in order to reduce the space of subsets \(rules considered. The idea is to first impose an ordering on the set of items, and then enumerate sets of items according to the ordering as illustrated in Figure 1 FIGURE 1 A completely expanded set-enumeration tree over with items ordered lexically I AC 256 AC AC 310 A C conf AC 256  sup AC 310  sup A   lift AC 256  sup 306  sup C  conf AC 256  327  conviction AC 256  sup 306  sup C  226 sup 306 1 conf AC 256  226   C AC 256 imp AC 256 min A   A 314 conf AC 256 conf A  C 256  226    C CU U U 1234     1 2 1,2 1,2,3 1,2,3,4 1,3 1,3,4 1,4 2,3 2,3,4 2,4 3 3,4 4 1,2,4 


4.1  Terminology We draw upon the machinery developed in previous work where we framed the problem of mining maximal frequent itemsets from databases as a set-enumeration tree search prob  Each  no de i n  th e tree i s represented  by two itemsets called a group The first itemset, called the head is simply the itemset \(rule\ enumerated at the given node. The second itemset, called the tail is actually an ordered set, and consists of those items which can be potentially appended to the head to form any viable rule enumerated by a sub-node. For example, at the root of the tree, the head itemset is empty and the tail itemset consists of all items in  The head and tail of a group will be denoted as and respectively. The order in which tail items appear in is significant since it reflects how its children are to be expanded \(Figure 3\ Each child of a group is formed by taking an item and appending it to to form Then is made to contain all items in that follow in the ordering. Given this child expansion policy, without any pruning of nodes or tail items, the set-enumeration tree enumerates each and every subset of exactly once We say a rule is derivable from a group if  and By definition, any rule that can be enumerated by a descendent of in the set-enumeration tree is also derivable from  Define the candidate set of a group to be the set consisting of the following itemsets 225 and  225 and for all  225 and  A group is said to be processed once the algorithm has computed the support of every itemset in its candidate set 4.2  Top-level algorithm description It is now possible to provide a top-level description of the algorithm, which we call Dense-Miner. The body \(Figure 2\ implements a breadth-first search of the set enumeration tree with Generate-Initial-Groups seeding the search The groups representing an entire level of the tree are processed together in one pass over the data-set. Though any systematic traversal of the set-enumeration tree could be used, Dense-Miner uses a breadth-first traversal to limit the number of database passes to at most the length of the longest frequent itemset. The use of hash-trees and other implementation details for efficient group processing is described in [4 Generate-Initial-Groups could simply produce the root node which consists of an empty head and for its tail However, our implementation seeds the search at the second level of the tree after an optimized phase that rapidly computes the support of all 1 and 2 item rules and their antecedents using array data-structures instead of hash trees Generate-Next-Level \(Figure 3\ generates the groups that comprise the next level of the set-enumeration tree Note that the tail items of a group are reordered before its children are expanded. This reordering step is a crucial optimization designed to maximize pruning efficiency. We delay discussing the details of item reordering until after the pruning strategies are described, because the particular pruning operations greatly influence the reordering strategy After child expansion, any rule represented by the head of a group is placed into by Extract-Rules if it is frequent and confident. The support information required to check if the head of a group represents a frequent or confident rule is provided by the parent of in the set-enumeration tree because and are members of its candidate set. As a result, this step can be performed before is processed The remaining algorithmic details, which include node pruning \(the P RUNE G ROUPS function\tem-reordering and post-processing \(the P OST P ROCESS function\, are the subjects of the next three sections 5.     Pruning This section describes how Dense-Miner prunes both processed and unprocessed groups. In Figure 2, note that groups are pruned following tree expansion as well as immediately after they are processed. Because groups are unprocessed following tree expansion, in order to determine if they are prunable, Dense-Miner uses support information gathered during previous database passes U ghg  tg  tg  g c g itg  316 hg  hg c  tg c  tg  i U rghg  r 314 rhg  tg  310 315 g g g hg  hg  C 310 hg  i  310 hg  i  C 310\310 itg  316 hg  tg  310 hg  tg  C 310\310 FIGURE 2 Dense-Miner at its top level. The input parameters minconf, minsup, minimp, and are assumed global C D ENSE M INER Set of Transactions   Returns all frequent, confident, large improvement rules present in Set of Rules Set of Groups G ENERATE I NITIAL G ROUPS  while  is non-empty do scan to process all groups in P RUNE G ROUPS  Section 5 G ENERATE N EXT L EVEL  E XTRACT R ULES  P RUNE G ROUPS  Section 5 return P OST P ROCESS  Section 7 T T R 306 254 G 254 T R G TG G R G 254 G RR 310 254 G G R RT U FIGURE 3 Procedure for expanding the next level of the set-enumeration tree G ENERATE N EXT L EVEL Set of groups   Returns a set of groups representing the next level of the set-enumeration tree Set of Groups for each group in  do reorder the items in  Section 6 for each item in do let  be a new group with  and return  G G c 306 254 gG tg  itg  g c hg c  hg  i  310  tg c  jj follows i in the ordering   G c G c g c  310 254 G c R g g hg  hg  C 310 g 


5.1  Applying the pruning strategies Dense-Miner applies multiple strategies to prune nodes from the search tree. These strategies determine when a group can be pruned because no derivable rule can satisfy one or more of the input constraints. When a group cannot be pruned, the pruning function checks to see if it can instead prune some items from Pruning tail items reduces the number of children generated from a node, and thereby reduces the search space. An added benefit of pruning tail items is that it can increase the effectiveness of the strategies used for group pruning. The observation below which follows immediately from the definitions, suggests how any method for pruning groups can also be used to prune tail items O BSERVATION 5.1: Given a group and an item  consider the group such that and If no rules derivable from satisfy some given constraints, then except for rule  no rules derivable from such that satisfy the given constraints The implication of this fact is that given a group and tail item with the stated condition, we can avoid enumerating many rules which do not satisfy the constraints by simply removing from after extracting rule if necessary. The implementation of PruneGroups, described in Figure 4, exploits this fact The group pruning strategies are applied by the helper function Is-Prunable which is described next. Because fewer tail items can improve the ability of Is-Prunable to determine whether a group can be pruned, whenever a tail item is found to be prunable from a group, the group and all tail items are checked once more 5.2  Pruning strategies The function Is-Prunable computes the following values for the given group  225 an upper-bound on the confidence of any rule derivable from  225 an upper-bound on the improvement of any rule derivable from that is frequent 225 an upper-bound on the support of any rule derivable from  Note that a group can be pruned without affecting the completeness of the search if one of the above bounds falls below its minimum allowed value as specified by minconf minimp, and minsup respectively. The difficulty in implementing pruning is not simply how to compute these three bounds, but more specifically, how to compute them given that acquiring support information from a large data-set is time consuming. We show how to compute these bounds using only the support information provided by the candidate set of the group, and/or the candidate set of its parent In establishing these bounding techniques in the remaining sub-sections, we assume the existence of a special 223derived\224 item that is contained only by those transactions in the data-set that do not contain the consequent itemset Similarly, for a given item we sometimes assume the existence of an item contained only by those transactions that do not contain These derived items need not actually be present in the data-set -- they are used only to simplify the presentation. For an itemset we have that We also exploit the fact that which holds whether or not and/or contain derived items 5.3  Bounding confidence T HEOREM 5.2: The following expression provides an upperbound on the confidence of any rule derivable from a given group   where and are non-negative integers such that and  Proof Recall that the confidence of a rule is equal to This fraction can be rewritten as follows where and  Because this expression is monotone in and antimonotone in we can replace with a greater or equal value and with a lesser or equal value without decreasing the value of the expression. Consider replacing with and with The claim then follows if we establish that for any rule derivable from 1 and \(2 For \(1\ote that It follows that and hence  For \(2\ote that Because g g itg  gitg  316 g  hg   hg  i  310  tg   tg  i  226  g  hg  i  310 r gir 316 g i itg  hg  i  310 FIGURE 4 Top level of the pruning function P RUNE G ROUPS Set of groups Set of rules   Prunes groups and tail items from groups within  and are passed by reference for each group in  do do if I S P RUNABLE  then remove  from  else for each do let be a group with and if I S P RUNABLE  then remove  from  put in if it is a frequent and confident rule while GR G G R gG try_again false 254 g gG itg  316 g  hg   hg  i  310  tg   tg  i  226  g  i tg  hg  i  310 R try_again true 254 try_again true  g uconf g  g uimp g  g usup g  g g c 330 Ci i 330 i Ic 330  310 sup Ic 330  310 sup I  sup IC 310  226  I 1 I 2 314 sup I 1  sup I 2  263 336 I 1 I 2 g x xy  xy y sup hg  tg  c 330  310\310  243 x sup hg  C 310  263 r sup rC 310 sup r  244 x  x  y   x sup rC 310   y sup r  sup rC 310  226  x  y  x  y  x  xy  y rg xx  263 yy  243 hg  r 314 sup rC 310 sup hg  C 310  243 xx  263 rhg  tg  310 315 


we have  Theorem 5.2 is immediately applicable for computing for a processed group since the following itemsets needed to compute tight values for and are all within its candidate set   and There are rules derivable from a given group and the support of these four itemsets can be used to potentially eliminate them all from consideration Note that if were frequent, then an algorithm such as Apriori would enumerate every derivable rule We have framed Theorem 5.2 in a manner in which it can be exploited even when the exact support information used above is not available. This is useful when we wish to prune a group before it is processed by using only previously gathered support information. For example, given an unprocessed group we cannot compute to use for the value of but we can compute a lower-bound on the value. Given the parent node of because is a superset of such a lower-bound is given by the observation below O BSERVATION 5.3: Given a group and its parent in the set-enumeration tree  Conveniently, the support information required to apply this fact is immediately available from the candidate set of  In the following observation, we apply the support lower-bounding theorem from [4 to ob tain  an o t her lo wer bound on again using only support information provided by the candidate set of  O BSERVATION 5.4: Given a group and its parent in the set-enumeration tree  When attempting to prune an unprocessed group, DenseMiner computes both lower-bounds and uses the greater of the two for in theorem 5.2 5.4  Bounding improvement We propose two complementary methods to bound the improvement of any \(frequent\ule derivable from a given group The first technique uses primarily the value of described above, and the second directly establishes an upper-bound on improvement from its definition Dense-Miner computes by retaining the smaller of the two bounds provided by these techniques Bounding improvement using the confidence bound The theorem below shows how to obtain an upper-bound on improvement by reusing the value of along with another value no greater than the confidence of the sub-rule of with the greatest confidence T HEOREM 5.5: The value of where is an upper-bound on the improvement of any rule derivable from  Proof Let denote the sub-rule of with the greatest confidence. Because is a proper sub-rule of any rule derivable from we know that is an upper-bound on  Because and we have  Dense-Miner uses the previously described method for computing when applying this result. Computing a tight value for requires knowing the sub-rule of with the greatest confidence. Because is not known Dense-Miner instead sets to the value of the following easily computed function if has a parent  otherwise The fact that follows from its definition. Its computation requires only the value of where is the parent of and the supports of and in order to compute The value can be computed whether or not the group has been processed because this information can be obtained from the parent group Bounding improvement directly A complementary method for bounding the improvement of any frequent rule derivable from is provided by the next theorem. This technique exploits strong dependencies between head items T HEOREM 5.6: The following expression provides an upperbound on the improvement of any frequent rule derivable from a given group  where  and are non-negative integers such that  and Proof sketch For any frequent rule derivable from  note that can be written as where the first term represents as in Theorem 5.2\ and the subtractive term represents the confidence of the proper sub-rule of with the greatest confidence. To prove the claim, we show how to transform this expression into the expression from the theorem statement arguing that the value of the expression never decreases as a result of each transformation To begin, let the subtractive term of the expression denote the confidence of a proper sub-rule of such that where denotes the item from that minimizes  rc 330  310 hg  tg  c 330  310\310 315 y sup hg  tg  c 330  310\310  243 sup rc 330  310  243 sup r  sup rC 310  226 y   uconf g  g xy hg  hg  C 310 hg  tg  310 hg  tg  C 310\310 2 tg  1 226 g hg  tg  C 310\310 g sup hg  tg  c 330  310\310  y g p ghg p  tg p  310 hg  tg  310 gg p sup hg p  tg p  c 330  310\310  sup hg  tg  c 330  310\310  243 g p sup hg  tg  c 330  310\310  g p gg p sup hg  c 330  310  sup hg p  i 330 c 330   310  itg  316 345 226 sup hg  tg  c 330  310\310  243 y g uconf g  uimp g  uconf g  z hg  uconf g  z 226 z max r  hg  315 conf r    243 g r s hg  r s r d g conf r d  conf r s  226 imp r d  uconf g  conf r d  263 z conf r s  243 imp r d   conf r d  conf r s  226 243 conf r d  z 226 243 uconf g  z 226 243 uconf g  zr s hg  r s z f z g  max f z g p   conf hg      gg p f z g  conf hg    f z g  max r  hg  315 conf r    243 f z g p  g p g hg  hg  C 310 conf hg   g g x xy  x xy b  226 xy b y sup hg  tg  c 330  310\310  243 b min i  hg  316 sup hg  i  226  c 330 i 330   310    263 x min max y 2 y b minsup  sup hg  C 310     rg imp r  x  x  y   x  a   x  y  a  b   226 conf r  r r s r r s ri m  226  i m i hg  sup hg  i  226  c 330 i 330   310  


Since we can only decrease the value of the subtractive term by such a transformation, we have not decreased the value of the expression Now, given and it is easy to show that  and Because the expression is anti-monotone in and and monotone in we can replace  with  with and with without decreasing its value We are now left with an expression identical to the expression in the theorem, except for occurring in place of Taking the derivative of this expression with respect to and solving for 0 reveals it is maximized when Note that for any rule derivable from  must fall between and Given this restriction on the equation is maximized at  We can therefore replace with without decreasing its value. The resulting expression, identical to that in the theorem statement, is thus an upper-bound on  To apply this result to prune a processed group  Dense-Miner sets to since the required supports are known. Computing a tight value for  where is the item in that minimizes this support value\ is not possible given the support values available in the candidate set of and its ancestors. Dense-Miner therefore sets to an upperbound on as computed by the following function when has a parent and where denotes the single item within the itemset  otherwise This computation requires only the value of which was previously computed by the parent, and the supports of candidate set members   and in order to compute  In applying theorem 5.6 to prune an unprocessed group Dense-Miner computes as above. For it lacks the necessary support information to compute so instead it computes a lowerbound on the value as described in section 5.3 5.5  Bounding support The value of is comparatively easy to compute because support is anti-monotone with respect to rule containment. For Dense-Miner simply uses the value of Other anti-monotone constraints, e.g those discussed in [1  c a n b e e x p l oi te d w i th s i m ila r  e a s e  6.     Item ordering The motivation behind reordering tail items in the Generate-Next-Level function is to, in effect, force unpromising rules into the same portion of the search tree. The reason this strategy is critical is that in order for a group to be prunable every sub-node of the group must represent a rule that fails to satisfy one of the constraints. An arbitrary ordering policy will result in a roughly even distribution of rules that satisfy the constraints throughout the search tree, yielding little pruning opportunities We experimented with several different ordering policies intended to tighten the bounds provided by the pruning functions. The strategy we found to work best exploits the fact that the computations for and both require a value and the larger the value allowed for the tighter the resulting bound. The idea then is to reorder tail items so that many sub-nodes will have a large value for This is achieved by positioning tail items which contribute to a large value of last in the ordering since tail items which appear deeper in the ordering will appear in more sub-nodes than those tail items appearing earlier. We have found that the tail items which contribute most to this value tend to be those with small values for This can be seen from Observation 5.4 which yields a larger lower-bound on when the value of summed over every tail item is small. The policy used by Dense-Miner is therefore to arrange tail items in decreasing order of  7.     Post-processing The fact that Dense-Miner finds all frequent, confident large-improvement rules and places them into follows from the completeness of a set-enumeration tree search and the correctness of our pruning rules, as established by the theorems from Section 5. Dense-Miner must still post-process because it could contain some rules that do not have a large improvement Removing rules without a large improvement is non-trivial because improvement is defined in terms of all the proper sub-rules of a rule, and all such rules are not necessarily generated by the algorithm. A naive post-processor for removing rules without a large improvement might, for every mined rule, explicitly compute its improvement by generating and testing every proper sub-rule. Because Dense-Miner is capable of mining many long rules, such an approach would be too inefficient Instead, the post-processor first identifies some rules that do not have a large improvement by simply comparing them to the other rules in the mined rule set It compares each rule to every rule such that and  If ever it is found that then rule is removed because its improvement is not large This step alone requires no database access, and removes almost all rules that do not have a large improvement To remove any remaining rules, the post-processor performs a set-enumeration tree search for rules that could potentially prove some rule in does not have a large improvement. The main difference between this procedure and the mining phase is in the pruning strategies applied For this search problem, a group is prunable when none of its derivable rules can prove that some rule in lacks a large improvement. This is determined by either of the following conditions rr s a 0 263 y  y 263b  b 243 a  y  b  a  0 b  b y  y x  x x  x  y 2 y b   gx  sup hg  C 310  minsup x  x minmax y 2 y b minsup  sup hg  C 310    x  x  x imp r  g y sup hg  tg  c 330  310\310  b sup hg  i m 226  i m 330 c 330   310  i m hg  g b sup hg  i m 226  i m 330 c 330   310  f b g  min f b g p sup hg p  i 330 c 330   310     g g p i hg  hg p  226 f b g  245  f b g p  hg  hg  C 310 hg p  hg p  C 310 sup hg p  i 330 c 330   310  g b y sup hg  tg  c 330  310\310  usup g  usup g  sup hg  C 310  uconf g  uimp g  y sup hg  tg  c 330  310\310  243 y sup hg  tg  c 330  310\310  sup hg  tg  c 330  310\310  sup hg  i 330 c 330   310  sup hg  tg  c 330  310\310  sup hg  i 330 c 330   310  sup hg  i 330 c 330   310  R R R r 1 R 316 r 2 r 2 R 316 r 2 r 1 314 conf r 1  conf r 2  226 minimp  r 1 R g R 


225 There exists no rule for which  225 for all rules such that  After groups are processed, any rule is removed if there exists some group such that and Because the search explores the set of all rules that could potentially prove some rule in does not have a large improvement, all rules without a large improvement are identified and removed Our post-processor includes some useful yet simple extensions of the above for ranking and facilitating the understanding of rules mined by Dense-Miner as well as other algorithms. The improvement of a rule is useful as an interestingness and ranking measure to be presented to the user along with confidence and support. It is also often useful to present the proper sub-rule responsible for a rule\222s improvement value. Therefore, given an arbitrary set of rules, our post-processor determines the exact improvement of every rule, and associates with every rule its proper subrule with the greatest confidence \(whether or not this subrule is in the original rule set\le-sets that are not guaranteed to have high-improvement rules \(such as those extracted from a decision tree\, the sub-rules can be used to potentially simplify, improve the generality of, and improve the predictive ability of the originals 8.     Evaluation This section provides an evaluation of Dense-Miner using two real-world data-sets which were found to be particularly dense in [4  1 The first data-set is compiled from PUMS census data obtained from It consists of 49,046 transactions with 74 items per transaction, with each transaction representing the answers to a census questionnaire. These answers include the age, taxfiling status, marital status, income, sex, veteran status, and location of residence of the respondent. Similar data-sets are used in targeted marketing campaigns for identifying a population likely to respond to a particular promotion. Continuous attributes were discretized as described in  though no frequently occurring items were discarded. The second data-set is the connect-4 data-set from the Irvine machine learning database repository It consists of 67,557 transactions and 43 items per transaction This data-set is interesting because of its size, density, and a minority consequent item \(\223tie games\224\ accurately predicted only by rules with low support. All experiments presented here use the \223unmarried partner\224 item as the consequent with the pums data-set, and the \223tie games\224 item with the connect-4 data-set; we have found that using other consequents consistently yields qualitatively similar results Execution times are reported in seconds on an IBM IntelliStation M Pro running Windows NT with a 400 MHZ Intel Pentium II Processor and 128MB of SDRAM. Execution time includes runtime for both the mining and post-processing phases The minsup setting used in the experiments is specified as a value we call minimum coverage where In the context of consequent constrained association rule mining, minimum coverage is more intuitive than minimum support, since it specifies the smallest fraction of the population of interest that must be characterized by each mined rule 8.1  Effects of minimum improvement The first experiment \(Figure 5\hows the effect of different minimp settings as minsup is varied. Minconf in these experiments is left unspecified, which disables pruning with the minimum confidence constraint. The graphs of the figure plot execution time and the number of rules returned for several algorithms at various settings of minimum support Dense-miner is run with minimp settings of .0002, .002, and 02 \(dense_0002, dense_002, and dense_02 respectively We compare its performance to that of the Apriori algorithm optimized to exploit the consequent constraint \(apriori_c This algorithm materializes only those frequent itemsets that contain the consequent itemset The first row of graphs from the figure reveals that apriori_c is too slow on all but the greatest settings of minsup for both data-sets. In contrast, very modest settings of minimp allow Dense-Miner to efficiently mine rules at far lower supports, even without exploiting the minconf constraint. A natural question is whether mining at low supports is necessary. For these data-sets, the answer is yes simply because rules with confidence significantly higher than the consequent frequency do not arise unless minimum coverage is below 20%. This can be seen from Figure 7 which plots the confidence of the best rule meeting the minimum support constraint for any given setting 2 This property is typical of data-sets from domains such as targeted marketing, where response rates tend to be low without focusing on a small but specific subset of the population The graphs in the second row of Figure 5 plot the number of rules satisfying the input constraints. Note that runtime correlates strongly with the number of rules returned for each algorithm. For apriori_c, the number of rules returned is the same as the number of frequent itemsets containing the consequent because there is no minconf constraint specified. Modest settings of minimp dramatically reduce the number of rules returned because most rules in these data-sets offer only insignificant \(if any\ predictive advantages over their proper sub-rules. This effect is particularly pronounced on the pums data-set, where a minimp setting of .0002 is too weak a constraint to keep the number of such rules from exploding as support is lowered. The increase in runtime and rule-set size as support is lowered is far more subdued given the larger \(though still small\inimp settings 1 Both data-sets are available in the form used in these experiments from http://www.almaden.ibm.com/cs/quest rR 316 hg  r 314 conf r   uconf g  226 minimp 263 rR 316 hg  r 314 rR 316 g hg  r 314 conf r   conf hg   226minimp  R http://augustus.csscr.washington.edu/census/comp_013.html http://www.ics.uci.edu/~mlearn/MLRepository.html 2 The data for this figure was generated by a version of Dense-Miner that prunes any group that cannot lead to a rule on the depicted support/confidence border. This constraint can be enforced during mining using the confidence and support bounding techniques from section 5 minimum coverage minsup sup C  244  


FIGURE 5 Execution time and rules returned versus minimum coverage for the various algorithms FIGURE 6 Execution time of dense_0002 as minconf is varied for both data-sets. Minimum coverage is fixed at 5% on pums and 1% on connect-4 FIGURE 7 Maximum confidence rule mined from each data-set for a given level of minimum coverage   1 10 100 1000 10000 100000 0 10 20 30 40 50 60 70 80 90 Execution time \(sec Minimum Coverage connect-4 apriori_c  dense_0002   dense_002   dense_02    1 10 100 1000 10000 100000 1e+06 0 10 20 30 40 50 60 70 80 90 Number of Rules Minimum Coverage connect-4 apriori_c  dense_0002   dense_002   dense_02    1 10 100 1000 10000 100000 0 10 20 30 40 50 60 70 80 90 Execution Time \(sec Minimum Coverage pums apriori_c  dense_0002   dense_002   dense_02    1 10 100 1000 10000 100000 1e+06 1e+07 0 10 20 30 40 50 60 70 80 90 Number of Rules Minimum Coverage pums apriori_c  dense_0002   dense_002   dense_02    0 500 1000 1500 2000 2500 3000 3500 20 25 30 35 40 45 50 55 60 65 Execution time \(sec minconf pums  connect-4  1 10 100 1000 10000 100000 1e+06 20 25 30 35 40 45 50 55 60 65 Number of Rules minconf pums  connect-4    0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100 Highest Rule Confidence Minimum Coverage pums  connect-4 


8.2  Effects of minimum confidence The next experiment \(Figure 6\ws the effect of varying minconf while fixing minimp and minsup to very low values. With connect-4, we used a minimum coverage of 1%, and with pums, a minimum coverage of 5%. Minimp was set to .0002 with both data-sets. As can be extrapolated from the previous figures, the number of rules meeting these weak minimp and minsup constraints would be enormous As a result, with these constraints alone, Dense-Miner exceeds the available memory of our machine The efficiency of Dense-Miner when minimum confidence is specified shows that it is effectively exploiting the confidence constraint to prune the set of rules explored. We were unable to use lower settings of minconf than those plotted because of the large number of rules. As minconf is increased beyond the point at which fewer than 100,000 rules are returned, the run-time of Dense-Miner rapidly falls to around 500 seconds on both data-sets 8.3  Summary of experimental findings These experiments demonstrate that Dense-Miner, in contrast to approaches based on finding frequent itemsets achieves good performance on highly dense data even when the input constraints are set conservatively. Minsup can be set low \(which is necessary to find high confidence rules as can minimp and minconf \(if it is set at all\This characteristic of our algorithm is important for the end-user who may not know how to set these parameters properly. Low default values can be automatically specified by the system so that all potentially useful rules are produced. Refinements of the default settings can then be made by the user to tailor this result. In general, the execution time required by Dense-Miner correlates strongly with the number of rules that satisfy all of the specified constraints 9.     Conclusions We have shown how Dense-Miner exploits rule constraints to efficiently mine consequent-constrained rules from large and dense data-sets, even at low supports. Unlike previous approaches, Dense-Miner exploits constraints such as minimum confidence \(or alternatively, minimum lift or conviction\ and a new constraint called minimum improvement during the mining phase. The minimum improvement constraint prunes any rule that does not offer a significant predictive advantage over its proper sub-rules. This increases efficiency of the algorithm, but more importantly it presents the user with a concise set of predictive rules that are easy to comprehend because every condition of each rule strongly contributes to its predictive ability The primary contribution of Dense-Miner with respect to its implementation is its search-space pruning strategy which consists of the three critical components: \(1\functions that allow the algorithm to flexibly compute bounds on confidence, improvement, and support of any rule derivable from a given node in the search tree; \(2\proaches for reusing support information gathered during previous database passes within these functions to allow pruning of nodes before they are processed; and \(3\ item-ordering heuristic that ensures there are plenty of pruning opportunities. In principle, these ideas can be retargeted to exploit other constraints in place of or in addition to those already described We lastly described a rule post-processor that DenseMiner uses to fully enforce the minimum improvement constraint. This post-processor is useful on its own for determining the improvement value of every rule in an arbitrary set of rules, as well as associating with each rule its proper sub-rule with the highest confidence. Improvement can then be used to rank the rules, and the sub-rules used to potentially simplify, generalize, and improve the predictive ability of the original rule set References 1 w a l  R.; Im ie lin ski  T   a n d S w a m i, A. 1 9 9 3   M i n i ng As so ciations between Sets of Items in Massive Databases. In Proc of the 1993 ACM-SIGMOD Int\222l Conf. on Management of Data 207-216 2 raw a l R.; M a n n ila, H Sri k an t  R T o i v o n en  H.; an d  Verkamo, A. I. 1996. Fast Discovery of Association Rules. In Advances in Knowledge Discovery and Data Mining AAAI Press, 307-328 3 K Ma ng a n a r is S a n d Sri k a n t, R 19 97  P a rtia l Cl a ssif i cation using Association Rules. In Proc. of the 3rd Int'l Conference on Knowledge Discovery in Databases and Data Mining 115-118 4 a rd o  R. J 1 9 9 8  Ef f i c i en tly Min i n g  Lo n g  P a ttern s fro m  Databases. In Proc. of the 1998 ACM-SIGMOD Int\222l Conf. on Management of Data 85-93 5  Mi c h ae l J. A a n d  Lin o f f G  S 1 9 9 7  Data Mining Techniques for Marketing, Sales and Customer Support John Wiley & Sons, Inc 6 Bri n, S  M o t w a n i, R.; Ullm a n J.; a n d  Tsu r S. 19 9 7 Dyn a m i c  Itemset Counting and Implication Rules for Market Basket Data. In Proc. of the 1997 ACM-SIGMOD Int\222l Conf. on the Management of Data 255-264 7 h e n  W   W   1 9 9 5 F a st Ef fecti v e Ru le In d u ctio n   In  Proc. of the 12th Int\222l Conf. on Machine Learning 115-123 8 In tern atio n a l Bu sin e s s Mac h in e s   1 9 9 6  IBM Intelligent Miner User\222s Guide Version 1, Release 1 9 m e t tin e n M   Ma nn ila  P  Ro nk a i ne n  P   a n d V e rk a m o  A  I. 1994. Finding Interesting Rules from Large Sets of Discovered Association Rules. In Proc. of the Third Int\222l Conf. on Information and Knowledge Management 401-407 10  Ng   R  T    L a k s hm ana n   V   S    Ha n  J   an d P a ng A  1 9 9 8   Exploratory Mining and Pruning Optimizations of Constrained Association Rules. In Proc of the 1998 ACM-SIGMOD Int\222l Conf. on the Management of Data 13-24 11 Ry mo n  R 1 9 9 2   Search  t h ro u g h Sy s t e m atic S e t En u m era tion. In Proc. of Third Int\222l Conf. on Principles of Knowledge Representation and Reasoning 539-550 1  Sha f e r  J  A g r a w a l R   an d Me ht a M 19 98  SPR I N T   A  Scalable Parallel Classifier for Data-Mining. In Proc. of the 22nd Conf. on Very Large Data-Bases 544-555 13  S m y t he P  and  Go od man   R  M 19 92 An I n f o r m at i o n Th eo retic Approach to Rule Induction from Databases IEEE Transactions on Knowledge and Data Engineering 4\(4\:301316 14  S r i k a n t   R    V u  Q an d Ag r a w a l  R  19 97 M i ni ng  A ssoc i a tion Rules with Item Constraints. In Proc. of the Third Int'l Conf. on Knowledge Discovery in Databases and Data Mining 67-73 15 W e bb, G. I 1 9 9 5 OP U S An Ef f i c i e n t Adm i ssible Algo rit h m for Unordered Search. In Journal of Artificial Intelligence Research 3:431-465 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


