Big Data and Policy design for Data Sovereignty: \nA Case Study on Copyright and CCL in South Korea \n \nHyejung Moon, Ph.D. in Public Policy \nInstitute of IT policy \nSeoul National University of Science and Technology nSeoul, Republic of Korea \nhyejung.moon@gmail.com \nHyun Suk Cho, Professor \nDepartment of Public Administration \nSeoul National University of Science and Technology \nSeoul, Republic of Korea nhyunsuk@seoultech.ac.kr\n \n \nAbstract— The purpose of this paper is as follows.  First, I am \ntrying to conceptualize big data as a social problem. Second, I \nwould like to explain the difference between big data and \nconventional mega information. Third, I would like to \nrecommend the role of the government for utilization of big \ndata as a policy tools. Fourth, while referring to copyright and \nCCL\(Creative Commons License like to suggest a direction of policy design for big data. \nAs for the result of this study, policy design for big data should \nbe distinguished from policy design for mega information to \nsolve data sovereignty issues From a law system perspective, \nbig data is generated autonomously. It has been accessed \nopenly and shared without any intention. In market \nperspective, big data is created without any intention. Big data \ncan be changed automatically in case of openness with \nreference feature such as Linked of Data. Some policy issues nsuch as responsibility and authenticity should be raised. Big \ndata is generated in a distributed and diverse way without any \nconcrete form in technology perspective. So, we need a \ndifferent approach. \nKeywords- Big Data; Mega Information; Policy Design; Data \nSovereignty; Copyright; CCL \nI.  INTRODUCTION \nThe rise of big data has brought both infinite \nopportunities and serious risks in economic, cultural, and \nsocial sectors nationally and internationally\(Moon & Cho, \n2012 effective responses to opportunities and risks derived \nfrom big data. Obama administration decided to invest about \n$ 200 million in big data sector for implementation of \nGovernment 2.0 last 2012\(DailyTech 04/02/2012 for government 3.0 project in \nnext 5 years, in spite of tightened e-government \nbudget\(ZDNet Korea 04/03/2013 Most governments \nare pushing big data projects in basic level without a specific \ndirection in the same way ISPs and consulting firms promote \nbig data for just private profits. Even though the interests in \nbig data are heightened in private market, the way big data is \nconceptualized and utilized are no different from the existing \napplication of mega information. IT venders such as IBM, \nSAP could not suggest a clear distinction between big data \nand mega information. Big data is just ambiguously \nconceptualized to have features like 3V volume, variety and \nvelocity 3Vs plus Value and complexity from the conventional mega \ninformation both conceptually and practically? Without a \nclear conceptualization of big data, I have a doubt how we \ncan respond effectively to both opportunities and risks which \nbig data has brought. \nBecause of this reason, first, I try to conceptualize big \ndata as a social problem. Second, I would like to explain how \nbig data is different from existing mega information. Third, I \ntry to suggest the role of the government for utilization of big \ndata and accompanied responsibility in terms of a policy ntools. Fourth, based on case studies of copyright and CCL, I \nwould like to explain the regulation for big data in terms of \ndata sovereignty. Finally, I would like to suggest a direction \nof policy design for big data age. \nTable I summarizes the theoretical background and \nresearch concept for policy design stage in this study. Policy \ndesign includes both technical analysis and political \nreasoning as a policy process for achieving a particular \npurpose\(Birkland, 2011 understand causal relation, 3 I.  THEORY AND CONCEPT FOR POLICY DESIGN \nStage Concept Theory \nSocial issue Conceptualization Knowledge Taxonomy\(Zeleny, 1987 Rowley, 2006 Public Commons\(Hardin, 1968 Hood, 1983 setting Regulation\(Lessig, 1999 1987 his research about knowledge taxonomy. \nRowley\(2006 big of wisdom hierarchy. I've referred the theory of \npublic commons for setting the scope of problem about nsovereignty of big data\(Hardin, 1968 tools among \navailable alternatives\(Hood, 1983 nSocialCom/PASSAT/BigData/EconCom/BioMedCom 2013\n978-0-7695-5137-1/13 $26.00 © 2013 IEEE\nDOI 10.1109/SocialCom.2013.165\n1026\nof regulation structure for conducting policy \nobjectives\(Lessig, 1999 CONCEPTUAL DISCUSSION OF BIG DATA \nThe term of data was academically distinguished from \ninformation by Zeleny\(1987 data, information, knowledge, and wisdom\(DIKW \ntaxonomy 


nby computing process. In the past it was impossible to \nmanage all data under resource constraints such as high costs \nand lack of technology. Therefore only a few important data \nwere gathered, translated and managed into information \nunder resource constrains in the last 20C. Most data which \nwere not translated into information were deleted and \nimpossible to use later. Only a few experts could access data \nand create information in this time. He said that sequential \nmanagement in order by data ? information ? knowledge \n wisdom for overcoming resource constrains in early \ninformation and technology age.  \nBig data age means that all electronic signal from \ncomputing has became a just huge size data seen from the \ndefinition of Zeleny\(1987 ncould the data become so huge volume? Behind this \nsituation, the development of an information and ncommunication technology has played an important role. \nThe condition of data processing has been improved owing \nto technological progress of processor and network. Whole \ndata could be saved permanently because of cost reduction \nfor data processing and data access. \nOn the other hand, Rowley \(2006 ncondition of rich resources and sufficient technology, \nwisdom hierarchy can be translated in dynamic and ninteractive way. So, the higher insight analysts have, the \nmore value is extracted from utilization of data Figure 2 \ndescribes the relative wisdom hierarchy with Human Input \nStructure and Computer Input Programmability. \n \nCitation: Zeleny, M. \(1987 nmanagement.” Human Systems Management, 7\(1 2006 representations of the DIKW hierarchy", Journal of Information Science, 33, 2: 163–180 \nreconstitution nFigure 1.  DISK hierarchy in information age and data age \nProcessing huge data needs high computing performance \nfrom technology perspective. Required performance has \nbecome lower for processing to wisdom side. On the other \nhand, handling the data required low performance in human \nefforts perspective Understanding the wisdom side needs \nmore hard effort from human capability. \nTherefore I would like to conceptualize big data as \nfollowing. Big Data is permanent data generated from whole \nelectronic signal during computing process. It has features \nwith volume, variety, velocity, complexity. It is basically nopened, shared and published to be accessible to any amateur \nanalyst. The values from big data are various according to \nanalyzer's insight. \nIII. POLICY DESIGN IN DIGITAL AGE \nThe key fact is openness of big data which has infinite \nvalue from these features. However there is also the potential \nrisk resulted from openness of big data. Big data is owned by \nboth all of us and none of us. Big data becomes a public \ncommons like grass in 'tragedy of the commons' suggested \nby Hardin\(1968 nbig data without discernment, all members of society can be \ndamaged. Therefore big data is a public good requiring \npublic engagement. We need to pay attention to the assertion \nof May\(1991 engage social \nproblems both actively and effectively. \nExisting mega information is created for specific purpose \nin private sector. But, big data is coincidentally generated \nwithout any objectives over public area. Because of public \nproperties of big data, potential risk could be increased as \ncompanies tried to seek private profit. Therefore big data is \none of important asset which needs government engagement \nand governance system. So, big data needs to be understood \nand controlled as SOC from policy perspective nPolicy design is different according to state's own \nresources. Hood\(1983 ndistinguished to nodality, authority, treasure, organization in \nTABLE II. \nTABLE II.  GOVERNMENT EFFECTRS nResource Nodality Authority Treasure Organization \nPolicy tool Messages Official token Moneys Treatments nMega information age \nwith weak resource \nBespoke \nmessage \nCustomized \npayments \nDirected \ntokens nIndividual \ntreatments \nBig data age \nwith rich resource \nBroadcast \nmessage Open payments \nBlanketed \ntokens nAt-large \ntreatments \nCitation: Hood, Christopher C. \(1983 R. \nHambleton. London: Macmillan, 18. reconstitution. \nNodality gives government the ability to traffic in ninformation on the basis of ‘figureheadeness’ or of having \nthe ‘whole picture’\(Simon, Smithburg & Tompson 1950 sent and \nreceived. Treasure gives government the ability to exchange, \nusing the ‘coin’ of ‘moneys’ and subject to a limit of \n‘fungibility’. Authority gives government the ability to \n‘determine’ in a legal or official sense, using tokens of \nofficial authority as the coin, and subject to a limit of legal \nstanding Organization gives government the physical ability \nto act directly, using its own forces rather than mercenaries. \nThe coin is ‘treatments’ or physical processing, and the \nlimiting factor is capacity Nodality’ woks on your \nknowledge and attitudes, ‘treasure’ on your bank balance, \n‘authority’ on your rights, status, and duties, and \n‘organization’ on you physical environment or even on your \n1027\nperson. Hood 1983 nage. \nAlmost solution has specified the shape of regulation for \npolicy implementation. Lessig\(1999 


that general \nregulation is combined with law, market, norm, \narchitecture\(a representative \nregulation in architecture area. Regulation structure of \nLessig\(1999 2. \nGeneral regulation \nstructure \(a b Technical regulation\(c 1999 NY: Basic \nbooks. reconstitution. \nNotation: . Social Problem \nFigure 2.  Difference of regulation structure nLaw based regulation are generally formed for solving \ntraditional problem such as financing issue and contract\(b traffics, Information & \nTechnology issues\(c to discuss the difference in policy \ndesign between big data and mega information to solve the \ndata sovereignty based on a case study of copyright and CCL \nin big data era. \n \n Policy Design of Copyright For Mega information \nStage ContentsG\nSocial issue Mega information in private sector \nCausal \nrelation nbackground: technical constrains, high cost \nreason: generation for particular purpose \nshape: private information\(centralized company for achieving profit \nPolicy tools \nnodality: fixed information \ntreasure: profit seeking \nauthority legal control \norganization: few company \nObjectives \n1957.law: copyright act \n1994.market: report reward system \n2007.technology: authentication algorithm \n2009.norm: National Assembly Library \nG\nThe Korean government made a decision that the \nNational Library will have authority to manage copyright  \npolicy since 2009. Ironically the National Library is now \nmanaging the copyright of online contents using CCL based \non the idea of openness and sharing. \n \n \n \nTABLE III.  POLICY DESIGN OF CCL FOR BIG DATA  \nStage Contents nSocial issue Big Data in public area \nCausal \nrelation \nbackground: high technology, low cost \nreason coincident creation \nshape: public data\(distributed, openness of  social network for value-oriented \nPolicy tools \nnodality: volume, velocity \ntreasure: values-oriented nauthority: autonomy \norganization: ex-post interpretation, complexity \nObjectives \n2001.norm: information sharing activity \n2002technology: tagging, web service, scripting \n2005.market: volunteer \n2005.law: citizens campaign \nG\nCCL is campaigning for data sharing in over 70 \ncountries and 100 non-profit foundations\(CC 2013 era. \nThe result of this case study is that there are four kinds \nof differences between big data and mega Table IV \nsummarizes the comparison in terms of background, cause, \nshape, usage perspectives. \nTABLE IV COMPARISON BETWEEN BIG DATA & MEGA-INFORMATION \nMega information Big Data \nBackground \n\(Technology of resources \n\(Low performance High technology Market purposes \n\(Profit seeking Value oriented Law information \n\(Centralized control Distributed autonomy Organization Previous definition \n\(Few experts Multiple Amateur from data for specific purposes. \nWe can get effective ROI when a few necessary data should \nbe translated into information under lack of resource and \ntechnology\(A-1 made from data. Therefore information was just  \ncreated for achieving specific purposes like profit seeking\(A-\n2 A-3 concept of \ninformation was clearly defined in order to avoid duplication \nfor the purpose of reducing the cost\(A-4 data is just huge data from all \nelectronic signal assembled during computing process. \nAssembling and processing big data has been possible by \nadvanced information and communication technologies. \nRecently the cost has been reduced for computing and also \nnetwork performance has been faster than before. So whole \ndata can be saved and used permanently\(B-1 data \nbecomes different\(B-2 ownership or \nmanagement\(B-3 B-4 rapid development of technology became a \nbackground to distinguish big data from conventional mega ninformation. The purpose of data generation was changing as \nmarket environments were evolving. Market trend have also \ninfluenced regulation law concerning data sovereignty. Data \naccessibility is no longer confined to few experts and spread \nto many amateur as a data analyst. The features of big data \nare described at TABLE V in terms of technology, law, \nmarket and organization perspective. \nTABLE V.  FEATURES OF BIG DATA \n Contents Features Utilization \nTechnology Available Volume, Velocity Public commons \nFree \nInexhaustibility \nMarket Contingency Variety \nLaw Open Distributed \nOrganization Amateur Complexity \n \nBig data leads public opinion We can find new \nopportunity and risk using big data such as weak signal. \nUtilization's effectiveness of big data is growing up. In this \ncase anyone can access and use big data for free. Big data \nnever has been 


exhausted. Therefore big data became public \ngoods under government's control.  \nV. CONCLUSION \nAS for the result of this case study, policy design for big \ndata should be distinguished from policy design for mega ninformation to solve the problem about data sovereignty.  \nIn law system perspective, big data autonomously is \ngenerated. It has been opened and shared without any \nintention. So there is no owner of big data. But problems \noccur always when we use any data for both profit seeking \nand risk prevention. So, we need to discuss about data \nsovereignty. \nIn market perspective, big data is created without any \nintention. Mega information is generated for special purpose \nwith consideration of budget and cost. In other hand, as big ndata has been generated inconsistently, the value of data \nbecomes variable according to analyzer's insight Therefore,  \nproper management is needed to enhance individual value \nwhen to make a analysis of big data in contrast to analyzing   \nmega information needing high cost. \nBig data has not any purpose, so it makes falsification \nwith any intention. Or it can be changed automatically with \nreference feature such as Linked of Data. So some issues \ncan be raised about responsibility and authenticity. \nBig data is generated in open and distributed way \nwithout any formation seen from technology perspective. So, \nwe need fresh conceptualization of big data to use it. We \nalso need to understand new technology about big data such \nas HDFS, NoSQL. \nFigure 3 describes the regulation structure of big data \nand mega information from law, market norm, and \narchitecture perspective. \n \nFigure 3.  Policy structure for Data Sovereignty \nImplications of this study are as follows. First of all, \npolicy for big data should be designed focused on technical \narchitecture under highly advanced technology environment. \nNext, it is necessary to establish an organization staffing ntechnical experts for designing technology-based policy. \nFinally, policy making needs to be implemented on a global \ndimension beyond individual states in big data era. \nREFERENCES \n[1] Moon, Hyejung & Cho, Hyun Suk 2012 4 63-82.\(in Korean   in Korean  A.. \(2011 3rd ed. \n\(Armonk, NY: M.E. Sharpe First edition, 2001; second edition \n2005  Zysman, John. \(1997 competition?. \nIndustry and Innovation 4\(2  2012 Big Data" \nSpending Spree.? 2 April. \n[6] Hardin, Garrett \(1968 n1243-1248. \n[7] Hood, Christopher C.. \(1983 London: Macmillan. \n[8] Lasswell, H. D. & Kaplan, A. \(1950 Inquiry. New York, NY: Yale University \nPress. \n[9] Lessig, Lawrence. \(1999 Cyberspace. New \nYork, NY: Basic books. \n[10] May, P. J.. \(1991 nPublics?. Journal of Public Policy, 11\(2  2006 representations of \nthe DIKW hierarchy", Journal of Information Science, 33\(2  Smithburg, D. W. and Thompson, V. A., Public \nAdministration \(Knopf, 1950  2013 Budget of E-government becomes Half  in \nthis year?. April, 03. \(in Korean  1987 Management support systems: towards \nintegrated knowledge management”. Human Systems Management, \n7\(1 59–70. \n1029\n 


tools by the industry, 7 \nwere referred to in the studies. Git, Jazz, Mercurial, Darcs, \nPerforce, Clearcase and Subversion \(TABLE V Mercurial were utilized in three academic \ndistributed projects, in which students were supposed to ncollaborate remotely. \nTABLE V INDUSTRIAL TOOLS SUPPORT \nTool Description Evidence \(S1-S22 distributed version control \nsystem, noted for its speed. \nTopology: Distributed \nCollaboration: Optimistic nS28, S29 \nJazz Source \nControl \n \nA platform for collaborative development \ncreated by IBM. Teams can choose to \nreplicate changes to separate RTC \nServers to allow for source code to be \nmastered in multiple locations for \navailability purposes. \nTopology: Centralized and Distributed   \nCollaboration: Optimistic and nPessimistic \nS5 \nJazz and \nFriendFeed \nThe Jazz client extension with a Java \nwrapper of the FriendFeed  API a  real-\ntime  feed  aggregator  that  consolidates  \nthe updates  from  a  number  of  social  \nnetworking websites. \nTopology: Centralized \nCollaboration: Optimistic and \nPessimistic \nS25 \nMercurial Mercurial is a distributed version control \nsystem, noted for its well-balanced \ncommand set. \nTopology: Distributed nCollaboration: Optimistic  \nS20 \nDarcs Darcs is a distributed version control \nsystem. \nTopology: Distributed nCollaboration: Optimistic \nS28 \nPerforce Teams at any location can transparently \nversion their work as part of a \ncollaborative workflow.  \nTopology: Distributed \nCollaboration: Optimistic and \nPessimistic \nS28 nRational \nClearcase \nClearcase is the market leader software \nconfiguration management solution that nprovides version control. ClearCase \nMultiSite enables file access across \nremote sites. \nTopology Centralized and Distributed \nCollaboration: Optimistic and \nPessimistic \nS28 \nSubversion Subversion is currently the most popular \ncentralized open source version control \nsystem.  \nTopology: Centralized nCollaboration: Optimistic and \nPessimistic \nS28 \n \nB. RQ2 – Which are the challenges related to Version nControl in DSD? \nThis question motivated the investigation of the \nchallenges that DSD projects face when it comes to Version \nControl. Thirteen challenges on this matter were gathered. \nThe challenges in the Version Control of DSD projects are \nsummarized in TABLE VI. The first column shows the \ncategories of challenges constructed from the data extracted \nfrom the evidences that are presented in the second column. \nThe frequencies show the number of occurrences of each \ncategory. Each occurrence was given the same weight, thus nthe frequencies merely reflect how many times a given \ncategory was identified in different studies, not how nimportant it may be. \nOne important finding is that the first challenge listed \nwas cited by half of the selected studies. Fourteen studies \nmentioned that dispersed software teams do not get \ninformation on what other teams are doing. For [S13], \ncurrent CM systems promote workspaces that isolate \ndevelopers from each other. \nAnother challenge, mentioned by nine studies, was the \nconflict detection delay. For [S16], only when a developer \nchecks in his changes, will his colleagues have access to \nthem and only when his colleagues synchronize their code \nwith the repository, will they become aware of new changes. \nVisualizing the traceability links between requirements \nwas mentioned by 5 studies as a version control challenge in \nDSD projects. Software artifact traceability is the ability to \ndescribe and follow the life of an artifact requirements, \ncode, tests, models, reports, plans, etc  n95\nWorking in different CM environments was a challenge \ncited by 3 studies. The decision to keep distinct CM nenvironments for each team brought together several \nconsequences [S15]. Communication delay was considered nproblematic in 2 studies. Projects with globally distributed \nmembers have to cope with communication delay due to \nphysical distance to the server [S7]. \nTABLE VI CHALLENGES DETECTED \nChallenge \(C1-C13 S1-S22 n\(Frequency: 11 collaboration, since conicts are only \ndetected when the engineers “check-in” their changes \n\(Frequency: 7 nS2, S6, S12, \nS16, S18, \nS21, S22, \nS24, S27 \nC3. It can be very difcult to visualize the traceability \nlink between requirements. \n \(Frequency: 5 n\(Frequency: 3 centralized topology \n\(Frequency: 2 ndistributed, there is always a risk that the  development  \nenvironment  starts  to  diverge  among  the sites. \n\(Frequency: 1 access  control  differently \n\(Frequency: 1 nbacked up if another developer updates them into his \nrepository. \n\(Frequency: 1 preparation time taken to set up CM \ninfrastructure \n\(Frequency: 1 repeatedly to \ndifferent tools: a developer makes changes to the source \ncode and files a bug report in the issue tracking system. \nAdditionally he might need to inform other developers \nabout them. \n\(Frequency: 1 nC11. Artifacts with different versions and content at each  \nsite \n\(Frequency: 1 


version  control  systems are  \nfocused    on    textual    \(i.e.,    source    code Frequency 1 Frequency: 1 challenges were reported as well, \nbut they were only mentioned in one study. Problems like \nnot enough preparation time taken to set up the CM \ninfrastructure, different versions of artifacts and each site, \nand dependency between the modules developed. \n \nC. RQ3 – Which strategies, techniques, models and \nprocesses are used to support in version control in \nDSD? \nThis section presents the models and frameworks \nproposed in the literature to support Version Control in DSD \nprojects. The first column shows the approach proposed to nVersion Control in DSD projects. The second column shows \nthe approaches’ concepts. The third column presents the \nevidence on the selected studies. \nTABLE VII PROPOSED APPROACHES  \nApproach Description Evidence \(S1-S22 nRepoGuard RepoGuard is a framework for \nintegration of development tools with \nsource code repositories RepoGuard is \nwritten in the Python programming \nlanguage, which allows for easy \nintegration of other tools The following \nversion control systems are currently \nsupported:  Subversion, Git, and Perforce.\nS9 nPeer-to-peer \nversion control\nThe decentralized peer-to-peer version \ncontrol system is built on top of  the p2p-\nframework FreePastry, which implements \nthe Pastry overlay routing and \nmaintenance. \nS7 \nConceptual nFramework  \nbased on \ncomponents  \nFramework for the evolution of software \nmodels in a collaborative modeling \nenvironment. It is built on BDI agent \narchitecture framework which aims to \nmaintain the consistency of project \nmodels and real-time conflict solving, \ngiven the changes made simultaneously \nby various devisor. \nS2 \nOSCAR \nOSCAR is an  architecture for  a  \ndistributed repository system to manage nactive artefacts. OSCAR defines active \nartefacts as two major components: the \nmeta-data that describes their properties \nand the content \(source code etc presentation, indexing, storage, \nmetrics. \nS23 \nChange \nSupport Model\nChange Support Model for distributed ncollaborative work is an approach that \nconstructs an information  repository  to \nprecisely reect  the state  of  work. It \nmanages  the  states  of  artifacts/products  \nmade  through collaborative work and the nstates of decisions made through \ncommunications. The information \nrepository allows detection of ninconsistencies  and  uncertainties. \nS24 \n \nTwo frameworks and approachs, and one architecture to \nVersion Control in DSD projects were identified. A \nframework for integration of development tools with source n96\ncode repositories was proposed, as well as a framework \nbased on agents to control changes. Furthermore one \ndecentralized peer-to-peer version control system was \nidentified, an architecture and a model that supports changes \nins distributed repository. \n \nD. RQ4 – How was the described research evaluated? \nThis question’s purpose was to determine how the \nselected works were evaluated. Among 29 selected studies, \nmost of them \(9 studies nuse examples, other 4 were not evaluated the proposal, and 3 \narticles are experience reports and experimental studies. \nOther 2 articles are systematic mapping. Only one study was \nevaluated combining both industry report and university \nreport, as shown in Figure 5. \n \nFigure 5 Research Evaluated \nV. CONCLUSIONS nThis section presents the final considerations and the \ndiscussions about the results. Furthermore discusses nlimitations of this research and further research. \nA. Discussion about the Results \nDSD is generally recognized  as  being  much more  \nchallenging  than  traditional  co-located  development. \nArticles mentioned in this study ensure that the distributed \nenvironment heightens the challenges faced in traditional nsoftware development. In this context, CM has a critical role. \nThe Version Control is used in all subsequent phases of \nsoftware planning and developing. Development, testing, \ndeployment, and installation are done based on the software \nconfiguration [1]. \nIn the distributed environment, a greater effort is required \nto guarantee that all the people involved have a perception of \nthe evolution of their work, and that the conflicts are \nresolved. Moreover, the people involved need to be sure that \nthey are working on the last version of the project artifacts, \nand that there is no room for inconsistency in the project. It \nis highly important that there is an aiding tool to version \ncontrol that keeps the artifacts consistency and improves the \nwork coordination. \nAn interesting reflection is made on [16], when the \nauthors say that “CM was put into the world exactly to \nhandle certain aspects of distribution on traditional projects”. \nThey say that rarely requirement engineers, designers, \ntesters, and programmers are sitting at the same place at the \nsame time.  \nNevertheless, there are only a few works on the topic that \nevidences what changes exactly in the Version Control for \nDSD projects, what the challenges related to the distributed \nenvironment are, and what are the proposed solutions to \nmitigate these challenges in this scenario.  \nThis research’s main question was How is Version \nControl performed in Distributed Software Development”. It \nwas asked in order to verify if the same tools used in \ntraditional software development are also used in the \ndistributed environment and whether they offer or not the \nneeded support. \nIn this sense, this work’s main contributions consists in a 


nmapping of the challenges and solution proposals to support \nversion control in the distributed development scenario: \n \n#1: Academic tools to support Version Control in \nDSD projects \nThis research evidenced that researchers are concerned \nabout allowing for a better perception of the work that is \nbeing developed by other remote team members, such as \nconstant conflict verification when it comes to Version \nControl in DSD Another spotted concern is present in the \nproject design phase, especially the evolutions of UML \nmodels nSeven academic tools have been identified \(ADAMS, \nPalantír, SYSIPHUS, ADAMS+STEVE, CoDesign and \nSyde, CASI and all of them are based in the client-server \nmodel. Whereas ADAMS, SYSIPHUS, ADAMS+STEVE, \nand CoDesign are mainly focused on collaborative modeling \nsupport, CASI, Palantír e Syde are focused on source code. \n \n#2 Industrial Tools to support Version Control in \nDSD projects   \nSeven version control tools that are popular in \nconventional software development were indicated by two \nselected studies. The authors used the Git, Jazz and \nMercurial tools in academic projects at which students \nworked in a distributed scenario.  \nJust a few studies identified in this research evinced the \nutilization of traditional co-located VCS. However, one ncannot state that these tools are not used at all, but it is fact \nthat their use is not frequently reported in the literature. \n \n#3: Obstacles detected by the searches regarding \nVersion Control in DSD \nThirteen challenges related to Version Constrol in DSD \nprojects have been identified, some of which were mentioned nmore times than other by the selected studies, as dispersed \nsoftware teams do not get information on what other teams \nare doing and the conflict detection delay.  \n97\nThe challenges that were mentioned several times are \nalso the ones that are the targets of the proposed aiding tools. \nPalantír, for instance, aims at providing a wider perception of \nthe work that is being developed in different workspaces. \n \n#4: Few works evaluated in industrial environments \nand experimental studies \nMost of the selected studies’ proposals were tested solely \nin academic environments or merely represent simple use \nexamples. This shows that is need a greater exchange \nbetween the academy and industry so that both can benefit \nfrom it. With this proximity thus, academics can level up the \nmaturity of their researches and propose more appropriate \nsolutions to the needs of software companies. \nB. Limitations \nThe main limitations and threats to the validity of this \nstudy lie in the fact that the mapping process was performed \nby only one researcher. This threat is considered acceptable \nby [14] for doctoral students that make use of this method. \nAccording to the text, it is sufficient that the thesis advisor \nengages in the protocol review and perform parts of the \nreview himself nIn addition, from the 11 studies selected from the \nresearch libraries, the backward snowballing and forward nsnowballing were used only twice. The first application of \nthe snowballing method resulted in 11 new studies. The \nsecond application of the snowballing method resulted in 7 \nnew studies. Although continuing to apply this technique in \nthese 7 new studies is expected from this research, it has not \nyet been possible due to time restrictions.  \nC. Further Research \nThe previously discussed limitations offer clear paths to nfurther research. The utilization of the snowballing method \nin the 7 new studies can enrich the information collected so \nfar on Version Conrol in DSD projects, with the \nempowerment of the evidence on the challenges and already \nlisted solutions, as well as new proposals. \nBesides, given the limited number of industrial reports, \nthe conduction of a survey in software companies with \ndistributed projects to identify how they perform the version \ncontrol may display clearer conclusions on how Version \nControl is performed in Distributed Software Development. \nACKNOWLEDGMENT \nThe authors would like to thank CAPES, CNPq, and \nFAPERJ for the financial support. \n \nREFERENCES \n[1]  E. Carmel, Global software teams: collaborating across borders nand time zones. Upper Saddle River, NJ, USA: Prentice Hall PTR, \n1999. \n [2]  L. Pilatti, J. L. N. Audy, and R. Prikladnicki, “Software \nconfiguration management over a global software development \nenvironment: lessons learned from a case study,” in Proceedings of \nthe 2006 international workshop on Global software development nfor the practitioner, New York, NY, USA, 2006, pp. 45–50.. \n[3]  S. ul Haq, “Issues in Global Software Development: A Critical \nReview,” Journal of Software Engineering and Applications, vol. \n04, no. 10, pp 590–595, 2011 \n[4]  M. Jiménez, M. Piattini, and A. Vizcaíno, “Challenges and \nImprovements in Distributed Software Development: A Systematic \nReview,” Advances in Software Engineering, vol. 2009, pp. 1–14, \n2009 n[5]  J. C. Binder, Global Project Management: Communication, \nCollaboration and Management Across Borders Gower \nPublishing, Ltd., 2007. \n[6]  B. Bruegge, A. D. Lucia, F. Fasano, and G. Tortora, “Supporting nDistributed Software Development with fine-grained Artefact \nManagement,” in Global Software Engineering 2006. ICGSE  ’06. \nInternational Conference on, 2006, pp. 213 –222. \n[7]  S. S. M. Fauzi, P. L. Bannerman, and M. Staples, “Software \nConfiguration Management in Global Software Development: A \nSystematic Map,” in Software Engineering Conference \(APSEC  C. Costa, A. C. C. França, and R. Prikladinicki, \n“Challenges and Solutions in Distributed Software Development \nProject Management: A Systematic Literature Review,” in 2010 5th \nIEEE International Conference 


on Global Software Engineering \n\(ICGSE  Medvidovic, N. \nKulkarni, G. M. Rama, and S. Padmanabhuni, “CoDesign: a highly \nextensible collaborative software modeling framework,” in 2010 \nACM/IEEE 32nd International Conference on Software \nEngineering, 2010 vol. 2, pp. 243 –246. \n[10]  B. O’sullivan, “Making sense of revision-control systems,” \nCommunications of the ACM, vol. 52, no. 9, pp. 56–62, 2009. \n[11]  D. Budgen, M. Turner, P. Brereton, and B. Kitchenham, “Using nmapping studies in software engineering,” in Proceedings of PPIG, \n2008, pp. 195–204. \n[12]  K. Petersen, R Feldt, S. Mujtaba, and M. Mattsson, “Systematic \nmapping studies in software engineering,” in 12th International \nConference on Evaluation and Assessment in Software \nEngineering, 2008, pp. 71–80. \n[13]  B. A Kitchenham and S. Charters, “Guidelines for performing \nSystematic Literature Reviews in  Software Engineering,” Keele \nUniversity and University of Durham, EBSE Technical Report \nVersion 2.3, 2007. \n[14]  J Webster and R. T. Watson, “Analyzing the Past to Prepare for \nthe Future: Writting a Literature Review,” 2002 n[15]  S. Jalali and C. Wohlin, “Systematic literature studies: database \nsearches vs. backward snowballing in Proceedings of the ACM-\nIEEE international symposium on Empirical software engineering \nand measurement New York, NY, USA, 2012, pp. 29–38. \n[16]  L. Bendix, J. Magnusson, and C. Pendleton, “Configuration nManagement Support for Distributed Software Development,” \npresented at the Proceedings of the Second International Software \nTechnology Exchange Workshop, Kista, Sweden, 2012. \nSELECTED STUDIES \n[S1] Berenbach and T. Wolf, “A unified requirements model; integrating \nfeatures, use cases, requirements, requirements analysis and hazard \nanalysis,” in Global Software Engineering, 2007. ICGSE 2007. \nSecond IEEE International Conference on, 2007, pp. 197 –203. \n[S2] H. K. Dam and A. Ghose, “An agent-based framework for distributed ncollaborative model evolution,” in Proceedings of the 12th \nInternational Workshop on Principles of Software Evolution and the \n7th annual ERCIM Workshop on Software Evolution, New York, \nNY, USA, 2011, pp. 121–130 n[S3] L. Bendix, J. Magnusson, and C. Pendleton, “Configuration \nManagement Stories from the Distributed Software Development \nTrenches,” in 2012 IEEE Seventh International Conference on Global \nSoftware Engineering ICGSE  ncollaborative synchronous UML modelling with fine-grained \n98\nversioning of software artefacts,” Journal of Visual Languages & \nComputing, vol. 18, no. 5, pp. 492–503, Oct. 2007. \n[S5] A. Meneely and L. Williams, “On preparing students for distributed \nsoftware development with a synchronous, collaborative development nplatform,” in Proceedings of the 40th ACM technical symposium on \nComputer science education, New York, NY USA, 2009, pp. 529–\n533. \n[S6] A. Sarma and A. van der Hoek, “Palantir: coordinating distributed \nworkspaces in Computer Software and Applications Conference, \n2002. COMPSAC 2002. Proceedings. 26th Annual International n2002, pp. 1093 – 1097. \n[S7] P. Mukherjee, C. Leng, W. W. Terpstra, and A. Schurr, “Peer-to-Peer \nBased Version Control,” in Parallel and Distributed Systems, 2008. \nICPADS  ’08. 14th IEEE International Conference on, 2008, pp. 829 \n–834. \n[S8] R. Kommeren and P. Parviainen, “Philips experiences in global \ndistributed software development,” Empirical Software Engineering, \nvol. 12, no. 6, pp. 647–660, 2007. \n[S9] M Legenhausen, S. Pielicke, J. Ruhmkorf, H. Wendel, and A. \nSchreiber, “RepoGuard: A Framework for Integration of \nDevelopment Tools with Source Code Repositories,” in Global \nSoftware Engineering, 2009. ICGSE 2009 Fourth IEEE International \nConference on, 2009, pp. 328 –331. \n[S10] S. S. M. Fauzi, P. L. Bannerman, and M Staples, “Software \nConfiguration Management in Global Software Development: A \nSystematic Map,” in Software Engineering Conference \(APSEC  F. Fasano, and G. Tortora, “Supporting \nDistributed Software Development with fine-grained Artefact nManagement,” in Global Software Engineering, 2006. ICGSE  ’06. \nInternational Conference on, 2006, pp. 213 222. \n[S12] J. young Bang, D. Popescu, G. Edwards, N. Medvidovic, N. \nKulkarni, G. M. Rama, and S Padmanabhuni, “CoDesign: a highly \nextensible collaborative software modeling framework,” in 2010 \nACM/IEEE 32nd International Conference on Software Engineering, \n2010, vol. 2, pp. 243 –246. \n[S13] A. Sarma Palantír: Enhancing Configuration Management Systems \nwith Workspace Awareness to Detect and Resolve Emerging nConflicts,” UNIVERSITY OF CALIFORNIA, Irvine, CA, 2008. \n[S14] A. Sarma, Z. Noroozi, and A. van der Hoek Palantir: raising \nawareness among configuration management workspaces,” in 25th \nInternational Conference on Software Engineering, 2003. \nProceedings, 2003, pp. 444 – 454. \n[S15] L. Pilatti, J. L. N. Audy, and R Prikladnicki, “Software \nconfiguration management over a global software development \nenvironment: lessons learned from a case study,” in Proceedings of \nthe 2006 international workshop on Global software development for \nthe practitioner, New York, NY, USA, 2006, pp. 45–50. \n[S16] L. Hattori and M. Lanza, “Syde: a tool for collaborative software \ndevelopment,” in 2010 ACM/IEEE 32nd International Conference on \nSoftware Engineering 2010, vol. 2, pp. 235 –238. \n[S17] B. Bruegge, A. H. Dutoit, and T. Wolf, “Sysiphus: Enabling informal ncollaboration in global software development,” in Global Software \nEngineering, 2006. ICGSE’06. International 


Conference on, 2006, pp. \n139–148. \n[S18] A. De Lucia, F. Fasano, R. Francese, and R. Oliveto, “Traceability nManagement in ADAMS,” in Proceedings of the International \nWorkshop on  Distributed Software Development 2005. \n[S19] L. Bendix, J. Magnusson, and C. Pendleton, “Configuration \nManagement Support for Distributed Software Development,” \npresented at the Proceedings of the Second International Software \nTechnology Exchange Workshop, Kista, Sweden, 2012. \n[S20] D. Rocco and W. Lloyd, “Distributed version control in the \nclassroom in Proceedings of the 42nd ACM technical symposium \non Computer science education, New York, NY, USA, 2011 pp. \n637–642. \n[S21] A. De Lucia, F. Fasano, R. Oliveto, and G. Tortora, “Fine-grained \nmanagement of software artefacts: the ADAMS system,” Software: \nPractice and Experience, vol. 40, no. 11, pp. 1007–1034, 2010. \n[S22 A. D. Lucia, F. Fasano, R. Oliveto, and G. Tortora, “Recovering \ntraceability links in software artifact management systems using \ninformation retrieval methods,” ACM Trans. Softw. Eng. Methodol., \nvol. 16, no. 4 Sep. 2007. \n[S23] C. Boldyreff, D. Nutter, and S. Rank, “Active artefact management \nfor distributed software engineering,” in Computer Software and \nApplications Conference, 2002. COMPSAC 2002. Proceedings. 26th \nAnnual International, 2002, pp. 1081–1086. \n[S24] P. T. T. Huyen and K. Ochimizu, “A Change Support Model for nDistributed Collaborative Work,” CoRR, 2012. \n[S25] F. Calefato, D. Gendarmi, and F. Lanubile, “Embedding social \nnetworking information into jazz to foster group awareness within \ndistributed teams,” in Proceedings of the 2nd international workshop \non Social software engineering and applications, New York, NY, \nUSA, 2009 pp. 23–28. \n[S26] A. Sarma, D. Redmiles, and A. van der Hoek, “Empirical evidence of \nthe benefits of workspace awareness in software configuration \nmanagement,” in Proceedings of the 16th ACM SIGSOFT nInternational Symposium on Foundations of software engineering, \nNew York, NY, USA, 2008, pp. 113–123. \n[S27 A. Sarma, D. F. Redmiles, and A. Van der Hoek, “Palantir: Early \ndetection of development conflicts arising from parallel code \nchanges,” Software Engineering, IEEE Transactions on, vol. 38, no. \n4, pp. 889–908, 2012 n[S28] J. Portillo-Rodríguez, A. Vizcaíno, M. Piattini, and S. Beecham, \n“Tools used in Global Software Engineering: A systematic mapping \nreview,” Information and Software Technology, vol. 54, no. 7, pp. \n663–685 Jul. 2012. \n[S29] X. Zhiguang, “Using Git to Manage Capstone Software Projects,” in \nThe Seventh International Multi-Conference on Computing in the \nGlobal Information Technology, Venice, Italy, 2012, p. 159 to 164. \n \n n99\n 


estimates with growth contingency applied from Table 4 nSpacecraft \(dry protection is designed to \nreduce faults that spin down the instrument. \n \nFigure 17. New NVM card for SMAP n  13\nand shielded cabling. The spacecraft structure could not be \nan adequate Faraday cage due to the large number of \npenetrations required. \nElectronics boxes were fabricated without joints where \npossible close-outs were sealed with EMI gaskets or EMI \ntape. Assemblies that were constrained to use high-density nD-connectors that leak emissions at L-band were treated \nwith conductive overwrap to provide a complete covering. \nPedestals for connectors on the C&DH assembly were \nadded to aid overwrap application, and a cover plate was \nadded to seal the gaps between faceplates of the individual \ncards in the box \(Figure 19 added other provisions to address EMC including a \ncabling design that assures a continuous Faraday cage nbetween electronics boxes using twisted wire pairs with a \nshielded jacket \(TPSJ TPSJ \nwith Laird tape provides better shielding at L-band than \nTPSJ with copper braid overwrap. Because radiated \nemissions can be directional and connectors are often a \nsource, the ICE was configured so that the connectors \npointed away from the reflector to further attenuate \nemissions that could reflect interfere with science \ninstruments. The BAPTA was identified by modeling to be a \nlikely source of leakage; therefore, the observatory structure \nabove the BAPTA was designed as a Faraday cage and the \nrotation pathway for emissions was directed into the \nspacecraft interior where any emissions would less likely \ninterfere with science measurements. \nExisting inherited designs \(most of the commercial space \nassemblies used on SMAP accepted without \nadditional modifications for EMC. EMC requirements were \nlevied on these assemblies but with the recognition some \nmay be found non-compliant. In these cases, SMAP will use \ntraditional EMI/EMC control techniques during integration \nand test in order to control emissions \(adding EMI tape or \ncloth to cover possible areas of leakage performs the \nkey on-orbit operations needed to implement the conical \nscanning scheme employed for data acquisition by the radar \nand radiometer. The observatory uses a zero momentum bias, \ndual-spin architecture to rotate its large antenna at a spin rate \nof 13–14.5 rpm, while the spacecraft bus provides a three-axis ncontrolled platform that maintains both itself and the \ninstrument section’s spin axis in a nadir-pointed orientation. \nThe major pointing and control functional aspects and design \ncharacteristics are illustrated in Figure 20. A key challenge \nFigure 18. EMC requirement in the radiometer band for \nassemblies outside the spacecraft structure. Inside the \nstructure, the lower limit is relaxed by 6 dB. \n \nFigure 20. SMAP pointing and control system functional aspects and design characteristics. \nFigure 19. Example of pedestals for the connectors on the \nC&DH assembly as well as a cover plate to seal the gaps \nbetween faceplates of the individual cards in the box. \n  14\nwith the spinning antenna stems from its large spin axis \nmoment of inertia, which at almost 240 kg-m2 is larger than \nthat of the spacecraft bus at about 190 kg-m2. \nFigure 21 shows the sensor and actuator suite and locations. \nThis control system configuration was informed by a prior ndesign concept for the Navy Remote Ocean Sensing System \n\(NROSS  similar \nsized-rotating antenna and nadir-pointing scheme \(NROSS \ndevelopment was halted after Preliminary Design Review in \nthe 1980s management, with \nmomentum compensation for the spun side and three-axis \ncontrol accomplished with a single set of four Reaction \nWheel Assemblies \(RWAs momentum compensation. In \na recent operational example of a nadir-pointing observatory \nemploying a momentum-compensated rotating antenna, \nWindSat/Coriolis, a dedicated momentum wheel was also \nused to counteract the antenna’s angular momentum, in a \nmanner similar to that planned for NROSS [21]. SMAP’s napproach provides a degree of functional redundancy for \nwheel failure and reduces control complexity nFigure 22 shows the system architecture, encompassing the \nattitude and spin rate determination functions attitude \ncontrol modes for both RWA and Reaction Control System \n\(RCS momentum \ncompensation, and the torque rod-based scheme employed \nfor RWA momentum management. For translational \nmaneuvers \(needed for orbit altitude maintenance used due to the \nmuch larger control authority offered by the thrusters. For \nnominal mapping operations, this system can control nadir \npointing errors due to precession and nutation to within \n0.5 deg, with a stability tolerance of ±0.3 deg \(3  and control is accommodating the flexible \nmodes, especially for the large antenna and its supporting \nboom while simultaneously controlling the antenna spin \nrate and nadir orientation to within the required tolerances. \nThis has been accomplished via careful engineering of the \nprimary frequencies associated with these various elements, \nto ensure adequate separation and avoid the potential for \ninterference or undesired 


resonance effects. Frequency \ndistribution of these system components is shown in Figure \n23 to illustrate this aspect of the design. The minimum 1st \nflexible mode frequencies of the solar array, as well as the nantenna and boom, became key design requirements on the \nstructure to ensure adequate separation from the spin control \nsystem and the observatory’s attitude control bandwidth. \n \nFigure 22. Pointing and control system architecture. \n \nFigure 21. Observatory sensor and actuator description.\n  15\n10. LAUNCH VEHICLE CHALLENGES \nThe launch vehicle selection process for SMAP involved \nboth programmatic and technical challenges to develop an \nobservatory design that was compatible with several \npotential medium- and large-class launch vehicles and to \naccommodate a launch vehicle selection late in the design \nlifecycle without stretching the development schedule or \ndelaying the launch ready date. SMAP’s strategy effectively \nisolated the observatory design from launch vehicle \nuncertainty by applying very conservative bounding design \nloads and environments designing to the most constraining \nfairing volume, and by developing a flexible launch vehicle \nadapter design to address vehicle-specific interface and \nmission design differences after launch vehicle selection nWhen SMAP began Formulation in September 2008, there \nwas a dearth of suitable, affordable medium-class launch \nvehicles commercially available [23]. SMAP was too \nmassive to be lofted on a Taurus XL. Delta II production \nhad been halted. The Falcon 9 had not yet flown. Evolved \nexpendable-class launch vehicles \(EELVs were costly and \ngrossly over-capable for SMAP, and SMAP’s tall stowed \nconfiguration effectively eliminated it as a co-manifest \noption on an EELV. The situation was further complicated \nbecause NASA’s Launch Services Program \(LSP NLS-2 new contract would be in \nplace and which vehicles would be initially available in the \ntimeframe SMAP expected a launch vehicle selection. \nThe Project also simultaneously explored whether a \npartnership with the DOD Space Test Program \(STP DOD agencies have high interest in \nusing SMAP data in their applications [3] and were strong \nadvocates to the STP for such a partnership. Under such a \npartnership, the DOD might have earlier access to critically nneeded soil moisture and freeze-thaw data to support their \napplications and NASA would realize a substantially lower \nmission cost. STP has occasional access to EELVs and more \nfrequent access to Minotaur IV vehicles. The Minotaur IV \nuses components with a significant flight heritage such as \nsurplus Peacekeeper stages, a Taurus fairing and attitude \ncontrol system, and a mix of Minotaur I, Pegasus, Taurus, \nand other Orbital Sciences standard avionics and software \n[24]. The use of surplus Peacekeeper stages means the nvehicle was defined by NASA to be a non-commercial \nlaunch system under the Commercial Space Act and ntherefore, it is not available for NASA acquisition. The Act \nallows for Minotaur use by DOD under certain ncircumstances that appeared potentially allowable for SMAP \nunder a DOD partnership. The Minotaur IV had not yet \nflown at that time, but it appeared to be a good potential \nmatch for SMAP. The potential cost benefit to NASA of a \nDOD partnership was substantial, especially given the \nuncertainties in the commercial acquisition process, and for \na time, the DOD partnership track seemed like the most \nlikely prospect for a launch vehicle for SMAP. For these \nreasons, SMAP strove to develop an observatory that \nmaintained Minotaur IV compatibility \(the “+” variant \nreplaces the Orion 38 upper stage with a Star 48 which \nprovides more lift capability preliminary and then \ndetailed final design until CDR in July 2012, when NASA \nannounced selection of a Delta II launch vehicle for SMAP. \nPersistent launch vehicle uncertainty drove SMAP towards \nmission and observatory designs that maintained \ncompatibility with several launch vehicles: Minotaur IV+, \nAtlas V, Falcon 9 and then later the Delta II when it was on-\nramped onto NLS-2 in October 2011. In most aspects, the \nMinotaur IV represented the most constraining vehicle \nchoice. The largest drivers were its 92-in fairing, and its lift ncapability constraint and non-restartable upper stage, which \nlimited its final orbit insertion capability for SMAP. \nTypically in early formulation before launch vehicle \nselection, missions employ enveloping and highly margined \ndesign environments for compatibility with likely launch \nvehicles. Launch vehicle selection is typically completed by \nmission PDR so that as the observatory enters the final \ndesign phase, expected launch load requirements decrease \naround the selected vehicle and maturing coupled loads and \nmission analyses. The easing of requirements allows fewer \ndesign iterations \(margin is ‘cashed in’ to address problems \n \nFigure 23. Frequency separation is the key to meet stability and performance requirements while not responding to \ndisturbances.  \n  16\nthat arise in the detailed design and highly margined loads environment \npersisted through the entire design lifecycle, which in turn \ndrove more design iterations to insure the design was \ncompatible with the higher loads cases. \nThe 92-in fairing constraint significantly drove the \nobservatory packaging to achieve a compact design. The \nspacecraft bus underwent several design iterations to package \navionics, power, and radar electronics designs as the 


nassociated electronics packaging designs matured, while also \naccommodating commercial space assemblies for other \nsubsystems [transponders, transmitters, miniature inertial \nmeasurement unit \(MIMUs etc The packaging \nrequirements contributed to decisions to develop the NVM \nslice within the C&DH to avoid having to accommodate a \nseparate solid state data recorder assembly. Harness design \nwas a challenge to accommodate difficult bends and close \nclearances. The structure design was also optimized to reduce \nmass to insure healthy margins against the Minotaur IV+ lift \ncapability. The spacecraft structure was designed with 7050 naluminum alloy and the structure includes a number of \nmachined cutouts and thin walls that are time-consuming to \nfabricate. Thermal control was also a challenge in such a \nsmall bus due to the power dissipations \(~1400 W complexity; but nearly every available exterior \nsurface is used as a radiator. \nPerhaps most significantly impacted by the 92-in fairing \nconstraint were the stowed RBA and solar array packaging. \nThe RBA has been the most sensitive assembly to launch \nvehicle uncertainty, undergoing several design iterations to \naddress stowed packaging and launch loads, and thermal \ndesign iterations to address evolving launch phase mission ndesign scenarios. The RBA along with the spin assembly is \nso intimately coupled to the observatory’s fundamental \narchitecture that these contracts were initiated early in Phase \nA; however the RBA is now the last flight assembly to be \ndelivered to observatory integration and test, less than one \nyear before launch because the start of flight manufacturing \nwas delayed until the design work could be stabilized and nconfirmed for the selected launch vehicle. \nThe solar array design was also driven by the 92-in fairing nconstraint. Unusually, SMAP’s solar array has no exposed \ncells when stowed. Because of the small bus size the \nadjacent sizes of the spacecraft to the solar array central \npanel are ‘busy’ and close clearances to the fairing envelope \nwould not allow for a ‘wrap around’ solar panel stowage \napproach. The solar array folds over on itself, placing the \noutboard cells facing inward. This feature places high \nemphasis on ensuring there are robust battery energy \nmargins available to accommodate solar array deployment \ncontingencies following launch vehicle separation. \nThe mission design for orbit insertion was also developed \ninitially around the Minotaur IV+ vehicle capability. The \nMinotaur IV+ upper stage is not restartable and to naccommodate disposal requirements, the initial target \nspecification placed the upper stage/observatory in an nelliptical orbit \(566 x 664 km timeframe, but \ndrove the spacecraft propulsion system to accommodate the \nadditional delta V \(and propellant capacity 685 km also enabled the maximum achievable \ndry mass on-orbit for the observatory, subject to the need for \n80-kg propellant tank capacity on board the spacecraft. \nTo accommodate the Delta II selection, the existing Launch nVehicle Adapter \(LVA interfaces \n\(Figure 24 upper stage is restartable, \nthe mission design was adapted to directly inject SMAP into \nits final science orbit. Additional secondary batteries were \nadded to the LVA to provide appropriate power margins \nduring the longer coast phase until separation and solar \narray deployment places the spacecraft on internal power n\(Minotaur IV+ injection time was 16 minutes, Delta II is 60 \nminutes increased mass and fairing \nvolume resources than the Minotaur IV+; unfortunately, \nwhen the Delta II was selected, the SMAP design was \n \nFigure 24. SMAP’s launch vehicle adapter design \naccommodates changes needed for late launch vehicle \nselection. \n  17\nmature and many subsystems were well into fabrication. It \nwas not cost effective to iterate through another design cycle \nto take advantage of the additional resources. The added \ndesign and development effort to remain compliant with the \nMinotaur IV+ have ultimately allowed SMAP to \naccommodate a late launch vehicle selection without \nstretching out the development schedule or launch ready \ndate, thereby allowing SMAP to retain its October 2014 \nlaunch ready date despite the launch vehicle uncertainty. \n11. LESSONS LEARNED \nSMAP has significantly benefited from the experience and \nlessons from Aquarius and SMOS for the science \nmeasurements and instrument design, and from MSL for the \navionics and power subsystem architectures. Personnel from \nthese predecessor missions were engaged early in SMAP’s ndevelopment so that their insight and lessons learned could \nbe integrated into the early architectural design stages. \nKey lessons learned from SMAP include: \n\(1 nplanned scenarios very early with regulatory functions \nand key stakeholders. Spaceborne L-band science nradars operate as secondary users within their allocated \nspectrum. The primary users, civil and defense aircraft \nterrestrial navigation systems, are increasingly wary \nabout potential interference from secondary users. \nSMAP conducted significant analysis and testing to \ndemonstrate very low interference risk potential 


and \ncoordinated the results with these users early. SMAP \nmade significant modifications to the radar design and \noperating approach to effectively eliminate risk of \ninterference to primary spectrum users. \n\(2 the launch vehicle early; this is particularly \nimportant for new observatory designs where the \nnumber of design iterations can be driven by assumed \nconstraints and conservative bounding environments \n\(especially if they persist into detail design packaging, repackaging to \nremain within fairing envelope, for instance iterations, and mission design \niterations to accommodate various launchers before a \nfinal launch selection was made. This was unavoidable \nfor SMAP given SMAP’s launch vehicle and other \ncircumstances.  \n\(3 Integrated observatory design—spacecraft and \ninstrument—SMAP is a highly integrated design that \nuniquely leverages spacecraft capabilities to simplify \ninstrument design and operation. This resulted in lower noverall design complexity and has also allowed for a \nmore reliable design by reducing the number of npossible major failure modes.  \n\(4 new design insights incrementally \nunfold and thereby often reveal new issues to be \nresolved within the design. Occasional team ‘resets’ \nwere imposed to reassess local requirements and design \ncomplexity within the context of the overall observatory \ndesign. These ‘resets’ often resulted in significant \nsimplification and in reduced development risk and \nuncertainty. \n\(5 napply resources to mature these design areas, and \nqualify critical essential electronics parts and nsubassemblies to reduce downstream risk. SMAP \nsuccessfully did this, aggressively applying resources to nradar, radiometer, avionics, spin/dynamics and control, \nand the RBA—the key development challenges for the nmission. As described earlier, the RBA and spin \nassemblies’ designs were fundamental to the \narchitectural definition and to enabling preliminary \ndesign to proceed, so these were selected early in \nFormulation and placed under contract. This reduced \nsubsystem architectural- and system-level redesign \ncycles that can set back progress. \n12. CONCLUSION \nThe SMAP observatory has been carefully designed to \naddress a number of unique challenges posed by the mission \nobjectives: \n? Achieving global coverage every 2–3 days with a single ninstrument and observatory \n? Achieving both high resolution and high soil moisture \naccuracy \n? Minimizing data loss or corruption from L-band \nterrestrial RFI \n? Using a deployable mesh reflector for L-band nradiometric measurements \n? Mechanical packaging of the large instrument antenna \nand spacecraft, with its associated structural design \ncompatibility with several small-to-medium class launch \nvehicles and to accommodate a relatively late vehicle \nselection without delaying launch. \n? Fault protection approach to minimize science \nmeasurement on-orbit down-time \n? Adapting planetary heritage avionics to an Earth science nmission application \n? Design for EMC to avoid L-band emissions that could \ndegrade science measurements \n Dynamics and pointing control of a large deployable \nspinning reflector \nThe design ensures that SMAP will provide high-quality \nscience data. \n  18\nREFERENCES \n[1] “Earth Science and Applications from Space: National nImperatives for the Next Decade and Beyond,” \nCommittee on Earth Science and Applications from \nSpace: A Community Assessment and Strategy for the \nFuture, National Research Council, The National \nAcademic Press 2007. ISBN-10: 0-309-14090-0, \nISBN-13: 987-0-309-14090-4. \n[2] JPL SMAP website: http://smap.jpl.nasa.gov n[3] Entekhabi, D., E. Njoku, P. O’Neill, K. Kellogg, et al., \n“The Soil Moisture Active Passive \(SMAP Mission,” \nProceedings of the IEEE, vol. 98, no. 5, May 2010. \n[4] Spencer, M., K. Wheeler, C. White, R. West J. \nPiepmeier, D. Hudson, and J. Medeiros, “The Soil \nMoisture Active Passive \(SMAP nRadar/Radiometer Instrument,” IEEE International \n2010 Geoscience and Remote Sensing Symposium \n\(IGARSS n[5] Camps, A., J. Gourrion, J. M. Tarongí, A. Gutiérrez, J. \nBarbosa, and R. Castro, “RFI Analysis in SMOS nImagery,” Proceedings of the 2010 IEEE International \nGeoscience and Remote Sensing Symposium, Honolulu, \nHI USA, July 2010, pp. 2007–2010. \n[6] Ruf, C., D. D. Chen, D. Le Vine, P. Matthaeis, and J. \nPiepmeier Aquarius Radiometer RFI Detection, \nMitigation and Impact Assessment,” Proceedings of the \n2012 IEEE International Geoscience and Remote \nSensing Symposium, Munich, Germany, July 2012. \n[7] Bradley, D., C Brambora, M. E. Wong, et al., “Radio-\nFrequency Interference \(RFI Active/Passive \(SMAP IGARSS International, July 2010. \n[8] Chan, S., M. Fischman, and M. Spencer, “RFI \nMitigation and Detection for the SMAP Radar,” \nGeoscience and Remote Sensing Symposium \n\(IGARSS  Spencer, M., S. Chan, E. Belz, J. Piepmeier, P. \nMohammed, and J. Johnson, “Radio Frequency \nInterference Mitigation for the Planned SMAP Radar \nand Radiometer,” Geoscience and Remote Sensing \nSymposium \(IGARSS 2011 IEEE International, pp. \n2440–2443. \n[10]  Jones, C., et al., “Analysis of Potential Interference \nfrom SMAP Radar Transmissions into FAA ARSR-3, \nARSR-4, and CARSR Radars in the 1215–1300 MHz \nBand and Interference Avoidance Strategies,” Jet \nPropulsion Laboratory internal document. \n[11] Huneycutt, B., S 


Hensley, G. Purcell, et al., “Test \nReport on the Effects of Pulsed Interference from L-\nband Spaceborne and Airborne Radars on GPS L2 \nReceivers,” Jet Propulsion Laboratory internal \ndocument, Nov. 1, 2010. \n[12 Sanders, F., et al., “NTIA Study regarding EMC \nbetween Proposed NASA SMAP Orbital Radar System \nand FAA ARSR Systems,” NTIA/ITS. \n[13] Mobrem, M., S. Kuehn, C. Spier, and E. Slimko, \n“Design and Performance of Astromesh Reflector \nOnboard Soil Moisture Active Passive Spacecraft,” \nIEEE 2012 Aerospace Conference, March 2012 n[14] Mohammed, P., SMAP L1B Radiometer Data Product \nAlgorithm Theoretical Basis Document, GSFC. \n[15 Spencer, M. W., C. W. Chen, H. Ghaemi, S. F. Chan, \nand J. E. Belz., “RFI Characterization and Mitigation \nfor the SMAP Radar,” TGARS, to be published. \n[16] West, R., SMAP L1 Radar Data Products Algorithm \nTheoretical Basis Document, Jet Propulsion Laboratory. \n[17] Heidecker, J., M. White, M. Cooper, D. Sheldon, F. \nIrom, and D. Nguyen, “Qualification of 128 Gb MLC \nNAND Flash for SMAP Space Mission,” Integrated \nReliability Workshop Final Report \(IRW  nQualification Guideline for Space Application” JPL \nPublication 12-1, Jet Propulsion Laboratory, Pasadena nCA, 2012. \n[19] Newson, S. L., and C. H. Huang, “SMAP \nElectromagnetic Compatibility \(EMC nPlan,” Jet Propulsion Laboratory Internal Project \nDocument, April 14, 2010. \n[20] Mak, P. H., “Pointing and Stabilization Issues of Large \nSpinning Antennas,” Proceedings of IEEE Position \nLocation and Navigation Symposium, Navigation into \nthe 21st Century, IEEE Plans, pp. 230–235, 1988. \n[21] Gaiser, P. W., K. M Germain, and E. M. Twarog, \n“WindSat—Spaceborne Remote Sensing of Ocean \nSurface Winds,” Proceedings of IEEE Oceans \nConference, vol. 1, p. 280, 2003. \n[22] Alvarez-Salazar, O. S., D. Adams, M. Milman, R. \nNayeri, S Ploen, L. Sievers, E. Slimko, and R. \nStephenson, “Precision Pointing Architecture of \nSMAP’s Large Spinning Antenna,” IEEE 2010 \nAerospace Conference, Big Sky, MT, to be published. \n[23] “Review of NASA’s Acquisition of Commercial \nLaunch Services,” NASA Office of Inspector General, \nFebruary 17, 2011. \n[24] “Minotaur IV Users Guide,” Release 1.1, Orbital \nSciences Corporation, January 2006. \n  19\nBIOGRAPHIES \nKent Kellogg received a B.S. in \nElectronic Engineering from \nCalifornia Polytechnic State \nUniversity, San Luis Obispo in n1983. He joined JPL’s Spacecraft \nAntenna Group in 1983 and is \ncurrently SMAP Project Manager. \nHis prior roles include managing \nJPL’s Telecommunication, Radar \nand Tracking Division, Spacecraft \nTelecommunications Equipment Section, SeaWinds/\nQuikSCAT Instrument and Project and supervising JPL’s \nSpacecraft Antenna Group nDr. Sam Thurman received B.S., \nS.M., and Ph.D. degrees in \nAerospace Engineering from \nPurdue University 1983 1985 1995 respectively. He jointed \nJPL in 1987 and is currently the \nSMAP Deputy Project Manager. \nHis prior roles include Deputy Manager of JPL’s \nAutonomous Systems Division, and several system \nengineering and management assignments in the Mars \nExploration Program. Before joining JPL, he worked as a \nmissile guidance analyst at the Charles Stark Draper \nLaboratory in Cambridge, Massachusetts.\t\nWendy Edelstein received her B.S. \ndegree in Electrical Engineering \nwith minor in Applied Mathematics \nfrom the University of California, \nSan Diego in 1988. She joined JPL \nin 1988 and is currently the SMAP \nInstrument Manager. Her prior \nroles include Deputy Section \nManager of the Radar Science & \nEngineering Section, Group \nSupervisor for the Advanced Radar Technology Group and \nradar technologist where her primary research interests \nincluded Transmit/Receive modules, RF component \nminiaturization and lightweight antenna technologies. \nDr. Michael Spencer received the nB.S. degree in Physics from the \nCollege of William and Mary, the \nM.S. degree in Planetary Science \nfrom The California Institute of \nTechnology, the M.S. degree in \nElectrical Engineering from the \nUniversity of Southern California, \nand a Ph.D. in Electrical and \nComputer Engineering from \nBrigham Young University. He joined JPL in 1990 and has\nserved as the SMAP Instrument System Engineer. He is \ncurrently the Deputy Manager of JPL’s Radar Science and \nEngineering Section; he is also the SMAP Instrument \nScientist. \nDr. Gun-Shing Chen received the \nB.S. degree in Mechanical \nEngineering from Cheng-Kung \nUniversity, Taiwan in 1977; an nM.S. degree in Aerospace \nEngineering from University of \nTexas at Austin in 1981; and a \nSc.D degree in Aeronautics and \nAstronautics from Massachusetts \nInstitute of Technology in 1986. He \njoined JPL’s Structures and Dynamics Research Group in \n1986 and is currently the SMAP Flight System Manager.\nHis prior roles include Group Supervisor, Section \nManager, Assistant Division Manager in the Mechanical \nSystem Division, Chief Engineer and Assistant Division \nManager in the Instruments and Science Data Systems \nDivision, Mechanical Project Element Manager \(PEM MICAS Microwave Limb Sounder \n\(MLS MER AMT received a \nB.S. Engineering from UCLA in \n1981 and Ph.D. in Chemical \nEngineering at the University of nPennsylvania in 1987. He joined \nJPL’s Spacecraft Power Section in \n1987 and is currently the SMAP \nMission Assurance Manager and is \nresponsible for safety, EEE parts, \nenvironments, reliability, software \nand 


hardware quality assurance. His prior roles include \nGroup Supervisor for the Power Electronics and Systems nGroup and Manager of the Power Systems Section. Prior to\njoining SMAP, he was the Deputy Mission Assurance nManager for the Juno project.  \n  20\nShawn Goodman received an M.S. \nin Mechanical Engineering from nRensselaer Polytechnic Institute. \nHe joined JPL’s Spacecraft \nMechanical Engineering Section in \n1991 and is currently SMAP \nProject System Engineer. His prior \nroles have included Group \nSupervisor, Deputy Section nmanager and Section manager of \nthe Spacecraft Mechanical Engineering Section. He has \nworked a number of flight projects including MSTI, \nSojourner, MGS, SIR-C, DS1, X2000, OPSP, MER, and \nMSL.\t\nBenhan Jai received his Masters \ndegree in Computer Engineering \nfrom the University of Southern \nCalifornia. He joined JPL in 1985 \nwhere he served in a number of \nengineering and management roles \ninvolving ground data system ndevelopment, mission operations \nsystem development, and mission \nmanagement. He is currently the \nSMAP Mission Systems Manager. His previous assignments \nhave included the development of the Ground Data System nand the Mission Operations System on many missions \nincluding NASA Scatterometer, SeaWinds, Mars Global nSurveyor, Stardust, Mars Odyssey, and Mars Surveyor \nOperations Projects. Prior to joining SMAP, Ben led the ndevelopment of mission operations and ground data \nsystems of the Mars Reconnaissance Orbiter Project \n\(MRO and led the entire flight teams of the MRO through \nlaunch, cruise, MOI, and aerobraking to the completion of nthe primary science phase of the mission.\t\nDr. Eni Njoku received the B.A. \ndegree in natural/electrical nsciences from the University of \nCambridge, U.K., in 1972 and the \nM.S. and Ph.D. degrees in \nelectrical engineering from MIT in \n1974 and 1976, respectively. He \njoined JPL in 1977 and he is \ncurrently the SMAP Project \nScientist, a JPL senior research \nscientist, and also supervisor of the Water and Carbon \nCycles Group. His prior roles have included managing \nJPL’s Geology and Planetology Section and serving as ndiscipline scientist for ocean and earth science data \nsystems at NASA. His primary research interests are in ndeveloping microwave remote sensing techniques for land \nsurface hydrology and climate. His current research nincludes microwave modeling, retrieval algorithm \ndevelopment, field experiments, and data analysis using nradiometers and radars. \nACKNOWLEDGEMENTS  \nThe work described in this paper was carried out at the Jet nPropulsion Laboratory, California Institute of Technology, \nunder a contract with the National Aeronautics and Space \nAdministration. Part of this work was also carried out at the \nNASA Goddard Space Flight Center. \n n \n 


Reinhartz-Berger, and A. Sturm, “OPCAT-\na bimodal CASE tool for object-process based system\ndevelopment,” in 5th International Conference on En-\nterprise Information Systems \(ICEIS 2003  Olsen, R. Haagmans, T. J. Sabaka, A. Kuvshinov,\nS. Maus, M. E. Purucker, M. Rother, V. Lesur, and\nM. Mandea The Swarm End-to-End mission simulator\nstudy : A demonstration of separating the various con-\ntributions to Earths magnetic field using synthetic data,”\nEarth, Planets, and Space, vol. 58, pp. 359–370, 2006.\n[10] R. M Atlas, “Observing System Simulation Exper-\niments: methodology, examples and limitations,” in\nProceedings of the WMO Workshop on the Impact of\nvarious observing systems on Numerical Weather Pre-\ndiction, Geneva Switzerland, 1997.\n[11] M. Adler, R. C. Moeller, C. S. Borden, W. D. Smythe,\nR. F. Shotwell, B. F. Cole, T. R Spilker, N. J.\nStrange, A. E. Petropoulos, D. Chattopadhyay, J. Ervin,\nE. Deems, P. Tsou, and J. Spencer Rapid Mission\nArchitecture Trade Study of Enceladus Mission Con-\ncepts,” in Proceedings of the 2011 IEEE Aerospace\nConference, Big Sky, Montana, 2011.\n[12] J. Hauser and D. Clausing, “The house of quality,”\nHarvard Business Review, no. May-June 1998, 1988.\n[13] T. L. Saaty, “Decision Making With the Analytic Hier-\narchy Process,” International Journal of Services Sci-\nences, vol. 1, no. 1, pp. 83–98, 2008.\n[14] A. M. Ross, D. E Hastings, J. M. Warmkessel, and\nN. P. Diller, “Multi-attribute Tradespace Exploration as\nFront End for Effective Space System Design,” Journal\nof Spacecraft and Rockets, vol. 41, no. 1, 2004.\n[15] W. L. Baumol On the social rate of discount,” The\nAmerican Economic Review, vol. 58, no. 4, pp. 788–\n802, 1968.\n[16] M. K Macauley, “The value of information : Measuring\nthe contribution of space-derived earth science data to\nresource management,” Journal of Environmental Eco-\nnomics and Management, vol. 22, pp. 274–282, 2006.\n[17 S. Jamieson, “Likert scales: how to \(ab Dec.\n2004.\n[18] R. C. Mitchell and R. T. Carson, Using Surveys to\nValue Public Goods: The Contingent Valuation Method,\nS. Aller, Ed. Washington DC: Library of Congress,\n1989.\n[19] O. C. Brown, P. Eremenko, and P. D Collopy, “Value-\nCentric Design Methodologies for Fractionated Space-\ncraft: Progress Summary from Phase 1 of the DARPA\nSystem F6 Program,” AIAA SPACE 2009 Conference &\nExposition, 2009.\n[20] a. Stoffelen, G. J Marseille, F. Bouttier, D. Vasiljevic,\nS. de Haan, and C. Cardinali, “ADM-Aeolus Doppler\nwind lidar Observing System Simulation Experiment,”\nQuarterly Journal of the Royal Meteorological Society,\nvol. 132, no. 619, pp 1927–1947, Jul. 2006.\n[21] A. Newell and H. A. Simon, Human Problem Solving.\nEnglewood Cliffs, NJ: Prentice Hall, 1972.\n[22] R. Lindsay, B. G. Buchanan, and E. A. Feigenbaum,\n“DENDRAL: A Case Study of the First Expert System\nfor Scientific Hypothesis Formation,” Artificial Intelli-\ngence, vol. 61, no. 2, pp. 209–261, Jun 1993.\n[23] B. G. Buchanan and E. H. Shortliffe, Rule-based Ex-\npert Systems: the MYCIN experiments of the Stanford\nHeuristic Programming Project. Addison-Wesley,\n1984.\n[24] P. Hart, R. Duda, and M. Einaudi PROSPECTORA\ncomputer-based consultation system for mineral explo-\nration,” Mathematical Geology, no. November 1977,\n1978.\n[25] J. McDermott, “R1: A Rule-Based Configurer of Com-\nputer Systems,” Artificial lntell., 19 39, vol. 19, no. 1,\npp. 39–88, Sep. 1982.\n[26] K. J. Healey, “Artificial Intelligence Research and Ap-\nplications at the NASA Johnson Space Center,” AI\nMagazine, vol. 7, no. 3, pp. 146–152, 1986.\n[27] C Forgy, “Rete: A fast algorithm for the many pat-\ntern/many object pattern match problem,” Artificial in-\ntelligence, vol. 19, no. 3597, pp. 17–37, 1982.\n[28] L. A. Zadeh, “Fuzzy Sets,” Information and Control,\nvol. 8, no. 3, pp. 338–353, Jan. 1965.\n[29] C. Haskins, “INCOSE Systems engineering handbook -\nA guide for system life cycle processes and activities,”\nSystems Engineering, 2006.\n[30] N. Das, D. Entekhabi and E. Njoku, “An Algorithm for\nMerging SMAP Radiometer and Radar Data for High-\nResolution Soil-Moisture Retrieval,” IEEE Transactions\non Geoscience and Remote Sensing, vol. 49, no. 99, pp.\n1–9, 2011.\n[31] A. Messac and A. Ismail-Yahaya, “Multiobjective ro-\nbust design using physical programming,” Structural\nand Multidisciplinary Optimization, vol. 23, no. 5, pp.\n357–371, Jun. 2002.\n20\n[32] B. Cameron, “Value flow mapping: Using networks\nto inform stakeholder analysis,” Acta Astronautica,\nvol. 62, no. 4-5, pp. 324–333 Feb. 2008.\n[33] R. Yager, “On ordered weighted averaging aggregation\noperators in multicriteria decision making,” Systems ,\nMan and Cybernetics, IEEE Transactions on, no. 1, pp.\n183–190, 1988.\n[34] J. Fortin, D Dubois, and H. Fargier, “Gradual Num-\nbers and Their Application to Fuzzy Interval Analysis,”\nIEEE Transactions on Fuzzy Systems, vol. 16, no. 2, pp.\n388–402, Apr. 2008.\n[35] H. Apgar, “Cost Estimating,” in Space Mission Engi-\nneering: The new SMAD. Hawthorne, CA: Microcosm,\n2011, ch. 11.\n[36] Chalmers University of Technology, “Use of P-band\nSAR for forest biomass and soil moisture retrieval,”\nEuropean Space Agency, Tech Rep., 2004.\n[37] D. Selva, “Rule-based system architecting of Earth\nobservation satellite systems,” PhD dissertation, Mas-\nsachusetts Institute of Technology, 2012.\n[38] H. H. Agahi, G. Ball, and G. Fox, “NICM Schedule &\nCost Rules of Thumb,” in AIAA Space Conference 2009,\nno. September, Pasadena, CA, 2009, pp 6512–6512.\nBIOGRAPHY[\nDaniel Selva received a PhD in Space\nSystems from MIT in 2012 and he is\ncurrently a post-doctoral associate in\nthe department of Aeronautics and As-\ntronautics at MIT. His research inter-\nests 


focus on the application of multi-\ndisciplinary optimization and artificial\nintelligence techniques to space systems\nengineering and architecture, in partic-\nular in the context of Earth observa-\ntion missions. Prior to MIT, Daniel worked for four years\nin Kourou \(French Guiana particular, he worked as a specialist in\noperations concerning the guidance, navigation and control\nsubsystem and the avionics and ground systems. Daniel has\na dual background in electrical engineering and aeronautical\nengineering, with degrees from Universitat Politecnica de\nCatalunya in Barcelona, Spain, and Supaero in Toulouse,\nFrance. He is a 2007 la Caixa fellow, and received the Nortel\nNetworks prize for academic excellence in 2002.\nEdward F. Crawley received an Sc.D. in\nAerospace Structures from MIT in 1981.\nHis early research interests centered on\nstructural dynamics, aeroelasticity, and\nthe development of actively controlled\nand intelligent structures. Recently, Dr.\nCrawley’s research has focused on the\ndomain of the architecture and design of\ncomplex systems. From 1996 to 2003\nhe served as the Department Head of\nAeronautics and Astronautics at MIT, leading the strategic\nrealignment of the department. Dr. Crawley is a Fellow of\nthe AIAA and the Royal Aeronautical Society \(UK is the\nauthor of numerous journal publications in the AIAA Journal,\nthe ASME Journal, the Journal of Composite Materials, and\nActa Astronautica. He received the NASA Public Service\nMedal. Recently, Prof Crawley was one of the ten members of\nthe presidential committee led by Norman Augustine to study\nthe future of human spaceflight in the US.\n21\n 


Mars program. It would be designed for low mass, lowpower and low temperature operation. The S-band antenna would be a smaller, simpler version of the antenna that flew on Deep Impact. Other antenna options would be available The UHF antennas have been flown on previous Mars lander/rover missions. There would be other alternatives for the S-band antenna and the UHF transceiver on the hub could use a larger power amplifier to talk to an orbiting asset as a backup to the S-band radio  Risk  The highest risk items for telecom would be the single string design for each element and six year design lifetime. However, the S-band radio has flight heritage. The UHF radios would be a new design but do not require new technology. They would be an engineering development 8. SYSTEM SUMMARY Mass Equipment List Table 5 shows a summary of the mass and power for each of the subsystems for the remote instrument units. The mass of one remote unit without the specified instrument is 26.6 kg with contingency specified at the subsystem level based on heritage. Table 6 shows a summary of mass and power by subsystem for the hub. The mass of the hub with contingency is 44.9 kg. Table 7 shows a mass summary for the entire package with appropriate contingencies added per the JPL  s Flight Project Practices and Design Principles Design Principles. The package totals 218.2 kg which includes four remote units, five instruments, one hub, and the carrier container\(s Table 5. Mass and power summary for remote units Remote Unit Mass CBE Contingency Total Power Power 14.3 kg 30% 18.5 kg 0.180 W 2 W Night /Day Thermal 2.0 kg 29% 2.5 kg 0 W Telecom UHF 0.2 24% 0.3 kg 2 W 40 W CDS 0.7 kg 30% 0.9 kg 1 W \(1/60th per hour 3 W Structure 3.4 kg 30% 4.4 kg 0 W Total x 1 unit 20.6 kg 29% 26.6 kg Diplexer S-Band Downconverter STDN command data to S/C CDS Pr oc es so r S-Band Exciter 9 dBi S-Band LGA UHF Downconverter Small UHF transceiver command data to S/C CDS 


to S/C CDS Pr oc es so r UHF PA UHF Monopole Command data to C&amp;DH Command dat  to C&amp;DH  Figure 7  Telecom block diagram for the S-band \(top bottom  units would be located on the hub while the remote units only contain a UHF system 15 9. OPERATIONAL SCENARIOS Daytime Operations During the day, the remote units and hub would be fully operational. The remote units would collect data from their instruments as specified by the science team and store it in the controller memory. Table 8 shows the data volume expected from each instrument. After 24 hours have passed the UHF telecom system on the hub is used to poll each of the remote units separately at the designated interval for the stored data. The hub then transmits the data direct-to-Earth using the S-band radio. This requires a maximum of eight hours at 50 kbps each day using the DSN 34 m antennas However, data rates as high as 120 kbps may be achieved reducing the downlink time. The hub has enough memory margin to accumulate data from all the instruments for three Earth days before it must downlink the data Nighttime Operations During nighttime operations, data collection at the remote units would be taking place. The magnetometer and seismometer collect data continuously. However, the seismometer operates at a reduced mode where the sampling rate is reduced to one-half of the daytime rate which has been deemed more than adequate by the science team. The remaining instruments collect data at various intervals that would be conducive to the science team  s current requirements. Telecom events would not be scheduled during the lunar night. The data accumulates in the controller memory over 16 Earth days \(~14 days at an equatorial location would be considered a worse case so two days have been added to be conservative data volume summary for each instrument during a 16 Earth day lunar night. When the sun comes up and the hub and remote units have sufficient power to run the telecom systems the hub polls each remote unit separately at a designated interval similar to operations during the day. The data would then be transmitted to Earth gradually over the next few days using the S-band radio Table 7. Mass summary for total package Unit Mass Contingency Mass + contingency 4 Remote Units 82.4 kg 29% 106.4 kg Hub 35.2 kg 27% 44.9 kg Instruments including cabling 17.3 kg 30% 22.5 kg Carrier Container\(s Total with heritage contingency 153.1 kg 29% 197.5 kg  System contingency  21.4 kg 14 Total Package  43% 218.9 kg Table 8. Instrument data volumes received at the hub over one Earth day in daylight operations Science Instrument Compressed Data Volume Received at Hub 


Volume Received at Hub Mb Seismometer 236 Magnetometer 58 Heat Flow Probe 2 Seismic Sounder 700 Instrument &amp; Hub Engineering Data 6 Total 1002 Hub Memory 5000 Margin 80  Table 6. Mass and power summary for hub Hub/Base Unit Mass CBE Contingency Total Power Power 14.3 kg 30% 18.5 kg 0.180 W 2 W Night /Day Thermal 13.4 kg 28% 17.2 kg 0 W Telecom UHF Telecom S-band 3.4 15% 3.9 kg 2 W 40 W CDS 0.7 kg 30% 0.9 kg 1 W \(1/60th per hour W \(day Structure 3.4 kg 30% 4.4 kg 0 W Total x 1 unit 35.2 kg 27% 44.9 kg 2.38 W avg at night  16 10. SUMMARY AND CONCLUSIONS The ALGEP modular design builds upon lessons learned from Apollo era ALSEP package and technology advances since that time. ALGEP meets the requirements of long lifetime survival while maintaining continuous operation of its instruments during the lunar night which can last up to 16 days at equatorial regions on the Moon. The package would be powered using solar arrays and batteries alone not requiring nuclear sources to supply power or maintain thermal control. This concept is feasible due to its lowpower operational mode at night The modular design and packaging scheme provides flexibility in deployment across all regions of the Moon including the farside pending the availability of an orbital communications asset. The relatively light ALGEP package could be accommodated on astronaut activity support vehicles, providing a method to distribute the packages across the Moon, ultimately gaining a Moon-wide understanding of lunar geophysical properties ACKNOWLEDGEMENTS This work was supported by the NASA Lunar Sortie Science Opportunities Program The work described in this publication was carried out at the Jet Propulsion Laboratory, California Institute of Technology under a contract with the National Aeronautics and Space Administration References herein to any specific commercial product process or service by trade name, trademark, manufacturer 


or otherwise does not constitute or imply its endorsement by the United States Government or the Jet Propulsion Laboratory, California Institute of Technology REFERENCES 1] NRC  Scientific Context for Exploration of the Moon   Washington D.C.: The Nat. Academies Press, 2007 2] Apollo 11 Prelim. Sci. Rept., NASA SP-214, 1969 3] Apollo 12 Prelim. Sci. Rept., NASA SP-235, 1970 4] Apollo 14 Prelim. Sci. Rept., NASA SP-272, 1971 5] Apollo 15 Prelim. Sci. Rept., NASA SP-289, 1972 6] Apollo 16 Prelim. Sci. Rept., NASA SP-315, 1972 7] Apollo 17 Prelim. Sci. Rept., NASA SP-330, 1973 8] ALSEP Termination Report, NASA RP-1036, 1979 9] NRC  New Frontiers in the Solar System: an Integrated Exploration Strategy  Decadal Survey D.C.: The Nat. Academies Press, 2003 10] International Lunar Network Science Definition Team Final Report, 2009 BIOGRAPHY Melissa Jones is a member of the technical staff in the Planetary and Lunar Mission Concepts Group at the Jet Propulsion Laboratory.  Current work includes development of small Lunar lander concepts and instrument packages to deploy on the Moon,  Report Manager for the Titan Saturn System Mission Outer Planets Flagship Mission study, and staffing various concept studies as a systems engineer on Team X, JPL  s mission design team.  Melissa graduated from Loras College with a B.S. in Chemistry and a Ph.D. in Space and Planetary Science from the University of Arkansas  Linda Herrell has a BA in math/computer science/languages \(University of Texas fluids and heat transfer \(City College of New York addition to analytical work in computer science and thermal and structural analysis, she has worked as both a payload \(instrument Earth orbiting \(Hubble Space Telescope, Earth Observing System \(EOS Cassini as Proposal Manager for several NASA science missions She currently serves as the Program Architect for NASA's New Millennium Program    Table 9. Instrument data volumes generated at the hub after 16 Earth day lunar night Science Instrument Compressed Data Volume Received at Hub Mb Seismometer 1980 Magnetometer 920 Heat Flow Probe 5 Seismic Sounder 0 Instrument &amp; Hub Engineering Data 72 Total 2977 Hub Memory 5000 Margin 40  17 Bruce Banerdt has been a research geophysicist at the California Institute of Technology's Jet Propulsion Laboratory since 1977, where he does research in planetary geophysics and instrument development for flight projects. He has been on science teams for numerous planetary missions 


on science teams for numerous planetary missions including Magellan, Mars Observer, Mars Global Surveyor and Rosetta. He was the US Project Scientist for the international Mars NetLander mission, for which he was also principal investigator of the Short-Period Seismometer experiment, and is currently the Project Scientist for the Mars Exploration Rovers. He led the Geophysics and Planetary Geology group at JPL from 1993-2005, and is the JPL Discipline Program Manager for Planetary Geosciences. He has held several visiting appointments at the Institut de Physique du Globe de Paris. He has a BS in physics and a PhD in geophysics from the University of Southern California  David Hansen is a member of the technical staff in the Communications Systems and Operations Group at the Jet Propulsion Laboratory. Current work includes the development of the telecom subsystem for the Juno project. David received a B.S. in Electrical Engineering from Cornell University and an M.S. in Electrical Engineering from Stanford University  Robert Miyake is a member of the technical staff in the Mission and Technology Development Group at the Jet Propulsion Laboratory. Current work includes the development of thermal control subsystems for interplanetary flagship missions to Jupiter and Saturn missions to Mars and the Earth Moon, and is the lead Thermal Chair for the Advanced Project Design Team Robert graduated with a B. S. from San Jose State University, with extensive graduate studies at UCLA University of Washington, and University of Santa Clara  Steve Kondos is a consultant to the Structures and Mechanisms group at the Jet Propulsion Laboratory. He currently is generating the mechanical concepts for small Lunar Landers and Lunar Science Instrument packages in support of various Lunar mission initiatives. He also provides conceptual design, mass and cost estimating support for various Team X studies as the lead for the Mechanical Subsystem Chair. Steve is also involved with various other studies and proposals and provides mentoring to several young mechanical and system engineers. He graduated with a B.S. in Mechanical Engineering from the University of California, Davis and has 28 years of experience in the aerospace field ranging from detail part design to system of systems architecture development. He has worked both in industry and in government in defense, intelligence commercial and civil activities that range from ocean and land based systems to airborne and space systems. Steve has received various NASA, Air Force, Department of Defense and other agency awards for his work on such projects as the NASA Solar Array Flight Experiment, Talon Gold, MILSTAR, Iridium, SBIRS, Mars Exploration Rovers ATFLIR, Glory Aerosol Polarimeter System and several Restricted Programs  Paul Timmerman is a senior member of technical staff in the Power Systems Group at the Jet Propulsion Laboratory Twenty-five years of experience in spacecraft design including 22 at JPL, over 250 studies in Team-X, and numerous proposals. Current assignments include a wide variety of planetary mission concepts, covering all targets within the solar system and all mission classes. Paul graduated from Loras College with a B.S. in Chemistry in 1983  Vincent Randolph is a senior engineer in the Advanced Computer Systems and 


the Advanced Computer Systems and Technologies Group at the Jet Propulsion Laboratory. Current work includes generating Command and Data Handling Subsystem conceptual designs for various proposals and Team X.  He also supports Articulation Control and Electronics design activities for the Advanced Mirror Development project. Vincent graduated from the University of California at Berkeley with a B.S. in Electrical Engineering 18  pre></body></html 


i models into time and covariate dependent dynamic counterparts  ii models and reliability analysis in a more realistic manner  iii level  whether or not functional components \(loyal generals diagnose correctly and take proper actions such as fault mask of failed components \(traitors asymmetric  iv survivability analysis. Evolutionary game modeling can derive sustainable or survivable strategies \(mapped from the ESS in EGT such as node failures such as security compromise level modeling in the so-called three-layer survivability analysis developed in Ma \(2008a this article  v offer an integrated architecture that unite reliability survivability, and fault tolerance, and the modeling approaches with survival analysis and evolutionary game theory implement this architecture. Finally, the dynamic hybrid fault models, when utilized to describe the survival of players in EGT, enhance the EGT's flexibility and power in modeling the survival and behaviors of the game players which should also be applicable to other problem domains where EGT is applicable  5. OPERATIONAL LEVEL MODELING AND DECISION-MAKING  5.1. Highlights of the Tactical and Strategic Levels  Let's first summarize what are obtainable at both tactical and strategic levels. The results at both tactical and strategic levels are precisely obtainable either via analytic or simulation optimization. With the term precisely, we mean that there is no need to assign subjective probabilities to UUUR events. This is possible because we try to assess the consequences of UUUR events \(tactical level ESS strategies \(strategic level time prediction of survivability. The following is a list of specific points. I use an assumed Wireless Sensor Network WSN  i of UUUR events: \(a actions which can be treated as censored events; \(b Cont' of Box 4.2 It can be shown that the replicator differential equations are equivalent to the classical population dynamics models such as Logistic differential equation and LotkaVolterra equation \(e.g., Kot 2001 Logistic equation, or the limited per capital growth rate is similar to the change rate of the fitness  xfxfi which can be represented with the hazard function or survivor functions introduced in the previous section on survival analysis.  This essentially connects the previous survival analysis modeling for lifetime and reliability with the EGT modeling. However, EGT provides additional modeling power beyond population dynamics or survival analysis approaches introduced in the previous section. The introduction of evolutionary theory makes the games played by a population evolvable. In other words, each player \(individual 


other words, each player \(individual agent and players interact with each other to evolve an optimized system Box 4.3. Additional Comments on DHF Models  The above introduced EGT models are very general given they are the system of ordinary differential equations. Furthermore, the choice of fitness function f\(x complexity to the differential equation system.  The system can easily be turned into system of nonlinear differential equations. The analytical solution to the models may be unobtainable when nonlinear differential equations are involved and simulation and/or numerical computation are often required  In the EGT modeling, Byzantine generals are the game players, and hybrid fault models are conveniently expressed as the strategies of players; the players may have different failure or communication behaviors Furthermore, players can be further divided into groups or subpopulations to formulate more complex network organizations. In the EGT modeling, reliability can be represented as the payoff \(fitness, the native term in EGT of the game. Because reliability function can be replaced by survivor function, survival analysis is seamlessly integrated into the EGT modeling. That is, let Byzantine generals play evolutionary games and their fitness reliability function  The evolutionary stable strategy \(ESS counterpart of Nash equilibrium in traditional games ESS corresponds to sustainable strategies, which are resistant to both internal mutations \(such as turning into treason generals or nodes such as security compromises represent survivable strategies and survivability in survivability analysis. Therefore, dynamic hybrid fault models, after the extension with EGT modeling, can be used to study both reliability and survivability 13 risks such as competing risks which can be described with CRA; \(c captured with the shard frailty.  We believe that these UUUR events are sufficiently general to capture the major factors/events in reliability, security and survivability whose occurrence probabilities are hard or impossible to obtain  Instead of trying to obtain the probabilities for these events which are infeasible in most occasions, we focus on analyzing the consequences of the events.  With survival analysis, it is possible to analyze the effects of these types of events on survivor functions. In addition, spatial frailty modeling can be utilized to capture the heterogeneity of risks in space, or the spatial distribution of risks \(Ma 2008a d UUUR events introduced previously. These approaches and models that deal with the effects of UUUR events form the core of tactical level modeling  To take advantage of the tactical level modeling approaches it is obviously necessary to stick to the survivor functions or hazard functions models. In other words, survival analysis can deal with UUUR events and offer every features reliability function provides, but reliability function cannot deal with UUUR events although survivor function and reliability function have the exactly same mathematical definition. This is the junction that survival analysis plays critical role in survivability analysis at tactical level. However, we 


recognize that it is infeasible to get a simple metric for survivability similar to reliability with tactical level modeling alone. Actually, up to this point, we are still vague for the measurement of survivability or a metric for survivability. We have not answered the question: what is our metric for survivability? We think that a precise or rigorous definition of survivability at tactical level is not feasible, due to the same reason we cited previously  the inability to determine the probabilities of UUUR events However, we consider it is very helpful to define a work definition for survivability at the tactical level  We therefore define the survivability at tactical level as a metric, Su\(t t function or reliability function with UUUR events considered. In the framework of three-layer survivability analysis, this metric is what we mean with the term survivability. The "metric" per se is not the focus of the three-layer survivability analysis. It is not very informative without the supports from the next two levels  strategic and operational models.  However, it is obvious that this metric sets a foundation to incorporate UUUR effects in the modeling at the next two levels  Due to the inadequacy of tactical level modeling, we proposed the next level approach  strategic level modeling for survivability. As expected, the tactical level is one foundation of strategic level modeling ii objectives: \(a affect survivability which survival analysis alone is not adequate to deal with; \(b survivability at tactical level is necessary but not sufficient for modeling survivability, we need to define what is meant with the term survivability at strategic level  With regard to \(a behaviors or modes which have very different consequences. These failure behaviors can be captured with hybrid fault models. However, the existing hybrid fault models in fault tolerance field are not adequate for applying to survivability analysis. There are two issues involved: one is the lack of real time notion in the constraints for hybrid fault models \(e.g., N&gt;3m+1 for Byzantine Generals problem synthesize the models after the real-time notions are introduced. The solution we proposed for the first issue is the dynamic hybrid fault models, which integrate survivor functions with traditional hybrid fault models. The solution we proposed for the second issue is the introduction of EGT modeling  With regard to \(b modeling our problem at strategic level, EGT modeling is essentially a powerful optimization algorithm.  One of the most important results from EGT modeling is the so-called evolutionary stable strategies \(ESS We map the ESS in EGT to survivable strategies in survivability analysis.   Therefore, at the strategic level, our work definition for survivability refers to the survivable strategies or sustainable strategies in the native term of EGT, which can be quantified with ESS  In addition to integrating dynamic hybrid fault models another advantage for introducing EGT modeling at strategic level is the flexibility for incorporating other node behaviors \(such as cooperative vs. non-cooperative those behaviors specified in standard hybrid fault models, as well as anthropocentric factors such as costs constraints  Without UUUR events, both tactical and strategic level 


Without UUUR events, both tactical and strategic level models default to regular reliability models. This implies that, in the absence of UUUR events, reliable strategies are sustainable or survivable.  This also implies that three-layer survivability analysis defaults to reliability analysis however, the three-layer approach does offer some significant advantages over traditional reliability analysis, as discussed in previous sections. Nevertheless, when UUUR events exist, reliable strategies and survivable strategies are different. This necessitates the next operational level modeling  5.2. Operational Level Modeling and Decision-Making  When UUUR events are involved, we cannot make real time predictions of survivability at tactical and strategic levels This implies that the implementations of survivable 14 strategies need additional measures that we develop in this section.  Box 5.1 explains the ideas involved with possibly the simplest example  Figure 4 is a diagram showing a simplified relationship between action threshold survivability \(TS survivability \(ES view since both TS and ES are multidimensional and dynamic in practice. Therefore, the sole purpose of the diagram is to illustrate the major concepts discussed above The blue curve is the survivability when survivable strategies specified by ESS are implemented at some point before time s.  The system is then guaranteed to hold survivability above ES. In contrary, if no ESS implemented before time s, then the system quickly falls below to the survivable level at around 40 time units  T i m e 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 1 0 0 Su rv iv ab ili ty M et ric S u t 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 E S S  i s  I m p lm e n t e d N o  E S S  is  I m p lm e n t e d ts E S T S  Figure 4. A Diagram Showing the Relationship Between TS and ES, as well as timing of s and t, with s &lt; t  6. SUMMARY  The previous sections discussed the major building blocks 


The previous sections discussed the major building blocks for the new life-system inspired PHM architecture. This section first identifies a few minor aspects that have not been discussed explicitly but are necessary for the implementation of the architecture, and then we summarize the major building blocks in a diagram  6.1. Missing Components and Links  Optimization Objectives  Lifetime, reliability, fault tolerance, and survivability, especially the latter two, are application dependent. Generally, the optimization of reliability and survivability are consistent; in that maximization of reliability also implies maximization of survivability. However, when application detail is considered, optimization of lifetime is not necessarily consistent with the optimization of reliability. Consider the case of the monitoring sensor network as an example. The network reliability is also dependent on connectivity coverage, etc, besides network lifetime. What may be further complicated is the time factor. All of the network metrics are time-dependent. A paradoxical situation between lifetime and reliability could be that nodes never 'sleep                                                   


          Box 5.1 Operational Level Modeling  Assuming that the ESS solution for a monitoring sensor network can be expressed with the following simple algebraic conditions: survivability metric at tactical level SU = 0.7, Router-Nodes in the WSN &gt; 10%, Selfish Nodes &lt; 40%. Even with this extremely simplified scenario, the ESS strategies cannot be implemented because we do not know when the actions should be taken to warrant a sustainable system.  These conditions lack a correlation with real time  The inability to implement ESS is rooted in our inability to assign definite probabilities to UUUR events, which implies that we cannot predict when something sufficiently bad will jeopardize the system survivability What we need at the operational level is a scheme to ensure ESS strategy is in place in advance  The fundamental idea we use to implement the ESS strategy is to hedge against the UUUR events. The similar idea has been used in financial engineering and also in integrated pest management in entomology. This can be implemented with the following scheme  Let us define a pair of survivability metrics: one is the expected survivability \(ES threshold survivability or simply threshold survivability \(TS ES is equivalent to the survivability metric at tactical level. ES corresponds to ESS at strategic level, but they are not equivalent since ESS is strategy and ES is survivability. TS is the survivability metric value \(at tactical level and TS can be obtained from strategic level models. For example, TS = SU\(s t condition for the implementation of ESS. In other words, the implementation of strategies that ensures TS at time s will guarantee the future ES level at time t.  To make the implementation more reliable and convenient multiple dynamic TSs can be computed at time s1, s2 sk, with si &lt; t for all i.  These TS at times s1, s2, ..., sk should be monitored by some evaluation systems  Unlike tactical and strategic levels, the operational level modeling is approximate. The term "approximate means that we cannot predict the real time survivability or we do not know the exact time an action should be taken. Instead, the action is triggered when the monitored survivability metric SU\(r survivability \(TS scheme of TS and ES, we ensure the ES by taking preventative actions \(prescribed by ESS and triggered by the TS consequences of UUUR events  Figure 4 is a diagram showing the above concepts and the decision-making process involved 15 This wakefulness \(never 'sleep short period but at the expense of network lifetime. Of course, when the network is running out of lifetime, network reliability ultimately crashes. This example reminds us that 


reliability ultimately crashes. This example reminds us that multi-objective optimization should be the norm rather than exception  Constraints and Extensions  Many application specific factors and constraints are ignored in this article. For example, we mentioned about spatial heterogeneity of environment, but never present a mathematical description The spatial heterogeneity can be modeled with the so-called spatial frailty in multivariate survival analysis \(Ma 2008a  Evolutionary Algorithm  Evolutionary game modeling when implemented in simulation, can be conveniently implemented with an algorithm similar to Genetic Algorithms \(GA ESS in the evolutionary game model with simulation is very similar to GA. Dynamic populations, in which population size varies from generation to generation \(Ma &amp; Krings 2008f of node failures. Another issue to be addressed is the synchronous vs. asynchronous updating when topology is considered in the simulation. This update scheme can have profound influences on the results of the simulation. Results from cellular automata computing should be very useful for getting insights on the update issue  6.2. Summary and Perspective  To recapture the major points of the article, let us revisit Figure 3, which summarizes the principal modules of the proposed life-system inspired PHM architecture. The main inspiration from life systems is the notion of individuals and their assemblage, the population. Population is an emergent entity at the next level and it has emergent properties which we are often more concerned with. Survival analysis, which has become a de facto standard in biomedicine, is particularly suitable for modeling population, although it is equally appropriate at individual level. Therefore, survival analysis \(including competing risks analysis and multivariate survival analysis comprehensively in the context of PHM in a series of four papers presented at IEEE AeroSpace 2008 \(Ma &amp; Krings 2008a, b, c, &amp; d proposed architecture. Survival analysis constitutes the major mathematical tools for analyzing lifetime and reliability, and also forms the tactical level of the three-layer survivability analysis  Besides lifetime and reliability, two other major modules in Figure 3 are fault tolerance and survivability. To integrate fault tolerance into the PHM system, Dynamic Hybrid Fault DHF 2008e, Ma 2008a make real-time prediction of reliability more realistic and make real-time prediction of fault tolerance level possible DHF models also unite lifetime, reliability and fault tolerance under a unified modeling framework that consists of survival analysis and evolutionary game theory modeling  DHG models also form the partial foundation, or strategic level, for the three-layer survivability analysis. At the strategic level, the Evolutionary Stable Strategies \(ESS which is mapped to survivable or sustainable strategies, can be obtained from the evolutionary game theory based DHF models. When there is not any UUUR event involved reliability and survivability are consistent, and reliable strategies are survivable. In this case, the strategic level modeling up to this point is sufficient for the whole PHM system modeling, and there is no need for the next level  operational level modeling  When there are UUUR events in a PHM system, the 


When there are UUUR events in a PHM system, the inability to determine the occurrence probabilities of UUUR events makes the operational level modeling necessary Then the principle of hedging must be utilized to deal with the "hanging" uncertainty from UUUR events. In this case reliability strategies are not necessarily survivable strategies At the operational level modeling, a duo of survivability metrics, expected survivability \(ES survivability \(TS the survivable strategies \(ESS level are promptly implemented based on the decisionmaking rules specified with the duo of survivability metrics then the PHM system should be able to endure the consequences of potentially catastrophic UUUR events. Of course, to endure such catastrophic events, the cost may be prohibitively high, but the PHM system will, at least, warn decision-makers for the potentially huge costs.  It might be cheap to just let it fail  Figure 3 also shows several other modules, such as security safety, application systems \(such as Automatic Logistics CBM+, RCM, Life cycle cost management, Real-time warning and alert systems architectures, but we do not discuss in this paper. Generally the new architecture should be fully compatible with existing ones in incorporating these additional modules. One point we stressed is that PHM system can be an ideal place to enforce security policies. Enforcing security policies can be mandatory for PHM systems that demand high security and safety such as weapon systems or nuclear plant facilities.  This is because maintenance, even without human-initiated security breaches, can break the security policies if the maintenance is not planned and performed properly  In perspective, although I did not discuss software issues in this paper, the introduced approaches and models should provide sufficient tools for modeling software reliability and survivability with some additional extension. Given the critical importance of software to modern PHM systems, we present the following discussion on the potential extension to software domain. Specifically, two points should be noted: \(1 architecture to software should be a metric which can 16 replace the time notion in software reliability; I suggest that the Kolmogorov complexity \(e.g., Li and Vitanyi 1997 be a promising candidate \(Ma 2008a change is because software does not wear and calendar time for software reliability usually does not make much sense 2 software reliability modeling.  Extending to general survivability analysis is not a problem either. In this article I implicitly assume that reliability and survivability are positively correlated, or reliability is the foundation of survivability. This positive correlation does not have to be the case. A simplified example that illustrates this point is the 'limit order' in online stock trading, in which limit order can be used in either direction: that stock price is rising or falling.  The solution to allow negative or uncorrelated relationships between reliability and survivability are very straightforward, and the solutions are already identified in previous discussions. Specifically, multiple G-functions and multi-stage G-functions by Vincent and Brown \(2005 very feasible solution, because lifetime, reliability and survivability may simply be represented with multiple Gfunctions. Another potential solution is the accommodation of the potential conflicts between reliability and survivability with multi-objective GA algorithms, which I previously suggested to be used as updating algorithms in the optimization of evolutionary games  


 The integration of dynamic hybrid fault models with evolutionary game modeling allows one to incorporate more realistic and detailed failure \(or survival individual players in an evolutionary game. This is because dynamic hybrid fault models are supported by survival analysis modeling, e.g., time and covariate dependent hazard or survivor functions for individual players. If necessary, more complex survival analysis modeling including competing risks analysis and multivariate survival analysis, can be introduced.  Therefore, any field to which evolutionary game theory is applicable may benefit from the increased flexibility in modeling individual players.  Two particularly interesting fields are system biology and ecological modeling.  In the former field, dynamic hybrid fault models may find important applications in the study of biological networks \(such as gene, molecular, and cell networks 2008g conjecture that explains the redundancy in the universal genetic code with Byzantine general algorithm. In addition they conducted a comparative analysis of bio-robustness with engineering fault tolerance, for example, the strong similarity between network survivability and ecological stability \(Ma &amp; Krings 2008g survivability analysis can be applied for the study of survivals or extinctions of biological species under global climate changes \(Ma 2008b  In this paper, I have to ignore much of the details related to the implementation issues to present the overall architecture and major approaches clearly and concisely. To deal with the potential devils in the implementation details, a well funded research and development team is necessary to take advantages of the ideas presented here. On the positive side I do see the great potential to build an enterprise PHM software product if there is sufficient resource to complete the implementation. Given the enormous complexity associated with the PHM practice in modern engineering fields, it is nearly impossible to realize or even demonstrate the benefits of the architecture without the software implementation. The critical importance of PHM to mission critical engineering fields such as aerospace engineering, in turn, dictates the great value of such kind software product  6.3. Beyond PHM  Finally, I would like to raise two questions that may be interested in by researchers and engineers beyond PHM community. The first question is: what can PHM offer to other engineering disciplines? The second question is: what kinds of engineering fields benefit most from PHM? Here, I use the term PHM with the definition proposed by IEEE which is quoted in the introduction section of the paper  As to the first question, I suggest software engineering and survivability analysis are two fields where PHM can play significant roles. With software engineering, I refer to applying PHM principles and approaches for dealing with software reliability, quality assurance, and even software process management, rather than building PHM software mentioned in the previous subsection. For survivability analysis, borrowing the procedures and practices of PHM should be particularly helpful for expanding its role beyond its originating domain \(network systems that control critical national infrastructures is a strong advocate for the expansion of survivability analysis to PHM. Therefore, the interaction between PHM and survivability analysis should be bidirectional. Indeed, I see the close relationships between PHM, software engineering, and survivability as well-justified because they all share some critical issues including reliability survivability, security, and dependability  


 The answer to the second question is much more elusive and I cannot present a full answer without comparative analysis of several engineering fields where PHM has been actively practiced. Of course, it is obvious that fields which demand mission critical reliability and dependability also demand better PHM solutions. One additional observation I would like to make is that PHM seems to play more crucial roles for engineering practices that depend on the systematic records of 'historical' data, such as reliability data in airplane engine manufacturing, rather than on the information from ad hoc events.  This may explain the critical importance of PHM in aerospace engineering particularly in commercial airplane design and manufacturing.  For example, comparing the tasks to design and build a space shuttle vs. to design and manufacture commercial jumbo jets, PHM should be more critical in the latter task  17    Figure 2. States of a monitoring sensor node and its failure modes \(after Ma &amp; Krings 2008e     Figure 3. Core Modules and their Relationships of the Life System Inspired PHM Architecture    REFERENCES  Adamides, E. D., Y. A. Stamboulis, A. G. Varelis. 2004 Model-Based Assessment of Military Aircraft Engine Maintenance Systems Model-Based Assessment of Military Aircraft Engine Maintenance Systems. Journal of the Operational Research Society, Vol. 55, No. 9:957-967  Anderson, R. 2001. Security Engineering. Wiley  Anderson, R. 2008. Security Engineering. 2nd ed. Wiley  Bird, J. W., Hess, A. 2007.   Propulsion System Prognostics R&amp;D Through the Technical Cooperation Program Aerospace Conference, 2007 IEEE, 3-10 March 2007, 8pp  Bock, J. R., Brotherton, T., W., Gass, D. 2005. Ontogenetic reasoning system for autonomic logistics. Aerospace Conference, 2005 IEEE 5-12 March 2005.Digital Object Identifier 10.1109/AERO.2005.1559677  Brotherton, T., P. Grabill, D. Wroblewski, R. Friend, B Sotomayer, and J. Berry. 2002. A Testbed for Data Fusion for Engine Diagnostics and Prognostics. Proceedings of the 2002 IEEE Aerospace Conference  Brotherton, T.; Grabill, P.; Friend, R.; Sotomayer, B.; Berry J. 2003. A testbed for data fusion for helicopter diagnostics and prognostics. Aerospace Conference, 2003. Proceedings 2003 IEEE  Brown, E. R., N. N. McCollom, E-E. Moore, A. Hess. 2007 Prognostics and Health Management A Data-Driven Approach to Supporting the F-35 Lightning II. 2007 IEEE AeroSpace Conference  Byington, C.S.; Watson, M.J.; Bharadwaj, S.P. 2008 Automated Health Management for Gas Turbine Engine Accessory System Components. Aerospace Conference 2008 IEEE, DOI:10.1109/AERO.2008.4526610 


2008 IEEE, DOI:10.1109/AERO.2008.4526610 Environment Covariates &amp; Spatial Frailty Applications: AL; Life Cycle Mgmt; Real-Time Alerts CBM+, RCM, TLCSM; Secret Sharing and Shared Control 18 Chen, Y. Q., S. Cheng. 2005. Semi-parametric regression analysis of mean residual life with censored survival data Biometrika \(2005  29  Commenges, D. 1999. Multi-state models in Epidemiology Lifetime Data Analysis. 5:315-327  Cook, J. 2004. Contrasting Approaches to the Validation of Helicopter HUMS  A Military User  s Perspective Aerospace Conference, 2004 IEEE  Cook, J. 2007. Reducing Military Helicopter Maintenance Through Prognostics. Aerospace Conference, 2007 IEEE Digital Object Identifier 10.1109/AERO.2007.352830  Cox, D. R. 1972. Regression models and life tables.  J. R Stat. Soc. Ser. B. 34:184-220  Crowder, M. J.  2001. Classical Competing Risks. Chapman amp; Hall. 200pp  David, H. A. &amp; M. L. Moeschberger. 1978. The theory of competing risks. Macmillan Publishing, 103pp  Ellison, E., L. Linger, and M. Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013  Hanski, I. 1999. Metapopulation Ecology. Oxford University Press  Hallam, T. G. and S. A. Levin. 1986. Mathematical Ecology. Biomathematics. Volume 17. Springer. 457pp  Hess, A., Fila, L. 2002.  The Joint Strike Fighter \(JSF concept: Potential impact on aging aircraft problems Aerospace Conference Proceedings, 2002. IEEE. Digital Object Identifier: 10.1109/AERO.2002.1036144  Hess, A., Calvello, G., T. Dabney. 2004. PHM a Key Enabler for the JSF Autonomic Logistics Support Concept. Aerospace Conference Proceedings, 2004. IEEE  Hofbauer, J. and K. Sigmund. 1998. Evolutionary Games and Population Dynamics. Cambridge University Press 323pp  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Huzurbazar, A. V. 2006. Flow-graph model for multi-state time-to-event data. Wiley InterScience  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis. Springer. 481pp  Kacprzynski, G. J., Roemer, M. J., Hess, A. J. 2002. Health management system design: Development, simulation and cost/benefit optimization. IEEE Aerospace Conference Proceedings, 2002. DOI:10.1109/AERO.2002.1036148  Kalbfleisch, J. D., and R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data. Wiley-InterScience, 2nd ed  Kalgren, P. W., Byington, C. S.   Roemer, M. J.  2006 Defining PHM, A Lexical Evolution of Maintenance and Logistics. Systems Readiness Technology Conference 


Logistics. Systems Readiness Technology Conference IEEE. DOI: 10.1109/AUTEST.2006.283685  Keller, K.; Baldwin, A.; Ofsthun, S.; Swearingen, K.; Vian J.; Wilmering, T.; Williams, Z. 2007. Health Management Engineering Environment and Open Integration Platform Aerospace Conference, 2007 IEEE, Digital Object Identifier 10.1109/AERO.2007.352919  Keller, K.; Sheahan, J.; Roach, J.; Casey, L.; Davis, G Flynn, F.; Perkinson, J.; Prestero, M. 2008. Power Conversion Prognostic Controller Implementation for Aeronautical Motor Drives. Aerospace Conference, 2008 IEEE. DOI:10.1109/AERO.2008.4526630  Klein, J. P. and M. L. Moeschberger. 2003. Survival analysis techniques for censored and truncated data Springer  Kingsland, S. E. 1995. Modeling Nature: Episodes in the History of Population Ecology. 2nd ed., University of Chicago Press, 315pp  Kot, M. 2001. Elements of Mathematical Ecology Cambridge University Press. 453pp  Krings, A. W. and Z. S. Ma. 2006. Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks Military Communications Conference, 23-25 October, 7 pages, 2006  Lamport, L., R. Shostak and M. Pease. 1982. The Byzantine Generals Problem. ACM Transactions on Programming Languages and Systems, 4\(3  Lawless, J. F. 2003. Statistical models and methods for lifetime data. John Wiley &amp; Sons. 2nd ed  Line, J. K., Iyer, A. 2007. Electronic Prognostics Through Advanced Modeling Techniques. Aerospace Conference 2007 IEEE. DOI:10.1109/AERO.2007.352906  Lisnianski, A., Levitin, G. 2003. Multi-State System Reliability: Assessment, Optimization and Applications World Scientific  Liu, Y., and K. S. Trivedi. 2006. Survivability Quantification: The Analytical Modeling Approach, Int. J of Performability Engineering, Vol. 2, No 1, pp. 29-44  19 Luchinsky, D.G.; Osipov, V.V.; Smelyanskiy, V.N Timucin, D.A.; Uckun, S. 2008. Model Based IVHM System for the Solid Rocket Booster. Aerospace Conference, 2008 IEEE.DOI:10.1109/AERO.2008.4526644  Lynch, N. 1997. Distributed Algorithms. Morgan Kaufmann Press  Ma, Z. S. 1997. Demography and survival analysis of Russian wheat aphid. Ph.D. dissertation, Univ. of Idaho 306pp  Ma, Z. S. 2008a. New Approaches to Reliability and Survivability with Survival Analysis, Dynamic Hybrid Fault Models, and Evolutionary  Game Theory. Ph.D. dissertation Univ. of Idaho. 177pp  Ma, Z. S. 2008b. Survivability Analysis of Biological Species under Global Climate Changes: A New Distributed and Agent-based Simulation Architecture with Survival Analysis and Evolutionary Game Theory. The Sixth 


International Conference on Ecological Informatics. Dec 25, 2008. Cancun, Mexico  Ma, Z. S. and E. J. Bechinski. 2008. A Survival-Analysis based  Simulation Model for Russian Wheat Aphid Population Dynamics. Ecological Modeling, 216\(2 332  Ma, Z. S. and A. W. Krings. 2008a.  Survival Analysis Approach to Reliability Analysis and Prognostics and Health Management \(PHM  AIAA AeroSpace Conference, March 1-8, 2008, Big Sky, MT, 20pp  Ma, Z. S. and A. W. Krings. 2008b. Competing Risks Analysis of Reliability, Survivability, and Prognostics and Health Management \(PHM  AIAA AeroSpace Conference, March 1-8, 2008.  Big Sky, MT. 20pp  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(I Dependence Modeling", Proc. IEEE  AIAA AeroSpace Conference, March 1-8, 2008, Big Sky, MT. 21pp  Ma, Z. S. and A. W. Krings., R. E. Hiromoto. 2008d Multivariate Survival Analysis \(II State Models in Biomedicine and Engineering Reliability IEEE International Conference of Biomedical Engineering and Informatics, BMEI 2008.  6 Pages  Ma, Z. S. and A. W. Krings. 2008e. Dynamic Hybrid Fault Models and their Applications to Wireless Sensor Networks WSNs Modeling, Analysis and Simulation of Wireless and Mobile Systems. \(ACM MSWiM 2008 Vancouver, Canada  Ma, Z. S. &amp; A. W. Krings. 2008f. Dynamic Populations in Genetic Algorithms. SIGAPP, the 23rd Annual ACM Symposium on Applied Computing, Ceara, Brazil, March 16-20, 2008. 5 Pages  Ma, Z. S. &amp; A. W. Krings. 2008g. Bio-Robustness and Fault Tolerance: A New Perspective on Reliable, Survivable and Evolvable Network Systems, Proc. IEEE  AIAA AeroSpace Conference, March 1-8, Big Sky, MT, 2008. 20 Pages  Ma, Z. S.  and A. W. Krings. 2009. Insect Sensory Systems Inspired Computing and Communications.  Ad Hoc Networks 7\(4  MacConnell, J.H. 2008. Structural Health Management and Structural Design: An Unbridgeable Gap? 2008 IEEE Aerospace Conference, DOI:10.1109/AERO.2008.4526613  MacConnell, J.H. 2007. ISHM &amp; Design: A review of the benefits of the ideal ISHM system. Aerospace Conference 2007 IEEE. DOI:10.1109/AERO.2007.352834  Marshall A. W., I. Olkin. 1967. A Multivariate Exponential Distribution. Journal of the American Statistical Association, 62\(317 Mar., 1967  Martinussen, T. and T. H. Scheike. 2006. Dynamic Regression Models for Survival Data. Springer. 466pp  Mazzuchi, T. A., R. Soyer., and R. V. Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Millar, R.C., Mazzuchi, T.A. &amp; Sarkani, S., 2007. A Survey of Advanced Methods for Analysis and Modeling of 


of Advanced Methods for Analysis and Modeling of Propulsion System", GT2007-27218, ASME Turbo Expo 2007, May 14-17, Montreal, Canada  Millar, Richard C., "Non-parametric Analysis of a Complex Propulsion System Data Base", Ph.D. Dissertation, George Washington University, June 2007  Millar, R. C. 2007. A Systems Engineering Approach to PHM for Military Aircraft Propulsion Systems. Aerospace Conference, 2007 IEEE. DOI:10.1109/AERO.2007.352840  Millar, R. C. 2008.  The Role of Reliability Data Bases in Deploying CBM+, RCM and PHM with TLCSM Aerospace Conference, 2008 IEEE, 1-8 March 2008. Digital Object Identifier: 10.1109/AERO.2008.4526633  Nowak, M. 2006. Evolutionary Dynamics: Exploring the Equations of Life. Harvard University Press. 363pp  Oakes, D. &amp; Dasu, T. 1990. A note on residual life Biometrika 77, 409  10  Pintilie, M. 2006. Competing Risks: A Practical Perspective.  Wiley. 224pp  20 Smith, M. J., C. S. Byington. 2006. Layered Classification for Improved Diagnostic Isolation in Drivetrain Components. 2006 IEEE AeroSpace Conference  Therneau, T. and P. Grambsch. 2000. Modeling Survival Data: Extending the Cox Model. Springer  Vincent, T. L. and J. L. Brown. 2005. Evolutionary Game Theory, Natural Selection and Darwinian Dynamics Cambridge University Press. 382pp  Wang. J., T. Yu, W. Wang. 2008. Research on Prognostic Health Management \(PHM on Flight Data. 2008 Int. Conf. on Condition Monitoring and Diagnosis, Beijing, China, April 21-24, 2008. 5pp  Zhang, S., R. Kang, X. He, and M. G. Pecht. 2008. China  s Efforts in Prognostics and Health Management. IEEE Trans. on Components and Packaging Technologies 31\(2             BIOGRAPHY  Zhanshan \(Sam scientist and earned the terminal degrees in both fields in 1997 and 2008, respectively. He has published more than 60 peer-refereed journal and conference papers, among which approximately 40 are journal papers and more than a third are in computer science.  Prior to his recent return to academia, he worked as senior network/software engineers in semiconductor and software industry. His current research interests include: reliability, dependability and fault tolerance of distributed and software systems behavioral and cognitive ecology inspired pervasive and 


behavioral and cognitive ecology inspired pervasive and resilient computing; evolutionary &amp; rendezvous search games; evolutionary computation &amp; machine learning bioinformatics &amp; ecoinformatics                 pre></body></html 


