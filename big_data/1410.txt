Two Cache Replacement Algorithms Based on Association Rules and Markov Models Keqiu Li 1  2  Wenyu Qu 3  Hong Shen 3 DiWu 4  and Takashi Nanya 2 1 
College of Computer Science and Technology Dalian Maritime University No 1 Linghai Road Dalian 116026 China 2 Research Center for Advanced Science and Technology RCAST The University of Tokyo 4-6-1 Komaba Meguro-ku Tokyo 153-8904 Japan 3 Graduate School of Information Science Japan Advanced Institute of Science and Technology 1-1 Asahidai Nomi Ishikawa 923-1292 Japan 
4 Department of Computer Science and Engineering Dalian University of Technology No 2 Linggong Road Ganjingzi District Dalian 116024 China Abstract In this paper two cache replacement algorithms are presented One is based on association rules in which we extend the LRU replacement algorithm by making it sensible to web access models extracted from web log data using web mining techniques The other one is based on Markov 
models in which we improve the LRU replacement algorithm by applying Markov models Key words Cache replacement web mining association rule Markov model 1 Introduction In recent years the effective distribution and maintenance of stored information has become a major concern for Internet users since the Internet is becoming increasingly congested and the popular web sites are suffering from 
overloaded conditions caused by the large number of simultaneous accesses When a user retrieves web pages from the Internet considerable latency is often experienced Web caching is an important approach for enhancing the delivery efìciency of web content and reducing the latencies experienced by users A userês request for a web page is directed to a nearby copy not to the original server thus reducing access time average server load and overall network trafìc The key aspect of the effectiveness of web 
caching is the document placement/replacement algorithm that can yield high hit ratio A number of cache replacement algorithms have been proposed in recent years which attempts to minimize various cost metrics such as hit ratio byte hit ratio average latency and total cost An overview of web caching replacement algorithms can be found in In this paper we propose two replacement algorithms based on association rules and Markov Models In the rst algorithm we evict an object from a chace based on sequential 
association rule model which captures the sequentiality and temporality In order to do this the rules constructed by the model preserve the sequence of the click stream of the antecedent and the sequence of the click stream of the consequent In the second algorithm we remove an object from the cache by applying Markov models in which we predict user behavior action by AllK th Order Markov models with support and conìdence limitations 
The rest of this paper is organized as follows Sections 2 and 3 present two cache replacement algorithms respectively Finally we conclude this paper in Section 4 Proceedings of the First International Conference on Semantics, Knowledge, and Grid \(SKG 2005 0-7695-2534-2/05 $20.00 © 2006 IEEE 


2 A Cache Replacement Algorithm Based on Sequential Association Rules In this section we rst preprocess the log data Then the concept of sequential association rule and the deìnition of sequential behavior model are introduced Finally a cache replacement algorithm is presented 2.1 Data preparation The syntax of the log le which contains all requests that a site has processed is speciìed in the CERN Common Log Format Basically  a n entry consists of 1 the userês IP address 2 the access date and time 3 the request method GET POST   4 the URL of the page accessed 5 the protocol HTTP 1.0 HTTP 1.1   6 the return code and 7 the number of bytes transmitted This set of logs contain enough information to reveal the set of sessions served by the site In our research the information we really want to obtain is a server session deìned as the click-stream in a user session for a particular web server A click-stream is deìned as a sequential series of page view requests Our system deìnes a compiler that transforms a set of logs L  which is expressed as L   L 1 L 2   L m   where L i  IP i TIME i METHOD i URL i PROT i  CODE i BYTES i  i 1  2   m and m is the number of logs of L  Furthermore we divide L into sessions and we have S   S 1 S 2   S n   where S i  IP i PAGES i   PAGES i  url i 1 url i 2   url i,p i   i 1  2   n  n is the number of sessions of S  and p i is the number of pages requested by user IP i in session S i  The set of URL s that form a session satisìes the requirements that the time elapsed between two consecutive requests is smaller that a given  t  The following is the algorithm of the compiler Algorithm 1  Algorithm of the Compiler Implemented Input L  t m Output S n Function Compiler  L  t m  for each L i of L do if METHOD i is GET and URL i is WEB P AGE then if there exists S k  OPENSESSIONS with IP k  IP i then if  TIME i  END T IME  S k    t then S k  IP k PAGE k  URL k  else CLOSESESSION  S k  OPENNEWSESSION  IP i   URL i  end if else OPENNEWSESSION  IP i   URL i  end if end if end for The lters implemented by our algorithm delete all entry logs that do not refer to a URL or that indicate an error Also sessions of length one or sessions three times as long as the average length of the set of sessions S are erased This is done to eliminate the noise that random accesses or search engines would introduce to the model Finally the average length of the set of sessions S is deìned as N  n  i 1 p i n  2.2 Sequential Association Rules The concept of Sequential Association Rules SAR is based on the concept of an N-Gram In the context of web mining an N-Gram of a session S i is deìned as any subset of N consecutive URL s of that session Given  A  the length of the sequence of URL s of the antecedent  C  the length of the sequence of URL softhe consequent and s the distance between the antecedent and the consequent A SAR is deìned as follows A  C A  url j url j 1   url  A   j C  url l url l 1   url  C   l l   A   j  s 1 A SAR expresses the following relations if the last click stream of length  A  of a user is A  then in s clicks the set of URL s C will be requested Each SAR is constructed from an N-Gram obtained from a session This means that some session S k of S contains an N-GRAM with N   A    C   s  Therefore we have S k    url k,j url k,j 1   url k  A   j 1   url k  A   j  n  url k,l   url k  C   l     where l   A   j  s 1  Each SAR has a degree of support and conìdence associated with it The support of a rule is deìned as the fraction of strings in the set of sessions of S where the rule successfully applies The support of a rule is give by   A P age  n  C   S i  S  A P age  n  c  S i   n where Page  n  represents the set of n pages in the session and  A P age  n  C  S i  is an N-Gram of S i  This is the way to model the distance between the antecedent and the consequent when containing the support The conìdence of a rule is deìne as the faction of times for which the antecedent A is satisìed and the consequent C is also true in s clicks The conìdence of the rule is deìned as   A P age  n  C    A P age  n  C    A  Proceedings of the First International Conference on Semantics, Knowledge, and Grid \(SKG 2005 0-7695-2534-2/05 $20.00 © 2006 IEEE 


SR  A  n  C   for a given a set of sessions S  is deìned as a set of tuples  SAR Counter   where each tuple has a SAR with an antecedent of length  A   a consequent with length  C  and a distance n between the antecedent and the consequent The Counter of each tuple indicates the number of times that the correspondence rule occurs in S  The following algorithm is used to obtain SR  A  n  C   Algorithm 2  Algorithm to Obtain SR  A  n  C  Input  A  n  C  S Output SR  A  n  C  Function Obtain S R  A  n  C  SR  A  n  C    for each S i of S do for k 1 top i do if k   A   n   C  p i then SAR  url i,k url i,k 1   url i  A   k  n url i  A   k  n   url i  C   k  n   A  if  SAR PAGE  n   SR  A  n  C  then Counter  SAR PAGES  n   Counter  SAR PAGES  n   1 else SR  A  n  C   SR  A  n  C    SAR 1 end if end if end for end for In order to preserve only the relevant information only those SARs which have support and conìdence bigger than a given threshold are considered With   the threshold of the support and   the threshold of the conìdence we will talk about the set of rules SR  A  n  C      as the rules of SR  A  n  C  with a support bigger than   and a conìdence bigger than    2.3 Sequential Behavior Model A Sequential Behavior Model is deìned by a tuple RU  where RU is a set of rules and  is the decision policy function RU can be deìned by any SR  A  n  C      or any subset or union of more than one SR  The function  is deìned as   C i 1  i 1  i 1      C i,l i   i,l i  i,l i  C i,j   C i,b where 1  i  k 1  j  l i  1  b  l i  The independent variable of  is the set of consequents obtained from RU for a given antecedent click stream and the dependent variable is the consequent or set of consequents predicted Some examples of policy functions include   C i 1  i 1  i 1      C i,l i   i,l i  i,l i  C i,j   C i,b The rst example predicts the consequent with the biggest conìdence the second one predicts the one with the biggest support and the third one picks the two consequents with the highest conìdence The policy function can be deìned depending on the speciìc application and on the characteristics of the web log used The tuple  RU   deìnes the on-line execution of the prediction system Given A the click stream of the last  A  pages requested by the user the set of predicted pages will be given by   RU  A   The problem of rule mining has also been presented from a Markov chain point of view A process is a Markov Chain process with stationary transitions and countable space state  if that process mo v e through a countable set I of states and at each stage it decides where to go by a random mechanism which depends only on the current state and not on the previous history or even on the time The rule mining problem can be seen as a stochastic process which moves through a countable set of states rul 1   url p  where p is number of web pages of that web server In each state url t  the next page visited will depend only on the actual page and not on the previous history Following these ideas presents a Mark o v model based approach for prediction and uses a Mark o v model for nding shortcuts compares N-gram prediction models for different sizes of N  which are equivalent to constructing an  N  1 order Markov model Each of these investigations consider only subsequences with consecutive accesses within transactions and do not present a formal model for taking into account the time gap between the antecedent and the consequent The Sequential Behavior Model proposed can also be seen from a Markov model point of view For example SR 1  1  1 is equivalent to the rst order Markov model In general a SR n 1  1 can be viewed as n order Markov model where each state will be identiìed with the antecedent of length  A  of each rule The conìdence of each rule which by deìnition will depend only on the  A  elements of the antecedent and not on the previous URL s will be equivalent to the probability of the transition in the equivalent Markov model 2.4 A Replacement Algorithm In our approach we keep the LRU criteria for assigning priorities while the overall strategy is extended by modifying the priorities of the entities already in cache as reaction to the new coming objects Based on the sequential behavior model introduced above we evict an object also by considering the new coming object according to SR  A  n  C       if the object to be evicted is in this set we will delay the eviction of this object since it will probably to be accessed soon we increase the priorities of this object and decide the object Proceedings of the First International Conference on Semantics, Knowledge, and Grid \(SKG 2005 0-7695-2534-2/05 $20.00 © 2006 IEEE 


to be evicted accordingly Our algorithm is shown as follows Algorithm 3  Cache Replaement Input SR  A  n  C  S    Input the object to be evicted Function Eviction  Obj  Call Algorithm 2 Call LRU if the object to be evicted is in SR  A  n  C      then move it to the top of the queue 3 A Cache Replacement Algorithm Based on Markov Model 3.1 Markov Models for Predicting Userês Actions As we have introduced techniques derived from Markov models have been extensively used for prediction the action a user will take next given the sequence of actions he or she has already performed For this type of problems Markov models are represented by three parameters  A,S,T   where A is the set of all possible actions that can be performed by the user S is the set of all possible state for which the Markov model is build and T is a  S  A  Transition Probability Matrix TPM where each entry t i,j corresponds to the probability of performing the action j where the process is in state i  The stat-space of the Markov models depends on the number of previous actions used in predicting the next action The simplest Markov model predicts the next action by only looking at the last action performed by the user In this model also known as the rst-order Markov model each action that can be performed by a user corresponds to a state in the model A somewhat more complicated model computes the predictions by looking at the last two actions performed by the user This is called the second-order Markov model and its states correspond to all possible pairs of actions that can be performed in sequence This approach is generalized to the K t h order Mardov model which computes the predictions by looking at the last K actions performed by the user leading to a state-space that contains all possible sequences of K actions For example consider the problem of predicting the next page accessed by a user on a web site The input data for building Markov models consists of web-sessions where each session consists of the sequence of the pages accesses by the user during his/her visit to the site In this problem the actions for the Markov model correspond to the different pages in the web site and the states correspond to all consecutive pages of length K that were observed in the different sessions In the case of rst-order models the states will correspond to single pages in the case of second-order models the states will correspond to all pairs of consecutive pages and so on Once the states of the Markov model have been identiìed the transition probability matrix can then be computed There are many ways in which the TPM can be built The most commonly used approach is to use a training set of action-sequences and estimate each t ji entry based on the frequency of the event that action a i follows the state s j  3.2 Selective Markov Models AllK th Order Markov model holds the promise of achieving higher prediction accuracies and improved coverage than any single-order Markov model at the expense of a dramatic increase in the state-space complexity This led us to develop techniques for intelligently combining different order Markov models so that the resulting model has low state complexity improved prediction accuracy and retains the coverage of the AllK th Order Markov model Our schemes were motivated by the observation that given a sequence of actions for which we need to predict the next most probable action there are multiple states in the AllK th Order Markov model that can be used to perform that prediction In fact there can be as many states as the number of the different order Markov models used to form the AllK th Order Markov model Now depending on the particular set of states involved each of them can have different prediction accuracies Based on this observation we can then start from the AllK th Order Markov model and eliminate many of its states that are expected to have low prediction accuracy This will alow us to reduce the overall state complexity without affecting the performance of the overall scheme The starting point for all of our algorithms is the AllK th Order Markov model obtained by building a sequence of increasing order Markov models However instead of using this model for prediction we use a number of techniques to eliminate certain states across the different order Markov models The set or states that survive this step then become the nal model used for prediction The goals of this pruning step is primarily to reduce the state complexity and secondarily improve the prediction accuracy of the resulting model We will refer to these models as selective Markov models The key step in our algorithm is the scheme used to determine to potential accuracy of a particular state In the rest of this section we present three different schemes with an increasing level of complexity The rst scheme simple eliminates the states that have very low support The second scheme uses statistical techniques to identify states for which the transition probabilities to the two most prominent actions are not statistically signiìcant Finally the third Proceedings of the First International Conference on Semantics, Knowledge, and Grid \(SKG 2005 0-7695-2534-2/05 $20.00 © 2006 IEEE 


scheme uses an error-based pruning approach to eliminate states with low prediction accuracy The support-pruned Markov model SPMM is based on the observation that states that have low support in the training set tend to also have low prediction accuracies Consequently these low support states can be eliminated without affecting the overall accuracy as well as coverage of the resulting model The amount of pruning in the SPMM scheme is controlled by the parameter  referred to as the frequency threshold In particular SPMM eliminates all the states of the different order Mardov models that are supported by fewer than  training-set instances There are a number of observations to be made about the SPMM scheme First the same frequency threshold is used for all the models regardless of their order Second this pruning policy is more likely to prune higher-order states as higher order states have less support thus dramatically reducing the state-space complexity of the resulting scheme Third the frequency threshold parameter   speciìes the actual number of training-set instances as it is often done in the context of association rule discovery This is done primarily for the following two reasons i the trustworthiness of the estimated transition probabilities of a particular state depend on the actual number of training-set instances and not on the relative number ii the total number of training-set instances is in general exponential on the order of the Markov model thus the same fractional pruning threshold will have a completely different meaning for the different order Markov models One of the limitations of the SPMM scheme is that it does not capture all the parameters that inîuence the accuracy of the state In particular the probability distribution of outgoing actions form a state is completely ignored For example consider a Markov state which has two outgoing actions/Branches such that one of them is substantially more probable than the other Even if the overall support of this state is somewhat low the predictions computed by this state will be quite reliable because of the clear difference in the outgoing probabilities On the other hand if the outgoing probabilities in the above example are very close to each other then in order for that difference to fe reliable they must be based on a large number of training instances Ideally we would like the pruning scheme to not only consider the support of the state but also weigh the probability distribution of the outgoing actions before making its pruning decision This observation led to us to develop the conìdencepruned Markov model CPMM scheme CPMM uses statistical techniques to determine for each state if the probability of the most frequently taken action is signiìcantly different form the probabilities of the other actions that can be performed from this state If the probability differences are not signiìcant then this state is unlikely to give high accuracy and it is pruned In contrast if the probability differences are signiìcant the state is retained The CPMM scheme determines if the most probable action is signiìcantly different than the second most probable action by computing the 100\(1    percent conìdence interval around the most probable action and checking if the probability of the second action falls within that interval If this true then the state is pruned otherwise it is retained If  p is the probability of the most probable action then its 100\(1    percent conìdence interval is given by  p  z  2    p 1   p  n  p   p  z  2   p 1   p  n 1 where z  2 is the upper  2 percentage point of the standard normal distribution and n is the frequency of the Markov State The degree of pruning in CPMM is controlled by  conìdence coefìcient As the value of  decreases the size of the conìdence interval increases resulting in more pruning Also note that if a state has a large number of examples associated with it then Equation 1 will compute a tighter conìdence interval As a result even if the difference in the probabilities between the two most probable actions is relatively small the state will most likely be retained As our earlier discussion indicated this feature is desirable In the previous schemes we used either the support of a state or the probability distribution of its outgoing branches to gauge the potential error associated with it However the error of each state can be also automatically estimated and used to decide whether or not to prune a particular state A widely used approach to estimate the error associated with each state is to perform a validation step During the validation step the entire model is tested using part of the training set validation set that was not used during the model building phase Since we know the actual actions performed by the sequences in the validation set we can easily determine the error-rates and use them for pruning This led us to develop the error pruned Markov model EPMM scheme Speciìcally we have developed two different error-based pruning strategies that use a different definition as to what constitutes the error-rate of a Markov state We will refer to these schemes as overall error pruning and individual error pruning The overall error pruning scheme works as follows First for each sequence in the validation set we use each one of the K single-order Markov models to make a prediction For each prediction we record whether that prediction was correct or not Once all the sequences in the validation set have been predicted we use these prediction statistics to calculate the error-rate of each state Next for each state of the highest-order Markov model we identify the set of states in the lower-order models that are its proper subsets For example if the highest-order state corProceedings of the First International Conference on Semantics, Knowledge, and Grid \(SKG 2005 0-7695-2534-2/05 $20.00 © 2006 IEEE 


responds to the action-sequence  a 5 a 3 a 6 a 7   then the lower-order states that are identiìed are  a 3 a 6 a 7  thirdorder  a 6 a 7  second-order and  a 7  rst-order Now if the error-rate of the highest-order state is higher than any of its subset lower-order sates it is pruned The same procedure of identifying the subset sates and comparing their error-rates is repeated for all the states in the lowerorder Markov models as well except the rst-order Markov model The states form the rst-order Markov model are never pruned so as not to reduce the coverage of the resulting model In the second scheme we rst iterate over all the higherorder states and for each of them we nd its subset states as escribed in the previous scheme Then we identify all the examples in the validation set that can be predicted using the higher-order state i.e the validation examples which have a sequence of actions corresponding to the higherorder state This set of examples are then predicted by the higher-order state and its subset states and the error-rates on these examples for each one of the states is computed If the error-rate of the higher-order state is greater then any of its subset states the higher-order Markov state is pruned The same procedure is repeated for all the lower-order Markov models except the rst-order Markov model Though both schemes follow a similar procedure of locating subset states and pruning the ones having high errorrates they differ on how the error-rates for each state is computed In the rst scheme every lower-order Markov state has a single error-rate value that is computed over the entire validation set In the second scheme each of the lower-order Markov states will have many error-rate values as it will be validated against a different set of examples for each one of its superset higher-order states 3.3 A Cache Replacement Algorithm Support the transition probability matrix of a Markov model is T  t i,j  m  n  where each element of t i,j corresponds to the probability of performing the action j when the process is in state i  we deìne the support of a rule as follows sup  R  t i,j  n  i 1 m  j 1 t i,j we deìne the conìdence of a rule as follows con  R  t i,j  m  j 1 t i,j We can obtain a new matrix  P  from the transition probability matrix by incorporating threshold on sup  R  and200101 sup  R   We present a cache replacement algorithm as follows Algorithm 4  Cache Replacement Input  P Input the object to be evicted Function Eviction  Obj  if the number of the object to be cached is i then J   j   t i,j  0  then Call LRU if the object to be removed is in J then move it to the top of the queue 4 Conclusion In this paper two cache replacement algorithms based on association rules and Markov models were presented and analyzed The algorithms should be implemented and experiments should also be conducted to show the validity of the algorithms References  D Albrecht I Zuk erman and A Nicholson Pre-Sending Documents on the WWW A Comparative Study  IJCAI99Porc of the 16th International Joint Conference on Artiìcial Intelligence 1999  C R Anderson P  Domongos and D S W eld Adaptive Web Navigation for Wireless Devices  Porc of the 17th International Joint Conference on Artiìcial Intelligence 2001  A Balamash and M Krunz An Overview of Web Caching Replacement Algorithms  IEEE Communications Surveys  Tutorials Vol 6 No 2 pp 44-56 2004  A Besta vros WWW Trafìc Reduction and Load Balancing through Server-Based Caching  IEEE Concurrency Special Issue on Parallel and Distributed Technology Vol 15 pp 56-67 1997 5 CERN Common Log Format  http://www.w3.org/Daemon/User/Conìg/Logging.htm#commonlogìle-format  D Freedman Markov Chains  Holden-Day Series in Probability and Statistics 1971  Z Su Q Y ang and H Zhang A Predicetion System for Multimedia Pre-Fetching on the Internet  Proc of teh ACM Multimedia Conference Acm 2000 Proceedings of the First International Conference on Semantics, Knowledge, and Grid \(SKG 2005 0-7695-2534-2/05 $20.00 © 2006 IEEE 


ered frequent 2-itemsets and the Apriori downward property is utilized to generate the minimal number of their candidate calendar patterns. Finally, all frequent itemsets and their cal endar patterns are discovered in one shot. Calendar-based temporal association rules are then obtained. Experimental results have shown that our method is more efficient than others References 11 R. Agrawal and R. Srikant. Fast Algorithms for Min ing Association Rules. In Proceedings of the Inrema tional Very Large Database Conference , pages 487 499,1994 2 ]  J. Han, G. Dong, and Y. Yin. Efficient Mining of Par tial Periodic Patterns in Time Series Databases. In Pro ceedings of the Inremational Conference on Data En gineering, pages 106-1 15, 1999 3] C. H. Lee, C. R. Lin and M. S. Chen. Sliding-Window Filtering: An Efficient Algorithm for Incremental Min ing. In Pmceedings of the ACM 10th Intemational Conference on Information and Knowledge Manage ment, pages 263-270,2001 4] Y. Li, P. Ning, X. S. Wang and S .  Jajodia. Dsicovering Calendar-based Temporal Association Rules. Data and Knowledge Engineering, Vo1.44, No.2, pages 193-21 8 2003 5] B. Ozden, S. Ramaswamy, and A. Silberscbatz. Cyclic Association Rules. In Proceedings of the 15th Inter national Conference on Data Engineering, pages 41 2 421,1998 6] S. Ramaswamy, S. Mahajan, and A. Silberschatz. On the Discovery of Interesting Patterns in Association Rules. In Proceedings of the International Very Large Database Corference , pages 368-379,1998 7] J. F. Roddick and M. Spiliopoulou. A Survey of Tem poral Knowledge Discovery Paradigms and Methods IEEE Trans. Knowledge and Data Engineering, Vol 14, Issue 4., pages 75C!-767,2002 3127 pre></body></html 


decreased. However, refer to Fig.6, it brings the following problem  110 100 010 100 100 011 001 111 100 011 100 34 33 34 DSD u u u                         u              101 100 001 100 100 010 011 111 100 011 100 


100 34 33 34 DSD u u u                          u             Fig.6: Over Hiding problem of setting  1 in S No matter the left-hand or right-hand equation, the support of {1, 2} in D' is 0. That is, item 1 and item 2 never appear toge ther, and they are mutual exclusive! This situation almost never happens in the normal database. The attackers may interest in this situation and infer that {1, 2} is hidden deliberately. To hide the sensitive patterns, only need to make their supports smaller than minimum support and need not to decrease their support to 0. To solve the problem, we inject a probability ? which is called Distortion probability into this approach. Distortion probability is used only when the column j of the sanitization matrix S contains only one  1  i.e. Sjj = 1 0 1 d   m k k j i k  S D  m j n i j i  d d d d   1  1     D  i j  h a s   j probability to be set to 1 and 1  j probability to be set to 0 Lemma 1: Given a minimum support ?, and a level of confidence c. Let {i, j} be a pattern in Marked-Set, nij be the support count of {i, j}. ? is the Distortion probability of column j Without loss of generality, we assume that Sij  1. If ? satisfies    D n i j  u  u  V U   a n d    


   c x n xijnx D x ij t          u    1 0 UU V where D| is the number of transactions in D, we can say that we are c confident that {i, j} isn  t frequent in D Proof: If probability policies are not considered and Sij  1, that is, while a row \(transaction according to the matrix multiplication discussed in the previous section, D'tj will be set to 0. We can view the nij original jtiD jtjtjt ijn DDD 21     f o r  e a c h  r o w  t i  c o n t a i n s   i   j   i n  D  a s  realizations of nij independent identically distributed \(i.i.d om variables ijn X X X      2 1      e a c h  w i t h  t h e  s a m e  d i s t r i b u  tion as Bernoulli distribution, B \(1 X i    1  a n d  t h e  f a i l u r e  i s  d e f i n e d  a s  X i    0     i    1  t o  n i j   T h e  p r o b  ability ? is attached to the success outcome and 1?? is attached to the failure outcome. Let X be a random variable and could be viewed as the number of transactions which include {i, j} in D such that jin     2 1    T h u s   t h e  d i s t r i b u t i o n  o f  X is the same as Binomial distribution, X?B \(nij              otherwise nx x n XP ij xnxij x ij 0 1,0     U the mean of X = nij?, and the variance of X = nij?\(1 If we want to hide {i, j} in D', we must let its support in D' be smaller than ?. Therefore, the expected value of X must be s m a l l e r  t h a n     D u V    T h a t  i s     m u s t  s a t i s f y     D n i j  u  u  V U   Moreover, the probability of the number of success which is 


Moreover, the probability of the number of success which is equal to or smaller than     1     u  D V   s h o u l d  b e  h i g h e r  t h a n  c  Therefore, U must satisfy    c x n xijnx D x ij t           u    1 0 UU V  If ? satisfies the two equations, we can say that we are c confident that {i, j} isn  t frequent in D When nij &gt; 30, the Central Limit Theorem \(C.L.T used to reduce the complexity of the equation and speed up the execution time of the sanitization process Moreover, if several entries in column j of S are equal to  1 such as Sij  1, Skj  1, Smj  1  etc, several candidate Dist ortion probabilities such as iU , kU , mU , etc are gotten. The Dis tortion probability of column j, ? j, is set to the minimal candid ate Distortion probability to guarantee that all corresponding pair-subpatterns have at least c confident to be hidden in D 3.3. Sanitization Algorithm Fig.7: Sanitization Algorithm According to the probability policies discussed above, the sanitization algorithm D'n  m = Dn  m  Sm  m is showed in Fig.7 Notice that, if the sanitization process causes all the entries in row r of D' equal to 0, randomly choose an entry from some j Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE where Drj = 1, and set D'rj = 1. It is to guarantee that the size of the sanitized database equals the size of the original database 4. Performance Evaluation 4.1. Performance Quantifying There are three potential errors in the problem. First of all some sensitive patterns are hidden unsuccessfully. Secondly some non-sensitive patterns cannot be mined from D'. And the third, new artificial patterns may be produced. However, the third condition never happens in both of our approach and SWA Besides, we also introduce the other two criteria, dissimilarity and weakness. All of the criteria are discussed as follows Criterion 1: some sensitive patterns are frequent in D'. This condition is denoted as Hiding Failure [10], and measured by     DP DP HF H H , where XPH represents the number of patterns contained in PHwhich is mined from database X Criterion 2: some non-sensitive patterns are hidden in D'. It is also denoted as Misses Cost [10], and measured by      DP DPDP MC 


MC H H H      w h e r e        X P H  r e p r e s s ents the number of patterns contained in ~PH which is mined from database X Criterion 3: the dissimilarity between the original and the sanitized database is also concerned, and it is measured by      n i m j ij n i m j ijij D DD Dis 1 1 1 1    Criterion 4: according to the previous section, Forward Inference Attack is avoided while at least one pair-subpattern of a sensitive pattern is hidden. The attack is quantified by        DPDP DPairSDPDP Weakness HH HH    where DPairS is the set of sensitive patterns whose pair subsets can be completely mined from D 4.2. Experimental Results Table 1: Experiment factor The experiment is to compare the performance between our approach and SWA which has been compare with Algo2a [4 and IGA [10] and is so far the algorithm with best performances published and presented in [12] as we know. The IBM synthetic data generator is used to generate experimental data. The dataset contains 1000 different items, with 100K transactions where the average length of each transaction is 15 items. The Apriori algorithm with minimum support = 1% is used to mine the dataset and 52964 frequent patterns are gotten Several sensitive patterns are randomly selected from the frequent patterns with length two to three items to be seeds With the several seeds, all of their supersets are included into the sensitive patterns set since any pattern which contains sensitive patterns should also be sensitive 0 0.05 0.1 0.15 0.2 0.25 0.3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern H id 


id in g F ai lu re SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern W e ak n es s SP SWA Fig.8: Rel. bet. RS and HF. Fig.9: Rel. bet. RS and weakness Fig.8, Fig.9, Fig.10, Fig.11 and Fig.12 show the effect of RS by comparing our work to SWA. Refer to Fig.8, because the level of confidence in our sanitization process takes the minim um support into account, no matter how the distribution of the sensitive patterns, we still are C confident to avoid hiding failure problems. However, there is no correlation between the disclos ure threshold in SWA and the minimum support. Under the sa me disclosure threshold, if the frequencies of the sensitive patte rns are high, the hiding failure will get high too. In Fig.9, beca use our work is to hide the sensitive patterns by decreasing the supports of the pair-subpatterns of the sensitive patterns, the val ue of weakness is related to the level of confidence. However according to the disclosure threshold of SWA, when all the pair subpatterns of the sensitive patterns have large frequencies, it may cause serious F-I Attack problems. Hiding failure and wea kness of SWA change with the distributions of sensitive patterns 0 0.1 0.2 0.3 0.4 0.5 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern M is se s C o st SP SWA 0 0.005 0.01 0.015 0.02 0.025 0.03 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern D is si m 


m il ar it y SP SWA Fig.10: Rel. bet. RS and MC.  Fig.11: Rel. bet. RS and Dis In Fig.10 and Fig.11, ideally, the misses cost and dissimilar ity increase as the RS increases in our work and SWA. However if the sensitive pattern set is composed of too many seeds, a lot of victim pair-subpatterns will be contained in Marked-Set. And as a result, cause a higher misses cost, such as x = 0.04 in Fig.10 Moreover, refer to the turning points of SWA under x = 0.0855 to 0.1006 in Fig.10 and Fig.11. The reason of violation is that the result of the experiment has strong correlation with the distri bution of the sensitive patterns. Because the sensitive patterns Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE are chosen randomly, several variant factors of the sensitive patt erns are not under control such as the frequencies of the sensiti ve patterns and the overlap between the sensitive patterns if the overlap between the sensitive patterns is high, some sensitive patterns can be hidden by removing a common item in SWA Therefore, decrease the misses cost and the dissimilarity 0 0.5 1 1.5 2 2.5 3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern E x ec u ti o n T im e h  SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set H id in g F a il u re SP SWA 


SWA Fig.12: Rel. bet. RS and time. Fig.13: Rel. bet. RL2 and HF Fig.12 shows the execution time of SWA and our approach As shown in the result, the execution time of SWA increases as RS increases. On the other hand, our approach can be separated roughly into two parts, one is to get Marked-Set which is strong dependent on the data, the other is to set sanitization matrix and execute multiplication whose execution time is dependent on the numbers of transactions and items in the database. In the experi ment, because the items and transactions are fixed, therefore, the execution time of our approach is decided by the setting of Marked-Set which makes the execution time changing slightly 0 0.05 0.1 0.15 0.2 0.25 0.3 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set W ea k n es s SP SWA 0 0.1 0.2 0.3 0.4 0.5 0.6 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set M is se s C o st SP SWA Fig.14: Rel. bet. RL2 and weakness. Fig.15: Rel. bet.RL2 and MC 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set D is si m il a ri ty SP SWA Fig.16: Rel. bet. RL2 and Dis Fig.13, Fig.14, Fig.15and Fig.16 show the effect of RL2 Refer to Fig.13 and Fig.14, our work outperforms SWA no matter what RL2 is. And our process is almost 0% hiding failure 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


