Robust Crowdsourced Learning Zhiquan Liu Luo Luo Wu-Jun Li Shanghai Key Laboratory of Scalable Computing and Systems Department of Computer Science and Engineering Shanghai Jiao Tong University China liuzhiquan@sjtu.edu.cn ricky@sjtu.edu.cn liwujun@cs.sjtu.edu.cn Abstract In general a large amount of labels are needed for supervised learning algorithms to achieve satisfactory performance Itês typically very time-consuming and money-consuming to get such kind of labeled data Recently crowdsourcing services provide an effective way to collect labeled data with much lower cost Hence crowdsourced learning CL which performs learning with labeled data collected from crowdsourcing services has become a very hot and interesting research topic in recent years Most existing CL methods exploit only the labels from different workers annotators for learning while ignoring the attributes of the instances In many real applications the attributes of the instances are actually the most discriminative information for learning Hence CL methods with attributes have attracted more and more attention from CL researchers One representative model of such kind is the personal classiìer PC model which has achieved the state-of-the-art performance However the PC model makes an unreasonable assumption that all the workers contribute equally to the nal classiìcation This contradicts the fact that different workers have different quality ability for data labeling In this paper we propose a novel model called robust personal classiìer RPC for robust crowdsourced learning Our model can automatically learn an expertise score for each worker This expertise score reîects the inherent quality of each worker The nal classiìer of our RPC model gives high weights for good workers and low weights for poor workers or spammers which is more reasonable than PC model with equal weights for all workers Furthermore the learned expertise score can be used to eliminate spammers or low-quality workers Experiments on simulated datasets and UCI datasets show that the proposed model can dramatically outperform the baseline models such as PC model in terms of classiìcation accuracy and ability to detect spammers Index Terms crowdsourcing crowdsourced learning supervised learning I I NTRODUCTION The big data era brings a huge amount of data for analyzing and consequently provides machine learning researchers with many new opportunities In general a large amount of labels are needed for supervised learning algorithms to achieve satisfactory performance Traditionally the labels are provided by domain experts and the labeling cost is high in terms of both time and money With the advent of crowdsourcing and human computation 2 in recent years it becomes practical to annotate large amounts of data with low cost For example with Internet-based crowdsourcing services such as Amazon Mechanical Turk 1 and CrowdFlower 2  it has become relatively 1 https://www.mturk.com 2 http://crowdîower.com cheap and less time-consuming to acquire large amounts of labels from crowds Another interesting case is the construction of ImageNet during which a lar ge number of images were efìciently labeled and classiìed by crowds As crowdsourcing services become more and more popular crowdsourced learning CL which performs learning with labeled data collected from crowdsourcing services has become a very hot and interesting research topic in recent years  Compared with the labels given by human experts the labels collected from crowds are noisy and subjective because workers annotators vary widely in their quality and expertise Previous work on crowdsourcing has sho wn that there exist some workers who give labels randomly Those workers who give labels randomly without considering the features attributes are called spammers Spammers will give labels randomly to earn money Besides the spammers some low-quality workers will also give noisy labels for the data Sorokin and Forsyth reports that some of the errors come from sloppy annotations Hence the existence of noisy labels makes CL become a very challenging learning problem To handle the noise problem in CL repeated labeling is proposed to estimate the correct labels from noisy labels Snow et al nd that a small number of none xpert annotations per instance can perform as well as an expert annotator Hence the typical setting of CL is that each training instance has multiple labels from multiple workers with different quality ability The existing CL methods can be divided into two classes according to whether instance features attributes are exploited for learning or not The rst class of methods exploits only the labels from different workers annotators for learning while ignoring the attributes of the instances Most of the existing methods such as those in  belong to this class In many real applications the attributes of the instances are actually the most discriminative information for learning Hence CL methods with attributes have attracted more and more attention from CL researchers Very recently some methods are proposed to exploit attributes for learning which have shown promising performance in real applications  11 23 Raykar et al 6 11 23 propose a t w o-coin model and extensions which can learn a classiìer from attributes and estimate ground truth labels simultaneously The drawback of this two-coin model is that it fails to model the difìculty of each training instance In a personal classiìer PC model is proposed which can model both the ability of workers and the difìculty of instances Experimental 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 338 


results in sho w that the PC model can achie v e better performance than most state-of-the-art models including the two-coin model However the PC model makes an unreasonable assumption that all the workers contribute equally to the nal classiìcation This contradicts the fact that different workers have different quality ability for data labeling In this paper we propose a novel model called robust personal classiìer RPC for robust crowdsourced learning Our model can automatically learn an expertise score for each worker This expertise score reîects the inherent quality of each worker The nal classiìer of our RPC model gives high weights for good workers and low weights for poor workers or spammers which is more reasonable than PC model with equal weights for all workers Furthermore the learned expertise score can be used to eliminate spammers or low-quality workers Experiments on simulated datasets and UCI datasets show that the proposed model can dramatically outperform the baseline models such as PC model in terms of classiìcation accuracy and ability to detect spammers II P ERSONAL C LASSIFIER M ODEL In this section we rst introduce the setting of crowdsourced learning CL or learning from crowds Then we brieîy introduce the personal classiìer PC model A Crowdsourced Learning A typical CL problem consists of a training set T   X  Y  I  where X   x i  x i R D  M i 1 is the matrix representation of the M training instances with D features the i th row of X corresponds to the training instance x i  the j th column of X corresponds to the j th feature of the instances Y is a matrix of size M  N with N being the number of annotators y ij denotes the element at the i th row and the j th column of Y which is the label of instance i given by annotator j  Note Y may contain a lot of missing entries in practice because it is not practical for each annotator to label all the instances I is an indicator matrix with the same size as Y  where I i,j 1 denotes the i th instance is actually labeled by the j th annotator and I i,j 0 otherwise We also deìne I j as the set of instances which are labeled by the j th annotator We focus on the binary classiìcation in this paper although the algorithm can be easily extended to multiple-class cases B Personal Classiìer Model The probabilistic graphical model for PC model is shown in Fig 1 a It assumes that the nal classiìer base model is a logistic function parameterized by w 0  P  y 1  x  w 0    w T 0 x  1  1+exp  w T 0 x   1 To overcome overìtting a zero-mean Gaussian prior is put on the parameter w 0  P  w 0    N  0   1 I   2 where N    denotes the normal distribution  is a hyperparameter and I denotes an identity matrix whose dimensionality depends on context The j th annotator is assumed to give labels according to a logistic function parameterized by w j  P  y 1  x  w j    w T j x  1 1+exp  w T j x   3 All the w j are assumed to be generated from a Gaussian distribution with mean w 0  P  w j  w 0   N  w 0   1 I   4 with  being a hyperparameter Putting together all the above assumptions we can get the negative log-posteriori as follows f  w 0  W   N  j 1  i I j l  y ij   w T j x i   N  j 1  2  w j  w 0  2    w 0  2 2  c 1  5 where W   w j  N j 1  l  s t  s log t 1  s og\(1  t  is the logistic loss and c 1 is a constant independent of the parameters To solve the convex optimization problem in 5 an iteration algorithm with two steps is derived in The rst step is to update w 0 with W xed w 0    N j 1 w j   N  6 The second step is to update W with w 0 xed The NewtonRaphson method is employed to update each w j separately w t 1 j  w t j    H  w t j   1 g  w t j   7 where w t j denotes the value of iteration t   is the learning rate H  w t j  is the Hessian matrix and g  w t j  is the gradient ij y i x N j  2  1 0 w j w j i ij y i x j N j  2  1 0 w j w k j i a PC model b RPC model Fig 1 Graphical models of PC and RPC 339 


III R OBUST P ERSONAL C LASSIFIER From 6 we can nd that all the annotators   w j  N j 1  contribute equally to the nal classiìer  w 0  which is unreasonable because different annotators may have different ability In this paper we propose a robust personal classiìer RPC model to learn the expertise score of each annotator More speciìcally each annotator will be associated with an expertise score which can be automatically learned during the learning process of our model The expertise score can be used to rank annotators and eliminate spammers A Model Fig 1 b shows the probabilistic graphical model of our RPC model Like PC model the nal classiìer base model of RPC for prediction is also a logistic function parameterized by w 0  P  y 1  x  w 0    w T 0 x  1  1+exp  w T 0 x   P  w 0    N  0   1 I   The j th annotator is also associated with a logistic function parameterized by w j  The prediction functions of the annotators in RPC model are as follows P  y ij  w j  x i  j  N    w T j x i    k j   1   8 P  w j  w 0  j  N  w 0   1 j I   9 P   j     G          1 j exp   j      10 where    and k are hyperparameters and      0 s   1 exp  s  ds is the Gamma function The main difference between RPC model and PC model lies in the distributions of P  y ij  w j  x i  j  and P  w j  w 0  j   More speciìcally all the annotators share the same  in PC model However we associate different annotators with different values of   j  s in RPC model which actually reîect the expertise ability of annotators This can be easily seen from the following learning procedure B Learning The maximum a posteriori MAP estimator of the model parameter w 0 and W can be obtained by minimizing the following negative log-posteriori f  w 0  W  N  j 1  i I j k j 2  y ij    w T j x i  2  N  j 1  j 2  w j  w 0  2    w 0  2 2  c 2  11 where c 2 is a constant independent of the parameters Solving the above optimization problem allows us to jointly learn the model parameters w 0 and W  and the expertise scores   j   We devise an alternating algorithm with two steps to learn the parameters In the rst step we x   j   and then optimize w 0 and W  In the second step we x w 0 and W  and then optimize   j   1 Optimization w.r.t w 0 and W  We update w 0 with W xed and then update W with w 0 xed We repeat these two steps until convergence Given W is xed setting the gradient of 11 w.r.t w 0 to zero we get w 0   N j 1   j w j     N j 1  j  12 where we can nd that annotators with different expertise scores contribute unequally to the nal classiìer This is different from the PC model in 6 Given w 0 is xed we can nd that the parameters  w j  N j 1 are independent of each other Hence we can optimize each w j separately The PC model uses Newton-Raphson method to solve the problem w.r.t w j  which is shown in 7 One problem with Newton-Raphson method is that we need to manually set the learning rate  in 7 However it is not easy to nd a suitable learning rate in practice Furthermore the learning method in 7 cannot necessarily guarantee convergence which will cause a problem about how to terminate the learning procedure In this paper we design a surrogate optimization algorithm for learning which can guarantee con v e r gence Fur thermore there are no learning rate parameters for tuning in the surrogate algorithm which can overcome the shortcomings of the learning algorithm in PC model The gradient g  w j  can be computed as follows g  w j   j  w j  w 0  k j  i I j 2   y ij   1    x i  13 where  is short for   w T j x i   The Hessian matrix H  w j  can be computed as follows H  w j   j I  k j   i I j     1  2  2 y ij 1   y ij  x im x in  m,n where x im denotes the m th element of x i   g  m n  m,n denotes a matrix with the  m n  th element being g  m n   Let s        1  2  2 y ij 1   y ij   Because 0  1  we can prove that s     0  0770293  Let  H  w j   j I 0  0770293 k j  i I j  x i x T i   We can prove that H  w j    H  w j   With the surrogate optimization techniques we can construct an upper bound of the original objective function By optimizing the upper bound which is also called a surrogate function we can get the following update rule w t 1 j  w t j    H  w t j   1 g  w t j   14 Compared with the learning algorithm in 7 it is easy to nd that there is no learning rate parameter for tuning in 14 We can also prove that this update rule can guarantee convergence The detailed derivation and proof are omitted for space saving 340 


2 Optimization w.r.t   j   Let y j   y ij  i I j  We can update  j with the learned w 0 and W  P   j  w j  w 0  X  y j   P  y j  X  w j  j   P  w j  w 0  j   P   j        i I j exp k j  y ij    w T j x i  2  2   exp  j  w j  w 0  2 2      1 j exp   j    D  I j  2    1 j  exp      w j  w 0  2  k  i I j  y ij    w T j x i  2 2   j   Hence p   j  w j  w 0  X  y j  G      where      D  I j  2       1 2   w j  w 0  2  k  i I j  y ij    w T j x i  2   where D is the dimensionality of instances and I j  denotes the number of elements in the set I j  The expectation of  j is   j  E   j       2   D  I j  2    w j  w 0  2  k  i I j  y ij    w T j x i  2  15 We can get some intuition from 15  w j  w 0  2 measures the difference from the parameter of the j th personal classiìer to the parameter of nal classiìer learned groundtruth classiìer and  i I j  y ij    w T j x i  2 denotes the error of the personal classiìer on the training data The larger these differences errors are the smaller the corresponding  j will be Thus  j reîects the expertise score ability of the annotator j  which can be automatically learned from the training data By combining 15 with 12 we can get an algorithm which can automatically learn an expertise score for each worker Based on these learned scores our RPC model can learn a nal classiìer contributed more from good workers but less from poor workers or spammers This is more reasonable than PC model with equal weights for all workers 3 Summarization We summarize the algorithm for RPC model in Algorithm 1 During the learning of RPC we can eliminate the spammers in each iteration after we have found them The performance of the learned classiìer in the following iterations can be expected to improve due to the reduced noise spammer In Algorithm 2 we present the variant of the PRC algorithm called RPC2 that iteratively eliminates spammers in each iteration Algorithm 1 Robust personal classiìer RPC Input features  x i  M i 1 labels y ij  0  1  i 1 M j 1 N indicator matrix I max iter while iter num  max iter do update w 0 based on 12 update each w j based on 14 update each  j based on 15 end while Output w 0  W    j  N j 1 Algorithm 2 Robust personal classiìer with spammer elimination RPC2 Input features  x i  M i 1 labels y ij  0  1  i 1 M j 1 N indicator matrix I max iter spammer num while remove num  spammer num do while iter num  max iter do update w 0 based on 12 update each w j based on 14 update each  j based on 15 sort   j  N j 1  and remove z workers with the lowest values of  j remove num  remove num  z end while end while Output w 0  W    j  N j 1 IV E XPERIMENTS In this section we compare our model with some baseline methods including state-of-the-art methods on CL We validate the proposed algorithms on both simulated datasets and UCI benchmark datasets k is set to 1 in our experiments A Baseline Methods We compare our RPC model with two baseline methods majority voting MV and PC model to evaluate the effectiveness of RPC model MV is a commonly used heuristic method in CL tasks and PC model is the most related one Furthermore PC model has achieved the state-of-the-art performance according to the experiments in 1 Majority Voting In MV all the annotators contribute equally and a training instance is assigned the label which gets the most vote This method is very simple but strong in practice We train a logistic regression classiìer on the consensus labels MV can be adapted to measure the expertise of each annotator worker We compute the similarity between the labels given by each worker and the majority voted labels The similarity is treated as a measure of workerês expertise based on the fact that high-quality workers usually give similar labels as the ground-truth labels 341 


2 Personal Classiìer PC Model The PC model in can learn a classiìer for the underlying ground-truth labels So we can measure its area under the curve AUC on the testing data However the PC model does not provide a direct mechanism to measure the ability expertise of each worker so we only compare RPC with MV in terms of discriminating good workers from spammers in the experiment B Simulated Data We rst validate our algorithm on simulated data We assume there exist two types of annotators The rst type is called good annotators Due to worker ability and understanding bias for the labeling task the good annotators are assumed to give correct labels at a certain probability which ranges from 0.65 to 0.85 in the experiment The second type of annotators is spammers Spammers are assumed to give labels randomly regardless of the features The dimensionality of the feature vectors is 30 and each dimension is generated from a uniform distribution U   0  5  0  5  The parameter of the base model w 0 is generated from a Gaussian distribution with zero mean and identity covariance matrix The ground truth labels are calculated from the logistic function in 8 The noisy labels given by each worker are then generated according to whether they are good annotators or spammers For all the experiments we run the experiments 10 times and report the average results Let M denote the number of training instances For all the experiments we generate 10 M instances as the testing set Let R denote the number of good annotators and S denote the total number of annotators So the number of spammers is S  R  We use two metrics to evaluate our algorithms against baseline methods As the ground truth labels are known we compute the AUC for all the classiìers Another evaluation metric is the ability to detect good annotators We rank the expertise scores and fetch top n workers Among them we compute the precision of good annotators The n is set to R in our experiment if there are R good annotators in the training set 1 Classiìcation Accuracy We report the AUC on some datasets with different M  R and S in Table I TABLE I AUC PERFORMANCE  Data Set Parameters MV PC RPC Random Data 1 M 100 R 5 S 50 0.6014 0.6608 0.6718 Random Data 2 M 200 R 5 S 50 0.7544 0.8233 0.9032 Random Data 3 M 400 R 5 S 50 0.7760 0.8559 0.9520 Random Data 4 M 300 R 5 S 10 0.8838 0.9278 0.9695 Random Data 5 M 300 R 5 S 50 0.7153 0.8077 0.9091 Random Data 6 M 300 R 5 S 90 0.6641 0.7240 0.8346 We can see from Table I that RPC outperforms PC model and MV method on all the datasets 2 Ability to Discriminate Good Annotators from Spammers We evaluate the ability of the proposed RPC model to discriminate good annotators from spammers We generate 5 good annotators in this experiment We rank the expertise scores and check the ratio of the 5 highest scores to be truly good annotators we call this metric precision at top 5 Table II shows that RPC model can detect good annotators more accurately than MV method TABLE II P RECISION OF DETECTING GOOD ANNOTATORS AT TOP 5 Data Set Parameters MV RPC Random Data 1 M 100 R 5 S 50 0.3200 0.5000 Random Data 2 M 200 R 5 S 50 0.4200 0.7600 Random Data 3 M 400 R 5 S 50 0.7400 0.9400 Random Data 4 M 300 R 5 S 10 0.9600 1.0000 Random Data 5 M 300 R 5 S 50 0.4800 0.7200 Random Data 6 M 300 R 5 S 90 0.3000 0.7600 3 Effect of the Number of Spammers We are also interested in the sensitivity of performance when the number of spammers ranges from small to very large Fig 2 shows the AUC performance and the precision to detect good annotators in the top 5 positions with increasing number of spammers The number of good annotators in the training set is 5 We can nd that all the results degrade as more spammers are added RPC model still performs better than PC model and MV method 10 20 30 40 50 60 70 80 90 100 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1    number of spammers Average AUC MV PC  RPC  RPC2  10 20 30 40 50 60 70 80 90 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  number of spammers precision of good annotators MV RPC  a AUC b precision Fig 2 AUC and precision of good annotators detected on simulated data We vary the number of spammers from 10 to 100 by steps of 10 4 Effect of Missing Labels It is not practical for each worker to annotate all the instances in the dataset We test our model in this scenario Fig 3 gives the AUC performance and the precision to detect good annotators in the top 5 positions with increasing number of spammers when each instance is labeled by 30 of the annotators All the results drop signiìcantly compared with those of complete labels but the proposed RPC models still work better than the baselines C UCI Benchmark Data We use the breast cancer dataset from the UCI machine learning repository for e v aluation The cancer dataset contains 683 instances and each instance has 10 features dimensions In our experiments 400 instances are used for training and the rest is for testing We simulate the noisy labels with the same strategy as that in the previous section We generate 5 good annotators and vary the number of spammers 342 


  10  20  30  40  50  60  70  80  90  100  0.55  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95          number of spammers Average AUC   MV  PC   RPC   RPC2     10  20  30  40  50  60  70  80  90  100  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8      number of spammers precision of good annotators   MV  RPC   a AUC b precision Fig 3 AUC and precision of good annotators detected on simulated data with missing labels We vary the number of spammers from 10 to 100 by steps of 10 The AUC and precision of detecting good annotators are shown in Fig 4 We can nd that the proposed RPC can outperform both PC model and MV in terms of AUC and RPC model does much better than MV in detecting good annotators Moreover the RPC2 can further improve the performance of PRC by eliminating the spammers during the learning procedure   10  20  30  40  50  60  70  80  90  100  0.76  0.78  0.8  0.82  0.84  0.86  0.88  0.9  0.92  0.94  0.96          number of spammers average AUC   MV  PC   RPC   RPC2     10  20  30  40  50  60  70  80  90  100  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1      number of spammers precision of good annotators   MV  RPC   a AUC b precision Fig 4 AUC and precision of good annotators detected on UCI dataset We vary the number of spammers from 10 to 100 by steps of 10 V C ONCLUSION A key problem in crowdsourced learning CL is about how to estimate accurate labels from noisy labels To deal with this problem we need to estimate the expertise level of each annotator worker and to eliminate the spammers who give random labels In this paper we propose a novel model called robust personal classiìer RPC model to discriminate high-quality annotators from spammers Extensive experimental results on several datasets have successfully veriìed the effectiveness of our model Future work will focus on empirical comparison between our model and other models such as those in on more real-world applications VI A CKNOWLEDGEMENTS This work is supported by the NSFC No 61100125 the 863 Program of China No 2012AA011003 and the Program for Changjiang Scholars and Innovative Research Team in University of China IRT1158 PCSIRT R EFERENCES  A J Quinn and B B Bederson Human computation a surv e y and taxonomy of a growing eld in Proceedings of the 2011 annual conference on Human factors in computing systems  ACM 2011 pp 1403Ö1412  L V o n Ahn Human computation  i n Design Automation Conference 2009 DACê09 46th ACM/IEEE  IEEE 2009 pp 418Ö419  J Deng W  Dong R Socher  L.-J Li K Li and L Fei-Fei ImageNet A Large-Scale Hierarchical Image Database in CVPR  2009  A P  Da wid and A M Sk ene Maximum lik elihood estimation of observer error-rates using the em algorithm Journal of the Royal Statistical Society Series C Applied Statistics  vol 28 no 1 pp 20 28 1979  P  Smyth U M F ayyad M C Burl P  Perona and P  Baldi Inferring ground truth from subjective labelling of venus images in NIPS  1994 pp 1085Ö1092  V  C Raykar  S  Y u L H Zhao G H V aladez C Florin L Bogoni and L Moy Learning from crowds Journal of Machine Learning Research  vol 11 pp 1297Ö1322 2010  Y  Y an R Rosales G Fung and J G Dy  Modeling multiple annotator expertise in the semi-supervised learning scenario in UAI  2010 pp 674Ö682    Acti v e learning from cro wds  i n ICML  2011 pp 1161Ö1168  J Y i  R  Jin A K Jain S Jain and T  Y ang Semi-cro wdsourced clustering Generalizing crowd labeling by robust distance metric learning in NIPS  2012 pp 1781Ö1789  H Kajino Y  Tsuboi and H Kashima  A con v e x formulation for learning from crowds in AAAI  2012  V  C Raykar and S Y u  Eliminating spammers and ranking annotators for crowdsourced labeling tasks Journal of Machine Learning Research  vol 13 pp 491Ö518 2012  Y  Baba and H Kashima Statistical quality estimation for general crowdsourcing tasks in KDD  2013  H Kajino Y  Tsuboi and H Kashima Clustering cro wds  i n AAAI  2013  S Oyama Y  Baba Y  Sakurai and H Kashima  Accurate inte gration of crowdsourced labels using workers self-reported conìdence scores in IJCAI  2013  K Mo E Zhong and Q Y ang Cross-task cro wdsourcing  i n KDD  2013  R Sno w  B OêConnor  D  Jurafsk y  and A Y  Ng Cheap and f ast but is it good evaluating non-expert annotations for natural language tasks in EMNLP  2008 pp 254Ö263  A Sorokin and D F orsyth Utility data annotation with Amazon Mechanical Turk in Computer Vision and Pattern Recognition Workshops 2008 CVPRWê08 IEEE 2008 pp 1Ö8  V  S Sheng F  J Pro v ost and P  G Ipeirotis Get another label improving data quality and data mining using multiple noisy labelers in KDD  2008 pp 614Ö622  J Whitehill P  Ruv olo T  W u  J  Ber gsma and J R Mo v ellan Whose vote should count more Optimal integration of labels from labelers of unknown expertise in NIPS  2009 pp 2035Ö2043  P  W elinder  S  Branson S Belongie and P  Perona The multidimensional wisdom of crowds in NIPS  2010 pp 2424Ö2432  Y  T ian and J Zhu Learning from cro wds in the presence of schools of thought in KDD  2012 pp 226Ö234  D Zhou J C Platt S Basu and Y  Mao Learning from the wisdom of crowds by minimax entropy in NIPS  2012 pp 2204Ö2212  V  C Raykar and S Y u  Ranking annotators for cro wdsourced labeling tasks in NIPS  2011 pp 1809Ö1817  K Lange D R Hunter  and I Y ang Optimization transfer using surrogate objective functions Journal of Computational and Graphical Statistics  vol 9 no 1 pp 1Ö20 2000  W  H W olber g and O L Mangasarian Multisurf ace method of pattern separation for medical diagnosis applied to breast cytology Proceedings of the National Academy of Sciences  vol 87 no 23 pp 9193Ö9196 1990  K Bache and M Lichman UCI machine learning repository   2013  A v ailable http://archi v e.ics.uci.edu/ml 343 


  7  Lorenz, R  D., Experi m e n t s i n Timel a pse C a m e r a  Observations of Dust Devil Activity at El Dorado Playa Nevada, Abstract #1573, 42nd Lunar and Planetary Science Conference, Lunar and Planetary Institute Houston, TX, 2011  Lorenz R  D., B  Jackson and J. B a rnes, Inexpensi v e Timelapse Digital Cameras for Studying Transient Meteorological Phenomena : Dust Devils and Playa Flooding, Journal of At mospheric and Oceanic Technology, 27, 246-256, 2010  C a st ano, A., A. F ukanag a J. B i esadecki, L. Neakrase, P Whelley, R. Greeley, M. Lemmon, R. Castano, S. Chien Automatic detection of dust devils and clouds on Mars Machine Vision and Applications, 19, 467-482, 2008  Lorenz R  D. and A Val d ez, Var i abl e W i nd R i p p l e  Migration at Great Sand Dunes National Park, Observed by Timelapse Imagery, Geomorphology, 133, 1-10, 2011 19 B a l m e  M. R A  Pa th a r e, S.M Me tzg e r  M C. To w n er  S.R. Lewis, A. Spiga, L.K. Fenton, N.O. Renno, H.M Elliott, F.A. Saca, T.I. Michae ls, P. Russell, J. Verdasca Field measurements of horizontal forward motion velocities of terrestrial dust devils: Towards a proxy for ambient winds on Mars and Earth, Icarus, 221, 632ñ645 2012  Koch, W  On B a y e si an Tracki ng and Dat a Fusi on  A Tutorial Introduction with Examples, IEEE Aerospace and Electronics Systems, 25, 29-51, 2010  Biographies Ralph Lorenz is a planetary scientist at the Johns Hopkins University Applied Physics Laboratory, with interests in atmospheres surfaces and their interactions, especially on Titan and Mars.  He worked for the European Space Agency on Phase B of the development of the Huygens probe to Titan, and subsequently built part of the instrumentation of the probeís Surface Science Package SSP\. Prior to joining APL in 2006, he spent 12 years in various positions at the Lunar and Planetary Laboratory at the University of Arizona, where he led observation planning for the Cassini RADAR investigation, and served on the science team of the New Millennium DS-2 mission to Mars. He is th e author of several books including ëSpinning Flightí, ëTitan Unveiledí, and ëSpace Systems Failures. He has a B.Eng in Aerospace Systems Engineering from the University of Southampton \(UK and a Ph.D. in Physics from the University of Kent at Canterbury \(UK\. He is the recipient of 5 NASA Group Achievement Awards   


  8  


Virtual Social Networks Analysis in Computational Social Network Analysis  ser Computer Communications and Networks A Abraham A.-E Hassanien and V Sn  ael Eds London Springer London 2010 ch 1 pp 3Ö25  J K orner  Fredman-k olmo s bounds and information theory   SIAM J Algebraic Discrete Methods  vol 7 no 4 pp 560Ö570 Oct 1986  T  Leighton and S Rao Multicommodity max-îo w min-cut theorems and their use in designing approximation algorithms J ACM  vol 46 no 6 pp 787Ö832 Nov 1999  M Bastian S He ymann and M Jacomy  Gephi An open source software for exploring and manipulating networks 2009  A.-L Barabasi and R Albert Emer gence of scaling in random networks Science  vol 286 no 5439 pp 509Ö512 1999 


application or middleware platform to collect request ows Thus it is much easier to deploy FChain in large-scale IaaS clouds Blacksheep correl a t e s t he change poi nt of s y s t em-l e v el metrics e.g cpu usage with the change in count of Hadoop application states i.e events extracted from logs of DataNodes and TaskTrackers to detect and diagnose the anomalies in a Hadoop cluster Kahuna-BB correl a t e s b l ack-box dat a system-level metrics and white-box data Hadoop console logs across different nodes of a MapReduce cluster to identify faulty nodes In comparison FChain is a black-box fault localization system which is application-agnostic without requiring any knowledge about the application internals We believe that FChain is more practical and attractive for IaaS cloud systems than previous white-box or gray-box techniques V C ONCLUSION In this paper we have presented FChain a robust blackbox online fault localization system for IaaS cloud computing infrastructures FChain can quickly pinpoint faulty components immediately after the performance anomaly is detected FChain provides a novel predictability-based abnormal change point selection scheme that can accurately identify the onset time of the abnormal behaviors at different components processing dynamic workloads FChain combines both the abnormal change propagation knowledge and the inter-component dependency information to achieve robust fault localization FChain can further remove false alarms by performing online validation We have implemented FChain on top of the Xen platform and conducted extensive experimental evaluation using IBM System S data stream processing system Hadoop and RUBiS online auction benchmark Our experimental results show that FC hain can achieve much higher accuracy i.e up to 90 higher precision and up to 20 higher recall than existing schemes FChain is light-weight and non-intrusive which makes it practical for large-scale IaaS cloud computing infrastructures A CKNOWLEDGMENT This work was sponsored in part by NSF CNS0915567 grant NSF CNS0915861 grant NSF CAREER Award CNS1149445 U.S Army Research Ofìce ARO under grant W911NF-10-1-0273 IBM Faculty Awards and Google Research Awards Any opinions expressed in this paper are those of the authors and do not necessarily reîect the views of NSF ARO or U.S Government The authors would like to thank the anonymous reviewers for their insightful comments R EFERENCES   A m azon E las tic Com pute Cloud  h ttp://a w s  a m azon com ec2   V i rtual c om puting lab  http://vcl ncs u  e du  P  Barham  A  D onnelly  R I s aacs  a nd R M o rtier   U s ing m agpie f or request extraction and workload modelling in 
 2004  M  Y  Chen A  A ccardi E  K icim an J  L lo yd D  P atters on A  F ox and E Brewer Path-based failure and evolution management in  2004  R F ons eca G  P o rter  R H  K atz S  S h enk e r  and I  S toica X T race A pervasive network tracing framework in  2007  I  Cohen M  G o lds z m i dt T  K elly  J  S ym ons  a nd J  S  Chas e Correlating Instrumentation Data to System States A Building Block for Automated Diagnosis and Control in  2004  I  C ohen S  Z h ang M  G o lds z m i dt J  S ym ons  T  K elly  a nd A  F ox Capturing indexing clustering and retrieving system history in  2005  S  D uan S  Bab u  a nd K  M unagala F a A s ys tem for a utom ating failure diagnosis in  2009  S  K andula R Mahajan P  V erkaik S  A garw al J  P a dhye a nd V  Bahl Detailed diagnosis in computer networks in  2009  A  J  O liner  A  V  K ulkarni and A  A ik en  U s ing c orrelated s u rpris e to infer shared inîuence in  2010  P  Bahl R Chandra A  G r eenber g  S  K andula D  A  M altz and M Zhang Towards highly reliable enterprise network services via inference of multi-level dependencies in  2007  Z  G ong X  G u  a nd J  W ilk es   P RE S S  P Redicti v e E las tic ReS ource Scaling for Cloud Systems in  2010  H  N guyen Y  T a n and X  G u P A L  P ropagation-a w are a nom aly localization for cloud hosted distributed applications in  2011  B Gedik H Andrade K L  W u P  S  Y u and M  D oo SP ADE  t he system s declarative stream processing engine in  2008  A pache H adoop S y s tem   http://hadoop apache  or g/co re   Rice uni v e rs ity bidding s y s tem   http://rubis  objectw eb  o r g   M Ben-Y e huda D  B reitgand M F actor  H  K o lodner  V  K r a v ts o v  and D Pelleg NAP a building blo ck for remediating performance bottlenecks via black box network analysis in  2009  Y  T a n X  G u  a nd H  W a ng  A dapti v e s ys tem anom aly prediction f or large-scale hosting infrastructures in  2010  D  L  M ills   A b rief his t ory o f N T P tim e m e m o irs o f a n i nternet timekeeper  2003  Y  T a n H  N guyen Z  S h en X  G u C V e nkatram ani and D  R ajan PREPARE Predictive Performance Anomaly Prevention for Virtualized Cloud Systems in  2012  M  Bas s e ville and I  V  N ikiforo v   Prentice-Hall Inc 1993  L  Cherkaso v a  K  O zonat N Mi J  S ym ons a nd E  Sm irni  Anom aly application change or workload change towards automated detection of application performance anomaly and change in  2008  P  Barham and e t al   X e n a nd the a rt of virtualization  i n  2003  T he ircache p roject  h ttp://www.ircache.net  H ttperf  h ttp://code google com  p htt p er f  S  K u llback  T h e ku llback-leibler distance  1987  X  Chen M  Z hang Z  M  M a o and P  B ahl  A utom ating n etw ork application dependency discovery experiences limitations and new solutions in  2008  M Y u  A  G reenber g  D  M altz J  Re xford L  Y u an S  K andula and C Kim Proìling network performance for multi-tier data center applications in  2011  M K  A guilera J  Mogul J  W iener  P  R e ynolds  a nd A  Muthitacharoen Performance debugging for distributed systems of black boxes in  2003  S  A g arw ala F  A l e g re K  S chw a n and J  M ehalingham  E 2E P r of A utomated end-to-end performance management for enterprise systems in  2007  P  Re ynolds  J  L  W iener  J  C M ogul M  K  A guilera and A  V ahdat  WAP5 black-box performance debugging for wide-area systems in  2006  R Apte L  Hu K  S chw a n and A  G hosh L ook W ho s T alking Discovering dependencies between virtual machines using cpu utilization in  2010  G Khanna I  L aguna F  A rshad an d S Bagchi Distr ibuted diagnosis of failures in a three tier e-commerce system in  2007  R R S a m b as i v an A  X  Z heng M  D e Ros a  E  K re v at S  W h itm an M Stroucken W Wang L Xu and G R Ganger Diagnosing performance changes by com paring request ows in  2011  J  T a n a nd P  N a ras i m h an  RA M S and B lackS h eep I nferring w h ite-box application behavior using black-box techniques CMU Tech Rep 2008  J  T a n X  P a n E  Marinelli S  K a vulya R  G andhi a nd P  N a ras i m h an Kahuna Problem diagnosis for mapreduce-based cloud computing environments in  2010 
OSDI NSDI NSDI OSDI SOSP ICDE SIGCOMM DSN SIGCOMM CNSM SLAML SIGMOD ICAC PODC Computer Communication Review ICDCS Detection of abrupt changes theory and application DSN SOSP The American Statistician OSDI NSDI SOSP DSN WWW HotCloud SRDS NSDI NOMS 
207 
30 
30 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





