Approximate Clustering In Association Rules Lawrence J Mazlack Computer Science University of Cincinnati Cincinnati, Ohio 45221-0030 mazlack@uc.edu Abstract Data mining holds the promise of extracting unsus pected information from very large databases A difficulty is that ways for discouvery are often drawnfrom methods whose amount of work increase geometrically with  quantity. Consequentially the use of these methods is problematic in very large data bases. Categorically bared association rules are a linearly complex data 
mining methodology Unfotunately rules formed from categori cal data often gneerate many fine grained rules The con cem is how might fine grained rules be aggregated anri the role that non-categorical data might have It appears that soft computing techniques may be useful 1 Introduction Data mining is an advanced tool for the management of large masses of data. Data mining is the process of secondary analysis of large databases. It is aimed at di couvering previously unknown relationships of value 
Data mining is secondary analysis because the data were not collected to answer questions now posed The data is examined in an attempt to discouver if there m patterns beyond those that were hypothesized before the data was collected. For example perhaps we are at look ing at long distance telephone call records The records were originally collected for billing Now they can be examined to recognize calling patterns involving such things as call length time-of-day customer calling plan, from where-to-where, etc There are several different strategies of performing data 
mining One approach is to use classic machine learning methods, such as entropy based learning How ever, most classic machine learning techniques are com putationally complex and are ill-suited to completely consider a large data set Also most machine learning results are not well suited for use by most human cli ents This is less than ideal as a goal of data mining is to help human analysts Consequently, data mining techniques suitable for the analysis of large databases have been developed Some data mining techniques focus on the discouvery of simple rules Of these, association rules are prototypical and the 
easiest to understand Association rules formed from categorical data often generate many fine grained rules. This work\222s concern is an exploration of how might fine grained rules be aggre gated and the potential role of non-categorical data 12 Association Rules Association rules Agrawal 19931 meet a necessary\222 data mining subgoal of having efficient data structures and algorithms. Their algorithms also have the advantage of being linearly upwardly scaleable The process of discouvering association rules is often based on Apriori Agrawal 19941 It is an unsupervised discouvery process However, there is often a predefined 
hierarchical concept tree that is applied to cluster the data in preparation for mining this is a form of supervised discouvery For example all brands of beer might be grouped into a single class called 223Beer.\224 Essentially hierarchy trees help cluster the data for analysis and re duce the number of attributes If some clustering is not somehow accomplished, there will be a multitude of fine grained, relatively infrequent rules Predefined hierarchy trees may not serve the purposes of data analysis well. Perhaps, implicit in the data there may be more useful hierarchy trees that go undiscouvered because 
of the pre-processing assumptions As with much real world data, the clusters that hierarchy trees represent can possibly be best formed by using fuzzy sets; or alternatively, rough sets It is a speculation of this work that it might be better to first identify fine grained rules then aggregate them This would percome one difficulty with association rules is that they may be overly fine grained. It may be possible to increase the granularity of the discouvered association rules either through \(a the use of dmouvered concept hierarchies b soft clustering the rules them s,elves, or increasing the granularity of the data before forming association rules 
Association Rules represent positive associations be tween attributes Most commonly the rules are devel oped from categorical data that is expressed as a 0/1 ma trix For example, consider Figure l Parts of this work were performed while the author was a visiting at BISC Computer Science Division EFXS Department University of California, Berkeley Efficiency is necessary because realistic databases may well con tain millions of records Less efficient algorithms often resort to stratified sampling However may become less satisfactory as the volume of records increase 0-7803-6274-W00/$10.00 O 2000 IEEE 256 


Figure 1 Example transactions This resulting tabular form t\(A is the one often used in developing association rules Notice that a Regardless of initial magnitude, t\(A values are repre sented either by categorical values of a 2230\224 or a 2231\224 for example for t the magnitude of chips is 2231\224 while for t the transactional magnitude is 2232\224 How ever in the t\(A matrix the actual magnitude of chips is lost and both f,,chjp and t2,chjPs is set to \2231\224 b There can be implicit hierarchy trees; for example in t Miller an American beer brand is purchased and in t Tuberg a Danish beer is purchased When these transactions are present in t\(A they are both repre sented as 2231\224s as tj,brcr Further in t both Miller and Tuberg are purchased and they are represented by a single 2231\224 as t3,beer After the categorical data matrix is developed the strength of association between term is determined Some restriction on result volume is useful lest too many rules to be examined by a human analyst may be developed. One way to do this is to only find association rules with a high level of support i.e occur with a frequency above a specified threshold A potential prob lem with thresholding is that rules of great interest but with only moderate support might not be captured This is particularly true in large, relatively sparse matrices The format of an assoication rule is When event in confidence factor7 will also event this occurs in support of cases Where Support relative occurrence Conwence degree rule true across individual cases When a customer buys beer  sausage For example in 80 of the cases he will also buy mustard this occurs in 15 of cases Several efficient algorithms for mining categorical asso ciation rules have been published Agrawal 19941 Mannila 19941 Toivonen 19961 4 Quantifying Association Rules In practice the information in many databases is not limited to categorical attributes but usually also con tains quantitative data It is a possibly undesirable over simplification to translate quantitative data into categori cal data. It may over-simplify results when data magni tudes or, quantities are lost For example should the purchase of two units of mustard be considered the same as the purchase of one unit If not how can magnitudes be best handled There are at least two kinds of informa tion that might be obscured differences of behavior due to differing quantities and trends 4 1 lncreasing Data Granularity While Forming Quanti fied Association Rules Several strategies have been pursued to deal with sca lar non-categorical data by incorporating quantification into the mined association rules For the most part they have attempted to increase the granularity of the data before forming the association rules Srikant and Agrawal Srikant 19961 mapped the quantitative association rules\222 problem and categorical rules into a Boolean association rules\222 problem thus extending the work started in Agrawal[1994 They dis cussed mining association rules over quantitative and categorical attributes They dealt with quantitative attrib utes by fine-partitioning the values of the attribute and then combining adjacent partitions as necessary For example a rule might contain quantified age range Age  30,39 and a resulting Boolean rule might be They introduced a measure of partial completeness that quantifies the information lost due to partitioning This measure is used to decide whether or not to partition a quantitative attribute and the number of partitions Cai  19991 considers transactions with quantities as supporting 223weighted\224 rules Cai wanted to balance be tween weight and support, believing that a separation of the two might ignore some interesting knowledge For example a less frequently occurring or supported item set might still be interested if An item is under promotion or if it is unusually profitable If an item is not considered very important in terms of the weights of its item sets but it is very a popular item in that many transactions contain it Aumann 1999 provides a Merent definition of quantitative association rules It is based on statistics They look for events that differ significantly from a Age 30,39  and Married Yes NumCars 2 257 


statistical norm and were then considered to be interest ing. They consider that the best description of for a set of quantitative values is its behavior is its distribution The approach is to look for a subset and its mean or medlan variance and compare it to the mean of the whole The idea is to identify a population subset that presents 223interesting behaviour.\224 Aumann finds a type of rule that is not found by other methods. For example when Sex female the female mean wage of 7.90/hour is interesting when overall mean wage  9.02 Aumann handles both quantitatively based and caterogi cally based rules, such as age E 20,40 3 Height mean  165 cm Liu  19991 presented a statistical approach for quanti tative mining It is supervised search The user specifies the target attribute\(s\Liu is concerned that typically a large number of associations are found; particularly when the attributes are hghly correlated To manage this they first prune the number of associations then find special subsets of the remaining rules The chi-square test was used instead of a minimum confidence measure Another statistical approach has been presented by Yao 4.2.2 Soft Methods There is a considerable history applying either fuzzy or rough set techniques to databases To name a few ap proaches, there are databases of fuzzy values, fuzzy query rules and rough set partitioning Both fuzzy and rough sets have been used in various data mining efforts Our focus is on the relatively unexplored area of rule aggrega tion the lightly explored clustering antecedents and/or consequents, and the role of quantification Chan  19971 presents an algorithm for increasing data granularity that eliminates the need for user-supplied thresholds for support and confidence and to find nega tive as well as positive association rules. Using fuzzy set theory linguistic terms are used to find the degree to which they characterize records in the database Another approach was suggested by Fukuda 1996a,bl It focused on computing two optimized ranges. One that maximizes the support on the condition that the confidence ratio is at least a given threshold; and another that maximizes the confidence ratio on the condition that the support is at least a given threshold They mined association rules of the form They used techniques from computational geometry convex hulls\They proposed algorithms that discovered optimized gain, support and confidence association rules for two classes of regions AE[V I  c Zhang  19991 extended the equi-depth partition algo rithm to mine fuzzy quantitative association rules They built at times on the Apriori algorithm They considered 1\222  X where an item a,v represents either a crisp value an interval if numerical or a fuzzy term and a is an attribute with value v If v is fuzzy, the item is fuzzy They use a minimum support for each attribute The super-candidate technique Agrawal 19941 is used Kuok  mines fuzzy association rules to avoid either ignoring or overemphasizing the elements near the boundaries in the mining process After partitioning the attribute domain as suggested by Srikant  19961 Kuok suggests ZfX is A then Y is B where X Y are a set of attributes and A B are fuzzy sets which describe X and Y respectively. A user-supplied threshold is used to test each side of the rule as to being \223satisfied.\224 Other efforts at association rules with fuzzy antece dents and consequents include Au 1998 19991 Fu  19981 Lee  19971 Au and Chan  19971 are working on the same problem. Fu\222s work is closely related to Kuock  19981 Du  19991 has a somewhat different approach to fuzzified ranges Somewhat related is the work on in complete data of Ng  19981 and Ali  19971 The difficulty with all the methods discussed in sec tion 4 is that their complexities are expensive 5 Frequent Sets An extension of simply forming association rules by counting is the technique of frequent sets Mannila 19941 Because many matrices of interest are sparse fkequent sets help focus on the development of heuristi cally more interesting rules The heuristic is that pat tems that occur more frequently are the more interesting An example of a sparse matrix of interest is a collec tion of grocery store transactions Grocery stores typi cally have well over 10,000 distinct items for sale Cus tomers typically purchase less than fifty items during a store visit Combinations of items that are purchased together often are the most interesting For example perhaps when a customer purchases bananas the most often purchased grocery store item there is a good chance that she will also purchase milk and corn flakes The rules developed through the frequent sets algo rithm may still be fine grained Unfortunately, the ante cedentlconsequent data clustering methods discussed in section 4 appear to be infeasible for frequent sets In stead an approach focusing on rule aggregation or on initial rule reduction would be better 6 Reducing Association Rules Generated If all possible association rules are developed for high dimensional data data with a large number of attributes the number of rules may be too extensive to be useful 258 


The upper boundary for the number of association rules for a data set is where n is the number of attributes I I Jd The number of associations would be much greater if attributes can take on multiple values and if a distinct association rule is developed for each value Four techniques can be used to reduce the count of generated association rules limiting rule dimensionality using a support level reducing quantification, applying concept hierarchies. Using a support level is a part of many methods already We need to seek elsewhere 7.1 Reducing Rule Complexity If there are n attributes only consider association rules of maximum length m This reduces the theoretical upper boundary of the number of association rules to c 14 J=l While producing rules that are more understandable, the count of rules is still too large If high support is used as rule interestingness high support will almost always imply shorter rules How ever you cannot know their maximum length be unless there is a limit on length Also it is possible that a sliding scale of support might be used. That is, less sup port for longer rules In any case it might be necessary to experimentally determine what a useful support level might be for a particular data set and/or application No particular number is written in stone Another approach to quantification that might be use ful would be to granulate values then apply quan~ed association rule techniques For example if academic grades are recorded from 100 to 0 they often are granu lized into A B C   Possibly the mountain method Yager, Filev 19941 might be applied to form the gran ules The mountain method may also me useful in clus tering association rules together 7.2 Concepty Hierarchies It is unclear whether hierarchy trees should be part of the initial formation of the transaction matrix categorical or non-categorical or dealt with later For example, should all beer brands be grouped together as a single class under all cases Or should they be presented separately Consider Figure 1 Should Miller and Tuberg be grouped together into a single class beer Should all beverages be grouped together i.e should Miller Tu berg and Coke be grouped together Can items be grouped together depending on context i.e perhaps sometimes beers should be in one group and Coke in another and sometimes they should all be together e.g 223picnic supplies\224\Is there some way of computa tionally deciding what should be grouped together Extending the example further Consider the case where we have more beer brands say Budwiser Miller Sam Adams and Tuberg Most regular beer drinkers would agree that there is a substantial taste difference between the four of them There may also be price differentiation Should Budwiser, Miller Sam Adams and Tuberg be grouped into a single category or Should each be grouped separately or Should Budwiser  Miller form one group while Adams  Tuberg form another group Currently, grouping is done by a human expert. This is not entirely satisfactory because a potentially useful groups may go unrecognized and b the expert may make an error Concept hierarchies group similar attributes together Intuitively they are desirable. However, the question of how to computationally recognize them is the question For example various types of wine might be all be placed into to concept of 223wine.\224 For example if we have six wines\222 Meie alcohol-free3 Burgundy4 Carminet\222 alcohol-free Chardonnay6 Gallo\222 Burgundy\222 Chalone\222 Pinot Noi Almadin\222 Chardonnay6 Carneros5 Chardonnay6 If the transaction set is as shown in Figure 2 It might be possible to recognize that the premium alco hol wines can be grouped together They are all associ ated with both green grapes and brie \(a premium cheese Similarly the basic alcohol wines can be grouped to gether they are all associated with basic cheddar cheese However if a pre-defined hierarchy was used to group all wines together, the association between type of wine and type of cheese would not be discouvered and the following would result This would result in loosing the information that premium wines are related to the purchase of brie and green grapes Why might this be important to a store Well if you stop carrying green grapes or brie you may also lose your premium wine customers 222 As defined by their label  A 223basic\224 wine extracted 222 A 223premium\224 wine Alcohol free wines are normally fermented, then their alcohol is A red wine A white wine 259 


tl t2 t3 t4 ts e7 ta t9 t6 Figure 2 Wine and cheese transactions for items of difering perceived quality green brand D'Arc cottage free free Gallo Pinot Almadin Carneros grapes cheddar brie cheese Burgundy Chardonnay Burgundy Noir Chardonnay Chardonnay 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 A human observer might not be able to easily rec ognize that the premium, alcohol wines can be grouped together. They are all associated with both green ppes and brie a premium cheese\Similarly the basic alcohol wines can be grouped together they are all associated with basic cheddar cheese Computationally it might be possible to do this However if a predefined hierar chy was used to group all wines together, the associa tion between type of wine and type of cheese would not be discouvered and the Figure 3 would result This would result in loosing the information that premium wines are related to the purchase of brie and green grapes Why might this be important to a store? Well if you stop carrying green grapes or brie you may also lose your premium wine customers tl t2 t3 t4 t5 t7 ta t9 t6 green brand D'Arc cottage grapes cheddar brie cheese wine 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 0 Figure 3 All wine grouped together It gets even worse if all of the cheese varieties are grouped together as well as in Figure 4 Now the analysis would tell us that any cheese and any wine am associated These results are clearly over-generalized green 1 1 1 1 1 1 1 Figure 4 All cheeese and wine grouped A hierarchy such as items E food E wine can be many levels deep There are essentially two ways of establishing hierarchies a have a human predefine together them or b learn them There are a number of different ways that they can be learned As part of a rough-set based approach Han 1992 used crisp concept hierarchies provided by the user be fore the initiation of data mining Alternatively Mazlack  19971 developed and tested a methodology to learn useful concept hierarchies while partitioning data bases. Potentially the same methodology could be ap plied here Questions that need to be answered are How to best recognize intermediate level useful hi erarchical categories For example in a grocery store how do we learn and choose between items E food, non-food food E  cheese, wine  wine E  no-alcohol, basic, premium  The role of quantification in recognizing useful hi erarchical categories 8 Summary Data mining holds the promise of extracting new in formation from very large databases. One of the difficul ties of data mining is that traditional ways of discouver ing new information from data are largely drawn from classic machine learning methods The work involved in these methods increases geometrically with the amount of data considered Consequentially the use of these methods is problematic in very large data bases Association rules are a linearly complex, user under standable way of doing data mining The results are particularly useful when it is important that both the Ides themselves and the methodology behind the rules development are understandable References References were not included because of space limi tations They may be had by contacting the author or from the web at http://www .ececs .uc edu/-mazlack/NAFIPS2000 html 260 


x x  VAt cos\(8 y y  Vat sin\(8 v  v+vat e e+se v_ vise  8  O+eAt xc  x  cos\(O+a IS ar  ax Jcxc  XI2  Y  Y 14 Assuming a static feature and a water column with  0 the first substantial constant properties, such that derivative in range can be reduced to Similarly the second substantial derivative can be shown to be 17 D2r DCI   Vsin\(a  Vcos\(a Dt2 Dt From the binary constraint there exist pairs of mea surements which could have originated from a single object When three measurements are found which are jointly compatible from a binary perspective \(ie pass three binary tests\the above constraint is used to test all three measurements together Stating with three measurements TO,CYO T~,cYI and TZ,~Z a parabola is fitted in range to find the first and second substantial derivative t  2tltZ  ZtOtl  t to  ti T2 to  tl tO  tZ tZ  tdT\222  tz  tl tO  tz Using least squares the slope of the three bearing measurements is found This leads to a hypothesis test of the form If each measurement in a threesome exists in no other threesomes that threesome initializes a trajectory While 9 maintains multiple hypotheses this implemen tation eliminates any hypothesis at this stage with any ambiguity to limit hypothesis growth Future implemen tations will mainlain niure hypotheses but will also have a more advanced sensor which uses spectral information for pruning Vi Experimental Results Once a measurement trajectory has been extracted, the data is used for concurrent mapping and localization using methods presented in 1221 including consistency testing with the spatiotemporal Mahalanobis distance curvature estimation, multiple vantage point initialization, and batch update as in 22 In this section we illustrate the extraction of trajectories and their use for CML for two experiments conducted in a testing tank with a binaural sonar mounted on the robotic gantry shown in Figure 4 The data for experiment I is shown above in Figures 5 and 6 In this experiment the sensor moved past a triangular target and two point objects fishing bobbers The side of the triangle was mapped and the two fishing bobbers were tracked and mapped as shown in Figure 8 The data for experiment 2 is shown in Figures 9 and IO This experiment was fully \223autonomous\224 in the sense that the desired sensor motion was computed online in response to the sensor data The robot adaptively fixated on a cylinder using a reactive sensing strategy 5 6 After detecting the cylinder, the sensor drove toward it and began to circle it while periodically scanning backwards to map two point objects located behind it The extracted measurement trajectories are shown in Figure 11 and the The algorithm concurrently estimated the curvature and center position of the cylinder the x y positions of the point objects, and the sensor trajectory Figure 12 ViI Conclusion Using geometric and dynamic constraints it is possible to make strong statements ahout corresvondence without  2to  6  tz  zt to  tz  Itz to  t knowing what type of feature is being observed Com putational issues associated with a brute-force search for correspondence are circumvented The method has been successfully applied for data segmentation Scenes that Da   Dt qt  t  t  tot1  tot2  tlt2 20 968 


 I                  222                                I I m 2s a Io 40 I 20 10 I..sdlima Irnssap Fig 7 Extracted measusement trajectories for experiment I compare with Figure 6 Fig 10 Range VI time for detected rems for experiment 2 Returns that are not grouped into a mjectary are discarded 0.5 Oi I 5 4 8 2 lrnMonirnl Fig 8 Estrmated tralectory and map for experiment 1 c        221          2236     I A*+-&s     2       ti i if t  o 4 2 0 1 1 6 I 10 I2 1 16 e Fig 9 Canesian projection of detected returns for experiment 2 i yI 40 I a 10 Wa*Ium,_ Fig 11 Extracted measusemem trajectories far experiment 2 IS 6 Fig 12 Raw data estimated sensor uajecton and object locations far experiment 2 969 


appear complex and hard-to-interpret when considered via Cartesian projection are much easier to explain us ing segmented echo trajectories The system incorporates automatic fixation which we believe is an essential part of this type of sonar interpretation approach This has been applied for example to good success in Kuc's work on object recognition 14 The problem of balancing multiple fixations is an unexplored problem \(for example observation of known objects to provide good navigation vs observation of unknown objects for mapping Future work will address 1 3-D tracking 2 the incorporation of spectral information and 3 extension to rough surfaces The primary motivating application for this work detection of undersea buried mines via low frequency synthetic aperture sonar is inherently a 3-D problem Recently we have obtained a large amount of data at the GOATS 2002 experiment using a synthetic aperture sonar that provides this information via use of two eight-element arrays The formulae presented in this paper apply in two dimensions the three dimensional equivalents are presently being derived and tested and initially results are very promising The issues of exploiting spectral information and cop ing with rough surfaces are inter-related The study of echo reflection from rough surfaces has been studied by Bozma and Kuc 3 The matching of new data to previ ously mapped features \(for recognition andlor navigation purposes is tremendously important This paper has not addressed this issue, hut it is anticipated that the trajectory segmentation method will be a valuable tool for early processing as input to recognizing previously mapped features Acknowledgments The authors are grateful to the entire MIT GOATS 2002 team, including H Schmidt P Newman M Bosse D Eickstedt J Edwards W Xu T.C Liu R. Damus J Morash S Desset J Dryer C Bohner and M Grund The authors also thank P Damhra for his efforts in creation of the robotic gantry for the testing tank This research has been funded in part by NSF Career Award BES-9733040, the MlT Sea Grant College Program under grant NA86RG0074 project RCM-3 and the Office of Naval Research under grants N00014-97-0202 and N00014-02-C-02 10 VIII REFERENCES I W Au The Sonar of Dolphins New York Springer Verlag 1993 121 B Barshan and R Kuc Differentiating sonar reflections from corners and planes by employing an intelligent sen sor IEEE Transactions on Panern Analysis and Machine Intelligence PAh41-12\(6 lune 1990 31 0 Bozma and R Kuc Characterizing pulses reflected from rough surfaces using ultrasound J Acoustical Society of America 89\(6 lune 1991 141 0 Bozma and R Kuc Single sensor sonar map building based on physical principles of reflection In Pmc IEEE Int Workshop on Intelligent Robots and Systems 199 1 5 V Braitenberg Vehicles: Experiments in Synthetic Psy chology The MIT Press 1984 61 R A Brooks Cambrian Intelligence The Early History of the Nw AI The MIT Press 1999  J Edwards, H. Schmidt and 1 LePage Bistatic synthetic apenure target detetection and imaging with an auv IEEE J Ocean Engineering 26\(4 2001 181 A Freedman A mechanism of acoustic echo formation Acustica 12\(1 1962 9 W E L Grimson Object Recognition by Computer The Role of Geometric Constraints MIT Press 1990 With contributions from T Lozano-Perez and D P Hut tenlocher IO A Heale and L. Kleeman Fast target classification using sonar In Pmc IEEE Inr Workshoo on Intellinent Robots and Systems pages 14461451 2601  Ill B K P Horn Robot Vision MIT Press 1986 I121 L Kleeman and R Kuc Mobile robot sonar for target localization and classification. Technical Repolt ISL-9301 Intelligent Sensors Laboratory Yale University, 1993 Perception as Bayesian Inference 1996 Fusing binaural sonar information for object recognition In IEEWSICWRSJ International Conference on Multisensor Fusion and Integrarion for Intelligent Sys tems pages 127-135 1996 I51 R Mann and A Jepson Non-accidental features in learning In AAAI Foll Symposium on Machine Learning in Vision 1993 161 D Mm Vision New York W H Freeman and Co 1982 I71 B A Moran I I Leonard and C Chryssostomidis Curved shape reconstruction using multiple hypothesis tracking IEEE J Ocean Engineering 22\(4 1997 I81 I N Newman Marine Hydrodynamics The MIT Press 1977 191 H Peremans K Audenaen and C J M Van A high resolution sensor based on tr-aural perception IEEE I131 D C bill and W Richards editors I41 R Kuc  Transactions on Robotics And Automation 9 1 Feb 1993 USA 20 W Richards editor Natural Computation The MIT Press 1988 PI1 W Richards I Feldman and A. Jepson From features to perceptual categories In British Machine Vision Confer ence 1992 22 R J Rikoski John I Leonard and Paul M Newman Stochastic mapping frameworks In IEEE International Conference on Robotics and Automation 2002 1231 K D Rolt Ocean platform and signal processing effects on synthetic aperture sonar performance Master's thesis MIT Februaty 1991 High-Level Vision Object Recognition and Visual Cognition The MIT Press 1996 24 S Ullman 970 


I Plenary Panel Session J Future Directions in Database Research  456 Chair Surajit Chaudhuri Microsoft Corporation Panelists Hector Garcia-Molina Stanford University Hank Korth, Bell Laboratories Guy Lohman IBM Almaden Research Center David Lomet Microsoft Research David Maier Oregon Graduate Institute I Session 14 Query Processing in Spatial Databases I Chair Sharma Chakravarthy University of Florida Processing Incremental Multidimensional Range Queries in a Direct Manipulation Visual Query Environment  458 High Dimensional Similarity Joins Algorithms and Performance Evaluation  466 S Hibino and E Rundensteiner N Koudas and K.C Sevcik Y Theodoridis E Stefanakis and T Sellis Cost Models for Join Queries in Spatial Databases  476 Mining Association Rules Anti-Skew Algorithms  486 J.-L Lin and M.H Dunham Mining for Strong Negative Associations in a Large Database of Customer Transactions  494 A Savasere E Omiecinski and S Navathe Mining Optimized Association Rules with Categorical and Numeric Attributes  503 R Rastogi and K Shim Chair: Anoop Singhal AT&T Laboratories S Venkataraman J.F Naughton and M Livny Remote Load-Sensitive Caching for Multi-Server Database Systems  514 DB-MAN A Distributed Database System Based on Database Migration in ATM Networks  522 T Hara K Harumoto M Tsukamoto and S Nishio S Banerjee and P.K Chrysanthis Network Latency Optimizations in Distributed Database Systems  532 I Session 17 Visualization of Multimedia Data I Chair Tiziana Catarci, Universita di Roma 223La Sapienza\224 W Chang D Murthy A Zhang and T.F Syeda-Mahmood Global Integration of Visual Databases  542 X 


The Alps at Your Fingertips Virtual Reality and Geoinformation Systeps  550 R Pajarola l Ohler P Stucki K Szabo and P Widmayer C Baral G. Gonzalez and T.C Son Design and Implementation of Display Specifications for Multimedia Answers  558 1 Session 18 Management of Objects I Chair: Arbee Chen National Tsing Hua University P Boncz A.N Wilschut, and M.L. Kersten C Zou B Salzberg, and R Ladin 0 Wolfson S Chamberlain S Dao L Jiang, and G. Mendei Flattening an Object Algebra to Provide Performance  568 Back to the Future Dynamic Hierarchical Clustering  578 Cost and Imprecision in Modeling the Position of Moving Objects  588 ROL A Prototype for Deductive and Object-Oriented Databases  598 A Graphical Editor for the Conceptual Design of Business Rules  599 The Active HYpermedia Delivery System AHYDS using the M Liu W Yu M Guo and R Shan P Lang W Obermair W Kraus and T Thalhammer PHASME Application-Oriented DBMS  600 F Andres and K. Ono S Chakravarthy and R Le S Mudumbai K Shah A Sheth K Parasuraman and C Bertram ECA Rule Support for Distributed Heterogeneous Environments  601 ZEBRA Image Access System  602 Author Index  603 xi 


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


