The Apacheí HadoopÆ framework provides parallel processing and distributed data storage capabilities that data analytics applications can utilize to process massive sets of raw data. These Big Data applications typically run as a set of MapReduce jobs to take advantage of Hadoopês ease of service deployment and large-scale parallelism. Yet, Hadoop has not been adapted for multilevel secure \(MLS\vironments where data of different security classifications co-exist To solve this problem, we have used the Security Enhanced Linux 
 Towards A Cross-Domain MapReduce Framework Thuy D. Nguyen, Mark A. Gondree, Jean Khosalim, Cynthia E. Irvine Department of Computer Science, Naval Postgraduate School Monterey, California 93943 tdnguyen, mgondree, jkhosali, irvine}@nps.edu  
Abstract 
SELinux\ Linux kernel extension in a prototype cross-domain Hadoop on which multiple instances of Hadoop applications run at different sensitivity levels. Their accesses to Hadoop resources are constrained by the underlying MLS policy enforcement mechanism. A benefit of our prototype is its extension of the Hadoop Distributed File System to provide a cross-domain readdown capability for Hadoop applications without requiring complex Hadoop server components to be trustworthy 
I NTRODUCTION  The US Department of Defense \(DoD\nd US Intelligence Community \(IC\ recognize they have a Big Data 
Keywords: MapReduce, Hadoop, cross-domain services multilevel security 
 
I 
problem. High volumes of streaming data are ingested from the tactical edge, originating from a variety of sensors  [1   T h e National Geospatial-Intelligence Agency anticipates collecting on the order of four petabytes, annually [2 Mi ssi o n a na l y t i c s  may further require archival data to compare with current intelligence data Agencies are embracing a data-centric model that empowers analysts to query data anywhere in the cloud, based on need-to-know. The IC envisions an agile, shared cloud architecture, following this paradigm [3 T h e N S A r e por t e d l y  operates three private clouds, already: a utility cloud, a storage cloud and a data cloud; the latter uses versions of Hadoop and MapReduce to manage intelligence analytics [4  Th e N a v al 
Tactical Cloud \(NTC\employs distributed cloud-based data services to provide timely access to mission-relevant intelligence and operational data under advanced AntiAccess/Area Denial conditions. The NTC architecture leverages an open-source software stack featuring HDFS Hadoop MapReduce, ZooKeeper and Accumulo  The ability of these products to appropriately handle data of multiple classifications is dubious. Researchers have already discovered commercial cloud products where information flows violating the isolation requirements for multi-tenancy both leaked cryptographic keys and exposed private data [6  The prototype described here is part of our larger investigation into security issues for cloud computing with Big 
Data from sources of different sensitivities. We describe our initial experiments using a modified MapReduce platform to perform Big Data analytics across security domains in an MLS environment, leveraging a novel architecture supported by an underlying secure platform. This MLS-aware 
cross-domain Hadoop 
CD-Hadoop\ prototype is implemented using Security Enhanced Linux 1 SELinux\ [8 nfi gur ed t o  e n for c e M L S policy following the Bell-LaPadula confidentiality policy model [9 Li nux m e d i a t e s  a c c e ss t o i nfor m a t i o n o f di ffe re nt  sensitivity levels based on hierarchical and non-hierarchical security labels of the subjects and objects; Hadoop itself is not involved in MLS policy enforcement. The problem of 
inadvertent contamination of low information by inept or malicious users is beyond the scope of this work The remaining sections describe Hadoop [1  t h e c o n c ep t  of operations, the system architecture, and the implementation of a CD-Hadoop prototype. We conclude by discussing performance tests used to evaluate the overhead incurred while processing read-down operations II 
B ACKGROUND  Apache Hadoop is an open-source implementation that is based on the Google File System  a nd t he Ma p R e duc e parallel computational model developed by Google   The  Hadoop Distributed File System \(HDFS\consists 2 of three 
 
components: the NameNode, Secondary NameNode and DataNode. Two components comprise the Hadoop MapReduce engine: the JobTracker and the TaskTracker. In a Hadoop cluster, there are one NameNode, one Secondary NameNode one JobTracker, and multiple DataNodes and TaskTrackers To run a MapReduce job, the client first copies the jobês input data, configuration file and Map and Reduce functions onto the Hadoop file system as HDFS files. Each file is divided into multiple HDFS blocks, stored on the DataNodes. The client then submits the job to the JobTracker, which creates a set of Map and Reduce tasks for the job. The JobTracker delegates these tasks to different TaskTrackers and monitors the progress of all jobs. Each TaskTracker executes the tasks 
assigned to it by the JobTracker and regularly informs the JobTracker about the status of all outstanding tasks and when it is ready to run a new task. Next, we describe the NameNode   1  In particular, Fedora 13 with Security Enhanced Linux enabled was used 2  Description reflects Hadoop v0.20.2, used in our prototype The views expressed in this material are those of the authors and do no t reflect the official policy or position of the Department of Defense or the U.S Government 
2013 IEEE Military Communications Conference 978-0-7695-5124-1 2013 U.S. Government Work Not Protected by U.S. Copyright DOI 10.1109/MILCOM.2013.243 1436 
2013 IEEE Military Communications Conference 978-0-7695-5124-1 2013 U.S. Government Work Not Protected by U.S. Copyright DOI 10.1109/MILCOM.2013.243 1436 


  
 and DataNode components in more detail, as our design significantly impacts those components C ONCEPT OF O PERATIONS  Hadoop enforces an application-level discretionary access control policy using permission bits similar to UNIX file access controls. Hadoop also maintains user sessions based on the user login IDs. However, with respect to mandatory access control Hadoop lacks the ability for an authenticated user to negotiate a session at a specific sensitivity level, which would be used to determine the resources that user can access under MLS policy In our CD-Hadoop prototype, the user session level is implicitly established by the sensitivity level of the network interface and TCP/IP port from which the request is received The Hadoop file system is structured as a hierarchical tree of directories and files, with an interface similar to the traditional UNIX file system. In a traditional Hadoop cluster there is only one file system and its root is at ç/é. In our MLSenhanced cluster, there are multiple file systems, one per sensitivity level Each of these file systems is managed by an HDFS instance that runs at that level. The root directory of a file system at a particular level is expressed as S YSTEM O VERVIEW  Software is considered 
     
A NameNode is a daemon that manages the HDFS file system namespace and coordinates file access requests from clients. An HDFS file consists of blocks that are replicated and stored on different DataNodes. The block size and replication factor are configurable; defaults are 64MB and 1x respectively. The NameNode decides where the blocks are to be replicated and informs the corresponding DataNode of its selection The primary HDFS namespace data structure  contains the metadata associated with individual files, e.g., file properties and the locations where each block and its replicas are stored. The NameNode also uses a transaction log  keep track of changes to the HDFS metadata. Both data structures are stored as files in the NameNodeês local file system. During start-up, the NameNode reads both files creates a new file in volatile memory, applies the changes indicated by the log, clears and persists both back to disk. During runtime, whenever the structure is updated, it is flushed to disk A DataNode is a daemon that provides the block storage functionality for the cluster. The DataNode stores blocks as files in its local file system. After getting information about the blocks associated with a particular HDFS file from the NameNode, a client sends data requests to the DataNodes that are directly responsible for those particular blocks. The DataNode sends periodic me ssages to the NameNode informing it of its status III e.g  The value is a user-defined string that is administratively associated with one SELinux sensitivity level. To be backward compatible with existing applications the CD-Hadoop prototype treats the traditional root directory  as the file system root at the userês session level. For example, a client running at SECRET can access files stored under the SECRET root directory as either or, simply, as  A user can access HDFS file objects using tools provided with the Hadoop distribution \(e.g., FS Shell\nd with HDFSaware applications that use the HDFS API. A user can read and write file objects at their session level, but can only read file objects at lower levels, i.e., the user can read any objects whose level is dominated by their session level. Writes are only permissible at the userês current session level IV if it executes without privileges in an MLS environment, and yet takes advantage of that environment to provide useful functionality  Fo r example, on a system enforcing a mandatory security policy as modeled by Bell and LaPadula, when an application executes at a particular sensitivity level, it can read from resources labeled at the same or lower levels but can only write to resources labeled at the same level or higher. If the application is modified to reflect the underlying mandatory policyÑe.g., to return the level of the data or make decisions based on the level of the dataÑwe say that the application is MLS-aware The Hadoop code base is large and very complex. Thus, it is prudent to minimize the code changes required to make Hadoop MLS-aware. The Hadoop MapReduce engine enables parallel data processing while HDFS provides distributed data storage. Although the MapReduce server processes keep their internal data structures on the local file system, the data used by the Map and Reduce application tasks are kept in the HDFS Hence, this project focuses on making HDFS MLS-aware, a step towards a secure Hadoop platform suitable for use in MLS environments. In our proof-of-concept design, HDFS server processes running at their particular sensitivity levels are cognizant of the file system namespaces at lower security levels and can access those file objects as the systemês security policy permits. To use this design with real data, each physical node in the Hadoop cluster must be hosted on a trusted platform that mediates the nodeês access to the local file system where local files are labeled\ according to an MLS policy Before discussing our implementation of the CD-Hadoop prototype, we describe the high-level functional requirements for the system and its information flow design. The CDHadoop system must satisfy the following requirements Allow users to modify data only at their session level Allow users to observe data at their session level and at lower sensitivity levels Support a backward compatible HDFS API, to allow existing applications \(which do not require read-down support\ to run unmodified Defer MLS policy enforcement to the underlying trusted computing base \(TCB Minimize the introduction of trusted processes, which would extend the TCB boundary. In particular, avoid a 
level unclass sec-level0 level  secret/<filename filename 
NameNode DataNode 
A fsImage edits fsImage edits edits edits B MLS-aware 
       
1437 
1437 


trusted proxy that can communicat e server processes running at different l e Minimize changes to the existing HD F is motivated by our desire to both m i simplify upgrading to new Hadoop rel Given these requirements, an informatio n the CD-Hadoop system was developed \(see F of the Hadoop server processes run at each s e the same physical node. The underlying TCB servers\ enforces domain separation, informa t and mandatory access controls. Our SELinu x uses sixteen sensitivity levels and up to 1,024 c Figure 1 In a CD-Hadoop cluster, there is one ph y node and multiple physical DataNode nod e N ameNode instances run on the same phy node, whereas DataNode instances are d i different physical nodes. A notional hete r enhanced cluster, capable of handling data different levels, is illustrated in Fig. 2 There are two types of DataNode insta n DataNode instance is the main handler of t h i.e., it is the owner of the files used to store local file system. A DataNode insta n user session level is responsible for han d  e with all HDFS e vels F S software. This i nimize code and eases n flow design for F ig. 1\. Instances e nsitivity level on not the Hadoop t ion flow control x based prototype c ategories  n Flow n both local and o nstrained by the t ion ca n only at the same level plication running HDFS file at e rver running at alf. Although the A C, it is trusted to y Details of the e next section T ATION  and read-down e m y sical NameNode e s. All per-level sical NameNode i stributed across r ogeneous MLSlabeled at three n ces. A  h e HDFS blocks the blocks in the n ce running at the d ling read-down requests on behalf of a primary D a the sensitivity level of the desire d b ecause a client cannot communic a DataNode instance if the client dominate the sensitivity level o instance, i.e., no write-down. Th e surrogate DataNode instances run n defined administratively via the HD In its original design, when a N ameNode daemon will direct t associated blocks from the Data N b locks. This process becomes m introduction of read-down suppor t request, a Name N ode instance runn i level must obtain the metadata of storage locations of the associated b instance running at the requeste d Similarly, when a client contact s request a block at a lower level \(as d the DataNode instance must locat e store the requested block in its loc a this local file is the same as the leve l Figure 2 
D ESIGN AND I MPLEMEN T This section describes the design mechanisms of the prototype CD-Hadoop syst e 
Prototype Design 
  
  
MLS-aware Hadoop Informatio n Information flow between an applicatio n remote\ and the Hadoop server processes is c o MLS policy. Specifically, the applica t communicate with Hadoop processes running as the applicationês session level. When an ap at SECRET requires read-access to an UNCLASS, it must request the HDFS s e SECRET to perform this operation on its beh HDFS server is not trusted with respect to M A perform its other security functions correctl y read-down implementation are discussed in th e V Example of rea d In the example scenarios depic t multi-block files with blocks stored and Z are singleb lock files store d Client-TS requests file W, a file at that level \(NN-TS\directs it to r e corresponding TS DataNode instan c contrast, when Client-TS requests lower level \(S\, it is instructed to DataNode instances \(DN1-TS D surrogate DataNodes for the b lock s on DN1-S, DN2-S\. This is becau is higher than the levels of the b l enforcement does not allow writed Upon receiving the requests, the DN1-TS, DN2-TS\ will perform re as dashed lines in Fig. 2\ and retu r Similarly, node DN3-S handles re a labeled at U. No other scenarios i n and are straight-forward, e.g., Clien t a taNode instance running at d blocks. This is necessary a te directly with a primary s session level does not o f the primary DataNode e number of primary and n ing on a physical node is F S configuration files client requests a file, the t he client to retrieve the N odes responsible for those m ore complicated with the t To handle a read-down i ng at the clientês sensitivity the requested file and the b locks from the NameNode d lower\ sensitivity level s a DataNode instance to d irected by the NameNode e and read the file used to a l file syste m The level of l of the requested block  d down operations t ed in Fig. 2, X and W are on different nodes, while Y d on separate nodes. When its level, the NameNode at e trieve the blocks from the c es \(DN1-TS, DN2-TS\ In read access to file X at a contact the co-located TS D N2-TS\, which are the s associated with the file X s e the clientês session level l ocks, and the MLS policy d own or read-up operations TS surrogate nodes for X ad-down operations \(shown r n the results to Client-TS a d-down requests for file Z n Fig. 2 involve read-downs t S accesses file X 
 
 
A surrogate primary 
1438 
1438 


      
 
The read-down mechanisms introduced in the CD-Hadoop prototype mostly impact the NameNode and DataNode logic and not the JobTracker and TaskTracker logic, as those components only interact with the NameNode and DataNode as HDFS clients In its original design, whenever a change is made to the file system \(e.g., when a directory is created\, the NameNode daemon updates the and data structures that it uses for file system management. The database keeps track of the current file hierarchy, while the  database records the DataNode where each block is stored Both databases are kept in the NameNodeês private memory and are not visible to other NameNode instances Our design introduces two additional data structures, the and the to allow a high sensitivity NameNode instance to look up file metadata and block storage information associated with a file at a lower level. Each NameNode instance manages its own and on the local RAM disk. For every write request, the NameNode instance servicing the request updates pertinent information in the and databases, and then copies the entire content of these databases to the and  structures Other NameNode instances consult the and structures to handle read-down requests, if their sensitivity levels strictly dominate the sensitivity levels of these objects. These other NameNodes use the file metadata obtained from the to check for file permission and block allocation, and the blocks-to-DataNodes mapping information from the to inform the requesting client where blocks are located. The client contacts surrogate DataNodes to perform the actual read-down data transfer Concurrent access to the and by different processes is synchronized using a lockfree multiple-reader, single-writer integrity mechanismÑa high reader gets a valid view of data at a lower level without using locks. Our read-and-retry data consistency mechanism has been designed so that no new covert channels are introduced Each block is stored as two filesÑa block file and a metadata fileÑon either the local file system or a remote file system, e.g., an NFS volume. In its original design, the DataNode daemon maintains a blocks-to-files map to keep track of the file used to store each block on the local file system. Created during initialization, the is updated at run-time whenever a new block is allocated to the DataNode or an existing block becomes inaccessible, e.g., the block is deleted or the file storing the block is corrupted When a client asks to read an HDFS file at a lower level the NameNode instance running at the clientês session level directs the client to contact the fileês surrogate DataNode, colocated with the primary DataNode actually handling the fileês blocks. To return a requested block at a lower level, a surrogate DataNode instance must have access to the  maintained by the primary DataNode instance, to locate the non-HDFS\ files associated with the requested block. Since the is kept in each DataNodeês private memory, a is used to capture the content of the on the systemês RAM disk, so that all surrogate DataNodes whose levels dominate that of the can read it while handling read-down requests The DataNode instances use the same read-and-retry synchronization mechanism utilized by the NameNode to access the shared  Given our objectives to minimize changes to the Hadoop code base and avoid introducing trusted processes, a number of design choices were made for the prototype implementation To ensure uniqueness, the NameNode daemon generates a 64-bit pseudo-random number \(via the Java pseudo-random number generator\ for the Block ID of each new block. In the CD-Hadoop prototype, the Block ID is extended to include the security label associated with the block, to partition the namespace across the Hadoop instances. The Extended Block ID includes a new 4-byte identifier \(Level ID\ describing the sensitivity level of each block. An alternative design is to introduce a new process to manage a pool of Block IDs for the entire cluster. In this alternative design, instead of generating the Block IDs, the NameNode would obtain them from the new process; however, adding a new process would complicate the overall design, and the additional inter-process communication may further decrease the performance of the NameNode With a Level ID in the block identifier, the DataNode can determine independently whether to use its own or a at some lower level to look for the file data. Without the Level ID, the DataNode must search, with a consequent performance impact, its and the s at all lower levels until the required data is found Regarding block storage, the original HDFS design scales linearly, as DataNodes work independently and the number of DataNodes in a cluster can grow over time. The management of the file namespace and block information, however introduces a performance bottleneck, since there is only one NameNode in the cluster and the NameNode keeps the  and databases entirely in memory. In addition, the NameNode is also highly susceptible to resource exhaustion The recently introduced HDFS Federation architecture [1  addresses NameNode scalability by keeping block information across multiple NameNodes. Although this approach partitions the Hadoop file system into multiple namespaces, the memory exhaustion problem still exists since the NameNode continues to keep the and databases in memory In the MLS-enhanced environment, the memory exhaustion problem on the NameNode is exacerbated since there are multiple instances of the NameNode server process on a physical node, requiring additional memory for the and structures. Using a separate 
Prototype Implementation Changes to NameNode Changes to DataNode Implementation Discussion Extended Block ID to Distinguish Levels Scalability across Sensitivity Levels 
B 1 fsImage blockMap fsImage blockMap Cache-fsImage Cache-blockMap CachefsImage Cache-blockMap fsImage blockMap Cache-fsImage Cache-blockMap Cache-fsImage Cache-blockMap Cache-fsImage Cache-blockMap Cache-fsImage CacheblockMap 2 volumeMap volumeMap volumeMap Cache-volumeMap volumeMap CachevolumeMap Cache-volumeMap C 1 volumeMap Cache-volumeMap volumeMap CachevolumeMap 2 fsImage blockMap fsImage blockMap CachefsImage Cache-blockMap 
1439 
1439 


Total HDFS changes 
Implementation Complexity Benchmark Configuration Benchmark Results 
 
3 A Recommender B fsImage blockMap blockMap 
 physical node as a Cache Manager to maintain these databases may ameliorate this problem. With multiple single-level instances of the Cache Manager on the new node, one per sensitivity level, each Cache Manager instance would provide services that a NameNode instance at the same level could use to store and retrieve these databases Using the sensitivity level of the receiving NIC as the requestorês session level, the current prototype can only handle simple policies with a small number of sensitivity levelsÑa typical server platform can support up to 16 NICs. This limits the prototypeês ability to scale up to a larger number of levels A more flexible system design such as the Monterey Security Architecture \(MYSEA\ could be leveraged to overcome this limitation. MYSEA supports an MLS LAN interface on which users can negotiate sessions at different levels 16 Si m i l a r to MYSEA, using one MLS-NIC would allow the prototype to support more complex policies TABLE 1 P ERFORMANCE E VALUATION  This section describes the performance of our MLS-aware Hadoop prototype on a virtualized test environment 
Source lines of code \(SLOC\can be quickly calculated and thus is commonly used as an intuitive metric to estimate the complexity of software and development cost in terms of program size. The Count Lines of Code \(CLOC\ tool [1  w a s utilized to compare the SLOC of the original Hadoop HDFS code with the MLS-aware HDFS code \(see Table 1 CLOC can calculate differences in blank, comment, and source lines in a given file, directory, or archive. The SLOC values shown in Table 1 summarize the number of source lines that were added, removed, modified or unchanged. The delta value is the sum of the addition, removal, and modification of source lines. The percent change value reflects the overall increase in the SLOC between the original and MLS-aware Hadoop, demonstrating that the prototype appears to meet our requirement to minimize changes to the existing software under the assumption that <10% overall change is acceptable VI The test cluster consists of eleven nodes distributed across two racks. Each node in the cluster is a virtual machine \(VM hosted on VMware ESXi 5.0.0. Rack-1 contains three server blades \(each, a Dell PowerEdge R710 system, with 8 CPUs x 2.925 GHz with hyper-threading active, 48GB of memory and Gigabit Ethernet\ack-2 contains a single server blade \(a Dell PowerEdge R610 system, with 8 CPUS x 2.26 GHz with hyper-threading active, 24GB of memory and Gigabit Ethernet\. The racks are connected with multi-port Gigabit Ethernet switches The Hadoop distribution includes a number of performance benchmarking tools that run as MapReduce jobs. The following tools were used to gauge the performance of the current prototype: \(a\NBench, a NameNode stress test; \(b TestDFSIO, an HDFS I/O performance test that reads and writes files in parallel; and \(c\TeraSort, a combination of three test applications designed to sort large amounts of data Mahoutês example program w a s used t o  measure performance, representing a real-world, popular MapReduce application Each test scenario was run on both the original Hadoop and the CD-Hadoop. The JobTrackerês web interface was used to collect job statistics. The HDFS directory used by each test program was removed before starting each test case The NNBench program stresses the NameNode by creating zero-length files, thus forcing the NameNode to repeatedly update its databases. Under this test, the performance of our prototype degrades almost exponentially as the number of files increases. This performance behavior is caused by the overhead of caching the and every time a file is created/modified. Note that NNBench is designed to strain the NameNode with excessive file system operations with zero data, and is not representative of a normal load The TestDFSIO program is designed to measure the I/O performance of HDFS in a normal context. The test consists of writing and reading three datasets of different sizes: 1GB 10GB and 20GB. The results indicate that for the 1GB and 10GB test cases, the prototype performance is marginally slower for both read and write tests. However for the 20GB test case, the writing overhead is much higher while the reading overhead is slightly lower \(but still within the margin of error These write operations take longer because there are more blocks associated with the 20GB file, so it takes more time to flush the entire to the RAM disk The TeraSort test suite was executed three times with different data sizes:  1GB, 10GB and 20GB. For each trial, its three test utilities are invoked in the same order: \(TeraGen 001\027  TeraSort 001\027 TeraValidate\out read-down operations, the TeraSort test results are roughly comparable to the performance of the original Hadoop. This indicates that the cost of accessing HDFS files has a minimal effect on the overall performance of a typical MapReduce job. However, the performance impact grows exponentially for read-down requests as the data size increases. The additional degradation was caused by the overhead of reading the cache databases, which the NameNode and DataNodes must do to handle a read-down request The Mahout Recommender test scenario used a 1MB dataset obtained from GroupLens Research [19 w h ich  consisted of 1 million ratings of 4000 movies by 6000 users 
M ODIFICATION C OMPLEXITY SLOC   SLOC Delta % Change Original Hadoop MLSaware Hadoop NameNode 14373 15974 1890 13.15 DataNode 6914 7399 692 10.01 Other modules 68328 68890 732 1.07 
 
89615 92263 3314 3.70  
   
1440 
1440 


                     
This test consisted of running the Recomme n times, in three different configurations: Had o without read-down, and CD-Hadoop with re the test programs discussed previously, whi c MapReduce job, the Recommender test uses jobs. The averages of the five runs are p resen t overhead of CD-Hadoop with no read-down original Hadoop, is approximately 3.25%; th e down overhead is about 0.08%, a total overhe a Figure 3  p ropose an approach that sup p computations on sensitive data, while preserv i the data providers Aira va t r uns o n S E SELinuxês default a relaxed policy that only constrains selected \(targeted modifies HDFS to enforce SELinux-like m control, expanding its reference monitor and T include complex application code. This is in c approach, which depends solely on SELin u enforcement to control information leaks resources VIII  subjects. Airavat m andatory access T CB boundar y to contrast with our u xês MLS policy through system h e feasibility and o nment. Multiple f ferent sensitivity e s are constrained o es not introduce g TCB boundary n t implementation u ce platform that e nts. Preliminary d ation under the stress test, but h small, realistic n access Big Data at multiple classifications w classification of data through dupli c complications due to poly-instanti utilization of data for mission-orien t R EFEREN C 1 
R ELATED W ORK  Roy C ONCLUSION  The CD-Hadoop prototype demonstrates t h practicality of using Hadoop in an MLS envir o instances of Hadoop servers can run at di f levels while their accesses to Hadoop resourc e by the underlying trusted OS. Our design d o any trusted processes outside the pre-existin g and only affects the HDFS servers. The curre n is a first step toward a highly secure MapRed u can be used in high-risk MLS environm e testing shows notable performance degra d pathological workload of the NameNode relatively modest performance overhead wit h access patterns. We believe a platform that ca n  n der program five o op, CD-Hadoop ad-down. Unlike ch use only one ten MapReduce t ed in Fig. 3. The compared to the e additional reada d of 3.34  Results e ntries, but its file p ical Big Data g er datasets may p erformance cost k e Mahout p orts MapReduce i ng the privacy of E Linux but uses 
et al targeted policy type enforcement 
Forum: The Magazine of the National I nformationWeek G o Proc. of the 16th AC M Communications Security of the IEEE Symposium on Security an d Proc. of the Nineteenth ACM Sy m Principles \(SOSP '03 Proc. of the Sixth D esign and Implementation \(OSDI '04  Proc. 1 3 Conference P roc. of the 2009 A C Computing \(ACM CCS STC è09 P roc. of the 2010 Military Comm u 2010 Geospatial Intelligence Intelligence Community o vernment M Conference on Computer and Proc d Privacy m posium on Operating Systems Symposium on Operating System  3 th National Computer Security C M Workshop on Scalable Trusted u nication Conference \(MILCOM 
Average of Mahout Benchmark The test data used contained one million e size was relatively small relative to ty p applications \(about 10MB\ Tests with lar g allow more meaningful reasoning about the p of the prototype with multi-job applications li k VII D. Meiron, S. Cazares, P. Dimotakis McNulty, D. Long, F. Perkins, W. P r Tonry, and P. Weinberger, çData an a Rep. JSR-08-142, Dec. 2008 2 P. Buxbaum, "GEOINT's big data c h Issue 3, pp. 4-7, 2012. http://goo.gl/sb Y 3 Intelligence and National Security All i what is commonly done," Whitepaper Accessed: March 2013 4 J. Nicholar Hoover, "NSA CI O architecture http://goo.gl/HMXbY. Accessed: Mar c 5 B. Junker, çNavy Tactical Cloud S T Annual C5ISR Government and 2012.  http://goo.gl/yMMCE. Accesse d 6 T. Ristenpart, E. Tromer, H. Shacham of my cloud: exploring information clouds,é in 2009 7 S. Chen,; R. Wang, X. Wang, and K Web Applications: A Reality Today a 8 The SELinux Notebook Ñ T h http://goo.gl/9wJkQ. Accessed: March 9 D. Bell and L. La Padula. çSec u Exposition and Multics Interpretatio n USAF. ESD-TR-75-306, MTR-2997 R  Apache Hadoop. http://hadoop.apache  S. Ghemawat, H. Gobioff and S-T. Le New York, NY   J. Dean and S. Ghemawat, çMapRedu c Large Clusters,é in  C. E. Irvine, T. Acheson, and M. F T multilevel file system,é in Washington, DC, pp. 450   Apache Hadoop: HDFS Federatio n March 2013  C. E. Irvine, T. D. Nguyen, D. J. Shi f Prince, P. C. Clark, and M. Gondree Architecture,é in Chic  T. D. Nguyen, M. Gondree, D. J. Shi f E. Irvine, çA Cloud-Oriented CrossD San Jose, CA, pp. 1702Ö1707 2  Count Lines of Code \(CLOC p r o Accessed: March 2013  Apache Mahout. http://mahout.apache  GroupLens Research project http://goo.gl/MydMo. Accessed: Marc h  I. Roy, S. T.V. Setty, A. Kilzer, V. Sh m Security and Privacy for MapReduce  Symposium on Networked Systems D 2010\ San Jose, CA, April 2010 w ithout increasing the c ation, necessarily incu r ring atio n can facilitate better t ed intelligence analysis C ES  F. Dyson, D. Eardley, S. Kelle r r ess, R. Schwitters, C. Stubbs, J a lysis challenges,é JASON, Tech h allenge Vol. 10 Y ch. Accessed: March 2013 i ance, "IC ITE: Doing in common Feb. 2013. http://goo.gl/uTIWR O pursues Intelligence-sharing April 21, 2011 c h 2013 T Objectives,é presented at the 6th Industry Partnership Summit d March 2013 and S. Savage, çHey, you, get off leakage in third-party compute Zhang.  çSide-Channel Leaks in a Challenge Tomorrow,é in pp. 191-206, 2010 h e Foundations,é 2nd Edition 2013 u re Computer Systems: Unified n Electronic Systems Division R ev.1. Hanscom AFB, MA. 1976 org/. Accessed: Sept. 2012 u ng, çThe Google file system,é in  2003 c e: Simplified Data Processing on San Francisco, CA, 2004 T hompson, çBuilding trust into a  459, 1990 n http://goo.gl/lhxzz. Accessed ff lett, T. E. Levin, J. Khosalim, C MYSEA: the Monterey Security a go, IL, pp. 39Ö48, 2009 f flett, J. Khosalim, T. E. Levin, C D omain Security Architecture,é in 2 010 o ject. http://cloc.sourceforge.net org/. Accessed: March 2013 MovieLens Data Sets h 2013 m atikov and E. Witchel, çAiravat  in Proceedings of the 7th Usenix D esign and Implementation \(NSDI 
  
1441 
1441 


not possible under other visualization frameworks because they lacked the ability to succinctly express the differences between the different overplotting treatments V I MPLEMENTATION The ability to interactively switch between different Abstract Rendering encodings is a signiìant advantage of Abstract Rendering over other libraries Efìciency changing rest directly on implementation decisions Abstract Rendering has been implemented in Java and Python This section describes the implementation in Java The Python implementation is similarly structured but differs in the parallelization strategy relying heavily on vectorization through NumPy The Abstract Rendering implementation follows the definition provided in Section II Aggregate info and transfer functions are directly represented though aggregate functions are slightly modiìed to provide efìcient execution in out-of-core environments The select function is replaced with a Renderer class that controls the overall data access order This includes access not just to the underlying dataset as the select function implies but also to the aggregate values i.e the order and frequency that the x/y values appear in Renderers fall into two general categories by pixel and by glyph Bypixel renderers perform selection essentially as described in Section II This strategy is efìcient when used with datasets that are spatially arranged such as a quad-tree Any given glyph will be accessed once for each pixel the glyph touches Many data structures do not efìciently handle the highly spatial nature of these queries and thus a pixel-oriented rendering strategy is not effective A glyph-based rendering strategy takes the opposite tack Each glyph is visited exactly once and each pixel may be updated multiple times This enables efìcient rendering when using non-spatial data structures because each glyph is visited in an order convenient to its container However the different iteration order means that synthetic values i.e the s xy values are typically updated multiple times The aggregation function does not receive a list of info function results as described in Section II Instead it receives one info result and a pre-existing aggregate value To compensate for this difference aggregate functions must provide a zero value and must be commutative/associative if deterministic execution is desired The zero is used to initialize the synthetic data space When presented with a list of info results the operator receives all of its operands at once and thus commutativity and associativity are not signiìcant The implementation supports a number of parallelization strategies and data conìgurations A full analysis is beyond the scope of this paper In brief thread-level parallelism and vector-based parallelism have been explored GPU distributed memory and streaming data conìgurations have also been investigated All use the components described   Figure 10 Scaling behavior of Abstract Rendering as processor count increases Scaling behavior is near linear as processors as are added but shows no additional improvements with hyper-thread processors processors 8-15 are hyperthreads and improve by less than 5 vs 8 cores above augmented with various helper functions to facilitate data access or partial result combination The out-of-core conìguration is used in performance analysis Section VI and to handle the Kiva data set Section IV-B VI P ERFORMANCE Section III described several Abstract Rendering phrasings demonstrating its expressive capabilities To be practical the framework must be performant as well as expressive An simple characterization of runtime performance was done with an eight physical core machine Two adjacency matrix visualizations were used The rst was the Kiva dataset described Section IV-B The second dataset is an adjacency-list of links found on Wikipedia receiving the same treatment The Wikipedia data set includes 153 million edges representing the links of the largest connected cluster if the category system is ignored Both data sets were binaryencoded adjacency-lists stored in a memory mapped le The le contents were streamed off disk and rendered in a glyphparallel strategy These datasets were used because the data volume is sufìcient to require out-of-core processing but require simple analysis to create a visual representation Figure 10 presents the average performance over 10 executions while keeping the core count xed In general more processors are more helpful but hyperthreading is not The scaling characteristic is similar between the two datasets but the difference shows the overhead of abstract rendering in general Even though this Wikipedia data set is four times larger than the Kiva data set the overall runtime is only three times longer on average Overall the performance numbers are generally supportive of interactive visualization applications 15 


VII F UTURE W ORK Current Abstract Rendering implementations closely tie the bin-elements with the display resolution and region This decision introduces view-dependent effects View-dependent effects are used advantageously in high-deìnition alpha composition but may not always be desirable Developing techniques for avoiding these effects and guidelines for their usage is an ongoing effort Section V described implementation considerations There are several unexplored options that may lead to more efìcient implementations or to implementations that run in more complex runtime environments Options include distributed memory or efìcient GPU execution The idea of binning is shared inMens Abstract Rendering can be applied to more than just overplotting High-deìnition alpha composition rests on the idea of measuring pixel-level information This same idea can be applied to screen-space metrics for visualization evaluation 17 Such applications are also being e xplored The idea of the transfer function comes from scientiìc visualization However years of research into transfer functions has yielded many interesting techniques Mixed rendering styles and context-aware highlighting are strong candidates for exploration Additionally as noted in Section I Z-ordering creates an implicit volume-like space Some volume-based techniques from scientiìc visualization maybe more directly applicable by more literally applying this metaphor VIII C ONCLUSIONS Visualizing large data sets inevitably runs into overplotting issues By considering the rendering process as binning Abstract Rendering provides a means to unify many overplotting techniques Furthermore those techniques can be encoded succinctly at compile time and executed efìciently at runtime R EFERENCES  S K Card J Mackinlay  and B Shneiderman Readings in Information Visualization Using Vision to Think  Morgan Kaufman 1999  M Bostock and J Heer  Proto vis A graphical toolkit for visualization IEEE Transactions on Visualization and Computer Graphics  vol 15 no 6 pp 1121Ö1128 2009  M Bostock V  Ogie v etsk y  and J Heer  D3 DataDriven Documents IEEE Trans Visualization  Comp Graphics Proc InfoVis  2011 A v ailable http vis.stanford.edu/papers/d3  J A Cottam Design and implementation of a stream-based visualization language Ph.D dissertation Indiana University 2011  L W ilkinson The Grammar of Graphics  2nd ed New York Springer-Verlag 2005  H W ickham  A layered grammar of graphics  Journal of Computational and Graphical Statistics  vol 19 no 1 pp 3Ö28 March 2010  G S Da vidson B Hendrickson D K Johnson C E Meyers and B N Wylie Knowledge mining with VxInsight Discovery through interaction Journal of Intelligent Information Systems  vol 11 no 3 pp 259Ö285 1998  J A Cottam and A Lumsdaine Extended assortitivity and the structure in the open source development community in International Sunbelt Social Network Conference  International Network for Social Network Analysis January 2008 A v ailable http://www.insna.org/PDF/Awards/awards  ms 2007.pdf  C Muelder  F  Gygi and K.-L Ma V isual analysis of inter process communication for large-scale parallel computing IEEE Transactions on Visualization and Computer Graphics  vol 15 no 6 pp 1129Ö1136 Nov 2009 Available http://dx.doi.org/10.1109/TVCG.2009.196  National Historical Geographic Information System V ersion 2.0 University of Minnesota Minneapolis MN 2011  A v ailable http://www nhgis.or g  T  Porter and T  Duf f Compositing digital images  SIGGRAPH Comput Graph  vol 18 no 3 pp 253Ö259 Jan 1984 A v ailable http://doi.acm.or g/10.1145 964965.808606  J Johansson P  Ljung M Jern and M Cooper  Re v ealing structure within clustered parallel coordinates displays in Proceedings of the Proceedings of the 2005 IEEE Symposium on Information Visualization  ser INFOVIS 05 Washington DC USA IEEE Computer Society 2005 pp 17 Available http://dx.doi.org/10.1109/INFOVIS.2005.30  H Hagh-Shenas V  Interrante C Heale y  and S Kim Weaving versus blending a quantitative assessment of the information carrying capacities of two alternative methods for conveying multivariate data with color in Proceedings of the 3rd symposium on Applied perception in graphics and visualization  ser APGV 06 New York NY USA ACM 2006 pp 164Ö164 A v ailable http://doi.acm.org/10.1145/1140491.1140541  T  E Oliphant Guide to NumPy  Provo UT Mar 2006  A v ailable http://www tramy us  Z Liu B Jiang and J Heer  immens Real-time visual querying of big data Computer Graphics Forum Proc EuroVis  vol 32 2013 A v ailable http vis.stanford.edu/papers/immens  J W  T u k e y and P  A T u k e y  Computer Graphics and Exploratory Data Analysis An Introduction in Proceedings of the Sixth Annual Conference and Exposition Computer Graphics  Fairfax VA Nat Computer Graphics Association 1985 pp 773Ö785  A Dasgupta and R K osara P ar gnostics Screen-space metrics for parallel coordinates IEEE Transactions on Visualization and Computer Graphics  vol 16 no 6 pp 1017Ö1026 Nov 2010 A v ailable http dx.doi.org/10.1109/TVCG.2010.184 16 


state of innovation stakeholder  node PQ It  s a balanced node Based on this, we could calculate the  node PQ Calculation process is: set different inn ovation stakeholders state i U  and j U Value of ij 000T can be get from  innovation time difference. Innovation stakeholdersí social effect and industrial effect can be obtained upon ij B  and ij G set according to relation between innovation stakeholders  Model 4.1 points out  that the value of Gij  directly affects social benefits and sector benefits. Large Gij  can lead to increasing benefits of the entire industry and the entire social growth Bij reflects big organizationís impact on businesses. Only strengthening the inter agent association within big organization and enhancing the str ategic partnership between enterprises can jointly promote the development of the entire industry, and bring more social benefits, so that each agent can be improved   5 Summary This paper puts forward the concept of the big organization based on the CSM t heory. It introduces the basic implication of the big organization and theoretical framework of the big organization including: the big organization's perspective  overall perspective, dynamic perspective, and new resource perspective; the big organizat ionís sense  the purpose of the organizational structure is innovation, organizational activities around the flow of information, breaking the traditional organizational structure, encouraging self run structure, and blurring organizational boundaries; the big organizationís platform  the platform ecosystem of the big organization ; the big organizationís operation mode  borderless learning mode, and cluster effect; the big organizationís theory  active management theory  leading consumers, and culture  entropy reduction theory  negative culture entropy and humanistic ecology theory  inspiring humanity, and circuit theory  a virtuous circle, and collaborative innovation theory  collaborative innovation stakeholder. This paper also discusses culture entropy reduction theory of the big organization  negative culture entropy, and coordinated innovation theory  innovation stakeholders collaboration. Culture entropy change model and collaborative in novation model are constructed   The research has just begun for the big organization. It also needs further improvement but remains the trend of the times   Reference  1  Gordon Pellegrinetti, Joseph Bentsman. Nonlinear Control Oriented Boiler Modeling A Benchmark Problem for Controller De sign [J  I E E E tr a n s a c tio n s o n c o n tr o l s y s te m s te c h n o lo g y 2 0 1 0  4 1\57 65  2  Klaus Kruger, Rudiger Franke, Manfred Rode Optimization of boiler start up using a nonlinear 457 


boiler model and hard constraints [J  E n e r gy 201 1 29   22 39 2251  3  K.L.Lo, Y.Rathamarit  State estimation of a boiler model using the unscented Kalman filter [J  I E T  Gener. Transm. Distrib.2008 2 6\917 931  4  Un Chul Moon, Kwang. Y.Lee. Step resonse model development for dynamic matrix control of a drum type boiler turbine system [J IE E E  T ra nsactions on Energy Conversion.2009 24 2\:423 431  5  Hacene Habbi, Mimoun Zelmat, Belkacem Ould Bouamama. A dynamic fuzzy model for a drum boiler turbine system [J  A u to m a tic a 2 0 0 9 39:1213 1219  6  Beaudreau B C. Identity, entropy and culture J   J o ur na l  o f  economic psychology, 2006, 27\(2 205 223  7  YANG M, CHEN L. Information Technique and the Entropy of Culture J  A cad e m i c E x ch a n g e  2006, 7: 048  8  ZHANG Zhi feng. Research on entropy change model for enterprise system based on dissipative structure J  Ind ustrial  Engineering and  Management 2007, 12\(1\ :15 19  9  LI Zhi qiang, LIU Chun mei Research on the Entropy Change Model for Entrepreneurs' Creative Behavior System Based on Dissipative Structure J  C h i n a S of t S c i e n c e  2009   8  1 62 166   458 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





