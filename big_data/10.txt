Integrating Multi-Objective Genetic Algorithms into Clustering for Fuzzy Association Rules Mining Mehmet KAYA Department of Computer Engineering F\365rat University 23119, Elaz\365 000 Turkey kaya@firat.edu.tr Reda ALHAJJ ADSA Lab, Department of Computer Science University of Calgary Calgary, Alberta, Canada alhajj@cpsc.ucalgary.ca Abstract In this paper, we propose an automated method to decide on the number of fuzzy sets and for the autonomous mining of both fuzzy sets and fuzzy association rules. We compare the proposed multi-objective GA based approach with: 1\URE based approach; 2\ Chien et al clustering approach Experimental results on 100K transactions extracted from the adult data of United States census in year 2000 show that 
the proposed method exhibits good performance over the other two approaches in terms of runtime, number of large itemsets and number of association rules 1. Introduction In general, quantitative mining algorithms either ignore or over-emphasize elements near the boundary of an interval The use of sharp boundary intervals is also not intuitive with rception. Some work has recently been done on the use of fuzzy sets in discovering association rules e.g., [1, 4  Ho we v e r  in  existing approaches fuzzy sets are either supplied by expert or determined by applying clustering algorithm. The former is not realistic because it is extremely hard for an expert to specify fuzzy sets. The latter approaches have not produced 
satisfactory results. They do not considered the optimization of membership functions; a user specifies the number of fuzzy sets and membership functions are tuned accordingly In this paper, we propose a clustering method that employs multi-objective GA for the automatic discovery of membership functions used in determining fuzzy quantitative association rules. Our approach optimizes the number of fuzzy sets and their ranges according to multi-objective criteria in a way to maximize the number of large itemsets with respect to a given minimum support value. So, we s and the time required to determine fuzzy sets. These two are in conflict with each other. So, we use a GA with multiple objective optimization capabilities known as 
Pareto GA   Experimental results demonstrate the effectiveness of the we compared the proposed approach, in terms of the number of produced large itemsets and interesting association rules, with CURE based approach 2 a nd C h ie n et al approach [3 w h ic h is a n e ffic i e n t  hierarchical clustering algorithm based on variation of density to solve the problem of internal partitioning The rest of this paper is organized as follows. Fuzzy association rule is defined in Section 2. Utilizing GA to determine membership functions is described in Section 3. A brief overview of CURE based approach and Chien et al work is given in Section 4. Experimental results are given in Section 5. Section 6 is summary and conclusions 
2. Fuzzy Association Rules Consider a database of transactions T  t 1 t 2 311,t n where each t j presents the j th tuple in T We use I  i 1 i 2 311,i m to represent all attributes that appear in T each quantitative attribute i k is associated with at least two fuzzy sets. The degree of membership of each value of i k in any of its fuzzy sets is directly based on the evaluation of the membership function of the particular fuzzy set with the value of i k as input. The value falls in the interval with the lo we r  
es \215not a member\216, the upper bound 1 indicates \215total membership\216; and all other values between 0 and 1, exclusive, specify \215partial membership\216. Finally, we use the following form for fuzzy association rule If Q  u 1  u 2  311  u p is F 1   p fff 111  21 K then R  v 1  v 2  311  v q is F 2   q fff 222  21 K   where I Q 001 and I 
R 001 are itemsets with 002  RQ I  F 1 and F 2 respectively, contain the fuzzy sets associated with corresponding attributes in Q and R  i.e i f 1 is a fuzzy set related to attribute u i and j f 2 is related to attribute v j  3. Multi-Objective GA for Automated Clustering We consider as objective functions the number of large itemsets and the gain in time, inverse of the time required to find all large itemsets in a given database. It is assumed that each of the n components of the objective vector is to be 
maximized. An optimal solution can be defined as a solution not dominated by any other solution in the search space  Such a solution is called Pareto optimal and the entire set of optimal trade-offs is called Pareto-optimal set   Each individual represents the base values of membership functions for a quantitative attribute from the given database We used membership functions in triangular shape To illustrate the utilized encoding scheme, consider a quantitative attribute, say i k  having 3 fuzzy sets, the corresponding membership functions and their base variables Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


are shown in Figure 1. Each base variable takes finite values value 1 k i b lies between the minimum and maximum values of attribute i k  denoted  min k i D and  max k i D spectively. Enumerated next to Figure 1 are the search intervals of all the base values and the intersection point k i R of attribute i k   small medium large 1 k i b 2 k i b 3 k i b 4 k i b 265 i k   min k i D  max k i D k i R  max  min   max    min   max  min   max  min  4 3 2 1 kkk kkk kkk kkk kkk iii iii iii iii iii DDb DRb RDb DDR DDb Figure 1 Membership functions and base variables of attribute i k We used 8 quantitative attributes in the experiments of this study and assumed that each attribute can have at most 7 fuzzy sets. So, a chromosome consisting of the base lengths and the intersecting points is represented in the form 1110 5 12 1 1110 5984763542321 12 1 888888111111111111121111 iiiiiiiiiiiiiiiiiiiiiiii bbRbbwbbRbbRbbRbbRbbRbbw KK where gene j i w denotes the number of fuzzy sets for attributes j i If the number of fuzzy sets is 2, then while decoding the individual, the first two base variables are considered and the others are omitted. However, if j i w is 3 then the next three variables are also taken into account. So as long as the number of fuzzy sets increases, the number of variables to be taken into account is enhanced too romosomes are represented as floating point numbers and their genes are the real parameters. While the value of a gene is reflected under its own search interval, the following formula is employed  min  max  min max k i k i k i k i jjjj bb g g bb 212   g is the value of the gene in search max g is the maximum value that gene g may take  min k i j b and  max k i j b are the minimum and the maximum values of the reflected area, respectively. Also, we used Pareto-based ranking procedure, where the rank of an individual is the number of solutions encoded in the population by which its corresponding decision vector is dominated. Individuals who are strong according to parent selection policy are candidates to form a new population. We adapted the elitism selection policy in our experiments Finally, after selecting chromosomes with respect to the evaluation function, genetic operators such as, crossover and mutation, are applied to these individuals To generate fuzzy association rules, the following formula is used to calculate the fuzzy support of itemset Z and its corresponding set of fuzzy sets F denoted S Z,F        T ztFf S Tt jij Zz z FZ ij j 000 003 004\004  004  265 where T denotes the number of transactions in database T  Each large itemset, say L is used in deriving all association rules L 212 S  000 S for each  LS 001 e strong association rules discovered are chosen by considering only rules with confidence over a pre-specified minimum confidence.  However, not all of these rules are interesting enough to be presented to the user. Whether a rule is interesting or not can be judged either subjectively or objectively. Ultimately, only the user can judge if a given rule is interesting or not, and this judgment, being subjective may differ from one user to another. However, objective interestingness criterion based on the statistics behind the used as one step towards the goal of weeding out presenting uninteresting rules to the user of CURE and Chein et al Work The process of CURE can be summarized as follows Starting with individual values as individual clusters, at each w cluster. This is repeated until only k clusters are left. As a result, the values of each attribute in the database are distributed into k clusters. The centroids of the k clusters are the set of midpoints of the fuzzy sets for the corresponding attribute. Here, note that in the process to obtain the membership functions by CURE clustering algorithm, the number of clusters, i.e., number of fuzzy sets should be given by the user beforehand. To overcome this restriction, we integrated a GA with CURE clustering approach A GA finds the most appropriate number of clusters according to a predefined fitness function. In the GA process used in this study, each variable holds the number of fuzzy sets only. This is because CURE clustering algorithm itself adjusts the base values of the membership functions As Chien et al clustering approach is concerned, it is an efficient hierarchical clustering algorithm based on variation of density to solve the problem of interval partitioning. For this purpose, two main characteristics for clustering numerical data are defined first. Then, a reasonable interval can be generated automatically by giving a proper parameter 005 ance of relative closeness and relative inter-connectivity. The reader is referred to for  more details about this clustering technique 5. Experimental Results Effectiveness of the proposed approach has been demonstrated by comparison with two existing clustering approaches: CURE based approach and Chien et al work We concentrate on testing the time requirements as well as s in the main factors that affect the proposed clustering process: finding nondominated sets, number of large itemsets, and number of association rules. The experiments have been conducted on Pentium III 1.4 GHz CPU with 512 MB memory and running Windows 2000. As experiment data, we used 100K transactions from the adult data of US census in 2000; we concentrated our analysis on 8 quantitative attributes. Further, in all the experiments conducted in this study, the GA process started with a population of 80 individuals for the GA-based approach and 30 individuals for the other approach. As the termination criteria for the developed GA programs, the maximum Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


number of generations has been fixed at 500. Finally, in all the experiments in which GA have been used, the minimum support was set to 10%, unless otherwise specified, and the maximum number of fuzzy sets has been specified as 7 for each of the three methods  1000 1250 1500 1750 2000 2250 2500 2750 3000 0 50 100 150 200 250 300 gain in time \(ms num be r o f l a rge i t e m s e t s MOGA CURE Chien's work Figure 2 Nondominated set using 20K transactions The first experiment is dedicated to find the nondominated set for each of the three different methods using 20K transactions. We decided to use 20K transactions because according to the next two experiments, the three approaches perform almost the same up to 20K transactions ported in Figure 2, where the three approaches are labeled as MOGA, CURE and Chein\220s work to represent the proposed approach, CURE based approach and Chien et al work based approach, respectively. MOGA mostly outperforms the others for both objectives 0 10 20 30 40 50 60 70 80 90 10 20 30 40 50 60 70 80 90 100 Number of Transactions \(K R unt i m e  S e c onds  MOGA CURE Chien's work Figure 3 Runtime to find large itemsets for optimum case The second experiment compares the runtime of the three approaches to find large itemsets for different numbers of transactions, varying from 10K to 100K. The results are reported in Figure 3. The runtime here represents the time s after the number of fuzzy sets and their ranges have been determined by employing the corresponding method. MOGA outperforms the other two approaches for all numbers of transactions. Finally, the curves plotted in Figure 3 demonstrate that the three methods are scalable with respect to the number of transactions The third experiment compares the runtime of the three approaches to find large itemsets when the number of fuzzy ported by the curves plotted in Figure 4. We have decided on considering 5 fuzzy sets in this experiment because it is approximately the average number of fuzzy sets found by each of the three es outperform MOGA; the extra time in MOGA is spent on optimizing membership functions 0 10 20 30 40 50 60 70 80 90 10 20 30 40 50 60 70 80 90 100 Number of Transactions \(K R unt i m e  s e c onds MOGA CURE Chien's work Figure 4 Runtime to find large itemsets for 5 fuzzy sets 6.5 7 7.5 8 0 102030405060708090100 Number of Transactions \(K lo g  T o ta l R u n tim e  m ilis e c onds   MOGA CURE Chien's work Figure 5 Total runtime required to find optimum fuzzy sets 0 250 500 750 1000 1250 1500 1750 2000 6 8 10 12 14 16 18 20 Minimum Support N u mb er o f  Lar g e r I t ems ets MOGA CURE Chien's work Figure 6 Number of large itemsets for optimum fuzzy sets The fourth experiment compares the total runtime required for each of the three methods to find optimum fuzzy sets for different numbers of transactions. The results are reported in Figure 5; the total runtime of MOGA is smaller than the other two approaches up to around 40K transactions after that, MOGA requires higher execution time than the other two approaches. The extra runtime is spent on optimizing membership functions. Figure 5 shows that all the three approaches scale well on the number of transactions r of large itemsets for different values of minimum support All the 100K transactions have been utilized and the optimum solution case has been considered. The results are reported by the curves plotted in Figure 6; MOGA finds larger number of large itemsets than the other two approaches. This is quite consistent with our intuition simply because MOGA puts more effort on the optimization Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


process and this has been reflected into finding better results than classical clustering approaches  0 250 500 750 1000 1250 1500 1750 2000 2250 2500 6 8 10 12 14 16 18 20 Minimum Support N u m b er o f L a rg e It em s e t s MOGA CURE Chien's work Figure 7 Number of large itemsets for 5 fuzzy sets 0 50 100 150 200 250 300 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 m Confidence N u m b e r of In t e re s t i ng A s s o c i a t i on Rul e s MOGA CURE Chien's work Figure 8 Number of association rules for optimum case 0 50 100 150 200 250 300 350 400 0.2 5 3 0.35 0.4 5 5 0.55 0.6 Minimum Confidence N u m b e r of I n t e r e st i ng A sso c i a t i on Ru le s MOGA CURE Chien's work Figure 9 Number of association rules for 5 fuzzy sets The sixth experiment is similar to the fifth but here 5 of the optimum case. The three curves plotted in Figure 7 show the number of large itemsets for different values of minimum support. For small the three curves is larger than the difference for the optimum solution case shown in Figure 6. Finally, for the two cases plotted in Figures 6 and 7, the curves become smoother and the difference between them decreases as the minimum support increases. This is true because as the minimum support s and approaches zero The last two experiments report the correlation between minimum confidence and number of interesting association overed for each of the three approaches. Figure 8 reports the values for the optimum solution case. Figure 9 gives the results in case the number of clusters is set to 5 for each of the three methods. MOGA optimizes the ranges of the membership functions and the number of fuzzy sets in a way that outperforms the other two approaches 6. Summary and Conclusions In this paper, we proposed a multi-objective GA based clustering method, which automatically adjusts the fuzzy sets to provide large number of large itemsets in low duration This is achieved by tuning together, for each quantitative attribute, the number of fuzzy sets and the base values of the membership functions. In addition, we demonstrated through experiments that using multi-objective GA has 3 important advantages over CURE and Chien et al work. First, the number of clusters for each quantitative attribute is determined automatically. Second, the GA-based approach optimizes membership functions of quantitative attributes for a given minimum support. So, it is possible to obtain more appropriate solutions by changing the minimum support in the desired direction. Finally, the number of large itemsets and interesting association rules obtained using the GAbased approach are larger. As a result, all these advantages show that the proposed approach is more appropriate and can be used more effectively to achieve optimal solutions than the classical clustering algorithms described in the literature ferences 1 K  C  C  C h an an d W  H  A u   215M i n i n g F u z z y  A sso c i at i o n Rules,\216 Proc. of ACM CIKM 209-215, 1997 2  S G uha  R  R a s t o g i and K  Sh i m  215C U R E  A n E f f i c i ent  Clustering Algorithm for Large Databases,\216 Information Systems 35-58, 2001 3 B  C  C h i e n Z  L  L i n and T  P  H o ng  215A n E f f i c i ent  Clustering Algorithm for Mining Fuzzy Quantitative Association Rules,\216 ress and NAFIPS Conference 1306-1311, 2001 4 T  P  H o n g C  S  K u o a n d S  C  C h i   215 M in in g A s s o c i a tio n  ve Data,\216 Intelligent Data Analysis  Vol.3, pp.363-376, 1999 5  B  L e nt  A  Sw am i and J W i do m  215C l u s t er i n g A sso c i at i o n Rules,\216 Proc. of IEEE ICDE pp.220-231  1997 6  R  J  M i l l e r a nd Y  Y a ng  215A sso c i at i o n R u l e s o v e r  I n t e r v al  Data,\216 Proc. of the ACM SIGMOD pp.452-461, 1997 8 R  S r ik a n t a n d R  A g ra w a l. \215 M in i n g Q u a n t i t a t i v e  Association Rules in Large Relational Tables,\216 Proc. of ACM SIGMOD 2, 1996 8 R  R  Y a g e r  215 F u z z y S u m m a rie s in D a ta b a s e M i n i n g 216  Proc of Artificial Intelligence for Application pp.265-269, 1995 9 W  Z h a n g   215 M in in g F u z z y Q u a n ti ta tiv e A s s o c i a t io n R u le s  216  Proc. of IEEE ICTAI pp.99-102, 1999   E  Z i t z l e r  and L  T h i e l e  215 M u l t i o bj ec t i v e E v o l ut i o nar y  Algorithms: A Comparative Case Study and the Strength 216 IEEE TEC Vol.3, pp.257-271, 1999  M K a y a and R  A l ha j j   215Mu l t i O b j e c t i v e G e n e t i c  Algorithm Based Method for Mining Optimized Fuzzy Association Rules,\216 Proc. of IDEAL Springer, Aug. 2004 Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


each record the number of transactional items is up to 8.  The transactional items are randomly chosen from the pool.  There is no duplication of the same item in a given Proceedings of the International Conference on Information Technology: Coding and Computing  ITCC  04 0-7695-2108-8/04 $ 20.00  2004 IEEE transaction record.  Each transactional item in this file has the initial bargain level of zero C. Remove the extra copies of the source and affiliated names from the pool of transactional items D. For those transactions in which both the source item and an affiliated item exist, set the bargain level of M percent of the total occurrences of the affiliated item to the discount level of the rule ri.  The value of M is randomly chosen, but its range is limited by the amount of discount The range of M for different L values is shown in Table 3 Step 2. End The behavior of the above algorithm for business rule 2 follows.  Before creating transaction records for the periods of t1 and t2, I1 and I4 are added three times \(based on the value of E in Table 3 items.  The transactional item pool now has 26 items Upon the completion of creation of the records for the periods of t1 and t2, the extra copies of the items I1 and I4 are removed from the pool For those transactions in which both I1 and I4 exist, the bargain value of I4 changes to 0.95.  In reality some of the customers do not take advantage of the discount for I4 Therefore, only M percent \(where, 70 ? M &lt; 100 for the example in hand to 0.95 and the rest of them are untouched Table 4: The number of common itemsets of F0 and F1 and the ratios of ? and ? ?using support values of 0.2, 0.3, and 0.4 Support Value Intersects |Gx| Dx Ex G1 20 1 1 G2 190 1 1 0.2 G3 5 1 1 G1 20 1 1 G2 20 1 1 0.3 G3 190 1 1 G1 5 1 0.71 G2 10 1 0.48 0.4 G3 0 0 0 The resulting file is F0.   The file F0 is created ten times for the business rules of Table 2.  Since the values for N and M are randomly chosen, the ten files are not the same All ten copies of the file F0 are analyzed using Apriori algorithm for the support value of 0.2 and the average of results are shown in Table 4.  This process was repeated for the support values of 0.3 and 0.4 and the results are also illustrated in Table 4 To observe the effects of the business rules, a new copy of every file F0 is created and the bargain level of all the items is changed to zero.  All ten copies of the new file \(F1 support values of 0.2, 0.3, and 0.4 and the average of results for each support value is shown in Tables 4.  As a part of the analysis of F0 and F1, the common items are determined among the corresponding itemsets.  The jitemsets created for the first F0 and F1, for example, have a set of items in common, Gj = \(j-itemsetF0 jitemsetF1 Gj | / | j-itemsetF1| determines the contents  closeness  of j-itemsetF0 and j-itemsetF1 together The results of the analyses reported in Table 4 deals with a mixture of discount values.  Another experiment 


with a mixture of discount values.  Another experiment was conducted that assumed  all the business rules initially have the same discount level of 0.05 and in ten iterations, the discount level reaches 0.95 \(incremented by 0.1 per iteration repeated to obtain results from the files with a mixture of discount levels.  The changes for support value of 0.4 and for different discount levels \(L Table 5: The number of itemsets using the business rules with the same discount levels L Intersect |Gx| Dx Ex G1 7 1 1 0.05 G2 21 1 1 G1 6.6 1 0.94 0.15 G2 18.6 1 0.89 G1 8 1 0.97 0.25 G2 28 1 0.94 G1 6.2 1 0.86 0.35 G2 16.2 1 0.76 G1 6 1 0.75 0.45 G2 15 1 0.54 G1 6 1 0.91 0.55 G2 15 1 0.83 G1 6 1 0.93 0.65 G2 15 1 0.86 G1 4.9 1 0.8 G2 9.7 1 0.62 0.75 G3 3.1 1. 1.0 G1 5 1 0.71 G2 10 1 0.48 0.85 G3 3.6 1 1 G1 3.3 1 0.45 G2 4 1 0.18 0.95 G3 0.3 0.2 0.13 Proceedings of the International Conference on Information Technology: Coding and Computing  ITCC  04 0-7695-2108-8/04 $ 20.00  2004 IEEE 5. Conclusions and Future Research For the synthetic data generated by this research, the results illustrated in Tables 4 and 5 reveal the following x   T h e  e f f e c t  o f  b u s i n e s s  r u l e s  a r e  n o t  o b s e r v e d  f o r  the support values less than 0.4 x   T h e  b a r g a i n  l e v e l  o f  l e s s  t h a n  o r  e q u a l  t o  0  1 5  does not have a significant effect on the associate rules using the threshold support value of 0.4 x   T h e  c o n t e n t s  o f  i t e m s e t s  c h a n g e  f a s t e r  t h e  number of itemsets x   T h e  n u m b e r  o f  i t e m s e t s  i s  n o t  a  f u n c t i o n  o f  t h e  discount level x   T h e  a s s o c i a t i o n  r u l e s  a n d  t h e i r  n u m b e r  a r e  different for each support value x   T h e  r e l a t i o n s h i p  b e t w e e n  t h e  c h a n g e s  i n  b a r g a i n  level and their effects on the number of itemsets is not linear.  When the bargain level goes up, the number of the itemsets goes down.   The declining rate is accelerated by increase in the support value The research team is in the process of studying the effects of the business rules on large datasets.  In addition the effects of business rules on multi-dimensional association analysis are also underway 6. References 1. R. Agrawal, T. Imielinski, and A. Swami  Mining association rules between set of items in large databases  Proceedings of 1993 ACM-SIGMOD International  Conference on Management of Data SIGMOD  93 216 2. R. Agrawal and R. Srikant  Fast algorithms for mining association rules  Proceedings of 1994 International Conference on Very Large Databases 


International Conference on Very Large Databases VLDB  94 487-499 3. H. Mannila, H. Toivonon, and A. I. Verkamo  Efficient algorithms for discovering association rules  Proceedings of AAAI  94 Workshop Knowledge Discovery in Databases \(KDD  94 WA, July 1994, pp. 181-192 4. S. Ramaswamy, S. Mahajan, and A. Silberschatz  On the discovery of interesting patterns in association rules  Proceedings of 1998 International  Conference on Very Large Databases \(VLDB  98 August 1998, pp. 368-379 5. R. Agrawal and R. Srikant  Mining sequential Patterns  Proceedings of 1995 International Conference on Data Engineering \(ICDE  95 Taiwan, March 1995, pp. 3-14 6. H. Mannila, H. Toivonon, and A. I. Verkamo  Discovery of frequent episodes in event sequences   Journal of Data Mining and Knowledge Discovery 1:259  289, 1997 7. K. Koperski and J. Han  Discovery of spatial association rules in geographic information databases  Proceedings of the 4th International Symposium on Large Special Databases \(SSD  95 Portland, ME, August 1995, pp. 47-66 8. B  zden, S. Ramaswamy, and A. Silberschatz  Cyclic association rules  Proceedings of 1998 International  Conference on Data Engineering ICDE  98 9. A. Savasere, E. Omiecinski, and S. Navathe  Mining for strong negative association in a large database of customer transactions  Proceedings of 1998 International  Conference on Data Engineering ICDE  98 10. H. Lu, J. Han, and L. Fang  Stock movement and ndimensional inter-transaction association rules   Proceedings of 1998 SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery DMKD  98  12.7 11. Han, J. and Kamber M., Data Mining: Concepts and Techniques, Morgan Kaufman Publisher, 2001, pp 225-236 Proceedings of the International Conference on Information Technology: Coding and Computing  ITCC  04 0-7695-2108-8/04 $ 20.00  2004 IEEE pre></body></html 


entries Consider the first leaf entry &lt;1,10100&gt; in the B-tree of R1 Its sketch 10100 equals the OR for sketches of r1 and r2 i.e., 10000, 10100, respectively each intermediate R-tree entry Ri, its sketch at any time t is the OR of sketches of all the regions in the subtree \(of Ri incremental maintenance algorithms follow those of the aRB-tree due to the similarity of the structures R-tree for the R 1 R2 r1 r2 r 3 r4 spatial dimensions 4B-tree for r 10000110002100001 1010043 3110001 10100 1B-tree for r 11100011002100001 1010054 4111001 11100 1B-tree for R 11100111002101001 1010154 4111001 11101 2B-tree for r 11000100003101001 1000154 4101001 11001 N 4 N 2 N 1 N 3 3B-tree for r 11111100002010001 5 5110001 11111 2B-tree for R 10100100003110001 4 5110001 11111 111115 Figure 3.3: A sketch index example 3.3 Query processing using the sketch index A straightforward algorithm for answering DC queries using the sketch index is to perform the search in a way similar to that in the aRB-tree. To illustrate this, we assume, for simplicity, the same extents of regions \(r1 r2  r4 R1, R2 in Figure 2.2a. Consider again the query q with window qr \(shown in Figure 2.2a  search algorithm initiates a result sketch RS with all bits set to 0, and gradually updates it. Specifically, the search starts from the root of the R-tree. Since R1 is contained in qr, we fetch the root N1 of its B-tree, where the first entry lt;1,11100&gt; indicates that the OR of all sketches in its subtree during [1,3] is 11100, which becomes the new value of RS. The child node N2 of the second root entry must be searched. Inside this node, entry &lt;4,11100&gt; qualifies qt and thus its sketch is OR-ed with RS \(which, however incurs no change to RS the R-tree and, since R2 partially intersects qr, accesses its child node, in which the only entry intersecting qr is r4 Hence, it visits N3 and N4 producing the final sketch RS=11100. In Figure 3.3, the visited B-tree nodes are shaded The above algorithm applies spatial and temporal conditions \(using qr and qt respectively ignores the pruning power of the sketches themselves Notice that in the previous example RS is already set to 11100 \(i.e., the final result search process \(i.e., after accessing the root of the B-tree 


search process \(i.e., after accessing the root of the B-tree of R1 not affect the final result at all. This motivates the following pruning heuristic Heuristic 3.1: Let RS be the current result sketch, and e an intermediate B-tree entry whose associated sketch is se Then, the sub-tree of e can be pruned if \(se OR RS  Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE According to this rule, the processing of the above query can avoid visiting node N4 because the sketch \(10100 its parent entry &lt;3,10100&gt; satisfies 10100 OR RS = RS The implication is that, in order to maximize the effectiveness of Heuristic 3.1, we should first try to maximize the 1's in RS, before descending intermediate B-tree  postpone  visiting nodes that may be pruned later as more bits of RS are set The next question is: which node accesses are avoidable and which ones are necessary To answer this question, let SRE be the set of R-tree entries whose B-trees must be accessed. Equivalently, each entry e in SRE satisfies the following conditions: \(i covered by query rectangle qr \(or its MBR intersects qr if e is a leaf ii i In the example of Figure 3.3, SRE={R1,r4}. Evidently accesses to the roots of their respective B-trees are unavoidable1. Hence we visit all of them \(in Figure 3.3 nodes N1 and N3 of these entries allow us to set \(possibly many without any further node access. The first entry of N1 has lifespan [1,3] \(3 is derived from the timestamp of the next entry 4 we can immediately update RS to its sketch 11100 Similarly, the lifespan [1,2] of the first entry in N3 is also contained in qt; hence its sketch 11000 is also taken into account, but does not change RS Now let us consider the remaining entries in N1 and N3 namely, &lt;4,11101&gt; and &lt;3,10100&gt;. Although, their lifespans are not contained in qt=[1,4], Heuristic 3.1 eliminates &lt;3,10100&gt;. Nevertheless, &lt;4,11101&gt; is not pruned by the heuristic because 11101 OR RS = 11101 RS. However, recall that our objective is not to retrieve the complete final RS. Instead, we are interested in the position of the left-most bit that is still 0. What is the possible left-most position \(of the final RS Given the current RS=11100 and entry &lt;4,11101&gt;, the answer is 4 \(i.e., the left-most 0 must be at the 4-th bit since the first 3 bits of both RS and the entry  s sketch are all 1. Therefore, the access to the child node of this entry can also be avoided, because \(even if we actually visit it the only possible change to RS is to set the 5-th bit to 1 which does not affect our estimation. This observation leads to another heuristic Heuristic 3.2: Let SU be the OR of the sketches of the entries whose sub-trees cannot be pruned so far. If p is the position of the left-most 0 in \(RS OR SU tree of an intermediate \(B-tree sketch se satisfies the following condition    1 1 AND 1...10...0  OR  AND 1...10...0e p p RS RS s       1 


1 Unlike the aRB-tree, we do not store sketches in the R-tree entries because this would decrease the node fanout Heuristic 3.2 subsumes 3.1 by providing a more general condition. Specifically, instead of requiring all bits of RS and \(RS OR se p?1 bits \(of the these sketches where p is decided by RS and SU together indicates a good access order for the child nodes of entries not pruned by Heuristic 3.2 Heuristic 3.3: Given a set of qualifying entries, we visit their child nodes in descending order of the number of 1  s in their sketches We use a heap to manage the entries which cannot be pruned yet, using the numbers of 1  s in their sketches as the sorting keys. As an example, consider another query whose \(i r1, r2, r3, r4 and contains the MBR of R1 but not R2, and \(ii qt=[1,4]. In this case, the algorithm first visits the roots of the B-trees of R1, r3, r4, after which RS=11100, and the heap contains two entries &lt;1,11111&gt; \(from the root of r3  s B-tree the second entry in the B-tree of R1 visit the child node of &lt;1,11111&gt; next since it has more 1  s. Figure 3.4 illustrates the pseudo-code of the improved algorithm \(referred to as sketch-prune in the sequel algorithm sketch_prune \(qr, qt 1.   initiate a  max  heap H accepting entries of the form lt;B-tree entry e, key&gt;;  set all bits of RS to 0 2. obtain the set SRE of R-tree entries whose B-trees must be searched 4. for each of entry e in SRE 5.  for each entry e' in the root of e.btree 6.   process_intermediate\(e', SRE, H 7. while \(H is not empty 8.  SU = the OR of the sketches of the entries in H 9.  p = the position of  the left-most 0 of SRE OR SU 10.  remove the top entry &lt;e, key&gt; from H; let the sketch of e be se 11.  let s be a sketch whose left-most \(p?l while the others are 0 12.  if \(RS OR se AND s RS AND s 13.   for each entry e' in e.child  \(its sketch se 14.          if \(e.child is leaf e'.lifespan intersects qt 15.     RS=se' OR RS 16.     if \(e' is an intermediate node 17.      process_intermediate\(e', Sfinal, H 18. let k be the position of the left-most 0 in RS 19. return 1.29  2k end sketch_prune Algorithm process_intermediate \(e, Sfinal, H e is an intermediate entry in the B-tree with sketch se; RS is the current result sketch; qt is the query interval; H is the heap 1. if e.lifespan is contained in qT  then RS=RS OR se 2. else if \(e.lifespan intersects qT 3.  insert &lt;e, number of  1  in se&gt; into H end process_intermediate Figure 3.4: The sketch-prune algorithm Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE Heuristic 3.3 provides a reasonably  good  access order but other more sophisticated and potentially better access orders exist. For instance, the order may be decided according to the number of additional bits in RS that may be set \(to 1 and two sketches 11100 and 00110; then according to this order, the second sketch will be processed first \(although it has fewer 1  s while the first sketch can set only one bit adjusting the sorting keys of the entries in the heap as the algorithm proceeds \(and RS changes 


algorithm proceeds \(and RS changes expensive if the heap size is large The description so far assumes that only one sketch is maintained per B-tree entry; however, the sketch-prune algorithm can be easily modified to support multiple sketches \(which, as discussed in Section 2.1, leads to higher accuracy are applied individually for each sketch to prune the entries that qualify the heuristic conditions in all sketches Then, Heuristic 3.3 determines the access order with respect to the total number of 1  s in all the sketches of an entry. The storage of the sketch index at each timestamp is linear to \(i ii log2n of each sketch, and \(iii used. As a result, the total space complexity \(for all T timestamps in the history m  R  T  logn 3.4 Supporting distinct sum queries The proposed method for DC \(distinct count can be applied to DS \(distinct sum the sketches of the leaves. The resulting sketches are then indexed and queried in exactly the same way as described in the previous section. Hence, it suffices to illustrate the specialized algorithm for creating the sum sketches Specifically, the problem is stated as follows: given a dataset with \(possibly duplicate object o, measure w distinct objects. That is, if an object appears with the same measure several times, its measure is added only once We solve this problem by reducing it to DC processing Given an input record \(o, w generation algorithm by inserting w different elements o,?1,w o,?2,w  o,?w,w to distinguish these elements. Consequently, the estimated  count  using FM is actually the sum of the w  s of distinct records \(o, w comparing both o and w problem2. The disadvantage of this approach is that, if w is large, inserting w different elements will be expensive Here we briefly describe an alternative algorithm for generating sum-sketches that remedies this problem \(more 2 An alternative approach is to insert elements of the form o,?i  count  is the sum of the maximum w  s for each distinct o details and proofs may be found in [CLKB04 idea is to leverage the observation of [FM85] that the first few \(say x almost estimator is only concerned with the first 0 in the final sketch, we only need to consider the part \(of the sketch starting at the \(x+1 ignore the insertion of those elements \(let their number be y function \(used by FM probability of setting the i-th bit equals 2?i, each element has probability ?xi=1\(2?i any of Hence, y follows the Binomial distribution3 Bin\(w, ?xi=1 2?i in order to decide how many bits after the x-th one is set and obtain the resulting sketch. Let the left-most 0 of this sketch be at position k'; then the corresponding position k in the sketch of inserting all w elements equals x+k There remains only one question: what is a good value for x? The analysis of [FM85] observes that inserting w distinct items sets the first x = log2 w?2 log2 log2 w bits of the resulting sketch to 1 with high probability. This value is adopted in our implementation. Finally, we note that this method can also be combined with PCSA to improve accuracy, as shown in Figure 3.5 algorithm  sum_PCSA \(DS, h, m, r dataset DS={\(o1,w1 o2,w2  h is a random function such that, Prob[h\(o,w the number of bits in each sketch 


the number of bits in each sketch 1. init m sketches s1, s2  sm, each with r bits, all set 0 2. for each \(o, w 3.       randomly pick a sketch si \(1?i?m 4. x = log2w  2log2log2w 5.       for j=1 to x 6.            si[j] = 1 7.       for j=1 to w?Bin\(w, ?xi=1\(2?i 8.            si[x+h\(o,j 9. k=0 10.  for i=1 to m do 11.       for  j=1 to r do 12.              if  si[j] = 0 then 13. k = k + j 14. break;  // go to the next sketch 15. return \( 1.29m  2 k/ m end  sum_PCSA Figure 3.5: Sketch generation and estimation for DS 4. Extensions In this section we present the application of the proposed techniques to related spatio-temporal problems. Section 4.1 uses sketches to reduce the size of general spatiotemporal databases and enhance the performance of 3 For Binomial distribution x~Bin\(n,p Prob[x=m] is \(nm 1?p Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE aggregate processing. Section 4.2 applies sketches to mine spatio-temporal association rules 4.1 Approximating general moving data The discussion in Section 3 assumes a set of regions that constitute the finest aggregation granularity, which may not be the case for the conventional spatio-temporal databases. In this scenario, each object o reports its location \(x,y maintains a tuples in the form &lt;o,x,y,t&gt;. Evidently, the size of the database table grows continuously, so that eventually it becomes prohibitively large \(especially if the number of monitored objects is high space complexity O\(n  T where n is the number of objects, and T is the number of timestamps in the history this deteriorates query performance. In the sequel, we show that, if the goal is to support aggregate queries, we can reduce the size and query overhead significantly, at the trade-off of some small error \(around 15% as shown in our experiments We manually impose a res  res regular grid over the data space \(i.e., each cell of the grid has length 1/res of the total axis extent resolution. Then, the sketch index is directly applicable by treating the grid cells as the finest aggregate granularity. It is worth mentioning that if the number of cells is relatively small \(i.e., low resolution approximation tends to over-estimate the actual result because an object, which does not fall in the query rectangle qr, but in a cell intersecting qr, will also be counted. This problem can be alleviated by setting res to a sufficiently large value \(e.g., 50 in our implementation is easy to verify that the space complexity is O\(\(res  T  logn T  logn further improvement, observe that we can actually remove the R-tree from the sketch index, because the cells indexed by the R-tree are regular. Specifically, it suffices to introduce a hierarchical decomposition as shown in Figure 4.1, where the grid at level i has resolution 2i, and the maximum level equals log2res Level 0 Level 1 Level L B-tree 


B-tree B-tree B-tree B-tree B-tree Figure 4.1: Grid-based approximation Note that, this hierarchy implicitly defines the parentchild relation among cells of different levels \(e.g., the shaded cell at level 0 is the ancestor of all the shaded cells in the lower levels associated with a B-tree managing the historical sketches about objects in its extent \(cells in intermediate levels resemble intermediate entries in the R-tree of a sketch index of cells \(in a particular grid i ii index, descending the hierarchy is only necessary for case i ii can be proven that, given the finest resolution res, the algorithm accesses O\(res  hB for any query hB is the maximum height of the B-tree 4.2 Mining spatio-temporal association rules Consider a user in region ri at time t. What is the probability p that this user will appear in region rj by time t+T? We denote such a spatio-temporal association rule with the syntax \(ri,T,p important in practice. For example, in mobile computing they can identify trends in user movements and lead to better allocation of antenna bandwidth to cater for potential network congestions in the near future Additional constraints, such that ri and rj must be within certain distance, may also be specified By maintaining the sketches of all regions at each timestamp as in Figure 3.2, we can answer the following question easily: given specific ri, rj, and a timestamp t how many users that are in ri at t, appear in rj at any of the following T timestamps \(i.e., t+1  t+T t sketch of ri at time t, and sj\(t t+T rj at the subsequent T timestamps. We first estimate the number n1 of objects at ri at time t \(using si\(t number n2 of objects at rj during time interval [t+1, t+T using ORt+Ti=t+1\(sj\(t+i number n3 of objects that appeared either in ri \(at time T or in rj during [t+1, t+T] \(using ORt+Ti=t+1\(sj\(t+i t Then, the number of objects that appear in ri at time t and then appear in rj during [t+1, t+T] equals n1+n2?n3. This idea naturally leads to a simple brute-force algorithm for discovering the association rules, which as shown in Figure 4.2, checks all possible instances of \(ri, rj, t algorithm associate_rule_mining \(T, p, c T is the horizon; p is the appearance probability; c is the confidence factor 1. for each region ri 2.  for each region rj 3.   sample=0; witness=0 4.   for each timestamp t in history 5.    sample 6.    s' = sj\(t+1 t+2 t+T 7.     n1=FM estimate from si\(t n3=estimate from si\(t 8.    if \(n1+n2?n3 9.   if \(witness/sample&gt;c ri,T,p end associate_rule_mining Figure 4.2: Algorithm for mining association rules Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE 5. Experiments This section experimentally evaluates the proposed methods. First, Section 5.1 examines the efficiency of the sketch-index in answering aggregate queries. Then 


sketch-index in answering aggregate queries. Then Section 5.2 studies the effect of approximating spatiotemporal data, while Section 5.3 presents preliminary results for mining association rules 5.1 Performance of sketch-indexes Due to the lack of real spatio-temporal datasets we generate synthetic data in a way similar to [SJLL00 TPS03] aiming at simulation of air traffic. We first adopt a real spatial dataset [Tiger] that contains 10k 2D points representing locations in the Long Beach county \(the data space is normalized to unit length on each dimension These points serve as the  airbases  At the initial timestamp 0, we generate 100k air planes, such that each plane \(i uniformly generated in [200,300], \(ii, iii destination that are two random different airbases, and iv  the velocity direction is determined by the orientation of the line segment connecting its source and destination airbases move continually according to their velocities. Once a plane reaches its destination, it flies towards another randomly selected also uniform in [0.02, 0.04 reports to its nearest airbase, or specifically, the database consists of tuples in the form &lt;time t, airbase b, plane p passenger # a&gt;, specifying that plane p with a passengers is closest to base b at time t A spatio-temporal count/sum query has two parameters the length qrlen of its query \(square number qtlen of timestamps covered by its interval. The actual extent of the window \(interval uniformly in the data space \(history, i.e., timestamps 0,100 air planes that report to airbases in qr during qt, while a sum query returns the sum of these planes  passengers. A workload consists of 100 queries with the same parameters qrlen and qtlen The disk page size is set to 1k in all cases \(the relatively small page size simulates situations where the database is much more voluminous specialized method for distinct spatio-temporal aggregation, we compare the sketch-index to the following relational approach that can be implemented in a DBMS. Specifically, we index the 4-tuple table lt;t,b,p,a&gt; using a B-tree on the time t column. Given a count query \(with window qr and interval qt SELECT distinct p FROM &lt;t,b,p,a&gt WHERE t?qt &amp; b contained in qr The performance of each method is measured as the average number of page accesses \(per query processing a workload. For the sketch-index, we also report the average \(relative Specifically, let acti and esti be the actual and estimated results of the i-th query in the workload; then the error equals \(1/100 set the number of bits in each sketch to 24, and vary the number of sketches The first experiment evaluates the space consumption Figure 5.1 shows the sketch index size as a function of the number of sketches used \(count- and sum-indexes have the same results more sketches are included, but is usually considerably smaller than the database size \(e.g., for 16 signatures, the size is only 40% the database size 0 20 40 60 80 


80 100 120 140 160 8 16 32 number of sketches size \(mega bytes database size Figure 5.1: Size comparison Next we demonstrate the superiority of the proposed sketch-pruning query algorithm, with respect to the na  ve one that applies only spatio-temporal predicates. Figure 5.2a illustrates the costs of both algorithms for countworkloads with qtlen=10 and various qrlen \(the index used in this case has 16 sketches also illustrate the performance of the relational method which, however, is clearly incomparable \(for qrlen?0.1, it is worse by an order of magnitude we omit this technique Sketch-pruning always outperforms na  ve \(e.g., eventually two times faster for qrlen=0.25 increases with qrlen, since queries returning larger results tend to set bits in the result sketch more quickly, thus enhancing the power of Heuristics 3.1 and 3.2. In Figure 5.2b, we compare the two methods by fixing qrlen to 0.15 and varying qtlen. Similar to the findings of [PTKZ02]4 both algorithms demonstrate  step-wise  growths in their costs, while sketch-pruning is again significantly faster The experiments with sum-workloads lead to the same observations, and therefore we evaluate sketch-indexes using sketch-pruning in the rest of the experiments 4 As explained in [PTKZ02], query processing accesses at most two paths from the root to the leaf level of each B-tree regardless the length of the query interval Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE sketch-pruning naive relational 0 100 200 300 400 500 600 700 800 900 0.05 0.1 0.15 0.2 0.25 number of disk accesses query rectangle length 300 0 100 200 400 500 600 1 5 10 15 20 number of disk accesses query interval length a qtlen=10 b qrlen=0.15 Figure 5.2: Superiority of sketch-pruning \(count As discussed in Section 2, a large number of sketches reduces the variance in the resulting estimate. To verify this, Figure 5.3a plots the count-workload error of indexes 


using 8-, 16-, and 32- sketches, as a function of qrlen qtlen=10 error \(below 10 it increases slowly with qrlen used, however, the error rate is much higher \(up to 30 and has serious fluctuation, indicating the prediction is not robust. The performance of 16-sketch is in between these two extremes, or specifically, its accuracy is reasonably high \(average error around 15 much less fluctuation than 8-sketch 32-sketch 16-sketch 8-sketch relative error 0 5 10 15 20 25 30 35 0.05 0.1 0.15 0.2 0.25 query rectangle length relative error 0 5 10 15 20 25 30 35 1 5 10 15 20 query interval length a qtlen=10, count b qrlen=0.15, count relative error query rectangle length 0 5 10 15 20 25 0.05 0.1 0.15 0.2 0.25 relative error query interval length 0 5 10 15 20 25 30 1 5 10 15 20 c qtlen=10, sum d qrlen=0.15, sum Figure 5.3: Accuracy of the approximate results The same phenomena are confirmed in Figures 5.3b where we fix qrlen to 0.15 and vary qtlen 5.3d \(results for sum-workloads number of sketches improves the estimation accuracy, it also leads to higher space requirements \(as shown in Figure 5.1 Figures 5.4a and 5.4b show the number of disk accesses for the settings of Figures 5.3a and 5.3b. All indexes have almost the same behavior, while the 32-sketch is clearly more expensive than the other two indexes. The interesting observation is that 8- and 16-sketches have 


interesting observation is that 8- and 16-sketches have almost the same overhead due to the similar heights of their B-trees. Since the diagrams for sum-workloads illustrate \(almost avoid redundancy 32-sketch 16-sketch 8-sketch number of disk accesses query rectangle length 0 50 100 150 200 250 300 350 400 0.05 0.1 0.15 0.2 0.25 number of disk accesses query interval length 0 50 100 150 200 250 300 350 1 5 10 15 20 a qtlen=10 b qrlen=0.15 Figure 5.4: Costs of indexes with various signatures Summary: The sketch index constitutes an effective method for approximate spatio-temporal \(distinct aggregate processing. Particularly, the best tradeoff between space, query time, and estimation accuracy obtained by 16 sketches, which leads to size around 40 the database, fast response time \(an order of magnitude faster than the relational method average relative error 5.2 Approximating spatio-temporal data We proceed to study the efficiency of using sketches to approximate spatio-temporal data \(proposed in Section 4.1 as in the last section, except that at each timestamp all airplanes report their locations to a central server \(instead of their respective nearest bases maintains a table in the form &lt;time t, plane p, x, y&gt;, where x,y with parameters qrlen and qtlen distinct planes satisfying the spatial and temporal conditions. For comparison, we index the table using a 3D R*-tree on the columns time, x, and y. Given a query, this tree facilitates the retrieval of all qualifying tuples, after which a post-processing step is performed to obtain the Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE number of distinct planes \(in the sequel, we refer to this method as 3DR method introduces a regular res  res grid of the data space, where the resolution res is a parameter. We adopt 16 sketches because, as mentioned earlier, this number gives the best overall performance Figure 5.5 compares the sizes of the resulting sketch indexes \(obtained with resolutions res=25, 50, 100 the database size. In all cases, we achieve high compression rate \(e.g., the rate is 25% for res=25 evaluate the query efficiency, we first set the resolution to the median value 50, and use the sketch index to answer workloads with various qrlen \(qtlen=10 


workloads with various qrlen \(qtlen=10 size \(mega bytes database size 0 20 40 60 80 100 120 140 160 25 50 100 resolution Figure 5.5: Size reduction Figure 5.6a shows the query costs \(together with the error in each case method. The sketch index is faster than 3DR by an order of magnitude \(note that the vertical axis is in logarithmic scale around 15% error observations using workloads with different qtlen Finally, we examine the effect of resolution res using a workload with qrlen=0.15 and qtlen=10. As shown in Figure 5.6c, larger res incurs higher query overhead, but improves the estimation accuracy Summary: The proposed sketch method can be used to efficiently approximate spatio-temporal data for aggregate processing. It consumes significantly smaller space, and answers a query almost in real-time with low error 3D Rsketch number of disk accesses query rectangle length 1 10 100 1k 10k 0.05 0.1 0.15 0.2 0.25 16 14% 15 15% 13 relative error number of disk accesses query interval length 1 10 100 1k 10k 1 5 10 15 20 16 15% 15% 12% 11 relative error a qtlen=10, res=25 b qrlen=0.15, res=25 0 500 1000 1500 2000 2500 25 50 100 number of disk accesses resolution 20% 15% 14 relative error c qrlen=0.15, qtlen=10 


c qrlen=0.15, qtlen=10 Figure 5.6: Query efficiency \(costs and error 5.3 Mining association rules To evaluate the proposed algorithm for mining spatiotemporal association rules, we first artificially formulate 1000 association rules in the form \(r1,T,90 with 90% confidence i randomly picked from 10k ones, \(ii in at most one rule, and \(iii Then, at each of the following 100 timestamps, we assign 100k objects to the 10k regions following these rules. We execute our algorithms \(using 16 sketches these rules, and measure \(i  correct  rules divided by the total number of discovered rules, and \(ii successfully mined Figures 5.7a and 5.7b illustrate the precision and recall as a function of T respectively. Our algorithm has good precision \(close to 90 majority of the rules discovered are correct. The recall however, is relatively low for short T, but gradually increases \(90% for T=25 evaluated in the previous sections, the estimation error decreases as the query result becomes larger \(i.e., the case for higher T 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 precision HT 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 recall HT a b Figure 5.7: Efficiency of the mining algorithm Summary: The preliminary results justify the usefulness of our mining algorithm, whose efficiency improves as T increases Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE 6. Conclusions While efficient aggregation is the objective of most spatio-temporal applications in practice, the existing solutions either incur prohibitive space consumption and query time, or are not able to return useful aggregate results due to the distinct counting problem. In this paper we propose the sketch index that integrates traditional approximate counting techniques with spatio-temporal indexes. Sketch indexes use a highly optimized query algorithm resulting in both smaller database size and faster query time. Our experiments show that while a sketch index consumes only a fraction of the space required for a conventional database, it can process 


required for a conventional database, it can process queries an order of magnitude faster with average relative error less than 15 While we chose to use FM sketches, our methodology can leverage any sketches allowing union operations Comparing the efficiency of different sketches constitutes a direction for future work, as well as further investigation of more sophisticated algorithms for mining association rules. For example, heuristics similar to those used for searching sketch indexes may be applied to improve the brute-force implementation ACKNOWLEDGEMENTS Yufei Tao and Dimitris Papadias were supported by grant HKUST 6197/02E from Hong Kong RGC. George Kollios, Jeffrey Considine and were Feifei Li supported by NSF CAREER IIS-0133825 and NSF IIS-0308213 grants References BKSS90] Beckmann, N., Kriegel, H., Schneider, R Seeger, B. The R*-tree: An Efficient and Robust Access Method for Points and Rectangles. SIGMOD, 1990 CDD+01] Chaudhuri, S., Das, G., Datar, M., Motwani R., Narasayya, V. Overcoming Limitations of Sampling for Aggregation Queries. ICDE 2001 CLKB04] Jeffrey Considine, Feifei Li, George Kollios John Byers. Approximate aggregation techniques for sensor databases. ICDE, 2004 CR94] Chen, C., Roussopoulos, N. Adaptive Selectivity Estimation Using Query Feedback. SIGMOD, 1994 FM85] Flajolet, P., Martin, G. Probabilistic Counting Algorithms for Data Base Applications JCSS, 32\(2 G84] Guttman, A. R-Trees: A Dynamic Index Structure for Spatial Searching. SIGMOD 1984 GAA03] Govindarajan, S., Agarwal, P., Arge, L. CRBTree: An Efficient Indexing Scheme for Range Aggregate Queries. ICDT, 2003 GGR03] Ganguly, S., Garofalakis, M., Rastogi, R Processing Set Expressions Over Continuous Update Streams. SIGMOD, 2003 HHW97] Hellerstein, J., Haas, P., Wang, H. Online Aggregation. SIGMOD, 1997 JL99] Jurgens, M., Lenz, H. PISA: Performance Models for Index Structures with and without Aggregated Data. SSDBM, 1999 LM01] Lazaridis, I., Mehrotra, S. Progressive Approximate Aggregate Queries with a Multi-Resolution Tree Structure. SIGMOD 2001 PGF02] Palmer, C., Gibbons, P., Faloutsos, C. ANF A Fast and Scalable Tool for Data Mining in Massive Graphs. SIGKDD, 2002 PKZT01] Papadias,  D., Kalnis, P.,  Zhang, J., Tao, Y Efficient OLAP Operations in Spatial Data Warehouses. SSTD, 2001 PTKZ02] Papadias, D., Tao, Y., Kalnis, P., Zhang, J Indexing Spatio-Temporal Data Warehouses ICDE, 2002 SJLL00] Saltenis, S., Jensen, C., Leutenegger, S Lopez, M.A. Indexing the Positions of Continuously Moving Objects. SIGMOD 2000 SRF87] Sellis, T., Roussopoulos, N., Faloutsos, C The R+-tree: A Dynamic Index for MultiDimensional Objects. VLDB, 1987 TGIK02] Thaper, N., Guha, S., Indyk, P., Koudas, N Dynamic Multidimensional Histograms 


SIGMOD, 2002 Tiger] www.census.gov/geo/www/tiger TPS03] Tao, Y., Papadias, D., Sun, J. The TPR*Tree: An Optimized Spatio-Temporal Access Method for Predictive Queries. VLDB, 2003 TPZ02] Tao, Y., Papadias, D., Zhang, J. Aggregate Processing of Planar Points. EDBT, 2002 TSP03] Tao, Y., Sun, J., Papadias, D. Analysis of Predictive Spatio-Temporal Queries. TODS 28\(4 ZMT+01] Zhang, D., Markowetz, A., Tsotras, V Gunopulos, D., Seeger, B. Efficient Computation of Temporal Aggregates with Range Predicates. PODS, 2001 ZTG02] Zhang, D., Tsotras, V., Gunopulos, D Efficient Aggregation over Objects with Extent PODS, 2002 Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE pre></body></html 


