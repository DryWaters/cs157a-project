SLPMiner An Algorithm for Finding Frequent Sequential Patterns Using Length-Decreasing Support Constraint Masakazu Seno and George Karypis Computer Science Department University of Minnesota, Minneapolis MN 55455  seno karypis cs.umn.edu Abstract Over the years a variety of algoriihms forfindingfre quent sequential patterns in very large sequential databases have been developed The key feature in most of these al goriihms is that ihey use a constani suppori constraint to control the inherenrly exponential complexity 
of ihe prob lem In general patterns thai conrain only a few iiems will tend to be inreresting ifihey have a high support whereas long patterns can still be interesting even if their support is relatively smnll Ideally we desire to have an algorithm thatfinds all the frequentpatrerns whose support decreases as afunciion of their length In this paper we present an al gorithm called SLPMiner thatfinds all sequential patterns thar satisfy a length-decreasing support constraini Our ex perimental evaluation shows 
that SLPMiner achieves up to hvo orders of magnitude of speedup by effectively exploit ing fhe lengrh-decreasing support constraint and that ifs runtime increases gradually as the average length of the se quences and the discovered frequeni patterns increases 1 Introduction Data mining research during the last years has led to the development of a variety of algorithms for finding frequent sequential patterns in very large sequential databases 7,9 51 These patterns can be used to find sequential association rules or extract prevalent patterns that exist 
in the sequences and have been effectively used in many different domains and applications The key feature in most of these algorithms is that they control the inherently exponential complexity of the prob lem by finding only the patterns that occur in a sufficiently large fraction of the sequences, called the support A limita This work WBS supported by NSF CCR-99725 19 EIA-9986C42 ACI 9982274 ACI-0133464 by Army Research Office mntra~t DAIDAAGSS 98-1-0441 by the DOE ASCI program and 
by Army High Performance Computing Research Center contract number DAAH04-95-C-0008 tion of this paradigm for generating frequent patterns is that it uses a constant support value, irrespective of the length of the discovered patterns In general patterns that con tain only a few items will tend to be interesting if they have a high support, whereas long patterns can still be interest ing even if their support is relatively small Unfortunately if constant-support-based frequent pattern discovery algo rithms are used to find some of the longer but infrequent patterns, they will end up generating 
an exponentially large number of short patterns Ideally we desire to have an al gorithm that finds all the frequent patterns whose support decreases as a function of their length Developing such an algorithm is particularly challenging because the downward closure property of the constant support constraint cannot be used to prune short infrequent patterns Recently 6 we introduced the problem of finding fre quent itemsets whose support satisfies a non-increasing function of their length An itemset is frequent only if its support is greater than or equal to the minimum support value determined 
by the length of the itemset We found a property that an itemset must have in order to support longer itemsets given a length-decreasing support constraint This property that we call the smallesi valid extension or SVE for short, enabled us to prune many short itemsets that are irrelevant to finding longer itemsets We developed an al gorithm called LPMiner that efficiently finds frequent item sets given a length-decreasing support constraint by pruning large portion of search space In this paper we extend the problem of finding pat terns that satisfy a length-decreasing support constraint 
to the much more challenging problem of finding sequen tial patterns We developed an algorithm called SLPMiner that finds all frequent sequential patterns that satisfy a length-decreasing support constraint SLPMiner follows the database-projection-based approach for frequent pattern generation, that was shown to lead to efficient algorithms and serves as a platform to evaluate our new three pNn ing methods based on the SVE property These pruning methods exploit different aspects of the sequential pattern discovery process and prune either entire sequences, items 0-7695-1754-4/02 17.00 0 2002 IEEE 418 


within certain sequences or entire projected databases Our experimental evaluation shows that SLPMiner achieves up to two orders of magnitude of speedup by effectively ex ploiting the SVE property and that its runtime increases gradually as the average length of the sequences \(and the discovered patterns\increases The rest of this paper is organized as follows Section 2 provides some background information Section 3 describes the basic pattern discovery algorithm of SLPMiner and how the length-decreasing support constraint can he exploited to prune the search space of frequent patterns The experimen tal results of our algorithm are shown in Section 4 followed by a conclusion in Section 5 2 Background 2.1 Sequence Model and Notation The basic sequence model that we will use was introduced by Srikant et al 71 and is defined as follows Let I  it iz   in be the set of all items An itemset is a sub set of items A sequence s  tl tz     tl is an ordered list of itemsets, where tj 5 I for 1 5 j  1 A sequential database D is a set of sequences The length of a sequence s is defined to he the number of items in s and denoted as SI Similarly, given an itemset t let It1 denote the number of items in t Given a sequential database D JDI denotes the number of sequences in D This model can describe a wide range of real data For example at a retail shop cus tomer transactions can he modeled by this sequence model such that an itemset represents a set of goods or items\pur chased by a customer at a visit and a sequence represents an ordered purchased itemsets history of a customer Sequence s  tl tz   ti is called a sub-sequence of sequence s  ti ti   ta I 5 rn if there exist 1 integers it iz    il such that 1 5 il  iz      ir  m and tj 2 tii j  1,2   I Ifs is a sub-sequence of s then we write s 2 s and say sequence s suppons s The support of a sequence s in a sequential database D denoted as UD\(S is defined to be lD81/lDl where D  si g s A si E D From the definition it always holds that 0  Q\(S 5 1 We use the term sequentialpattern to refer to a sequence when we want to emphasize that the sequence is supported by many sequences in a sequential database We assume that we can give a lexicographic ordering on the items in I Although an itemset is just a set of items without the notion of ordering it is essential to he able to de fine an ordering among the items for our algorithm When we represent the items in an itemset we order the items ac cording to the lexicographic ordering and put those ordered items within matched parentheses  When we represent the items in a sequence we represent each itemset in this way and mange these itemsets according to the ordering in the sequence within matched angled parentheses  2.2 Sequential Pattern Mining with Constant The problem of finding frequent sequential patterns given a constant minimum support constraint 71 is formally de fined as follows Delkitinn 1 Sequential Pattern Mining with Constant Support Given a sequential database D and a minimum support a 0  a 5 l find all sequences each of which is supported by at least raID11 sequences in D Efficient algorithms for finding frequent itemsets or se quences Z 8 1 4 3 5 101 in very large itemset or se quence databases have been one of the key success stories of data mining research The key feature in these algorithms is that they control the inherently exponential complexity of the problem by using the downward closure property This property states that in order for a pattern of length 1 to be frequent all of its suh-sequences must be frequent as well As a result once we find that a sequence of length I is infrequent we know that any longer sequences that in clude this particular sequence cannot be frequent and thus eliminate such sequences from further consideration 23 Finding Patterns with Length-Decreasing Recently we introduced the idea of length-decreasing sup port constraint I61 that helps us to find long itemsets with low support as well as short itemsets with high support A length-decreasing support constraint is given as a func tion of the itemset length f\(1 such that f\(1  f\(1a for any la la satisfying 1  lb The idea of introducing this kind of support constraint is that by using a support func tion that decreases as the length of the itemset increases we may be able to find long itemsets that may he of in terest without generating an exponentially large number of shorter itemsets We can naturally extend this idea to the sequence model by using the length of the sequence instead of the length of the itemset Figure 1 shows a typical length decreasing support constraint In this example the support constraint decreases linearly to the minimum value and then stays the same for sequential patterns of longer length For mally the problem of finding this type of patterns is stated as follows Definition 2 Sequential Pattern Mining with Length Decreasing Support Given a sequential database D and a length-decreasing support constraint f 1 where f 1 is a non-increasingfunction defrned over all the positive in tegers and always 0  f\(1  1 jind all the sequential patterns each s ofwhich satisjies UD\(S  f\(ls1 Finding the complete set of frequent sequential patterns that satisfy a length-decreasing support constraint is par ticularly challenging since we cannot rely solely on the Support Support 419 


o.owI   1 I IO Length Of sulUenCc Figure 1 An example of typical length-decreasing support constraint downward closure property of the constant support pat tern mining Notice that under a length-decreasing sup port constraint, a sequence can be frequent even if its sub sequences are infrequent since the minimum support value decreases as the length of a sequence increases We must use rninl?l f\(1 as the minimum support value lo apply the downward closure property. which will result in finding an exponentially large number of uninteresting infrequent short patterns  Length-deereasing suppnronsminl f 1 l lh II 8':SVEofa   Lengthof sequence Figure 2 Smallest valid extension SVE A key property regarding sequences whose support de creases as a function of their length is the following Given a sequential database D and a particular sequence s E D if the sequence s is currently infrequent UD\(S  f\(Is1 then uD\(s  min\({llf\(l 5 UD\(S is the mini mum length that a sequence s such that s 3 s must have before it can potentially become frequent Figure 2 illus trates this relation graphically The length of s is nothing more than the point at which a line parallel to the x-axis at y  UD\(S intersects the support curve; here, we essen tially assume that the best case in which s exists and it is supported by the same set of sequences as its suh-sequence s This property is called the smallest valid extension prop erty or SVE property for short and was initially introduced for the problem of finding itemsets that satisfy a length decreasing support constraint 61 3 SLPMiner Algorithm We developed an algorithm called SLPMiner that finds all the frequent sequential patterns that satisfy a given length decreasing support constraint SLPMiner serves as a plat form to develop and evaluate pruning methods for reduc ing the complexity of finding this type of patterns Our de sign goals for SLPMiner was to make it generic enough so that any conclusions drawn from our experiments can carry through other database-projection-based sequential pattern mining algorithms 3,5 This section consists of two main parts First we will explain how SLPMiner finds frequent sequential patterns Second we will explain how SLPMiner prunes unnecessary data by using three different pruning methods that exploit the SVE property 3.1 Sequential Database-Projection-based Algo rithm SLPMiner finds frequent sequential patterns using the database-projection-based approach First we describe the general idea of the the database-projection-based approach and then discuss about details specific to SLPMiner The description of the database-projection-based approach is based on 3 SLPMiner grows sequential patterns by adding an item at a time It uses a prefix tree that determines which items are to he added to grow each pattern Each node in the tree represents a frequent sequential pattern with one item added to the end of the sequential pattern that its parent node rep resents As a result, if a node represents a sequential pattern p its parent node represents the length Ipl  1 prefix of p For example if a node represents a pattern l 2,3 its parentnode represents l 2 SLPMiner starts from the root node that represents the null sequence to find all the frequent items in the input database and expands the root node into the child nodes cor responding to the frequent items Then it recursively moves to each child node and expands it into child nodes that rep resent frequent sequential patterns SLPMiner grows each pattern in two different ways namely itemset extension and sequence extension Item set extension grows a pattern by adding an item to the last itemset of the pattern where the added item must be larger than any item in the last itemset of the original pat tern For example l 2 is extended to 1 2,3 by itemset extension but cannot be extended to l 2,l or I 2,2 Sequence extension grows a pattern by adding an item as a new itemset next to the last itemset of the pat tern For example l 2 is extended to l 2 2 by sequence extension Figure 3 shows a sequential database D and its prefix tree that contains all the frequent sequential patterns given minimum support 0.5 Since D contains a total of four se quences a pattern is frequent if and only if at least two sequences in D support the pattern The root of the tree represents the null sequence At each node of the tree in the figure, its pattern and its supporting sequences in D are depicted together with symbol SE or E on each edge rep resenting itemset extension or sequence extension respec tively 420 


Figure 3 The prefix tree of a sequential database At each node we need to know the support of each pos sible extension to see whether it is frequent or not In prin ciple we can count the number of supporting sequences at each node by scanning the input sequential database D However, if only a small number of sequences in D support the pattern scanning the whole database costs too much for a pattern We can avoid this overhead by scanning a database called pmjected database which is generally much smaller than the original sequential database D The projected database of a sequential pattern p has only those sequences in D that support p For example at the node 2,3 in Figure 3 its projected database needs to con tain only sl s2,s4 since s3 does not support this pattern Furthermore we can eliminate preceding items in each se quence that will never be used to extend the current pattern For example at the node 2 in Figure 3 we can store sequence sl  2,3 instead of sl itself in its projected database Overall, database projection reduces the amount of sequences that need to be processed at each node and enhances efficient pattern discovery There are various database-projection-based algorithms for both finding frequent itemsets and finding frequent se quential patterns I 3 4 51 SLPMiner builds the tree in depth first order and generates a projected database at ev ery node explicitly to maximize opportunities for applying the various pruning methods As a result its overall ap proach is similar to that used by Prefixspan 51 However the main difference between them is that SLPMiner gener ates several projected databases at a time before exploring those generated child nodes, whereas Prefixspan generates and explores one projected database at a time 3.2 Performance Optimizations Expanding each node of the tree SLPMiner performs the following two steps First it calculates the support of each item that can be used for itemset extension and each item that can be used for sequence extension by scanning the projected database D once Second, SLPMiner projects D into a projected database for each frequent extension found in the previous step Since we want SLPMiner to be able to run against large input sequential databases the access to the input database and all projected databases is disk-based To facilitate this SLPMiner uses two kinds of buffers a read-buffer and a write-buffer The read-buffer is used to load a projected database from disk If the size of a projected database does not fit in the read-buffer SLPMiner reads part of the database from disk several times The write-buffer is used to temporally store several projected databases that are gen erated at a node by scanning the current projected database once using the read-buffer There are two conflicting re quirements concerning how many projected databases we should generate at a time In order to reduce the number of database scans we want to generate as many projected databases as possible in one scan On the other hand, if we keep small buffers for many projected databases simultane ously within the write-buffer, it will reduce the size of the buffer assigned to each projected database leading to ex pensive frequent YO between the write-buffer and disk In order to balance these two conflicting requirements SLP Miner calculates the size of each projected database when calculating the support of every item in the current pro jected database before it actually generates new projected databases Then SLPMiner generates projected databases as many as they fit in the write-buffer by one database scan writes those projected databases on the write-buffer to the disk, and traverses only those generated child nodes in depth first order This method also facilitates storing each pro jected database in a chunk rather than fragmented small pieces which improves and stabilizes disk U0 efficiency dramatically Even though the disk U0 of SLPMiner is quite efficient it is still a bottle-neck of the total performance In order to reduce the size of projected database, SLPMiner prunes all items from a projected database if the support is less than rniol2l f\(l since such items will never contribute to any frequent sequential patterns 3.3 Pruning Methods Given a length-decreasing support constraint SLPMiner follows the sequential database-projection-based approach explained so far using rnin,>l f\(1 as the constant minimum support constraint Then SLPMiner outputs sequential pat terns if their support satisfies the given length-decreasing support constraint But this algorithm itself does not reduce the number of discovered patterns and will be very ineffi cient as our experimental results will show. In this subsec tion we introduce three pruning methods that exploit the length-decreasing support constraint using the SVE prop erty 3.3.1 Sequence Pruning SP The first pruning method is used to eliminate certain se quences from the projected databases Recall from Section 3 that SLPMiner generates a projected database at every 421 


node Let us assume that we have a projected database D at a node N that represents a sequential pattern p Each se quence in D hasp as its prefix lf p is infrequent we know from the SVE property that in order for this pattern to grow to something indeed frequent it must have a length of at least f-'\(o~\(p Now consider a sequence s that is in the projected database at node N i.e s E D The largest se quential pattern that s can support is of length SI  Ipl Now if SI  Ipl 5 f-'\(oD\(p then s is too short to support any frequent patterns that have p as prefix Consequently s does not need to be considered any further and can he pruned We will refer to this pruning method as the sequence prun ing method or SP for short which is formally defined as follows Definition 3 Sequence Pruning Given a length decreasing support constraint f 1 and a projected database D at a node representing a sequential pat tern p a sequence Y E D can be pruned fmm D if f\(lsl  lPl  oD\(P SLPMiner checks if a sequence can he pruned before inserting it onto the write-buffer We evaluated the com plexity of this method in comparison with the complexity of inserting a sequence to a projected database There are three parameters required to prune a sequence IsI Ipl and UD As the length of each sequence is pan of sequence data structure in SLPMiner, it takes a constant time to cal culate Is1 and PI As for UD we know this value when we generated the projected database for the pattern p Eval uating function f takes a constant time because SLPMiner has a lookup table that contains all possible 1 f\(l pairs Thus the complexity of this method is just a constant time per inserting a sequence 33.2 Item Pruning IP The second pruning method eliminates some items of each sequence in projected databases Let us assume that we have a projected database D at a node N that represents sequential pattern p and consider an item i in a sequence s E D From the SVE property we know that the item i will contribute to a valid frequent sequential pattern only if where up i is the support of item i in D This is he cause of the following The longest sequential pattern that s can participate in is Is1  IpI and we know that in the suhtree rooted at N sequential patterns that extend p with item i have support at most UD Now from the SVE property, such sequential patterns must have length at least f UD i in order to he frequent As a result if equation I does not hold, item i can he pruned from the sequence s Once item i is pruned then op\(i and 1.9 decrease possibly allowing further pruning Essentially this pruning method eliminates some of the infrequent items from the short sequences We will refer to this method as the item pruning method or If for short which is formally defined as follows Definition 4 Item Pruning Given a length-decreasing support constraint f 1 and a projected database D at a node representing a sequential pattern p an item i in a sequence s E D can be pruned from s if Is1  pl  f-'\(UD'\(i We can implement this pruning method simply as fol lows for each projected database D repeat scanning D to collect support values of items and scanning D again to prune items from each sequence until no more items can he pruned Then we can project the database into a pro jected database for each frequent item in the pruned pro jected database This algorithm, however requires multiple scans of the projected database and hence will be too costly Instead we can scan a projected database once to col lect support values and use those support values for prun ing items as well as for projecting each sequence Notice that we are using approximate support values that might he higher than the real values since the support values of some items might decrease during the pruning process SLP Miner applies IP before generating a projected sequence s of s as well as after generating s just before inserting s onto the write-buffer By applying IP before projecting se quences, we can reduce the computation of projecting se quences By applying IF once again for projected sequence s we can exploit the reduction of length Is1  ls'l to prune items in SI furthermore. Pruning items from each sequence is repeated until no more item can be pruned or the sequence becomes short enough to be pruned by SP IP can potentially prune larger portion of projected database than SP since it always holds that OD\(P 2 UD i and hence f-'\(a~\(p 5 f-'\(up\(i However the pruning overhead of IP is much larger than that of SP Let us consider the complexity of pruning items from a sequence s The worst case is that only one item is pruned in ev ery iteration over the items in s Since this can he repeated as many as the number of items in the sequence the worst case complexity for one sequence is O\(n where n is the number of items in the sequence 3.3.3 Structure-based Pruning Given two sequences sl s2 of the same length k these two sequences are treated equally under SP and IP In fact the two sequences can be quite different from each other For same I-sequence l 2 3 and 4 hut never support the same k-sequences for k 2 2 From this ohser vation we considered ways to split aprojecteddatabase into smaller equivalent classes By having smaller databases in example 1,2,3,4 and 11 Z 31 4 support the 422 


stead of one large database we may be able to reduce the depth of a certain path from the mot to a leaf node of the tree As a structure-based pruning we developed the min-max pruning method The basic idea of the min-max pruning is to split a projected database D\222 into two 0  0 such that D and D contribute to two disjoint sets of frequent sequential patterns In order to separate D\222 into such D and D we consider the following two values for each sequence s E D\222 1 a\(s  the minimum number of itemsets in frequent sequential patterns that s supports 2 b\(s  the maximum number of itemsets in frequent These two values define an interval a b\(s that we call the min-max interval of sequence s If two sequences s s\221 E D\222 satisfy a b\(s n a\(.\222 b\(s\222  0 then Y and s\221 cannot support any common sequential pattern since their min-max intervals are disjoint If we have D and D satisfy U8e~;[a\(s s n U.,,;[a\(s b\(s  0 then D and D support distinct sets of frequent sequential patterns However this is not possible in general. Instead D\222 will be split into three sets A B C of sequences as shown in Figure 4 More precisely these three sets are defined for some positive integer k as follows A\(k  sls E D\221 A b\(s  k B\(k  sls E D\222 A a\(s 2 k sequential patterns that s supports k  D\222-\(AUB A\(k and B\(k support distinct sets of frequent sequen tial patterns whereas A\(k and C\(k as well as B\(k and C\(k support overlapping sets of frequent sequential pat terns From these three sets we form D  A\(k U C\(k and D  B\(k k If we mine frequent sequential pat terns of length up to k  1 from D and patterns of length no less than k from 04 we can gain the same patterns as we would from original D\222 1      1 k Min-max interval Figure 4 Min-max intervals of a set of sequences Through our experiments we observed IC1 is so close to ID\222 that mining 0 and D defined above will cost more than mining the original database D\221 However we can prune entire D\222 if both ID;I and ID are smaller than the minpl f\(1 Furthermore we can increase this minimum support by the fact that any sequential patterns that the current pattern p can extend to is of length at most mU.rD!\(ISI  Ip Now from the SVE prop erty we know that if both ID:I and ID are smaller than l\(maX.EDt\(\(sI  PI then we can eliminate entire D\222 Essentially, this means that if we can split a projected database into two subsets each of which is too small to be able to support any frequent sequential pattern, then we can eliminate the entire original projected database We call this pruning method the min-mar pruning or MP for short which is formally defined as follows Definition 5 Min-Max Pruning Given a length decreasing support constraint f\(l and a projected database D\221 at a node representing a sequential pattern p entire D\222 can be pruned there exists a positive integer k such that We apply MP just after a new projected database D\221 is generated if the entire sequences in D\221 is still kept on the write-buffer and if ID\222 5 1.2f\(max.ro,\(lSl The first condition is necessary to avoid costly disk U0 and the second condition is necessary to increase the probability of successfully eliminating the projected database The algo rithm of MP consists of two parts the first part to calculate the distribution of the number of sequences over possible min-max intervals and the second part to find k that satis fies condition 2 The first part requires scanning D\221 on memory once and finding the min-max interval for each se quence For each sequence s SLPMiner generates a his togram of itemset size and calculates a\(s by summing up itemset sizes from the largest one The other value b\(s is simply the numberof itemsets ins This part requires O\(m where m is the total number of itemsets in D\222 The second part uses an n x n upper triangular matrix Q  qij where qij  I{s~Q\(s iAb\(s jAs E D\222}Iandnisthemax imum number of itemsets in a sequence in D\222 Matrix Q is generated during the database scan of the first part Given matrix Q we have k-I n lA\(k IC\(k  CCqij kl j=i 423 


constraint that decreases linearly with the length of the fre quent sequential pattern In particular the initial value of support was set to 0.001 and it was decreased linearly down to 0.0001 for sequences of up to length lCllT1/2J We ran SPADE 191 to compare runtime values with SLP Table 1 Parameters for datasets used in our tests Miner When running SPAbE we used the depth first search option which leads to better performance than the breadth first search option on our datasets We set the mini mum support value to he minl>l f\(l Using relations 224 4.1 Results IAQ  1 IC\(k  1  IA\(k IC\(k  xqkj j Tables 2 and 3 show the experimental results that we ob L tained for the DSI and DS2 datasets respectively Each C%krow of the tables shows the results obtained for a differ IB\(k  1  IC\(k  1  1B\(k  Ic\(k  I we can calculate IA\(k IC\(k and IB\(k IC\(k incre mentally for all k in O\(nz So the total complexity of the min-max pruning for one projected database is O\(m  n\222 This complexity may be much larger than the runtime re duction by eliminating projected databases However our experimental results show that the min-max pruningmethod generally reduces the total runtime substantially when other pruning methods are not used together 4 Experimental Results We experimentally evaluated the performance of SLPMiner using a variety of datasets generated by the synthetic se quence generator that is provided by the IBM Quest group and was used in evaluating the AprioriAll algorithm 171 All of our experiments were performed on Linux workstations with AMD Athlon at 1 SGHz and 2GB of main memory All the reported runtime values are in seconds We used two classes of datasets DS1 and DS2 each of which contained 25K sequences For each class we gener ated different problem instances as follows For DSI we varied the average number of itemsets in a sequence from IO to 30 by interval 2 obtaining a total of 1 I datasets DSI IO DS1-12 224\222 DS1-30 For DS2 we varied the average number of items in an itemset from 2.5 to 7.0 by interval 0.5 obtaining a total of 10 datasets DS2-2.5, DS2-3.0   DS2-7.0 For DS1-z we set the average size of maximal potentially frequent sequences to be 212 For DS2-x we set the average size of maximal potentially frequent itemsets to he x/2 Thus the dataset contains longer frequent patterns as z increases The characteristics of these datasets are sum mariwd in Table 1 where ID1 is the number of sequences IC1 is the average number of itemsets per sequence IT1 is the average number of items per itemset N is the number of items IS is the average size of maximal potentially fre quent sequences and IT is the average size of maximal potentially frequent itemsets In all of our experiments, we used a minimum support ent DS1-x or DS2-2 dataset, specified on the first column The column labeled \223SPADE\222 shows the amount of time taken by SPADE which includes the runtime of prepro cessing to transform the input sequential database into the vertical format 191 The column labeled \223None\224 shows the amount of time taken by SLPMiner using a constant sup port constraint that corresponds to the smallest support of the support curve, that is 0.0001 for all datasets The other columns show the amount of time required by SLPMiner that uses the length-decreasing support constraint and a to tal of five different varieties of pruning methods and their combinations For example, the column label 223SP corre sponds to the pruning scheme that uses only sequence prun ing, whereas the column labeled \223SP+IP+MP\224 corresponds to the scheme that uses all the three pruning methods. Note that values with a 223-\224 correspond to experiments that were aborted because they were taking too long time A number of interesting observations can be made from the results in these tables First even though SLPMiner without any pruning method is slower than SPADE the ra tio of runtime values is stable ranging from 1.9 to 2.7 with average 2.3 This shows that the performance of SLPMiner is comparable to SPADE and good enough as a platform to evaluate our pruning methods Second either one of pruning methods performs better than SLPMiner without any pruning method In particu lar SP IP SP+lP and SP+IP+MP have almost the same speedup For DSI the speedup by SP is about 1.76 times faster for DS 1-10.7.6 1 times faster for DS1-16 and 14 1.16 times faster for DSI-22 Similar trends can be observed for DS2 in which the performance of SLPMiner with SP is 1.76 times faster for DS2-2.5,8.78 times faster for DS2-3.5 and 296.59 times faster for DS2-5.0 Third SP pruning alone can achieve almost the best per formance among all the other tested combinations This was counter-intuitive for us since we expected SP+IP would be much better than SP or IP alone On the other hand this result shows that many other sequential pattern mining al gorithms can exploit the SVE property by using SP since it 424 


Table 2 Comparison of pruning methods using DSI I i Table 3 Comparison of pruning methods using DS2 is easy to implement For example, it is straight-forward to implement SP in Prefixspan 5 for both its disk-based pro jection and pseudo-projection Even SPADE 91 which has no explicit sequence representation during pattern mining can use SP by adding the length of sequence to each record in the vertical database representation Fourth, among the three pruning methods SP leads to the largest runtime reduction IP leads to the second largest runtime reduction and MP achieves the smallest reduction The problem with MP is the overhead of splitting a database into two subsets Even so it seems surprising lo gain such a great speedup by MP alone This shows a large part of the runtime of SLPMiner with no pruning method is accounted for by many small projected databases that never contribute to any frequent patterns As for SP and IP SP is slightly better than IP because IP and SP prune almost the same amount of projected databases for those datasets hut IP has much larger overhead than SP Fifth, the runtime with three pruning methods increases gradually as the average length of the sequences and the discovered patterns\increases whereas the runtime of SLP Miner without any pruning increases exponentially 5 Conclusion In this paper we presented an efficient algorithm for find ing all frequent sequential patterns that satisfy a length decreasing support constraint The key insight that en abled us to achieve high performance was the smallest valid extension property of the length-decreasing support con straint References I R Aganval C Agganual V Prasad. and V Crestana A tree projection algorithm for generation of large itemsets for as satiation rules IBMResearch Report RC21341 November 1998 2 R Agrawal and R Srikant Fast algorithms for mining asso ciation rules In Proc of rhe ZOrh Inr'l Conference on Very Large Dorabases Santiago, Chile, September 1994 I31 V Guralnik N Garg and G Karypis Parallel tree projec tion algorithm for sequence mining In European Confer ence on Parollel Processing pages 3 10-320 2001 4 I Han I Pei and Y Yin Mining frequent patterns with out candidate generation In PNK 2000 ACM-SICMOD Inr Con Management ofDara SICMOD'W pages 1-12 Dal las TX May 2ooO 5 I Pei 1 Han B Mortaravi-Asl H Pinto Q Chen U Dayal and M HSU Prefixspan Mining sequential pat terns by prefix-projected growth In ICDE pages 215-224 2001 6 M Seno and G Karypis Lpminer An algorithm for find ing frequent itemsets using length-decreasing support con straint In Isr IEEE Conference on Dm Mining 2001 7 R Srikant and R Agrawal Mining sequential patterns In llrh Inr Con Dora Engineering 1995 I81 R Srikant and R Agrawal Mining sequential patterns Generalizations and performance improvements In Srh Inl Con Exlending Dambase Technology 1996 9 M 1 Zaki Fast mining of sequential paterns in very large databases Technical Report TR668 1997 IO M I Zaki and K Gouda Fast vertical mining using diffsets Technical Report 01-1 RPI 2001 425 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


