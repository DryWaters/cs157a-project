Ce Yu Lianmeng Li Jizhou Sun Jian Xiao Jiajun Li Zhaohui Shang 
School of Computer Science and Technology Tianjin University Tianjin China yuce lilianmeng jzsun xiaojian tju.edu.cn tjulijiajun@163.com Astrophysics Center Tianjin Normal University Tianjin China National Astronomical Observatories CAS Beijing China 
A Scalable Real-time Photometric System for Automatic Astronomical Observations on Dome A 
  
zshang@gmail.com 
Abstract 
Deployed on Dome A of Antarctica the AST3 astronomical telescopes are required to perform the sky survey in the extreme unmanned environments and process the observed data in real-time to offer the astronomers far back home with the timely observation results A scalable real-time photometric system is proposed and designed for the automatic astronomical observations of AST3 A GPU-based algorithm for image subtraction photometry is proposed to improve the efìciency of this most time-consuming prodedure in the data processing workîow To impove the reliability the system is 
organized in a HA cluster and equipped with a specially designed daemon The system has been practically utilized in the astronomical observations with the development of AST3 telescopes 
Keywords 
real-time photometric system big data automatic astronomical observation HA cluster adaptive system AST3 
I I NTRODUCTION The AST3 Antarctic Survey Telescopes consisting of an array of three 0.5m optical telescopes project is being carried out by the Chinese Center for Antarctic Astronomy on Dome A of Antarctica with science goals including research on supernovas and extrasolar  W ith a 
surface elevation of 4093 meters Dome A is the highest place in Antarctica with its lowest temperature reaching below 80 
C in the night Annually there are only around 20 days suitable for the researchers to work there As planned researchers can only reach Antarctica annually or even biennially by Xue Long icebreaker Thus AST3 is required to be capable to work without the guard of mannual operations over the long and harsh Antarctic nights When in operation each of the AST3 telescopes theoretically provides 200MB images per 2.4 minutes which reaches up to 360GB a day which cannot be transmitted back 
002 
through the slow and expensive satellite communications On dome A of Antarctica the only possible means of communication is the Iridium satellites whose total bandwidth is only 28.8 kb/s Although TCP/IP is supported itês almost unacceptable to transmit those images back even in a speed of up to 10 kb/s claimed by Iridium in their direct Internet  It w ould be a huge cost considering the e xorbitant charge However the massive data far away is required to be processed timely according to its intended science purposes The major science missions of AST3 telescopes include searching for supernovae and extrasolar planets Supernova 
a kind of stellar explosion is extremely luminous and can cause a burst of radiation that often brieîy outshines an entire galaxy before fading from view over several weeks or months During this interval the telescopes are expected to record the complete procedure of the explosion Thus observations are needed to be carried on continuously in the area where a possible supernova is discovered which raises request for the system to offer the latest transients In search of an extrasolar planet the light curves of transients are supposed to be accumulated for analyzing whether it is a planet and the characteristics of it Once again the latest 
transients should be prepared for the sake of further studies It canêt be tolerated if we fail to offer the astronomers the upto-date observation results In this case these data need to be processed especially classiìed and preliminarily photometered locally And only the nal results compressed text les and some of the important images will be transmitted back home The data processing procedures include image subtraction PSF tting photometry aperture photometry light curve analysis etc most of which are quite time-consuming considering the data scale The image processing has to be performed in real-time to catch the objects which changed 
notably in short time such the initial phase of a supernova explosion Of course all observed data will be stored in special designed disk array for further batch processing to do other type of astronomical research after the data disks are carried back with the next round of national exploration plan to Antarctic We canêt do more data processing in-situ at Dome A constrained by the power supply It means that a real-time system is required during the observations in order to undertake the new images from the telescopes and deal with each of them within the limited 
2014 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing 978-1-4799-2784-5/14 $31.00 © 2014 IEEE DOI 10.1109/CCGrid.2014.36 900 


2.4 minutes In addition as the AST3 project is still ongoing the three sets of telescopes and their corresponding processing computers will be transported to the Antarctic one by one gradually Therefore the system needs to be scalable enough to guarantee the normal operation of the existing facilities when new telescopes and computers are added On the conditions mentioned above there are three main challenges we are facing 1 Real-time data processing The observed images need to be processed within the limited time before the next image produced 2 Scalability The system should be scalable with the addition of new facilities 3 Reliability To adapt to the extreme environment reliability of both infrastructure and software system should be considered seriously Accordingly we set out to design and implement a realtime photometric system for AST3 to meet these challenges It is required to equip AST3 with a scalable system which is both efìcient enough to accomplish real-time photometric processing and reliable enough to keep itself running smoothly in the extreme and unmanned surrounding In the rest of the paper the detailed design and implementation of this system will be demonstrated Section II introduces the basic data processing workîow of the astronomical observations and the GPU based parallel algorithm we proposed to speed up the image subtraction photometry Section III presents the architecture of the system namely how the computers in this system are organized and how they cooperate with each other Section IV gives a detailed description of the mechanisms to help the system keep reliable which are implemented in the AST3 Daemon And the conclusions of the paper and our future work are presented in section V II D ATA P ROCESSING W ORKFLOW The data processing of AST3 start from the observation of images and obtain all of the observed celestial bodies and the star catalogs and light curves of the discovered transient Subsequential data processing depend on the preliminary observation results And before data processing it is required to establish some standards utilizing pre-selected observational data which include the selection of the photometric standards of the observation sky area and the astrometric standards of the sky coordinate the establishment of the observing sky templates metering and location calibration The workîow of the data processing is shown in Figure 1 After a round of observation is completed the CCD images are handled by the necessary processes the image photometry pipeline We need to detect the sources from the images and calibrate their locations with templates and Figure 1 Data processing workîow of AST3 astrometric standards Subsequently the most important processes photometries are performed The result star catalogs contain the locations and magnitudes of all of the sources Light curves of changed sources need to be analyzed in real time in order to nd special transient sources And the transient sources will be picked out with which as the center the small stamp images will be cut out separately and transmitted back together with the star catalogs The photometric processing involves three different optional methods difference image analysis namely image subtraction photometry PSF-ìtting photometry aperture photometry Among these three metering processes the image subtraction is of great importance and will be most frequently used during the actual observations of AST3 to obtain the transient catalogs which can help in the observations of supernovae and extrasolar planets searching The core component of the image subtraction photometry the astronomical image subtraction photometry algorithm AISP with man y oating point operations is v ery time-consuming An image of 10K 10K size will take approximately 6 minutes to be processed with AISP which is obviously intolerable in this project where we only get 2.4 minutes to deal with every single image In terms of the real-time processing requirement of the AST3 project a major attempt on improving the perfor 
 
901 


 in AISP where k is the length of the kernel m is the length of the image and n is the width of the image According to our statistical analysis almost 60 to 80 of the time is occupied in the convolution part when large scale images are dealt with in AISP Thus in our proposed algorithm GBAISP we focus on the optimization of the convolution part In GBAISP pre-processing part and the image subtraction are done on CPU and convolution is done on GPU The convolution in the original serial algorithm AISP divides the image into multiple sub-stamps which have the same size with the convolution kernel And before each sub-stamp is convolved its convolution kernel which is spatiallyv needs to be calculated In CUD A frame w ork a kernel function launches a grid of thread blocks Threads within a block cooperate via shared Therefore the image can be mapped into one CUDA grid Each Figure 4 Performance improvement of GPU-based image subtraction photometry sub-stamp can be mapped into a block within the grid accordingly And the convolution of each pixel in the substamp is delivered to the threads within the block There are two levels of parallelization the calculation of each substamp and the calculation of each pixel within the substamp The detailed mathematical analysis and implementation of GBAISP is in another corresponding paper under publication In the experiments the time consumed by the original algorithm AISP and the proposed algorithm GBAISP was tested respectively when the input images and the template images are in different sizes ranging from 1K 1K to 10K 10K Figure 4 presents the time used by the original algorithm AISP Intel i7 CPU 2.67GHz 6GB Memory and the proposed algorithm GBAISP Nvidia GTX660 GPU in the whole process of image subtraction photometry from pre-processing to the result image outputting respectively When dealing with larger scale images 8K 8K to 10K 10K the speedup made by the proposed algorithm GBAISP is very obvious In the case of 10K 10K the runtime of the whole substraction photometry procedure sharply declines from over 300 seconds to 125 seconds III A RCHITECTURE OF THE S YSTEM The AST3 telescopes are equipped with three data processing computers by which a high availability cluster HA cluster will be formed This HA cluster manages and schedules the three computers uniformly Each node of the cluster deals with the observation data from the speciìc telescope when they are operating normally Within each node we establish mathematical model to describe the CPU utilization memory usage disk partition  volume occupancy rate I  O device status and other key resources of the operation environment and the resource consumption indicators during the operation of each data processing module and then determine a dynamic task scheduling strategy to adjust 
2 
     
Figure 2 The owchart of AISP Figure 3 Image subtraction photometry mance of the existing subtraction photometric algorithm is tak W e designed and implemented an ef cient GPUbased improvement algorithm GBAISP GPU based Astronomical Image Subtraction Photometry to accelerate AISP The implementation is based on the HOTPANTS a popular astronomical image analysis package with CUDA GPU Figure 2 presents the the owchart of AISP The AISP processes can be divided into three parts pre-processing convolution and image subtraction In pre-processing the input image I and the template image T are stored into memory from disk And then the template image is convolved with a computed kernel K In the end the input image is subtracted by the convolved template image from which a result image can be generated As in Figure 3 the input image is subtracted by the convolved template image so that the differences will be found The complexity of convolving the template image is O\(mnk 
902 


AST3 Daemon is booted in int 3 and int 5 on the Linux operating systems by default prepared for both text and graphic interface users and is started automatically once the operating system booted This design is believed to spend less operating system resources and can make sure that it can run all by itself once the computers started up Some planned triggers are included in AST3 Daemon When the telescope offers new images to the processing 
A Start as a Daemon B Task Triggering 
Figure 5 HA Cluster conìguration of the system the real-time data processing according the state of system resources A daemon independent from data processing is designed to adjust the data processing according to conìguration les and support remote control of the system and updates of some functional modules The conìguration les and sources codes can be transmitted to the computers via Iridium satellite And on the other hand this daemon also works as a reliable service to prevent the node that it is deployed on from breaking down and keep it running smoothly which will be described in detail in the next section To facilitate the description this daemon is named as the AST3 Daemon in the followings The AST3 Daemon measures and controls the running status of the node Once serious faults occur in one of the nodes which stop to work accordingly the data processing work on it will be transferred temporarily or permanently to another node The AST3 Daemon supports two kinds of HA mechanisms cyclic backup mechanism and redundant backup mechanism As shown in Figure 5 given the 3 computers A B and C corresponding to the 3 telescopes in cyclic backup mechanism B is the backup of A C is the backup of B and A is the backup of C and in redundant backup mechanism an added node D is included which wonêt get involved in any data processing work in normal occasions and will become the backup node when one of the nodes A B C down Within each computer node AST3 Daemon is deployed and manages the running of the node itself Figure 6 The workîow chart of AST3 Daemon IV T HE AST3 D AEMON The AST3 Daemon is designed to be equipped with the features as belows 
903 


computer AST3 Daemon will get the image les and startup new subprocesses to perform a series of photometric processing tasks to deal with them The triggering plan here is that if there are images in the task pool and there are computing resources available the system will create a new subprocess with the fork function which calls the photometric functions to photometer the next image in queue The task pool works as a buffer especially when the process of the last images take too much time or even some of processes crashed down Process level parallelism adopted here makes it possible for the computer to deal with several tasks simultaneously which contributes to the performance of the whole system After its own initialization AST3 Daemon can get informed of whether there are images in the receiving directory named as task pool in the system by the inotify mechanism And then it will create processes with exec functions to handle them and record logs There is another reason that the inotify mechanism is used here is that we need to make sure the image from the telescopes has already been transported completely to the directory in case of the possibility that the uncompleted images cause single point fault during the photometric processing AST3 Daemon monitors the tasks that have already started in real-time Once a task is detected to be stopped by an exception such as faults in the photometric functions AST3 Daemon will obtain the information kill the process to avoid the system from crashing caused by single point failure If the execution time of a certain task is not tolerable it will be considered to be an unknown failure and forced to stop and restart All of the normally ended processes send end messages to AST3 Daemon to inform it and the system marks them as normal ones and logs their information This mechanism is very essential for AST3 Daemon to keep the whole real-time system running normally and play its adaptive role Once the photometric processes are triggered AST3 Daemon will record their information such as PIDs which can be used in the detection of a processês operation status and examine them periodically If the status turns out to be abnormal for example the process quited unexpectedly or turned to a zombie process the information of it will be logged and AST3 Daemon will kill the task and try to restart it When the number of consecutive failures of a certain task exceeds a certain threshold the task will be discarded and the information will be logged If the execution time of a certain task is not tolerable it will be considered to be an unknown failure and forced to stop and restart All of the normally ended processes send end messages to AST3 Daemon before quiting to inform it and the system marks them as normal ones and logs their information Table I R OULOGS AND R ELOGS Roulogs Relogs Classiìcation DEBUG INFO ERROR FATAL itself Content All kinds of useful information Execution status Usage Potential data analysis afterwards Recovery Stored in Text les Binary Compress Compressed when les get long Never AST3 Daemon records the information that users might be concerned about and may be useful in the subsequent data analysis and those are related to the system operations and can be utilized in the real-time monitoring especially in the failure detections and crash recoveries Logging is an important approach that can keep a realtime system working normally and help it to resume from disasters The logs of AST3 Daemon are divided into two parts and are named as the routine logs and the recovery logs The routine logs as the Roulogs record the useful information about the execution of the system including DEBUG for debug information INFO for routine operation information ERROR for error information and FATAL for fatal and unrecoverable information The Roulogs are designed to play their rules in potential data analysis afterwards The recovery logs as the Relogs store the execution status in order to help nd the recent crash The Relogs which are light and brief and only contains those exactly useful information are designed to be used in the recovery processes The Roulogs are kept in text les And after the length of a logging text le gets over a certain threshold the text le will be compressed with gzip and a new text le will be created to succeed While the Relogs are kept in binary without any compression due to its different usage and scale from Roulogs A speciìc protection mechanism is proposed for AST3 Daemon in which another parallel process is run alongside with AST3 Daemon and if AST3 Daemon crashes this process will restart AST3 Daemon immediately so that AST3 Daemon can recover the current state of the execution ow and continue the computing tasks before crash The cross-protection mechanism is introduced here to help AST3 Daemon itself out of dreams after falling asleep accidentally After AST3 Daemon is started a new Protect Server process will be created These two processes periodically shake hands with each other to make sure that they are both alive And once one side of them fails to feedback for more than a certain number of times the other side will take it as fault and try to restart the failed one The handshake principle is introduced here to protect AST3 
C Real-time Monitoring D Logging E Cross-protection and Crash Recovery 
904 


Daemon from crashes and make it possible for it to recover automatically from failure even without the involvement of humans This cross-protection mechanism can be enabled through the dynamic conìguration le described in the last section if the user wants to make this system even more lightweight Besides the original cront process of Linux help to protect AST3 Daemon which can periodically check processes and help to restart them once crashes happen The core parameters of AST3 Daemon can be modiìed according the actual need through the conìguration le which allows users to manage the the adaptive system exibly as much as possible The overall work ow of AST3 Daemon is shown in Figure 6 AST3 Daemon is designed to be adaptive to handle certain events such as new arrival of images exceptions in the photometric processes and system crashes Meanwhile a series of mechanisms are proposed to help AST3 Daemon achieve these adaptations and help the processing computer accomplish the real-time data processing tasks AST3 Daemon monitors the newly arrived images and put them into the task pool after which new sub-processes are created to deal with those images And it also periodically examines whether there is any exceptions in the running tasks or not to keep the system working smoothly No matter a task ends normally or not the corresponding information will be logged for further use V C ONCLUSION AND F UTURE W ORK The real-time photometric system in this paper is to help the AST3 telescopes process the observation data timely and locally so that the astronomers can catch the variable objects such as new supernovae According to the astronomical science goals of AST3 a real-time photometric workîow which can handle the instantly captured large-scale images properly and efìciently is designed and implemented in the system GPU-based parallelization is used to improve the performance of image subtraction photometric processing To ensure the computers can work stably in the extreme conditions at Dome A an HA cluster is introduced into the system utilizing the corresponding computers with the telescopes and within each node AST3 Daemon manages the processing tasks and works as an adaptive service to keep the system reliable SoC-based solution for image subtraction photometry is also under consideration for better performance and power efìceinc The rst set of telescope and data processing system AST3-1 has been carried to and deployed on Dome A in the year of 2012 And the second set of telescope and data processing system AST3-2 has been tested in Mohe the most northern town of China with long night and low temperature in the winter AST3-2 is on the schedule of deployment with the next exploration plan to Dome A This scalable real-time photometric system is also expected to serve the planning astronomical telescope project of China A CKNOWLEDGMENT This work was supported by The National Basic Research Program of China 973 Program 2013CB834902 and The National Natural Science Foundation of China under Grant No.61303021 and No.U1231108 R EFERENCES  Li Zhengyang Y uan Xiangyan Cui Xiangqun W ang Daxing Gong Xuefei Du Fujia et al Status of the rst Antarctic Survey Telescopes for Dome A Proceedings of SPIE The International Society for Optical Engineering v 8444 2012  AST3 http://aag.bao.ac.cn/ast3  Iridium Bandwidth and Internet Do wnload Speeds http://www.mailasail.com/Support/Iridium-Bandwidth  Shang Zhaohui Hu K eliang Hu Y i Li Jiliang Li Jin Liu Qiang et al Operation control and data system for Antarctic Survey Telescope AST3 Proceedings of SPIE The International Society for Optical Engineering vol 8448 2012  C Alard R H Lupton A method for optimal image subtraction The Astrophysical Journal vol 503\(1 pp 325-331 1998  Austin B T omane y and Arlin P  S Crotts Expanding the realm of microlensing surveys with difference image photometry The Astronomical Journal vol 112\(6 pp 2872-2895 1996  Mujin Y ang Ce Y u Jizhou Sun et al GAISP GPU accelerated astronomical image subtraction photometry algorithm Application Research of Computers Vol.28 No.10 pp.39403943 2011  HO TP ANTS http://www.astro.washington.edu/users/becker/HOTPANTS.html  C Alard Image subtraction using a space-v arying k ernel  Astronomy and Astrophysical Supplement Series vol 144\(2 pp 363-370 2000  NVIDIA Corporation NVIDIA CUD A C programming guide Version 4.2 2012  Qiang Zhang Jizeng W ei SoC Implementation of Astronomical Image Difference Algorithm Computer Engineering Vol.38 No 5 pp 240-242 2012  KDUST  http://www kdust.or g 
F Dynamically Conìgurable 
905 


2014 International Conference on Computing for Sustainable Global Development INDIACom Type Detection Rate As per the third iteration, the detection rate of the one-stop is much better than SNORT. The results of the third iteration are shown as under in table VIII  TABLE VIII: DETECTION RATE Iteration 3 Also as per the results, there is decrease in the rate of false alarm generation. The results of false alarm generation for second iteration are as shown in table below  TABLE VII: FALSE ALARM  C       SNORT SNORT SNORT 98.2 98.4 98.6 98.8 99 99.2 99.4 Snort Onestop Journal of Security Engineering\ 5, no. 4 \(2008\: 8 http://www.sersc.org/journals/JSE/vol5_no4_2008/3.p   Successful Real-Time Security Monitoring, Riptech  Inc. white paper September 2001 4 iteration 3  A CKNOWLEDGMENT  I would like to thanks Prof.S.M.K.Quadri for his valuable support R EFERENCES   1 Lazarevic, A., Ertoz, L., Kumar, V., Ozgur, A., & Srivastava, J. \(2003 May\. A comparative study of anomaly detection schemes in network intrusion detection. In Proceedings of the third SIAM international conference on data mining  \(Vol. 3,  pp. 25-36\. Society for Industrial Applied 3 onestop Fig. 8 Detection Rate  Also there is great decrease in the false alarm rate, which is important as far as intrusion detection systems are concerned The figures of false alarm generation at third iteration are as given in table below  TABLE IX: FALSE ALARM After third iteration the results came same for other iterations too. i.e detection rate = 99.2% and false alarm rate =0.99 VIII 1.33 1.22 98.6 99.2 1.33 0.99                                    Fig. 7:False Alarm detection rate snort snort 804 onestop Fig. 9:  False Alarm  2 SAKURAI, Kouichi, and Tai-hoon Kim. "A Trend in IDS researches  One-stop One-stop One-stop Anderson, James P. Computer security threat monitoring and surveillance. Vol. 17. Technical report, James P. Anderson Company Fort Washington, Pennsylvania, 1980 Type Percentage of False Alarm Type Percentage of False Alarm C ONCLUSION  The current research if focused on mitigating false alarm and design of a new hybrid model which will adopt both the techniques. In this paper, we have proposed a new hybrid model which combines both the modules i.e anomaly and signature based also we have added few more modules which will check for false alarm and will add new rules to the signature database automatically thus we can say, it is a model which will detects novel intrusion made on the system and is reducing the number of false alarm. In future, we will try to implement the model for intrusion preventions also. We will also try to build the model using machine learning also 


Zanero, Stefano. \215Analyzing TCP Traffic Patterns Using Self Organizing Maps\216. Image analysis and Processing-ICIAP 2005 Springer Berlin Heidelberg, 2005. 83-90  2014 International Conference on Computing for Sustainable Global Development INDIACom L. Kuang, "DNIDS: A Dependable Network Intrusion Detection System Using the CSI-KNN Algorithm", Master thesis, Queen\220s University Canada, September 2007   5 Haines JW, Lippman R, Fried DJ, Zissman MA, Tran E, Boswell SB 1999 DARPA intrusion detection evaluation: design and procedures MIT Lincoln Laboratory Technical Report, TR-1062, Massachusetts USA; 2001  Bloedorn, E., Christiansen, A. D., Hill, W., Skorupka, C., Talbot, L. M Tivel, J. \(2001\. Data mining for network intrusion detection: How to get started. MITRE Technical Report 7 Rakes, T. R., Deane, J. K., & Paul Rees, L. \(2012\. IT security planning under uncertainty for high-impact events. Omega, 40\(1\, 79-88 9 Tavallaee, Mahbod, et al. "A detailed analysis of the KDD CUP 99 data set." Proceedings of the Second IEEE Symposium on Computational Intelligence for Security and Defence Applications 2009. 2009  Gudadhe, M., Prasad, P., & Wankhade, K. \(2010, September\. A new data mining based network intrusion detection model. In Computer and Communication Technology \(ICCCT\, 2010 International Conference on \(pp. 731-735\. IEEE  Mahoney MV, Chan PK. An Analysis of the 1999 DARPA/Lincoln laboratory evaluation data for network anomaly detection. Recent advances in intrusion detection. In Sixth international symposium, Raid 2003, Pittsburgh, PA, USA; 8\20510 September 2003. p. 220\20539  Bilal Maqbool Beigh, Uzair Bashir and Manzoor Chahcoo. Article Intrusion Detection and Prevention System: Issues and Challenges International Journal of Computer Applications 76\(17\:26-30, August 2013. Published by Foundation of Computer Science, New York, USA  Roesch, M. \(1999, November\ Snort: Lightweight Intrusion Detection for Networks. In LISA \(Vol. 99, pp. 229-238 6 805 Carl, Glenn, et al. "Denial-of-service attack-detection techniques Internet Computing, IEEE 10.1 \(2006\2-89                       Chandola, V., Banerjee, A., & Kumar, V. \(2009\. Anomaly detection: A survey. ACM Computing Surveys \(CSUR\ 41\(3\ 15  O'Mahony, Michael P., Neil J. Hurley, and Gu\351nol\351 CM Silvestre Recommender systems: Attack types and strategies." AAAI. 2005  Wu, H., Schwab, S., & Peckham, R. L. \(2008\ U.S. Patent No 7,424,744. Washington, DC: U.S. Patent and Trademark Office 8 Ghorbani, Ali A., Wei Lu, and Mahbod Tavallaee. "Evaluation Criteria Network Intrusion Detection and Prevention. Springer US, 2010. 161183  Denning, D. E. \(1987\. An intrusion-detection model. Software Engineering, IEEE Transactions on, \(2\, 222-232  Beigh, B. M., & Peer, M. A. \(2011\. Intrusion Detection and Prevention System: Classification and Quick  Patcha, Animesh, and Jung-Min Park. "An overview of anomaly detection techniques: Existing solutions and latest technological trends Computer Networks 51.12 \(2007\: 3448-3470  Lippman R, Haines JW, Fried DJ, Korba J, Das K. Analysis and results of the 1999 DARPA off-line intrusion detection evaluation. In Proceedings of the third international workshop on recent advances in intrusion detection, Toulouse, France; 2\2054 October 2000. p. 162\20582  Kendall K. A database of computer attacks for the evaluation of intrusion detection systems. Master Thesis, Massachusetts Institute of Technology, Lexington, MA; 1999  Data set, DARPA intrusion detection evaluation data set;1999.<http://www.ll.mit.edu/IST/ideval/data/1999 1999_data_index.html  


1 2 3 3 3 3 1 3 1 1 1 1 1 1 2 1 1 1 1 1 2 G G G G G G G c g f k G G G h h G G i G G G G G G V V v u V v V v v G v v v G V v G u v u v V v v V V E E V V E V v V E v V u V u v V V u T T s V T s  T T V G G i E E E E u E i i i i i i i i i i i i i i i i i i i i i i i i i i i i i d d i d d i d i i i i i i i i i i i i 212 i i 212 i i f G   and and and and and and and can be handled similarly In such a way Type-2 nodes can be effectively reduced in is generated from two can be computed In c G c G c G in in in itself is an is still a vertex cover of in line 4 of Algorithm 3 by joining in into into in in the in the next iteration of  In the following we introduce techniques to re duce the nodes and edges when constructing G  Thus the key point to reduce the I/O cost of the  when adding nodes into from  before adding can be removed from is removed can be removed from and Get SCC and there is no need to add v G v G v G v G v G v G v G v G v G v G v G h G h G h G h G E in each graph in each graph Example 6.1  when generating can be removed because  we check whether Node Reduction has been covered by node b e l j VII I/O C OST M INIMIZATION In this section we show how to optimize our contractionexpansion based is a Type-1 node from constructed in the from which is computed using can only hold                                                                                                                                                                             Type-2 nodes are order sensitive i.e if node in line 8-9 of Algorithm 3 for each edge smallest nodes using operator line 2 of Algorithm 3 A sequential scan of Fig 5 shows the process of the graph expansion phase to expand the graphs in order of  when constructing  if there does not exit another node may not be a Type-2 node Note that our aim is to reduce nodes in To remove Type-1 node  we only keep the nodes with both using a dictionary  In addition it is straightforward that each edge denoted by light gray nodes and has already been computed using Algorithm 3 the following two types of nodes can be removed from are Type-2 nodes after scanned suppose G are the removed nodes when constructing denoted by dark gray nodes In with a single node Finally there are two in Algorithm 3 Such an operation does not generate any extra I/O cost in Algorithm 3 In order to reduce Type-2 nodes when scanning all edges in which many not reside entirely in the main memory Suppose algorithm on approach by further reducing the I/O cost The I/O cost can be reduced in two ways 1 to reduce the number of graphs constructed in the graph contraction phase and 2 to reduce the number of nodes and edges for each graph s in later iterations A Type-2 node th graph contraction phase without increasing the I/O complexity Although sSCC g g f i i c k b l k i j G c d e b a l g f h i k j m SCC f g e c f c e f g b i l j k d or 0 Any node with              6 4                       1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 212    Thus     in later iterations as  Since SCC SCC SCC SCC SCC SCC SCC nodes respectively has been added into  Such edges can be reduced in a lazy way when generating Type-1 For a node Type-2 For a node is a valid node set of with  Similarly the with with with will be removed in with G G needs to be added after line 2 of Algorithm 3 to eliminate parallel edges in  0 We develop two methods to reduce the edge size in order to reduce the intermediate results We will discuss the ef\336ciency in our performance studies Firstly for parallel edges with the same form out out out out out out out out 0 deg deg deg deg deg deg deg deg nbr nbr nbr nbr nbr nbr nbr nbr u G u v u v u v u v u w v>u in in in in in in in in in in in i i i i i i i i i i i i i  all nodes nodes in memory since a node with higher degree are less possible to be removed from 4 3 2 1  However such a solution needs to check whether Semi u>v generated in the graph contraction phase in Fig 4 The dashed circles in each graph algorithm is to reduce the number of nodes and number of edges cannot be combined with other nodes to form new  According to the node selection condition in Lemma 5.1 without generating any extra I/O cost in Algorithm 3 can be removed because when  Given graph such that Lemma 7.1 can reside entirely in the main memory By doing so we can reduce the number of Type-2 nodes in Edge Reduction can be removed from 0 SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC s are computed namely 002 212 002 002   024    002 002 002 004 004   002       wehave to make sure that th iteration According to the stop condition of graph contraction reducing the number of graphs is equivalent to reducing the number of nodes Omitted due to lack of space is an  and there is no need to include again The situation for to cover the edge Given  0  Fig 5 Graph Expansion Example computed according to Lemma 6.4 This can be done in line 8 of Algorithm 4 by checking whether 216 if  By applying the  for node s of node  for node  Thus node with a single node in without introducing new I/O cost thus we only reduce those Type-1 and Type-2 nodes that are easy to be identi\336ed The following lemma can be used to reduce Type-1 nodes or or  we only maintain the top  only one of them needs to be kept in 212 1 2  suppose  200  200   A Type-1 node  4 3 2 1 4 3 3 4 3 2 1 1 1 1 G E  G G V V V V E V V v V V v v V V   V V V  V E  E w            0  0     and nodes  If so edge  we introduce two ways to reduce the number of edges when generating  it is possible that G   017      Proof Sketch Ext Ext 002 002 002 002 g G G  


Size of Largeincrease The reasons are twofold Firstly when increases according to the node selection scheme to construct decrease There are two reasons Firstly when the memory size increases the stop condition for graph contraction is easier to be satis\336ed since more nodes can 336t in memory Secondly when the memory size increases the costs of the external sorts in both graph contraction and graph expansion phases decrease  Average Degree Number of Largeincreases the time and I/O consumptions for both increases the number of iterations in graph contraction increases This is because when number of edges increases the cost to sort and scan edges in each iteration increases thus more time and I/Os are consumed in each iteration                                     2 4 1 u E  V v E    u v  G  V E M V M V K G V V M KB Range 25M,50M,100M,150M,200M 2,3,4,5,6 200M,300M,400M,500M,600M 400K 8K 20,30,40,50,60 30,40,50,60,70 10K to in the operator in line 4 both in line 4 and augmented in all nodes in in line 5-7 VIII P ERFORMANCE S TUDIES In this section we conduct experimental studies by comparing four external algorithms for is the number of bytes to keep a node in memory We set the max time cost to be 24 hours If a test does not stop in the time limit we will denote it using until all nodes form an to Size of Small The results are shown in Fig 7\(a and Fig 7\(b for time and I/O costs respectively When the memory size increases the time and I/O costs for both 100M 400M 40 1 1 50 TABLE I R ANGE AND D EFAULT V ALUE FOR P ARAMETERS Parameter in all cases since more nodes/edges are removed in each iteration in Operator  operator speci\336es a unique total order among all nodes in the graph operator in line 9 when generating algorithm needs to hold and and and and and used in introduced in 26  w h i ch is cu r r e n tly th e m o s t I O ef 336cien t sem i e x t er n a l algorithm for and  Secondly when and out out out out out out out Fig.6\(a and Fig 6\(b show the time and I/O costs when varying the number of edges of WEBSPAM-UK2007 from 20 to 100 respectively v G v G v G v G v G v G v G v G v G v G v G v G v G v G 002 212 327 327  002 002 327 327 327 327 327       2 cannot stop in the time limit even if the graph contains only 20 of the edges When Size of 8 our e x t e rnal cont ract i on-e xpans i o n b as ed algorithm Algorithm 2 and our algorithm by applying the optimization techniques introduced in Section VII in new edges are added into In Algorithm 3 in order to make use of the new s The graphs contain nodes from 25M to 200M with average degree varying from 2 to 6 A synthetic graph is generated as follows We construct a graph computation namely the external contraction based 13 t he e x t e rnal DFS based by randomly selecting all nodes in SCC SCC SCC SCC SCC SCC  we apply the algorithm computation The  Finally additional random nodes and edges are added to the graph The parameters for synthetic datasets and their default values are shown in Table VIII outperforms 1PB 1PB since it cannot stop in all cases Memory Size need to be computed in Ext Ext Ext Ext EM Ext EM Ext Ext Ext Ext Ext Ext Ext Ext    3  002  For the semi-external algorithm  In our experiments we use a real large web graph and several synthetic datasets The real web graph is WEBSPAM-UK2007 4  which consists of 105,896,555 web4 barcelona.research.yahoo.net/webspam/datasets/uk2007/links   4H   8H   12H   16H   20H   24H   INF   20   40   60   80   100   Time\(hour Ext-SCC-Op   Ext-SCC                           DFS-SCC                 Largein in in in in in in 2 plus one disk block in the main memory that is Size of MassiveNumber of Massive\(a Time Vary Memory   1M   2M   3M   4M   5M   6M   7M   8M   INF   400M   600M   800M   1G   Number of I/Os Ext-SCC-Op   Ext-SCC                       DFS-SCC               iff one of the following three conditions holds 1 b I/Os Vary Memory Fig 7 WEBSPAM-UK2007 Varying Memory Size pages in 114,529 hosts in the UK domain The graph contains 105,895,908 nodes and 3,738,733,568 edges with the average degree 35 per node For synthetic data we generate 3 different kinds of datasets denoted Massive.The 4K,6K,8K,10K,12K 6K,8K,10K,12K,14K u w u G u G u G u G u G u G u G  For any  containing different sizes of and Small 327     V i i i i i d d d i i De\036nition 7.1 DFS Op Op Op Op Op Op 217 before adding  The default memory size is  In our experiments we do not show the results of  thus more iterations are needed according to the stop condition of graph contraction in 327 Semi add Datasets Exp-1 Performance on WEBSPAM-UK2007 in Algorithm 3 more nodes will be selected in id id 200K,300K,400K,500K,600K as follows DFS SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC  By considering  All the algorithms are implemented using Visual C 2005 and tested on a PC with Intel Core2 Quar 2.66GHz CPU and 3.5GB memory running Windows XP The disk block size is  according to Theorem 5.3 nodes with small degrees are removed when constructing 256 400   256 400 Default INF 327 deg deg deg deg deg deg deg deg deg deg deg deg deg deg deg deg deg deg deg deg deg  4 w V V E V G V G   u  v V G G E E E E G where can be further reduced We rede\336ne the operator Number of SmallD M s s s i i i i i i i 1 1 1 1 1 1 u>v 4  Secondly using operator  and for each removed node  s 336rst Then we add edges among the nodes in an  We vary the memory size from                     a Time Vary Graph Size   1M   2M   3M   4M   5M   6M   7M   8M   INF   20   40   60   80   100   Number of I/Os Ext-SCC-Op   Ext-SCC                           DFS-SCC                 b I/Os Vary Graph Size Fig 6 WEBSPAM-UK2007 Varying Graph Size Percent   4H   8H   12H   16H   20H   24H   INF   400M   600M   800M   1G   Time\(hour Ext-SCC-Op   Ext-SCC                       DFS-SCC               


SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC increases the time and I/O costs for both dataset dataset The results for both Largedatasets are similar to those in the Massive When either the average   800 105 895 908 8  256  847 200 600 25 200 50 12 30 70 f I/Os SmallTo test the synthetic data we vary the memory size M   K  M M M M M M M M M M D D D D K s s s of nodes from 2 to 6 The time and I/O costs on Largedo not have signi\336cant impact on the ef\336ciency of our algorithms as long as by 20 on average for both time and I/O consumptions Fig 8\(c and Fig 8\(d show the results on Large 1 1 4  the costs for both d I/Os Vary Degree   c Time Vary Degree   25 327         Size and G M G M V V V V V V V K E G Wevary the node size decrease sharply The reason is that in order to process the graph using in all test cases to to is smaller the decrease rate is larger This is because when is smaller more iterations are needed for both to  and the time and I/O costs are shown in Fig 9\(a and Fig 9\(b respectively When to to respectively Fig 9\(g and Fig 9\(h show the time and I/O costs when varying the number of 4   s and and and and and and and and Wevary the average degree Size   Size   Time\(hour Number of I/Os Time\(hour Number of I/Os Time\(hour Number of I/Os Time\(hour Number of I/Os Time\(hour Number of I/Os Time\(hour Number of I/Os Time\(hour Number of I/Os 2H   3H   4H   5H   INF   200M   300M   400M   500M   600M   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 200K   400K   600K   800K   1M   1.2M   INF   200M   300M   400M   500M   600M   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 1H   2H   3H   4H   5H   INF   200M   300M   400M   500M   600M   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 200K   400K   600K   800K   1M   1.2M   INF   200M   300M   400M   500M   600M   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 1H   2H   3H   4H   5H   INF   200M   300M   400M   500M   600M   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 200K   400K   600K   800K   1M   1.2M   INF   200M   300M   400M   500M   600M   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 8H   12H   16H   20H   INF   25M   50M   100M   150M   200M   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 1M   2M   3M   4M   5M   INF   25M   50M   100M   150M   200M   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 2H   4H   6H   8H   10H   INF   2   3   4   5   6   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 400K   600K   800K   1M   1.2M   INF   2   3   4   5   6   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 2H   3H   4H   5H   INF   4K   6K   8K   10K   12K   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 400K   600K   800K   1M   INF   4K   6K   8K   10K   12K   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 2H   3H   4H   5H   INF   30   40   50   60   70   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 400K   600K   800K   1M   INF   30   40   50   60   70   Ext-SCC-Op   Ext-SCC                           DFS-SCC                 e Time Small,andwhen increase This is because when n in Synthetic Data in Synthetic Data DFS DFS DFS Exp-5 Vary Op Op Op Op Op Op Op Op Op Op Op Op Ext  The time and I/O costs on Massiveare shown in Fig 9\(c and Fig 9\(d respectively When decrease When f I/Os Vary Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext Ext  V  V Number in Synthetic Data c Time Large\(a Time Vary e Time Vary cannot stop in limited time in all cases Similar to the results on the real dataset in Fig 7 when dataset and Fig 8\(e and Fig 8\(f show the results on Smallconsume less than 1 hour increases the time and I/O consumptions for both consumes more than 20 hours while both are the number of nodes and the number of edges of the graph As a result the size of memory is needed thus when the memory size is dataset are shown in Fig 8\(a and Fig 8\(b respectively dataset and this is true for all the remaining test cases when varying other parameters in synthetic data In the following due to the lack of space we only show the test results on the Large and in the graph contraction phase the contraction rate decreases when the number of iterations increases since the graph becomes denser with larger number of iterations is larger and the cost on each iteration to scan and   Exp-4 Vary Average Degree in Synthetic Data from from increases the time and I/O consumptions for both and the number of outperforms outperforms cannot stop within the time limit when outperforms in all cases When the memory increases from increases the time and I/O costs for both When a Time Massive\(b I/Os Massiveis larger is larger the gap between is larger This is because when number of edges is larger more edges can be pruned by the edge reduction techniques used in increases the number of edges increases As a result more iterations are needed and larger cost is consumed in each iteration as analyzed in Exp-1 when varying the graph size size increases or the number of are not in\337uenced much As anal yzed in Section VII the key factors that in\337uence the cost of Num   Num Fig 9 Synthetic Data Largecan be directly applied on the original graph to output all                       Fig 8 Synthetic Data Vary Memory Size outperforms  and Smallincrease This is because the stop condition for graph contraction is harder to be satis\336ed when  sort nodes/edges is larger when   Fig 9\(e and Fig 9\(f show the time and I/O costs when varying the average no iteration is needed and size from SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC 1H   4H   200K   1H   200K   1H   200K   sfrom b I/Os Vary g Time Vary h I/Os Vary        218 d I/Os LargeSemi Semi Exp-2 Vary Memory Size Exp-3 Vary Node Size   


are 336xed This also explains why the results in the three datasets Massiveis a primitive operation in directed graph exploration which has been studied for both internal memory model and external memory model In the internal memory model strongly connected components of a directed graph can be computed in s for a directed graph with the assumption that the nodes of the graph cannot reside entirely in memory We overcome the de\336ciencies of the existing external    sort sort computation algorithms and propose a new two-phase algorithm with graph contraction followed by graph expansion We analyze the I/O cost of our approach and show that our algorithm can signi\336cantly reduce the number of random I/Os We propose techniques to further reduce the I/O cost of our algorithm and con\336rm the I/O ef\336ciency of our approaches using extensive experiments on both real and synthetic web scale graphs The work was supported by grant of the Research Grants Council of the Hong Kong SAR China No 418512 R EFERENCES  J  A bello A  L  Buchs baum  a nd J  W e s t brook A f unctional a pproach to external graph algorithms s of a graph Zhang et al 26 i mpro v e s uch a n a l gori t h m by constructing and maintaining a special in-memory spanning tree of the graph The semi-external algorithms 23 an d  2 6  are introduced in details in Section III Other than the problem of 336nding time based on DFS 12  A naive way to externalize the internal DFS algorithm requires s Such an algorithm may end up an in\336nite loop and cannot compute all  33\(2 2001  H  Y ildir im  V  C haoji and M  J  Z aki Grail Scalable reachability index for large graphs s repeatedly until the graph 336ts in memory then an internal memory algorithm is used to 336nd the 336nal sor DFS tree on external directed graphs several problems in the external memory model are studied in the literature Dementiev et al 14 p ro vi de an i m pl ement a t i o n o f a n e xt ernal m emory minimum spanning tree algorithm based on the ideas of 22 which performs extremely well in practice even though theoretically inferior to the algorithms of 1   1 0   A jw an i e t al 4 6  propos e i mpl e ment at i ons of e x t e rnal undi rect ed breadth-\336rst search algorithm with the idea from 18 Ul rich Meyer et al 20 21  19  des i gn and i mpl e ment pract i c al I/O-ef\336cient single source shortest paths algorithm on general undirected sparse graphs Surveys about designing I/O ef\336cient algorithms for massive graphs can be found at 24 5  X C ONCLUSIONS In this paper we study I/O ef\336cient algorithms to 336nd all  3\(1 2010  J  Hellings  G  H  F letcher  and H  H averkort Ef\336cient external-memory bisimulation on dags In  3\(1 2010  Z  Z h ang J  X Y u  L  Q in L  C hang a nd X L i n I/O e f 336 cient Computing sccs in massive graphs In scan by maintaining the list of nodes that should not be traversed using tournament trees 17 and b uf fered repos i t o ry t rees 8  respectively Despite their theoretical guarantees these algorithms are considered impractic al for general directed graphs that encountered in real applications Cosgaya-Lozano and Zeh 13 p res e nt a c ont ract i o n b as ed al gori t h m w hi ch cont ract s V M E B V B 2 are similar as stated in Exp-2 IX R ELATED W ORK Finding strongly connected components of a directed graph V G V G E G E V E E V E  14\(1 1985  T  H  Cor m en C  S tein R  L  R i v es t and C  E  L eis e r s on  2003  U Me yer a nd N Z e h I/O-ef 336c ient undirected shortest paths with unbounded edge lengths In s Both DFS based algorithm 8 and c ont ract i o n b as ed algorithm 13 a re i n t roduced i n det a i l s i n S e ct i o n III In addition to external algorithms there are semi-external algorithms for ACM Comput Surv Introduction to Algorithms IFIP TCS         Proc of ESA\22202 Proc of ESA\22206 SIAM J Comput Commun ACM Proc of SIGMOD\22213  32\(3 2002 2 A  A ggar w a l a nd J  S  V itter  T h e i nput/output com p le xity of s o r ting and related problems  31\(9 1988  A  V  A ho J  E  Hopcroft a nd J  D Ullm an I/Os Chiang et al 10 propos e a n a l gori t h m with I/O complexity  Addison-Wesley 1983 4 D  A jw ani R D e m e ntie v  and U  M e y er  A com putational s tudy of external-memory bfs algorithms In  2006  D  A jw ani a nd U Me yer   6\(1 2011  A  L  Buchs baum  M  H  G oldw a sser S Venkatasubramanian and J Westbrook On external memory graph traversal In  2002  J  S  V itter  E x ter n al m e m o r y algor ithm s and d ata s tr uctur e s   2007 7 E  A ngel R Cam p igotto a nd C L a f o r e s t  A nalys i s a nd com p ar is on of three algorithms for the vertex cover problem on large graphs with low memory capacities  1995  N  Chiba a nd T  N i s h izeki A r bor icity and s ubgr aph lis ting algor ithm s   2009  R Dem e ntie v  P  Sanders  D  S chultes  and J  F  S ibe y n E ngineering an external memory minimum spanning tree algorithm In  2012  V  K u m a r a nd E  J  Schw abe Im pro v e d a lgorithm s and d ata s tructures for solving graph problems in external memory In  2002  U  M e yer a nd V  O s ipo v  D es ign a nd im plem entation o f a pr actical i/o-ef\336cient shortest paths algorithm In  2009  U Me yer a nd N Z e h I/O-ef 336c ient undirected shortest paths In  2006  J  F  S i be yn E x ter n al connected com p onents  I n  2013 A CKNOWLEDGMENT  Algorithmic Operations Research          267        and computation which assume that all nodes of the graph can 336t in the main memory Sibeyn et al 23 propose a semi-external DFS  which can be used to 336nd all og  pages 457\320468 2012  Y  J  C hiang M T  Goodrich E  F  Gro v e  R  T am as s i a D E  V e ngrof f and J S Vitter External-memory graph algorithms In  Proc of ALENEX\22207 Proc of SIGMOD\22212 Proc of SODA\22295 Proc of SEA\22209 PVLDB Proc of ALENEX\22209 Proc of ESA\22203 PVLDB                    McGraw-Hill 2001  A  Cos g ayaL o zano a nd N  Z e h A h eur i s tic s t r o ng connecti vity algorithm for large graphs In Algorithmica LargeProc of SPAA\22202 G O O O O Algorithmics of Large and Complex Networks Data Structures and Algorithms SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC SCC  Later Kumar and Schwabe 17 a nd B u chs b aum e t a l  8  i mprove the I/O complexity to  chapter 1 Design and Engineering of External Memory Traversal Algorithms for General Graphs Springer 2009  D  A jw ani U Me yer  and V  O s i po v  Im pro v e d e xternal m em ory b fs implementation In  2000  J  Cheng Y  K e  S  Chu and C Cheng E f 336 cient p roces s i ng of distance queries in large graphs a vertex cover approach In  2004  W  F a n J  L i  S  M a H W a ng and Y  W u Graph hom om orphis m revisited for graph matching  1996  K Mehlhorn a nd U Me yer  E xtern al-memory breadth-\336rst search with sublinear i/o In  2004  J  F  Sibe yn J  Abello a nd U Me ye r Heuristics for semi-external depth 336rst search on directed graphs In Proc of SWAT\22204  and SmallProc of SODA\22206 Proc of SODA\22200 219 Proc of SPDP\22296 Proc of SIGMOD\22212 


                  


             


 





 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


