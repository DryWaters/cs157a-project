Scalable Infrastructures for Data in Motion David Ediger Rob McColl Jason Po ovey Dan Campbell Georgia Tech Research Institute Atlanta Georgia Abstract Analytics applications for reporting and human interaction with big data rely upon scalable frameworks for data ingest storage and computation Batch processing of analytic workloads increases latency of results and can perform redundant computation In real-world applications new data points are continuously arriving and a suite of algorithms must be updated to reîect the changes Reducing the latency of re-computation by keeping algorithms online and up-to-date enables fast query 
experimentation and drill-down In this paper we share our experiences designing and implementing scalable infrastructure around NoSQL databases for social media analytics applications We propose a new heterogeneous architecture and execution model for streaming data applications that focuses on throughput and modularity I O VERVIEW AND B ACKGROUND Among the many use cases of big data the characteristics of streaming social media have motivated a paradigm shift in algorithms and system design for large-scale computing Aggregate data set sizes range on the order of petabytes 
New data arrives continuously Twitter and Facebook report sustained data volumes of 5,000 to 50,000 events per second with momentary peaks over 100,000 events per second  Computing in a batch model over the full data set might require hours or daysÑtoo long for an Internet in which conversations can become trending topics in a matter of minutes With high-velocity data in motion the objective is to minimize the latency between when data is generated and when a suite of analytic applications is updated to reîect the new information Data arising from social media contain rich semantic information Aside from the content which itself can vary by 
language and format restrictions we can obtain information about the poster and any other actors that are mentioned including geographical and proìle data It is not uncommon to have presences on multiple services Aggregation and correlation of data by actor across systems is an ongoing requirement The questions that analysts may ask are similarly varied A simple query might look like What are the most common words and phrases right now Applying social network analysis techniques to the graph of who is speaking to or referring to whom one might ask Who are the inîuential posters and how do they form communities Looking at the content we can extract topics of discussion through classiìcation and measure 
sentiment of text for positive and negative response These methods can be combined in many variations to drill down into the data Having developed state-of-the-art parallel algorithms to solve many of these queries on dynamic data sets 4 5  this paper describes and justiìes a scalable infrastructure for managing streaming data and analytic tools that supports real-time applications with complex queries In the remainder of this section we describe existing approaches to large-scale analytics as well as give a brief overview of the algorithms of interest In Section II we describe a feed-forward streaming architecture that supports multiple data ingest streams a 
multithreaded in-memory graph database and a scheduler for analytic modules that publish and subscribe to result stores Section III analyzes the data ingest portion of the system including a new template-based text parser that converts structured data objects such as Tweets in JSON format into graph edges Section IV describes the multithreaded inmemory server process that manages the graph data store and schedules algorithms Last we demonstrate how to implement RESTful and stateful HTTP interfaces to this data store Prior Work Research and development of large-scale data mining frameworks for complex data have focused largely on distributed 
memory homogeneous clusters A variety of execution models are employed MapReduce bulk synchronous parallel asynchronous message passing on-disk and in-memory Graph algorithms particularly when applied to social network analysis have garnered much attention in the research community because of data scale and their inherent complexities Pegasus is a graph mining system that runs on Apache Hadoop MapReduce and the Hadoop Distributed File System HDFS Algorithms are expressed in a data-parallel model that operates on all graph edges or vertices simultaneously Googleês Pregel implements a distrib uted graph processing framework with a bulk synchronous parallel BSP exe 
cution model Vertices are mapped to cluster nodes with their incident edges At each time step vertices receive messages from other vertices compute and modify their local state and send messages to other vertices to be received in the next time step Graph algorithms are easily expressed in this model although communication volume can be high The GraphLab e x ecution model e xtends BSP with asynchronous execution Each vertex is computed in parallel During execution it can read the state of neighboring vertices 
2014 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing 978-1-4799-2784-5/14 $31.00 © 2014 IEEE DOI 10.1109/CCGrid.2014.91 875 


002\003\004\005\006\007\010 011\003\004\012\003\005\010 010 010 010 010 010 013\014\015\016\017\010\011\003\004\012\003\005\020\010\021\022\002\023\013\011\024\010 025\003\015\026\027\030\031\006\010 032\033\003\004\033\034\035\010 032\027\021\036\037\010 014\015\031\026\005\010 025\003\015\026\027\030\031\006\010 025\003\015\026\027\030\031\006\010 025\003\015\026\027\030\031\006\010 002\003\004\005\006\007\010\011 012\013\014\015\016\017\003\020\011 002\003\004\005\006\007\010 011\003\004\012\003\005\010 002\003\004\005\006\007\010 011\003\004\012\003\005\010 002\003\004\005\006\007\010 011\003\004\012\003\005\010 Fig 1 Conceptual overview of the STINGER streaming framework The framework converts raw social media data to representations amenable to graph databases NoSQL and relational databases Analytics use the data source most appropriate for their queries perform computation and signal neighboring vertices The computation section is decomposed into three phases gather apply and scatter This decomposition enables the framework to further parallelize the gather and scatter portions of the computation Spark is an in-memory MapReduce frame w ork for distributed computing that runs on top of Hadoop 2 YARN A suite of tools for SQL machine learning graph analytics and streaming computations are built on top of Spark Our work on STINGER focuses on the streaming graph model In this model an unending stream of graph edge insertions and deletions arrives continuously Graph algorithms minimize re-computation by isolating only the changes to the graph data structure and metric result STINGER is a multithreaded in-memory graph data structure that supports edge and vertex types edge and vertex weights and edge timestamps For a comparison and performance evaluation of these and other open source graph databases please refer to A number of efforts are underway that implement distributed event-processing frameworks The Apache Incubator is supporting development of Storm S4 and Samza Amazon Web Services recently announced the commercial availability of Kinesis IBM offers InfoSphere Streams as a distributed event-driven system With streaming graph algorithms the state of the algorithm can be too large to forward from node to node for each event in a distributed cluster This work expands the horizons of distributed event-processing frameworks incorporating complex graph analysis and leveraging large shared memory servers to create a more general execution model for stream computing in heterogeneous applications II S YSTEM D ESIGN In a real-time streaming social network analytics application to turn data into actionable knowledge our objective is to increase throughput and decrease latency The pipelined feedforward architecture in Figure 1 ingests raw social media data from a variety of services Parsing data is usually embarrassingly parallel and can be accomplished with a scale-out cluster Data objects are transformed into representations understood by the databases Multiple databases are used to answer different types of queries Graph-based information is sent to a graph database where streaming social network analysis algorithms continuously update metrics of interest Topic classiìcation and sentiment analysis compute in real-time as the text content ows through the system Raw data can be stored in a NoSQL database  document store such as MongoDB or a traditional relational database such as MySQL or MS SQL Server for post-hoc analysis Consistency between databases is not necessary because online algorithms do not access historical data In a pipelined system the throughput of the system is bound by the slowest stage Using multiple types of data stores ensures that queries are handled by the database most capable of responding quickly Having observed many graph algorithms in a streaming data scenario several core design principles emerge First all analytics need a static view of the portion of the graph currently being examined and the list of updates to be made The alternative is to design algorithms to operate on a data structure that could be changing while they are reading Designing for this scenario requires handling of complex corner cases that are unlikely to occur In our framework algorithms always receive a snapshot view of the entire graph before and after sets of updates 
876 


A second guiding design principle is that analytics should be able to utilize the results of other analytics While many research papers consider algorithms in isolation it is good practice to be able to reuse results For example a samplingbased algorithm would need to consider the connected components of the graph to ensure that each component is represented according to the distribution of component sizes Likewise a community detection algorithm that is agnostic to scoring function could subscribe to another running algorithm to provide its scoring input Last in a streaming context with no beginning or end it is important that the server continue to run and algorithms can come and go during execution To meet this requirement algorithms must run in separate processes from the server registering via a known protocol Running algorithms in separate processes adds a layer of isolation With appropriate watchdog timeouts algorithms cannot stall the server from making forward progress on the incoming data stream In the next three sections we will describe in detail how to design and implement a streaming data framework capable of processing hundreds of thousands to millions of events per second on commodity hardware Our motivating application is social media analysis that combines real-time network analysis text analysis and historical queries in an interactive web-based visualization III D ATA I NGEST To produce data sets from dynamic sources researchers often crawl data sources over a short window of time to create a static snapshot or capture the stream of changes that can be replayed at a later time Ofîine these snapshots and stream samples may be carefully manipulated into easily parsed formats in order to be ingested by the system This data munging and ingest process usually requires a non-trivial amount of work from an expert and can take signiìcantly longer than the actually running analytics In order to truly be able to analyze streaming data this process must be an automated and efìcient part of the infrastructure In the workîow presented in Figure 2 the stream processes on the left-hand side handle the ingest task It is the responsibility of a stream to produce batches containing some combination of edge updates vertex updates and metadata to send to the server The data in the batch is obtained through either consuming an input data source or generating synthetic data within the stream process Input data sources can include static or replayed sources such as les or databases or dynamic sources such as streaming web endpoints The stream process must understand how to infer the relationships in the data to produce edges and vertices Although the raw data itself is not stored in STINGER the stream could send the raw data through and include a reference in each edge and vertex update to indicate which data object created the update This allows streaming algorithms to analyze content along with the relational data The provided system uses the concept of templates to enable exibility and ease-of-use to provide generic JSON JavaScript Object Notation and CSV Comma-Separated Values parsing streams Each assumes that its input will be a template read from a le and a stream of entire valid JSON objects or CSV rows one per line read from standard input The template is expected to follow a format similar to the input stream with one or more valid JSON objects or CSV rows but should contain reserved keywords in place of values to indicate where vertices weights timestamps types etc should be found The stream data is checked against each template and extra data is ignored Data that results in at least one match will be added as metadata This concept could easily be extended to a variety of formats including XML log les and even plain text In order for the system to process the data it must be stored and transmitted in a common and serializable format In the workîow presented in Figure 2 Googleês Protocol Buffer serialization library is used Protocol Buf fers were selected based on a balance of exibility compactness language support and speed of serialization and deserialization In addition to the run-time library a compiler is provided that uses message format deìnitions in proto les to produce implementation code in the desired language C Python and Java supported by Google additional language support provided by the community The Batch message used by streams contains four variable-sized arrays edge insertions edge deletions vertex updates and metadata In addition to being used by streams to send data to the server three other message types two of which contain a nested Batch message are used by the server to communicate with the other process types Streams send batches to the server via TCP The server multiplexes incoming connections and data from one or more streams accross a handful of threads Incoming data is atomically enqueued to be processed by the system on a rst-come rst-served basis This design means that stream processing can easily be distributed to one or more separate machines and that content analysis could be performed as a pre-processing step if needed IV S ERVER AND A LGORITHM S CHEDULING The STINGER data structure and the streaming graph algorithms are designed and tuned for multicore shared memory systems Today it is possible to buy commercial off-the-shelf COTS servers with up to 2 TB of DRAM Systems from SGI and Cray can be built with up to 64 TB of shared memory Shared memory computing leverages the relatively high internal interconnect bandwidth and low latency inside the chassis Communicating over an Inìniband or 10Gb Ethernet network generally incurs at least a factor of 10 decrease in bandwidth and increase in latency compared to the capability of the DRAM For data-intensive algorithms this gap can make distributed computation expensive In high-performance 
877 


002\003\004 002\003\004\005\006\007\010\003\011\012 013\014\015\016\012 017\007\020\021\003\020 002\003\004\005\006\007\010\003\011\012 022\014\023 017\007\020\021\003\020 002\003\004\005\006\007\010\003\011\012 002\003\024\010 017\007\020\021\003\020 025\007\026\011\027\004\012 030\011\031\003\012 032\003\026\003\020\007\010\027\020 017\006\007\033\026\012\002\003\024\010 025\007\034\012\022\014\023 025\007\034\012\013\014\015\016 014\002\035\016\032\030\025\012 014\003\020\036\003\020 017\020\027\010\027\037\027\006\012 !""\003\020\012\030\011\031\003\012 \007\010\037#\012\027\036\003\020\012\002\022\017 014#\007\020\003\011\012$\003\004\027\020%\012$\007\005\005\003\011\012\014\002\035\016\032\030\025\012&\007\010\007\012\014\010\020!\037\010!\020\003 022\027\026\026'\012 022\027\004\005 017\007\031\003 025\007\026 022\006!\021\010'\012 022\027\003 014$$\012 025\003\021!\006\010\021 014$$\012 025\003\021!\006\010\021 014$$\012 025\003\021!\006\010\021 022 014$$\012 025\003\021!\006\010\021 022\027\004\004'\012 003\010 014$$\012 025\003\021!\006\010\021 013\014\015\016\012 025\017\022  014\010\020\003\007\004\012 017\020\027\037\003\021\021 006\031\027\020\033\010#\004\012 017\020\027\037\003\021\021 027\026\033\010\027\020\012 017\020\027\037\003\021\021 014#\007\020\003\011\012 003\004\027\020 017\020\027\010\027\037\027\006\012 !""\003\020\012\036\033\007\012\002\022\017 014#\007\020\003\011\012$\003\004\027\020%\012\025\003\007\011\021 Fig 2 Data ows from left to right All stages aligned vertically execute on the data in parallel All processes in direct contact with shared memory h ave write access to that memory Execution is synchronized by the server Streams algorithms and monitors can join or leave the workîow at any time contexts the ne-grained light-weight synchronization provided by atomic operations within a single memory space can be highly beneìcial when cooperation between threads and processes is needed Leveraging shared memory simpliìes algorithms No data partitioning or sharding is necessary Massive data sets in social media are notoriously difìcult to partition in a balanced manner W e will use the shared memory to isolate analytic processes from the server process increasing resilience to failure and security Using shared memory to share data among processes increases the efìciency of multicore as singlethreaded analytics can be run simultaneously The STINGER server in Figure 2 is a multithreaded server process that is responsible for accepting incoming edges in the form of Protocol Buffers applying the updates to the graph managing the in-memory data store and registering and scheduling analytic algorithms Graph edges arising from social media are tagged with screen names or user handles in the form of unique identiìer strings To conserve space and simplify indexing the STINGER server maintains a look-up table that converts unique identiìer strings to unique integers and vice versa The performance of this component of the system is critical as all external requests will use strings to identify vertices The mapper is implemented as a pre-allocated hash table that is double the number of vertices in size must be a power of 2 to map from strings to vertex identiìers a look-up table to map from vertex identiìers back to strings and a pre-allocated stack to store string data Although safe parallel deletion from the table is possible following semantics similar to insertion created mappings are permanent at this time due to the use of stack-based allocation for string storage Strings are hashed by XORing each 64-bit word zero padded then applying a 64-bit mix function and masking the result The hash function is optimized for speed over uniformity Atomic in-place locking operations are used on the hash table when creating new entries following the same full-empty emulation used in This ne-grained locking approach allows for safe parallel insertion and guarantees that two threads creating the same string mapping will receive the same vertex identiìer Functions for looking up mappings in each direction are also provided The scalability of this design is shown in Figure 3 in which 100,000 words of 12 or more letters were randomly selected and inserted in parallel The test is performed with a an initially empty map and b with a map with the set already inserted The number of random insertions is tested at 1x 2x and 10x the size of the set An equivalent mapping using a C STL map and vector protected with a lock is also tested for comparison Results show that given a data source with multiple relationships forming around individuals i.e edges are new but the vertices involved are likely to have been seen such as a social network the data structure performance does scale with parallelism and drastically outperforms the simple lock A Shared Memory Mapping Analytics or algorithm clients are compiled into standalone executables and linked with the STINGER dynamic library Algorithms run in isolated processes on the same system as the STINGER server process At the beginning of execution a client algorithm registers with the server on a known port It declares a unique name known dependencies and requests a pool of local storage more on these later in the section 
878 


 1 2 4 8 16 32 10 5 10 6 10 7 10 8 Threads Insertions per second Scalability of stinger names 10P.sting 1E.sting 2E.sting 10E.sting 10E.map 10P.map Fig 3 The scaling performance of the stinger names mapping compared with a C STL map and STL vector protected with a lock E indicates an empty mapping at start and P a pre-ìlled map 1,2 and 10 indicate 100k 200k or 1M insertions were randomly selected from a set of 100k words Experiment conducted on a dual Intel Xeon E5-2670 with 64 GB main memory Note 10P.map and 10E.map are nearly indistinguishable The client receives from the server a shared memory mapping of the in-memory graph data structure that can be mapped private to allow for local modiìcation The advantage of this shared mapping is that the client algorithm computes on a static version of the graph while the server process can update incoming edges in parallel The server process is responsible for scheduling client algorithms so as to avoid data races After a client rst registers and receives a private mapping of the data structure it enters the initialization phase In this phase the algorithm computes its results from scratch for the current state of the data After completing initialization the algorithm is scheduled alongside other client analytics The processing loop consists of three phases Phase one is the Pre-Update phase in which clients view the graph in a state before any updates are made Not all algorithms require a preupdate phase but some such as clustering coefìcients will be comparing the state of the graph before and after updates are made Phase two of the update cycle is when the server applies the batch of new edges to the graph During this phase client algorithms must pause because the graph is in an inconsistent state When updates are complete the server notiìes client algorithms to enter the Post-Update phase In this phase each client maps the new graph data structure and receives the batch of updates Using this information it is able to update or recompute its metrics The nite state machine that regulates algorithm clients is depicted in Figure 4 The STINGER client library is responsible for negotiating 035\026\033\010\033\007\006\033+\007\010\033\027\026 017\020\003,-\005\011\007\010\003 030\020\020\027\020 017\027\021\010,-\005\011\007\010\003 035 014-\022\022\030\014\014 035 035 014-\022\022\030\014\014 014-\022\022\030\014\014 Fig 4 Finite state machine for client algorithms with the server and re-mapping the data structure for each new phase The developer need only supply initialization preupdate and post-update functions that take a pointer to the data structure The client library is available in C/C with a Python wrapper implemented using CTypes The Python wrapper has proven useful in our work to quickly connect content to powerful analysis libraries such as scikit-learn and the Natural Language Toolkit which can in turn provide additional information to graph algorithms The wrappers also enable faster prototyping of algorithms that are meaningfully connected into the workîow B Pub/Sub Vertex Model The client algorithm connects to and registers with the STINGER server it has the opportunity to request a pool of storage This pool of storage is allocated by the server in shared memory and advertised to all other algorithms This feature enables an algorithm to publish its result set so that it can be read by other clients The storage pool consists of one row per vertex in the graph The client speciìes the number of columns per row and the data type of each column This speciìcation is given in a description string passed from client to server Supported data types are f for single-precision oating point d for doubleprecision oating point i for 32-bit signed integer l for 64bit signed integer and b for 8-bit binary In the description string data types are listed rst followed by a name for each column For example a client implementing PageRank might specify  d pagerank  using a single double-precision oating point value per vertex called pagerank A client algorithm implementing community detection might require an integer per vertex to store the community ID and a double per vertex to store the score Its data description eld would specify  ld communityId score  The server allocates a contiguous chunk of memory according to the description string speciìcations and gives a handle to the client The client maps this portion of memory read-write It is the only process that will write to this space Other clients can subscribe to this result set re-mapping it privately with 
879 


 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 0 20 40 60 80 100 120 140 160 180 Number of Algorithms Milliseconds Latency of Workîow start latency Fig 5 This chart gives the average latency between the minimum and maximum start time for algorithms running in parallel in a single stage of the workîow as the number of algorithms is increased from 2 to 16 each update phase The server and client libraries coordinate re-mapping transparently C Roles  Dependencies The STINGER execution model with pub/sub enables one algorithm to use the results of another algorithmês computation An example use case might be a community detection algorithm that uses connected components information as well as the PageRank scores of each vertex as input to the computation To fully implement this feature the STINGER server coordinates algorithm dependencies and scheduling without the need to double buffer result sets When a client registers with the server it expresses its dependencies by giving the string identiìers of algorithms that it subscribes to The server constructs a directed acyclic graph DAG of algorithms and schedules their execution in a parallel level-synchronous manner Each algorithm remaps the data structure and any other client result sets at the beginning of execution ensuring that the data subscribed to is most current The overhead of running multiple single-threaded algorithms in parallel within the workîow is plotted in Figure 5 In the gure the start latency computed as the difference between the start of the pre-processing phase of the earliest algorithm and the same point for the latest algorithm is measured as the number of processes is increased from 2 to 16 The gure demonstrates a small cost to adding algorithms likely due to sequentially sending the start messages which could be improved through parallelization or switching from TCP to UDP On average the latency between the end of the pre-processing phase of an algorithm and the beginning of the pre-processing phase of an algorithm dependent on it was approximately 29 milliseconds To avoid freezing the system algorithms must respond before a watchdog timer runs out typically about ve seconds If an algorithm cannot keep up with the forward progress of the system it is placed back in initialization state and can re-join the computation after it has reset its internal state An example timeline in Figure 6 shows the interaction between client algorithms and the server Today STINGER supports only required dependencies However optional dependencies may aid system exibility In our example of community detection using PageRank scores if PageRank was running the community detection algorithm could utilize the results through the pub/sub mechanism However if PageRank was not available the community detection algorithm would compute the score itself or provide an alternate means of scoring The corollary to optional dependencies are role-based algorithms Calculating optimal results of many graph queries is NP-Complete or worse As a result many heuristic approximations have been proposed In a system like STINGER several clients could run each computing a different heuristic approximation A role-based system would enable each to advertise that they are computing the same metric but in a different way Other clients subscribing to their results could utilize one several or all of the result sets as appropriate V E XTERNAL I NTERFACES Each client algorithm can produce a very large dense result vector during each update cycle A client producing a single double-precision oating point value for each vertex in the graph consumes 8 GB assuming one billion vertices in the graph It is prohibitive to send 8 GB to an analyst workstation every minute The memory bandwidth of the server is much higher than the network bandwidth between the server and the workstation The analyst usually does not need all of this data but rather is looking for speciìc data points As a result we can minimize client bandwidth by performing as much of the operation on the server where the data is already in memory before sending the results to the client at the very last moment For this reason the STINGER execution model employs a special class of algorithm client known as the monitor process It is a standalone process that is scheduled like a client algorithm but cannot publish data and is dependent on all other algorithms in the system By virtue of this attribute it is always scheduled in the last update slot and has access to all other algorithms result stores In Figure 6 the HTTP Endpoint is a monitor process that we will now describe Our primary monitor process is an HTTP endpoint that serves a RESTful interface to the algorithm result stores and graph data Using the embedded Mongoose HTTP server we accept JSON-RPC POST requests The JSON-RPC protocol is a simple remote procedure call that utilizes JSON 
880 


 022\002\035\023\030 022\027\004\004!\026\033\010%\012&\003\010\003\037\010\033\027\026 017\025\030 017\015\014\002 022\027\026\026\003\037\010\003\011\012\022\027\004\005\027\026\003\026\010\021 017\025\030 017\015\014\002 017\025\030 017\007\031\003\025\007\026 017\025\030 017\015\014\002 017\025\030 017\017/0 014\002\035\016\032\030\025\012\014\003\020\036\003\020  005 006\005\006\007\006\010 Fig 6 This timeline shows the interaction between client algorithms monitor processes and the STINGER server as a batch of edge insertions and del etions are applied to the graph In this example the Community Detection algorithm is registered as dependent on PageRank and Connected Components algorit hms a JavaScript GUI for sentiment and topic tracking b JavaScript tool for subgraph connectivity visualization Fig 7 Examples of two analytics visualizations built on the STINGER streaming framework formatting to encode a method and parameters JSON is a convenient choice because it is ubiquitous in web-based client-side computing RapidJSON is an open source C library for parsing and formatting JSON strings and was chosen for performance Basic RPC methods include querying what algorithms are running and the data description of each algorithm More advanced methods perform queries on the client data stores For example we provide a method to return the top/bottom n values from a data array This method sorts all of the data inmemory and extracts only the values of interest An optional parameter to all methods returns the vertex identiìer strings with the result Sampling over a data array is achieved by an optional parameter that speciìes the number of samples or the bucket size and includes exponential sampling Combining sampling with sorting effectively computes a histogram on the server returning kilobytes instead of gigabytes A similar RPC method returns the results for a speciìc set of vertices Another method performs a reduction over the data array such as a sum min or max Each time a query is made the requester chooses to receive the current results immediately or wait for an update If waiting for an update the HTTP server will not respond to the request until the next batch of edges is processed In cases where a streaming algorithm does not exist or the results are input-dependent such as a connectivity query we extend our RESTful API by implementing sessions For example if the query is What are the shortest paths between vertex A and B  we would register our query with the 
881 


server which would create a session compute the current shortest paths and return the edge list to the requester with a session identiìer In subsequent requests the requester need only reference the session identiìer and receive a list of edge insertions and deletions since the last query that can be used to keep the requester in sync with the server The visualization corresponding to such a query can be seen in Fig 7\(b We have also used monitor processes to periodically checkpoint the state of the graph to disk for an additional layer of resilience Monitor processes can perform tasks such as checking the consistency of metadata within the data structure to identify errors We provide a monitor process that periodically snapshots a client result store to a MongoDB instance VI C ONCLUSION As data volumes continue to grow it will become impossible to store all raw data that is collected At each time step data will be computed and analyzed and only those elements perceived to be most important will be stored This environment will create the need for new infrastructures and execution models for computation In this paper we have described the STINGER framework for streaming data analytics The system utilizes heterogeneous clusters to ingest data from multiple social media services form in-memory graph representations of relational data store raw data in traditional databases for historical search and give analysts up-to-the-minute insight into a suite of streaming analytics The system design is focused on performance and modularity minimizing latency and increasing throughput The infrastructure that we have described is free and open source software It is BSD licensed and available at www.stingergraph.com  In Figure 7 you will nd screenshots of several web-based tools that have been implemented on the STINGER architecture These tools combine complex graph analytics topic classiìcation text sentiment analysis and geolocation for real-time streaming data analysis R EFERENCES  T witter  Goal  April 2012 http://blog.uk.twitter com/2012/04/goal.html  F acebook K e y f acts  May 2012 http://newsroom.fb.com/content/default.aspx?NewsAreaId=22  D Ediger  K Jiang J Riedy  and D A Bader  Massi v e streaming data analytics A case study with clustering coefìcients in Workshop on Multithreaded Architectures and Applications MTAAP  Atlanta Georgia Apr 2010  D Ediger  E J Riedy  D A Bader  and H Me yerhenk e T racking structure of streaming social networks in 5th Workshop on Multithreaded Architectures and Applications MTAAP  May 2011  J Riedy and D Bader  Multithreaded community monitoring for massive streaming graph data in Parallel and Distributed Processing Symposium Workshops PhD Forum IPDPSW 2013 IEEE 27th International  May 2013 pp 1646Ö1655  R McColl O Green and D Bader   A ne w parallel algorithm for connected components in dynamic graphs in Conference on High Performance Computing HiPC 2013 IEEE 20th International  December 2013  U Kang C E Tsourakakis and C F aloutsos Pe gasus A peta-scale graph mining system implementation and observations in Proceedings of the 2009 Ninth IEEE International Conference on Data Mining  ser ICDM 09 Washington DC USA IEEE Computer Society 2009 pp 229Ö238  G Male wicz M H Austern A J Bik J C Dehnert I Horn N Leiser  and G Czajkowski Pregel A system for large-scale graph processing in Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data  ser SIGMOD 10 New York NY USA ACM 2010 pp 135Ö146  A Buluc  and K Madduri Parallel breadth-ìrst search on distributed memory systems in Proceedings of 2011 International Conference for High Performance Computing Networking Storage and Analysis  ser SC 11 New York NY USA ACM 2011 pp 65:1Ö65:12  J E Gonzalez Y  Lo w  H Gu D Bickson and C Guestrin Po wergraph Distributed graph-parallel computation on natural graphs in Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementation  ser OSDIê12 Berkeley CA USA USENIX Association 2012 pp 17Ö30  M Zaharia M Cho wdhury  M J Franklin S Shenk er  and I Stoica Spark Cluster computing with working sets in Proceedings of the 2Nd USENIX Conference on Hot Topics in Cloud Computing  ser HotCloudê10 Berkeley CA USA USENIX Association 2010 pp 10Ö10  D A Bader  J Berry  A Amos-Binks D Cha v arr  a-Miranda C Hastings K Madduri and S C Poulos STINGER Spatio-Temporal Interaction Networks and Graphs STING Extensible Representation Georgia Institute of Technology Tech Rep 2009  R McColl D Ediger  J Po ovey C D and D Bader A performance evaluation of open source graph databases in Parallel Programming for Analytics Applications PPAA 1st Workshop on  Sept 2 pp 1Ö5  Google Protocol b uf fers  http://code.google.com/apis/protocolb uf fers  D Ediger  R McColl J Riedy  and D A Bader  STINGER High performance data structure for streaming graphs in The IEEE High Performance Extreme Computing Conference HPEC  Waltham MA Sep 2012 best paper award  K Lang Finding good nearly balanced cuts in po wer la w graphs  Yahoo Research Tech Rep 2004  S L yubka Mongoose  https://code.google.com/p/mongoose  JSON-RPC W orking Group JSON-RPC 2.0 speciìcation  http://www.jsonrpc.org/speciìcation  RapidJSON  https://code.google.com/p/rapidjson 
882 


            


       


        


             


      


 14   Variable name="Request_ID" kind="linkToIndication" dataType="UINT16" units="none   Variable name="Result" kind="indication" dataType="UINT16" units="none   Enumeration    Option name="No_Error value="1 description Request was successful  are valid   Option name="Failure value="2 description Request could not be fulfilled  Variables returned are invalid   Enumeratio n   Variable   Variable name="Device_ID" kind="ID" dataType="UINT16" units="none    Variable name="RSSI" kind="signalStrength" dataType="UINT16" units="none    Variable name="Battery_Status" kind="batteryLife" dataType="UINT16" units none    Variable name="Architecture" kind="hardwareArchitecture" dataType="UINT08" units="none     Enumeration   Option name="cc2430" value="1" description="The device is a TI cc2430 board   Option name="cc2530" value="2 description="The device is a TI cc2530 board   Enumeration    Variable    Variable name="TIM_Type" kind="nodeType" dataType="UINT08" units="none     Enumeration   Option name="coordinator" value="1" description="The node is a cting as a ZigBee  coordinator   Option name="router" value="2" description="The node is acting as a ZigBee  router     name="end device value="3 description="The node is acting as a ZigBee  end device   Enumeration    Variable   DataReplyMsg   Request   Interface     Interface id="3" name="Thermistor" description="Thermistor service interface   Request   CommandMsg id="1" name="DVS_GET_TEMPERATURE.request   Variable name="Re quest_ID" kind="linkToIndication" dataType="UINT32" units="none   Variable name="Device_ID" kind="ID" dataType="UINT32" units="none   CommandMsg   DataReplyMsg id="1" name="DVS_GET_TEMPERATURE.indication   Variable name="R equest_ID" kind="linkToIndication" dataType="UINT32" units="none   Variable name="Result" kind="indication" dataType="UINT08" units="none   Enumeration   Option name="No_Error value="1 description="Request was successful  are valid   Option name="Failure value="2 description="Request could not be fulfilled  Variables returned are invalid   Enumeration   Variable   Variable name="Temperature kind="tempera ture dataType="FLOAT64 units="F rangeMin="0 rangeMax="100" accuracy="0.10   Variable name="Thermistor_Status" kind="thermistorStateIndication" dataType="UINT08" units="none   Enumeration   Option name="No_Error" value 1" description="The device has detected no errors   Option name="Error" value="2" description="The device has detected an error   Enumeration   Variable   DataReplyMsg   Request   Request   Command Msg id="2" name="DVS_GET_SENSOR_RATE.request   Variable name="Request_ID" kind="linkToIndication" dataType="UINT32" units="none   Variable name="Device_ID" kind="ID" dataType="UINT32" units="none   CommandMsg   DataReplyMs g id="2" name="DVS_GET_SENSOR_RATE.indication   Variable name="Request_ID" kind="linkToIndication" dataType="UINT32" units="none   Variable name="Result" kind="indication" dataType="UINT08" units="none   Enumeration    Option name="No_Error value="1 descr iption="Request was successful  are valid   Option name="Failure value="2 description Request could not be fulfilled  Variables returned are invalid   Enumeration   Variable   Variable name="Sensor_Publish_Rate" kind="duration" dataType="FLOAT64" units="s   DataReplyMsg   Request   Request  


 15   CommandMsg id="3" name="DVS_SET_SENSOR_RATE.request   Variable name="Request_ID kind="linkToIndication" dataType="UINT32" units="none   Variable name="Device_ID" kind="ID" dataType="UINT32" units="none    Variable name="Sensor_Publish_Rate" kind="rateChangeRequest" dataType="FLOAT64" units="s   CommandMsg    DataReplyMsg id="3" name="DVS_SET_SENSOR_RATE.indication   Variable name="Request_ID" kind="linkToIndication" dataType="UINT32" units="none   Variable name="Result" kind="indication" dataType="UINT08" units="none   Enumerat ion   Option name="No_Error value="1 descri ption="Request was successful Variables returned are valid   Option name="Failure value="2 description Request could not be fulfilled  Variables returned are invalid   Enumeration   Variable   Variable name="Sensor_Publish_Rate" kind="duration" dataType="FLOAT64" units="s    Variable name="Sensor_Publish_Rate_Status kind="PublishRateStateIndication dataType="UINT08 units="none   Enumer ation   Option name="Not_Supported value="1 description="The sensor device does not implement this rate   Option name="Rate_Changed value="2 description="The sensor device has updated its publish rate   Enumerat ion   Variable   DataReplyMsg   Request   Notification   DataMsg id="1" msgRate="1" msgArrival="PERIODIC" name="DVS_GET_TEMPERATURE_PERIODIC.indication   Variable name="Temperature kind="temperature dataType="FLOAT64  units="F rangeMin="0 rangeMax="100" accuracy="0.10   Variable name="Thermistor_Status" kind="thermistorStateIndication" dataType="UINT08" units="none   Enumeration   Option name="No_Error" value="1" description="The de vice has detected no errors   Option name="Error" value="2" description="The device has detected an error   Enumeration   Variable   DataMsg   Notification   Interface  xTEDS    


 16  ES   1 a   Gi l s t r a p    Ba l d w i n   S      Fa u l t  T o l e r a n c e  i n  e  Wi r e l e s s  S e n s o r  Ne t wo r k s    I E E E  Ae r o s p a c e   2 0 1 0  P 1 4 8 0 5  2 Al e n a   R  a  F  O s s e n f o r t  J  I nt e l l i ge nt  W i r e l e s s  Se n s o r  N e t w o r k s  f o r  Sp a c e c r a f t  H e a l t h  M o n i t o r i n g    AA  In fo t e c h  C o n fe re n c e  2 0 1 2  3 C om pl e t e  s t a nda r d doc um e nt s  a nd w hi t e  pa pe r s  a r e  av ai l ab l e at  t h e e o r g  w e b  p a g e   e o r g S ta n d a r d s  Zi g B e e Ne t wo r k De v i c e s  Ov e r vi e w  a s px  4 G 133 1 2013  S pa c e  P l ug d Pl a y  A r c h i t e c t u r e  St a n d a r d s  D e v e l o p m e n t G u id e b o o k P u b lis h e d 2 0 1 3  5 P l u g d Pl a y   Pn P  St r u c t u r e s  f o r  Sa t e l l i t e  A p p l i c a t i o n s   ht t p   w w w  a f s bi r s t t r  c om  P ubl i c a t i ons  D oc um e nt s  I nnov n 042309 Sp a c e W o r k s 6 pdf  6 I EEE  ht t p   w w w  ni s t  gov e l  i s d i e e e  i e e e 1451 c f m  7 Z a ck  r ef er en ce U R L   ht t p   v as t  u ccs  ed u  p r o j ect s  t r au m ag p s _ f i l es  d o cs  ch i p co n  Z k f   8 C C 2 530 R e f e r e nc e  U R L   www t i  c o m  l i t  d s  s y m l i n k  cc2 4 3 0 f   9 R ef er en ce A r ch i t ect u r e f o r  S p ace I n f o r m at i o n  Ma n a g e m e n t   C C S D S  3 1 2  0 G 0  Gr e e n  B o o k   M a r c h  2013  0 D DS  r e f e r e n c e  UR L   p o r t a l s  g  dds    1 A AC  M i c r o t e c  r e f e r e n c e  UR L   p p n p a a c m ic r o te c c o m in d e x p h p in tr o d u c tio n to ug d pl a y ht m l  2 D ig i r e f e r e n c e U R L  ht t p   w w w  di gi  c om  pr oduc t s  Zi g B e e   3 P B o o n m a a n d J S u z u k i  T o w ar d  I n t er o p er ab l e Pu b l i s h  Su b s c r i b e  C o m m u n i c a t i o n  b e t w e e n  W i r e l e s s  Se n s o r  N e t w o r k s  a n d  A c c e s s  Ne t wo r k s    I n  Pr o c   o f  IE E E  In t e r n a t i o n a l  W o r k s h o p  o n  In f o r m a t i o n  R e t r i e v a l  i n  Se ns or  N e t w or k s   I R SN  L a s V e g a s N V J a n u a r y 2 0 0 9   B IO G R A P H Y  Ri c h a r d  L   Al e n a  is  a   En g i n e e r  i n  t h e  I n t e l l i g e n t  S y s t e m s  Di v i s i o n  a t  NAS A Am e s   M r   Al e n a  wo r k e d  o n  t h e G r o u n d  D at a Sy s t e m  a n d  p e r f o r m e d  Co m m u n i c a t i o n s  A n a l y s i s  d u r i n g  ope r a t i ons  f or  t he  L C R O S S  L una r  Mi s s i o n  a n d  o n  a v i o n i c s  a n d  s o f t w a r e  a r c h i t e c t u r e s  f o r  Lu n a r  S u r f a c e  S y s t e m s  f o r  h u m a n  m i s s i o n s   H e  w a s  t h e  c o le a d  f o r  th e  A d v a n c e d  D ia g n o s tic  S y s te m s  f o r  I n l Sp a c e  St a t i o n   I SS  Pr o j e c t   d e v e l o p i n g  m o d e l d di a gnos t i c  t ool s  f or  s pa c e  ope r a t i ons   H e  w a s  t he  c hi e f  ar ch i t ect  o f  a f l i g h t  ex p er i m en t  co n d u ct ed  ab o ar d  S h u t t l e an d  M i r  u s i n g  l ap t o p  co m p u t er s   p er s o n al  d i g i t al  as s i s t an t s  an d  s er v er s  i n  a w ir e le s s  n e tw o r k  f o r  th e  I S S  H e  w a s  a ls o  th e  te c h n ic a l le a d  f o r  th e  D a ta b u s  A n a ly s is  T o o l f o r  In t e rn a t i o n a l  S p a c e  S t a t i o n  o n or bi t  di a gnos i s   H e  w a s  gr oup l e a d f or  I nt e l l i ge nt  M obi l e  T e c hnol ogi e s   de ve l opi ng pl a ne t a r y e xpl or a t i on s ys t e m s  f or  f i e l d s i m ul a t i  Al e n a  h o l d s  a n  M  S   i n  E l e c t r i c a l  E n g i n e e r i n g  a n d  Co m p u t e r  S c i e n c e  f r o m  t h e  U n i v e r s i t y  o f  Ca l i f o r n i a   Be r k e l e y   H e  i s  t h e  w i n n e r  o f  t h e  N A S A  S i l v e r  S n o o p y  Awa r d  i n  2 0 0 2   a  NAS A Gr o u p  Ac h i e v e m e n t  Awa r d  i n  1998 f or  hi s  w or k on t he  I S S  P ha s e  1 P r ogr a m T e a m a n d  a  Sp a c e  Fl i g h t  A w a r e n e s s  A w a r d  i n  1 9 9 7    Joh n  O s s e n f or t  is  a C om put e r  S ci en t i s t  an d  em p l o y ee o f     at  N A S A  A m es  R es ear ch  Ce n t e r    cu r r en t l y  w o r k i n g   th e  D is c o v e r y  a n d  S y s te m s  H e a lth  re s e a rc h  a re a  in te g r a tin g  f a u lt ma n a g e me n t  t e c h n ol ogi e s  w i t h ad v an ced  t es t i n g  an d  de m ons t r a t i on of  t he  O r i on M ul t i pur pos e  C r e w  V e hi c l e   I n  th e  p a s t h e  h a s  wo r k e d  i n  n e t wo r k i n g  a n d  s y s t e m s  ad m i n i s t r at i o n  o n  s ev er al  ex p l o r at i o n  p r o j ect s  an d  pa r t i c i pa t e d i n ous  fi e l d  s i m u l a t i o n s   a s s i s t i n g  i n  a l l  p ect s  o f  w i r ed  an d  w i r el es s  n et w o r k  d es i g n   d ep l o y m en t   tr o u b le s h o o tin g  a n d  m a in te n a n c e  J o h n  h a s  a  d u a l B A  de gr e e  i n A nt hr opol ogy a nd E a s t  A s i a n S t udi e s  f r om  Wa s h i n g t o n  U n i v e r s i t y  i n  S t   L o u i s    Th o m  S t o n e  is  a  S e n io r  C o m p u te r  Sc i e n t i s t  w i t h  C o m p u t e r  Sc i e n   h  Ba c h e l o r s  d e g r e e  a t  SU NY  St o n y  B r o o k   Mr   S t o n e  h a s  be e n a t  N A S A  A R C  e m pl oye d by va r i ous  c ont r a c t or s  s i nc e  1989  wo r k i n g  on a dva nc e d ne t w or ki ng  He  wa s  a n  e n g i n e e r  wi t h  t h e  NAS A Sc i e n c e  I n t e r n e t  p r o j e c t  o f f i c e  w h e r e  h e l ed  t h e p r o j ect  t h at  b r  re l i a b l e  In t e rn e t  c o n n e c t i o n s  t o  re m o t e  l o c a t i o n s  i n c l u d i n g  U  S   ba s e s  i n A nt a r c t i c a  i nc l udi ng M c M ur do S t a t i on a nd Am u n d s o n  S c o t t  S o u t h  P o l e  S t a t i o n   He  wa s  p r i n c i p a l  en g i n eer  f o r  co m m u n i cat i o n s  f o r  t h e N A S A  S ear ch  f o r  t er r es t r i al  I n t el l i g en ce  S E T I   p r o j ect  an d  w as  a s en i o r  en g i n eer  f o r  t h e S p ace S t at i o n  B i o l o g i cal  R es ear ch  P r o j ect   Be f o r e  h i s  i n v o l v e m e n t  w i t h  N A S A   S t o n e  w a s  e m p l o y e d  i n  th e  c o m p u te r  a n d  c o m m u n ic a tio n s  in d u s tr y  a n d  ta u g h t te le c o m m u n ic a tio n s a t th e u n d e r g ra d u a t e  l e v e l    


 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


