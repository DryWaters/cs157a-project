 
 
1  2       3     
   
   
 
    
   Gipps 
 
No.70601015 No.91324022 No.91024032, No.91224008, No.70833003 
    4  5 
MOBIL 
Nagel-Schreckenberg 
7   
6 8 
        
  
9 
 
  
    
Tamminga  
Traffic Simulation Framework   
Pan  Gora  
  
DGIT  
 
Game  
liuyi@tsinghua.edu.cn 
Transform   
100084  
 
Individual 
Decision  
  
  
 
  Research on Meta-model of Driver Behavior in Agent-based Traffic Evacuation Simulation Shengcheng YUAN, Yi LIU  Gangqiao WANG, Yi LIU, Hui ZHANG Institute of Public Safety Research, Department of Engineering Physics, Tsinghua University, Beijing 100084 
    
 Corresponding author liuyi@tsinghua.edu.cn   Abstract: Driver behavior diversity affects traffic process gre atly especially for the traffic evacuation process. Taking the driver behavior diversity into consideration, this paper developed a meta-model of driver behavior in microscopic traffic evacuation simulation, named DGIT frame, which are Decision, Game, Individual and Transform. DGIT frame integrates heterogeneous models in different levels into a unified workflow. By using this frame, driver behavior models can be coupled in various scenarios, between different agents, during multi-period and within big data resources. Based 
on integrating models with different structures or functions, DGIT may describe driver behavior diversity satisfactorily A simulation system is developed in this paper with DGIT frame, and comparison simulation experiments were performed. Simulation results prove the capability of DGIT frame for describing diversity and evaluating plan and decision strategy Key Words: Meta-model, Driver Behavior Variability, Traffic Evacuation Simulation, Model Coupling 2014 IEEE 
1805 978-1-4799-3708-0/14/$31.00 c 
002 


 Decision p s   Observation     2 DLC = \(Sensor Observation, Generation, Actuator     Sensor  OpenTraffic   Game 11    Generation   4            i p s    1 1  Individual     WfMC  p d Generation Decision, Game, Individual, Transform   Transform  1     p d    2           1             4    Sensor  Generation  1     e Sensor Actuator  2  6   p i  Generation p d Helbing        1   3 5 DGIT  e i driving life cycle, DLC Observation  p d new  1806      2    3  XPDL   3        Activities    p s i p i       1          Actuator 1       Observation  Actuator p s  p d 4  p i p s p d old p d new  p d new e  5                      p d old  2014 26th Chinese Control and Decision Conference CCDC  


2. DGIT Decision  0, 1      nodes  G D  Transform   3  Game   p i  DGIT        7                    Decision = \({nodes  2014 26th Chinese Control and Decision Conference CCDC G D    G D      7 p d  4  1807 DGIT   DGIT     DGIT  distance    p s p d  Generation  attr attr 3.1 3.2 Individual      m            G D      8    P      1   2    3          DGIT             1          5  G D 3               6  i   j      n     attr     p s p i  p d new  Event 


2. DGIT   DGIT  Event i Profit ij Measure ij           G G            Game = \({Event Profit}, {Measure}, G G  Profit  Transform = \({State T  Transition}, G T  Operation  Measure  Operation    State I  State I       State T  State T    4  5 6  9        0-1  0-1       Event   State T  DGIT  XPDL   p d      p d new   G I               11 G T      12  1808 8 9  Transition  Individual = \({State I  Operation}, G I  State T Transition            p s p d   p s p d                   G G  attr        1 10 3.5   1  10  5  6   7  2                     p d  p i  p i  p d  p d    State I State I State T Transition  3.4 4     G I                               State I Operation  3.3 2014 26th Chinese Control and Decision Conference CCDC 


2014 26th Chinese Control and Decision Conference CCDC     ArcGIS   A   7 8 3  9 4        109  45500     length i 109    DGIT  speed i C  1809 10  2581      DGIT      13      5.66 8   3    i   9  10   107 4  11       p i  p s p d          13    9  8  15.89km 2            BPR     i  i  1.22 554       122            Gipps    Gipps DGIT   DGIT  


D. Helbing, I. J. Farkas, P. Molnar, et al. Simulation of pedestrian crowds in normal and evacuation situations Pedestrian and evacuation dynamics, Vol.21, 21-58, 2002 13  S. Ossen, S. P. Hoogendoorn. Heterogeneity in car-following behavior: Theory and empirics Transportation research part C: emerging technologies Vol.19, No.2, 182-195, 2011 2  R. E. Wilson. An analysis of Gipps's car-following model of highway traffic. IMA journal of applied mathematics Vol.66, No.5, 509-537, 2001 14  Bureau of Public Roads. Traffic Assignment Manual. US Department of Commerce, 1964   K. Nagel, M. Schreckenberg. A cellular automaton model for freeway traffic. Journal de Physique I, Vol.2, No.12 2221-2229, 1992 8  2014 26th Chinese Control and Decision Conference CCDC DGIT      A. Kesting, M. Treiber, D. Helbing. General lane-changing model MOBIL for car-following models Transportation Research Record: Journal of the Transportation Research Board, Vol.1999, No.1, 86-94 2007 7  X. Pan, C. S. Han, K. Dauber, et al. A multi-agent based framework for the simulation of human and social behaviors during emergency evacuations. Ai & Society Vol.22, No.2, 113-132, 2007 10      P. Murray-Tuite, B. Wolshon. Evacuation transportation modeling: An overview of research, development, and practice. Transportation Research Part C: Emerging Technologies, Vol.27, 25-45, 2013 5  12                  204\204DGIT   P. Gora. Traffic Simulation Framework. Computer Modelling and Simulation \(UKSim\KSim 14th International Conference on. IEEE, 345-349, 2012 11  12\(a    M. Treiber, D. Helbing. Memory effects in microscopic traffic models and wide scattering in flow-density data Physical Review E, Vol.68, No.4, 046119, 2003 3  P. G. Gipps. A behavioural car-following model for computer simulation. Transportation Research Part B Methodological, Vol.15, No.2, 105-111, 1981 6  12\(b  11  H. Tu, G. Tamminga, H. Drolenga, et al. Evacuation plan of the city of almere: simulating the impact of driving behavior on evacuation clearance time. Procedia Engineering, Vol.3, 67-75, 2010 4  1  A. J. Pel, M. C. J. Bliemer, S. P. Hoogendoorn. A review on travel behaviour modelling in dynamic traffic simulation models for evacuations. Transportation, Vol.39 No.1, 97-123, 2012 9  G. Tamminga, M. Miska, E. Santos, et al. Design of Open Source Framework for Traffic and Travel Simulation Transportation Research Record: Journal of the Transportation Research Board, Vol.2291, No.1, 44-52 2012 12  1810             DGIT     DGIT       DGIT   


We next de“ne function  n  x  where x  R n  0    n  x   M    m  W  n  m    w   m x w    n  w 1 x  w Lemma 1 Given any uploading scheme  and a randomized workload assignment WA  we have a randomized uploading scheme  WA  which satis“es E  Cost 1   WA  Cost 2   WA   max x   M   x   Cost 2    Cost 1    Proof  Since the traf“c pattern in ISP  m r    r is exactly the same as ISP m  we only consider one stage Let us consider scheme  rst In the rst stage the cost is Cost 1     m  M max i,w  v m i,w    max i    M   v  i,w  where v m i,w indicates the transfer speed in ISP m during  t i t i 1  for workload w     M   v  i,w  is the sum of the largest  M  values of v  i,w when given i  The inequality holds because there are at most  M  non-zero speeds for any given duration  t i t i 1   We next have the cost of the second stage Cost 2     m  r max i,w  z r v m i,w       r z  r  m max i,w  v m i,w       r z  r max i    M   v  i,w  The cost of the rst stage in  WA is E  Cost 1   WA    m  M   WA m  W  n  WA m ax i   w   WA m v i,w     M  max i   WA m  W  n  WA m    w   WA m v i,w   The second equality above holds because the assignment is uniformly random Similarly The cost of the second stage in  WA is E  Cost 2   WA    M    WA m P  n  WA m   r max i  z r  w   WA m v i,w     M     r z  r max i   WA m  W  n  WA m    w   WA m v i,w   Again because for any  t i t i 1   there are at most  M  values of v i,w  0 Wehave  M    WA m  W  n  WA m    w   WA m v i,w       M   v  i,w    M    WA m  W  n  WA m    w   WA m v i,w    n w 1  v  i,w   n  v   M   v   where v  is an  M  dimensional subvector of v  R n  0   which contains all non-zero transfer speeds in  t i t i 1   Therefore the ratio for the rst stage is E  Cost 1   WA  Cost 1      M    WA m  W   WA m ax i   w   WA m v i,w   max i    M   v  i,w    M    WA m  W   WA m    w   WA m v i  w      M   v  i  w   max x   M   x  where i  argmax i   w   WA m v i,w    Similarly the ratio for the second stage is also bounded by max x   M   x   i.e  E  Cost 2   WA  Cost 2    012 max x   M   x   This proves Lemma 1 Let S   j  be the j th Stirling number for  elements de“ned as the number of partitions of a set of size  into j subsets Let B  be the  th Bell number de“ned as the number of partitions of a set of size   The Bell number is relatively small when  is small B 1 1 B 2 2 B 3  5 B 4 15  The de“nitions also imply   j S   j  B  The following lemma is proven by Greiner et al  Lemma 2 16    N and  012 M   max x   M   x    j 1 S   j   M    M  j   M  j   Theorem 6 Given a  competitive algorithm with respect to cost for the single ISP case then the randomized online algorithm is B    competitive in expectation Proof  Let   be the optimal uploading scheme the corresponding randomized uploading scheme is   WA  The algorithm we use is  WA  Since the workloads in   WA and  WA are the same we have E  Cost 1   WA    E  Cost 1    WA  11 since the algorithm is  competitive Similarly E  Cost 2   WA    E  Cost 2    WA  12 since the traf“c pattern in ISP  m r    r is exactly the same as in ISP m  Lemma 1 implies E  Cost 1    WA  Cost 2    WA   max x   M   x   Cost 1     Cost 2     13 Since   M   x  is a monotonically increasing function of  weuse    as an upper bound of  1  obtaining a corresponding upper bound of   M   x   Combining Eqn 11 12 and 13 as well as Lemma 2 we have the following expected cost of the randomized online algorithm IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 2028 


E  Cost 1   WA  Cost 2   WA    E  Cost 1    WA  Cost 2    WA    max x   M   x   Cost 1     Cost 2           j 1 S     j   M     M  j   M  j   Cost 1     Cost 2           j 1 S     j   Cost 1     Cost 2      B    OP T Remark  For a single link we can employ Heuristic Smoothing whose competitive ratio is smaller than 2 with respect to maximum traf“c volume Then the competitive ratio of Algorithm 2 is 2  in cost Thus Algorithm 3 is 2  B  competitive in expectation When  2  the competitive ratio is 8 a constant regardless of the number of mappers VI P ERFORMANCE E VALUATION We have implemented Simple Smoothing Heuristic Smoothing as well as the randomized online algorithm for performance evaluation through simulation studies The default input W t is generated uniformly at random as shown in Fig 2 where all data are normalized i.e  scaled down by max t W t  We assume there are 5 mappers at different locations and 5 reducers at different locations We choose  2  thus the charge function f m  x  f m,r  x  x 2  A The Single ISP Case First we compare Heuristic Smoothing with Simple Smoothing The two algorithms are executed under a delay requirement D 5  Fig 3 illustrates the traf“c volume scheduled at each time slot Compared with Simple Smoothing Heuristic Smoothing results in a maximum traf“c volume this is about 28 smaller Heuristic Smoothing tries to exploit the available delay to average the traf“c and is less sensitive to the uctuation of traf“c demand as compared with Simple Smoothing For example at around t 10  the traf“c of Simple Smoothing increases abruptly due to high traf“c demand in the input around t 40  it goes down due to low traf“c demand In comparison Heuristic Smoothing results in more even traf“c distributions around t 10 and t 40  Next we examine how the tolerable delay affects the performance of the proposed online algorithms We execute Simple Smoothing Heuristic Smoothing and ITA against a variety of delays ranging from D 0 to D 24  We also compute the of”ine optimum as a benchmark The observed competitive ratios are shown in Fig 4 The results suggest that both Simple Smoothing and Heuristic Smoothing perform much better than ITA Heuristic Smoothing also beats Simple Smoothing by a smaller margin Heuristic Smoothing approaches the of”ine optimum rather closely the observed competitive ratios are always below 1  5 and usually around 1  2  much better than the theoretically proven upper bound in Theorem 4 Heuristic Smoothing is further evaluated under other random inputs including Poisson distribution in Fig 5 Gaussian distribution in Fig 6 and a speci“cally designed random input in Fig 7 All results verify that Heuristic Smoothing works best among the three online cost minimization algorithms B The Cloud Scenario We implemented the randomized algorithm in Algorithm 3 and the native algorithm in Sec V-A They are evaluated under three types of inputs uniform distribution Poisson distribution and Gaussian distribution We compare the costs of the two algorithms using these inputs as shown in Fig 8 Fig 9 and Fig 10 respectively We observe that the randomized algorithm achieves much lower cost than the native algorithm in particular with longer tolerable delays For example Fig 8 shows that the randomized algorithm saves approximately 45 cost as compared with the native algorithm when D 5  and it saves more than 68 when D 10  This suggests that longer tolerable delays provide the randomized algorithm more space of maneuver leading to more evident cost reduce We further investigate the in”uence of   the ratio of original data size to the intermediate data size Results are shown in Fig 11 When D is small a large  causes a rather high cost However when a large D is used e.g  D 40  even a large  only produces a relatively small cost   0 5 10 15 20 0 10 20 30 40 0 0.2 0.4 0.6 0.8 Delay window size  Normalize Cost Fig 11 Relationship between traf“c cost and parameters D    VII C ONCLUSION ISPs now charge big data applications with a new interesting percentile based model leading to new online algorithm design problems for minimizing the traf“c cost paid for uploading big data to the cloud We studied two scenarios for such online algorithm design in this work A Heuristic Smoothing algorithm is proposed in the single link case with proven better performance than the best alternative in the literature and a smaller competitive ratio below 2  A randomized online algorithm is designed for the MapReduce framework achieving a constant competitive ratio by employing Heuristic Smoothing as a building module We have focused on MAX charge rules and leave similar online algorithm design for 95-percentile charge rules as future work R EFERENCES 1 Amazon Elastic Compute Cloud  http://aws.amazon.com/ec2 2 Linode  https://www.linode.com/speedtest 3 Amazon EC2 Case-studies  http://aws.amazon.com/solutions/casestudies IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 2029 


   0 20 40 60 80 100 0 0.2 0.4 0.6 0.8 1 Time Normalized Data Traffic   Uniform Input Fig 2 Uniformly Random Input   0 20 40 60 80 100 120 0 0.2 0.4 0.6 0.8 Time Normalized Scheduled Traffic   Simple Smoothing Heuristic Smoothing Fig 3 Simple Smoothing vs Heuristic Smoothing D 10   0 5 10 15 20 25 1 1.5 2 2.5 3   Delay window size Competitive Ratio   ITA Simple Smoothing  Heuristic Smoothing  Fig 4 Competitive ratio over various delay window sizes under input of uniform distribution   0 5 10 15 20 25 1 1.5 2 2.5   Delay window size Competitive Ratio   ITA Simple Smoothing  Heuristic Smoothing  Fig 5 Competitive ratio over various delay window sizes under input of Poisson distribution   0 5 10 15 20 25 1 2 3 4 5   Delay window size Competitive Ratio   ITA Simple Smoothing  Heuristic Smoothing  Fig 6 Competitive ratio over various delay window sizes under input of Gaussian distribution   0 5 10 15 20 25 1 2 3 4 5 6 7 8   Delay window size Competitive Ratio   ITA Simple Smoothing  Heuristic Smoothing  Fig 7 Competitive ratio over various delay window sizes under a speci“cally designed input   5 10 15 20 25 0 0.2 0.4 0.6 0.8 1 Delay window size Normalize Cost   Randomized Algorithm Native Algorithm Fig 8 Comparison between the proposed randomized algorithm and the native algorithm under input of uniform distribution and  2    5 10 15 20 25 0 0.2 0.4 0.6 0.8 1 Delay window size Normalize Cost   Randomized Algorithm Native Algorithm Fig 9 Comparison between the proposed randomized algorithm and the native algorithm under input of Poisson distribution and  2    5 10 15 20 25 0 0.2 0.4 0.6 0.8 1 Delay window size Normalize Cost   Randomized Algorithm Native Algorithm Fig 10 Comparison between the proposed randomized algorithm and the native algorithm under input of Gaussian distribution and  2   E E Schadt M D Linderman J Sorenson L Lee and G P  Nolan Computational Solutions to Large-scale Data Management and Analysis Nat Rev Genet  vol 11 no 9 pp 647…657 Sep 2010  L Golubchik S Khuller  K  Mukherjee and Y  Y ao T o Send or not to Send Reducing the Cost of Data Transmission in Proc of IEEE INFOCOM  2013  L Zhang C W u  Z  Li C Guo M Chen and F  Lau Mo ving Big Data to The Cloud An Online Cost-Minimizing Approach IEEE Journal on Selected Areas in Communications  vol 31 no 12 pp 2710…2721 2013  H W ang H Xie L Qiu A Silberschatz and Y  Y ang Optimal ISP Subscription for Internet Multihoming Algorithm Design and Implication Analysis in Proc of IEEE INFOCOM  2005  S Peak Be yond Bandwidth The Business Case F o r Data Acceleration White Paper  2013  D K Goldenber g L Qiuy  H  Xie Y  R Y ang and Y  Zhang Optimizing Cost and Performance for Multihoming in Proc of ACM SIGCOMM  2004  A Grothe y and X Y ang T op-percentile T raf c Routing Problem by Dynamic Programming Optimization and Engineering  vol 12 pp 631…655 2011  F  Y ao A Demers and S Shenk er   A Scheduling Model for Reduced CPU Energy in Proc of IEEE FOCS  1995  N Bansal T  Kimbrel and K Pruhs Speed Scaling to Manage Ener gy and Temperature J ACM  vol 54 no 1 pp 3:1…3:39 Mar 2007  S Albers F  M  uller and S Schmelzer Speed Scaling on Parallel Processors in Proc of ACM SPAA  2007  B Bingham and M Greenstreet Ener gy Optimal Scheduling on Multiprocessors with Migration in Proc of IEEE ISPA  2008  E Angel E Bampis F  Kacem and D Letsios Speed Scaling on Parallel Processors with Migration in Euro-Par 2012 Parallel Processing  ser Lecture Notes in Computer Science C Kaklamanis T Papatheodorou and P Spirakis Eds Springer Berlin Heidelberg 2012 vol 7484 pp 128…140  G Greiner  T  Nonner  and A Souza The Bell is Ringing in Speedscaled Multiprocessor Scheduling in Proc of ACM SPAA  2009  M A Adnan Y  Ma R Sugihara and R Gupta Dynamic Deferral of Workload for Capacity Provisioning in Data Centers http://arxiv.org/abs/1109.3839  Y  Y ao L Huang A Sharma L Golubchik and M Neely  Data Centers Power Reduction A Two Time Scale Approach for Delay Tolerant Workloads in Proc of IEEE INFOCOM  2012  B Cho and I Gupta Ne w Algorithms for Planning Bulk T ransfer via Internet and Shipping Networks in Proc of IEEE ICDCS  2010  M Adler  R  K  Sitaraman and H V enkataramani  Algorithms for Optimizing the Bandwidth Cost of Content Delivery Comput Netw  vol 55 no 18 pp 4007…4020 Dec 2011  J Dean and S Ghema w at MapReduce Simpli“ed Data Processing on Large Clusters Commun ACM  vol 51 no 1 pp 107…113 Jan 2008  S Rao R Ramakrishnan A Silberstein M Ovsiannik o v  and D Reeves Sail“sh A Framework for Large Scale Data Processing Yahoo!Labs Tech Rep 2012  B Heintz A Chandra and R K Sitaraman Optimizing MapReduce for Highly Distributed Environments Department of Computer Science and Engineering University of Minnesota Tech Rep 2012  H Beck er and J Riordan The Arithmetic of Bell and Stirling numbers American journal of Mathematics  vol 70 no 2 pp 385…394 1948 IEEE INFOCOM 2014 - IEEE Conference on Computer Communications 2030 


212\212 t 007 n\t 212 212 005 327 212\212  arg 1  n x ett x x ExpxF   dse t EOL F us s use total 2 2 2 0 63 2 1 exp1  max min 14 b 007 327\212\327 327 212 5.0 1 63 s V Exp A A Ct dd ref p use 007 13  spacing fairly well. However, the sigma in the space distribution function could vary with different products, which is the most  uncertain source in this failure rate calculation method. Therefore, a careful evaluation of the peak failure rate spacing with different sigma values is extremely critical for reliability engineers. Carefully determining a MinIns to assure that product reliability can be kept away from a massive fallout condition is very useful  11   12   Figure 28: Probability associated TDDB concept  Figure 29: Combination of Vbd and TDDB big data to determine field acceleration factor with many t63 data points   Figure 30: Calculated failure rate based on Equations 9 and 10. All cases show a critical spacing with the highest failure rate  Figure 31: Examples showing the peak failure rate spacing is insensitive to sigma and mean change while peak failure rate is a strong function of both sigma and mean change On the other hand, the spacing may not be the only parameter to determine the overall TDDB. Based on Equation 13, it is well known that a Weibull CDF is a function of t63 or V63 and As a t63 or V63 distribution and an intrinsic  versus t63 or V63 relation can be exclusively determined by our proposed big data method discussed above, naturally a superposition based approach can be directly applied to calculate the realistic chip level failure rate without a bridge to the spacing. As t63 and V63 are determined by all relevant breakdown parameters, not just spacing, therefore this method should be a universal method for chip level failure rate calculation. The concept also can be applied to all reliability failure mechanisms including BEOL EM, BEOL SIV, and FEOL gate dielectric with either Lognormal or Weibull statistics. The total failure rate summing from all chips with different t63 or V63 values and correspondingly different probabilities over the probability density function could be described by Equations 14 and 15  3A.1.10    dxxFxPxF x x total 


003 003\003 t n\t 1 2 2 2 2 2 1 15  where n is the area scaling ratio, x could be either t63 or V63 k is the shape parameter and is the scale parameter of the Weibull distribution. For VRDB and TDDB, using P\(x\ in a Weibull format is recommended. As in Weibull, the distribution shape could be preserved after area scaling, which was supported by some experimental data. Also using P\(x\ in Weibull format generates a more conservative failure rate number as compared to P\(x\ in Normal format. However regardless of which P\(x\ format is used, generally, the failure rates obtained by Equations 13-15 are much smaller as compared to the numbers calculated by the traditional method Table 1 and Figure 30 illustrate one lowk TDDB with k 2.55 and 64nm pitch interconnect as an example. With our new proposed method, the summation of failure rates from chips with t63=0 hour to chips with t63=100000 hours is significantly smaller than the single failure rate produced by the traditional method. Therefore, the E max for a realistic reliability can be significantly lifted up  Table 1 Method  vs. t63 used for FR calculation t63 Failure Rate at End of Life A.U Traditional 0.62 No fixed 3050 New 1.69 Yes Varied 6.01     Figure 32: An example of calculated failure rate versus t63 based on Equations 12-14 for P\(x\ith a Weibull format   IV  D ISCUSSION AND C ONCLUSIONS  In conclusion, a new big data \(aggregation of stress data diagnostics data, simulation data, and yield data\eneration and analytics method is proposed to address MOL PC-CA and BEOL lowk TDDB challenges. With this new method without the introduction of any new TDDB acceleration model, reliability can be met with good confidence for various processes based on the square-root of E model. Since BEOL lowk TDDB and MOL PC-CA TDDB are so sensitive to processing and to structural layout, different processes and different structure layouts could potentially require different TDDB models. The breakdown mechanisms at different stress voltage regions could also be different. Therefore, unless we perform long-term TDDB stresses to validate a TDDB model at all situations such as material change at different technology nodes, different lowk used at different levels, and critical process changes, the question about the correct TDDB model would still exist from different stress voltage regions, from different test structure layouts, and from one technology node to another.  Alternatively, restoring a true Weibull slope by our data deconvolution is an easier way to improve overall TDDB projection. Without any long-term TDDB stresses, a real and improved failure rate projection can be quickly established. Furthermore, wafer processing can also be carefully diagnosed and meaningfully compared with our proposed big data generation and analytics method. A new diagnostics reliability concept is naturally embedded in our method. Therefore in addition to a simple \223pass\224 or \223fail\224 reliability judgment for qualification and process development, a precise reliability failure root cause analysis with a potential process fix guideline can also be provided by our method. As a consequence, a huge cost and time saving for new technology development and qualification can be achieved. Lastly, as mentioned above, our new method could be a prerequisite to developing a reliable TDDB acceleration model if significant die-to-die variation is present in our stress data A CKNOWLEDGMENT  The authors wish to acknowledge all the people working within IBM alliance programs for 32nm SOI and Bulk CMOS technology development. This work has been supported by the independent Bulk CMOS and SOI technology development projects at the IBM Microelectronics Division, Semiconductor Research & Development Center, Hopewell Junction, NY 12533 R EFERENCES  1 S  Y o ko g a w a S  U n o  I   K a to H   T s uch i y a T  S h im iz u an d M S a kam o to  49 th Annu. IEEE Rel. Phys Symp, 2011, pp. 149-154 2 F  C h e n  M  S h i n o s k y an d J. A i t k e n I EEE T r an s. o n El e c tr o n D e v i ce s v58, no 9, pp. 3089-3098, 2011   F e ll er  A n n   M a th Sta t   v 1 4  pp 389 40 0  1 943  4 A  T y ag i an d  M  A  B a y o u m i  I EEE T r an s o n Se mico n d u c to r  Manufacturing, v5, no 3, pp. 196-206, 1992 5 F  Ch e n e t  al I EEE I R PS 2 0 1 3  P I 1  1 1  5  6 K  Y  Y i a n g  e t al I EEE I R PS 2 0 1 0  p p  5 6 2 5 6 5      7 F  Ch e n e t  al I EEE I R PS 2 0 1 2  p p  6 A 4  1  6 A 4  9     212\212  212 k k mean x Exp xk xP or xx Exp xP    3A.1.11 212 


Figure 7  Portfolios of hosted payload and procured assets been presented in order to gain some level of con\002dence on the results that the tool outputs This discussion has focused on the validation of the scheduling algorithm by benchmarking it with operational schedules from the TDRSS Finally this paper has presented two case studies for future implementations of the TDRSS system Based on a demand forecast for the network a 002rst case study has considered the problem of selecting the frequency band to be supported and how to allocate them into the relay satellites It has been shown that maximum performance architectures require a mix of optical and RF payloads that support high throughput communications as well as reliable low data rate communications If the traditional procurement strategy is assumed 050NASA buys and operates the relay satellites\051 then monolithic architectures are preferable unless more than two high gain antennas render the resulting satellite con\002guration too complex In turn the second case study has extended this analysis by introducing architectures with hosted payloads It has been shown that according to the current available pricing model hosted payload architectures are clearly preferable than the traditional procurement strategy with cost savings between 15 to 30 for the same level of system performance It has then been discussed the advantages of having mixed procured and hosted payload architectures as a compromise to obtain networks with reduced lifecycle costs that can still address the requirements of highly sensitive and reliable applications Results have demonstrated that high data rate payloads 050speci\002cally optical payloads\051 are the best candidates to be hosted 050with savings up to 28%\051 thanks to their reduced mass and power requirement On the other hand low data rate communication payload should be allocated in privately owned satellites as they require bigger antennas and power ampli\002ers that increase the burden on the host platform Future Work The main streams of future work are as twofold On one hand additional features should be added to the model in order to better capture the complexity of the network con\002gurations 050e.g coupling between the costs of the communication payloads depending on the level of on-orbit processing the they perform\051 Additionally the size of the tradespace is currently limited to less than two thousand architectures due to computational limitations a stringent limitation given the possible combinations from the identi\002ed architectural decisions The solution currently under development is to include a genetic algorithm that alleviates this problem by iteratively generates new populations of architectures by combining the best previously evaluated networks On the other hand the other main stream of future work is related to exercising the tool in a variety of architectural decisions and mission scenarios In the presented case studies only geosynchronous constellations were considered although the tool allows comparing them with systems that place relays in MEO and LEO orbits Additionally the tool can also provide insight in valuing inter-satellite links and how the cost of putting in orbit their extra communication payloads can be leveraged by reducing the number of operating ground stations Moreover couplings between the payload allocation and fractionation strategy should also be further explored so as to understand if monolithic architectures still become preferable if relay satellites can be decomposed in clusters of independent antennas and payloads Finally supplementary what-if analysis can be conducted based on 0501\051 the demand forecast for the 2020-2030 time frame 0502\051 granted spectrum allocations to NASA and 0503\051 technology improvements that increase the spectral ef\002ciency of the communication payloads 12 


A PPENDIX Table 7  Acronyms Arch Architecture CER Cost Estimating Relationship DESDYNI Deformation Ecosystem Structure and Dynamics of Ice DSN Deep Space Network EIRP Equivalent Isotropically Radiated Power GEO Geosynchronous Orbit GRTG Guam Remote Ground Terminal GSFC Goddard Space Flight Center HD High De\002nition HP Hosted Payloads ISL Intersatellite Link ISS International Space Station LCC Life Cycle Cost LEO Low Earth Orbit MEO Medium Earth Orbit MIT Massachusetts Institute of Technology MOC Mission Operating Center MPCV Multi-Purpose Crew Vehicle NASA National Aeronautics and Space Administration NCCDS Network Control Center Data System NDA Non-Disclosure Agreement NEN Near Earth Network NISN NASA Integrated Services Network NOAA National Oceanic and Atmospheric Administration RF Radio Frequency SA Single Access SBRS Space based Relay Study SCaN Space Communication and Navigation SN Space Network STGT Second TDRSS Ground Terminal STK Systems ToolKit TDRSS Tracking and Data Relay Satellite System TT&C Telemetry Tracking and Command USGS United States Geological Survey WSGT White Sands Grount Terminal A CKNOWLEDGMENTS This project is funded by NASA under grant NNX11AR70G Special thanks for Gregory Heckler David Milliner and Catherine Barclay at NASA GSFC for their help getting the dataset and their feedback on our study R EFERENCES   National Aeronautics and Space Administration 223Space Communications and Navigation 050SCaN\051 Network Architecture De\002nition Document 050ADD\051 Volume 1  Executive Summary,\224 Tech Rep 2011   227\227 A v ailable http   www.nasa.gov/directorates/heo/scan   e a Sanchez Net Marc 223Exploring the architectural trade space of nasas space communication and navigation program,\224 in Aerospace Conference 2013 IEEE  2013   P Brown O  Eremenko 223Fractionated space architectures a vision for responsive space,\224 Tech Rep   S M V  D C E Teles J 223Overview of TDRSS,\224 Advances in Space Research  vol 16 pp 67\22676 1995   Analytical Graphics Inc A v ailable http   http://www.agi.com   D Selva Valero 223Rule-Based System Architecting of Earth Observation Satellite Systems by,\224 Ph.D dissertation Massachusetts Institute of Technology 2012   K D W  S P Davidson A 223Pricing a hosted payload,\224 in Aerospace Conference 2012 IEEE  2012   W J Larson and J R Wertz Space mission analysis and design  Microcosm Inc 1992   M Adinol\002 and A Cesta 223Contributed Paper Heuristic Scheduling of the DRS Communication System,\224 vol 8 1995   National Aeronautics and Space Administration Space Network Users Guide 050 SNUG 051  2007 no August 2007   e a Tran J J 223Evaluating cloud computing in the nasa desdyni ground data system,\224 in Proceedings of the 2nd International Workshop on Software Engineering for Cloud Computing  2011 B IOGRAPHY  Marc Sanchez Net is currently a second year M.S student in the department of Aeronautics and Astronautics at MIT His research interests include machine learning algorithms and rule-based expert systems and their suitability to the 002elds of system engineering and space communication networks Prior to his work at MIT Marc interned at Sener Ingenieria y Sistemas as a part of the team that develops and maintains FORAN a CAD/CAM/CAE commercial software for shipbuilding Marc received his degrees in both Industrial engineering and Telecommunications engineering in 2012 from Universitat Politecnica de Catalunya Barcelona Dr Daniel Selva received a PhD in Space Systems from MIT in 2012 and he is currently a post-doctoral associate in the department of Aeronautics and Astronautics at MIT and an adjunct Assistant Professor in the Sibley School of Mechanical and Aerospace Engineering at Cornell University His research interests focus on the application of multidisciplinary optimization and arti\002cial intelligence techniques to space systems engineering and architecture Prior to MIT Daniel worked for four years in Kourou 050French Guiana\051 as a member of the Ariane 5 Launch team Daniel has a dual background in electrical engineering 13 


and aeronautical engineering with degrees from Universitat Politecnica de Catalunya in Barcelona Spain and Supaero in Toulouse France He is a 2007 la Caixa fellow and received the Nortel Networks prize for academic excellence in 2002 Dr Bruce Cameron is a Lecturer in Engineering Systems at MIT and a consultant on platform strategies At MIT Dr Cameron ran the MIT Commonality study a 16 002rm investigation of platforming returns Dr Cameron's current clients include Fortune 500 002rms in high tech aerospace transportation and consumer goods Prior to MIT Bruce worked as an engagement manager at a management consultancy and as a system engineer at MDA Space Systems and has built hardware currently in orbit Dr Cameron received his undergraduate degree from the University of Toronto and graduate degrees from MIT Dr Edward F Crawley received an Sc.D in Aerospace Structures from MIT in 1981 His early research interests centered on structural dynamics aeroelasticity and the development of actively controlled and intelligent structures Recently Dr Crawleys research has focused on the domain of the architecture and design of complex systems From 1996 to 2003 he served as the Department Head of Aeronautics and Astronautics at MIT leading the strategic realignment of the department Dr Crawley is a Fellow of the AIAA and the Royal Aeronautical Society 050UK\051 and is a member of three national academies of engineering He is the author of numerous journal publications in the AIAA Journal the ASME Journal the Journal of Composite Materials and Acta Astronautica He received the NASA Public Service Medal Recently Prof Crawley was one of the ten members of the presidential committee led by Norman Augustine to study the future of human space\003ight in the US Bernard D Seery is the Assistant Director for Advanced Concepts in the Of\002ce of the Director at NASA's Goddard Space Flight Center 050GSFC\051 Responsibilities include assisting the Deputy Director for Science and Technology with development of new mission and measurement concepts strategic analysis strategy development and investment resources prioritization Prior assignments at NASA Headquarters included Deputy for Advanced Planning and Director of the Advanced Planning and Integration Of\002ce 050APIO\051 Division Director for Studies and Analysis in the Program Analysis and Evaluation 050PA&E\051 of\002ce and Deputy Associate Administrator 050DAA\051 in NASA's Code U Of\002ce of Biological and Physical Research 050OBPR\051 Previously Bernie was the Deputy Director of the Sciences and Exploration Directorate Code 600 at 050GSFC\051 Bernie graduated from Fair\002eld University in Connecticut in 1975 with a bachelors of science in physics with emphasis in nuclear physics He then attended the University of Arizona's School of Optical Sciences and obtained a masters degree in Optical Sciences specializing in nonlinear optical approaches to automated alignment and wavefront control of a large electrically-pumped CO2 laser fusion driver He completed all the course work for a PhD in Optical Sciences in 1979 with emphasis in laser physics and spectroscopy He has been a staff member in the Laser Fusion Division 050L1\051 at the Los Alamos National Laboratories 050LANL\051 managed by the University of California for the Department of Energy working on innovative infrared laser auto-alignment systems and infrared interferometry for target alignment for the HELIOS 10 kilojoule eight-beam carbon dioxide laser fusion system In 1979 he joined TRW's Space and Defense organization in Redondo Beach CA and designed and developed several high-power space lasers and sophisticated spacecraft electro-optics payloads He received the TRW Principal Investigators award for 8 consecutive years Dr Antonios A Seas is a Study Manager at the Advanced Concept and Formulation Of\002ce 050ACFO\051 of the NASA's Goddard Space Flight Center Prior to this assignment he was a member of the Lasers and Electro-Optics branch where he focused on optical communications and the development of laser systems for space applications Prior to joining NASA in 2005 he spent several years in the telecommunication industry developing long haul submarine 002ber optics systems and as an Assistant Professor at the Bronx Community College Antonios received his undergraduate and graduate degrees from the City College of New York and his doctoral degree from the Graduate Center of the City University of New York He is also a certi\002ed Project Management Professional 14 


 





 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


