Marcelo Veiga Neves C  esar A.F De Rose Kostas Katrinis Hubertus Franke 
Pontiìcal Catholic University of Rio Grande do Sul Porto Alegre Brazil Email marcelo.neves@acad.pucrs.br IBM Research  Ireland Dublin Ireland Email katrinisk@ie.ibm.com IBM TJ Watson Research Center Yorktown Heights NY US Email frankeh@us.ibm.com 
Pythia Faster Big Data in Motion through Predictive Software-Deìned Network Optimization at Runtime 
Abstract 
The rise of Internet of Things sensors social networking and mobile devices has led to an explosion of available data Gaining insights into this data has led to the area of Big Data analytics The MapReduce framework as implemented in Hadoop is one of the most popular frameworks for Big Data analysis To handle the ever-increasing data size Hadoop is a scalable framework that allows dedicated seemingly unbound numbers of servers to participate in the analytics process Response time of an analytics request is an important factor for time to value/insights While the compute and disk I/O requirements can be scaled with the number of servers scaling the system leads to increased network trafìc 
Arguably the communication-heavy phase of MapReduce contributes signiìcantly to the overall response time the problem is further aggravated if communication patterns are heavily skewed as is not uncommon in many MapReduce workloads In this paper we present a system that reduces the skew impact by transparently predicting data communication volume at runtime and mapping the many end-to-end ows among the various processes to the underlying network using emerging software-deìned networking technologies to avoid hotspots in the network Dependent on the network oversubscription ratio we demonstrate reduction in job completion time 
between 3 and 46 for popular MapReduce benchmarks like Sort and Nutch 
I I NTRODUCTION Driven by the tremendous adoption of electronic devices and the high penetration of broadband connectivity globally the generation of electronic data grows at an unprecedented rate In fact this rate is expected to steadily grow due to increasing adoption of trending data-heavy technologies arguably Internet of Things social networking and mobile computing The knowledge that can be extracted by processing this vast amount of data has sparked interest in 
Distributed computing Data communication Data processing 
Keywords 
building scalable commodity-hardware based and easy to program systems resulting today in a signiìcant number of purpose-built data-intensive analytics frameworks e.g MapReduce Dryad 2 and IBM Infosphere Streams 3 often captured by the market-coined term Big Data analytics The data-heavy nature of workloads run by Big Data analytics systems in conjunction with the need to scale-out to hundreds or even thousands of compute nodes for capacity speedup or capability immense input/scratch storage of the workload requiring proportionally high number of nodes reasons incurs high data-movement activity in the datacenter where such analytics systems are deployed For 
instance a recent analysis of MapReduce MR traces from Facebook revealed that 33 of the execution time of a large number of jobs is spent at the MapReduce phase that shufîes data between the various data-crunching nodes This creates an obvious incentive to optimize the communicationintensive part of such applications via appropriate dynamic network control Until recently the toolset for application-induced network control was limited to a small set of protocols e.g Quality of Service protocols that were embedded into network devices at manufacturing time unable to be dynamically changed to keep up with application needs Software-Deìned Networks SDN break this inîexibility 
offering ne-grained programmability of the network and high modularity to allow for directly interfacing application orchestration systems with the network Leveraging from this evolution in networking technology this paper presents Pythia 1  a system that employs real-time communication intent prediction for Hadoop and uses this predicti v e knowledge to optimize the datacenter network at runtime aiming at Hadoop MapReduce acceleration More speciìcally Pythia sports an instrumentation middleware that tracks transparently to applications and the Hadoop framework itself Hadoop map task processes at runtime and mines intermediate output data that will even 
tually be transferred to remote nodes for further processing during the reduction phase Prediction intelligence is collected at a central controller and in turn ingested by a chain of network control algorithms routing ow scheduling that optimize network resource allocation against faster Hadoop MapReduce shufîe phase completion and thus accelerate MapReduce job completion We prototyped the proposed scheme in a high-end cluster 1 According to Greek Mythology Pythia was an ancient Greek priestess at the Oracle in Delphi widely credited for her prophecies inspired by Apollo 
2014 IEEE 28th International Parallel & Distributed Processing Symposium 1530-2075/14 $31.00 © 2014 IEEE DOI 10.1109/IPDPS.2014.20 82 


and evaluated the performance improvement it brings to various HiBench benchmarks Our results manifest that Pythia achieves consistent acceleration of MapReduce workloads with speedup ranging from 3 to 46 depending on the network blocking ratio and the workload We also present results conìrming that our communication intent prediction approach and implementation is able to accurately in terms of shufîe ow size and timely within several seconds prior to actual ow occurrence in the network forecast MapReduce intermediate data movement The direct value driven by this work is in the performance improvement brought to Hadoop MapReduce by optimizing its communication-intensive phase via application-speciìc instrumentation and specialized network control Beyond this immediate contribution a more far-reaching anticipated impact of the present work lies in forming a concrete and tangible materialization of the value that the softwaredeìned concept can bring to the end-user by enabling tighter and constant application/workload afìnity to the IT infrastructure The rest of this paper is structured as follows We review and put our work in the context of related work in the Section VI Section II builds up on background concepts and technologies used in this paper and also drives the motivation for this work by uncovering open problems and discussing value potential Sections III and IV present the Hadoopspeciìc and network-related technical speciìcation of our system We present a quantitative evaluation of the beneìt of our system to Hadoop MapReduce workloads in Section V and conclude with summary and elevation of the ndings in Section VII II B ACKGROUND AND M OTIVATION Hadoop MapReduce is the open-source realization of Googleês MapReduce distrib uted data-processing framework It is gaining increasing traction and deployment base in various domains requiring batch-processing large-scale analytics primarily due to linear system scalability support for both structured and unstructured data sources a fairly straightforward programming model and transparent to the user runtime parallelism A remarkably versatile set of applications can be implemented by using the two primitive functions of the MR model namely and  The map function ingests input data records values and maps them to an application-speciìc key-space In turn the output intermediate map output in Hadoop terminology of the map function is sorted and fed to an application-speciìc reduce function e.g summation of numeric values From an implementation perspective Hadoop job execution is orchestrated by a two-level hierarchy of control entities the jobtracker which runs on the Hadoop clusterês master node and is the job-level control entity and one or more tasktrackers each running on every Hadoop compute node termed slave Among others a tasktrackerês role is to initiate/control map/reduce task processes that implement the applicationprovided map/reduce function Speciìcally a tasktracker starts a map task with a discrete chunk of data typically a distributed le system data block as input the sorted intermediate output of the map task is in turn hashed to the set of reducers which are also started by tasktrackers across the Hadoop cluster making sure that no two reducers process key,value pairs with the same key Following coordination steps taken between the tasktrackers and the jobtracker the tasktracker that a sample reducer is controlled by fetches from each nished map taskês tasktracker the set of key,value pairs that hash to the speciìc reducer-ID Seen from the map taskês tasktracker server the communication pattern corresponds to a shufîing of intermediate output data from the mapper server to the jobês reducer servers thereby being termed as the shufîe phase After a reducer task has fetched all data produced by the entire set of map tasks it proceeds to the nal reduce phase and then writes back the reduction result to a le stored in the distributed le system Given the increasingly high volumes of data that typical analytics applications ingest it is straightforward that the volume of data that need to be shufîed during a Hadoop job is in many cases relatively high thereby calling for solutions to shorten the duration of the shufîe phase In the rest of this section we motivate this through a MapReduce job execution example Figure 1a depicts the sequence diagram of the execution of a toy-sized sort job in a 1Gbps non-blocking network obtained by a custom visualization tool we have developed The job uses three map tasks slots and two reducers whereby the three distinct phases of interest in this paper are clearly annotated distributed le system phases are omitted for brevity It can be clearly observed that the networkheavy shufîe phase takes up a substantial fraction of job execution time motivating thus further work to optimize the network against the network-throughput intensive phase of MapReduce An additional observation that also motivates the type of work presented in this paper is the disproportionality of the intermediate output data sizes fetched by the two reducers speciìcally reducer-0 receives 5x times more data compared to reducer-1 This is not an uncommon problem in MapReduce executions job skew effect caused by non-uniform data distribution in the key space While this problem can be addressed at multiple levels e.g by dynamically adapting the partitioning function that governs the volume of data assigned to each reducer our work intends to address it at the system/network level Intuitively if reducer-0 receives ve times more data then it is straightforward that also the ows terminated at reducer-0 should get ve times more network capacity bandwidth than reducer-1 It is this end goal that motivates the applicationaware ow-level network control materialized in this paper Until recently the network in commodity deployments 
map reduce 
    
83 


key,value pairs from mapper-0 ow-1 and reducer-1 fetching key,value pairs from mapper-1 ow-2 is shown Due to the load-unawareness of ECMP-like ow allocation this candidate allocation leads to the adversarial effect of assigning a relatively large ow 159MB to a highly-loaded path 95 load Path-1 even if there is available network capacity to complete the shufîe transfer faster Note that this effect is not a side-effect of nominal network capacity i.e bad ow packing can lead to suboptimal network utilization even in non-blocking networks  Replacing ECMP with a load-a w are o w scheduling scheme e.g Hedera w ould to some e xtent a v oid such adversarial ow allocations however still not manage to unleash the entire optimization potential For instance in the example presented in Figure 1 schemes like Hedera would fail to recognize the criticality of ow-1 to MapReduce job progress and as such even if both ows are recognized and served as elephant ows the proportionality in the allocation of network resources in relation with application semantics and application state will be far from optimal A primary objective of this work is to address this gap and through that drive the value of software-deìned architectures to dataintensive distributed applications III P YTHIA A RCHITECTURE At the highest level of abstraction Pythia is a distributed system software with two primary cooperating components corresponding to the sensor/actuator paradigm a a Hadoop instrumentation middleware that runs on every Hadoop slave server or Virtual Machine and whose role is to predict future shufîe transfers at the level of mapper/reducer server or Virtual Machine pair during MR runtime and b an orchestration entity that ingests on a per job basis future shufîe communication intent events and optimizes the network during runtime aiming at reducing total job completion time In the rest of this paper we assume a baremetal Hadoop deployment for the sake of space and thus use the term slave server to refer to the operating-system 
    
a Hadoop sort job sequence diagram b Adversarial shufîe ow allocation to the network Figure 1 Motivational Hadoop job analysis and implications of conventional network control was from a control/management point operated as a blackbox offering very low capability of application-induced ne-grained control e.g controlling network policy at the granularity of a single ow Software-deìned networks SDN materialize the long-awaited decoupling between the control and data forwarding logic of network elements switches/routers moving the control-plane off the network elements and on to a centralized network controller where virtually any logic controlling network elements in any desired ways can be implemented in software In the context of Big Data applications software-deìned networks provide for the ability to program the network at runtime in a manner such that data movement is optimized against faster service-aware and more resilient application execution Still the transition to an application-controlled softwaredeìned infrastructure is not straightforward We make the case for this in Figure 1b where we depict a candidate execution of the job shown in Figure 1a on two racks within a wider datacenter In this example the network has two alternative paths between the two racks Path-1 and Path2 in Figure 1b and employs Equal Cost Multi-Pathing ECMP for o w allocation to multiple paths ECMP has been proposed as a solution in such environments mainly due to its simplicity and efìcient implementation on network hardware Figure 1b shows also utilization buffer occupancy at switch ports facing the datacenter network Given that ECMP employs random local hashing of ow packets to output ports at every network switch a possible allocation of two shufîe ows namely reducer-0 fetching 
84 


Figure 2 Pythia Architecture right-hand side and Control Software Block Diagram left-hand side level entity that hosts a distinct Hadoop tasktracker Still it is important to note that our solution is fully compatible with Hadoop deployments in virtualized cloud environments too The right-hand side of Figure 2 depicts the architecture of the system within a reference cluster/datacenter infrastructure comprising a set of server racks The Hadoop cluster is typically deployed in a subset of this server pool Intra-rack data communication e.g shufîing or HDFS block movement occurs via one or more edge switches Top of Rack ToR switches that all in-rack servers connect to whereas the data communication network provides for inter-rack data communication Pythia leverages the programmability offered by software-deìned networks SDN to achieve negrained timely and efìcient allocation of network resources to shufîe transfers Currently Pythia supports a datacenter network compatible with the standard protocol realization of the SDN concept namely OpenFlow Figure 2 shows also an additional management network that is physically distinct and typically of much lower bisection and cost to the data network interconnecting all devices servers switches This network is typically used for management/administration/provisioning purposes for out-of-band control-/management-plane communication between OpenFlow switches and the network controller and although not a prerequisite due to low network overhead incurred by our system cf Section V-C is also the physical network used to carry all control/monitoring trafìc incurred by Pythia to minimize disruption to application data trafìc At startup time Pythia initiates an instrumentation process at every server hosting a Hadoop tasktracker As conveyed by the respective block diagram on the left-hand side of Figure 2 the instrumentation middleware constantly monitors its local tasktracker for job/task progress activity while also providing for mapping of mapper/reducer identiìcation from Hadoop namespace to network location i.e resolution of IP address per map/reduce task ID Since Hadoop normally starts to schedule reducers only after a few mappers have been completed by default 5 it is expected that some ow intention detections will have unknown destinations To remedy this a collectorês thread monitors for reducer initialization events and lls these incomplete shufîe intention entries with reducer destination information as soon as the latter becomes available More importantly each instrumentation process tracks its local tasktracker for newly spawned map tasks At the event of a new map task creation the instrumentation process locates the local le system path where intermediate map task output will be spilled to and subscribes to the local le system service for receiving asynchronous notiìcations whenever new les are created under this path Per Hadoop workings intermediate output les are written to disk at map task completion time Whenever the notiìcation of such an incident is received by the instrumentation process i.e after a mapper has nished it decodes the le\(s containing the intermediate map output and calculates the size of key,value pairs that correspond and will be shufîed to each one of the jobês reducers Last it serializes the per-reducer predicted shufîe size in a message together 
  
85 


with the ID of the respective map task and transmits it to the Pythia collector server entity It is important to note that the Pythia prediction instrumentation middleware is  both to the Hadoop implementation as well as to applications running on top and thus can be seamlessly deployed on any existing Hadoop cluster The optimization and network programming part of Pythia is shown on the top left-hand side block diagram of Figure 2 logically comprising a prediction notiìcation collector multi-path ow routing algorithm logic and a targeted ow allocation or scheduling block As it will be extensively elaborated on in the next section all these modules work in tandem to respond to shufîe prediction notiìcations and optimize ow scheduling in a manner that leads to faster shufîe phase completion time and thus to job acceleration Currently all of the SDN network control logic of Pythia is implemented in the form of modular components within an alliance OpenFlow controller project namely OpenDaylight  IV N ETWORK S CHEDULING In this section we describe the functionality and implementation of the Pythia network scheduling module Essentially the module ingests information about the physical network topology the current link-level network utilization and the application communication intention and computes an optimized allocation of ows to network paths such that shufîe transfer times are reduced as a last step it maps the logical ow allocation to the physical topology and installs the proper sequence of forwarding rules on the switches comprising the data network The network scheduling module is implemented as a plugin module within OpenDaylight a community-led industry-supported open source OpenFlow controller framework Internally the network scheduling module consists of a Hadoop MapReduce runtime collector and a ow allocation module The Hadoop Runtime collector is responsible for receiving and aggregating the application-level information collected by each serverês monitor cf Section III In addition the collector aggregates all ows that emanate from a distinct server mapper and are terminated to a distinct reducer server into a single ow entry that sums up the ow sizes of its constituents ows Ideally one would prefer to use the classical ve-tuple deìnition of an application ow  source-address,destination-address,sourceport,destination-port,protocol-type  to create OpenFlow forwarding entries during network programming However a Hadoop shufîe owês TCP destination port number cannot be determined in advance during prediction time since it is only assigned by the sourcing server as soon as the ow starts i.e socket bind Therefore ow aggregation proves necessary The hypothetical limitation of this is that it cancels the ability to apply differentiated network scheduling for reducer tasks running on the same server In practice and throughout our experimentation we have not identiìed the criticality of supporting such a feature as a performance booster On the positive side having a module supporting ow aggregation adds future exibility to the Pythia system particularly with regard to forwarding state conservation in softwared-deìned networks SDN Given the high cost and thus limited size of the memory part of network devices storing so called wildcard rules as is the case with fouror ve-tuple rules lar ge-scale future SDN netw ork setups may force routing at the level of server aggregations e.g racks or sets of racks-PODs Pythia can easily respond to such a requirement by populating the ow aggregation module with server location-awareness and an appropriate aggregation policy that maps ows to rackor POD-pairs The ow allocation module implements both routing and ow allocation algorithms During startup it ingests topology events from OpenDaylight and generates a routing graph that represents the underlying multi-path network topology Also during initialization it computes the k-shortest paths among all server pairs in the network graph where leaf vertices intermediate vertices and edges represent servers network switches and network links respectively The kshortest path implementation uses hop-count as the distance metric This module relies on the OpenDaylight topology update service to recompute the routing graph only when a change occurs in the physical network topology By doing so and given that the k-shortest-path implementation that uses successive calls to the Dijkstra shortest-path algorithm is O for small  we are able to keep the routing computation overhead off the data path Moreover it provides fault tolerance since the routing graph is updated at the event of link or switch failure The problem of optimally distributing ows among the available paths in a multi-path network to satisfy trafìc demands is normally referred to as Multi-Commodity Flow problem which is known to be NP-complete for integer/unsplittable ows Ho we v er  there are a number of practical heuristics for simultaneous ow routing that can be applied to typical datacenter network topologies In this work we used a rst-ìt bin-packing heuristic to jointly allocate sets of predicted shufîe transfer ows to available paths Our heuristic combines the link utilization information provided by the OpenDaylight link load update service with the communication intention information collected by our Pythia monitor to distribute the ows among the available paths It also employs the knowledge of the application-level transfers to differentiate the portion of the network load that is due to shufîe transfers from background trafìc due to over-subscription As such it is possible to determine the amount of available bandwidth along a given path Finally the aggregated ows are assigned to the path that has the highest available bandwidth We note that our design is modular enough to support further ow scheduling algorithms the latter forms also part of our on-going work in this space 
transparent 
  N k 
 
3 
86 


The Pythia ow module handles only ows that are part of communication prediction for applications subscribed to the Pythia collector module The rest of the datacenter trafìc is handled through default datacenter network control processes In this paper we assume that ows other than those handled by Pythia are allocated to the available k-shortest paths via an ECMP-like EqualCost Multi-Path scheme According to ECMP all packets belonging to a distinct ow are hashed to the same output port and thus path at every intermediate network device thus resembling a random load-unaware ow allocation scheme Our current ECMP implementation uses the ve-tuple  source-address,destination-address,sourceport,destination-port,protocol-type  to compute a ow hash and assigns a path to a ow based on a modulus computation on the ow hash value and the number of available paths in the routing graph V E VALUATION R ESULTS Our experimental setup consists of 10 identical servers each equipped with 12 x86 64 cores 128 GB of RAM and a single SATA disk The servers are organized in two racks 5 servers per rack interconnected by two OpenFlow enabled Top-of-Rack ToR switches IBM G8264 RackSwitch with two links between them A distinct server runs an instance of the OpenDaylight network controller and is directly connected to each of the ToR switches through a management network as described in Section III In terms of software all servers run Hadoop 1.1.2 installed on top of Red Hat Enterprise Linux 6.2 operating system Since our setup has only a single HDD disk per server with measured serial read rate of 130MBytes/sec and multiple cores accessing it in parallel we decided to conìgure Hadoop to store its intermediate data in memory Otherwise Hadoop would operate in a host-bound range i.e disk I/O rate would be the bottleneck thus resulting in the setup being indifferent to any improvement brought to the network by Pythia Having a balance between CPU memory I/O and network throughput is common in production-grade Hadoop clusters and therefore following the above practice is justiìed in the absence of Hadoop servers with arrays of multiple disks As described in Section II a reducer task does not start its processing phase until all data produced by the entire set of map tasks have been successfully fetched Therefore the shufîe phase represents an implicit barrier that depends directly on the performance of individual ows Thus even a single ow being forwarded through a congested path during the shufîe phase may delay the overall job completion time In order to demonstrate the ability of Pythia to choose good paths to shufîe transfer Figure 3 Nutch job completion times using Pythia resp ECMP and relative speedup ows and accelerate MapReduce applications we conducted a number of experiments evaluating job completion times under different network over-subscription ratios We used ECMP as baseline since it has been used as the ow allocation algorithm in multi-path datacenter networks The various over-subscription ratios we experimented with are simulated by populating the network links with background trafìc speciìcally using the iperf tool to generate constant bit rate UDP streams We chose two benchmarks from the HiBench benchmark suite that are kno wn to be netw ork-intensi v e sort and Nutch indexing The sort application is one of the examples that are provided by the Hadoop distribution It is widely used as baseline to Hadoop performance evaluations and is representative of a large subset of real-world MapReduce applications e.g data transformation We conìgured sort to use an input data size of 240GB The Nutch indexing application is part of Apache Nutch a popular open source web crawling/indexing software project and is representative of one of the most signiìcant uses of MapReduce large-scale search indexing systems We conìgured Nutch to index 5M pages amounting to a total input data size of 8GB Figure 3 depicts Nutch job completion times using Pythia and ECMP respectively and the relative speedup Times are reported in seconds and represent the average of multiple executions As can be observed Pythia outperforms ECMP for different over-subscriptions ratios The maximum speedup was obtained for the 1:20 over-subscription ratio case where Pythia improved job performance by 46 It is worth noting that job completion times for Nutch using Pythia do not signiìcantly increase by handing more network capacity to Hadoop and are comparable to the respective job completion time measured in a network without over-subscription 242 seconds in our setup This indicates that the Pythia ow allocation algorithm coupled with early ow size knowledge manages to almost optimally assign the maximum capacity that Hadoop MapReduce needs 
  
A Experimental Setup B Job Speedup de facto 
002 
87 


Figure 4 Sort job completion times using Pythia resp ECMP and relative speedup Figure 4 shows the results for the sort application Unlike Nutch sort jobs running over Pythia are not able to maintain similar job completion times over different oversubscriptions ratios We believe this is due to the individual shufîe ow characteristics particularly because the smaller ows created by Nutch increase the opportunity for optimization However Pythia is still able to outperform ECMP for different over-subscription ratios with a improvement of up to 43 Given the value of the communication intent prediction middleware as a standalone component that could also be used in multiple other runtime optimizations of the Hadoop infrastructure beyond network scheduling e.g storage or early skew prediction we elaborate here on the timeliness and ow size accuracy of the prediction middleware To evaluate these metrics for the prediction middleware we deployed NetFlow network monitoring probes across all servers comprising our testbed prototype together with a NetFlow collector at a server that was connected to all servers comprising the Pythia prototype Clocks on all servers in this setup were synchronized to 100ms accuracy We then run multiple runs using various benchmarks capturing both a cumulative trafìc volume over time that each Hadoop server sourced towards other Hadoop server nodes as predicted by Pythia and b all shufîe ow trafìc port 50060 exchanged between pairs of Hadoop servers in our setup using the NetFlow monitoring system In addition we post-processed the NetFlow traces to obtain cumulative per-server sourced shufîe ow volume compatible with the measurements we obtained from the Pythia predictor Figure 5 plots the outcome of this analysis for a single server sourcing shufîe trafìc to the various reducers Server4 We observe that there is a substantial distance between the predicted and the measured trafìc curves approximately 9sec at minimum which effectively translates to Pythia being able to predict the trafìc volume that will exit a Hadoop server well in advance of the time that the actual trafìc will Figure 5 Prediction promptness/accuracy over time for trafìc emanating from a single Hadoop tasktracker server 60GB integer sort job start entering the network This nding was consistent across all servers and throughout our experimentation with other workloads Generally the timeliness of Pythia prediction was found to be operating in a safe margin relative to the time budget that contemporary networking hardware allows for programming the network at runtime typically in the order of 3-5ms/îow installed Intuitively the timeliness of prediction depends on the time gap between a map task nish event and the event of a reducer task starting to fetch data from the nished mapper Given that Hadoop limits the number of parallel transfers that each reducer can initiate at every instance of time especially in larger-scale setups we expect the above time gap affecting prediction timeliness not to be sensitive to Hadoop conìguration parameter setup We are currently working on modeling the problem using relevant Hadoop parameters as input and designing experiments to conìrm this insensitivity as part of our on-going work Pertaining to accuracy in predicting trafìc volume Pythia was always able to never lag the actual trafìc measurement trace in terms of cumulative trafìc volume sourced As seen in Figure 5 Pythia is over-estimating trafìc volume by a factor of 3%-7 for a single server While we donêt expect this churn to be detrimental or lead to measurable overcommitment of network resources we believe that it has its source in the accuracy of how Pythia computes the protocol overhead that it adds to the shufîe ow prediction volumes collected from Hadoop servers the instrumentation process works at the application-layer and therefore the protocol overhead that needs to be added to predict the ow volume as it will be seen on the wire is computed based on known protocol header sizes Last we report on the overhead induced by the instrumentation middleware Based on preliminary measurements per Hadoop server average CPU and I/O overhead ranged from 2 to 5 while memory occupancy overhead was 
C Prediction Efìcacy and Overhead 
88 


insigniìcant given our commodity server conìguration Intuitively overhead comprises a constant dc factor stemming from continuous monitoring of MapReduce task progress and a spike factor stemming from index le analysis at the event of a map task nish Recognizing that the instrumentation overhead characterization merits further study we plan to address it in our follow-up work together with characterizing the network overhead incurred by Pythia prediction notiìcation messages VI R ELATED W ORK Due to the inherent nature of data-intensive distributed analytics frameworks to move large volumes of data between application server nodes recent research work in the area has started focusing on optimizing against network bottlenecks e.g TCP Incast that may impede optimal performance of such workloads Camdoop manages to reduce the volume of data shufîed in MapReduce jobs by employing in-network combiners Similarly Yu et al mo v e parts of the merge phase into the network thus de-serializing the map from the reduce phase and bringing substantial improvement due to increased parallelism in phase execution Both approaches can act complementary to our work and even more beneìt from the advance knowledge of future shufîe ows that Pythia provides for Big data applications are among the obvious beneìciaries of the ne degree of programmability that the software-deìned infrastructure movement brings along Focusing on the data-movement part Wang et al identify various opportunities for runtime network optimization to accelerate MapReduce jobs potentially using an appropriate abstraction framework to interface applications with the infrastructure such as Coîow or MR Orchestrator 22 Orchestra is a v ersatile frame w ork sho wcasing the v alue of network-awareness e.g network-state aware scheduling of sets of interdependent ows and/or network-optimized execution e.g by dynamically manipulating ow rates to big data movement patterns shufîe broadcast However Orchestra requires explicit support from the big data framework it optimizes e.g Hadoop and thus unlike Pythia cannot be used without reworking the design and implementation of the application framework Still should Hadoop reach a level that it interfaces with dynamic infrastructure orchestration frameworks like Orchestra the integration of our system as a sub-component of such frameworks is rather straightforward Among all related work in the eld FlowComb is a framework with signiìcant overlap with our system it employs shufîe-phase communication intention to apply intelligent ahead-of-îow-occurence network optimization towards MapReduce performance improvement In fact the rst public communication on FlowComb occured while we were already in the process of developing our prototype Next though to the similarities there are also subtle differences network optimization ow scheduling in FlowComb does not leverage application intelligence except from predicted ow volumes even if the use-case driving the work grants access to such information Instead Pythia draws from past manifestations about the v alue in taking o w criticality into consideration therefore incorporating ow priority as a criterion in network optimization in addition to ow sizes and network topology/state At the engineering level our implementation of predicting ows based on deep Hadoop index/sequence le analysis results in more timely prediction compared to the results communicated by FlowComb Last while recognizing that reports on on-going w ork the testbed used for the evaluation of FlowComb used only a single network over-subscription ratio 1:10 for 1Gbps server NICs and was likely to exhibit high-latency due to using software switches In this setting it is hard to assess how FlowComb will perform in higher-capacity production grade datacenter networks Nevertheless there is great value in the FlowComb work and its recent appearance strengthens the argument for the timeliness and relevance of the research reported in this paper VII C ONCLUSIONS This paper proposed Pythia a system that improves the performance of Hadoop MapReduce jobs through runtime communication intent prediction and ne-grained control of the underlying datacenter network After motivating the relevance and need for the work presented herein through analysis of exemplary executions of Hadoop jobs and treatment of the incurred ows thereof by the network we outlined the architecture of our system and elaborated in the functionality and algorithms embedded into its constituent components A good portion of the work has been invested in developing a system prototype within a downsized datacenter located in our lab We presented quantitative results of our Pythia prototype obtained in a 2-rack testbed setup and using hardware OpenFlow switches and various MapReduce workloads as input Our evaluation manifests that Pythia achieves signiìcant acceleration of the Hadoop workloads under test up to 46 bringing an improvement that varies depending on network capacity available to Hadoop and the speciìcities of the workload Given the value of our prediction middleware as a standalone component we also presented evaluation results manifesting its superior ability to timely predict ows well in advance prior to their occurrence in the network together with its ability to predict ow sizes fairly accurately The contribution of the present work to faster Big Data analytics and thus reduced time-to-insight through acceleration of the Hadoop analytics framework is profound And while most of the system work on Hadoop has focused on improving other parts of the framework e.g job scheduling partitioning or the underlying infrastructure e.g compute 
89 


resource allocation we show through this work that there is great potential and value in optimizing large-scale analytics runtimes against the underlying network From a more elevated perspective we rate the present work as a tangible value case supporting the realization of largescale distributed computing as a programmable stack in accordance with the software-deìned argument R EFERENCES  J Dean and S Ghema w at Mapreduce simpliìed data processing on large clusters in 
 ser OSDIê04 Berkeley CA USA USENIX Association 2004 pp 10Ö10  M Isard M Budiu Y  Y u A Birrell and D Fetterly Dryad distributed data-parallel programs from sequential building blocks in  ser EuroSys 07 New York NY USA ACM 2007 pp 59Ö72 A v ailable http://doi.acm.org/10.1145/1272996.1273005  IBM P  Zik opoulos and C Eaton  1st ed McGraw-Hill Osborne Media 2011  M Cho wdhury  M Zaharia J Ma M I Jordan and I Stoica Managing data transfers in computer clusters with orchestra  vol 41 no 4 p 98 Oct 2011  Apache Softw are F oundation 2013 Hadoop Online Available http://hadoop.apache.org  S Huang J Huang J Dai T  Xie and B Huang The hibench benchmark suite Characterization of the mapreduce-based data analysis in  2010 pp 41Ö51  Y  Kw on M Balazinska B Ho we and J Rolia  A study of skew in mapreduce applications in  2011  C Hopps 2000 Analysis of an Equal-Cost MultiPath Algorithm RFC 2992 IETF A v ailable http://tools.ietf.org/html/rfc2992.html  A Greenber g J R Hamilton N Jain S Kandula C Kim P Lahiri D A Maltz P Patel and S Sengupta Vl2 a scalable and exible data center network in  New York NY USA ACM 2009 pp 51Ö62  M Al-F ares S Radhakrishnan B Ragha v an N Huang and A Vahdat Hedera dynamic ow scheduling for data center networks in  ser NSDIê10 Berkeley CA USA USENIX Association 2010 p 1919 A v ailable http://dl.acm.org/citation.cfm?id=1855711.1855730  2009 OpenFlo w switch speciìcation v ersion 1.0.0 Online Available http://archive.openîow.org/documents/openîowspec-v1.0.0.pdf  T  White  OêReilly 2011  Linux F oundation 2013 Opendaylight collaborati v e project  A v ailable http://www opendaylight.or g  R T restian G.-M Muntean and K Katrinis Micetrap Scalable trafìc engineering of datacenter mice ows using openîow in  2013 pp 904 907  S Ev en A Itai and A Shamir  On the Comple xity of T ime Table and Multi-Commodity Flow Problems in  Washington DC USA IEEE Computer Society 1975 pp 184Ö193  Apache Softw are F oundation 2013 Apache nutch Online Available http://nutch.apache.org  Y  Chen R Grif t D Zats and R H Katz Understanding tcp incast and its implications for big data workloads EECS Department University of California Berkeley Tech Rep UCB/EECS-2012-40 Apr 2012 A v ailable http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS2012-40.html  P  Costa A Donnelly  A Ro wstron and G OêShea Camdoop exploiting in-network aggregation for big data applications in  ser NSDIê12 Berkeley CA USA USENIX Association 2012  W  Y u Y  W ang and X Que Design and e v aluation of network-levitated merge for hadoop acceleration  vol PP no 99 2013  G W ang T  E Ng and A Shaikh Programming your network at run-time for big data applications in  ACM Press 2012 p 103 A v ailable http://dl.acm.org/citation.cfm?doid=2342441.2342462  M Cho wdhury and I Stoica Coîo w a netw orking abstraction for cluster applications in  ACM Press Oct 2012 pp 31Ö36  B Sharma R Prabhakar  S.-H Lim M T  Kandemir  and C R Das MROrchestrator a ne-grained resource orchestration framework for MapReduce clusters in  Honolulu HI US IEEE Jun 2012 pp 1Ö8  A Das C Lumezanu Y  Zhang V  Singh and G Jiang Transparent and exible network management for big data processing in the cloud in  San Jose CA US USENIX Jun 2013 
Proceedings of the 6th conference on Symposium on Opearting Systems Design  Implementation Volume 6 Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems 2007 Understanding Big Data Analytics for Enterprise Class Hadoop and Streaming Data ACM SIGCOMM Computer Communication Review Data Engineering Workshops ICDEW 2010 IEEE 26th International Conference on 5th Open Cirrus Summit Proceedings of the ACM SIGCOMM 2009 conference on Data communication Proceedings of the 7th USENIX conference on Networked systems design and implementation Hadoop The Deìnitive Guide 2 ed Integrated Network Management IM 2013 2013 IFIP/IEEE International Symposium on Proceedings of the 16th Annual Symposium on Foundations of Computer Science Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation Parallel and Distributed Systems IEEE Transactions on Proceedings of the rst workshop on Hot topics in software deìned networks HotSDN 12 Eleventh ACM Workshop on Hot Topics in Networks HotNets-XI IEEE 5th International Conference on Cloud Computing CLOUD 5th USENIX Workshop on Hot Topics in Cloud Computing HotCloud 13 
90 


REQUIREMENTS  AND  TARGET  AUDIENCE Knowledge of LexisNexisí High Performan ce Computing Cluster \(HPCC\not required \(the HPCC serves as Smart Viewís backend  Anyone who is tasked with or has an interest in entity reso lution as it pertains to big data is encouraged to attend  DEMO  DURATION The demo will be presented in a 30-minute session  A/V AND EQUIPMENT Personal computer and a projector  PRESENTERíS  BIOGRAPHY Mary Galvin has a decade of professional expertise in the software industry serving in a variety of technical & management roles on pr oducts and projects. Mary s technical domain expertise encompasses both huma n language technology \(HLT as well as data intensive processing systems such as LexisNexisí Hi gh Performance Computing Cluster \(HPCC  Mary earned her bachelorís degree in comput er engineering from Villanova University in 2003, where she also received minors in comput er science, Spanish, and naval science. In addition to being proficient in Spanish, she has a strong knowledge of the Arabic and Chinese languages. Mary has been certified as a Project Management Professional \(PMP Management Institute \(PMI   


The 2014 International Conference on Collabora tion Technologies and Systems \(CTS 2014 May 19 - 23, 2014, Minneapolis, Minnesota, USA   CTS 2014 DOCTORAL DISSERTATION COLLOQUIUM   A Federated Cloud of Things for Emergency Management Gilberto Taccari Adviser Luca Spalazzi Universit‡ Politecnica delle Marche, Ancona, Italy   Designing Mobile Interactions for Athletes Susanne Koch Stigberg Advisers Hilda Tellioglu and Steinar Kristoffersen Vienna University of Technology, Austria and ÿs tfold University College, Halden, Norway       


The 2014 International Conference on Collabora tion Technologies and Systems \(CTS 2014 May 19 - 23, 2014, Minneapolis, Minnesota, USA   CTS 2014 POSTER PAPERS AND POSTERS Partial List   POSTER PAPERS  Information Exchange and Fusion in a Collaborative Environment using Semantic Information Requirements Joshua M. Powers, Keith D. Shapiro, David S. Monk Securboration Inc., Florida, USA; C5T Corporation, Illinois, USA  An Effective Approach Using Eco-feedback to Motivate Energy Conservation Behaviors Hieu Huynh Chi stfold University College, Halden, Norway  Saving Energy with Pleasure: Designing EnergyRace Web Application Ksenia Dmitrieva stfold University College, Halden, Norway  Promoting Energy Efficient Behavior through Energy-Related Feedback Que Tran stfold University College, Halden, Norway  Collaborative System to Investigate Mental Models the Information Architecture Automatic Tool \(IAAT Cristina Olaverri-Monreal, Joel GonÁalves Technische Universit‰t M¸nchen, Germany  A New Scheme to Evaluate the Accuracy of Knowledge Representation in Automated Breast Cancer Diagnosis Juan Shan, Lin Li Pace University, New York, USA; Mu rray State University, Kentucky, USA  Disaster Relief Management and Resilience Using Photovoltaic Energy Salahuddin Qazi, William Young State University of New York Institute of Technology New York, USA; SUN Tree Consulting, Florida, USA   TECHNICAL POSTERS  From Microscope to Computer: Using Facebook to Assist Medical Laboratory Scientists in Nigeria Access and Navigate eLearning Courses Jarret Cassaniti, Nandini Jayarajan, Rebecca Shore, Simone Parrish, Lisa Mwaikambo Johns Hopkins Bloomberg School of Public Health Center for Communication Programs, Maryland, USA  Semantic Search & Integration to Climate Data Ranjeet Devarakonda, Giriprakash Palani samy, Line C. Pouchard, Biva Shrestha Oak Ridge National Laboratory, Tennessee, USA  


Triggering Creativity through Semantic Cross-domain Web Crawling and Routing Francesco Taglino, Fabrizio Smith Institute for System Analysis and Co mputer Science, CNR, Rome, Italy  Integration of Emerging Learning Technologies in Secondary Schools: A Burkina Faso Case Study Romaric R. Zongo University of Minnesota ñ Twin Cities, Minnesota, USA  Visualization of Energy Consumption: Motivating fo r a Sustainable Behaviour Through Social Media Caroline So  e Olsen stfold University College, Halden, Norway   INDUSTRY POSTERS  Annotation and Deep Search of Se mi-Structured Technical Documents Chris Macks Progeny Systems Corporation, Virginia, USA   


  Paper 2581 Version 2  14   Flight i nformation and processing   Presentation and projection of flight profiles   Real time operational information   Managing both pilot operator and air traffic controller workloads   Procedural processes supported by enhanced automation capabilities   Control sector complexity and management   Separation a ssurance   Airspace d esign and p rocedures   Investment in resolving these constraints  will drive the rate of adoption in civil UAS operations in the NAS One p art of the investment will be driven by the FAA\222s \223F acilities and Equipment\224 F&E Budget over  the next ten years Another part will be driven by defense and federal investment  in R&D  The remaining part will be driven by private investment seeking to develop profitable supplie r s and manufacturing firms of UAS Unfortunately   private investment will not fully launch until there is a clear indication of the public investment into implementing the requisite air traffic management  and air traffic control  ATM/ATC changes into the NAS infrastructure and there i s a common  approach through avionic standards in UA V  equipage   It is expected that avionic standards would lower the end user costs The impact on NAS infrastructure is the outline in the following subsection  Today\222s Air Traffic Control System  Today\222s air traffic control system is built around four basic pillars. These are communications, navigation, surveillance and infrastructure. The mortar holding these pillars together is aviation policy, rules and procedures  The FAA Air Traffic Organizat ion \(ATO\ has identified over 25 FAA procedural orders and notices that will need to be revised to accommodate routine  UAS operations in the NAS. This does not include advisory and informational materials that need to be developed and briefed to air traffi c controllers, pilots and other aviation stakeholders Additionally NAS investments to accommodate UAS efforts and associated costs must extend  beyond training requirements and procedural developments to include modification of infrastructure to permit an d to optimize efficiency and safety of UAS operations in the NAS  The majority of UAS operating in the NAS today are predominantly operated by the DoD. They were not designed with NAS compatibility in mind but rather to meet military mission needs. It is e xpected future commercial UAS will be designed and operated much more along the lines of manned aircraft. For example, current DoD UAS do not have Flight Management Systems and associated NAS aeronautical database and cannot be program m ed to fly approved F AA instrument procedures such as an instrument landing system \(ILS\ approach. Future commercial UAS operating in the NAS will need have integrated avionics systems and NAS capabilities similar to those of manned aircraft. However, unique challenges associa ted with UAS will remain and drive functional changes across many NAS project and programs. Some of these anticipated drivers or changes are listed by FAA program area below  En Route Automation Modernization \(ERAM 227 ERAM will be impacted by the need to be able to accept and process and display lengthy and complex UAS flight plans that include automated flight profile contingencies to the air traffic controller. This includes the ability to display a 20 minute planned route of flight on a controller\222s displa y suite  ERAM needs to be able to display to review to modify and to approve a UAS flight plan request originating in an en route center airspace environment as a 24 hour file and fly replacement to the current constrained  COA  process now is in use  Nati onal Voice System \(NVS  227 Future air traffic control voice switching systems must be able to provide timely and direct pilot to controller voice and data e.g text messaging\ communications via Voice Over Internet Protocol \(V o IP  Terminal Automation Mode rnization and Replacement TAMR  227 TAMR needs to be able to display to review to modify and to approve a UAS flight plan request originating in a terminal airspace environment as a 24 hour file and fly replacement to the current constrained COA  process now is use. TAMR also needs to be able to surveil approved UAS flights within the terminal environment  Terminal Information Display System \(IDS\ and En Route Information Display System \(ERIDS  227 Information support systems need to be able to inges t to and  to rapidly display unique local procedures and associated graphics associated with UAS operations. These include pilot communications and UAV control lost ink procedures  System Wide Information Management \(SWIM  227 SWIM  must be able to rapidly mov e critical UAS information around the NAS to provide the right information to the right controller or pilot at the right time and in the desired  format  Flight Information Exchange Model \(FIXM  227 UAS flight information needs to move to and from other Air Navigation Service Providers \(ANSP\ throughout the world and new data terminology must be created that supports the collection, processing and dissemination of UAS movements through the airspace  Air Traffic Control Optimum Training Solutions \(ATCOTS  227 UAS provides new training challenges for FAA in creating new teaching curricula that contains new procedures and paradigms used to accommodate UAS safely and efficiently in the NAS. This is notable c hallenge given 


  Paper 2581 Version 2  15  the diversity and dynamic of evolving UAS technologies and missions  Future Facilities 227 FAA real estate assets may likely be affected as new UAS related systems are development and deployed. UAS NAS integration may drive additional FAA manpow er needs for positions of operations, coordination and management. This may require additional physical footprints to accommodate new positions of operations personnel and their supporting infrastructure  Next Generation Weather Processor \(NWP\ and Weathe r and Radar Processor \(WARP  227 The removal of a pilot in the cockpit also removes the ability to \223see\224 from a traditional pilot\222s perspective. Not only does this relate to the need for electronic systems for detect and avoid \(i.e., the FAR reference 91.113 for see and avoid\ but also affects many aspects of flight safety associated with detection and avoidance of hazardous weather from visual perspective of the manned cockpit. Determination of flight visibility is one such factor. It should also be noted tha t many of the UAS flying in the NAS today have a very low tolerance to hazardous weather such as turbulence and icing  Detect and Avoid \(DAA  227  Depending upon architecture and functionality, detect and avoid for UAS could have a profound impact on a variety of FAA systems and infrastructure components. DAA can be airborne based, and affect Automatic Dependent Surveillance Broadcast \(ADS B\ In and Out and an associated Cockpit Display of Traffic Information \(CDTI\nd Traffic Alert and Collision Avoida nce System \(TCAS\ functionality, including designs for the new Next Generation Airborne Collision Avoidance Systems \(ACAS X\. GBSAA that leverages ASR 11 Surveillance Radars and Standard Terminal Automation Replacement Systems \(STARS\ or TAMR also demands automation and surveillance system modification to provide this capability. Additional future work on portable primary three dimensional \(3D\ radars integrated in the NAS will also demand close analysis  The previous overview of FAA air traffic control sys tems gives insight in the investment required to be undertaken within the F AA\222s F&E  budget It also s erve s to illustrate the pervasiveness of changes that the UAS will eventually have on air traffic control and its supporting  ATM  infrastructure   It is impo rtant to recognize these ATM/ATC changes need to be planned for, funded and deployed. It must also be recognized that these UAS related investments do not stand alone in that these changes must be done while other planned ATC system improvements are taking  place I nvestments   such as   the Surveillance Broadcast System Program  National Voice Switch, Future Cyber Security Data Communications, and others are planned under the Next Generation Air Transport System  NextGen and others  Obviously   if future UA S forecasts are to be sustained it will entail a major shift in current FAA investment in addition to the  currently planned funding of NextGen  The forecasts outlined in this paper  will require an increase in funding levels of between $500 million and $1 billion dollars per year over and above current FAA requested F&E funding levels   Doing so will greatly facilitate realization of the $30 billion per year UAS market  9   S UMMARY  From an aviation perspective, UAS represents a new and disruptive technology  challenging the staid institutions policy, procedures and technologies that exist today and have served manned aircraft well for the last fifty years or more. This UAS technology supports an incredibly wide range of uses that not only allows old challeng es to be addressed in new ways but also creates new innovative world markets for hundreds, if not thousands, of new creative applications answering the call of \223better, faster and cheaper.\224  A growing number of early adopters and innovators of aviation tech nologies are beginning to realize the significance of a growing diversity and number of UAS  This growth  is  driven by a confluence of technological developments in airframes, powerplants, sensor s command and control sys tems and information management The  economic  value  of this industry projected to be $30 billion per year supporting 300,000 American jobs by 2035   As concluded b y the study team   the realization of UAS market forecasts will be constrained  unless key investment is performed in   The NAS ATC/A TM facilities needed to be updated to support new aircraft performance and controller capabilities   Training for ATC to be ready to adopt the new unmanned aircraft performance with acceptable workload    ATC to pilot communications that have the availability  integrity confidentiality and continuity as to not heavily impact either  pilot o r  controller workload   Development of Civil standards for the c ontrol and communications data link that w ill foster low cost access for a b road  range of UAS 226  this could in clude standards of f orm, fit and function as well as covering signal in space  networked channel  access and common control exchange applications    This key investment will develop a C 3  approach to support NAS integration.  It is agreed that other areas such as of DAA are important for investment, but there are already multiple economic incentive s  following this path  


  Paper 2581 Version 2  16  R EFERENCES  1 223 Unmanned Aircraft System \(UAS\ Service Demand 2015 2 035: Literature Review and Projections of Future Usage 224  Volpe National Transportation Systems Center Cambridge MA September 2013, Version 0.1   2 AINonline, June 8, 2012, Bill Carey   3  UAV Vision Pty Ltd  2/10 Uralla Street  P ort  M acquarie  NSW 2444  Australia   4  M1 D  Thermal FLIR PTZ Camera, Sierra Pacific Corporation, Las Vegas, NV   5   Experimental Advanced Research Lidar  USGS.gov  Retrieved 8 August 2007   6  Luke Wallace  Arko Lucieer  Christopher Watson and Darren Turner 223Devel opment of a UAV LiDAR System with Application to Forest Inventory\224, Remote Sensing Volume 4 issue 6, May 25 2012 http://www.mdpi.com/2072 4292/4/6/1519  7 M ag r et t a  J o a n  223 Wh y B u s i n e s s M o d e l s M at t er  224 H ar v a r d  Business Review, pp 3 8, May 2002   B IOGRAPH IES  Chris Wargo  is a program manager and director business development for Mosaic ATM, Inc 226  a firm specializing in air traffic management systems development unmanned systems and data management systems for the aviation sector. He also leads the Autonomous  Systems Group and  served as a Chair of the System Engineering Working Group of RTCA SC 203  He has also held positions as President of Computer Network Software, Inc., Vice President and General Manager for ARINC, Inc., C3I Program Manager RC A Automated Systems and GE, as well as Systems Engineer for GTE Sylvania, Electronic Systems Group, and the US Army. In his role as a leader in aviation  next generation systems engineering, he has participated in numerous ICAO, RTCA AEEC, IEEE and IATA c ommittees and standards working groups throughout his 30 year defense and aeronautical systems career. He has presented a number of papers and chaired many industry conference sessions related to the CNS and network system programs, project and technologie s of the general, business and air transport community.   He has a BSEE from the University of Wisconsin and an MS, Systems Engineering, from the University of Southern California, and has attended the Defense Systems Management College and the Advanced Ma nagement Program at the Harvard Business Sch ool    Jason Glaneuski  is a Program Manager and Operations Research Analyst in the Air Traffic Management Systems Division at the Volpe National Transportation Systems Center \(Volpe Center\ in Cambridge, MA His  Division applies information technology and operations research disciplines to enhance the capacity, safety and security of the National Airspace System. A key component of this work is developing concepts and designing automated decision support tools a nd capabilities  that provide solutions to existing and anticipated traffic flow issues. Mr. Glaneuski has experience both performing and managing technical work in the areas of traffic flow management \(TFM\, time based flow management \(TBFM and unmanned aircraft systems \(UAS\ sense and avoid SAA\, among others. Mr. Glaneuski is a graduate of Daniel Webster College in Nashua, NH, where he received his B.S in Aviation Management and Flight Operations. Prior to joining the Volpe Center in 2001, Mr. Glaneus ki worked for the FAA\222s Free Flight Program Office in Washington, D.C   Mark Strout  is an Operations Research Analyst and Project Manager in the Air Traffic Management Systems Division at the Volpe Center in Cambridge, MA. Mr. Strout provides project oversight and technical expertise attendant to several ATM related concepts and capabili ties, including traffic flow management \(TFM\, time based flow management \(TBFM\, National Airspace System Common Reference \(NCR unmanned aircraft systems and flow based trajectory management \(FBTM\. Mr. Strout is a graduate of the University of Virgini a, where he received his B.S. in Aerospace Engineering   Gary Churc h is the President of Aviation Management Associates, Inc. a Washington, DC based aviation firm established in 1984 to provide consulting services related to the Federal Aviation Administ ration and the National Aeronautics and Space Administration Mr. Church leads a team of two dozen aviation domain experts. He is a former air traffic controller and Manager of Air Traffic Control for the Air Transport Association \(now known as Airlines fo r America as well as an instrument rated pilot. Mr. Church has authored many air traffic control related articles and reports over the years as well participat ing  in numerous  key aviation related committees on behalf of FAA and others Mr. Church majored in Economics and Physics at the Un iversity of California Berkeley       
























































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


