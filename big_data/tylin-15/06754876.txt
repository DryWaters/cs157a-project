Prashant Bachanna, Vivek Jalad and Sharanbasappa Shetkar Dept. of Electronics and Communication Engineering, Lingaraj Appa Engineering College, Bidar, Karnataka INDIA Prashant.bachanna@gmail.com  Vivec53@gmail.com  sshetkar@gmail.com 
Design and analysis of high speed low power reusable on chip bus based on wishbone 
Abstract 
 000\003 000\003 
FPGA and ASIC design based on SoC technology have been widely used in the embedded systems. A flexible interconnection scheme is crucial in SoC design. In this paper, we adopt the Wishbone bus to interconnect a variety of devices due to its open architecture and many a free IP core with 
a Wishbone interface supplied by Open Cores organization. In general SoC system, a single bus interconnects all devices that are not divided into high-performance unit such as CPU,on-chip ram and low speed devices like uart,gpio and so on. It leads to a big problem: all Wishbone bus cycles run at the speed of the slowest device. We have to add the corresponding logic to regulate the system frequency for some low-speed devices, but it causes a new 
problem which increases the overall system power consumption In view of the drawback, based on Wishbone bus, the paper proposes a double bus that makes first level Wishbone bus and the second level bus to interconnect the different devices according to the speed of the devices.Finally, we set up a SoC system to verify the performance of the proposed bus and the result shows that the double bus is feasible in low-power SoC 
design I INTRODUCTION 
Wishbone bus Specification is defined as a kind of common interface within the interconnection of IP core. So it defines a standard set of signals and bus cycles to connect different modules, and it can interconnect between soft-core, solid-core and hardcore. Furthermore, it has no special required on the development tools and target hardware and almost compatible with all existing mainstream EDA synthesis tool, and you can use a variety of hardware 
description language to achieve it. a double bus that makes first level Wishbone bus and the second level bus to interconnect the different devices according to the speed of the devices.Finally,we set up a SoC system to verify the performance of the proposed bus and the result shows that the double bus is feasible in low-power SoC design 
Main implementation of the double wishbone as shown below where it consists of two 
II P ROPOSED S TUCUTURE 
wishbone bus and opb bus inside address decoder wishbone bus 2 and wishbone bus 1 is select the signal based on address decoding .data is input to the opb bus from data generator, address decoder decode the address and select the particular wishbone bus for data transmission to peripheral device. Every peripheral device is assign with address if the assign address is high it select the high data transmission wishbone bus otherwise if lower address it select low data transmission 
wishbone bus The figure1 below can apply to the wb_conmax structure  used as double wishbone bus structure so that it cover the master 
slave interface through bridge.  In the design,wb_conmax0 directly adopts the wb_conmax provided by Opencores organization;wb_conmax1 handle address decoding on the basis of wb_conmax0;The bus bridge process the interconnection and synchronization of data and 
 
2014 Fifth International Conference on Signals and Image Processing 978-0-7695-5100-5/13 $31.00 © 2013 IEEE DOI 10.1109/ICSIP.2014.37 197 
2014 Fifth International Conference on Signals and Image Processing 978-0-7695-5100-5/13 $31.00 © 2013 IEEE DOI 10.1109/ICSIP.2014.37 197 
2014 Fifth International Conference on Signal and Image Processing 978-0-7695-5100-5/13 $31.00 © 2013 IEEE DOI 10.1109/ICSIP.2014.37 197 


Figure 2 Master slave inter connection through addres decoder 
Figure 1 Double wishbone bus opb connection 
III.     Simulation 
Wb_conmax1 address encode is independent of the wb_conmax,and we can simply assign the relative address0x0000_0000,0x0100_0000,0x0200_0000,0x030 0_000,,0x 0f00_0000.Each slave device have address space of 16MB that satisfy the requirement of the lowspeed devices in embedded systems. According to figure 3,wb_conmax1 was connected to slave interface 13 of wb_conmax0,the address of low-speed devices is 0xd000_0000,0xd100_0000,0xd200_000,0xd300_0000 0x df00_0000. We get address encoding of bus bridge by decoding of the 24th to 27th of address bus and select signal of first-level bus, as is showed in figure 2 
a Address coding b. Synchronous logic 
address signal between wb_conmax0 and wb_conmax1. The devices interconnect through the standard wishbone bus signal,unlike the AMBA which consiste of AHB and APB.So the bus bridge do not think over the bus signal conversion,just need to realize the task of address encode and synchronization within different clock siginal 
In the double wishbone bus,we must design synchronous logic of bus brideg because two level bus have different frequency.The logic is designed by response signal waiting mechanism, In the design,the second-level bus ope rating frequency is the even-frequency of the first-level bus frequency so the synchronization logic is designed by the response signal waiting mechanism. The frequency of hign-speed bus is 4 times or 2 times as often than low-speed bus in multi-level bus system Taking  into accout the operating frequency of the peripheral devices in sec ondary-level bus,ratio of frequency of two levle bus set up to four times in the bus bridge 
Simulating the written VHDL code in ISE simulator and verifying the waveforms generated by the simulator. If the required simulated output is not achieved, then the VHDL code is checked and necessary corrections are made. The required waveforms are noted down as a reference to the 
198 
198 
198 


 
synthesis stage or at the final stage. The waveforms are presented subsequently All the VHDL module files are integrated and a top file is selected to run the whole implementation.  Implementation compresses of different stages like translate, map and place and route \(PAR\ramming file is generated which creates a bit fo rmat file, which is dumped on the Xilinx or SPARTAN-III using Xilinx \(software through a Joint Task Acting Group \(JTAG communication cable.  On the hardware, if the design works according requirement, then design is corrected, if any error occurs then reconsider the design As we can seen from simulation result the chip through higher order wishbone bus and when chip select signal is high the data is trasmitted through lower order wishbone bus Figure 3. final simulation result of OPB bus The paper design a kind of double wishbone bus based on wb_conmax released by Open Cores organization. The two different type of IP core link with the two level bus respectively,the architecture can not only improve the "Design-reuse" ,it also provide the optimal strategy for reduction of power consumption.Finally,the paper set up a SoC system based on double wishbone bus and get synthesis report and power analysis report, the result verifys the feasibility of low-power design by adopting double bus structure in SoC system  This implementation design only for 2  periperal device it can be extended for                         more devicess.The timing and clock frequence can be reduced by reducing the number of resource utilization. Area and po wer consumption can even be reduced by reducing number of gate and flip flop 
IV IMPLEMENTATION V RESULTS VI CONCLUSION AND FUTURE WORK 
select signal low the data èaabbccddê trasmit 
199 
199 
199 


  Rudolf Usselmen,Opencore s SoC B us  Review,http://www.opencore s.org,January 2001  R icha rd Herve ille W I SHBONE S y stem onChip\(SoC\nterconnection Architecture for Portable IP Cores.Revision: B. 3 pp:4- 32,September 2002   Raul F a jardo,M I nimal Ope nR I S C S y stem on Chip,Rev 1.1,http://www.opencores.org,September 2010   Rudolf Usselmann, W I SHB O NE Conmax  I P  Core,Rev.1.1,http://www.opencores.org/project,Oct ober 2001  n e Wolf, F P GA Based S y stem D e sign Prentice Hall, 2005  Ha rtwig J e schke,Eff icienc y mea su re s fo r SOC concepts,Journal of System Architecture,vol 54,pp1039-1045,April 2008.1  y as Ka nta Swa in, De sig n and Ve rifica tion of WISHBONE bus Interface for SoC integration,Mater thesis,National Institute of Technology ,Rourkela,Orissa,January 2010  sad,Dh anabal R L ow Pow e r AS I C  Design for Auto Mation in Various Industrial Applications,International Jouranl of Advanced Engineering Sciences and Technologis,Vol 6\(1\,pp 144- 149 ,2011  Resve Seleh,Steve Wi lton,Shahriar Mirabbasi,et al,"System-onchip Reuse and integration,"Procedings of the IEEE,Vol 94\(6\,pp 1050- 1069,June 2006 
REFERENCES 
200 
200 
200 


A C B P\(A,B,C  f 1 f 2 f 1  P\(A|C f 2  P\(C|B\P\(B 
Variable Defines a rand om variable Factor Correspond to factors of the joint distribution Relationship Indicates which random variables are part of which factor Random variables in factor graphs are again represented in form of circles, possibly clamped to observed values. The factors of a joint distribution are associated with constructs connected via undirected arcs to all variables used in the factors. An example is illustrated in Figure 3. The joint distribution is split up into two factors and As there can be different ways of splitting up a joint distribution, there can be different factor graphs describing the same joint distribution The goal of inference algorithms is to compute unknown quantities from those that are known. Given a set of latent random variables and a set of observed random variables  probabilistic inference can be described as the problem of computing a distribution over the latent variables given the observed on A s  m e ntio n e d ab ov e in a B a y esia n a p proach in which traditional parameters are treated as latent variables, inference can be used to solve a wide range of problems such as prediction for new data A number of general purpose algorithms have been developed in the past refers to the fact that these algorithms are not tailored to a specific probabilistic model. Instead, they can be adapted to different models, and graphical models provide a formalism facilitating these adaptions On a broad level, inference algorithms can be classified into exact and approximate algorithms. Exact are all algorithms that always deliver the correct solution Notable examples are the jun ction tree algorithm  which works on any kind of graphical model, and the sum-product algorithm w h ic h can be ap plied  to  tree-structured factor graph Problematic about these algorithms is that they are often inapplicable to real world problems. On the one hand, the sum-product structure and can only be used with a limited number of models. On the other hand, the more general junction tree algorithm is computationally intractable for all but the simplest model  E x a ct algorit hm s are al so restricted to  discrete variables or linear Gaussian models   Unfortunately, one cannot expect to find an efficient and exact solution for the general case  Hence, there is a huge field of research dealing with approximate inference. There are two main types of methods: deterministic and stochastic approximations A famous representative of the first type is mean field approximation. The idea is to replace the original intractable distribution with an approximate form with additional independencies added to ensure tractability Minimizing the difference between the original distribution and the approximate form delivers an approximation that can be used for inference. Variational Message Passing \(VMP  is a general purpose algorithm doing this on graphical models. Alternatively, the Expectation propagation \(EP ca n be  used. The difference compared to VMP is that the minimization is done in another way. As approximations, these algorithms do not necessarily return a correct solution. They may be far off and do not provide an estimate of accuracy Stochastic approximations do not approximate the functional form of a distribution but perform sampling instead. The idea is that any property of a distribution can be approximated by drawing and averaging over a sufficiently large number of samples. For virtually all models of interest, Markov Chain Monte Carlo MCMC\methods can be used. They construct a markov chain whose stationary di stribution is the one from which samples shall be drawn A popu lar s p ecial  case of MCMC is Gibbs sampling which can be applied to a wide range of graphical m   A comparison of these inference algorithms is found in Table 3. Stochastic inference is better with respect to accuracy. It will converge to solutions of any accuracy eventually while deterministic algorithms may deliver bad results no matter how long they run Sampling can also be applied to a wider range of problems. It can be cumbersome to apply deterministic 
Construct Symbol Meaning 
algorithm severely restricts a graphês the approximationês 
Table 2. Basic constructs of factor graphs Figure 3. A factor graph corresponding to the directed graphical model of Figure 1 2.3 Inference Algorithms 
022 025 022 011 031  031  002  031   031   
factor General purpose 
762 


2.4 Modeling environments 
Criterion Inference algorithms Name Language Inference algorithms 
Table 3. Comparison of classes of approximate inference algorithms Deterministic Stochastic Table 4. Software packages for graphical models Deterministic Stochastic 
algorithms to more elaborate distributions  T h e y score though when it comes to performance. Sampling may require long running times until convergence is reached. Only for few, very large models, sampling may be more efficient It is also n o t trivial to co n figure a sampler and to diagnose convergence. Deterministic methods can be used more easily Accuracy  Expressiveness  Performance Usability There are several software packages that implement general purpose inference algorithms for graphical models. Murphy i e w ed a num ber of  th e m   While the published review is relatively old, the accompanying website 2 is updated regularly. At the time of writing it lists 68 packages, each of which is characterized along several dimensions. To identify suitable candidates for a machine learning MDE approach, I filtered the list in the following way First, all software packages not having an API were discarded. There are several s tandalone software tools While some of them are well-developed, their major disadvantage is that their models cannot be integrated as components into other software environments Second, all software packages not fully supporting continuous nodes were discarded. Many of the tools work only with discrete random variables or may support continuous variables by discretization or sampling only. Expressiveness is severely reduced when using only discrete variables. Hence, focusing on the more flexible packages seems reasonable Five software packages were left for closer inspection after filtering. They ar e listed in Table 4 along with the corresponding programming languages as well as the inference algorithms they support. Almost all packages offer support for inference via stochastic approximations. Infer.NET d JA GS 34] s u pport ordinary Gibbs sampling. Stan uses a method called Hamiltonian Monte Carlo Sampling  2 http://www.cs.ubc.ca/~murphyk/Software/bnsoft.html Last updated 12 February 2013 Blaise 3 uses some other form of MCMC. Regarding Blaise, I was not able to find source code for download. Deterministic approximations are supported only by BayesBlock 4 through a variational algorithm and by Infer.NET, which allows users to choose between VMP and EP As discussed in Section 2.3, both deterministic and stochastic inference approa ches have their pros and cons. Hence, I chose the Infe r.NET library as a starting point to explore the possibilities of model-driven probabilistic data analysis as it supports both approaches Infer.NET is a C# library that allows specifying probabilistic models via its API. Internally, it compiles the code to more efficient code on which inference algorithms can be run automatically Models are created by defining variable objects and connecting them with factor objects. Hence, it closely resembles the factor graph thinking. Variables may have prior distributions if used in conjunction with distribution factors. Their distribution may also be defined as a function of other variables. For instance, a variable could be the sum of two others Infer.Net offers a wide array of distributions and other types of factors BayesBlocks Python C x Blaise Java x Infer.NET C x x JAGS Java x Stan C R x From the perspective of model-driven engineering graphical models constitute an interesting development in the field of machine learning. The most important aspect is the move away from standard procedures for specific problems that must be adapted and glued together. Instead, these new approaches strictly separate the problem definition from the solution strategy. Declarative modeling languages like A Mathematical Programming Language \(AMPL\ [36 dem o n s trate t h e  usefulness of this idea for the case of optimization  3 http://publications.csail.mit.edu/abstracts/abstracts07 bonawitz/bonawitz.html 4 http://research.ics.aalto.fi/bayes/software 
3 A Model-driven Critique 
userês 
763 


may play the same role in todayês lgorithmês ed factors \(sum of two or more variables inner product of random vectors, É\hich is doc necessary to specify the variablesê roles 
  
variables random observed type value type is parameter for produces Node name Plate 
4. Model-driven Engineering for Machine Learning 4.1 A Graphical Modeling Language 
problems. Users can focus on the important things modeling the domain mathematically and they can use any method they want to have the problem solved Reliable standard software with a declarative interface has been a key driver in the success story of linear programming as it enabled widespread industry applications  G rap h ical m o d els a n d g e n e ral p u rpose inference algorithms big data analytics challenges Moreover, graphical models could be interpreted as a DSML for probabilistic modeling. They provide dedicated constructs needed to specify a probabilistic model on a conceptual level. They also come with general purpose algorithms working on these models Algorithms adapting themselves to the graph structure could be interpreted as transformation engines generating model-specific inference code. Hence, they offer the two constituent elements of MDE technology: a DSML and a transformation engine  Although some literature on graphical models may suggest a graphical model is all one needs to do inference, this is not entirely true. Graphical models are meant as a device conveying the structure of a model to ease the inference a derivation. However they leave many things unspecified. Among them are obvious things such as the types of distributions being used, but also more complicated aspects such as which variables are connected with each other when their relationships cross the borders of plates. Consider the simple example of Figure 1. Whether the variables A B, and C are discrete or re al and which distributions are used to define the model cannot be seen. As a consequence, a direct correspondence of visual models and program code cannot be established Consistent with this observation, none of the libraries in table 4 provides facilities for graphically specifying graphical models. Instead APIs are called to create them. While the process of modeling resembles that of drawing a model \(and specifying, on the way, the additional information that is needed\ no actual visualization connected to the code is created I argue that this way of modeling in code leaves untapped a considerable part of the benefits graphical models may provide. Most importantly, visual languages can further boost productivity. Direct improvements might be realized since specifying models graphically avoids mistakes in the source code. Much more significant though can be secondary effects. Machine learning components integrated into information infrastructures of enterprises will not be developed by a single person but by teams that are subject to employee turnover. New employees could more easily familiarize themselves with visual models than with code A direct, formal connection between visual models and code also ensures alignment that can otherwise be lost easily over time as the software is modified It is worth noting that modeling Bayesian networks is different than using grap hical user interfaces \(GUIs of statistical software packages such as SPSS. The former define distributions \(cf. Section 2.1\hile the latter are used to chain to gether the statistical techniques implemented in the software package To develop a conceptual model of an engine transforming a graphical model to executable code, the first step is to define a DSML containing all necessary information. Graphical modeling languages are a good starting point but do not contain sufficient information To approach the problem of designing the DSML, I have analyzed the Infer.NET modeling API against the backdrop of the modeling languages discussed in Section 2.2. The result is an initial proposal for a DSML A model of its abstract syntax can be found in Figure 4. The rationale behind it is as follows The main constructs of Infer.NET models are which are either or Both have associated data types stor ed in the attribute and the latter have an attribute called to store the observation. Although it is po ssible to define constants as well their use is discouraged. The code generated by the library must be recompiled each time a constant value changes. Observed variables can be changed without recompilation Variables are connected to each other via factors They define the distributions over the variables. Hence Infer.NET models are very similar to factor graphs. A difference is that they can be thought of as directed Factors can be of different types. They broadly fall into the classes of distribution factors \(Gaussian, discrete umented in the attribute  A factor is parameterized by at least one variable, which is captured in the relationship. If more than one variable is required and they cannot be used interchangeably, it is e.g., mean and variance parameters of a Gaussian All factors define a distribution over exactly one random variable which is indicated by the relationship Both variables and factors are subsumed in the abstract entity type which can be given a textual description \(the attribute\des can lie in two types of areas. They can be in a in which case 
764 


Range name new  Range plateSize\Named name 
Figure 4 Abstract syntax of the DSM L in entity relationship notation 
plateSize is higher than selects GateOption Gate value is nested in priors infer Observed Variable predict GenerateModel InferPosteriors MakePredictions GenerateModel GenerateModel 
they are not single nodes bu t arrays of nodes indexed by the plate. The index runs from one to If a node belongs to multiple plates, it becomes a highdimensional jagged array. To consistently index these arrays, an arbitrary total order must be defined on the plates. It is codified in the relationship It can occur that a variable being a parameter for a factor is associated to a diff erent set of plates than the factor is. In this case, it must be specified how the mismatch should be resolved There can be two cases First, the factor can lie in a plate in which the variable is not \(more factors than variables\. This can be resolved by feeding the variable into all the factors. The other possibility is that th e variable lies in a plate in which the factor is not \(more variables than factors Similarly to the first case, all variables can be fed into the factor \(if the specific type of factor can handle them\ any case, if only a specific variable or factor should be used and not all, a selector variable must be defined. The ternary relationship codifies this Nodes may also lie within a to allow for describing mixture models Gate options belong to exactly one which in turn belongs to exactly one random variable. The valu e associated with a gate option must be a possible value of that random variable. It is specified using the attribute. Gate options may be nested in each other as documented in the relationship When using the model, the purpose is to infer the posterior distributions conditioned on all observations to make predictions for new data. Hence, it must be indicated which observed variables  replaced with inferred poster iors. This is done using the Boolean attribute of the entity type After setting the posteriors, training data will be replaced with new data. Any variable for which no new data will be available can be marked as one for which a posterior predictive distribution shall be evaluated This is done using the attribute Based on the DSML from Section 4.1 a simple code generation scheme can be defined. The basic skeleton consists of a single C# class with three methods  and Most relevant is which is why I discuss code generation for this method only Ignoring gates for the moment can be structured into different areas as illustrated in Figure 5. First, ranges must be defined that are used as indices for plates. This is done by processing all plates of a model and inserting a line of code for each of them 
4.2 Code Generation Scheme 
is parameter for produces is contained in is higher than plateSize role D,T infer selects is contained in belongs to is switched by predict is nested in D,T value 1,n 1,n 0,n 0,n 0,n 0,n 0,n 0,n 0,n 0,n 0,n 0,n 0,n 1,1 1,n 1,1 0,n 1,1 1,1 type type value name name 
Factor Variable Node Plate Random Variable Observed Variable Gate GateOption 
765 


Harvard Business Review Communications of the ACM Communications of AIS Communications of the ACM 
5. Conclusion, Limitations, and Outlook 6. References 
Range definition area Variable definition area Factor definition area Branching block area 
Variable varName dataType varDistName varName varDistName var3Name var1Name var2Name selects GateOption Branching block area branch 
Model area 
varName Variable New dataType Named varName Variable  dataType varName varName Variable  dataType Random\(varDistName var3Name = var1Name > var2Name using  Variable If\(branch one model area  using  Variable IfNot\(branch other area  
T. H. Davenport and D. J. Patil, çData Scientist\004: The Sexiest Job Of the 21st Century Data Scientist S. Chaudhuri, U. Dayal, and V. Narasayya, çAn overview ss intelligence technology  H. J. Watson, çTutorial: Business Intelligence Present, and Future  S. Madden, and M. Stonebraker, çA Comparison of Scale Data Analysis,é in  S. Ghemawat, çMapReduce\004: Simplified Data Processing on Large Clusters  
Figure 5 Structure of the model area 
The second step is to define all variables. Again this can be done by proce ssing all entities of type and appending lines of code and are taken from the corresponding attributes of the entities. Additionally, the variable must be declar ed as a class attribute The third step is to define the factors. Hence, all entities of type factor must be processed to append new lines of code in the factor definition area. The structure The first line assigns a distribution object to a variable  is an object representing a distribution. The second line defines the distribution over a Boolean variable as the probability of being larger than  If any variable belongs to a plate, the variable definition code is changed to an array version with corresponding dimensions. The factor definition also iterates over these dimensions. Special care is necessary if entries in the relationship are encountered Fortunately, it can be handled easily by using the selecting variable as an index Finally, it must be accounted for the gates used in the model. This is done by nesting model areas into each other in the same way the entities are nested. A designated root mod el area is created first All other model areas are nested by putting them into the of their parent model area Gates are defined in Infer NET using different selection methods. If the selecting variable is Bernoulli, the code will look like this Motivated by the lack of data scientists in industry I have proposed an initial conceptualization of a DSML supporting code generation from visual representations of probabilistic models for big data analytics Starting at existing notations, extensions have been defined based on analysis of the Infer.NET modeling API How modeling constructs correspond to code has been illustrated informally by a rudimentary code generation scheme. However, no actual code generator for the Infer.NET library has been implemented yet. Doing so would substantiate the claim that the DSML does in fact cover all constructs necessary. I plan to address this limitation in the future by implementing a DSML  Another limitation is the focus on only a single library, namely Infer.NET Referring back to the analogy of linear programming, the ideal DSML would be library-independent and could generate code for a wide range of them Unfortunately these libraries are in an earlier stage of developmen t. In the near future, I do not expect them to be s tandardized an d stable enough to ensure easy interoperabilit y. Integrating other libraries can therefore be a long term goal only It might be fruitful though to analyze standalone software packages that have been discarded in this have GUIs, yet they are often meant to be used for educational examples instead of sophisticated models \(e.g., DoodleBUGS, the GUI of the BUGS project  eles s s u c h  GUIs may inform the design of concrete syntax for the DSML and could also reveal deficits that have not shown up so far. A review is left to future research 
1  no. October, 2012 2  of busine vol. 54, no. 8, pp. 88 98, 2011 3  Past vol. 25, no. 39 pp. 487 510, 2009 4 A  P a v l o, E P a u lso n  A. Rasin, D. J A b ad i, D. J. De W i tt Approaches to Large 2009, pp. 165 178 5 J. De an an d   vol. 51, no. 1, pp. 1 13, 2008 
of the code depends on a fa ctorês type. Examples are using Microsoftês Visualization and Modeling SDK paperês review. Some of them 
SIGMODê09 
766 


Harvard Business Review Knowledge and Information Systems SIGKDD Explorations Advances in Knowledge Discovery and Data Mining IEEE World Congress on Computational Intelligence \(WCCI 2008 Philosophical transactions Series A Mathematical physical and engineering sciences Journal of Visual Languages Computing Computer 9th Workshop on Domain-Specific Modeling at OOPSLA 9th International Software Product Line Conference Pattern Recognition and Machine Learning The Elements of Statistical Learning - Data Mining, Inference, and Prediction Statistical Science Neural Information Processing Systems NIPS Machine Learning: A Probalbistic Perspective Journal of the Royal Statistical Society Series B \(Methodological IEEE Transactions on Information Theory Artificial Intelligence Journal of Machine Learning Research Statistics in Medicine Bayesian Analysis International Society for Bayesian Analysis Bulletin 3rd International Workshop on Distributed Statistical Computing \(DSC 2003 AMPL: A Modelling Language for Mathematical Programming Operations Research 
UAIê01 
6   October, 2012 7  X  W u  V  Kum a r, J. Ross Qu in lan J. G h o s h, Q  Ya n g  M. Motoda, G. J. McLachlan, A. Ng, B. Liu, P. S. Yu, Z vol. 14, no. 1, pp. 1 37, 2007 8 M Ha ll, E Fran k  G  Holm es B P f ah rin g e r P   vol. 11, no. 1 pp. 10 18, 2009 9 U. M  Fay y ad  G  P iatetsk y S h ap iro, an d  P  Smy t h   in  1996, pp. 1 34 10  2008, pp. 1 24 11 T  Mi n k a, J. W in n J. G u iver, an d D  Kn o w les 12  vol. 371, no. 1984, 2013 13  the visual model-driven development of next generation software, vol. 17, no. 6, pp. 528 550, 2006 14   vol. 39, no. 2, pp. 25 31, 2006 15  Use of Domain, 2009 16  Specific 2005, vol. 3714, pp. 198 209 17  rial on Learning With Bayesian 18  19 C M Bisho p  New York: Springer, 2006 20 T  Ha stie, R. T i b s h irani an d  J. Friedm a n   2nd ed. 2009 21   vol. 19, no. 1, pp. 140 15  5, 2004 22  Version 2 23  24  2008, pp. 1073 1080 25 K Mu r p h y  The MIT Press, 2012 26  computations with probabilities on graphical structures and vol. 50, no. 2 pp. 157 224, 1988 27  the Sum, vol. 47, no. 2, pp. 498 519, 2001 28 P inference in Bayesian belief networks is NP, vol. 60, pp. 141 153, 1993 29  vol. 6, no 1, pp. 661 694, 2005 30  2001, pp. 362 369 31 D L u n n D. S p ieg elh a lter, A  T h o m as, an d N Best vol. 28, no. 25, pp. 3049 3067, 2009 32  M P. W a nd, J  T  O r m e r od S. A  P a doa n  a n d R   distributions vol. 6, no. 4, pp. 1 48 2011 33   vol. 14, pp. 13 15, 2007 34  Bayesian Gr 2003 35  V ersion 1.3 36 R  F o u rer D M. G a y  an d B W  Ke rn igh a    Monterey: Duxbury Press, Brooks/Cole Pub. Co., 2002 37  vol. 50, no. 1, pp. 42 47, 2002 
A. McAfee and E. Brynjolfsson, çBig Data: The Management Revolution Zhou, M. Steinbach, D. J. Hand, and D. Steinberg, çTop 10 algorithms in data mining  Reutemann, and I. H. Witten, çThe WEKA data mining software: an update  From data mining to knowledge discovery: an overview,é in  C. M. Bishop, çA New Framework for Machine Learning,é in  Infer.NET 2.5,é 2012 C. M. Bishop, çModel based Machine Learning H. Giese and S. Henkler, çA survey of approaches for intensive systems  D. C. Schmidt, çModel Driven Engineering  J. K‰rn‰, J. P. Tolvanen, and S. Kelly, çEvaluating the Specific Modeling in Practice,é in J. P. Tolvanen and S. Kelly, çDefining Domain Modeling Languages to Automate Product Derivation\004 Collected Experiences,é in  D. Heckerman, çA Tuto Networks,é Redmond, 1996 M. I. Jordan, çAn introduction to probabilistic graphical models,é 2003 M. I. Jordan, çGraphical models  OMG, çUnified Modeling Language \(UML 4.1 August 2011.é 2011 OMG, çBusiness Process Model and Notation \(BPMN Version 2.0 January 2011.é 2011 T. Minka and J. Winn, çGates: A graphical notation for mixture models,é in  S. L. Lauritzen and D. J. Spiegelhalter, çLocal their application to expert systems  F. R. Kschischang and B. J. Frey, çFactor Graphs and Product Algorithm  Dagum and M. Luby, çApproximating probabilistic hard  J. Winn and C. M. Bishop, çVariational Message Passing  T. P. Minka, çExpectation propagation for approximate Bayesian inference,é in  The BUGS project: Evolution, critique and future directions  Fr¸hrwirth, çMean field variational Bayes for elaborate   K. P. Murphy, çSoftware for Graphical models: a review  M. Plummer, çJAGS\004: A Program for Analysis of aphical Models Using Gibbs Sampling JAGS\004 Just Another Gibbs Sampler,é in Stan Development Team, çStan Modeling Language Userês Guide and Reference Manual 2013 G. B. Dantzig, çLinear Programming  
767 


             


                                          


                                             


                      


                                               


   


                                


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


