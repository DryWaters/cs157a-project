Association Analysis with One Scan of Databases Hao Huang I Xindong Wu  and Richard Relue   Dept of Math and Computer Science, Colorado School of Mines Golden, Colorado 8040 1 USA  Department of Computer Science, University of Vermont Burlington Vermont 05405 USA Abstract Mining frequent patterns with an FP-tree avoids costly candidate generation and repeatedly occurrence frequency checking against the support threshold It therefore achieves better performance and eflciency than Apriori like algorithms However the database still needs IO be scanned twice to get the 
FP-tree This can be very time-consuming when new data are added to an existing database because nu0 scans may be needed for nor only the new data but also the existing data This paper presents a new data structure P-tree Pattern Tree and a new tech nique, which can get the P-tree thmugh only one scan of the database and can obtain the corresponding FP-tree with a specified support threshold Updating a P-tree with new data needs one scan of the new data only and the existing data do not need to be re-scanned 1 Introduction 
An association rule is an implication of the form X Y where X and Y are sets of items and X n Y  4 The support s of such a rule is that s of transactions in the database contain X U Y the confidence c is that c of transactions in the database contain X also contain Y at the meantime A rule can be considered interesting if it sat isfies the minimum support threshold and minimum confi dence threshold, which can be set by domain experts Most of the previous research with regard 
to association mining was based on Apriori-like algorithms l They can be de composed into two steps 1 Find all frequent itemsets that hold transaction support above the minimum support threshold 2 Generate the desired rules from the frequent itemsets if they also satisfy the minimum confidence threshold Apriori-like algorithms iteratively obtain candidate item sets of size k  1 from frequent itemsets of size k Each iteration requires a scan of the original database It is costly 221This research is supported in pan by the U.S Army Research Lab ratory and the U.S Army 
Research Office under grant number DAAOIP 02-1-0178 and inefficient to repeatedly scan the database and check a large set of candidates for their occurrence frequencies. Ad ditionally, when new data come in we have lo run the entire algorithms again to update the rules Recently an FP-tree based frequent patterns mining method 2 developed by Han et a1 achieves high efficiency compared with Apriori and TreeProjection 3 algorithms It avoids iterative candidate generations The rest of the paper is organized as follows We review the FP-tree structure in Section 2 In Section 3 we intro duce a new 
FP-tree based data structure called pattern tree or P-tree, and discuss how to generate the P-tree by only one database scan How to generate an F\222P-tree from a P-tree is discussed in Section 4 Section 5 deals with updating the P-tree with new data, and Section 6 provides a reference for our experimental results 2 Frequent Pattern Mining and the Frequent Pattern Ree The frequent-pattern mining problem can be formally defined as follows Let I  il,iz  in be a set of items and D be a transactions database where each transaction T 
is a set of items and T c I An unique identifier called its TID is assigned with each transaction A transaction Tcon tains a pattern P a set of items in I if P 2 T The support of a pattern Pis the number of transactions containing P in D We say that P is a frequent pattern if P\221s support is no less than a predefined minimum support threshold  A frequent pattern tree is a prefix-tree structure storing frequent patterns for the transaction database where the support of each tree node 
is no less than a predefined min imum support threshold  The frequent items in each path are sorted in their frequency descending order More fre quently occurring nodes have better chances of sharing the prefix strings than less frequently occurring ones that is to say more frequent nodes are closer to the root than less fre quent ones In short, an FP-tree is a highly compact data structure, \223which is usually substantially smaller than the original database, and thus saves the costly database scans in the subsequent mining processes\224 Z After the construction of an FP-tree. we can use this data 0-7695-1754402 17.00 Q 2002 IEEE 629 


structure to efficiently mine the complete set of frequent patterns with the F'P-growth algorithm which is a divide and-conquer method performed as follows 1 Derive a set of conditional paths, which co-occurs with a suffix pattern, from the FF-tree 2 Construct a conditional FP-tree for each set of the con ditional paths 3 Execute the frequent pattern mining recursively upon the conditional FP-tree The study in 2 shows that the F'P-growth algorithm is more efficient and scalable than both Apriori and TreePro jection The FP-tree based algorithm has some inherent advantages: the new data structure is desirably compact and the pattern growth algorithm is efficient with the data struc ture But it also has the following problems A new F'P-tree requires scanning the database twice Although a validity support threshold watermark 21 is realizable there is no guarantee of complete database information for the FP-tree when new data come into the database If the specific threshold is changed we will have to rerun the whole FP-tree construction algorithm that is rescan the database twice to get the new corresponding frequent item list and a new FP-tree Even if the threshold remains the same an FP-tree can't be constructed or updated at real-time Each con struction or updating needs to go from scratch, and scan the new and old data twice 3 Patterns Generation with the Pattern nee The F'P-tree based method has to scan the database twice to get an FP-tree, whose central idea is to get the list L of item frequencies in the first time and then construct the FP tree in the second time according to L A Pattern Tree \(P-tree for short\unlike FP-tree, which contains the frequent items only, contains all items that ap pear in the original database We can obtain a P-tree through one scan of the database and get the corresponding FP-tree from the P-tree later The construction of a P-tree can be divided into two steps as well 1 When retrieving transactions from a database we can generate a P-tree by inserting transactions one by one after we sort the items of each transaction in some or der \(alphabetic numerical or any other specific order and meanwhile record the actual support of every item into the item frequency list L 2 After the first and only scan of the database we sort L according to item supports The restructure of the P tree consists of similar insertions in the first step. The only difference is that one needs to sort the path ac cording to L before inserting it into a new P-tree This approach makes the hest use of the occurrence of the common suffix in transactions thereby constructing a more compact tree structure than F'P-tree 3.1 Algorithm Algorithm 1 P-Tke Generation Input A transaction database DB and a minimum suppon threshold Output A Pattern tree The pattern tree can be created in WO steps Srq I CO~WUCI II P-rrec P and obtain lha irem frequency in L minisup I P tRoor 3 Foreach transacuon T in the transaction database 2 L t m a Son T into I I T;l in alphabetic order Here in each soned transaction T  It I T I is the hat item of the transaction and T is the remaining items in the wansaction b nsrrr\([t I T<l P e Update L with items in I IT The function nserf\([t I T P performs as follows Function InxerK[r 1 T P BEGIN FOR each of P'a child node N THEN IF r.iremNome  N.iremName Insm\(T NI ENDIF RETURN ENDIF ENDFOR Create a new Node N N'.itemNomecr.ilemNamr N'freyuencye I P.childLisitN IF T is not empty ENDIF RETURN THEN lnser~\(T N END Step 2 Rerrructure rhe ittirid P-free P I newP ROOI 2 For each path pi from the root to a leaf in the initial P-wee P Until pi  4 do a The common suppon of each ikm in pi is that of the node next to the 1st branching-node If there is no branching-node in pi the common support of each item is the actual supprt of each item in p A branching-node is a node aflrr which there exists more than one branch in the tree b Get a suhpath p from p with the common suppon for every item  c Sortp according to L d lnsen the soned pi into the new P-tree by calling function Im..\(p newP e Pt Pi Pi 630 


3.2 Analysis The P-tree generation algorithm needs exactly one scan of the database and one scan of the initial P-tree The run ning time depends on how the patterns distribute in the database The more high frequent patterns in the database the faster the algorithm will be The lower bound is the run time of one scan of the database In the contrary the less the high frequent patterns in the database the slower the algorithm will be The upper bound is the runtime of two database scans 3.3 ture which has the following properties Pattern Tree A Formal Definition A pattern tree or P-tree for short is a rooted tree struc 1 2 3 4 5 4 The root is labeled as 223Root\224 All other items are either its children or its descendants Each node except the root is composed of three fields itemName frequency and childLisr where iiemName stands for the actual item in the transaction database frequency represents the transaction support of the item in the database and childList stores a list of its child nodes A path in a P-tree represents at least one transaction and the corresponding occurrence\(s\which is the fre quency of its least frequent item\(s A node holds more or equal frequency to its children or descendants Note that the root node doesn\222t have the actual meaning in transactions so we don\222t consider its frequency A prefix shared by several paths represents the com mon pattern in those transactions and its frequency The more paths share the prefix, the higher frequency it has FP-Tree Generation from the P-lhe From the definition of the P-tree, we can observe that an FP-tree is a sub-tree of the P-tree with a specified support threshold which contains those frequent items that meet this threshold and hereby excludes infrequent items We will propose an algorithm and analyze it in this section 4.1 Algorithm After the generation of the P-tree, we can easily get the frequent item list given a specific support threshold All we need to do is to get rid of those infrequent items from item frequency list L Next, we prune the P-uee to exclude the infrequent nodes by checking the frequency of each node along the path from the root to leaves Because the fre quency of each node is not less than that of its children or descendents we delete the node and its subtrees at the same time if it is infrequent Algorithm 2 FP Generation from lhe P-lkee Input A P-tree P the frequency list L the support threshold 6 Output An FP-tree I Frequent Item List FIList t r 2 For each item i in L If ifrequency 2 C Add i 10 Ntirr 3 Son FlList in frequency descending order 4 Invoke chrck\(P The function check is described ar follows Function check\(M BEGIN FOR each child c of the node N IF c t Fltisr check\(c0 Delete c and the possible subrree starting from e THEN ELSE ENDIF ENDFOR RETURN END 4.2 Analysis In practice we can compare the user-defined minimum support threshold with the occurrence recorded in the item frequency list So the pruning could he done according to the following two rules 1 If the minimum support threshold is higher than the occurrence of most items then we can check the items along the path beginning from the mot as mentioned in Section 3.1 Once an infrequent item is found, its subtree including itself is deleted from the pattern tree 2 When the occurrence of most items is above the mini mum support threshold we can check the items along the path beginning from the leaves, the inverse order with the first rule As long as a frequent item is found we keep it and prune its subtree Regardless of which rule is applied, the algorithm checks at most half amount of items in a pattern tree In the mining process, the users always need to adjust the support thresh olds to achieve an appropriate one If the support thresh old is set too high the process may produce fewer frequent items and some important rules can not he generated On the other hand if the support threshold is set too low the process may produce too many frequent items and some rules may become meaningless One advantage of our ap proach is that we can easily get different FP-trees corre sponding to different support thresholds. When the support threshold is changed no further database scans are needed 5 Updating the Pattern Tree with New Data One concern with the P-tree is how to update it with new data In this section we will propose an algorithm to solve the problem and illustrate the process with an example As the database can always be updated how to update the old rules is an important problem in data mining There 631 


are two ways to update an FP-tree. One is to apply the con struction algorithm to the new database i.e scan the up dated database twice In this case, the previous two scans of the old database are discarded The other is to set \223a va lidity support threshold called watermark in 21 The wa termark goes up to exclude the originally infrequent items while their frequency goes up But it may need to go down since the frequency of frequent items may drop when more and more transactions come in This solution can\222t guar antee the completeness of the generated association rules With new information the originally infrequent items may become frequent and vice versa Since we can generate the P-tree by scanning the database only once we are also able to update the P-tree by one scan of new data without the need for two scans of the existing database and the second scan for the new data We can first insert the new transactions into the P-tree according to the item frequency list and meanwhile update the list Then a new P-tree can be restructured according to the updated item frequency list In the case there comes a new item which does not appear in the existing database we can assume its support is 0 and append it as a leaf node 5.1 Algorithm Algorithm 3 P-Tree Updating Input The onginal P-tree PI the onginal item frequency list L and a new transaction dahbase DE\222 Note that with B compact format the orig inal P-tree PI conhins all items in the exisling transaction database no matter whether or not they ace frequent Output Updated pattern tree P2 Sfep I Expand PI using vew dnro and meanwhile updore L I Fer each transaction Tin he new transaction dambase DE\222 a Son T according to the original frequency list 1 h Inssn\(T PI c Update L wifh items in T 2 Sort L in frequency descending order Srep 2 Remumre fhr expanded P-lree PI im P2 according lo the up dofed L I P2 e Root 2 For each path pi in PI Until pi  4 do a Let s be the common suppn of each item in p b Gel a sub-path pi from pi with Ule common suppon for every c Son p according lo L d Insm\(p PI e pi tpj pi item 5.2 Analysis The most difficult problem concerning the FP-tree is to handle updates in the database. Once some new transactions are added a new FP-tree has to be constructed to deal with these changes The main advantages of the above algorithm in Section 5.1 are 1 2 3 6 There is no further need to scan the existing database because the original P-tree is already a compact ver sion Thus the algorithm makes updating the P-tree more efficient by reusing the old computations on the original database We need to scan the new data only once According to 2 an FP-tree is obtained by two scans of the entire database, including the existing and new database In the worst case the cost of our algorithm is still O\(m  n where 711 is the maximum length of trans actions and ri the number of the transactions in the database Tests and Results We have performed experiments with multiple FP-tree generation and FP-tree updating while new data are added Our test results show that the P-tree method outperforms the FP-tree method by an factor up to an order of magnitude in large datasets The test environment test databases and de tailed results are omitted in this paper due to size restrictions and can he found in 41 7 Conclusions We have proposed a new data structure pattern tree or P tree, and discussed how to obtain the P-tree by one database scan and how to update the P-tree by one scan of new data Moreover we have addressed how to get the correspond ing FP-trees from the P-tree with different user-specified thresholds and also the completeness property of the P-tree We have implemented the P-tree method and presented the test results in 4 showing that our method always outper forms the FP-tree method The key point of our method is to make best use of the P-tree structure which presents a large database in a highly condensed format and avoids the second database scan References l M:Y Chen I Han and P Yu Data Mining An Ovewiew from a Database Perspective IEEE Transmiions on Knowl edge and Data Engineering S\(6 866-883 1996 2 1 Han J Pei and Y Yin Mining Frequent Patterns Without Candidate Generation Proc of ACMlnr ConJ on Mamge menr of Dora SICMOD 1-12,2000 3 R Agarwal C Agganual and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Disrribured Compuring 61\(3 350 371,2001 I41 H Huang X Wu and R Relue Association Anal ysis with One Scan of Databases Universiry of Ver mont Compurer Science Technical Report CS-02-3 2002 http://www.cs.uvm.edu/tr/CS-02-03.shtml 632 


and 4177 records. Each attribute is quantitative except for the attribute sex Attribute sex is denoted as io  the quantitative attributes including length, diameter, height whole weight shucked weight viscera weight shell weight and rings are respectively denoted as i,;..,i Nine attributes are partitioned into three fuzzy sets large middle and small these fuzzy sets are denoted as im\(l 2 3 0,1  8 Table 1 shows some original parameters of triangular fuzzy numbers Table 1 Some original parameters attributes large middle length 0.50,0.63,1.2 0.34,0.50,0.63 diameter 0.39,0.5,0.97 0.25,0.39,0.5 height 0.13,O 18,1.67 0.09,O. l4,O. 18 whole weight 0.96,1.56,4.622 0.36,0.93,1.52 shucked weight 0.42,0.69,2.38 0.16,0.41,0.67 viscera weight 0.21,0.34,1.22 0.08,0.22,0.34 shell weight 0.28,0.45,1.59 0.1 1,0.27,0.44 rings 9.8,16.29,44.67 6.84,10.52,16.33 We use the former eight attributes i,,...,i to predict the attribute i rings which denotes the age of abalone In order to make prediction it is required to mine fuzzy association rules with such as If io is io  and i is i,\(I then i is is 1,2,3 m=0,1,..,,8 We randomly select 2784 records in database as the training samples the rest as the testing samples We first apply 300 fuzzy association rules to make prediction directly and get the mean linear error 3.634351 on the training samples and mean linear error 3.771415 on the testing samples Let T=lOOO L=61 P,=0.6 P,=O.I a=0.35 b=5 using the prediction method in section.3 we get mean linear error 1.659679 on the training samples and mean linear error 1.770291 on the testing samples Table 2 shows some new parameters of triangular fuzzy numbers Table 2 Some new parameters attributes large middle length 0.54,0.8,0.99 0.34,0.55,0.68 diameter 0.4,O.S 1,1.01 0.23,0.33,0.51 height 0.1 1,0.17,1.49 0.07,0.13,0.19 whole weight 1.02,2.65,4.15 0.2,0.71,1.62 shucked weight 0.39,0.59,2.08 0.23,0.40,0.64 viscera weight 0.19,0.54,1.03 0.02,0.16,0.3 shell weight 0.27,0.37,1.322 0.09,0.26,0.5 1 rings 10.08,17.33,38.7 8.39,13.1,18.9 The experimental results show that it is effective to improve prediction precision by adjusting triangular fuzzy numbers of fuzzy partition with genetic algorithm, which makes it possible to make an effective prediction of the mined fuzzy association rules Table 3 shows the prediction precisions at the different number of fuzzy association rules Although the prediction precision is not perfect and should be improved farther we claim that the prediction precision is acceptable in this high-dimensional system with nine attributes Table 3 The prediction precisions Number of fuzzy Training mean Testing mean association rules linear error linear error 300 1.659679 1.770291 100 1.752211 1.840357 60 1.814739 1.915229 40 1.952426 2.048995 We mine 300 fuzzy association rules and let T=lOOO L=61 Pc=0.6 P,=O.I a=0.35 b=5 Ae=2  using the prediction method in section 4 we get prediction precision 72.5575 on the training samples and testing precision 71.0696 on the testing samples. At the same time we use the results obtained from the prediction method in section 3 we get prediction precision 72.1624 on the training samples and testing precision 70.5671 on the testing samples This shows that if we allow a little error between prediction value and actual value the prediction method in section 4 is more flexible and effective than the prediction method in section 3 6 Conclusions Quantitative attributes are partitioned into several fuzzy sets by fuzzy e-means algorithm and search technology of Apriori algorithm is improved to discover interesting fuzzy association rules We present the first prediction method of fuzzy association rules and analyze the shortcoming of this prediction method Last we present the second prediction method of fuzzy association rules with the variable threshold and compare the two prediction methods In the second prediction method a little error between prediction value and actual value is allowed When the error is less than a given threshold prediction value is regarded as acceptable or rational The results of experiment show that the second prediction method can obtain the different prediction precision corresponding to the different error threshold chosen by the users so it is more flexible and effective than the first prediction method 102 


References I R Agrawal T Imieliski and A Swami 223Mining association rules between sets of items in large databases\224 Proc of ACM SIGMOD Conference on Management of Data Washington DC 1993 pp.207-2 16 z R Agrawal R Srikant 223Fast algorithms for mining association rules\224 Proc of the 1994 International Conference on Very Large Databases Santiago Chile 1994, pp.487-499 131 R Agrawal J C Shafer 223Parallel mining of association rules design implementation and experience\224 Special Issue on Data Mining IEEE Transactions on Knowledge and Data Engineering 1996,8\(6\pp.962-969 141 Lu Jianjiang 223Research on algorithms of mining association rules with weighted items\224 Journal of Computer Research and Development 2002,39\(10 pp.1281-1286 in Chinese 151 R Srikant R Agrawal 223Mining quantitative association rules in large relational tables\224 Proc of the ACM SIGMOD Conference on Management of Data Montreal, Canada 1996 pp.1-12 61 M.K Chan F Ada, and H.W. Man 223Mining fuzzy association rules in database\224 SIGMOD Record 1998,27\(1\41-46 171 Lu Jianjiang Qian Zuoping Song Ziling 223Application of normal cloud association rules on prediction\224 Journal of Computer Research and Development 2000 37\(1 I 1317-1320 \(in Chinese SI Lu Jianjiang Song Zilin Qian Zouping 223Mining linguistic valued association rules\224 Journal of Soflare 2001,12\(4 pp.607-611 in Chinese 191 B Lent A Swami J Widom 223Clustering association rule\224 Proceedings of the International Conference on Data Engineering Birmingham England 1997 pp.220-23 1 IO B Liu W Hsu Y Ma 223Integrating classification and association rule mining\224 Proceedings of the International Conference on Knowledge Discovery and Data Mining New York 1998, pp.80-86 I I C.Z Janikow 223Fuzzy decision trees Issues and methods\224 IEEE Trans on Systems Man and Cybernetics  Part B Cybernetics 1998,28\(1\pp.1 14 1121 W.H Au K.C.C Chan 223Classification with degree of membership A fuzzy approach\224 Proc of the 1st IEEE Int\222l Con on Data Mining San Jose CA 2001 pp.35-42 1131 Zou Xiaofeng Lu Jianjiang Song Zilin 223Classification system based on fuzzy class association rules\224 Journal of Computer Research and Development 2003,40\(5 65 1-656 \(in Chinese 141 R.J Hathaway J.W Davenport and J.C Bezdek 223Relational dual of the c-means algorithms\224 Pattern Recognition 1989,22\(2 pp.205-212 IS Huang Chongfu Tanslated 223Fuzzy engineering\224 Xian Xi\222an JiaoTong Uniuersiq Press 1999 in Chinese I161 Z Michalewicz, \223Genetic algorithms  data structure  evolution programs\224 Springer- Verlag, Newyork 1994 103 


not share an y itemsets with the b oundary of an y itemset X k 2X whic hisac hild of X  In other w ords for eac h c hild X k 2X of X w e remo v e from F  X c  all mem ber itemsets in F  X k c   Then these pruned b oundaries ma ybe used in order to generate the rules The resulting algorithm is illustrated in Figure 6 This algorithm uses as input the itemsets X whic h are generated in the 014rst phase of the algorithm at the appropriate lev el of minsupp ort  The algorithm FindBoundary of Figure 5 ma y b e used as a subroutine in order to generate all the b oundary itemsets These b oundary itemsets are then pruned and the rules are generated b y using eac h of the itemsets corresp onding to the b oundary in the an teceden t 4.1 Rules with constrain ts in the an teceden t and consequen t It is easy enough to adapt the ab o v e rule generation metho d so that particular items o ccur in the an teceden t and/or consequen t Consider for example the case when w e are generating rules from a large itemset X  Supp ose that w e desire the an teceden t to con tain the set of items P and the consequen t to con tain the set of items Q W e assume that P  Q 022 X  W e shall refer to P as the ante c e dent inclusion set  and Q as the c onse quen t inclusion set  In this case w e need to rede\014ne the notion of maximalit y and b oundary itemsets A v ertex v  Y  is de\014ned to b e a maximal ancestor of v  X  at con\014dence lev el c an teceden t inclusion set P  and consequen t inclusion set Q if and only if P 022 Y  Q 022 X 000 Y  S  Y  S  X  024 1 c  and no strict ancestor of Y satis\014es all of these constrain ts Equiv alen tl y  the b oundary set con tains all the itemsets corresp onding to maximal ancestors of X  It is easy to mo dify the algorithm discussed in Figure 5 so that it tak es the an teceden t and consequen t constrain ts in to accoun t The only di\013erence is that w e add an un visited v ertex v  T  to LIST if and only if S  T  024 S  X  c  and T 023 P  Also a v ertex v  R  is added to BoundaryList  only if it satis\014es the mo di\014ed de\014nition of maximalit y  5 Generation of the adjacency lattice In this section w e discuss the construction of the adjacency lattice The pro cess of constructing the adjacency lattice requires us to 014rst 014nd the primary itemsets There are t w o main constrain ts in v olv ed in c ho osing the n um ber of itemsets to prestore 1 Memory Limits In order to a v oid I/O one ma y wish to store the primary itemsets and corresp onding adjacency lattice in main memory  1 Recall that Theorem 2.1 c haracterizes the size required b y the adjacency lattice for this purp ose Assume that w e desire to 014nd N itemsets Note that b ecause of ties in the supp ort v alues of the primary itemsets supp ort v alues ma y not exist for whic h there are exactly N itemsets Th us w e assume that for some slac kv alue N s w e wish to 1 Storing the adjacency lattice on disk is not suc h a bad option after all The total I/O is still prop ortional to the size of the output rather than the n um b er of itemsets prestored Recall that the graph searc h algorithms used in order to 014nd the large itemsets and asso ciation rules visit only a small fraction of the v ertices in the adjacency lattice F unction NaiveFindThr eshold\(Numb er ofIt emset s N Slack N s  b egin High  max i f Supp ort of item i g Low 0 Gener ated 0 while  Gener ated 62  N 000 N s N  b egin Mid  High  Low   2 Gener ated  DH P  Mid  end return Mid  end Algorithm ConstructL attic e\(Numb er ofItem sets N Slack N s  b egin p  NaiveFindThr eshold\(N N s  F or eac h itemset X  f i 1 i r g with S  X  025 p do Add the v ertex v  X  to the adjacency lattice with lab el S  X  Add the edge E  X 000f i k g X  for eac h k 2f 1 r g end Figure 7 Constructing the adjacency lattice 014nd a primary threshold v alue for whic h the n um ber of itemsets is b et w een N 000 N s and N  2 Prepro cessing Time There ma y b e some practical limits as to ho wm uc h time one is willing to sp end in prepro cessing Consequen tly ev en if it is not p ossible to 014nd N itemsets within the prepro cessing time it ough t to b e able to terminate the algorithm with some v alue of the primary threshold for whic h all itemsets with supp ort ab o v e that v alue ha v e b een found A simple w a y of 014nding the primary itemsets is b y using a binary searc h algorithm on the v alue of the primary threshold using the DHP metho d discussed in Chen et al as a subroutine This metho d is somewhat naiv e and simplistic and is not necessarily e\016cien t since it requires m ultiple executions of the DHP metho d This metho d of 014nding the primary threshold is discussed in the algorithm NaiveFindThr eshold of Figure 7 The time complexit y of the pro cedure can b e impro v ed considerably b y utilizing a few simple ideas 1 It is not necessary to execute the DHP subroutine to completion in eac h and ev ery iteration F or estimates whic h are lo w er b ounds on the correct v alue\(s of the primary threshold it is su\016cien t to terminate the procedure as so on as N or more large itemsets ha v e b een generated at the lev el of supp ort b eing considered 2 It is not necessary to start the DHP pro cedure from scratc h in eac h iteration of the binary searc h pro cedure It is p ossible to reuse information b et w een iterations Let I  s  denote the itemsets whic hha v e supp ort at least s  It is p ossible to sp eed up the preprocessing algorithm b y reusing the information a v ailable in I  Low  Generating k itemsets in I  Mid  is only a matter of pic king those k itemsets in I  Low  whic h ha v e supp ort at least Low  This do es not mean that ev ery itemset in I  Mid  can b e immediately generated using this metho d Recall from 1 ab o v e that the DHP algorithm is often terminated b efore completion if more than N itemsets ha v e b een generated in that iteration Consequen tly  not all itemsets in I  Low  ma ybea v ailable but only those k itemsets for whic h k 024 k 0  for some k 0 are a v ailable Th us w eha v e all 


 0 1 2 3 4 5 6 7 8 9 x 10 4 0 0.002 0.004 0.006 0.008 0.01 0.012 0.014 0.016    Primary threshold Number of itemsets prestored T10.I4.D100K  T10.I6.D100K  T20.I6.D100K  Figure 8 Threshold v aration with itemsets prestored DataSet Conf Sup DHP Online T10.I4.D100K 90 0  3 100 sec instan taneous T10.I6.D100K 90 0  3 130 sec instan taneous T10.I6.D100K 90 0  2 240 sec 2 seconds T20.I6.D100K 90 0  5 100 sec instan taneous T able 3 Sample illustration s of the order of magnitude adv an tage of online pro cessing those k itemsets in I  Mid  v ailable for whic h k 024 k 0  These itemsets need not b e generated again 6 Empirical Results W e ran the sim ulation on an IBM RS/6000 530H w orkstation with a CPU clo c k rate of 33MHz 64 MB of main memory and running AIX 4.1.4 W e tested the algorithm empirically for the follo wing ob jectiv es 1 Prepro cessing sensitivit y The prepro cessing tec hnique is sensitiv e to the a v ailable storage space The larger the a v ailable space the lo w er the v alue of the primary threshold W e tested ho w the primary threshold v alue v aried with the storage space a v ailabili t y W e also tested ho w the running time of the prepro cessing algorithm scaled with the storage space 2 Online pro cessing time W e tested ho w the online pro cessing times scaled with the size of the output W e also made an order of magnitude comparison b et w een using an online approac h and a more direct approac h 3 Lev el of redundancy W e tested ho w the lev el of redundancy in the generated output set v aried with user sp eci\014ed lev els of supp ort and con\014dence W e sho w ed that the lev el of redundancy in the rules is quite high Th us redundancy elimination is an imp ortan t issue for an online user lo oking for compactness in represen tation of the rules 6.1 Generating the syn thetic data sets The syn thetic data sets w ere generated using a metho d similar to that discussed in Agra w al et al Generating the data sets w as a t w o stage pro cess 0 1 2 3 4 5 6 7 8 9 x 10 4 0 2 4 6 8 10 12 14 16 18 x 10 4    Number of itemsets prestored Relative Computational Effort for preprocessing T10.I4.D100K  T10.I6.D100K  T20.I6.D100K  Figure 9 Computation v ariation with itemsets prestored 0 5000 10000 15000 0 10 20 30 40 50 60    Number of rules generated Response Time in seconds T10.I4.D100K  T10.I6.D100K  T20.I6.D100K  Figure 10 Online resp onse time v ariation with rules generated 20 30 40 50 60 70 80 90 100 0 2 4 6 8 10 12   Support fixed at 0.15 Confidence Total Rules Generated Essential Rules  T10.I4.D100K  T10.I6.D100K   Figure 11 Redundancy lev el v ariation with con\014dence 


 0.1 0.15 0.2 0.25 0 10 20 30 40 50 60 70 80 90   Confidence fixed at 90 Support Total Rules Generated Essential Rules  T10.I4.D100K  T10.I6.D100K   Figure 12 Redundancy lev el v ariation with supp ort 1 Generating maximal p oten tially large itemsets The 014rst step w as to generate L  2000 maximal p oten tially large itemsets These p oten tially large itemsets capture the consumer tendencies of buying certain items together W e 014rst pic k ed the size of a maximal p oten tially large itemset as a random v ariable from a p oisson distribution with mean 026 L  Eac h successiv e itemset w as generated b y pic king half of its items from the curren t itemset and generating the other half randomly  This metho d ensures that large itemsets often ha v e common items Eac h itemset I hasaw eigh t w I asso ciated with it whic hisc hosen from an exp onen tial distribution with unit mean 2 Generating the transaction data The large itemsets w ere then used in order to generate the transaction data First the size S T of a transaction w as c hosen as a p oisson random v ariable with mean 026 T  Eac h transaction w as generated b y assigning maximal p oten tially large itemsets to it in succession The itemset to b e assigned to a transaction w as c hosen b y rolling an L sided w eigh ted die dep ending up on the w eigh t w I assigned to the corresp onding itemset I  If an itemset did not 014t exactly itw as assigned to the curren t transaction half the time and mo v ed to the next transaction the rest of the time In order to capture the fact that customers ma y not often buy all the items in a p oten tially large itemset together w e added some noise to the pro cess b y corrupting some of the added itemsets F or eac h itemset I w e decide a noise lev el n I 2 0  1 W e generated a geometric random v ariable G with parameter n I  While adding a p oten tially large itemset to a transaction w e dropp ed min f G j I jg random items from the transaction The noise lev el n I for eac h itemset I w as c hosen from a normal distribution with mean 0.5 and v ariance 0.1 W e shall also brie\015y describ e the sym b ols that w eha v e used in order to annotate the data The three primary factors whic hv ary are the a v erage transaction size 026 T  the size of an a v erage maximal p oten tially large itemset 026 L  and the n um b er of transactions b eing considered A data set ha ving 026 T  10 026 L  4 and 100 K transactions is denoted b y T10.I4.D100K W e tested ho w the primary threshold v aried with the n um b er of itemsets prestored This result is illustrated in Figure 8 The 014gure sho ws that the primary threshold initially drops considerably as the n um b er of primary itemsets increases but it b ottoms out after a while W e also illustrate the v ariation of the computational e\013ort required with the a v ailable storage space in Figure 9 W e note that for the itemset T10.I4.D100K the computational e\013ort required in order to 014nd additional large itemsets after 014nding 20000 itemsets increases considerably with the n um b er of itemsets prestored This is b ecause for this particular data set the a v erage size of a maximal p oten tially large itemset or bask et is only 4 Consequen tly  the total n um b er of p ossible large itemsets is relativ ely limited On the other hand the computational e\013ort for prepro cessing required b y the data sets T20.I6.D100K and T10.I6.D100K is relativ ely similar This sho ws that the computational e\013ort required to 014nd a sp eci\014c n um b er of primary itemsets is more sensitiv e to the size of a t ypical bask et in the data rather than to the size of a transaction W e also tested the v ariation in the online running time of the algorithm with the n um b er of rules generated W e ran the online queries for v arying lev els of input parameters in order to test the correlation b et w een the running time and the n um b er of rules generated This is illustrated in Figure 10 This result is signi\014can t in that it sho ws that the running time of the algorithm increases linearly with the n um b er of rules generated for all the data sets used The absolute magnitude of time required in order to generate the rules w as an order of magnitude smaller than the time required using a direct itemset generation approac h lik e DHP  A brief summary of some sample relativ e 014ndings is illustrated in T able 3 W e also discuss the lev el of redundancy presen t in the rule generation pro cedure Figures 11 and 12 illustrate that the n um b er of redundan t rules is often m uc h larger than the n um b er of essen tial rules The b enc hmark for measuring the lev el of redundancy is referred to as the redundancy ratio and is de\014ned as follo ws Redundancy Ratio  T otal Rules Generated Essen tial Rules 1 Th us when the redundancy ratio is K  then the n um ber of redundan t rules is K 000 1 times the n um b er of essen tial rules The redundancy ratio has b een plotted on the Y-axis in Figures 11 and 12 W e see that in most cases the n um ber of redundan t rules is signi\014can tl y larger than the n um ber of essen tial rules This illustrates the lev el to whic h useful rules often get buried in large n um b ers of redundan t rules Also the redundancy lev el is m uc h more sensitiv e to the supp ort rather than the con\014dence The lo w er the lev el of supp ort the higher the redundancy lev el 7 Conclusions and Summary In this pap er w ein v estigated the issue of online mining of asso ciation rules The t w o primary issues in v olv ed in online pro cessing are the running time and compactness in represen tation of the rules W e discussed an OLAP-lik e approac h for online mining asso ciation rules whic ha v oids redundancy  


Ac kno wledgemen ts W ew ould lik e to thank V S Ja yc handran and Jo el W olf for their extensiv e commen ts and suggestions References  Aggarw al C C and Y uP  S Online Generation of Asso ciation Rules IBM R ese ar ch R ep ort R C 20899  Agra w al R Imielinski T and Sw ami A Mining association rules b et w een sets of items in v ery large databases Pr o c e e dings of the A CM SIGMOD Confer enc e on Management of data pages 207-216 W ashington D C Ma y 1993  Agra w al R and Srik an tR.F ast Algorithms for Mining Asso ciation Rules in Large Databases Pr o c e e dings of the 20th International Confer enc eon V ery L ar ge Data Bases pages 478-499 Septem b er 1994  Agra w al R and Srik an t R Mining Sequen tial P atterns Pr o c e e dings of the 11th Internation al Confer enc e on Data Engine ering pages 3-14 Marc h 1995  Agra w al S Agra w al R Deshpande P  M Gupta A Naugh ton J F Ramakrishnan R and Sara w agi S On the Computation of Multidimensi on al Aggregates Pr o c e e dings of the 22nd International Confer enc eon V ery L ar ge Datab ases pages 506-521  Chen M S Han J and Y uP  S Data Mining An Ov erview from Database P ersp ectiv e IEEE T r ansactions on Know le dge and Data Engine ering V olume 8 Num b er 6 Decem b er 1996 pages 866-883  Dyreson C Information Retreiv al from an Incomplete Data Cub e Pr o c e e dings of the 22nd International Confer enc eon V ery L ar ge Datab ases pages 532-543 Mumbai India 1996  Gupta A Harinara y an V and Quass D Aggregatequery pro cessing in data w arehousing en vironmen ts Pr o c e e dings of the 21st Confer enc eon V ery L ar ge Datab ases Zuric h Switzerland Septem b er 1995  Han J and F u Y Disco v ery of Multiple-Lev el Assocaition Rules from Large Databases Pr o c e e dings of the 21st Internation al Confer enc eon V ery L ar ge Data Bases Zuric h Switzerland 1995 pages 420-431  Harinara y an V Ra jaraman A and Ullman J Implemen ting Data Cub es E\016cien tly  Pr o c e e dings of the 1996 A CM SIGMOD c onfer enc e on Management of Data Mon treal Canada June 1996 pages 205-227  Houtsma M and Sw ami A Set-orien ted Mining for Asso ciation Rules in Relational Databases Pr o c e e dings of the 11th Internation al Confer enc e on Data Engine ering Marc h 1995 pages 25-33  Kaufman L and Rousseeu wP J Finding Gr oups in Data A n Intr o duction to Cluster A nalysis Wiley Series in Probabilit y and Mathematical Statistics 1990  Klemen ttinen M Mannila H Ronk ainen P  T oiv onen H and V erk amo A I Finding in teresting rules from large sets of disco v ered asso ciation rules Pr o c e e dings of the Confer enc e on Information and Know le dge Managements Gaithersburg MD USA 28 No v 2 Dec 1994  Len t B Sw ami A and Widom J Clustering Asso ciation Rules Pr o c e e dings of the Thirte enth International Confer enc e on Data Engine ering pages 220-231 Birmingham UK April 1997  Mannila H T oiv onen H and V erk amo A I Ef\014cien t algorithms for disco v ering asso ciation rules AAAI Workshop on Know le dge Disc overy in Datab ases pages 181-192 Seattle W ashington July 1994  Ng R T and Han J E\016cien t and E\013ectiv e Clustering Metho ds for Spatial Data Mining Pr o c e e dings of the 20th Internation al Confer enc eon V ery L ar ge Data Bases San tiago Chile 1994 pages 144-155  P ark J S Chen M S and Y uP  S An E\013ectiv e Hash Based Algorithm for Mining Asso ciation Rules Pr o c e e dings of the 1995 A CM SIGMOD International Confer enc e on Management of Data pages 175-186 Ma y 1995  Piatetsky-Shapiro G Disco v ery  Analysis and Presentation of Strong Rules Know le dge Disc overy in Datab ases 1991  Sa v asere A Omiecinski E and Na v athe S An E\016cien t Algorithm for Mining Asso ciation Rules in Large Data Bases Pr o c e e dings of the 21st Internation al Confer enc eon V ery L ar ge Data Bases Zuric h Switzerland 1995 pages 432-444  Sh ukla A Deshpande P  M Naugh ton J F and Ramasam y K Storage Estimation for Multidimensi on al Aggregates in the Presence of Hierarc hies Pr o c e e dings of the 22nd Internationa l Confer enc eon V ery L ar ge Datab ases pages 522-531 Mum bai India 1996  Srik an t R and Agra w al R Mining Generalized Asso ciation Rules Pr o c e e dings of the 21st International Confer enc eon V ery L ar ge Data Bases  pages 407-419 Septem b er 1995  Srik an t R and Agra w al R Mining quan titativ e association rules in large relational tables Pr o c e e dings of the 1996 A CM SIGMOD Confer enc e on Management of Data Mon treal Canada June 1996  T oiv onen H Sampling Large Databases for Asso ciation Rules Pr o c e e dings of the 22nd International Conferenc eonV ery L ar ge Datab ases pages 134-145 Mumbai India 1996  Ziark o W The Disco v ery  Analysis and Represen tation of Data Dep endencies in Databases Know le dge Disc overy in Datab ases 1991 


CMP A Fast Decision Tree Classifier Using Multivariate Predictions  449 H Wang and C Zaniolo Mining Recurrent Items in Multimedia with Progressive Resolution Refinement  461 0 Zai'ane J Hun and H Zhu Panel Session 22 Is E-Commerce a New Wave for Database Research Moderator Anant Jhingran IBM T.J Watson Research Center USA Panelists Sesh Murthy IBM T.J Watson Research Center USA Sham Navathe, Georgia Institute of Technology USA Hamid Pirahesh IBM Almaden Research Center USA Krithi Ramamrithan University of Massachusetts-Amherst USA Industrial Session 23 Java and Databases Pure Java Databases for Deployed Applications  477 N Wyatt Database Technology for Internet Applications  700 A Nori Session 24 Association Rules and Correlations Finding Interesting Associations without Support Pruning  489 E Cohen M Datar S Fujiwara A Gionis P Indyk R Motwani J Ullman and C. Yang Dynamic Miss-Counting Algorithms Finding Implication and Similarity Rules with Confidence Pruning  501 S Fujiwara J Ullman and R Motwani Efficient Mining of Constrained Correlated Sets  512 G Grahne L Lakshmanan and X Wang Session 25 Spatial and Temporal Data Analyzing Range Queries on Spatial Data  525 J Jin N An and A Sivasubramaniam Data Redundancy and Duplicate Detection in Spatial Join Processing  535 J.-P Dittrich and B Seeger Query Plans for Conventional and Temporal Queries Involving Duplicates and Ordering  547 G Slivinskas C Jensen, and R Snodgrass xi 


Industrial Session 26 XML and Databases Oracle  The XML Enabled Data Management System  561 S Banerjee V Krishnamurthy M Krishnaprasad, and R Murthy XML and DB2  569 J Cheng and J Xu Session 27 High-Dimensional Data Independent Quantization An Index Compression Technique for High-Dimensional Data Spaces  577 S Berchtold, C Bohm H Jagadish H.-P. Kriegel and J Sander Deflating the Dimensionality Curse Using Multiple Fractal Dimensions  589 B.-U Pagel F Korn and C. Faloutsos Similarity Search for Multidimensional Data Sequences  599 S.-L Lee S.-J Chun D.-H Kim, J.-H Lee and C.-W Chung Session 28 Web-Based Systems WRAP An XML-Enabled Wrapper Construction System for Web Information Sources  611 L Liu C Pu and W. Hun Self-Adaptive User Profiles for Large-scale Data Delivery  622 U Cetintemel M Franklin and C. Giles Industrial Session 29 Main Memory and Small Footprint Databases In-Memory Data Management in the Application Tier  637 The TimesTen Team SQLServer for Windows CE -A Database Engine for Mobile and Embedded Platforms  642 P Seshadri and P. Garrett Join Enumeration in a Memory-Constrained Environment  645 I Bowman and G Paulley xii 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


