Generalized Causal Inductive Reasoning Model Based on Generalized Causal Cellular Automata Bingru Yang Xin Li Wei Song School of Information Engineering University of Science and Technology Beijing Beijing 100083 China E-mail sgyzfr@yahoo.com.cn Abstract-Based on cellular automata theory inductive logic causal model is proposed Then the generalized causal cellular automata which can deal with both random uncertainty and fuzzy uncertainty is proposed by integrating the inductive logic causal model and language field theory Based on the generalized causal cellular automata the 
generalized causal inductive reasoning model is presented Comparisons with ordinary fuzzy reasoning are listed Finally the applications of proposed methods are discussed in brief Keywords language field theory generalized cellular automata generalized inductive logic causal model I INTRODUCTION When we construct the reasoning rule of expert system the major rules are from experience of fmite facts Aiming at the random uncertainty of inductive reasoning A W Burks established cellular automata theory which 
combine probability causality and inductive reasoning 
 2 However fuzzy information is another uncertainty that should be considered in the research of inductive reasoning Dealing with random uncertainty and fuzzy uncertainty together is the necessary condition for constructing complete cellular automata and inductive logic causal model In view of Burks cellular automata and based on our language field theory we presented generalized cellular automata and generalized inductive logic causal model and the corresponding generalized 
causal inductive reasoning model which can simultaneously deal with both random uncertainty and fiuzzy uncertainty to certain extents The proposed model and method are suitable for complex system and have been applied to intelligent air conditioner and evaluation of association rules II INDUCTIVE LOGIC CAUSAL MODEL THE NEW DESCRIPTION OF CELLULAR AUTOMATA To introduce the logic causal model we first discuss the cellular automata The formalized description of cellular automata is shown as follows 
Definition 1 In discrete Euclidean spatial-temporal condition II  U T E q7 is called the cellular automata where U is state or changing state space element of U is called state T is the time queue element of T is called time E is the set of cells element of E is called the cell 77  PI  p2  I is the set of mapping and f  E x T  U is called mapping Definition 2 c ran pi 
x ran mpj is called necessary causal 3eEE Vt E T relation p,\(N\(e  pj\(e t  determined by the above formula is called necessary causal regulation Definition 3 H  U  is called causal cellular automata if qp N\(e t pj N\(e t   which is necessary causal regulation satisfies the following conditions 1 Finite changing principle The necessary causal regulation is based on finite set that can 
describe the characters of all the spatial-temporal regions 2 Existence principle of causality Regulations can not only be applied in certain region but also most regions of the automata 3 Consistency of causality This principle is suitable for the whole cellular automata Definition 4 X  S II is called inductive logic causal model if 1 S  SaaSj   SM  is the possible causal world decided by necessary causal regulation where 
Sa is the real causal world Si  Vil,Vi2  IIV are history that constitute Sj  and every history is different spatial-temporal world 2 Hl is the cellular automata defined in defmition 3 and every possible causal world is described by corresponding cellular automata The above-mentioned model lays foundation for the research of causality of uncertain random information system Furthermore we use language field and language 0-7803-9422-4/05/$20.00 02005 IEEE 375 


value structure which can help the study of fuzzy uncertainty and fuzzy state or changing state III LANGUAGE FIELD AND LANGUAGE VALUE STRUCTURE The language field and language value structure are used for enhancing the cellular automata with the ability to describe fuzzy uncertainty In this section we introduce language field and language value structure in brief 3 Definition 5 In the corresponding basic variable region of language variable the dots in the middle of every overlapping subinterval like S  and its adjacent land 243  6 is generally the allowed error value are called standard samples dots the value interval  e S  e  taken is called standard values any other dots are called nonstandard samples dots they make up standard sample space and nonstandard sample space respectively Definition 6 In standard structure of state description u C  D,I N N is called language field if 1 D is the set of all overlapping intervals of basic variable on R 2 N  s is a finite set of language value 3 N is a complete ordering on N 4 I N  D is a standard value mapping and satisfies isotonicity Definition 7 In standard structure of state description U for the language field C  D,I,N,.<N F  C W K is a language value structure of C if 1 C satisfies definition 6 2 K is a natural number 3 W N  RK  it satisfies the following Vn1 n2 E N\(n1 N n2 4 W\(n1 c W\(n2  Vnl n2 E N\(n1  n2 W\(n W\(nn in which d is a lexicographic order on RK In Fuzzy state description standard structure U  when R is defimed to 0 1  Definition 6 and Definition 7 defines Fuzzy language field and language value structure respectively  Theorem 1 Expansion theorem Given two language fields C1 and C2 C is an expansion of C2 if and only if C and C2 are the same type language field it means IN IN21 The proof can be found in 3 Theorem 2 disisomorphism theorem Suppose that F is a language value structure of C then F and Fdouble the double expansion of F are disisomorphism under the weighting Hamming distance The proof can be found in 3 IV GENERALIZED CAUSAL CELLULAR AUTOMATA AND GENERALIZED CAUSAL INDUCTIVE MODEL By combining the above-mentioned inductive logic causal model and language field theory we propose the generalized causal cellular automata and generalized causal inductive reasoning model We hope they can deal with both random uncertainty and fuzzy uncertainty at the same time Definition 8 In discrete Euclidean spatial-temporal condition under the transformation from real fuzzy space to fiuzzy language field 11  C F T E 4 is called generalized cellular automata where C is the language field corresponding to fuzzy state space F are the set of language values corresponding to different fuzzy states in C T is time sequence element of T is called time E is the set of cells element of E is called the cell i.e space range   VI,V2  is left compound mapping set its element Vlj  Wo 1j that is the state of cell e at time t ascertained by endued state mapping Oj and describe the state with corresponding language value then through the mapping W in language value structure the k-dimensions vector discrete type express state can be ascertained Definition 9 H1  H  is called generalized cellular automata if OP N\(e t  j N\(e   which is necessary causal regulation satisfies the following conditions besides 1 3 in definition 3 1 Causal state principle In the process of continuous and gradually-changing causality for arbitrary sample all the possible states of cell e at time t are certainly conduced by positive such as small of language value and negative such as not small of language value including various possibilities of neighborhood of cell e N\(e at time t 2 Transformation principle between state and changing state When the language fields of state and changing state of cause and effect are isomorphic the principles which are suitable for state are also suitable for changing state vice versa Definition 10 Generalized inductive logic causal model is the semantic structure X  S H satisfies the following conditions 1 S  Sa'Sl SM}m Si l 2  M is possible causal world governed by necessary causal regulation and relative theories of 4 Sa is real causal world S7  Vil,V  Vi are history that constitute Sj and every history is different spatial-temporal world 2 fl is the generalized causality cellular automata defined in definition 9 and every possible causal world is described by generalized causality cellular automata 376 


V GENERALIZED CAUSAL INDUCTIVE REASONING MODEL BASED ON GENERALIZED CELLULAR AUTOMATA Based on generalized cellular automata and generalized inductive logic causal model the generalized causal inductive reasoning model is proposed In generalized inductive causality model suppose S is effect and A B C etc are causes When using generalized causal cellular automata to describe state or changing state relation of standard sample space at time t we first get the language value of various states or changing states of causality and the corresponding vector expression For example A has these five language values vary very little  vary little  vary moderately vary large vary very large and the corresponding vector 4  ai bi ci di e3 i  1,2,3,4,5 is called standard state vector of A at time t Definition 11 In standard sample space suppose that A\(i expresses the state or changing state of reason A at time t and SWX expresses the standard vector of state or changing state of result S at time t then the necessary causal regulation p A,t e p S,t is given out by relation matrix with fuzzy and random state or changing state that is q A,t   S,t H E At  x AY where C\(H,E is the inductive affirmation function it indicates that the support of evidence E to hypothesis H that is necessary causal regulation The inductive confirmation function of hypothesis H is the ratio of norm of multiply of the two examination matrixes C\(H,E  I ISEI I  IIAEI I  Under the construction of generalized inductive logic causal model in possible causal world taking state or changing state matrix with causal relation information as background the inductive reasoning pattern to get all the possible state or changing state of result S can be conduced by reason A in some state small precondition is S A>o\(M0+Mj j=1,2  Major inductive premise set of matrix premise reason variable Inductive conclusion set of result vectors Under the construction of generalized inductive logic causal model in the possible casual world according to the state or changing state type belong to reason state or changing state input vector and the type of local big precondition such as A,\(1 o t S,t\(l  we can find the unique knowledge matrix M matching with it through self-organize mode in basic knowledge base Taking M as the background big precondition in order to obtain the state conclusion of result S which can be induced by reason A at a  its automatic reasoning mode is Major premise a S o Mc Minor premise Conclusion That is conclusion S may be gained through twice synthesis rule Above-mentioned reasoning model is based on single language field Obviously it can also fit for compound language field defined above VI COMPARISONS AND APPLICATIONS According to the above discussion we can find that the difference between the proposed reasoning method and general fiuzzy reasoning 4 lies in 1 The proposed reasoning method deals with random uncertainty fuzzy uncertainty and qualitative information 2 The proposed reasoning method can deal with monotonous and non-monotonous reasoning 3 The proposed reasoning method can be executed in parallel way 4 The proposed reasoning method can present quantitative explanation of reasoning results 5 The proposed reasoning method is more suitable for causal reasoning The proposed generalized causal cellular automata and generalized causal inductive reasoning model can deal with random uncertainty and fuzzy uncertainty at the same time Thus these kinds of model and method are suitable for complex system Now we list some applications of them in brief Firstly the generalized inductive model can solve the problem of causal disturbance to certain extents 3 ThUS multi variables of complex problem can be considered together The model has been applied to intelligent air conditioner controlling which deals with temperature humidity and speed of air etc simultaneously 5 By adding this kind of model the air conditioner is enhanced with the features of self-adaptation self-organization etc Secondly association rules are one of the major techniques of data mining 6 Association analysis is the 377 11 M  MO  q10'\(A t 9 l pjlol S P Ml  pi\(ol A t e b pjlll S tv  M  ipilol A t e 0 pljnl S P 


discovery of association rules showing attribute-value conditions that occur frequently together in a given set of data Association rule is widely used for market basket analysis Since the association rule analysis usually produces large volume of rules which makes it difficult for people to fmd the interesting pattern So evaluation of association rules is of importance Existing evaluation methods are usually based on raw information but not knowledge By using the proposed causal reasoning model knowledge based on raw data can be got and can help us find the most interesting rules based on both raw data and discovered knowledge 17 VII CONCLUSIONS AND FURTHER WORK Based on cellular automata theory inductive logic causal model is proposed Then the generalized causal cellular automata which can deal with both random uncertainty and frizzy uncertainty is proposed by integrating the inductive logic causal model and language field theory Based on the generalized causal cellular automata the generalized causal inductive reasoning model is presented Comparisons with general fuzzy reasoning are also listed Finally the applications of proposed methods are discussed in brief Bioinformatics encompasses a study of inherent genetic information underlying molecular structure resulting biochemical function and the exhibited phenotypic symptoms And it offers new opportunity for computer researchers In our future work we plan to explore the proposed model and reasoning method in Bioinformatics to uncover some interesting complex relationship ACKNOWLEDGMENT This research was supported by Chinese National Science Foundation under grants 69835001 Ministry of Education China under grants 2000 175 and Science Foundation of Beijing under grants 4022008 REFERENCES 1 A W Burks and H Wang The logic of automata part I Journal of the ACM vol.4 no 2 pp.193-218 April 1957 2 A W Burks and H Wang The logic of automata part II Journal of the ACM vol.4 no.3 pp.279-297 July 1957 3 B R Yang FIA and CASE based on fuzzy language field Fuzzy Sets and Systems vol 95 no.1 pp.83-89 1998 4 W Siler and J J Buckley Fuzzy Expert Systems and Fuzzy Reasoning New York Wiley Publisher 2004 5 B R Yang Controlling Methods and Devices of Combined Intelligent Air Conditioner Chinese Innovation Patent ZL94105764.X 1994 6 R Agrawal and R Srikant Fast algorithms for mining association rules Proceedings of the 20th International Conference on Very Large Data Bases Santiago Chile 1994 pp.487499 7 B R Yang and Y X Qi An evaluation method for causal association rules in KDD Journal of Software vol 13 no.6 pp 1 142-1147 2002 378 


how these maps could he used to uncover seed points; noise and outliers; and boundary points A. Identijfving Seed Points In applications involving huge amounts of data,"Seed points" identification may be needed to reduce the complexity of data-driven algorithms. This process can be extremely difficult, if not impossible, for data with uneven, noisy, hetero geneous distributions. In addition to sampling, seed points can offer excellent initialization for techniques that are sensitive to the initial parameters such as clustering Using the PossibilisticMap, seed points can be identified as points with high typicality, where typicality is defined as BB w I*, m Dlln i B. Identijfving Noise Points and Outliers Noise and outlier detection is a challenging problem. In fact when data has a large, unknown proportion of outliers, most learning algorithms break down and cannot yield any useful solution. Using the PossibilisticMap, Noise and outliers can be identified as those oints located of xj in the PossibilisticMap, then xj is a noise point if llxjpll is small C. Identihing Boundary Points The process of boundary points identification is paramount in many classification techniques. It can be used to reduce the training data to \(or emphasize to achieve higher focus in learning class boundaries. Using the FuzzyMap, boundary points can be identified as points that do not have a strong commitment to any cluster of a given attribute. In other words, they are points that belong to multiple units and have low punty values, where the degree of purity of xj, Pxj, is defined as words, if xp'[{u Pxj = min { m a  {uji 6 M N", ,st4 1" "km i D. Illustrative example Fig.2 displays a simple data set with 4 clusters. First, the projection of the data along each dimension is partitioned into 2 clusters using the FCM [13]. Next, possibilistic la bels were assigned to each xj in all 4 clusters using \(4 In Fig. 2\(h 1150 1 and each point xj is mapped to x,'= uIj \(1 1 2 2 a b 4 Fig. 2. Exploring the MembershipMaps. \(a b different typicality, \(c Txj 20.95 \(black squares and 0.25&lt;Txj&lt;0.50 \(squares with shade that gets lighter As can be seen, points located at the core of each cluster have the highest degree of typicality and could be treated as seeds. The degree of typicality decreases as we move away from the clusters. Points near the origin, i.e., IlxrII is small lt;0.25 Fig.Z\(b identify boundary points, fuzzy labels were assigned to each xj in all 4 clusters using eq. \(3 xf= pit c xi I Pxj &lt; 0.75 are mainly the cluster boundaries E. Discussion 1 The richness of the discovered information comes at the cost of added dimensionality. In fact, the MembershipMap can significantly increase the dimensionality of the feature space, especially if the data has a large number of clusters However, the cost of increasing the dimensionality can be offset by the significant benefits that can be reaped by putting this knowledge to use to facilitate Data Mining tasks. For example, we have shown how Fuzzy and Possibilistic Maps can easily identify seed, border, and noise points. If taken into account, these samples can be used to obtain a significantly reduced data set by sampling, noise cleaning, and removing border points to improve classicluster separation. This cleaning 


border points to improve classicluster separation. This cleaning can facilitate most subsequent supervised and unsupervised learning tasks. It is also possible, after the cleaning, to go back to the original feature space. Thus, benefiting both from lower dimensionality and lower data cardinality The cost of the high dimensional MembershipMap can also be offset by the following additional benefits: \( i values are mapped to the interval [O,l]; \(ii tributes can be mapped to numerical labels, thus, data with mixed attribute types can be mapped to numerical labels within O,l]. As a results, a larger set of DM algorithms can be investigated and \(iii interpreted and thresholded in the new space Even though the attributes are treated independently, they do get combined at a later stage. Hence, information such as correlations is not lost in the membenhip space. In fact, treat ing the attributes independently in the initial \(1-D clustering is analogous to traditional feature scaling methods such as 25-29 Ju/y, 2004 * Budapest, Hungary min-max or normalization, except that the MembershipMap approach does not destroy intricate properties of an attribute's distribution using a single global transformation. In a sense our approach applies a specialized local transformation to each from the PossibilisticMap without my knowledge about the true class labels, is higher for those samples located near the m e  class centroid subcluster along an attribute, and is thus non-linear. :~ 1.0  _ 5 .. _ Oe 0.1 VI. EXPERIMENTAL RESULTS L _ -  _ gt We illustrate the performance of the proposed transforma tion using several benchmark data sets'. These are the Iris Wisconsin Breast Cancer, Heart Disease, and the satellite data the 3 classes. The Wisconsin Breast Cancer data has 9 features O P sets. The Iris data has 4 features and 50 samples in each of 0 0  0 5  1 .o 1.5 29 D/slan- fO f r Y B  desa --an  a   and 2 classes, namely, benign and malignant. These 2 classes have 444 and 239 samples respectively. The Heart Disease data has 13 features and 2 classes. The first class, absence of heart disease, has 150 samples. and the second class, presence of heart disease, has 120 samples. The satellite data has 35 partitioned into a training set \(4,435 2,000 information extracted from the Membership maps The first step in the MembershipMap transformation con sists of a quick and rough segmentation of each feature. This is a relatively easy task that can rely on histogram thresholding _ -  _  g o a  -_ features, 6 classes, and a total of 6,435 samples. This data is L = _  I_ _ The ground truth of these sets will he used to validate the 6.4 O B  0 8  Memberohip I" erfYsl51800 b Fig. 3. Validating identified regions of interest or clustering I-D data. The results reported in this paper were obtained using the Self-Organizing Oscillators Network SOON  of finding the optimal number of clusters in an unsupervised way. We have also hied other simple clustering algorithms such as the K-Means, where the number of clusters i s  specified after inspecting the I-D projected data. The results were comparable. Next, fuzzy and possibilistic labels were assigned 


comparable. Next, fuzzy and possibilistic labels were assigned to each sample using equations \(3 4 q=1. Finally, the fuzzy \(possibilistic to create the FuzzyMap \(PossibilisticMap 15 dimensions, the Wisconsin Breast Cancer maps have 50 dimensions, the Heart Disease maps have 39 dimensions, and the Satellite maps have 107 dimensions To validate the degree of purity, we use the ground truth and compute the membership of each sample using eq.\(3 wherc the distances are computed with respect to the classes centroids. Fig. 3\(b versus their membership in the true class. The distribution shows a positive correlation indicating that the purity com puted using the FuzzyMap without any knowledge about the true class labels can estimate the degree of sharing among the multiple classes, i.e, boundary points. Moreover, we notice that all samples from class I have high purity, and the few samples with low purity belong to either class 2 or 3. This information is consistent with the known distribution of the Iris data where class 1 is well-separated while classes 2 and 3 overlap A. Identifiiing Regions of Interest B. Clustering the Membership Maps To identify seed points, noise and outliers for the Iris data This experiment illustrates the advantage of using the Mem- we compute the typicality of each  sample. Points with high bership Maps to improve clustering. We apply the Fuzzy C- typicality would correspond to seed  points, while points with Means to cluster the different data sets in the original space, low typicality would correspond  to noise points. Since the FuzzyMap, and PossibilisticMap. For all cases, we fix the Iris data has 4 dimensions, the  identified points could not be visualized as in Fig. 2. Instead, we rely on the ground truth to Of clustcrj to the actual number  Of classes, use the same random points as initial centers. validate the results. We use the class labels and compute  the centroid of each Then, we compute the distance from To assess the validity of each partition we  use the true class each point to the centroid of its class, Theoretically, points laheis to the purity Of the The  are with small distances would correspond to seeds, and points shown in Table I .  For each confusion  matrix, the column refers Fig, 3\(a  distances to a plot of the sample typicality their distance As can be seen, both maps improve the clustering  results for to the true class centroid. The distribution clearly suggests data we note here that a partition a negative Typicality, which was extracted only of the Iris data could be obtained using other  clustering algorithms and/or mnltinle clusters to renresent each class Available at "hnp: / /~~ . ics .uc i .edui  mlearn/MLRepositoryhhnP However, our goal here is to  illustrate that the Membership 1151 TABLE I CLUSTERINO RESULTS FUZZ-IEEE 2004 that is uncovered, and can even be avoided by going hack to the original feature space after data reduction and cleaning Data Set /I Originalspace 111 FutzyMap 1 1  PossMap Thus, benefiting both from lower  dimensionality and lower I 50 0 0 11 The projection and clustering steps in the MembershipMap are not restricted to the original attributes. In fact, our approach can be combined with other feature reduction techniques For instance, Principal Component Analysis can he used to Breast 422 22 Cancer 19 220 3 236 3 236 117 33 ~ ~~ ~~~ ~~~ I Disease 11 23 97 111 21 99  Eigenvectors, and then the membershir, transformation can he auulied on the Droiected TABLE II CLASSIFICATION RESULTS _ 


_ data in the same way. In fact, one can even project the data on selected 2-D or 3-D subsets of the attributes, and then apply membership transformations. I Original Space [ FuzyMap I PossMap Training I 89.29% 1 88.86% I 93.57 Testing I 85.50% I 86.30% I 87.30% ACKNOWLEDGMENTS This material  is based upon work supported by the National Science Foundation under Grant No. IIS-0133415 Maps can improve the distribution of the data so  that simple clustering algorithms are reliable. REFERENCES C. Classification using the Membership Maps This experiment uses the 36-D satellite data to illustrate the use of the Membership Maps to improve data classification Only the training data is used to learn the mapping. The test data is mapped using the same cluster parameters obtained for the training data. We train a Back Propagation Neural Networks with 36 input units, 20 hidden units, and 6 output units on the original data and two other similar nets with 107 input units, 20 hidden units, and 6 output units on the Fuzzy and possibilistic maps. Table I1 compares the classification results in the 3 spaces. As can he seen, the results on the testing data are slightly better in both transformed spaces. The above results could be improved by increasing the number of hidden nodes to compensate for the increased dimensionality in the mapped spaces. Moreover, the classifications rates could be improved if one exploits additional information that could be easily extracted from the possibilistic and fuzzy maps For instance, by using boundary points for bootstrapping and filtering noise points VII. CONCLUSIONS We have presented a new mapping that facilitates many data mining tasks. Our approach strives to extract the underlying sub-concepts of each attribute, and uses their orthogonal union to define a new space. The sub-concepts of each attribute can be easily identified using simple I-D clustering or histogram thresholding. Moreover, since fuzzy and possibilistic labeling can tolerate uncertainties and vagueness, there is no need for accurate sub-concept extraction. In addition to improving the performance of clustering and classification algorithms by taking advantage of the richer information content, the MembershipMaps could he used to formulate simple queries to extract special regions of interest, such as noise, outliers boundary, and seed points There is a natural trade-off between dimensionality and information gain. Increased dimensionality of the Member shipMap can be offset by the quality of hidden knowledge 1152 I ]  U. Fayyad, F. PiatetskqGhapiro, P. Smyth, and R. Uthurusamy. Ad wnces in Knowledge Discowr,v and Data Mining, MIT Press. 1996 Z] D. Pyle. Doto Preporrnion for Data Mining, Morgan Kaufmann 1999 131 R. N. Shepard  The analysis of proximilies: multidimensional scaling with an unknown distance function I and 11  PsychomePiko. vol. 27 pp. 125-139 219-246. 1962 4] T. Kohonen. Sel/lOqpnizotion and Associative Memoy, Springer Verlag, 1989 5] J. W. Sammon  A nonlinear mapping for data analysis  IEEE Transactions on Computms. vol. 18. pp. 401-409. 1969 6] H. V. Jagadish  A retrieval technique for similar shape  in ACM SICMOD. 1991, pp. 208-217 71 Chnstos Faloutsos and King-Ip Lin  FastMap: A fast algorithm for indexing, dala-mining and visualization of traditional and multimedia datasets  in SICMOD. 1995. 00. 163-174 SI  H. Almkllim and T. G. Dienerich  Learning with many irrelevant feamres  in Ninth National Conf AI, 1991, pp. 547-552 91 K. Kim and L. A.  Rendell  The feahrre selection problem: Traditional methds and a new algonthm  in Tenth Notional Con/ AI, 1992. pp 129-1 34 I O ]  Jyrki Kivinen and Heikki Mannilq  The power of sampling in knowledge discovety  1994. pp. 77-85 I11 R.O.Duda and P. E. Ha&amp; Patfen, Closslficnfion and Scene Analysis John Wiley and Sons. 1973 I21 L. Kaufman and P. R O U S S ~ ~ U W ,  Finding Groups in Data: An lnmducrion 


I21 L. Kaufman and P. R O U S S ~ ~ U W ,  Finding Groups in Data: An lnmducrion to Cluster Ano!vsis, John Wiley and Sons. 1990 I31 J. C. Bezdek, Pattem Recognition with Fury Objective Function Algorithms. Plenum Press, New York, 1981 1141 H.  Frigui and R. Krishapuram  A robust competitive clustering algorithm with applications in computer vision  IEEE Pons. Pan Annlwis Mach. Intell., vol. 21, no. 5. pp. 45M65, 1999 I51 R. Knshnapuram and J. Keller  A possibilistic approach lo clustering   IEEE Trans. Fuzzy Sysfems, vol. 1,  no. 2. pp. 98-110, May 1993 I61 F. R. Hampel, E. M. Roncheni, P. J. Rousseeuw. and W. A. Stahel Robust Stnti.vtics the Appmoch Based on Influence Funoions, John Wiley &amp; Sans, New York 1986 1171 2. Wang and G.  Klir. F u q  measure theo?, Plenum Press. New York 1992 I81 M.B. Rhouma and H. Frigui  Self-organization of a population of coupled oscillaton with application to clustering  IEEE Trans. Pntl Anntvsis Mach. Intell., vol. 23, no. 2. pp. 180-195, 2001 pre></body></html 


   c x n xijnx D x ij t          u    1 0 UU V where D| is the number of transactions in D, we can say that we are c confident that {i, j} isn  t frequent in D Proof: If probability policies are not considered and Sij  1, that is, while a row \(transaction according to the matrix multiplication discussed in the previous section, D'tj will be set to 0. We can view the nij original jtiD jtjtjt ijn DDD 21     f o r  e a c h  r o w  t i  c o n t a i n s   i   j   i n  D  a s  realizations of nij independent identically distributed \(i.i.d om variables ijn X X X      2 1      e a c h  w i t h  t h e  s a m e  d i s t r i b u  tion as Bernoulli distribution, B \(1 X i    1  a n d  t h e  f a i l u r e  i s  d e f i n e d  a s  X i    0     i    1  t o  n i j   T h e  p r o b  ability ? is attached to the success outcome and 1?? is attached to the failure outcome. Let X be a random variable and could be viewed as the number of transactions which include {i, j} in D such that jin     2 1    T h u s   t h e  d i s t r i b u t i o n  o f  X is the same as Binomial distribution, X?B \(nij              otherwise nx x n XP ij xnxij x ij 0 1,0     U the mean of X = nij?, and the variance of X = nij?\(1 If we want to hide {i, j} in D', we must let its support in D' be smaller than ?. Therefore, the expected value of X must be s m a l l e r  t h a n     D u V    T h a t  i s     m u s t  s a t i s f y     D n i j  u  u  V U   Moreover, the probability of the number of success which is 


Moreover, the probability of the number of success which is equal to or smaller than     1     u  D V   s h o u l d  b e  h i g h e r  t h a n  c  Therefore, U must satisfy    c x n xijnx D x ij t           u    1 0 UU V  If ? satisfies the two equations, we can say that we are c confident that {i, j} isn  t frequent in D When nij &gt; 30, the Central Limit Theorem \(C.L.T used to reduce the complexity of the equation and speed up the execution time of the sanitization process Moreover, if several entries in column j of S are equal to  1 such as Sij  1, Skj  1, Smj  1  etc, several candidate Dist ortion probabilities such as iU , kU , mU , etc are gotten. The Dis tortion probability of column j, ? j, is set to the minimal candid ate Distortion probability to guarantee that all corresponding pair-subpatterns have at least c confident to be hidden in D 3.3. Sanitization Algorithm Fig.7: Sanitization Algorithm According to the probability policies discussed above, the sanitization algorithm D'n  m = Dn  m  Sm  m is showed in Fig.7 Notice that, if the sanitization process causes all the entries in row r of D' equal to 0, randomly choose an entry from some j Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE where Drj = 1, and set D'rj = 1. It is to guarantee that the size of the sanitized database equals the size of the original database 4. Performance Evaluation 4.1. Performance Quantifying There are three potential errors in the problem. First of all some sensitive patterns are hidden unsuccessfully. Secondly some non-sensitive patterns cannot be mined from D'. And the third, new artificial patterns may be produced. However, the third condition never happens in both of our approach and SWA Besides, we also introduce the other two criteria, dissimilarity and weakness. All of the criteria are discussed as follows Criterion 1: some sensitive patterns are frequent in D'. This condition is denoted as Hiding Failure [10], and measured by     DP DP HF H H , where XPH represents the number of patterns contained in PHwhich is mined from database X Criterion 2: some non-sensitive patterns are hidden in D'. It is also denoted as Misses Cost [10], and measured by      DP DPDP MC 


MC H H H      w h e r e        X P H  r e p r e s s ents the number of patterns contained in ~PH which is mined from database X Criterion 3: the dissimilarity between the original and the sanitized database is also concerned, and it is measured by      n i m j ij n i m j ijij D DD Dis 1 1 1 1    Criterion 4: according to the previous section, Forward Inference Attack is avoided while at least one pair-subpattern of a sensitive pattern is hidden. The attack is quantified by        DPDP DPairSDPDP Weakness HH HH    where DPairS is the set of sensitive patterns whose pair subsets can be completely mined from D 4.2. Experimental Results Table 1: Experiment factor The experiment is to compare the performance between our approach and SWA which has been compare with Algo2a [4 and IGA [10] and is so far the algorithm with best performances published and presented in [12] as we know. The IBM synthetic data generator is used to generate experimental data. The dataset contains 1000 different items, with 100K transactions where the average length of each transaction is 15 items. The Apriori algorithm with minimum support = 1% is used to mine the dataset and 52964 frequent patterns are gotten Several sensitive patterns are randomly selected from the frequent patterns with length two to three items to be seeds With the several seeds, all of their supersets are included into the sensitive patterns set since any pattern which contains sensitive patterns should also be sensitive 0 0.05 0.1 0.15 0.2 0.25 0.3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern H id 


id in g F ai lu re SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern W e ak n es s SP SWA Fig.8: Rel. bet. RS and HF. Fig.9: Rel. bet. RS and weakness Fig.8, Fig.9, Fig.10, Fig.11 and Fig.12 show the effect of RS by comparing our work to SWA. Refer to Fig.8, because the level of confidence in our sanitization process takes the minim um support into account, no matter how the distribution of the sensitive patterns, we still are C confident to avoid hiding failure problems. However, there is no correlation between the disclos ure threshold in SWA and the minimum support. Under the sa me disclosure threshold, if the frequencies of the sensitive patte rns are high, the hiding failure will get high too. In Fig.9, beca use our work is to hide the sensitive patterns by decreasing the supports of the pair-subpatterns of the sensitive patterns, the val ue of weakness is related to the level of confidence. However according to the disclosure threshold of SWA, when all the pair subpatterns of the sensitive patterns have large frequencies, it may cause serious F-I Attack problems. Hiding failure and wea kness of SWA change with the distributions of sensitive patterns 0 0.1 0.2 0.3 0.4 0.5 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern M is se s C o st SP SWA 0 0.005 0.01 0.015 0.02 0.025 0.03 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern D is si m 


m il ar it y SP SWA Fig.10: Rel. bet. RS and MC.  Fig.11: Rel. bet. RS and Dis In Fig.10 and Fig.11, ideally, the misses cost and dissimilar ity increase as the RS increases in our work and SWA. However if the sensitive pattern set is composed of too many seeds, a lot of victim pair-subpatterns will be contained in Marked-Set. And as a result, cause a higher misses cost, such as x = 0.04 in Fig.10 Moreover, refer to the turning points of SWA under x = 0.0855 to 0.1006 in Fig.10 and Fig.11. The reason of violation is that the result of the experiment has strong correlation with the distri bution of the sensitive patterns. Because the sensitive patterns Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE are chosen randomly, several variant factors of the sensitive patt erns are not under control such as the frequencies of the sensiti ve patterns and the overlap between the sensitive patterns if the overlap between the sensitive patterns is high, some sensitive patterns can be hidden by removing a common item in SWA Therefore, decrease the misses cost and the dissimilarity 0 0.5 1 1.5 2 2.5 3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern E x ec u ti o n T im e h  SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set H id in g F a il u re SP SWA 


SWA Fig.12: Rel. bet. RS and time. Fig.13: Rel. bet. RL2 and HF Fig.12 shows the execution time of SWA and our approach As shown in the result, the execution time of SWA increases as RS increases. On the other hand, our approach can be separated roughly into two parts, one is to get Marked-Set which is strong dependent on the data, the other is to set sanitization matrix and execute multiplication whose execution time is dependent on the numbers of transactions and items in the database. In the experi ment, because the items and transactions are fixed, therefore, the execution time of our approach is decided by the setting of Marked-Set which makes the execution time changing slightly 0 0.05 0.1 0.15 0.2 0.25 0.3 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set W ea k n es s SP SWA 0 0.1 0.2 0.3 0.4 0.5 0.6 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set M is se s C o st SP SWA Fig.14: Rel. bet. RL2 and weakness. Fig.15: Rel. bet.RL2 and MC 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set D is si m il a ri ty SP SWA Fig.16: Rel. bet. RL2 and Dis Fig.13, Fig.14, Fig.15and Fig.16 show the effect of RL2 Refer to Fig.13 and Fig.14, our work outperforms SWA no matter what RL2 is. And our process is almost 0% hiding failure 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


