Building Hierarchical Keyword Level Association Link Networks for Web Events Semantic Analysis  Junyu Xuan, Xiangfeng Luo, Shunxiang Zhang, Zheng Xu, Huimin Liu and Feiyue Ye School of Computer Engineering and Science & High Performance Computing Center Shanghai University, 200072  Shanghai, China xuanjunyu, luoxf, sxzhang, xuzheng, hmpippo and yefy} @shu.edu.cn   Abstract With the increase of information scale of web events on the time, it is extremely difficult and challenging to grasp the semantics of web events artificially, because of the limitation of the time and energy of human beings. Herein, we 
propose a method to map the web event to keyword level association link network \(KALN\ for deep analysis of the semantics of web events, such as the evolution semantics of web events. Firstly, the original KALN is constructed at a given time by traditional data mining technologies. Then, the hierarchical KALN, consisted of Theme Layer Network Backbone Layer Network and Tidbit Layer Network, is built based on the original KALN by information entropy to identify the different semantic levels of the web event, including stable semantics, sub-stable semantics and unstable semantics. With 
the semantic analysis of hierarchical KALN, human could easily gain a thorough understanding of the web event. Finally experiments show that our method can effectively capture the different level semantics of web events Keywords-web event ; association link networks ; semantic analysis I   I NTRODUCTION  When an event happens in the society or a topic attracts broad attention on the web, there will be a huge number of web pages covering that on the web. As time goes by hundreds and thousands of web pages, blogs and posts come 
out with the evolution of events. The large scale of information makes it impossible for human to grasp the event by reading the massive web pages one by one. So it would be very useful for human to know what the events are talking about if the semantics of web events can be well organized and automatically showed at each time stamp For example, in one day, there are about 7000 web pages covering the Libya War that happened on Mar.20, 2011. It is hard and useless to read all these web pages, because web pages are so many that it would be a kill of time and energy However, what do people want to know through reading the 
web pages about this event? Perhaps, on Mar.23, 2011, most people want to know what have not changed and what kind of changes has been made. The final goal of this paper is to effectively solve this problem  In order to automatically analyze a web event, the first thing is to represent the web event in an automatic way which should preserve the semantics as much as possible. In this paper, we use the keyword level association link network KALN\o represent a web event, as shown in Fig. 1. The KALN is a complex network comprised of keywords and association rules between keywords, which can be mined 
from the web pages. The KALN could be considered as a capture of semantics of the web event at a given time stamp Through the analysis of the KALNês properties, we could get the properties of the web event. For example, the hierarchical property of the KALN reflects the hierarchy of the web event Then, based on information entropy, an algorithm is proposed to identify the hierarchical semantics of the KALN Here, a new web event stability is defined by the keywords weight distribution and is measured by the information entropy. Different level semantics of the web events have their counterpart in hierarchical KALN, such as stable 
semantics, sub-stable semantics and unstable semantics. If these semantics can be automatically identified, it will be easy for people to know the changed semantics and unchanged semantics of the web event. Finally, any variation of web event will also have the counterpart on the hierarchical KALN. The detailed definition is given in Section 3. In fact, there must be some properties dominating this web event while some trivial properties are dispensable Here, the properties of web events are represented by their keywords The main contributions of this paper are 1\ to represent the web events using their KALNs, 2\ to introduce the 
information entropy for KALN/web event stability measurement, 3\ and to build the hierarchical KALN to  Figure 1 The KALN representation of a web event. At a time point the event is represented by a network, which is constructed by keywords and their relations extracted from web pages about this event. And these web pages may come from different sources, such as ChinaDaily, BBC Google news 
2011 Ninth IEEE International Conference on Dependable, Autonomic and Secure Computing 978-0-7695-4612-4/11 $26.00 © 2011 IEEE DOI 10.1109/DASC.2011.163 988 
2011 IEEE Ninth International Conference on Dependable, Autonomic and Secure Computing 978-0-7695-4612-4/11 $26.00 © 2011 IEEE DOI 10.1109/DASC.2011.163 988 
2011 IEEE Ninth International Conference on Dependable, Autonomic and Secure Computing 978-0-7695-4612-4/11 $26.00 © 2011 IEEE DOI 10.1109/DASC.2011.163 987 


identify different level semantics of the web event  The rest of this paper is organized as follows. Section 2 reviews the related work. In Section 3, the general problem is formally defined and two sub-problems are extracted, which are discussed in Section 4 and Section 5 respectively. Section 6 shows the experimental results and Section 7 concludes II  R ELATED W ORK  Topic Detection and Tracking \(TDT\ has been discussed over ten years. Some similar works like [6, 4, 3  ar e  conducted to detect the appearance of topics, but no more analysis of topics are not involved. And the usersê query is used to reduce the scale of data and realize the query-guided event detection [12  T h e to p i c s u mm a r y i s g e n e r a ted to  describe this topic in [2 H o w e v e r th ey o n ly f o cu s o n th e selecting of important sentences. Other related work, such as 11, 10  h a v e an aly zed th e li f e cy cles  of t o pics u s in g th ei r  defined models. The competitive relations of topics have been studied in their life cycle in [7  T h e b a s i c m e trics  o f  the complex network are introduced in [1  In  5    th e  keywords and the co-occurrence of them are considered together, but it does not introduce the complex network Yang [1 t r i e s t o m o de l t h e e v e n t e v o l ut i o n r e l a t i o n shi ps  between events in an incident. However, it is only keywords that are considered in these models without considering the association rules between keywords. Luo and Xu have considered the association rules of keywords to build ALN of web pages in [9 a n d s o m e suc c e ssful app l i c a t i o n h a ve proposed in [8, 13  ba sed  o n t h a t   I n fa c t  ut i l i z i ng t h e K A LN  consisting of keywords and their association rules, the semantics of a topic can be well captured in our work III  P ROBLEM F ORMULATION  The general problem of the semantic analysis of web event using its KALN can be formulated as follows In this paper, the ultimate goal is to recognize the different level semantics of a web event. In order to achieve this goal, two important sub-questions need to be answered   How to represent the event at each time stamp with the existing data   How to identify the different level semantics of an event at a given time Suppose we have a collection of time-indexed web pages about a web event   12   n Www w  where i w refers to a web page coming from news sites, blogs or forums. Then we give some basic definitions as follows D EFINITION 1 \(Keyword level Association Link Network  A KALN consists of keywords and the association rules of keywords, which can be defined as      tktrt K ALN S S  1 where  kt S  is the keyword set and  rt S  is the rules set at a given time t  which are all extracted from the web pages of events D EFINITION 2  Web Event A  web event is defined as a time series of KALNs  12   n tt t WE KALN KALN KALN   2 where i t K ALN is a KALN at given time i t According to the above definitions, we can represent a web event as time series KALNs based on the original web pages. Meanwhile, the semantics of these web pages is preserved more than the situation of only considering keywords because not only the keywords but also the relations of keywords are considered here. Finally, the different level semantics of KALN at a given time should be identified IV  O RIGINAL KALN  A NALYSIS  Before the study of web event, it would be necessary to have a deep understanding of KALN at first. A KALN is the semantic description of an eventês status at a given time which is composed of keywords and association rules of them. Some other models or methods use the distribution of keywords in the data set. In fact, not only the keywords but also the relations of them should be considered in describing the event, because they are both basic semantic elements of an event and almost play the same important role at the semantic analysis of web events  A  Original KALN Construction Given a collection of web pages about an event at a given time t by utilizing some existing keyword extraction methods \(i.e tf-idf Term Frequency and Inverse Document Frequency\\e can get the node set of KALN from this data set. As we all know, there are plenty of keyword extraction algorithms, such as  SNR Signal-Noise Ratio MI Mutual Information\d 2 002 based method. Once the keywords are confirmed, next step is to extract the association rules of them. There are also many works on that Since they are not the main concern of this paper, here we just select tf-idf to get keywords and Apriori algorithm to get the association rules Apparently, the more precise the keyword and association rule extraction algorithms are, the better the event is described and the variation of KALN is more close to the evolution of an event B  Using Entropy to Measure KALN The entropy has been used in measuring the stability of a system. Here, we look upon the KALN as a system constructed by keywords with different properties. Actually a keyword in KALN has many properties, such as term frequency \(TF\, document frequency \(DF\ and so on. We simply select DF as the unique property of keywords Therefore, the systemês uncertainty, namely KALN stability is determined by the DF distribution of keywords  In a KALN, there are plenty of keywords with different power in describing an event. At the same time, the evolution process of them must have their own properties, and the variation of the sub-KALN, which is composed by a category of keywords, must be different. For example, in the 
989 
989 
988 


event Japan earthquake the word earthquake exists from beginning to the end. On the contrary, the word rescue only exists for a period of time  For the sake of measuring the KALN stability and distinguishing different types of keywords, we introduce the information entropy. Our KALN entropy definition is given as follows D EFINITION 3 \(KALN Entropy The KALN Entropy is the measurement of KALN systemês uncertainty    1  log   keywordnum KALN K ALN j j j j k keywords KALN H pp pDfj Dfk  003   002 002 002 3 where KALN H is the entropy value of KALN  keywordnum KALN is the keyword number of KALN  D f j is DF of keyword j in KALN and j p is the proportion of keyword jês DF to the sum of all keywordsê DF  in the KALN  Then, we make a thorough study about \(3\ of which three characteristics are discussed as follows C HARACTERISTIC 1  KALN Entropy increases with keyword number  For log jj p p  002 is a positive numerical value KALN H  is always a positive numerical value too. So, the entropy value increases with the number of keywords in KALN. That means the more keywords a KALN has, the less the stability that KALN has. One way of explaining this is that with more keywords, there is tending to be more semantics hidden in KALN, like subtopics. To a web event, that means a lot of aspects have been discussed. Therefore, the systemês stability is decreased C HARACTERISTIC 2  KALN Entropy is sensitive to the keywords DF distribution  The j p  actually reflects the keywords DF distribution which contains every wordês important degree compared with other words. Here, this important degree means the contribution of each keyword to the entire semantics of a web event and the power to describe or dominate a web event. When each word has equal important degree, the final entropy value reaches the maximum which means the system has the maximal uncertainty. In other words, the bigger the difference between the important degrees of keywords in a KALN is, the less the stability that KALN has. The explanation is that the main content of this event will be fixed, while a web event has one or a finite number of dominant keywords. Instead, while all the keywords have equal status, there is no predominant theme and çeverything in this KALN could be the main content of this event. So, the stability of the KALN is bad. For example, suppose a KALN has only two keywords, K a and K b And the DF ratio is ç9:1 It is clearly shown that this event is mainly talking about K a  So the semantics of this KALN is stable and the stability of this KALN is well. On the contrary, if the DF ratio is ç5:5 we will not be sure about what this event is talking about C HARACTERISTIC 3  KALN Entropy will reach a stable value with the adding of new keywords in descending order of DF How many and which keywords can approximately describe a KALN or the information of a web event in one day? If the order of adding keywords is fixed as descending it can be observed that the KALN entropy will get close to a stable value. It means that the influence of the new adding keywords is too small to have impact on the stability of the former keywords. So, a proper number of keywords can be selected to represent this web event The above section has discussed how to measure the stability of a KALN using entropy. Next, we turn to another objective in this section: the use of this in identifying the different types of semantics in a KALN based on information entropy C  KALNês Entropy Computation In order to distinguish different impacts on the KALNês entropy by different keywords, we compute a series of Entropies of KALNs which contain different number of keywords. The procedure is described as follows Step 1 Original KeywordSet is consisted of all the keywords in a KALN Step 2 The KALN is considered as a system constructed by one keyword with maximize DF. Remove this keyword from KeywordSet Get the first Entropy 0 H by \(3 Step 3 Select the keyword with maximize DF in the KeywordSet to add to this system. Remove this keyword from KeywordSet Compute the new Entropy i H  Step 4 Do Step3 until KeywordSet is empty. Finally, we get 01   n HH H   It is obvious that we have added keyword to the system one by one in the descending order of DF. The advantage of this order is that the final entropy value curve can be relative smooth compared with randomly adding keywords  Figure 2 The entropy values with the variation of keyword set. The new  keyword is added into the keyword set in the order of DF descent. The x-axis is the keyword number and the y-axis is the entropy value. \(Web event: Japan earthquake Time: May 9, 2011 0 1 2 3 4 5 6 7 8 9 10 1 24 47 70 93 116 139 162 185 208 231 254 277 300 323 346 369 392 415 438 461 484 507 530 553 576 599 622 Entropy Value Keyword ID Entropy Value \(H\ve 
990 
990 
989 


The reason why we construct this series of entropies is that the difference of two consecutive entropy values in the series can show the impact on the former system by the new adding keyword. Apparently, the bigger the difference is, the bigger impact from the new adding keyword is and the more powerful this keyword can characterize this event at the given time. As shown in Fig. 2, the shape of entropy value curve, which is strictly ascending and ascending rate continually becomes small, well match the properties of the KALN entropy V  S EMANTIC A NALYSIS OF H IERARCHICAL KALN   In this section, we present how to use the entropy value curve to split original KALN for answering the second sub-problem proposed in Section 3, which is to identify the hierarchical property of KALN A  Disscusiton on the Hierarchical Property of KALN There are two kinds of situations to influence the stability of KALN system. First, while the number of keywords is fixed, the change of DFês ratio can lead to the change of stability of KALN system as talked in DEFINITION 3 Second, while the keywordsê DFs in existing KALN system are fixed, the new adding keywords can also lead to the change. Actually, the second situation also satisfies the DEFINITION 3, because the new KALN systemês DF ratio relative to former system also has some change after adding new keywords. Next, we focus on the second situation As the procedure of entropy computation discussed above in Fig. 2, the difference of two pointsê values on the entropy curve reflects the importance degree of the latter keyword relative to the former keyword set. The bigger curvature of a point is, the smaller contribution the keyword added at this point makes to the former keyword set and the more stable the former system is. An explanation is that if the characteristic reflected by the new adding keyword has a little impact on the uncertainty of the web event, this characteristic or this keyword can be neglected. During the recursively splitting, it can be found that different impacts on the different granularity of KALN systems can be made by the same keyword. The reason for this phenomenon is that the KALN system has the hierarchical property. The low level keyword may have big impact on the low level KALN system, but it may have small impact on the high level KALN system at the same time. For example, in the event  Japan Earthquake the keyword earthquake belongs to high level KALN and the keyword evacuate belongs to low level KALN. The evacuate has big impact on the sub event  People and Corporation Evacuation and has small impact on the whole event Japan Earthquake  In order to identify this hierarchical property of KALN the property of entropy value curve is analyzed. Here, we use the curvature of a point on the curve to denote the influence degree of the keyword added at this point. In mathematics curvature of plane curve reflects the bend sharpness of a curve. To the entropy value curve, that means how much contribution to characterize this event by the new adding keyword. The bigger curvature of a point is, the smaller contribution the keyword added at this point makes B  Algorithm to Compute the Curvature Due to the complexity and the time consuming of the classical method \(curve fitting and compute second-order derivative\ used to get the curvature of plane curve, below we give an approximate method For example in Fig. 3, suppose we want to get the curvature of point A in this curve. Without curve fitting and computing second-order derivative, we can simply use the angle BA 004\004   as the curvature of point A, which is formed by two tangent lines at the point A and its former point B. It is defined as follows  Curv B A A 004\004   4  where A Curv  is curvature of point A and A 004  B 004 are the angles formed by tangent lines of point A and point B with x axis respectively It would be easy to obtain A 004  of point A if the slope of tangent line of point A is get. So, our goal here is to get the slope of each point without curve fitting. As shown in Fig. 3 the slopes of secant line BA and AC are   B ABABA Syyxx    5  and   AC A C A C Syyxx    6  where i x and i y  are the coordinates of point i  Along with the curve, the tangent line of point A actually is the limit when secant lines BA and AC tend to coincide Under the circumstance of limited points around A \(without curve fitting\ of real data, it is reasonable to use the  Figure 3 The example of computing the curvature of point A on the entropy curve. The x-axis is the keyword set, and each point represents a keyword in this set. The y-axis is the entropy value corresponding to the ke y word 
991 
991 
990 


summation of two slopes, which belong to the two points nearest to point A, as the slope of point A as      12 ABA AC CA CA BA SS S yy yy yy 005\005 005 003  004 005   004 006 002\002  7  where A S  is the slope of A and \002 is the mixing weight to reflect the distances between point A and points B,C. The more a point is close to point A, the more the slope of that point is close to the slope of point A Finally, we can get A 004 as  arctan  1 arctan 2   arctan     AA AC BA BABA AC AC BA AC B A A C BAAC BC AC B A BAAC BC S yy yy xxyy yy yy yy xxyy yy yyyy yy x x x x fixedtimespan yyyy yy 004  007  010  007\010 011\012 011\012   011\012 011\012  011\012 011\012    011\012 011\012 011\012  013\014 013\014 007\010  006 011\012  013\014   006  002 002 003 8 Considering our goal which is to distinguish the characterizing power of different keywords, the A 004 is only used to compare with other points. Additionally, the function arctan\(x is a monotonic function. So, we could only use the approximate value in \(8 C  Hierarchical KALN Construction According to the different curvatures of different points the original keyword set can be divided into two parts with the boundary of average value of curvatures of all keywords As shown in Fig. 4, we can get a binary tree by dividing keyword set recursively, in which nodes are keyword sets and root node is the original keyword set. Through this tree we have a new and deep understanding of the KALN and the event at this given time. Every nodeês left child node is the core keywords of this node and could well characterize this node. Herein, we just split the original keyword set into three layers, as shown in the Fig. 5, which are corresponding to the three nodes marked in Fig. 4  At first, we could get the two points of division  1  2   i iKeywordSetKALN p i iKeywordSetKALN p curv curv NumofKeywordSet KALN curv curv NumofKeywordSet KALN 003 007 003 003 004  004 004 005 004 004  004 007 006 002 002  9  where 1 p curv is the first division point and the original keyword set K ALN is split at this point. The Theme keyword set is got from this splitting. And The K ALN 007 is the left part, which is split at the second division point 2 p curv The Backbone keyword set and Tidbit keyword set are formed through this splitting  Then, we give each layerês definition as follows D EFINITION 4  Theme Layer Network \(Top Layer     1  ThemeLN i i p C keyword curv curv   10   ThemeLN ThemeLN KALN C 010  11   Figure 5 The new identified three layer network from the orignial KALN. The left part graph is original KALN. The right part three graphs are Theme Layer Network, Backbone Layer Network and Tidbit Layer Network from top to bottom. \(Web event: Japan earthquake Time: May 9 2011  Figure 4 The binary tree representation of a web event. The root of this tree is the original keyword set of a we b event and the left child of it is the core set of root and the right child is the left set. The marked three nodes in this tree is selected to represent the web events here 
992 
992 
991 


The theme layer network is comprised of keywords, which satisfy the condition that curv is bigger than 1 p curv and their association rules between them. This layer network is the core of the original network. It expresses what this KALN or this event is talking about and has a little variation with the time D EFINITION 5  Backbone Layer Network \(Middle Layer   12  BackboneLN i p i p C keyword curv curv curv   12   BackboneLN BackboneLN KALN C 010  13  The backbone layer network is comprised of keywords which satisfy the condition that curv is smaller than 1 p curv and bigger than 2 p curv and their association rules between them. This layer network is the backbone of the original network. It gives more details than the Theme Layer Network and shows main content of this event. As time goes by, it has phase variation  D EFINITION 6  Tidbit Layer Network \(Bottom Layer    2  TidbitLN i i p C keyword curv curv   14   TidbitLN TidbitLN KALN C 010  15  The theme layer network is comprised of keywords, which satisfy the condition that curv  is smaller than  2 p curv and their association rules between them. This layer network is the tidbit of the original network. It gives details of all aspects of this event and is the most sensitive to time in three layers D EFINITION 7  Hierarchical Keyword level Association Link Network \(Hierarchical KALN     H ierarchicalKALN ThemeLN BackboeLN TidbitLN   16  The Hierachical KALN is consisted of ThemeLN BackboneLN and TidbitLN, each of which reflects a semantic level of Hierarchical KALN  In our tests, we found that the keyword and arc numbers of each layer network are approximately one order smaller than its lower layerês, as Table I shows. Of course, it depends on the DF distribution of keywords. However, the density remarkably decreases from the top layer to the bottom layer The splitting of small number of keywords leads to the big decrease of arcs, which expresses that different keywords have different characters and small number of nodes has big number of arcs. The Diameter and Mean Distance of Theme layer network are 2 and 1.03. That means the distance between two nodes/keywords is smaller than 2. It is close to a complete graph and the new adding keywords almost needs to link to every node/keyword of old network. This condition is so rigid that the theme layer KALNês structure is the most stable. So this layer KALN reflects the stable semantics. On the contrary, the Density and the Diameter of Tidbit Layer KALN are 0.016 and 8 respectively, which means this layer network is a sparse graph. It is not sensitive to lose a node or add a new node. So this layer KALN reflects the unstable semantics So far, 1\ we have a deep understanding of a KALN viewed as a tree, and split it into three layer networks. 2 Meanwhile, through the recursively splitting the core set of original keyword set and terminating at a desired precision the most characterizing keywords can be identified by our algorithm VI  E XPRIMENTS AND D ISSCUSSIONS  A  Data Preparation In this section, we describe the dataset in our experiments The Chinese web pages of Libya War are collected from Feb 22 2011 to Apr, 22 2011 in the way of using search engines including www.Google.com.hk and www Baidu.com By the way, the algorithm proposed in this paper can handle Chinese and English web pages either. Finally, 11868 web pages are downloaded with time stamps, which may come from different web sites. However, the number of search engines results is huge. It is hard and unnecessary to get all these web pages. We just select the top 500 web pages to be the sample and we suggest that the main keywords of the web event at the given time have been included by the selected web pages because of the relevance ranking of search enginesê results  B  Experiments on Libya War In order to show the evaluation methods proposed in Section 4,5 and 6, some important days are selected to build hierarchical KALN for comparing with the important things happened in that day. So, the chronicle of Libya war is list in Table II and the hierarchical analysis results which are generated by our methods are shown in Table III and Table IV Due to the low accuracy of the algorithm of extracting the main text of web pages, which is not main concern of this paper, there are some noise words in each layer KALN. As space is limited, the top 10 keywords of each layer KALN are list in Table IV to reflect three levels semantics of the web event    TABLE I  C OMPLEX N ETWORK P ARAMATERS OF T HREE L AYER N ETWORKS   W EB EVENT   J APAN E ARTHQUAKE T IME   M AY 9  2011   Parameters KALN Theme Layer Backbone Layer Tidbit Layer number of nodes 12 48 584 number of arcs 128 986 5346 Density 0.97 0.437 0.016 AverageDegree 10.667 20.542 9.154 ClusteringCoeffic ient 0.973 0.756 0.825 MeanDistance 1.03 1.661 3.325 Diameter 2 4 8 
993 
993 
992 


TABLE II  T HE C HRONICLE OF L IBYA W AR  FROM NEWS WEBSITES  SUCH AS REUTERS  COM  CHINADAILY  COM  Time point \(day Some import things happened in that day Feb. 26  2011 The Council imposed an arms embargo against Libya, Gaddafi to freeze their overseas assets abroad Mar. 1  2011 United Nations condemned by the UN General Assembly plenary meeting by Botswana, Gabon, Jordan and other countries asked about the suspension of Libya's membership in the Human Rights Council draft resolution Mar. 5  2011 Gaddafi elite troops fighting brigade of the oil city of Khamis zawyet stormed the rebels back to be outdone, the two sides at war six hours later, the Mediterranean Sea re-back the petrochemical industry in Hong Kong Las Lanu husband refineries Mar. 19  2011 The first air raids Libya, France war, the U.S. Navy in the middle of the night, through its deployment of more ships in the Mediterranean, northern air defense systems to Libya launched a missile attack, the British Royal Air Force sent fighter planes  involved in the subsequent air strikes Apr. 10  2011 Including South African President Zuma made four African countries, including Libya, problem-solving roadmap, Gaddafi agreed to accept the road map. But Libya said the day after the opposition refused to accept the road map TABLE III  T HE E XPERIMENT R ESULT OF F IVE D AYS C ORRESPONDING TO T HE C HRONICLE OF L IBYA W AR  Time point \(day Keyword Number Keyword Number of  Theme LN Keyword Number of Backbone LN Keyword Number of  Tidbit LN Feb. 26  2011 1873 18 107 1748 Mar. 1  2011 2968 22 116 2830 Mar. 5  2011 1988 14 106 1868 Mar. 19  2011 4910 24 156 4730 Apr. 10  2011 3737 23 133 3581 TABLE IV  T HE H IERARCHICAL S EMANTIC OF E ACH D AY    D UE TO OUR DATA ARE C HINESE WEB PAGES  THE CORRESPONDING ENGLISH WORDS ARE GIVEN TO ASSIST READING  Time point day Theme LN  Backbone LN  Tidbit LN  Feb 26 2011 007˝!®\004n Libya 014—L\031 international 007Y\027 civil war 033\023 political 014—\021 state 011s crude oil 022\024\010 situation  oil 5b\014 America 007 global Au\007 plan 011 break out 033D\031B datum 016Í"\033 foreign exchange 4»\030 maintain 8◊\014 scope 035\016\035X organization 005o\024h supply 010\024G level CD\010 trend 1*+9 strategy 004ﬁ\034 Iran 030T\035\003 technique 033\023\022\024 political situation 004ü\0079 intervene 021,!\022 deposit 017\010 activity 006|\016 save 031~\033 measure 007G2 relationship Mar 1 2011 007˝!®\004n Libya 014—L\031 international 007Y\027 civil war 007 global 014—\021 country 5b\014 America 033\023\024p state 011 break out 022\024\010 situation 004\001\003 Middle East 4ò4 organization 033\0231 policy 011•\022 development 015\004\011\016 area 011E\035\016 crisis 024ÈCK lead to 004À\036\020 price 0256\010 situation  oil 020f\005 media 006 Europe 024ŒA construct 6†\034C background 035S\015 plate Eg\0079 import 004x\033 transaction Fº\007 part  financing 007@\023 announce 030T\035\003 technology Mar 5 2011 007˝!®\004n Libya 007Y\027 civil war 014—L\031 international 014—\021 state 5b\014 America 011°\021Õ#\022 opposition 033\023\024p government 003 world 022\024\010 situation  oil 033\0231 policy 007o\004_ military Aï\021r confirm 005±\030x protect 011 break out 015\004\011\016 area 0115\030"9 Gaddafi 026J\010 depravation 007@\023 publish 0ü\015\016 standpoint 024\031 increase 017\005\003 unemployment rate 005!5B position 012Q life 031\\\035\027 authorize 005ò5+\033 Russia 031 reveal 004Ù\032 spread 016Í"\033 foreign exchange 004\017\035\027 sovereignty Mar 19 2011 007˝!®\004n Libya 007Y\027 civil war  war 5b\014 America 004\001\014 China 014—L\031 international 033\023\024p government 014—\021 country 007o\004_ military 022\024\010 situation 011•\010 launch 033\023\024p\007o Government troop 6\(\011‹\014 the U.N L\0060N air defense 033çM6 part 010oG force 011Z\011 history 033\023 political 6—\010o power 003 world NRK consultant 0NK space FS\007 avoid 0N\007o air force C_\010 trend 014 weapon 027‰\035\000 cost L‘"\026 demand GõA interview 011 televisionstation Apr 10 2011 007˝!®\004n Libya 033\023\024p government 011°\021Õ#\022 opposition 007o\004_ military 027Ï\035\016 warplane 014—L\031 international  arms  Benghazi 016ƒ\033 statement 010 action 007áA resolution 030∫\027 intercept XT¢"∂G Tripoli 014—\021 country 005±\030x protect FºK army 033\003\030 support E±\011 violate 026\0174 president ED bomb 004 the masses 031 reveal 004 show Aå\011 permit 006…\022\024 deadlock 021 inspection EøL forced landing 007!\011 be free from FºKS minister 0050 function 
994 
994 
993 


VII  C ONCLUSION AND F UTURE S TUDY  In this paper, we have proposed a method to build temporal KALNs of web events which are used to assist people to identify the hierarchical semantics of web events  Our contributions are as follows 1  The original KALN has been constructed and is considered as a new semantic representation of web events to capture the semantics of web events at a given time 2  Information entropy has been introduced to measure the KALN stability, which is a deep understanding of corresponding web eventês stability  3  Based on the series of entropy values, the Hierarchical KALN, composed of ThemeLN, BackboneLN and TidbitLN, has been built to reflect different level semantics of the web events, including stable semantics sub-stable semantics and unstable semantics. That enables people to grasp the different level semantics of web events and distinguish the different weights of different characteristics which are represented by keywords Through the experiments, it is demonstrated that our work can well describe the semantics of web events There are several interesting further research points. First the dynamics between two consecutive time stamps could be measured through the complex network metrics. Second, the patterns of different web events must be different, which can be mined based on our existing work. Finally, we can do the most challenging prediction work. With the work of semantic analyzing and tracking on a web event, the maximum possible status of this event could be forecasted A CKNOWLEDGMENT  Research work reported in this paper was partly supported by the key basic research program of Shanghai under grant no. 09JC1406200, by the National Science Foundation of China under grant nos. 91024012, 61071110 and 40976108, and by the Shanghai Leading Academic Discipline Project under grant no. J50103 R EFERENCES  1  R. Albert and A. L. Barabasi, çStatistical mechanics of complex netwroks,é Reviews of Modern Physics, vol. 74: no. 47, Jan. 2002 doi:10.1103/RevModPhys.74.47 2  J. Allan, R. Gupta, and V. Khandelwal, çTemporal Summaries of News Topics,é Proceedings of ACM SIGIR 2001\(SIGIR 01\, ACM Press, Sept. 2001, pp. 10-18, doi:10.1145/383952.383954 3  T. Brants , F. R. Chen , and A. O. Farahat, çA system for new event detection,é Proceedings of the 26th SIGIR conference on Research and development in information ret rieval\(SIGIR 03\, ACM Press, Jul 28ÖAug. 1, 2003, pp. 330-337, doi:10.1145/860435.860495 4  F. Can, S. Kocberber, O. Baglioglu, S. Kardas, H. C. Ocalan, and E Uyar, çNew event detection and topic tracking in Turkish,é Journal of the American Society for Information Science and Technology, vol 61, Apr. 2010, pp. 802Ö819, doi:10.1002/asi.v61:4 5  F. Holz and S. Teresniak, çTowards Automatic Detection and Tracking of Topic Change,é In Proc. CICLing\(CICLing 01\,Sringer LNCS 6008, Mar. 2010, pp.327-339 doi:10.1007/978-3-642-12116-6_27 6  G. Kumaran and J. Allan, çText classification and  named entities for new event detection,é Proceedings of international ACM SIGIR conference on Research and development in information retrieval\(SIGIR 04\, ACM Press, Jul. 2004, pp.297-304 doi:10.1145/1008992.1009044 7  J. Leskovec, L. Backstrom, and J. Kleinberg, çMeme-tracking and the dynamics of the news cycle,é Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining ACM Press, June 28-July 01, 2009, pp. 497-506 doi:10.1145/1557019.1557077 8  X. Luo, X. Wei, and J. Zhang, çGuided Game-Based Learning Using Fuzzy Cognitive Maps,é IEEE Transactions on Learning Technologies, vol.3, no.4, Oct.-Dec. 2010 , pp.344-357, doi 10.1109/TLT.2010.26 9  X. Luo, Z. Xu, J. Yu, and X. Chen, çBuilding Association Link Network for Semantic Link on Web Resources,é IEEE Transactions on Automation Science and Engineering, vol. 8, no. 3, July 2011 pp.482-494, 10.1109/TASE.2010.2094608   Q. Mei and Ch. Zhai, çDiscovering Evolutionarry Theme Patterns from Text- An Exploration of Temporal Text Mining,é Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining\(KDD 05\, ACM Press, Aug 2005, pp. 198-207, doi:10.1145/1081870.1081895   S. Morinaga and K. Yamanishi, çTracking Dynamics of Topic Trends using a Finite Mixture Model, é Proceedings of the 2004 ACM SIGKDD international conferecnce on Knowledge discovery and data mining\(KDD 04\, ACM Press, Aug. 2004, pp. 811-816 doi:10.1145/1014052.1016919   A. Sun and M. Hu, çQuery-Guided Event Detection From News and Blog Streams,é Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on , vol. 41, no. 5, Sept. 2011 pp.834-839, doi:10.1109/TSMCA.2011.2157129   Z. Xu, X. Luo, J. Yu, and W. Xu, çMining Web search engines for query suggestion,é Concurrency and Computation: Practice and Experience, vol.23, no.10, Jul. 2011, pp.1101-1113 doi:10.1002/cpe.1689   Z. Xu, X. Luo, and W. Lu, çIncremental building association link network,é COMPUTER SYSTEMS SCIENCE AND ENGINEERING, vol. 26, no. 3, Dec. 2009, pp.153-162, doi 10.1109/ICPADS.2009.14   C. C. Yang , X. Shi, and C. P. Wei, çDiscovering event evolution graphs from news corpora,é IEEE Trans. Syst., Man, Cybern. A Syst., Humans, vol. 39, no. 4 Jul. 2009, pp.850-863 doi:10.1109/TSMCA.2009.2015885   
995 
995 
994 


9374610 /people/facultyinfo.asp people/search.asp?sort=pt 9374685 /news/default.asp /people/facultyinfo.asp 9374720 /courses/ /news/default.asp  SESSION #2 \(USER_ID = 22 9185108 /admissions/ /programs 9185138 /news/default.asp /news/default.asp  SESSION #3 \(USER_ID = 33 9226945 /people/search.asp?sort=pt /people 9226975 /people/facultyinfo.asp people/search.asp?sort=pt 9227072 /advising/ /courses 9227098 /people/search.asp?sort=pt /news/default.asp  After preprocessing which includes the removal of redundant URLs within every session, considering the time stamp of the first request in every session as the time stamp of the session, and the removal of the referrer we get SESSION #1 \(USER_ID = 11 9374553 /people/search.asp?sort=pt people/facultyinfo.asp news/default.asp courses  SESSION #2 \(USER_ID = 22 9185108 /admissions news/default.asp  SESSION #3 \(USER_ID = 33 9226945 /people/search.asp?sort=pt people/facultyinfo.asp advising news/default.asp Then we build the set of items which consists of the set of unique URLs of all sessions Set of items I = {/people/search.asp?sort=pt people/facultyinfo.asp news/default.asp, /courses/, /admissions/, /advising We then used the Apriori algorithm to mine for frequent itemsets. We set the minimum support to be 20% which is 


equivalent to minimum support count equal to 2 Candidate 1-itemset news/default.asp \(support count =3 people/search.asp?sort=pt\( support count =2 people/facultyinfo.asp\(support count =2 Candidate 2-Itemset news/default.asp, /people/search.asp?sort=pt}\(support count = 2 news/default.asp, /people/facultyinfo.asp}\(support count 2 people/search.asp?sort=pt, /people/facultyinfo.asp support count= 2 Candidate 3-Itemset news/default.asp, /people/search.asp?sort=pt people/facultyinfo.asp\(support count= 2 and because the available dataset was not suitable to run our experiments, the main goal of our future work is to find a real life representative dataset that fits our experimental needs or use one of the available dataset generators to generate a suitable one Candidate 3-Itemset represents the frequent itemset f . The set of covering transactions for the frequent itemset f  are in session #1 and session #3 which have the time  stamps 9374553 and 9226945 respectively. The time stamps are then ordered to get the time series corresponding to the frequent itemset f :  REFERENCES 1] Agrawal, R., Imieliski, T., and Swami, A. Mining association rules between sets of items in large databases. In: SIGMOD NY, USA, ACM Press \(1993 207-216 S = {9226945, 9374553 The experimental results we have got were not representative enough to reflect the applicability of our approach. Such bad results were expected because the dataset we used was not the one we need. On one hand because the time interval of the transactions is too small \(2 weeks other hand, the transactions represent user sessions in a university website, and usually the transactions recorded in the web log file of university websites are not more than course registration, search for an assignment, etc.  The dataset we need to run an effective experiment should have 


a long time interval for example a dataset that represents customers transactions in a retail website within two years As far as we know, such dataset does not exist especially when we talk about time stamped datasets. This lack of such datasets amplifies the need to develop time stamped transactional datasets generators which is discussed in [2 2]  Asem Omari, Regina Langer, and Stefan Conrad. TARtool: A Temporal Dataset Generator for Market Basket Analysis. In Proceedings of the 4th International Conference on Advanced Data Mining and Applications \(ADMA 2008 2008. Springer Lecture Notes in Artificial Intelligence \(LNAI August 2008 3] Asem Omari. Data Mining for Improved Website Design and Enhanced Marketing.  In Yukio Ohsawa and Katsutoshi Yada editors, Data Mining for Design and Marketing,  volume 5 of Chapman Hall/CRC Data Mining and Knowledge Discovery Series, chapter  6. Chapman Hall/CRC, First edition, November 2008 4] Asem Omari, Alexander Hinneburg, and Stefan Conrad Temporal Frequent Itemset Mining. In Proceedings of the Knowledge Discovery, Data Mining and Machine  Learning workshop \(KDML 2007 5]  B. Ozden, S. Ramaswamy and A. Silberschatz: Cyclic Association Rules. In: ICDE  98: Proceedings of the  Fourteenth International Conference on Data Engineering,  Washington, DC USA, IEEE Computer Society \(1998 412421 6] C. Antunes and A. Oliveira: Temporal Data Mining: An Overview. In: Proceedings of the Workshop on Temporal Data Mining, of Knowledge Discovery and Data Mining KDD01, San Francisco, USA \(2001 VI. APPLICATION FIELDS This method can be applied in different fields. One application field is in search engine log files for example to find out the most frequently searched keywords in the last time period. Another application field is in web usage mining for example to find out the most visited web pages in the last 3 months in some website. It can also be applied to a transaction dataset in a physical store or business to find out the most frequently bought products or used services in the last time period. Any other problem that needs to study the behavior of some items with respect to time can be a good application field 


7] Ding-An Chiang, Shao-Lun Lee, Chun-Chi Chen, and MingHua Wang. Mining Interval Sequential Patterns. International Journal of Intelligent Systems, 20\(3 359373 8] J. Han and M. Kamber: Data Mining Concepts and Techniques Morgan Kaufmann Publishers, San Francisco \(2001 9] Kaidi Zhao and Bing Liu. Visual Analysis of the Behavior of Discovered Rules. In Workshop Notes in ACM SIGKDD-2001 Workshop on Visual Data Mining, San  Francisco, CA, 2001 10] Mannila, H., Toivonen, H.: Multiple uses of frequent sets and condensed representations. In: KDD, Portland, USA \(1996 pp\(189-194 11] Q. Yang and X. Wu: 10 Challenging Problems in Data Mining Research. Volume 5, World Scientific Publishing Company International Journal of Information Technology and Decision Making \(2006 597604  VII. SUMMARY AND FUTURE WORK In this paper, we presented a new measure to mine for interesting frequent itemsets. This measure is based on the idea that interesting frequent itemsets are mainly covered by many recent transactions.  This measure reduces the cost of searching for frequent itemsets by minimizing the search interval. Furthermore, it can be used to improve the search strategy implemented by the Apriori algorithm 12] Sheikh, L.M. Tanveer, B. Hamdani, M.A. Interesting Measures for Mining Association Rules. In proceedings of the 8th International Multi-topic Conference INMIC, 2004. pp \(641 644 13] W. Lin, M. A. Orgun and G. Williams: An Overview of Temporal Data Mining. In:  Proceedings of the 1st Australian Data Mining Workshop, Canberra, Australia,   University of Technology, Sydney \(2002 8390  


processed, until all attributes in A have been exhausted and we get the final fuzzy version of the dataset E. At the end, all attributes would have categorical values for each record in the fuzzy dataset E. Thus, by applying the aforementioned pre-processing, given any dataset D with initial crisp attributes \(set A fuzzy records. And each of these is further iteratively converted to generate more fuzzy records, until each crisp attribute has been taken into account and we get our final fuzzy dataset E  VI. COUNTING IN FUZZY ASSOCIATION RULES Crisp ARM algorithms calculate support of itemsets in various ways Record-by-record counting; as in Apriori Counting using tidlists; for example, ARMOR Tree-style counting; as in FPGrowth In this section, we describe how counting is done in various fuzzy ARM algorithms using membership functions and how our pre-processing technique can be used to generate fuzzy datasets which can be used by any fuzzy ARM algorithm Table I. t-norms in Fuzzy sets  t-norm TM\(x, y x, y TP\(x, y TW\(x, y x + y ? 1, 0  A. Counting in Fuzzy Apriori The first pass of Apriori counts item occurrences to determine the large 1-itemsets. Any subsequent pass k consists of two phases. First, the large itemsets found in the k-1 the kth pass. Next, the database is scanned and the supports of candidate itemsets are counted. In any pass k, each record is selected in a sequential manner and the supports for the candidate itemsets, occurring in that particular record, are increased by one. Thus, the counting in Apriori is done in a record-by-record manner Fuzzy Apriori is a modified version of the original Apriori algorithm, and can deal with fuzzy records. Fuzzy 


Apriori counts the support of each itemset in a manner similar to the counting in Apriori; the only difference is that it calculates sum of the membership function corresponding to each record where the itemset exists. Thus the support for any itemset is its sum of membership functions over the whole fuzzy dataset. This calculation is done with the help of a suitable t-norm \(see Table I We generated the fuzzy dataset required for Fuzzy Apriori using our pre-processing methodology. The crisp dataset \(FAM95 sections 4 and 5, and the resultant fuzzy dataset was used as input to the Fuzzy Apriori algorithm. More details of how FPrep was used for pre-processing before Fuzzy Apriori can be found in [21] \(though the pre-processing methodology used in [21] is not explicitly names as FPrep  B. Counting in Fuzzy ARMOR Each record in the dataset is marked by a unique number called transaction id \(tid order. A tid-list of an itemset X is an ordered list of TIDs of transactions that contain X. ARMOR is based on the Oracle algorithm and is totally different from Apriori in that it calculates the support of each itemset by creating its tidlist and counting the number of tids in the tidlist. The count of any itemset is equal to the length of its corresponding tidlist The tidlist of an itemset can be obtained as the intersection of the tidlists of its mother and father \(for example, ABC is generated by intersecting AB and BC started off using the tidlists of frequent 1-itemsets In a similar manner, Fuzzy ARMOR also creates the tidlist for each itemset by intersecting the tidlists of its mother and father itemsets. And for each tid in the tidlist, it calculates the membership function  \(again using a suitable t-norm support for an itemset is thus the sum of the membership functions associated with each tid in its tidlist We have also developed an initial implementation of Fuzzy ARMOR [21]. This algorithm uses the same fuzzy dataset as input as that was used for Fuzzy Apriori. There is no change, whatsoever, made to this fuzzy dataset after it was generated initially \(for Fuzzy Apriori processing technique. Even though Fuzzy Apriori and Fuzzy 


ARMOR operate in different ways and process data differently, the fuzzy dataset created using our preprocessing technique can be used as input for both the algorithms. This is because the fuzzy dataset is generated in a standard manner of fuzzy data representation \(as described in section 5 ARM algorithm. More details of how FPrep was used for pre-processing before Fuzzy ARMOR can be found in [21 C. Counting in Fuzzy FPGrowth FPGrowth uses a compact data structure, called frequent pattern tree \(FP-tree structure and stores quantitative information about frequent patterns. Only frequent length-1 items will have nodes in the tree, and the tree nodes are arranged in such a way that more frequently occurring nodes will have better chances of sharing nodes than less frequently occurring ones. FP-treebased pattern fragment growth mining starts from a frequent length-1 pattern, examines only its conditional pattern base constructs its \(conditional recursively with such a tree. The support of any itemset can be calculated from its conditional pattern base and from the nodes in the FP-tree, which correspond to the itemset Fuzzy FPGrowth also works in a similar manner by constructing an FP-tree, with each node in the tree corresponding to a 1-itemset. In addition, each node also has a fuzzy membership function  corresponding to the 1itemset contained in the node. The membership function for each 1-itemset is retrieved from the fuzzy dataset while constructing the FP-tree, and the sum of all membership function values for the 1-itemset is its support. The support for a k-itemset \(where k ? 2 corresponding to the itemset by using a suitable t-norm  VII. RELATED WORK 3] describes the current status and future prospects of applying fuzzy logic to data mining applications. In [4] and 5], the authors discuss two facets of fuzzy association rules namely positive rules and negative rules, and describe briefly a few rule quality measures, especially for negative rules. The authors in [6] take this discussion further by describing in detail the theoretical basis for various rule quality measures using various t-norms, t-conorms, S 


implicators, and residual implicators. [8] and [9] illustrate quality measures for fuzzy association rules and also show how fuzzy partitioning can be done using various t-norms, tconorms, and implicators. The authors in [8] go a step further and do a detailed analysis of how implicators can be used in the context of fuzzy association rules Last, [7] and [10] take diametrically opposing stands on the usefulness of fuzzy association rules. The authors of [7 do a data-driven empirical study of fuzzy association rules and conclude that fuzzy association rules, after all, might not be as useful as thought to be. But the authors of [10 defended the usefulness of fuzzy association rules, by doing more experimental work, and then corroborating their stand through the successful results of their empirical research In addition to the fuzzy clustering based methodology briefly mentioned in [7], [19] and [20] describe methodologies for generating fuzzy partitions \(using nonfuzzy hard clustering original dataset into a fuzzy form. [19] uses k-Medoids CLARANS  CURE for the same. The hard clusterings so generated are then used to derive the fuzzy partitions. In such cases, where hard clustering is used, typically the middle point of each fuzzy partition is taken as reference \(membership  = 1 with respect to which the memberships for other values belonging to that partitions are calculated. [22] goes even a step further, and uses Multi-Objective Genetic Algorithms in the process for finding fuzzy partitions. Such methodologies which use hard clustering, or non-fuzzy methods are one way to obtain fuzzy versions of original datasets before any fuzzy ARM can ensue. But, with FPrep we use only fuzzy methods, fuzzy clustering to be more specific, in order to ensure consistency, and to have the notion of fuzziness maintained throughout. The main motive behind doing so is to ensure that any processing preceding the actual fuzzy ARM process, also involves fuzzy methods. Thus, the whole end-to-end process, right from the moment the processing of original crisp dataset starts till the time the final frequent itemsets are generated, involves only fuzzy methods and is holistic in nature  VIII. EXPERIMENTAL RESULTS 


The experimental results of FPrep as compared to other such non-fuzzy methods, on the basis of various parameters, are described below  A. Results from First Dataset We have tested FPrep against the automated methods for generating fuzzy partitions proposed in [19], [20]. These use hard clustering algorithms CLARANS \(k-Medoids CURE respectively. The main tangible metric to compare our approach to the ones proposed in [19], [20] is the time taken for execution. And, the dataset used for doing so is the USCensus1990raw dataset http://kdd.ics.uci.edu/databases/census1990 has around 2.5M transactions, and we have used nine attributes present in the dataset, of which five are quantitative and the rest are binary. The attributes, with their respective number of unique values, on which the evaluation was done, are as follows Age - 91 unique values Hours  100 unique values Income1  55089 unique values Income2  13707 unique values Income3  4949 unique values  Using each of the three methodologies being evaluated three fuzzy partitions were generated for each of these attributes. The results are illustrated in fig. 7, which has the y-axis in log10 form for ease of perusal.  The same are also available in Table II. As far as speed is concerned, for attributes having very low number of unique values \(~ 100 there is no big difference among the three methods. FPrep and CURE perform five times better than CLARANS for the attributes Age and Hours, both of which have around 100 unique values. But, the real differences become apparent for higher number of unique values. For attribute Income3, with 4949 unique values, we see that FPrep is nearly nine times faster than CURE, and nearly 2672 times faster than CLARANS, and for attribute Income2, with 13707 unique values, it is 27 times faster than CURE, and 13005 times faster than CLARANS. For attribute Income1, having 55089 unique values, FPrep is 46 times faster than CURE. No comparison was done with CLARANS for this attribute, as 


the time needed for execution exceeded 100000 seconds Thus, from this analysis we see that FPrep, which uses FCM clustering, clearly outperforms the CLARANS and CURE based methods on the basis of speed. The execution times for CLARANS and CURE mentioned in fig. 7 and Table II do not include the time required to create fuzzy sets, and calculate the membership value  for each numerical data point in every fuzzy set for the numerical attribute under consideration. These times also do not take into account the time required to transform crisp numerical attributes to fuzzy attributes, and derive the fuzzy dataset from the original crisp dataset The fuzzy partitions generated for each of the five numerical attributes for the USCensus1990raw dataset are shown in Table III. Coincidentally, generating three fuzzy partitions for each numerical attribute seemed a perfect fit In addition to the superior speeds achieved by FPrep, as illustrated in fig. 7 and Table II, Table III indicates the semantics and the quality of the fuzzy partitions generated by FPrep. Moreover, the number of frequent itemsets generated by a fuzzy ARM algorithm \(like fuzzy ARMOR and fuzzy Apriori minimum support threshold, is illustrated in fig. 8   Fig. 7. Algorithm, numerical attribute comparison based on speed \(log10 seconds   Fig. 8. Number of frequent itemsets for various minimum support values  B. Results from Second Dataset We have also applied FPrep on the FAM95 dataset http://www.stat.ucla.edu/data/fpp transactions. Of the 23 attributes in the dataset, we have used the first 18, of which six are quantitative and the rest are binary. For each of the six quantitative attributes, we have generated fuzzy partitions using FPrep. A thorough analysis with respect to execution times, has already been performed on the USCensus1990raw dataset \(which is manifolds bigger in size than the FAM95 dataset both on the basis of number of transactions and number of unique values for numerical 


attributes dataset has been done solely to provide further evidence of the quality and semantics of the fuzzy partitions generated by FPrep. The details of the same are in Table IV. In this case, the number of fuzzy partitions is different for different numerical attributes. Thus, the number and type of fuzzy partitions to be generated is totally dependent on the attribute under consideration. A graphical representation of the fuzzy partitions generated for the attribute Age has already been provided in fig. 5, and clearly shows the Gaussian nature of the fuzzy partitions. The nature and shapes of fuzzy partitions for the rest of the attributes are also similar. Last, the number of frequent itemsets generated for different minimum support values is illustrated in fig. 8  C. Analysis of Results With FPrep, we can analyze and zero in on the number and type of partitions required based on the semantics of the numerical attributes, which the methods detailed in [19 20] do not necessarily facilitate. Then, FPrep, backed by FCM clustering, takes care of the creating the fuzzy partitions, especially assigning membership values for each numerical data point in each fuzzy partition. In section 8.A we have already shown that FPrep is nearly 9 to 44 times faster than the CURE-based method, and 2672 to 13005 times faster than the CLARANS-based method. FPrep is not only much faster than other related methods, but also generates very high quality fuzzy partitions \(Table III and IV much user-intervention. We have created a standard way of representing any fuzzy dataset \(converted from any type of crisp dataset efficacy of the same is corroborated by the successful implementation of Fuzzy Apriori and Fuzzy ARMOR on the fuzzy dataset \(converted from crisp version of FAM95 dataset an initial implementation of Fuzzy ARMOR, are very encouraging. FPrep, when used in conjunction with these fuzzy ARM algorithms, generates a pretty good number of high-quality frequent itemsets \(fig. 8 frequent itemsets generated for a particular minimum support is same, irrespective of the fuzzy ARM algorithm 


used IX. CONCLUSIONS In this paper we have highlighted our methodology, called FPrep, for ARM in a fuzzy scenario. FPrep is meant for seamlessly and holistically transforming a crisp dataset into a fuzzy dataset such that it can drive a subsequent fuzzy ARM process. It does not rely on any non-fuzzy techniques and is thus more straightforward, fast, and consistent. It facilitates user-friendly automation of fuzzy dataset 1 0 1 2 3 4 5 Age - 91 Hours - 100 Income3 4949 Income2 13707 Income1 55089 Ti m e lo g1 0 se co nd s Numerical Attribute - Number of Unique Values FCM CURE CLARANS 0 500 1000 1500 2000 2500 


3000 0.075 0.1 0.15 0.2 0.25 0.3 0.35 0.4 N o o f F re qu en t I te m se ts Minimum Support USCensus1990 FAM95 generation through FCM, and subsequent steps in preprocessing with very less manual intervention and as simple and straightforward manner as possible. This methodology involves two distinct steps, namely creation of appropriate fuzzy partitions using fuzzy clustering and creation of fuzzy records, using these partitions, to get the fuzzy dataset from the original crisp dataset FPrep has been compared with other such techniques, and has been found to better on the basis of speed. We also illustrate its efficacy on the basis of quality of fuzzy partitions generated and the number of itemsets mined by a fuzzy ARM algorithm which is preceded by FPrep. This preprocessing technique provides us with a standard method of fuzzy data \(record that it is useful for any kind of fuzzy ARM algorithm irrespective of how the algorithm works. Furthermore, this pre-processing methodology has been adequately tested with two disparate fuzzy ARM algorithms, Fuzzy Apriori and Fuzzy ARMOR, and would also work fine with other fuzzy ARM algorithm REFERENCES 1] Zadeh, L. A.: Fuzzy sets. Inf. Control, 8, 338358 \(1965 2] Chen G., Yan P., Kerre E.E.: Computationally Efficient Mining for Fuzzy Implication-Based Association Rules in Quantitative Databases. International Journal of General Systems, 33, 163-182 


2004 3] Hllermeier, E.: Fuzzy methods in machine learning and data mining Status and prospects. Fuzzy Sets and Systems. 156, 387-406 \(2005 4] De Cock, M., Cornelis, C., Kerre, E.E.: Fuzzy Association Rules: A Two-Sided Approach. In: FIP, pp 385-390 \(2003 5] Yan, P., Chen, G., Cornelis, C., De Cock, M., Kerre, E.E.: Mining Positive and Negative Fuzzy Association Rules. In: KES, pp. 270-276 Springer \(2004 6] De Cock, M., Cornelis, C., Kerre, E.E.: Elicitation of fuzzy association rules from positive and negative examples. Fuzzy Sets and Systems, 149, 7385 \(2005 7] Verlinde, H., De Cock, M., Boute, R.: Fuzzy Versus Quantitative Association Rules: A Fair Data-Driven Comparison. IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics 36, 679-683 \(2006 8] Dubois, D., Hllermeier, E., Prade, H.: A systematic approach to the assessment of fuzzy association rules. Data Min. Knowl. Discov., 13 167-192 \(2006 9] Dubois, D., Hllermeier, E., Prade, H.: A Note on Quality Measures for Fuzzy Association Rules. In: IFSA, pp. 346-353. Springer-Verlag 2003 10] Hllermeier, E., Yi, Y.: In Defense of Fuzzy Association Analysis IEEE Transactions on Systems, Man, and Cybernetics - Part B Cybernetics, 37, 1039-1043 \(2007 11] Agrawal, R., Imielinski, T., Swami, A.N.: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Record, 22, 207216 \(1993 12]  Agrawal, R., Srikant, R.: Fast Algorithms for Mining Association Rules. In: VLDB, pp. 487-99. Morgan Kaufmann \(1994 13] Han, J., Pei, J., Yin, Y.: Mining Frequent Patterns without Candidate Generation. In: SIGMOD Conference, pp. 1-12. ACM Press \(2000 14] Han, J., Pei, J., Yin, Y., Mao, R.: Mining Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree Approach. Data Mining and Knowledge Discovery, 8, 5387 \(2004 15] Pudi V., Haritsa J.R.: ARMOR: Association Rule Mining based on Oracle. CEUR Workshop Proceedings, 90 \(2003 16] Dunn, J. C.: A Fuzzy Relative of the ISODATA Process and its Use in Detecting Compact, Well Separated Clusters. J. Cyber., 3, 32-57 1974 17] Hoppner, F., Klawonn, F., Kruse, R, Runkler, T.: Fuzzy Cluster Analysis, Methods for Classification, Data Analysis and Image Recognition. Wiley, New York \(1999 


18] Bezdek J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms. Kluwer Academic Publishers, Norwell, MA \(1981 19] Fu, A.W., Wong, M.H., Sze, S.C., Wong, W.C., Wong, W.L., Yu W.K. Finding Fuzzy Sets for the Mining of Fuzzy Association Rules for Numerical Attributes. In: IDEAL, pp. 263-268. Springer \(1998 20] Kaya, M., Alhajj, R., Polat, F., Arslan, A: Efficient Automated Mining of Fuzzy Association Rules. In: DEXA, pp. 133-142. Springer \(2002 21] Mangalampalli, A., Pudi, V. Fuzzy Association Rule Mining Algorithm for Fast and Efficient Performance on Very Large Datasets In FUZZ-IEEE, pp. 1163-1168. IEEE \(2009 22] Kaya, M., Alhajj. Integrating Multi-Objective Genetic Algorithms into Clustering for Fuzzy Association Rules Mining. In ICDM, pp. 431434. IEEE \(2004  Table II. Algorithm, numerical attribute comparison based on speed \(seconds  Algorithm Age - 91 Hours - 100 Income3 - 4949 Income2 - 13707 Income1 - 55089 FCM 0.27 0.3 3.13 6.28 79.4 CURE 0.25 0.25 28.67 163.19 3614.13 CLARANS 1.3 1.34 8363.53 78030.3 Table III. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions Age Old Middle Aged Young Hours More Average Less Income1 High Medium Low Income2 High Medium Low Income3 High Medium Low  Table IV. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions AGE Very old Around 25 Around 50 Around 65 Around 35 HOURS Very High Zero Around 40 Around 25 INCHEAD Very less Around 30K Around 50K Around 100K INCFAM Around 60K Around 152K Around 96K Around 31K Around 8K TAXINC Around 50K Around 95K Around 20K Very less FTAX Around 15K Very less Around 6K Very high Around 33K  


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


