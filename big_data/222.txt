PERSONALIZED RECOMMENDATION BASED ON ITEM DEPENDENCY MAP Sun-Hee Youm Dong Sub Cho Department of Computer Science and Engineering Ewha Womans University ABSTRACT In data mining we want to find hidden knowledge unexpected pattem and new rule from massive data In this paper we intend to find user\222s item purchasing pattem and recommend goods that he/she wants So we suggest item dependency map which express relation between purchased items Using an algorithm that we 
suggest we can recommend an item, which a user has not bought yet but maybe is likely to interested in And item dependency map is used as pattems for association in hopfield network so we can extract users\222 global purchasing item pattem only using users\222 partial information Hopfield network is an iterative auto-associative network consisting of a single layer of fully connected processing elements which can function as an associative memory And hopfield network can extract global information from sub-information Therefore this algorithm 
obtains an advantage of hopfield networks Our algorithm can be applied to real web site and help web master to know users\222 taste 1 INTRODUCTION Data mining is motivated by the decision supporter problem faced by most large companies. Many companies collect and store massive amounts of sales data and they want to find hidden knowledge which is unexpected pattems and new rules from those data Through data mining they can extract lots of knowledge Association rule generalization clustering similarity search sequential pattem neural 
network decision tree and etc Then they apply these rules to real business\222s decision problem Especially we concem about goods recommendation in which a user is likely to interested We introduce a way to extract important information from data and recommend goods to user using item dependency map that we suggest and hopfield network Item dependency map is matrix that expresses relations between items In other words each element value of matrix is approximate probability of which after a 
user purchases A helshe purchases B 0-7803-7090-2~0l/$10.00  2001 IEEE 250 2 PRELIMINARIES In this chapter we explain the hopfield network that is applied to our algorithm And we explain sequential pattem mining that analyze data in order of time as related work 2.1 Hopfield Network Hopfield network is one of neural networks which is iterative auto-associative networks consisting of a single layer of fully connected processing elements 
that can Sunction as an associative memory Hopfield networks are quite interesting from the information processing system point of view They do support the parallel-distributed information storage aspect common to all neural network models but information processing is not really performed in a parallel fashion The processing elements perform computations in an asynchronous fashion This allows the network to avoid the difficulties encountered in propagating synchronization signals throughout the large network. Another advantage is that this relieves the need of requiring that each processing 
dement have global knowledge of the entire network at all times For the discrete hopfield network processing elements may take on either binary 110 or bipolar\(l/-1 values A pattem constituting N discrete binary value can be presented to the network, resulting in a state vector for the system The network is fully connected in the sense that each processing element is connected to every other processing element The weights are symmetric and there is no connection between a neuron and 
itself We intend to use the fact that Hopfield network has features that it can extract global information from sub information We can infer the user\222s global item purchase pattem by using user\222s partial item purchase pattem as hopfield network\222s input So with the result value, we can recommend item that users are satisfied with 2.2 Sequential Pattern Mining Since bar-code appeared retail organizations can collect and store massive sales data referred to as the basket data ISIE 2001 Pusan KOREA 


These data generally consists of transaction date and purchased items Retail organizations stores data through bar-code in order of date When many companies solve the problem they analysis these data and use the results for resolving the problem Sequential pattern mining is introduced first by Agrawal[2 This method is different from association rule because sequential pattern mining analyze data on order of time Each continuous data consists of transactions that have items customer-id time etc and each transaction is sorted by transaction time After all user sequential pattern mining is to find user\222s continuous pattern that satisfies minimum support Here we define minimum support as percentage of continuous data that include pattern Sequential pattem mining starts in a filed of retail industry marketing additional sale customer satisfaction and etc But now it is applied to field of science and management Customer ID 1 2 3 ITEM DEPENDENCY MAP Customer Sequence 1 14 112 116 111 112 3.1 Problem Statement As we mentioned item dependency map is matrix that expresses relation between items In this chapter we explain the method to decide item dependency map We use user\222s transaction data We are given a database D of customer item purchase transaction Each transaction consists of following .fields customer id customer group id transaction time and the purchased items in the transaction Customer group id is identification of group of which people have 221same concern or job with each other.\222For example if we identify group id by age group can be lo\222s O\222S O\222S and etc We do not consider quantities of items purchased in a transaction An itemset is a non-empty set of items A sequence is an ordered list We denote an itemset I by il i2  im where ij is an item An itemset is ordered by increasing transaction-time corresponds to a sequence This is the item sequence Given a database D of customer transactions we extract item dependency maps to use hopfield network 3.2 Preprocessing We explain the preprocessing by showing the example Consider the database shown in Table 1 We have to make item dependency maps by each group Table.2 and Table.3 are item sequence divided by group id So later item sequence of each group is used to make item dependency map of each group Table 1 Database Sorted by Customer Group ID Customer ID and Transaction Time Table 1 Item Sequence of Group 1 Table 2 Item Sequence of Group 2 3.3 Processing 3.3.1 Pre item dependency map First we make pre item dependency map using item sequence of each group Let the number of items m After all preprocessing we will get a m*m matrix We assign each item the number from 1 tom if kth pre item dependency map is above we suggest two way to calculate a value of element \(i j  Method 1-1 Each itemset is traversed and a value of element i j is calculated An algorithm is like this for\(i=O i<# of itemset in each sequence item i item  1 while\(unti1 all items are traversed in each itemset tempgtl  transaction[i][item 25 1 ISIE 2001 Pusan KOREA 


the item of ith itemset tempgt2  transaction[i][item+l  pre-Pattem[group-id][temp-ptl][tempgt2  pre_Pattem[group_id][tempgtl][temp_pt2  1  1 item  1 First method is concemed about items purchased continuously by only one step If a user purchase item j after purchasing item i we add 1 to the value of element ij And then we divide the summation by total number of purchased item i in item sequence Ex\Let suppose each itemsets of item sequence of group A be like this 111 113, 115 112 111 115 112\and we number each items. Item 1 1 1\222s number is 1 item 1 12\222s number is 2 and so on Then 1,3  1/2 3,5  1/1 5,2  2/2, \(1 5  1/2 In case of element 1,3 a user purchased item 3 after purchasing item 1 and item 1 is purchased totally 2 times So the value of element 1,3 is 112 Method 1-2 Second method is concemed about interval time between two item purchased Previous method can express only relation between purchased item and rightly next purchased item But in case of purchased many items for short time several items have relation each other So we give weight after counting how many times item is separated other item If a user purchased item j after hetshe purchased a, b and i weight is low 1 a*a O<as 1 The way to calculate value of element i j is partly same with 1st method If a user purchase item j after purchasing item i we add 1 to value But this algorithm needs additional calculation We have to add away items\222 relation value, namely weighted value a for\(i=O; i of itemset in each sequence item i item  1  while\(unti1 all items are traversed in each itemset tempgtl  itemset[i][item tempgt2   item 1   pre-Pattem[group-id tempgtl  Ltempgt21  pre-Pattem[group-id][tempqtl][temp-pt2  1  1 forCj=item-2 j>O j value  value  a tempgtl  itemset[i]b tempgt2   item  pre-Pattem[group-id tempgt 13 tempgt2  pre-Pattem[group-id][tempgtl tempgt2  value 1 item  1 See the way to calculate using example Let suppose item purchase itemset be same with previous example Then if we apply this method we have to calculate 3,2 and 1,2 that is not calculated in first method and the value of element 13 changes Here in case of element 1,2 a user purchased item 2 after purchasing item 1,3 and 5 in 1\222\221 itemset and item 5 in Yd itemset 1,2 a  a  a 3,2  dl 13  1  a 1st item is purchase by both itemset so a denominator\222s value are 2 3,2  dl 3th item is purchased only one bme in itemset so In this method an element\222s value reflects relation denominator\222s value is 1 between two items considered 3.3 2 Item dependency map 4fter we make pre item dependency map of each group we need post processing to decide item dependency map\222s value namely bipolar\(-l/l value because item dependency map of each group is the pattem in hopfield network The value of each element is 1 or 1 We decide the value of item dependency map through testing weather the value of pre item dependency map is over a criterion which we determine Here we suggest two method Method 2-1 First of all we define the terminology we use Pk\(i J is the value of element i j of item dependency map in kth group t  the of purchaseditemj after purchaseditemi by totaluser the#of purchaseditemi by totaluser If the value of Pk\(i j is over than E\(t  1 otherwise  l\(or 0 Method 2-2 Second method uses ranking To begin with we rank Pk\(i j about total group. And then we determine marginal high ranking R as the group\222s number So if ranking of Pk\(i j is over R 1 otherwise l\(or 0 252 ISIE 2001 Pusan KOREA 


In case of first method if the number of group is small, the  Mehmet M Dalkilic Edward L Roberston average of group is dangerous because too heavy or small 223Information dependencies,\224 Proceedings of the value makes average not to represent standard value nineteenth ACM SIGMOD-SIGACT-SIGART symposium Now we have k pattems through above processing These on Principles of database systems pp 245  253,2000 pattems are used for association memory in hopfield network An association memory made using these 4 Rosa Meo 223Theory of dependence values,\224 ACM methods is applied to hopfield network program Trans. Database Syst pp 380  406, Sep. 2000 4 CONCLUSIONS AND FUTURE WORK 4.1 Evaluation To evaluate the performance of the algorithm, we intend to generate synthetic customer transactions And we apply our algorithm to those transactions in four situations In other words we simulate combination of two preprocessing and two post processing After we make an item dependency map we use it as input pattem in hopfield network And then to validate we reuse partial user\222s itemsets When a user data of group A is input and the conclusion is similar to item dependency map of group A we can thing that the algorithm that we suggest is good At early experiment when we give a weight value\(a and use ranking performance is best But we have to decide the factor whether the output is similar to the pattern of item dependency map And we need more evaluation 4.2 Conclustions In this paper we have introduced item dependency map which express relation between purchased item This algorithm can recommend an item that yet a user don\222t buy but maybe is likely to interested in And item dependency map is used pattems for association in hopfield network so we can extract users\222 global purchasing item pattem with users\222 partial information But we cannot test fully and determine parameter function about the number of groups and items And we will suggest optimal value of a and R in the future Continuously we will develop this algorithm and evaluate more perfectly Until now we suggest item dependency map with experimenting In future we will extend algorithm modifylng weight and add refining method Then we apply developed algorithm to real web site 5.REFERENCE   11 Tarun Khanna 223Foundations of Neural Networks,\224 Addison-Wesley Publishing Company 1990 2 Rakesh Agrawal Ramakrishnan Srikant 223Mining Sequential Pattems,\224 In Proc of the 1 1 th Int\222l Conference on Data Engineering Taipei Taiwan March 1995 253 ISIE 2001 Pusan KOREA 


For example avg\(S and median\(S are both prefix de creasing w.r.t value descending order and prefix increasing w.r.t value ascending order There still exist some constraints that cannot be pushed by item ordering For example the constraint avg\(S  median\(S  O5 does not admit any natural ordering on items w.r.t which it is convertible We call such constraints inconvertible 3.3 Summary: a classification on constraints As a general picture, constraints only involving aggre gate functions can be classified into the following cate gories according to their interactions with the frequent item set mining process anti-monotone monotone, succinct and convertible which in turn can be subdivided into convert ible anti-monotone and convertible monotone The inter section of the last two categories is precisely the class of strongly convertible constraints \(which can be treated either as convertible anti-monotone or monotone by ordering the items properly Figure 1 shows the relationship among the various classes of constraints ccmvcmhlc wnvcnihle convenihle Figure 1 A classification of constraints and their re lationships Some commonly used convertible constraints are listed in Table 4 4 Mining Algorithms In this section, we explore how to mine frequent itemsets with convertible constraints efficiently The general idea is to push the constraint into the mining process as deep as possible, thereby pruning the search space In Section 4.1 we first argue that the Apriori algorithm cannot be extended to mining with convertible constraints efficiently Then a new method is proposed by examining an example Section 4.2 presents the algorithm 31Cd for mining frequent itemsets with convertible anti-monotone constraints Algorithm FIC which computes the com plete set of frequent itemsets with convertible monotone constraint, is given in Section 4.3 Section 4.4 discusses mining frequent itemsets with strongly convertible con straints The constraint requires the median item in the itemset is with the av erage value 4.1 Mining frequent itemsets with convertible con straints An example We first show that convertible constraints cannot be pushed deep into the Apriori-like mining Remark 4.1 A convertible constraint that is neither mono tone nor anti-monotone nor succinct cannot be pushed deep into the Apriori mining algorithm Rationale As observed earlier for such a constraint e.g avg\(S 5 v subsets \(supersets\of a valid itemset could well be invalid and vice versa Thus within the levelwise framework no direct pruning based on such a constraint can be made In particular whenever an invalid subset is eliminated without support counting its supersets that are not suffixes cannot be pruned using frequency For example itemset df in our running example violates the constraint avg\(S  25 However an Apriori-like al gorithm cannot prune such itemsets Otherwise, its superset adf which satisfies the constraint, cannot be generated Before giving our algorithms for mining with convertible constraints, we give an overview in the following example Example 6 Let us mine frequent itemsets with con straint C  avg\(S  25 over transaction database 7 in Table 1 with the support threshold   2 Items in every itemset are listed in value descending order R 40 f\(30 dzo 410 b\(O h\(-10 20 e\(-30 It is shown that constraint C is convertible anti-monotone w.r.t R The mining process is shown in Figure 2 R a-f-g-d-bc-e Figure 2 Mining frequent itemsets satisfying con straint avg\(S  25 By scanning 7 once we find the support counts for ev ery item Since h appears in only one transaction, it is an infrequent items and is thus dropped without further con sideration The set of frequent l-itemsets are a f g d b c and e listed in order R Among them, only a and f satisfy 431 


Constraint f\(S 5 v f is a prefix decreasing function I  Yes  f\(S 5 v f is a prefix increasing function Yes   Convertible Convertible Strongly anti-monotone monotone convertible Table 4 Characterization of some commonly used SQL-based convertible constraints  means it depends on the specific constraint the constraint6. Since C is a convertible anti-monotonecon straint, itemsets having g d b c or E as prefix cannot satisfy the constraint. Therefore, the set of frequent itemsets satis fying the constraint can be partitioned into two subsets 1 The ones having itemset a as a prefix w.r.t 2232 i.e 2 The ones having itemset f as a prefix w.r.t 2 i.e those containing item a and those containing item f but no n The two subsets form two projected databases 5 which are mined respectively 1 Find frequent itemsets satisfying the constraint and having a as a prefix First n is a frequent itemset satisfying the constraint Then the frequent itemsets having a as a proper prefix can be found in the sub set of transactions containing n which is called n projecred database Since n appears in every transac tion in the a-projected database it is omitted The n projected database contains two transactions bcdf and cde f Since items b and e is infrequent within this pro jected database, neither ab nor ne can be frequent So they are pruned The frequent items in the a-projected database is f d c listed in the order R Since nc does not satisfy the constraint there is no need to create an ac-projected database To check what can be mined in the a-projected database with af and ad as prefix respectively we need to construct the two projected databases and mine them This process is similar to the mining of a projected databases The a f projected database con tains two frequent items d and c and only a fd satisfy the constraint. Moreover since a f dc does not satisfies the constraint the process in this branch is complete Since a fc violates the constraint there is no need to construct a f c-projected database The ad-projected database contains one frequent item c but adc does not satisfy the constraint Therefore the set of fre quent itemsets satisfying the constraint and having a as prefix contains a a f a f d and ad 221The fact that itemset g does not satisfy the constraint implies none of any I-itemsets after g in order R can satisfy the constraint avg 2 Find frequent itemsets satisfying the constraint and having f as a prefix Similarly the f-projected database is the subset of transactions containing f with both n and f removed It has four transactions bcd bcd!y C~E and ceg The frequent items in the pro jected database are g d 6 c e listed in the order of\221R Since only itemsets fg and fd satisfy the constraint we only need to explore if there is any frequent item set having f!g or fd as a proper prefix which satisfies the constraint The projected fg-database contains no frequent itemset with fg as a proper prefix that sat isfies the constraint Since b is the item immediately after d in order K and fdb violates the constraint any itemset having fd as a proper prefix cannot satisfy the constraint Thus f and fg are the only two frequent itemsets having f as a prefix and satisfying the con strain t In summary the complete set of frequent itemsets satis fying the constraint contains 6 itemsets n f af nd nfd fg Our new method generates and tests only a small set of itemsets 4.2 FXd Mining frequent itemsets with convert ible anti-monotone constraint Now let us justify the correctness and completeness of the mining process in Example 6 First we show that the complete set of frequent itemsets satisfying a given convertible anti-monotone constraint can be partitioned into several non-overlapping subsets It leads to the soundness of our algorithmic framework Lemma 4.1 Consider a transaction database 7 a support threshold and a convertible anti-monotone constraint C w.~t an order 72 over a set of itenis I Let al a   a be the itenis satisfying C The coniplete set offrequent item sets satisjj!ing C can be partitioned into in disjoint subsets the jth subset 1 5 j 5 m contains frequent itemsets sat isfying C and having aj as a prejix We mine the subsets of frequent itemsets satisfying the constraint by constructing the corresponding projected database 438 


Definition 4.1 Projected database Given a transaction database 7 an itemset a and an order R 1 Itemset p is called the max-prefix projection of trans action tid It E 7 w.r.t R if and only if 1 a C It and p C It 2 a is a prefix of j3 w.r.t R and 3 there exists no proper superset y of p such that y 5 It and y also has a as a prefix w.r.t R 2 The a-projected database is the collection of max prefix projections of transactions containing a w.r.t R Remark4.2 Given a transaction database 7 a support threshold  and a convertible anti-monotone constraint C Let a be a frequent itemset satisfying C The complete set of frequent itemsets satisfying C and having a as a prefix can be mined from the a-projected database The mining process can be further improved by the fol lowing lemma Definition 4.2 \(Ascending and descending orders An order R over a set of items I is called an ascending order for function h  2  R if and only if 1\for items a and b h\(a  h\(b implies a R 6 and 2 for itemsets a U a and a U b such that both of them have a as a prefix and a R b f\(a U a 5 f\(a U b R-l is called a descending order for function h For example, it can be verified that the value ascending order is an ascending order for function aug\(S and a de scending order for function maz\(S S Lemma 4.2 Given a convertible anti-monotone constraint C E f\(S 6 v 6 E I w.bt ascending/descending order R over a set of items I where f is a prefix function Let a be a frequent itemset satisfying C and al a2    a be the set of frequent items in a-projected database listed in the order of R 1 Ifitemset a U ai 1 5 i  m violates C forj such that i  j 5 m itemset Q U  aj also violates C 2 If itemset a U aj 1 5 j  m satisfies C but a U  aj  aj+l violates C no frequent itemset having a U  aj as a properprefi satisfies C Based on the above reasoning we have the algo rithm FICA as follows for mining Frequent ltemsets with  Convertible Anti-monotone constraints Algorithm 1 FICA Given a transaction database 7 a support threshold  and a convertible anti-monotone con straint C w.r.t an order R over a set of items I the algo rithm computes the complete set of frequent itemsets satis fying the constraint C Method Call fiea 0,T function fieQ a a is the itemset as prefix and 71 is the projected database 1 2 3 4 5 Scan 71 once find frequent items in 71 Let I be the set of frequent items within 71 such that Vu E I C\(a U a  true If I  0 return, else Vu E I output a U a as a frequent itemset satisfying the constraint If C is in form off S 8 where f is a prefix function and 0 E 5  using Lemma 4.2 to optimize the mining by removing items b from I such that there exists no frequent itemset satisfying C and having a U  b as a proper prefix Scan 71 once more Vu E I generate a U a projected database 71,u For each item a in II call fiea\(a U a 71au{a Rationale The correctness and completeness of the algo rithm has been reasoned step-by-step in this section The efficiency of the algorithm is that it pushes the constraint deep into the mining process so that we do not need to gen erate the complete set of frequent itemsets in most of cases Only related frequent itemsets are identified and tested As shown in Example 6 and in the experimental results the search space is decreased dramatically when the constraint is sharp 4.3 3X Mining frequent itemsets with mono tone constraints In the last two subsections an efficient algorithm for mining frequent itemsets with convertible anti-monotone constraints is developed. Under similar spirit an algorithm for mining frequent itemsets with convertible monotone constraints can also be developed Due to lack of space instead of giving details of formal reasoning, we illustrate the ideas using an example and then present the algorithm Example 7 Let us mine frequent itemsets in transaction database 7 in Table 1 with constraint C G avg\(S 5 20 Suppose the support threshold   2 In this example, we use the value descending order R exactly as is used in Ex ample 6 Constraint C is convertible monotone w.r.t order R After one scan of transaction database 7 the set of fre quent 1-itemsets is found Among the 7 frequent 1-itemsets g d 6 c and e satisfy the constraint C According to the definition of convertible monotone constraints, frequent itemset having one of these 5 itemsets as a prefix must also satisfy the constraint That is the g d b e and e projected database can be mined without testing constraint C because adding smaller items will only decrease the value of avg But a and f-projected databases should be mined with constraint C testing However as soon as its fre quent k-itemsets for any k satisfy the constraint, constraint checking will not be needed for further mining of their pro jected databases We present the algorithm TZCM for mining frequent itemsets with convertible monotone constraint as follows 439 


Algorithm 2 FIC Given a transaction database 7 a support threshold and a convertible monotone constraint C w.r.t an order R over a set of items I the algorithm com putes the complete set of frequent itemsets satisfying the constraint C Method Call ficm 0,7,1 function ticm TI check-flag 1 Scan 71 once find frequent items in TIa If check-flag is 1 let f be the set of frequent items within 71 such that Vu E I C\(a U a  true and 1 be the set of frequent items within 71 such that Vb E I C\(a U b  false If check-flag is 0 let I be the set of frequent items within 71 and I be 0 2 Vu E I output Y U a as a frequent itemset satisfy ing the constraint 3 Scan 71 once more Vu E fliUIl generate au{a projected database 71,u 4 Foreach itema inI~~,callfic,\(aU{a},7~au~a 0 Foreach itemainfI;,call fic,\(aU{a},71 1 Rationale The correctness and completeness of the algo rithm can be shown based on the similar reasoning in Sec tion 4.2 Here, we analyze the difference between 31CM with an Apriori-like algorithm using constraint-checking as post-processing Both F1CM and Apriori-like algorithms have to gener ate the complete set of frequent itemsets no matter whether the frequent itemsets satisfy the convertible monotone con straint The frequent itemsets not satisfying the constraint cannot be pruned. That is the inherent difficulty of convert ible monotone constraint The advantage of TICM a ainst Apriorix-like algo rithms lies in the fact that FIG only tests some of fre quent itemsets against the constraint. Once a frequent item set satisfies the constraint, it guarantees all of frequent item sets having it as a prefix also satisfy the constraint. There fore, all that testing can be saved An Apriori-like algorithm has to check every frequent itemset against the constraint In the situation such that constraint testing is costly such as spatial constraints, the saving over constraint testing could be non-trivial. Exploration of spatial constraints is beyond the scope of this paper 4.4 Mining frequent itemsets with strongly convert ible constraints The main value of strong convertibility is that the con straint can be treated either as convertible anti-monotone or monotone by choosing an appropriate order The main point to note in practice is when the constraint has a high selec tivity fewer itemsets satisfy it converting it into an anti monotone constraint will yield maximum benefits by search a is the itemset as prefix 71 is the a-projected database and check-flag is the flag for constraint checking space pruning When the constraint selectivity is low \(and checking it is reasonably expensive\then converting it into a monotone constraint will save considerable effort in con straint checking The constraint awg\(S  w is a classic example 5 Experimental Results To evaluate the effectiveness and efficiency of the algo rithms, we performed an extensive experimental evaluation In this section we report the results on a synthetic trans action database with IOOK transactions and 10K items The dataset is generated by the standard procedure described in l In this dataset the average transaction size and aver age maximal potentially frequent itemset size are set to 25 and 20 respectively The dataset contains a lot of frequent itemsets with various length This dataset is chosen since it is typical in data mining performance study The algorithms are implemented in C All the exper iments are performed on a 233MHz Pentium PC with 128MB main memory running Microsoft WindowsNT To evaluate the effect of a constraint on mining frequent itemsets we make use of constraint selectivity where the selectiviy S of a constraint C on mining frequent itemsets over transaction database 7 with support threshold is de fined as  of frequent itemsets NOT satisfying C  of frequent itemsets 6 Therefore a constraint with 0 selectivity means every fre quent itemset satisfies the constraint, while a constraint with 100 selectivity is the one cannot be satisfied by any fre quent itemset The selectivity measure defined here is con sistent with those used in 7,61 To facilitate the mining using projected databases we employ a data structure called FP-tree in the implementa tions of FICA and FIC FP-tree is first proposed in SI and also be adopted by 8,9 It is a prefix tree structure to record complete and compact information for frequent item set mining A transaction database/projected database can be compressed into an FP-tree while all the consequent projected databases can be derived from it efficiently We refer readers to 5 for details about FP-tree and methods for FP-tree-based frequent itemset mining Since FP-growth 5 is the FP-tree-based algorithm mining frequent itemsets and is much faster than Apriori we include it in our experiment. Comparison among FICA 3ZCM and FP-growth makes more sense than using pure Apriori as the only reference method 5.1 Evaluation of FZCA To test the efficiency of FZCd w.r.t constraint selec tivity in mining frequent itemsets with convertible anti monotone constraints, we run a test over the dataset with 440 


160 g 140 f 120 s 100 1 Figure 3 Scalability with constraint selectivity            A     FP-gmwth h FIC\(A 20 D FIC\(A 80  04 i 00 02 04 06 08 10 Support threshold Figure 4 Scalability with support threshold support threshold   0.1 The result is shown in Fig ure 3 Various settings are used in the constraint for various selectivities As can be seen from the figure 31CA achieves an al most linear scalability with the constraint selectivity As the selectivity goes up i.e fewer itemsets satisfy the con straint 31CA cuts more search space since one frequent itemset not satisfying the constraint means all frequent itemsets having it as a prefix can be pruned We compare the runtime of both Apriori and FP-growth in the same figure All these two methods first compute the complete set of frequent itemsets and then use the constraint as a filter So their runtime is constant w.r.t constraint selectivity However only when the constraint selectivity is 0 i.e every frequent itemset satisfies the constraint does FICA need the same runtime as FP-growth In all other situations FICA always requires less time We also tested the scalability of FZC\224 with support threshold and the number of transactions respectively The corresponding results are shown in Figure 4 and Figure 5 From these figures we can see that 3ZCA is scalable in both cases Furthermore the higher the constraint selectiv ity the more scalable FZCA is That can be explained by the fact that 3ZCd always cuts more search space using constraints with higher selectivity 5.2 Evaluation of FZCM As analyzed before convertible monotone constraint can be used to save the cost of constraint checking but it cannot cut the search space of frequent itemsets In our experi ments since we use relatively simple constraints, such as those involving avg and sum the cost of constraint check ing is CPU-bound However the cost of the whole frequent itemset mining process is I/O-bound This makes the effect of pushing convertible monotone constraint into the mining process hard to be observed from runtime reduction In our experiments 31CM achieves less than 3 runtime benefit 0 200 400 600 800 1000 Number of transactions K Figure 5 Scalability with number of transactions in most cases However if we look at the number of constraint tests performed, the advantage of FICM can be evaluated objec tively FZC can save a lot of effort on constraint testing Therefore in the experiments about 31C\222 the number of constraint tests is used as the performance measure We test the scalability of 3ZCM with constraint selec tivity in mining frequent itemsets with convertible mono tone constraint The result is shown in Figure 6 The fig ure shows that FZCM has a linear scalability When the constraint selectivity is low, i.e most frequent itemsets can pass the constraint checking most of constraint tests can be saved This is because once a frequent itemset satisfies a convertible monotone constraint every subsequent frequent itemset derived from corresponding projected database has that frequent itemset as a prefix and thus satisfies the con straint, too We also tested the scalability of 31CM with support threshold The result is shown in Figure 7 The figure shows that FZCM is scalable Furthermore the lower the con straint selectivity the better the scalability FZCM is In summary our experimental results show that the method proposed in this paper is scalable for mining fre quent itemsets with convertible constraints in large transac tion databases The experimental results strongly support our theoretical analysis 6 Discussions: Mining Frequent Itemsets with Multiple Convertible Constraints We have studied the push of single convertible con straints into frequent itemset mining 223Can we push mul tiple constraints deep into the frequent pattern mining pro cess?\222 Multiple constraints in a mining query may belong to the same category e.g all are anti-monotone or to different categories Moreover different constraints may be on dif ferent properties of items e.g some could be on item price 441 


1  0 20 40 60 80 100 Selectivity Figure 6 Scalability with constraint selectivity others on sales profits the number of items etc As shown in our previous analysis unlike anti monotone, monotone and succinct constraints convertible constraints can be mined only by ordering items properly However different constraints may require different and even conflicting item ordering Our general philosophy is to conduct a cost analysis to determine how to combine mul tiple order-consistent convertible constraints and how to se lect a sharper constraint among order-conflicting ones The details will not be presented here for lack of space 7. Conclusions Constraints involving holistic functions such as median algebraic functions such as avg or even those involving dis tributive functions like sum over sets with positive and neg ative item values are difficult to incorporate in an optimiza tion process in frequent itemset mining The reason is such constraints do not exhibit nice properties like monotonicity etc. A main contribution of this paper is showing that by im posing an appropriate order on items, such tough constraints can be converted into ones that possess monotone behavior To this end we made.a detailed analysis and classification of the so-called convertible constraints We characterized them using prefix monotone functions and established their arithmetical closure properties As a byproduct we shed light on the overall picture of various classes of constraints that can be optimized in frequent set mining While con vertible constraints cannot be literally incorporated into an Apriori-style algorithm they can be readily incorporated into the FP-growth algorithm Our experiments show the effectiveness of the algorithms developed We have been working on a systematic implementation of constraint-based frequent pattern mining in a data min ing system More experiments are needed to understand how best to handle multiple constraints An open issue is given an arbitrary constraint, how can we quickly check if it is strongly convertible We are also exploring the use of constraints in clustering 160000   t 221FP-growth tFIC\(M 20 tFIC\(M 80 0.0 0.2 0.4 0.6 0.8 1.0 Support threshold Figure 7 Scalability with support threshold References I R Agrawal and R Srikant. Fast algorithms for mining asso ciation rules In Proc 1994 Int Con Very Large Data Bases VLDB\22294 pages 487-499 Santiago, Chile, Sept 1994 2 R J Bayardo R Agrawal and D Gunopulos Constraint based rule mining on large dense data sets In Proc 1999 Int Conj Data Engineering ICDE\22299 Sydney, Australia Apr 1999 3 S Brin R Motwani and C. Silverstein Beyond market bas ket Generalizing association rules to correlations In Proc 1997 ACM-SIGMOD Int Con Management of Data SIG MOD\22297 pages 265-276 Tucson Arizona May 1997 4 G Grahne L Lakshmanan and X Wang Efficient min ing of constrained correlated sets In Proc 2000 Int Con Data Engineering \(ICDE\222OO pages 5 12-521 San Diego CA Feb 2000 5 J Han J Pei and Y Yin Mining frequent patterns with out candidate generation In Proc 2000 ACM-SIGMOD Int Con Managementof Data \(SIGMOD\222OO pages 1-12 Dal las, TX May 2000 6 L V S Lakshmanan R Ng J Han and A Pang Opti mization of constrained frequent set queries with 2-variable constraints In Proc 1999 ACM-SIGMOD Int Con Man agement of Data SIGMOD\22299 pages 157-168 Philadel phia PA June 1999 7 R Ng L V S Lakshmanan J Han and A Pang Ex ploratory mining and pruning optimizations of constrained associations rules In Proc 1998 ACM-SIGMOD Int Con Management of Data SlGMOD\22298 pages 13-24 Seattle WA June 1998 8 1 Pei and J Han Can we push more constraints into fre quent pattem mining In Proc 2000 Int Con Knowl edge Discovery and Data Mining KDD\222OO pages 350 354, Boston MA Aug. 2000 9 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets In Proc 2000 ACM SIGMOD Int Workshop Data Mining and Knowledge Dis covery DMKD\222OO pages 1 1-20 Dallas TX May 2000 lo R Srikant Q Vu and R Agrawal. Mining association rules with item constraints In Proc 1997 Int Con Knowledge Discovery and Data Mining KDD\22297 pages 67-73 New port Beach CA Aug 1997 442 


expect this optimization to be of greatest bene\336t when the transaction sizes are large r example if our transaction is T 000 f A\000 B 000 C\000 D\000 E g  k 000 3 fan-out 000 2 then all the 3-subsets of T are f ABC,ABD,ABE,ACD,ACE,ADE,BCD,BCE,BDE,CDE g  Figure 2 shows the candidate hash tree C 3  We ave to increment the support of every subset of T contained in C 3  We egin with the subset AB C  and hash to node 11 and process all the itemsets In this downward path from the root we mark nodes 1 4 and 11 as visited We then process subset AD B  and mark node 10 Now consider the subset CDE  We see in this case that node 1 has already been marked and we can preempt the processing at this very stage This approach can r consume a lot of memory r a n fan-out F  for iteration k  e need additional memory of size F k to store the 337ags In the parallel implementation we have to keep a VISITED 336eld for each processor bringing the memory requirement to P\000F k  This can still get very large especially with increasing number of processors In we sho w a mechanism by which further reduces the memory requirement to only k 000F  The approach in the parallel setting yields a total requirement of k 000F 000P  5 Experimental Evaluation Database T I D Total Size T5.I2.D100K 5 2 100,000 2.6MB T10.I4.D100K 10 4 100,000 4.3MB T15.I4.D100K 15 4 100,000 6.2MB T20.I6.D100K 20 6 100,000 7.9MB T10.I6.D400K 10 6 400,000 17.1MB T10.I6.D800K 10 6 800,000 34.6MB T10.I6.D1600K 10 6 1,600,000 69.8MB Table 2 Database properties 5.1 Experimental Setup All the experiments were performed on a 12-node SGI Power Challenge shared-memory multiprocessor Each node is a MIPS processor running at 100MHz There\325s a total of 256MB of main memory The primary cache size is 16 KB 64 bytes cache line size with different instruction and data caches while the secondary cache is 1 B 128 bytes cache line size The databases are stored on an attached 2GB disk All processors run IRIX 5.3 and data is obtained from the disk via an NFS 336le server We used different synthetic databases with size ranging form 3MB to 70MB 2  and are generated using the procedure described in These databases mimic the transactions in a retailing en vironment Each transaction has a unique ID followed by a list of items bought in that transaction The 2 While results in this section are only shown for memory resident databases the concepts and optimization are equally applicable for non memory resident databases In non memory resident programs I/O becomes an important problem Solutions to the I/O problem can be applied in combination with the schemes presented in this paper These solutions are part of future research 11 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


  0 500 1000 1500 2000 2500 0 2 4 6 8 10 12 Number of Large Itemsets Iterations Large Itemset at Support = 0.5 222T5.I2.D100K\222  222T10.I4.D100K\222   222T15.I4.D100K\222   222T20.I6.D100K\222   222T10.I6.D400K\222   222T10.I6.D800K\222   222T10.I6.D1600K\222  Figure 3 Large Itemsets per Iteration data-mining provides information about the set of items generally bought together Table 2 shows the databases used and their properties The number of transactions is denoted as jD j  average transaction size as j T j  and the average maximal potentially large itemset size as j I j  The number of maximal potentially large itemsets j L j 000 2000 and the number of items N 000 1000 We refer the reader to for more detail on the database generation All the e xperiments were performed with a minimum support value of 0.5 and a leaf threshold of 2 i.e max of 2 itemsets per leaf We note that the  improvements shown in all the experiments except where indicated do not take into account initial database reading time since we speci\336cally wanted to measure the effects of the optimizations on the computation Figure 3 shows the number of iterations and the number of large itemsets found for different databases In the following sections all the results are reported for the CCPD parallelization We do not present any results for the PCCD approach since it performs very poorly and results in a speed-down on more than one processor 3  5.2 Aggregate Parallel Performance Table 3 s actual running times for the unoptimized sequential and a naive parallelization of the base algorithm Apriori for 2,4 and 8 processors without any f the techniques descibed in sections 3 and 4 In this section all the graphs showing  improvements are with respect to the data for one processor in table 3 Figure 4 presents the speedups obtained on different databases and different processors for the CCPD parallelization The results presented on CCPD use all the optimization discussed 3 Recall that in the PCCD approach every processor has to read the entire database during each iteration The resulting I/O costs on our system were too prohibitive for this method to be  12 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


