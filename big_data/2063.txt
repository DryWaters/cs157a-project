Abstrac tSeveral algorithms have been proposed to solve the problem of mining frequent inter-transaction itemset. However the low efficiency of support calculation for inter-transaction itemsets is still a challenging problem that eliminates the performance of mining algorithms. This paper provides intertransaction association rule mining algorithm using effective technique for support calculations. The proposed technique is based on cross correlation and bitwise operations. The experimental results show a significant improvement of performance up to several orders of magnitude compared to First Intra Then Inter \(FITI\gorithm Keywords Data Mining, Inter-Transaction Association Rule Mining, Cross Correlation  I  I NTRODUCTION  Association rule mining \(ARM\nsidered one of the 
most critical data mining components. This is not only because it discovers frequently occurring patterns in databases, but also the associations between these patterns attributes Based on the terminology [1 h e A R M i s bri e fl y des c ri be d  as follows I={i 1 i 2 i n  is a set of n binary literals called items. A transaction T i is a set of items in I such that T i   I A database D is a set of transactions. A transaction T i is said to contain a certain set of items \(itemset C iff C   T i Itemset that consists of K items is called K itemset. The support of an 
itemset C denoted  c is the percentage of transactions that contain C An itemset is considered frequent, if its support is greater than or equals to user defined value minimum support   min_sup The maximal frequent itemset is defined as the frequent itemset that is not a subset of any other frequent itemset. The association rule is presented in the form A  B  read A implies B here A and B are two itemsets and A   B  The association rule support is the support of an itemset C where C  A U B The rule is frequent iff itemset C is frequent. The rules in this form provide the relations 
between itemsets in the same transaction and called intratransaction association rules. The other type of association rules that provides the relations between itemsets in different transactions called inter-transaction association rules. The difference between the two types could be cleared through the following example. Taking stock data as an example intra-transactional mining, can find rules like, “If stock A goes up, and stock B goes up, then stock C will go up on the same day \(support = 5%, confidence = 80%\while intertransactional mining can find rules like “If stock A goes up on the first day, and stock B goes up on the second day, then stock C will go up on the third day \(support = 5%, confidence 80   In inter-transaction ARM, the relations between itemsets are discovered for specific domain. This domain may be time as above example or location or any other domain based on natural of examined database. In case of continuous domain 
as time it can be divided into equal intervals as hours, days weeks.  Every transaction in database is associated with a dimensional attribute, which is value in the database domain The transactions are stored in form T = \(T d T i  where T i is the set of items in T and T d is the dimensional attribute of the transaction. For time as domain and interval is a day T = \(T d T i  represents  that set of items T i that happened in day T d  Another important concept in inter-transaction ARM is maxspan, which is user defined value that represents the maximum interval size in domain, where inter-transaction association rules consider inte rested. For example maxspan 5 days, user is interested only in association rules that contain relations between itemsets in time interval less than or equals 
to 5 days, and rule like “If stock A goes up on the first day and stock B goes up on the second day then stock C will go up on after seven days \(support = 5%, confidence = 80%\s uninteresting rule since the time interval within the rule is more than the user defined maxspan. The first stage in intertransaction ARM algorithms is to  discover frequent interitemsets that are set of extended items in form i i d where i i  is an intra-transaction item and d is the relative position of the item in the interval of  inter-itemset contains that item For example inter-itemset E  a\(0\c\(2\,b\(5 represents the pattern: if item a is found in a transaction has dimensional attribute d item c will be found in the transaction with d+2  and item b will be found in transaction with 
d+5 The interitemset E can be represented in form \(a , 0 , c , 0 , 0 , b  Lu et al ed t h e con cept  of i n t e rt r an s act i o n an d provided EH-Apriori algorithm to discover frequent intertransaction patterns; later Tung et al pr oposed more efficient algorithm, FITI [2 f irst in tratran s actio n  th en  intertransaction\. The FITI algorithm is based on the concept a frequent inter-itemset contains only the frequent intraitemsets. Based on that concept, FITI algorithm discovers first the frequent intra-itemsets then discovers inter-itemsets The FITI algorithm is composed of 3 phases; that are abridged as follows Phase 1: Frequent intra- itsemsets are mined during this phase by using the Apriori algorithm y ot her enhanced algorithm  After that, the itemsets are stored in a data structure, called Frequent-Itemsets Linked Table \(FILT\. The FILT data structure is composed of an itemset Hash Table, with nodes linked by several kinds of 
links.  These links are Lookup Links, Generator and Extension Links, Subset Links, and Descendant Links Phase 2, Database Transformation: After completing the formation of the data structure FILT, the next step is to CROSS CORRELATION BASED INTE R-TRANSACTION ASSOCIATION RULE MINING TECHNIQUE  A. M. Ghanem 2 B. Tawfik 1 and M. I. Owis 1 1 Biomedical Engineering Department, Cairo University, Giza, Egypt 2 Faculty of Information Systems, Suez Canal University, Ismailia, Egypt e-mail: ahmed  o p timal-s y s.co m 


transform the database into a set of encoded Frequent-Itemset Tables, called FIT tables. Number of tables is equal to the length of the longest frequent intra-itemset that has been discovered in the first phase. Each table F k will be in the form d i IDset i where d i is the value of the dimensional attribute and IDset i is the ID s of frequent k-intra-itemsets that are found in the transaction Phase 3, Mining Frequent Inter-Itemsets: In this phase, the mining algorithm follows the Apriori basics to perform levelwise mining. So, it uses frequent k-inter-itemsets \(for k>2\o form candidate \(k+1\-inter-itemsets  The performance of FITI is sensitive for the increasing in the number or average size of frequent intra-itemsets in examined database FITI algorithm requires complicated data structure to store the frequent Intra- itemsets in phase one. In case of low minimum support, number of frequent itemsets will be increased and more memory space will be required to store them. In phase two, FITI transform the database in set of encode files. The number of these files is equal to the length of longest frequent intra-itemset, which increased as minimum support reduced. The number of transaction in the encoded files is the number of transaction in the original database file. Number of encoded files and required space to store them can increase dramatically in case of large database with long average frequent itemsets size. Moreover, FITI in phase three, uses complicated support calculation method based on encoded files in phase two, and concept of megatransaction which contains all transaction in sliding window In this work, we propose an inter-transaction ARM algorithm provides more efficient support calculations   II. M ETHODOLOGY  The proposed inter-transaction association rule mining algorithm called CC-BIT which based on cross correlation and bitwise operations. In CC-BIT, first an intra-transaction ARM algorithm is applied to discover all frequent intra itemsets. The resulting frequent itemsets are stored in hash tree. Second, bit vector for every frequent item is generated Then cross correlation between every two vector is calculated and frequent 2-inter-itemsets are discovered. Finally, intertransaction candidate generation method is applied on the resulting frequent inter-itemsets to generate the next candidate inter-itemsets. The support of candidate interitemsets is calculated using cross correlation, and bits counting then the frequent inter-itemsets are discovered. The candidate inter-itemsets generation, and support calculation and evaluation processes are repeated until all frequent itemsets are discovered and non candidate itemset can be generated  A  Bit vectors encoding  In second step of CC-BIT, bit vector for every frequent item is generated. Every bit represents the existence of an item in a transaction in the DB, so the number of ones in the bit vector for item i is the support count of i Figure 2, shows the bit vectors of five items in database in Figure 1.  Bit vector for intra-itemset is generated by applying bitwise AND operation between the bit vectors of items in this intraitemset. In CC-BIT, to reduce processing time and required memory space, not all frequent intra- itemsets have bit vector representation only that are represented in at least one candidate inter-itemset  TID Items 1 a,b,e,g,j 4 c,f,j 6 a,e,d,h 9 a,c,e,h 10 b,c,f Fig. 1: Example dataset Fig. 2: Bit vector representation for five items in database in Figure 1   B  Support calculation  Support calculation of inter-itemset is a time consuming process. Moreover, it is frequently applied after every candidate generation process to evaluate candidate interitemsets. For that reason, reducing the execution time of support calculation increases the performance of mining process dramatically. CC-BIT uses the concept of cross correlation to increase the performance of support calculation process. The cross correlation is a measure of similarity of two functions. For discrete functions, cross correlation is defined as    1  Where f and g are two discrete functions r is cross correlation between f and g and k is time delay. In case of f  and g are two bit vectors can be generated by bitwise shift k times and multiplication in   can be replaced by bitwise AND operation. Finally summation can be calculated by counting ones in the bit vector. Cross correlation value at k = 3  represents the number of cases where f[n equal one and g[n+3 equal one If f and g are the bit vectors for items a and c respectively then r[3 will be the support count of inter-itemset a,0,0,c,0  Day Items  a  b  c  d  e 1 a,b,e,g,j  1  1  0  0  1 2  0  0  0  0  0 3  0  0  0  0  0 4 c,f,j  0  0  1  0  0 5  0  0  0  0  0 6 a,e,d,h  1  0  0  1  1 7  0  0  0  0  0 8  0  0  0  0  0 9 a,c,e,h  1  0  1  0  1 10 b,c,f  0  1  1  0  0 11  0  0  0  0  0 12  0  0  0  0  0 13  0  0  0  0  0 


 Equation 1 can be calculating into two steps as    1.a    1.b  In first step, equation 1.a, shifting and bitwise AND operation are applied. In second step, number of ones in resulting bit vector is calculated. Figure 3 shows the first step of calculating cross correlation for inter-itemset a,0,0,c,0   Inter-itemsets as a,0,0,c,0 contain only two individual items. So that the cross correlation can be applied directly for supports calculations. While inter-itemsets as a d,0,0,b c,0  that contain one or more intra-itemsets, the bit vectors for intra-itemsets must be calculated first before applying cross correlation. Another form of inter-itemsets contain more than two items or intra-itemsets  as d,0,c,0,a For this form we define multi cross correlation as      2  Equation 2 can be calculating into tree steps as    2.a    2.b    2.c  For inter-itemset b,0,c,0,a  f, g and q are the bit vectors for items b, c and a respectively while k =2 and l =4 In first step, equation 2.a, bit vector of item c is shifted by 2 where k 2. Then bitwise AND is applied between the shifted bit vector and bit vector for item b In step 2, equation 2.b, bit vector of item a is shifted by 4 where l 4. Then AND operation is applied for shifted bit vector and resulting bit vector from step one. In step 3, equation 2.c, number of ones in resulting bit vector from step two is calculated. The result of last step is the support count of inter-itemset b,0,c,0,a  For inter-itemsets have more than three intra-itemsets, Step 2 is repeated for all extra intra-itemset. Then ones in the resulting bit victor are counted to get the support count. This technique for calculating support count required less execution time and storage space compared to FITI  III. PERFORMANCE EVALUATION  The performance of the CC-BIT algorithm is compared to the FITI performance with different two datasets. Datasets are generated using the same method as a ble 1 Summarize the generated datasets parameters The FITI and CC-BIT algorithms are coded in java, and all the experiments are performed on a Core 2 Duo processor with 2 GB main memory running under Open Suse 11.0 Linux operating system. In implementing the algorithms Apriori algorithm is used for frequent intra-itemsets discovering. The results in Figure 4 show execution time versus minimum support. Minimum support varies from .002 008. For dataset 1, the execution time of CC-BIT is 8 times smaller than FITI at minimum support .002, and this ratio is reduced to 2 times at minimum support .007.  While in dataset2, the ratio varies between 12 and 1 in the two dataset FITI, execution time is more sensitive for reducing minimum support. That is because, reducing minimum support affects the performance of the three phases of FITI. In phase one reducing minimum support increases the execution time   Fig. 3: Example on first step proposed technique for inter-itemset support count calculations   T ABLE I Datasets parameters  Parameter Meaning Dataset 1 Dataset 2 D Number of transactions 20k 100k T Average size of transactions 6 10 MT Maximum size of the transactions 8 12 L Number of the potentially frequent itemsets 1000 1000 I Average size of the potentially frequent itemsets 2 6 MI Maximum size of the potentially frequent itemsets 6 8 N Number of items 1000 1000 MS  Maximum span 4 8  required to discover frequent intra-itemsets. Moreover, it increases both the number and the size of generated frequent intra-itemsets. Increasing the number of frequent intraitemsets increases the time for building the hash tables and different links. In phase two, increasing size of frequent intraitemsets increases number of FIT files. While in phase three reducing minimum support increases number of candidate inter-itemsets and required database scans   IV. CONCLUSION  We introduced the concept of using cross correlation in calculating  support for inter-transaction itemsets, and a  c  a,0,0,0,0   0,0,0,c,0  a,0,0,c,0  1  0  1  1  1 0  0  0  0  0 0  0  0  0  0 0  1  0  0  0 0  0  0  0  0 1  0  1  1  1 0  0  0  1  0 0  0  0  0  0 1  1  1  0  0 0  1  0  0  0 0  0  0  0  0 0  0  0  0  0 0  0  0  0  0  


provided technique for inter-transaction ARM. The proposed technique is based one bit vectors, bitwise operations, and cross correlation. Using bit v ector, reduces the required memory space compared to FITI algorithm. While using bitwise operations and concept of cross correlation, reduces the required execution time for support calculations, which implies that increasing in the proposed algorithm performance up to several orders of magnitude compared to FITI in most cases     Fig. 4: Change in execution time with different minimum support for FITI and CC-BIT algorithms   REFERENCES    A g ra w a l, T  Im ieli ns k i a n d A  S w a m i  M i n i n g association rules between sets of items in large databases in: Proceedings of ACM SIGMOD 1993, pp. 207–216 2 A  K  H   T ung H   Lu J  H a n a n d L Fe ng  E f f i c i e nt  mining of intertransaction association rules IEEE Transactions on Knowledge and Data Engineering vol. 15 no. 1, pp.43 – 56, 2003    L u J. H a n  a n d L   F e ng   B e y on d i n t r at ra n s act i o n  association analysis: mining multidimensional intertransaction association rules ACM Transactions on Information Systems vol. 18, no. 4, pp.423 - 454, 2000  R  A g ra w a l  an d R  S r ik a n t F a s t a l g o rit h m s f o r m i n i n g  association rules in: Proceedings of International Conference on Very Large Data Bases 1994, pp. 487–499  J  S. P a rk M.S. C h en a nd P  S. Yu  A n Eff ecti v e h a s h based algorithm for mining association rules in Proceedings of ACM SIGMOD 1995, pp. 175–186  d arin P   P u ch eral, an d F. W u  B it m a p bas e d algorithm for mining association rules in: Springer, 14 th  Bases de Donnes Avances Tunisia, 1998, pp. 157-176   Z a k i   S calable al g o rithm  f o r as s o ciation m i n i ng    in: IEEE Transaction on Knowledge and Data Engineering vol. 12, no. 3, pp. 372-390  8 J  H a n a n d J  P e i  M i n i n g fr e q ue nt p a t t e r n  gr o w t h   Methodology and implications in: ACM SIGKDD Explorations 2000, vol. 2, no. 2, pp. 14-20   J   T  L ee, C h ung Sh e ng W a n g  An eff i cie n t alg o rit h m  for mining frequent inter-transaction patterns in: Elsavier international journal on Information Sciences pp. 34533476, 2007 


Fig 2 Feature of the relation   4.4. Ensemble  Ensemble learning as one of the most research directions in machine learning has received much attention [13 t h e s t ep 9 of T a bl e 3  w e i m prov e  prediction performance with an ensemble of base learners. The difficulty of scoring, as analyzed in section 2, is tackled in the segment. In ensemble, we classify a sample by threshold voting and then compute its score on average according to the classification result. The detail process is as follows When scoring, we adjust the threshold variable v  For a sample, if it is classified to the positive class by no less base learners than v we will classify it to the positive class, and its score is the average of probabilities 2 of the base learners who classify it to the positive class; Otherwise, it will be classified to the negative class, and its score is the average of positive probabilities of base learners who classify it to negative class  Fig 3 Relation between the threshold v and accuracy Fig.3 reflects that when v is 15, the true positive fraction achieves the highest value, and the false negative achieves a higher value. Considering producing a low score for a potential home loan customer is with a higher cost than producing a high    2 In LIB_SVM, SVM can provide two probabilities for each sample: one is the probability of the sample classified to the positive class, and the other is that of the sample classified to the negative class. The sum of the two probabilities is 1 score for a credit card customer, as analyzed in section 2, we prefer to misclassify negative samples other than positives when we must make a choice alternatively, so we set the threshold v to be 15. The accuracies of the two classes for different thresholds are showed in Fig.3  5. Evaluation  Prediction results have been reported on dataset of real world provided by PAKDD competition 2007 with AUC as the evaluation metric 3 Some representative modeling algorithms and their AUC values are shown in shown in Table 6. It is shown that our solution is effective with its AUC value 60.73%. It is evident that TreeNet + Logistic Regression” and “MLP + n-Tuple Classifier” are much better than the other algorithms The grand champion team of the competition propose TreeNet + Logistic Regression” to solve the crossselling task with the AUC value is 70.01%. And our solution works better than “Neural Network Decision Tree” and “Predicted Apriori Association Rules + Nearest Neighbors  Table 6 Predicting results of some representative algorithms  Algorithms AUC TreeNet + Logistic Regression 70.01 MLP + n-Tuple Classifier 69.62 Our solution 60.73 Structural Resonance in Multi-dimensional Data 56.35 Neural Network 55.96 Random Forest 55.16 Decision Tree 55.53 Tree Boosting 52.42 Customer Growth Model Clustering Support Vector Machine 52.24 Predicted Apriori Association Rules Nearest Neighbors 50.06  6. Conclusion  The solution to cross-selling problems proposed in the paper is an ensemble method based on majority voting, whose essence can be interpreted as follows Since we train twenty different base learners with various training datasets, we can get twenty groups of different support vectors, each of which establish a    3 The competition result can be seen from http://lamda.nju.edu.cn/conf/pakdd07/dmc07/results.htm, and the ID of our team is P060  
132 
132 


classification hyperplane in a 40-dimension space 4  Ensembling base learners with the threshold v equals 15 means combining at least 15 hyperplanes to separate a group of subspaces for positive class from the 40-dimension space. These combined hyperplanes in the 40-dimension space  works as a piecewise linear classifier in a 2-dimension space in some sense. The only difference is these combined hyper-planes are not static, but dynamic when classifying a given sample Formally, let N L L L    2 1 represent the N hyperplanes. For a sample x if x in the subspace v L L L    2 1 where  j L      2 1 N L L L and threshold predefined v    our algorithm will classify it to the positive class Otherwise, it will be classified to the negative class  In the paper, we proposes an ensemble method to solve cross-selling problems. The task is of class imbalance and cost-sensitivity. We combine oversampling and under-sampling methods to solve the class imbalance problem, and solve cost-sensitivity by adjusting the parameters of base learners. At last, we adopt majority voting to get an ensemble of base learners. Experiment on prediction dataset provided by PAKDD Competition 2007 shows our solution is effective and efficient Though we a dopt SVM as the base learner and it has been shown effective, there may be more suitable algorithms to solve cross-selling problems, since our solution is a wrapper ensemble method. So exploring more effective and efficient methods for the task seems to be interesting. Besides majority voting, more advanced ensemble strategies are anticipated to get a better performance  Under a deeper analysis to the dataset, it is found that there are several redundant and irrelevant variables in cross-selling datasets, so effective variables selection will improve performance of the solution proposed in this paper. Since the task is to predict a score for each customer, logic regression and learning to rank methods can be anticipated to exhibit better performances in the task, and it is implied from the competition result in some sense. In the view of decision making, learning to rank methods may be a better solution compared to classification and regression methods, since the decision makers have more freedom to the quantity or the proportion of cross-selling customers according to their ranks  7. References  1 S  B. L i B  H. S u n a n d  R. W ilc ox  C ross-se lling  sequentially ordered products: an application to consumer    4 The modeling dataset is 40 dimensions, so it is a 40-dimension space banking services Journal of Marketing Research 42\(2  2004, pp.233-239 2 R  C  W  W ong A  W  C  Fu, K  W a ng D a t a  m i ning  f o r  inventory item selection with cross-selling considerations Data Mining and Knowledge Discovery, 11\(1 2005, pp.81112 3 B Bh ask e r, H.H P a rk  J P a rk an d H S. Kim   P ro d u c t  recommendations for cross-selling in electronic business”, In Proceedings of the Nineteenth Australian Joint Conference on Artificial Intelligence \(AUS-AI’06 Lecture Notes in Computer Science \(LNCS\ 4304, Springer, 2006 pp. 10421047 4 J Sc he f f e r  D e a ling w ith m i s s ing da ta    Research Leters of  Information and  Mathematical Science, vol.3 2002 pp.153-160 5 R  A k ba ni, S. K w e k a nd N  J a pk ow ic z  A ppl y i ng  support vector machines to imbalanced datasets”, In Proceedings of the Fifteenth European Conference on Machine Learning \(ECML2004 Lecture Notes in Artificial Intelligence, pp.39-50  Daviso n and D  V Hin k ley  Bootstrap Methods and Their Application Cambridge University Press A.C.,1997 7 S. L e s s m a nn S olv i ng im ba la nc e d c l a s s i f i c a tion Problems with Support Vector Machines”, In Proceedings of the International Conference on Artificial Intelligence ICAI’04 pp.214-220 8 S. F. C r one S  L e s s m a nn, a n d R  Sta h l boc k   E m p ir ic a l  Comparison and Evaluation of Classifier Performance for Data Mining in Customer Relationship Management”, In Proceedings of the IEEE 2004 International Joint Conference on Neural Networks \(IJCNN2004 2004, pp.443448 9 T h e L A MD A e r T e a m P red i ctin g Fu tu re Cu st o m ers v i a  Ensembling Gradually Expanded Trees presented at PAKDD’06 Competition 2006 10  Z.Q. Bia n a n d X  G  Zha n g   Pattern Recognition   Tsinghua Publish House, Beijing, 2000, pp.299-303. \(in Chinese 11 Y   Xia o a n d C  Z. H a n   M e t ho d of R e duc ing Fa ls e  Positive Alerts Based on Support Vector Machine in Intrusion Detection Computer Engineering  Vol.32 No.17 2006, pp.25-27  1 N  V  Ch aw la L  O Hall  K W  Bo wyer an d W  P   Kegelmeyer, “SMOTE: Synthetic Minority Oversampling Technique Journal of Artificial Intelligence Research, 16 2002 pp. 321-357 1 Z  H Zh ou a n d W Tan g  C l u st er er e n sem b l e   Knowledge-Based Systems 19 \(1 2006,  pp.77-83 14 H. Lappalainen, J. W. Miskin, “Ensemble Learning Advances in Independent Component Analysis M. Girolami Ed., Berlin, Springer Verlag Scientific Publishers, 2000 pp.75-92 15 A  Krog h  P  S o llic h  S ta tist i c a l m e c h a n ic s of e n se m b le  learning Physical Review E  55\(1 1997, pp.811-825    
133 
133 


 Kluwer Academic Publishers Springer, New York 1st edition, 2001 14  S c h e f f e r   T   F i n d i n g  A s s o c i a t i o n  Ru l e s  t h a t  T r a de Support Optimally Against Confidence th The Elements of Statistical Learning self_care_guide/Urogenital/Postate%20Cancer.pdf  Accessed, 25 August, 2008 11  A g r a w a l   R  T   I m i e l i n s k i     A   S w a m i   M i n i n g  association rules between sets of items in large databases, In Proceedings of the 1993 ACM SIGMOD international conference on Management of data  The Netherlands 42 2001 61-95  Ordonez C Association rule discovery with the train and test approach for heart disease predictio n 207\226 216 12 001 13  H a s t i e   T    R  T i b s h i r a n i     J  H   F r i e d m a n   Proceedings of the 5th European Conference on Principles and Practice of Knowlege Discovery in Databases\(PKDD'01 IEEE Transactions on Information Technology in Biomedicine, 10\(2\, 2006. 334 \226 343 001 Freiburg, Germany : SpringerVerlag, 2001. 424-435 15  F l a c h   P  A     L a c h i c h e   N   Co n f i r m a t i o n g u i d e d  discovery of first-order rules with Tertius 10  P h a r m a c y   h t t p    w w w  p h a r m a c y  g o v  m y    


 7. Conclusions  In this paper we have proposed an intelligent and efficient technique to reassess the distances between dynamic XML documents when one or all of the initially clustered documents have changed. After the changes, the initial clustering solution might become obsolete - the distances between clustered XML documents might have changed more or less depending on the degree of modifications \(insert update, delete\hich have been applied. Re-running full pair-wise comparisons on the entire set of modified documents is not a viable option, because of the large number of redundant operations involved Our proposed technique allows the user to reassess the pair-wise XML document distances, not by fully comparing each new pair of versions in the clustering solution, but by determining the effect of the temporal changes on the previously known distances between them. This approach is both time and I/O effective, as the number of operations involved in distance reassessing is greatly reduced  References  1  Beringer, J. and H\374llermeier, E., Online clustering of parallel data streams Data and Knowledge Engineering 58\(2\,  2006, 180-204 2  Catania, B. and Maddalena A., A Clustering Approach for XML Linked Documents, Proceedings of the 13th International Workshop on Database and Expert Systems Applications \(DEXA\22202\, IEEE 2002 3  Chen, M.S., Han, J. and Yu, P., Data Mining: An Overview from Database Perspective, IEEE Transactions on Knowledge and Data Engineering vol. 8, 1996, 866-883 4  Cormen, T., Leiserson, C. and Rivest, R Introduction to algorithms, MIT Press, 1990 5  Costa, G., Manco, G., Ortale, R. and Tagarelli, A., A tree-based Approach to Clustering XML documents by Structure, PAKDD 2004, LNAI 3202, 137-148 Springer 2004 6  Dalamagas, T., Cheng, T., Winkel, K.J. and Sellis, T 2004, Clustering XML documents by Structure SETN 2004, LNAI 3025, 112-121, Springer 2004 7  Ester, M., Kriegel, H.P., Sander, J., Wimmer,M. and Xu, X., Incremental Clustering for Mining in a Data Warehousing Environment, Proc.of the 24 th VLDB Conference, New York, USA, 1998 8  Garofalakis, M., Rastogi, R., Seshadri, S. And Shim K., Data Mining and the Web: Past, Present and Future Proceedings of WIDM 99 Kansas, US, ACM 1999 9  Mignet, L., Barbosa, D. and Veltri, P., The XML web : a first study, In Proceedings of the 12 th  International Conference on WWW, 500-510 2003   Nayak, R., Xu, S., XCLS: A Fast and Effective Clustering Algorithm for Heterogeneous XML Documents, In Proceedings of the 10 th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, Singapore, LNCS 3918, 2006   Rusu, L.I., Rahayu, W. and Taniar, D., A methodology for Building XML Data Warehouses International Journal of Data warehousing Mining, 1\(2 67-92, 2005   Rusu, L.I., Rahayu, W. and Taniar D.,  Maintaining Versions of Dynamic XML Documents, In Proceedings of the 6th International Conference on Web Information Systems Engineering, New York NY, USA, November 20-22, 2005, LNCS 3806   Rusu, L.I., Rahayu, W. and Taniar, D., Warehousing Dynamic XML Documents, In Proceedings of the 8 th  International Conference on Data Warehousing and Knowledge Discovery \(DaWaK 2006 LNCS 4081 Springer, 175-184, 2006   Shen, Y. and Wang, B., Clustering Schemaless XML documents, CoopIS / DOA/ODBASE 2003, LNCS 2888, 767-784, Springer 2003   Yoon, J. P., Raghavan, V., Chakilam, V., and Kerschberg, L., BitCube: A Three-Dimensional Bitmap Indexing for XML Documents J. Intel. Inf Syst 17, 2-3 \(Dec. 2001\, 241-254   XML data repository, online at http www.cs.washington.edu / research / projects / xmltk xmldata  
456 
456 


5 Related Work There exists extensive previous work on both the mining of software repositories and on the use of clustering algorithms in software engineering This discussion focuses on the most similar and recent work in the area of software evolution Mining Software Repositories Our technique was partially inspired by the work of Zimmermann et al and Y ing et al 17 on the mining of association rules in change history As described in Section 1 we sought to expand the technique to be able to recommend larger but less precise clusters of elements to guide program navigation Bouktif et al also investigated how to recommend cochanges in software development As opposed to the work cited above Bouktif et al used change patterns instead of association rules Also their approach does not attempt to reconstruct transactions and can consider associated 002les that were changed in different transactions ChangeDistiller is a tool to classify changes in a transaction into 002ne-grained operations e.g addition of a method declaration and determines how strongly the change impacts other source code entities Our approach uses similar repository analysis techniques but is focused on providing task-related information as opposed to an overall assessment of a system's evolution Finally repository mining can also be used to detect aspects in the code In this conte xt aspects are recurring sets of changed elements that exhibit a regular structure Aspects differ from the clusters we detect in the regular structure they exhibit which may not necessarily align with the code that is investigated as part of change tasks Clustering Analysis The classical application of clustering for reverse engineering involves grouping software entities based on an analysis of various relations between pairs of entities of a given version of the system Despite its long and rich history  e xperimentation with this approach continues to this day For example Andreopoulos et al combined static and dynamic information K uhn et al used a te xtual similarity measure as the clustering relation and Christl et al used clustering to assist iterative semi-automated reverse engineering The main dif ferences b e tween most clusteringbased reverse engineering techniques and the subject of our investigation is that the entities we cluster are transactions rather than software entities in a single version of a system For this reason our analysis is based strictly on the evolving parts of the system Both Kothari et al and V an ya et al 15 recently reported on their use of clustering to study the evolution of software systems The idea of using change clusters is the same in both works and ours but the purpose of the work is different Kothari et al use change clusters to uncover the types of changes that happened e.g feature addition maintenance etc during the history of a software system Vanya et al use change clusters which they call evolutionary clusters to guide the partitioning of a system that would increase the likelihood that the parts of the system would evolve independently In contrast we cluster transactions based on overlapping elements not 002les to recommend clusters to support program navigation as opposed to architectural-level assessment of the system Finally Hassan and Holt evaluated on 002ve open source systems the performance of several methods to indicate elements that should be modi\002ed together This study found that using historical co-change information as opposed to using simple static analysis or code layout offered the best results in terms of recall and precision The authors then tried to improve the results using 002ltering heuristics and found that keeping only the most frequently cochanged entities yielded the best results As opposed to our approach the evaluated 002ltering heuristics were only applied on entities recovered using association rules and not using clustering techniques The focus of their study was also more speci\002c as they recommend program elements that were strictly changed  as opposed to recommending elements that might be inspected by developers 6 Conclusion Developers often need to discover code that has been navigated in the past We investigated to what extent we can bene\002t from change clusters to guide program navigation We de\002ned change clusters as groups of elements that were part of transactions or change sets that had elements in common Our analysis of close to 12 years of software change data for a total of seven different open-source systems revealed that less than 12 of the changes we studied could have bene\002ted from change clusters We conclude that further efforts should thus focus on maximizing the quality of the match between the current task and past transactions rather than 002nding many potential matches Our study has already helped us in this goal by providing reliable evidence of the effectiveness of some 002ltering heuristics and useful insights for the development of additional heuristics Acknowledgments The authors thank Emily Hill and Jos  e Correa for their advice on the statistical tests and the anonymous reviewers for their helpful suggestions This work was supported by NSERC 
25 
25 
25 
25 
25 


References  B Andreopoulos A An V  Tzerpos and X W ang Multiple layer clustering of large software systems In Proc 12th Working Conf on Reverse Engineering  pages 79–88 2005  S Bouktif Y G Gu  eh  eneuc and G Antoniol Extracting change-patterns from cvs repositories In Proc 13th Working Conf on Reverse Engineering  pages 221–230 2006  S Breu and T  Zimmermann Mining aspects from v ersion history In Proc 21st IEEE/ACM Int'l Conf on Automated Software Engineering  pages 221–230 2006  A Christl R K oschk e and M.-A Store y  Equipping the re\003exion method with automated clustering In Proc 12th Working Conf on Reverse Engineering  pages 89–98 2005  D 020 Cubrani  c G C Murphy J Singer and K S Booth Hipikat A project memory for software development IEEE Transactions on Software Engineering  31\(6 465 2005  B Fluri and H C Gall Classifyi ng change types for qualifying change couplings In Proc 14th IEEE Int'l Conf on Program Comprehension  pages 35–45 2006  A E Hassan and R C Holt Replaying de v elopment history to assess the effectiveness of change propagation tools Empirical Software Engineering  11\(3 2006  D H Hutchens and V  R Basili System s tructure analysis Clustering with data bindings IEEE Transactions on Software Engineering  11\(8 1985  D Janzen and K De V older Na vig ating and querying code without getting lost In Proc 2nd Int'l Conf on AspectOriented Software Development  pages 178–187 2003  J K ot hari T  Denton A Shok ouf andeh S Mancoridis and A E Hassan Studying the evolution of software systems using change clusters In Proc 14th IEEE Int'l Conf on Program Comprehension  pages 46–55 2006  A K uhn S Ducasse and T  G  021rba Enriching reverse engineering with semantic clustering In Proc 12th Working Conf on Reverse Engineering  pages 133–142 2005  M P  Robillard T opology analysis of softw are dependencies ACM Transactions on Software Engineering and Methodology  2008 To appear  M P  Robillard and P  Mangg ala Reusing program in v estigation knowledge for code understanding In Proc 16th IEEE Int'l Conf on Program Comprehension  pages 202 211 2008  J Sillito G Murph y  and K De V older Questions programmers ask during software evolution tasks In Proc 14th ACM SIGSOFT Int'l Symposium on the Foundations of Software Engineering  pages 23–34 2006  A V an ya L Ho\003and S Klusener  P  v an de Laar and H van Vliet Assessing software archives with evolutionary clusters In Proc 16th IEEE Int'l Conf on Program Comprehension  pages 192–201 2008  N W ilde and M C Scully  Softw are reconnaissance Mapping program features to code Software Maintenance Research and Practice  7:49–62 1995  A T  Y ing G C Murph y  R Ng and M C Chu-Carroll Predicting source code changes by mining change history IEEE Transactions on Software Engineering  30\(9 586 2004  A Zeller  The future of programming en vironments Integration synergy and assistance In Proceedings of the 29th International Conference on Software Engineering The Future of Software Engineering  pages 316–325 2007  T  Zimmermann and P  W eißgerber  Preprocessing C VS data for 002ne-grained analysis In Proc 1st Int'l Workshop on Mining Software Repositories  pages 2–6 May 2004  T  Zimmermann P  W eißgerber  S Diehl and A Zeller  Mining version histories to guide software changes In Proc 26th ACM/IEEE Int'l Conf on Software Engineering  pages 563–572 2004 A Clustering Algorithm This algorithm is not sensitive to whether a given program element exists or not in a given version of a program For example if method m exists in one version it is considered a valid program element even if it is removed in a later version In the rest of this section we use the term program element to refer to the uniquely identifying representation of the element e.g a Java fully-quali\002ed name Let T be a transaction modeled as a set of program elements changed together during the history of a software system Let T be a sequence of transactions In this algorithm a cluster is also modeled as a set of elements 1 Input  T  A sequence of transactions 2 Parameter  M IN O VERLAP  A positive non-zero value indicating the minimum overlap between two transactions in a cluster 3 Var  C  A set of clusters initially empty 4 for all T i 2 T do 5 MaxOverlap  0 6 MaxIndex  000 1 7 for all C j 2 C do 8 if j C j  T i j  MaxOverlap then 9 MaxOverlap  j C j  T i j 10 MaxIndex  j 11 end if 12 end for 13 if MaxIndex   0  MaxOverlap 025 M IN O VERLAP  then 14 C MaxIndex   C MaxIndex  T i  15 else 16 NewCluster  T i 17 C  C  f NewCluster g 18 end if 19 end for 20 return C B Systems Analyzed System home pages last veri\002ed 7 May 2008 Ant ant.apache.org Azureus azureus.sourceforge.net Hibernate www.hibernate.org JDT-Core www.eclipse.org/jdt/core JDT-UI www.eclipse.org/jdt/ui Spring springframework.org Xerces xerces.apache.org 
26 
26 
26 
26 
26 


