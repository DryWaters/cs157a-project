Mining Asso ciation Rules with W eigh ted Items C.H Cai Ada W.C F u C.H Cheng and W.W Kw ong Departmen t of Computer Science and Engineering The Chinese Univ ersit y of Hong Kong c hcai,adafu,c hc heng,wwkw ong@cse.cuhk.edu hk Abstract Disc overy of asso ciation rules has b e en found useful in many applic ations In pr evious work al l items inab asket datab ase ar etr e ate d uniformly We gener alize this to the c ase wher e items ar e given weights to r e\015e ct their imp ortanc e to the user The weights may c orr esp ond to sp e cial pr omotions on some pr o ducts or the pr o\014tability of di\013er ent items We c an mine the weighte d asso ciation rules with weights The downwar d closur epr op erty of the supp ort me asur ein the unweighte dc ase no longer exist and pr evious algorithms c annot b e applie d In this p ap er two new algorithms wil l b e intr o duc e d to hand le this pr oblem In these algorithms we make use of a metric c al le d the k supp ort b ound in the mining pr o c ess Exp erimental r esults show the e\016ciency of the algorithms for lar ge datab ases Keyw ords  data mining asso ciation rules b asket data supp ort c on\014denc e weighte d items 1 In tro duction Computers store large amoun ts of retailing transactions in a retailing business Mark eting managers will b e in terested in useful information that can b e extracted from these large databases As the amoun t of retailing information increases dramatically  there is a new c hallenge of 014nding in terested information e\016cien tly  One promising approac h is the mining of asso ciation rules for bask et databases whic h has b een in v estigated b y[1  2  6  7  5  etc Most of these w orks are fo cused on mining binary asso ciation rules A binary asso ciation rule Buys  Bread   Buys  Ham  with suppor t  0.6 conf idence  0.8 sa ys that the probabilit y of buying bread and ham is 0.6 and the probabilit y of buying ham is 0.8 giv en that a transaction con tains bread The in terestingness of the rule dep ends on the n um b er of o ccurrences of bread and ham In this pap er w ein tro duce the notion of w eigh ted items to represen t the imp ortance of individual items In a retailing business a mark eting manager ma yw an t to mine the asso ciation rules with more emphasis on some particular pro ducts in mind and less emphasis on other pro ducts F or example some pro ducts ma y b e under promotion and hence are more in teresting or some pro ducts are more pro\014table and hence rules concerning them are of greater v alues This results in a generalized v ersion of asso ciation rule mining problem whic hw e call w eigh ted asso ciation rule mining F or example if the pro\014t of the sofa is m uc h higher than the b ed then the rule Buys  Pillo w   Buys  Sofa  is more in teresting than Buys  Pillo w   Buys  Bed  When w e compute the w eigh ted supp ort of the rule w e can consider b oth the supp ort and the imp ortan t ratio w eigh ts factors A simple attempt to solv e the problem is to eliminate the en tries of items with small w eigh ts Ho w ev er a rule for a hea vy w eigh ted item ma y also consist of lo ww eigh ted items F or example w ema y b e promoting a pro duct A and 014nd that it is a\013ected b y another pro duct B for whic hw eha v e initially no interest Hence the simple approac h do es not w ork in this case Another approac h is adopting the existing fast algorithms for 014nding binary asso ciation rules suc has the Apriori Gen Algorithm Suc h algorithms dep end on the do wn w ard closure prop ert y whic hgo v erns that subsets of a large itemset are also large Ho w ev er it is not true for the w eigh ted case in our de\014nition and the Apriori Algorithm cannot b e applied In this pap er w e prop ose new algorithms to mine w eigh ted binary asso ciation rules Tw o algorithms MINW AL\(O and MINW AL\(W are designed for this purp ose Exp erimen tal result sho ws 


that these algorithms ha v e reasonable p erformance for large databases and MINW AL\(O p erforms b etter than MINW AL\(W in most cases The pap er is organized as follo ws In Section 2 the de\014nitions of mining w eigh ted asso ciation rules will be in tro duced In Sections 3 and 4 algorithms will b e describ ed for t w o di\013eren t de\014nitions of w eigh ted supp ort P erformance study will b e rep orted in Section 5 Finally  Section 6 is a conclusion 2 W eigh ted Asso ciation Rules Similar to and 5 w e consider a database with a set of transactions D  and a set of items I  f i 1 i 2   i n g  Eac h transaction is a subset of I  and is assigned a transaction iden ti\014er h TID i  De\014nitio n1 A n asso ciation rule has the form of X  Y  wher e X 032I  Y 032I  and X  Y    W e de\014ne the terms of supp ort and the con\014dence as in De\014nitio n2 The supp ort of the asso ciation rule X  Y is the pr ob ability that X  Y exists in a tr ansaction in the datab ase D  De\014nitio n3 The con\014dence of the asso ciation rule X  Y is the pr ob ability that Y exists given that a tr ansaction c ontains X  i.e Pr  Y n X  Pr  X  Y  Pr  X  1 In large databases the supp ort of X  Y is tak en as the fraction of transactions that con tain X  Y  The con\014dence of X  Y is the n um b er of transactions con taining b oth X and Y divided b y the n um ber of transactions con taining X  Giv en a set of items I  f i 1 i 2  001\001\001 i n g w e assign a w eigh t w j for eac h item i j  with 0 024 w j 024 1 where j  f 1  2  001\001\001 n g  to sho w the imp ortance of the item According to De\014nition 2 w e can de\014ne the w eigh ted supp ort for the w eigh ted asso ciation rules De\014nitio n4 The w eigh ted supp ort of a rule X  Y is 0  X i j 2  X  Y  w j 1 A  S uppor t  X  Y  2 Similar to 1  a supp ort threshold and a con\014dence threshold will b e assigned to measure the strength of the asso ciation rules Bar co de Item T otal Pro\014t w eigh ts  1 Apple 100 0.1  2 Orange 300 0.3  3 Banana 400 0.4  4 Milk 800 0.8  5 Co ca-cola 900 0.9  T able 1 Example of pro duct database TID Bar co des TID Bar co des 1 1245 2 145 3 245 4 1245 5 135 6 245 7 2345 T able 2 T ransaction database De\014niti on 5 A k itemset X is c al le da small itemset if the weighte d supp ort of such itemset is less than the minimum weighte d supp ort  w minsup  thr eshold or 0  X i j 2 X w j 1 A  S uppor t  X   w minsup 3 Otherwise X is a large k itemset  De\014niti on 6 A n asso ciation rule X  Y is c al le d an in terest in g rule if X  Y is a lar ge itemset and the c on\014denc e de\014ne d in de\014nition 3 of the rule is gr e ater than or e qual to a minimum c on\014denc e thr eshold Example 1 Supp ose in a retailing store a database is sho wn in T ables 1 and 2 T able 1 sho ws the information ab out the items in the retailing store The information includes the bar co de n um b er of the items the names of suc h item the total pro\014ts of the items the giv en w eigh ts etc T able 2 sho ws the transaction database F or eac h transaction there will b e a transaction identi\014er h TID i  and the bar co de n um b ers of the items F or simplicit y  the bar co de n um b ers will b e represen ted in the form of natural n um b ers f 1  001\001\001  5 g  Supp ose that there are 5 items and totally 7 transactions in the transaction database If the v alue of w minsup is 0.4 then f 2 5 g will b e a large itemset since 0.3+0.9 002 5 7  0.86 025 0.4 


By the same argumen t f 4 5 g and f 2 4 5 g will b e large itemsets 2 2.1 Reasoning b ehind the problem de\014nition In this problem w ew an t to get a balance b et w een the t w o measures whic h are w eigh t and supp ort W e ha v e three parameters to consider in the w eigh ted asso ciation rule w eigh ts of items supp ort of itemsets and the con\014dence factor W eha v ec hosen to compute a w eigh ted supp ort of an itemset whic histhe pro duct of the total w eigh t of items in the itemset and the supp ort of the itemset Supp ose w e separate the supp orts and w eigh ts W e can only 014nd itemsets ha ving b oth su\016cien t supp ort and w eigh t Ho w ev er this ma y ignore some in teresting kno wledge The seman tics of w eigh t is a measure of the imp ortance of an itemset If an itemset is v ery imp ortan t for example it is under promotion or it is highly pro\014table then ev en if not man y customers ha v e b ough t it it is still an in teresting itemset to the user On the other hand if an itemset is not considered v ery imp ortan t in terms of the w eigh ts but it is v ery p opular so that man y transactions con tain it it is also an in teresting itemset Another feasible alternativ e is to 014nd itemsets that ha v e either su\016cien t supp ort or w eigh t Ho w ev er this ma y not allo w us to handle zero w eigh t no in terest items e\016cien tly  no matter ho w high the supp ort ma y be There is one p ossible problem with our de\014nition Ev en if eac h item has a small w eigh t when the n um ber of items in an itemset is large the total w eigh tma y b e large Dep ending on the application requiremen ts this ma yor ma y not b e desirable It ma y b e desirable if the user considers a rule with a n um b er of items whic h con tribute to a su\016cien t pro\014tabilit y together is in teresting It ma y not b e desirable if a rule with man y ligh tw eigh ted items should not b e considered in teresting Here w e also consider another problem de\014nition in whic h the w eigh ted supp ort of an itemset is normalized b y the size of the itemset This will b e discussed in Section 4 The seman tics of the rules will b e di\013eren t and it will dep end on the need of eac h application to consider the applicabilit y of this normalization 3 Mining W eigh ted Asso ciation Rules A new algorithm is needed to solv e the mining of the w eigh ted asso ciation rules An e\016cien t algorithm for mining binary asso ciation rules has b een prop osed in 1  called Apriori Gen The reason wh y Apriori Gen w orks is b ecause if an itemset is large all the subsets of that itemset m ust b e large Ho w ev er for the w eigh ted case the meaning of large itemset is mo di\014ed to handle w eigh ted supp ort It is not necessary true for all subsets of a large itemset b eing large In Example 1 f 2 4 g is a subset of the large itemset f 2 4 5 g  but it is not a large itemset 3.1 k-Supp ort Bound Giv en a database with T transactions w e de\014ne the supp ort coun t SC of a large k itemset X to b e the transaction n um b er con taining X  and it m ust satisfy SC  X  025 w minsup 002 T P i j 2 X w j 4 Let I b e the set of all items Supp ose that Y is a q itemset where q<k  In the set of the remaining items  I\000 Y  let the items with the  k 000 q  greatest w eigh ts i r 1 i r 2   i r k 000 q  W e can sa y the maxim um p ossible w eigh t for an y k itemset con taining Y is W  Y k  X i j 2 Y w j  k 000 q X j 1 w r j 5 in whic h the 014rst sum is the sum of the w eigh ts for the q itemset Y  and the second sum is the sum of the  k 000 q  maxim um remaining w eigh ts F rom Inequalit y 4 the minim um coun t for a large k itemset con taining Y is giv en b y B  Y k  030 w minsup 002 T W  Y k  031 6 W e called this the k supp ort b ound of Y W e tak e an upp er b ound of the v alue since the function B  Y k  is an in teger Example 2 Refer to T ables 1 and 2 the 3-supp ort b ound for the itemset f 2 4 g is 030 0  4 002 7 0  3+0  8  9 031 2 7 It means if the itemset f 2 4 g is the subset of an y large 3-itemsets the coun t of the itemset f 2 4 g m ust b e greater than or equal to 2 2 The algorithm for mining w eigh ted asso ciation rules can b e established according to the ab o v e prop erties of the k supp ort b ound for all p ossible k itemsets 


3.2 Algorithm for Mining W eigh ted Asso ciation Rules An algorithm for mining w eigh ted asso ciation rules has the follo wing inputs and output Inputs A database D with the transactions T  t w o threshold v alues w minsup and minconf w eigh ts of the items w i  with ascending order total n um ber of transactions and the total n um b er of the items Output A list of in teresting rules Notations D The database w the set of item w eigh ts L k set of large k itemsets C k set of k itemsets whic hma ybe k subsets of large j itemsets for j 025 k SC  X  no of transactions con taining itemset X w minsup w eigh ted supp ort threshold minconf con\014dence threshold Algorithm 1 MINW AL\(O 1 Main Algorithm  w minsup minconf  D w 2 size Searc h  D  3 L   4 for i=1;i 024 size;i 5 C i  L i    6 for eac h transaction do 7 SC C 1  Coun ting  D w 8 k=1 9 while  j C k j\025 k  10 k 11 C k  Join  C k 000 1  12 C k  Prune  C k  13  C k  L k  Chec king  C k  D  14 L L S L k  15 Rules SC L 16 ends The subroutines are outlined as follo ws 1 Searc h  D  The subroutine accepts the database 014nds out the maxim um size of the large itemset in that transaction database D  and returns the maxim um size 2 Coun ting  D w This subroutine cum ulates the supp ort coun ts of the 1-itemsets The k supp ort b ounds of eac h 1-itemset will b e calculated and the 1-itemsets with supp ort coun ts greater than an y of the k supp ort b ounds will b e k ept in C 1  3 Join  C k 000 1  The Join step generates C k from C k 000 1 as in F or example if w eha v e f 1 2 3 g and f 1 2 4 g in C k 000 1  f 1 2 3 4 g will b e generated in C k  4 Prune  C k  During the prune step the itemset will b e pruned in either of the follo wing cases  a A subset of the candidate itemset in C k do es not exist in C k 000 1  b Estimate an upp er b ound on the supp ort coun t SC of the joined itemset X  whic h is the minim um supp ort coun t among the k di\013eren t k 000 1 of X in C k 000 1  If the estimated upp er b ound on the supp ort coun t sho ws that the itemset X cannot b e a subset of an y large itemset in the coming passes from the calculation of k supp ort b ounds for all itemsets that itemset will b e pruned 5 Chec king  C k  D  The c hec king pro cedure scans the transaction database up dating the coun ts of all candidate itemsets in C k  By the similar metho d in the prune step prune the candidate itemsets for those not satisfying the supp ort coun t b ounds for all p ossible large itemsets The remaining candidate itemsets will b e k ept in C k A t the same time the large itemsets L k will b e generated from C k b yc hec king the exact w eigh ted supp ort of the itemsets 6 Rules  S uppor t count L  Find the rules from the large itemsets L similar to The framew ork of our prop osed algorithm for mining w eigh ted asso ciation rules is similar to the Apriori Gen Algorithm but the detailed steps con tain some signi\014can t di\013erences T o b egin with w e also generate large itemsets with increasing sizes Ho wev er since the subset of a large itemset ma y not b e large w e cannot generate candidate k itemsets simply from the large  k 000 1 as in Apriori Gen W e shall 014nd a w a yto k eep the k itemsets whic hma y generate large j itemsets for j 024 k  in the follo wing passes In order to extract suc h k itemsets from the database w e use the j supp ort b ound v alues During the op eration the j supp ort b ounds will b e calculated for all the candidate k itemsets where j is an yn umber bet w een k and the maxim um p ossible size of the large itemset If the coun t of the existing k itemset is less than all of the j supp ort b ounds w e can sa y that it cannot b e the subset of an y large itemsets in the coming passes and it can b e pruned The k itemset whic hma y con tribute to b e subsets of  future large itemsets will b e k ept in C k  Example 3 F rom the T ables 1 and 2 w e will sho who w the large itemsets are generated from the transaction database 


Supp ose the w minsup w eigh ted minim um supp ort threshold is 1 1 During the searc h pro cess the algorithm will scan the size of eac h transaction only  and returns the maxim um p ossible size of the large itemsets whic h is 4 in this example 2 P ass I k=1 where k is the size of the itemset During the coun ting step the transaction database will b e scanned once similar in P ass I of the Apriori Gen 1  The coun ts of the items 1-itemsets will b e found during this stage F or eac h item w e can calculate the supp ort b ounds for all coming passes from the information item supp orts coun t and w eigh ts F or this example the coun ts of the items f 1,2,3,4,5 g are f 4,5,1,6,7 g resp ectiv ely  Let us denote the 1-itemset con taining item 1 b y I a  The k supp ort b ounds of the k itemsets con taining I a are giv en b y B  I a k  B  I a  2  l 1 002 7 0  1+0  9 m 8 B  I a  3  l 1 002 7 0  1+\(0  8+0  9 m 4 B  I a  4  l 1 002 7 0  1+\(0  4+0  8+0  9 m 4 The k-supp ort b ounds of the other items are calculated as similar as ab o v e The coun t of the item 1 is 4 whic h implies that it ma y b e the subset of the large 3 or 4-itemsets W e should k eep item 1 in C 1  By similar argumen t items 2 4 and 5 will b e k ept in C 1  Item 3 will b e pruned b ecause none of the k supp ort b ounds for item 3 is less than or equal to the coun t of item 3 Therefore C 1 will b ecome ff 1 g  f 2 g  f 4 g  f 5 gg  By similar metho d all the candidate and large itemsets will b e generated as the follo wing P ass I I k=2 During the join step the algorithm will generate the follo wing p oten tially large itemsets f 1 2 g  f 1 4 g  f 1 5 g  f 2 4 g  f 2 5 g  f 4 5 g During the prune step the estimated upp er b ounds for the ab o v e corresp onding itemsets will b e 4,4,4,5,5 6 resp ectiv ely  All the supp ort b ounds are calculated as ab o v eP ass I After calculating all the supp ort b ounds all the itemsets in C 2 will remain as all of them ma ybe large in the coming passes During the c hec king step the up dated coun ts for the itemsets will b e 2,3,4,5,5,6  resp ectiv ely  F rom the supp ort b ounds calculated in the prune step f 1 2 g cannot con tribute to an y large itemsets in the curren t or future passes and it will b e pruned in this stage After calculating the w eigh ted supp ort in the remaining candidate set C 2  ff 1  4 g  f 1  5 g  f 2  4 g  f 2  5 g  f 4  5 gg w e found that the large itemset is f 4 5 g  and w e put f 4 5 g in to L 2  The itemsets in C 2 will b e used in next pass W e will use the ab o v e metho d in the remaining pass 3 Rule generation from the L  L  L k is exactly the same as the Apriori Gen 1  2 4 Mining Normalized W eigh ted asso ciation rules In this section w e fo cus on the mining of w eigh ted asso ciation rules for whic h the w eigh t of an itemset is normalized b y the size of the itemset W e can still apply the previous algorithm MINW AL\(O in Section 3 for this case simply  with a mo di\014cation of the de\014nitions of large itemsets and k supp ort b ound see b elo w Ho w ev er w e will design another new algorithm and w e shall presen t this called MINW AL\(W in Section 4.1 De\014niti on 7 The normalized w eigh ted supp ort of a rule X  Y is given by 1 k 0  X i j 2  X  Y  w j 1 A  S uppor t  X  Y  8 wher e k  size of the itemset  X  Y  De\014niti on 8 A k itemset X is c al le da small itemset if the normalize d weighte d supp ort of such an itemset is less than the minimum weighte d supp ort  w minsup  or  P i j 2  X  w j k  S uppor t  X   w minsup 9 Otherwise it is a large k itemset 


W e can de\014ne the k supp ort b ound for the normalized case De\014nitio n9 The minimum supp ort c ount for a lar ge k itemset which c ontains Y is c al le d the k supp ort b ound of the itemset Y with normalize d weight and it is given by B  Y k  030 k w minsup W  Y k  002 T 031 10 4.1 Another approac h for normalized w eigh ted case In the follo wing w e will design an algorithm whic h generate large itemsets in an iterativ e manner as in Apriori-Gen and also e\013ectiv ely pruning candidate itemsets in eac h iteration Although the closure propert y of the subset of a large itemset m ust b e large still do es not hold in the normalized w eigh ted case w e can 014nd the closure prop ert y in a di\013eren t manner De\014nitio n10 F or an itemset X  f x 1 001\001\001 x n g  let the smal lest weight of the items b e w i  A n itemset Y  X  Z  wher e the items in Z have weights al l less than w i isc al le da lo w-order sup erset of X  De\014nitio n11 A n itemset Y 032 X  such that e ach item in Y has a weight gr e ater than or e qual to e ach item in X 000 Y isc al le da high-order subset of X  Lemma 1 If an itemset Y is lar ge then any highor der subset of Y must also b e lar ge Pro of Let X b e a high-order subset of Y  The a v erage w eigh tof X is greater than or equal to the a v erage w eigh tof Y  The supp ort of X is also greater than or equal to that of Y  Hence the w eigh ted supp ort of X is greater than or equal to that of Y If Y is large then X will b e also large F or example if w eigh ts of items 1,2,4,5 are in ascending order and f 1 2 4 5 g is a large 4-itemset then itemsets f 5 g  f 4 5 g and f 2 4 5 g m ust also b e large Lemma 2 A lar ge  k 1  X must b e a lowor der sup erset of some lar ge k itemset Y  Pro of If X is large then from Lemma 1 an y highorder subset of X m ust also b e large Let x b e the item in X with the lo w est w eigh t Then Y  X 000 x is a high-order subset of X and it m ust b e large Hence X is a lo w-order sup erset of Y  With the ab o v e 014ndings w e can mo dify some steps in the previous algorithm The mo di\014ed algorithm is presen ted in the follo wing 4.2 Algorithm for Mining Normalized W eigh ted Asso ciation Rules The inputs and output for an algorithm for mining normalized w eigh ted asso ciation rules are the follo wing Inputs Same as Algorithm 1 Output In teresting normalized w eigh ted asso ciation rules Most of the notations are similar to that of Algorithm 1 except for the follo wing Notations C k set of candidate k itemsets w minsup normalized w eigh ted supp ort threshold Algorithm 2 MINW AL\(W 1 Main Algorithm  w minsup minconf  D w 2 size Searc h  D  3 L   4 for i=1;i 024 size;i 5 C i  L i    6 for eac h transaction do 7 SC C 1  Coun ting  D w 8 k=1 9 while  C k 6  036  10 k 11 C k  Join  L k 000 1  12 C k  Prune  C k  13  L k  Chec king  C k  D  14 L L S L k  15 Rules  SC L  16 ends The subroutines of MINW AL\(W are outlined as follo ws 1 Searc h  D  Same as in Algorithm 1 2 Coun ting  D w Same as in Algorithm 1 3 Join  L k 000 1  The subroutines Join Prune and Chec king generate L k and C k  The main job in the join step is to generate C k F rom Lemma 2 a candidate k itemset m ust b e a lo w-order sup erset of some large  k 000 1 In this step w e join the large itemsets in L k 000 1 with one of the items that ha v elo w er w eigh ts to form a lo w-order sup erset F or example if w 1 024 w 2 024 001\001\001 024 w 5 are the w eigh ts of the item f 1,2,3,4,5 g  and f 3 5 g is a large itemset found in pass I I the join step will construct f 1 3 5 g and f 2 3 5 g itemsets only  Those joined itemset will b e put in C k  f 3 4 5 g cannot b e a large itemset b ecause if it is then f 4 5 g should b e a large itemset 4 Prune  C k  During the prune step a candidate k itemset X will b e pruned if all the j supp ort 


b ounds of the X  j 024 k  are greater than the smallest kno wn supp ort coun t among the  k 000 1 subsets of X  whic h is an estimation and an upp er b ound of the supp ort coun t of the k itemset X  5 Chec king  C k  D  The c hec king pro cedure will b e done similar to Algorithm 1 except that the remaining candidate itemset will b e the set of large itemset L k  and the next pass will b e based on the L k to generate the candidate sets 6 Rules  SC L  Same as in Algorithm 1 The framew ork for our prop osed algorithm for mining w eigh ted asso ciation rules with a v eraging of w eigh ts is similar to Apriori Gen 1  and Algorithm 1 but with some ma jor di\013erences in the details Although the large k itemsets can b e generated from large  k 000 1 but it is not true that all the subsets of a large itemset should b e large The mo di\014cation is on the generation of candidate sets In the Apriori Gen pro cess 1  generating the candidate sets C k is based on the large itemsets L k 000 1  Here w e also generate C k from L k 000 1 Ho w ev er w ew ould not consider k itemsets where all  k 000 1 are in L k 000 1  since not all  k 000 1 of a large k itemset should b e large Instead w e shall consider the lo w-order sup ersets of the itemsets in L k 000 1 from Lemma 2 During the prune step w ec hec k the estimated w eigh ted supp ort of the itemsets The di\013erence b et w een this and the Apriori Gen is not to c hec k the subsets of the itemsets b eing large W e use the supp ort b ound v alues instead Example 4 Refer to T ables 1 and 2 w e will sho who w the large itemsets are generated from the transaction database Supp ose that the w minsup w eigh ted minim um supp ort is 0.45 Similar to Algorithm 1 w e 014nd the maxim um p ossible size of the large itemsets whic his 4 P ass I  k 1 where k is the size of the itemset The supp ort coun ts SC of the 1-itemset f 1,2,3,4,5 g will b e f 4,5,1,6,7 g F or these coun ts for example the w eigh ted supp ort of item f 1 g will b e 0  1 002 4 7  0.06 By the closure prop ert y  the high-order subset of a large k itemset should b e large Therefore items f 3 g  f 2 g and f 1 g will b e pruned Hence C 1  ff 4 g  f 5 gg  P ass I I k=2 During the join step the algorithm will generate the follo wing p oten tially large itemsets D Num b er of transactions T Av erage size of the transactions I Av e size of the max p oten tially large itemsets L Num b er of maximal p oten tially large itemsets N Num b er of items T able 3 P arameters of the syn thetic database f 5 4 g  f 5 3 g  f 5 2 g  f 5 1 g  f 4 3 g  f 4 2 g  f 4 1 g During the prune step the estimated w eigh ted supp ort of itemset f 5 4 g will b e 0  8+0  9 2 002 6 7  0.72 Others will b e calculated as the ab o v e During the c hec king step the up dated coun t for the itemset f 5 4 g will b e 6 F rom the calculation of the w eigh ted supp ort f 4 5 g is found to b e a large itemset and is put in L 2  W e will use the same metho d for the remaining passes un til no itemset found in candidate set 2 5 P erformance Study A p erformance study is carried out for the t w o algorithms MINW AL\(O and MINW AL\(W A series of exp erimen ts are done to sho w the p erformance of these algorithms on an UltraSparc 1 mac hine with 256MB of main memory  The algorithms are written in C and the timing is measured b y the cpu time calculated from the built-in timing functions of UNIX Shell In order to con trol di\013eren t parameters in the exp erimental setup w e use syn thetic databases and w eigh ts in the exp erimen t The metho d of generating suc h synthetic data will b e explained in Section 5.1 In the follo wing exp erimen ts w e use di\013eren t transaction databases with the same set of item w eigh ts The reason for using the same item w eigh ts is to k eep this factor constan t in order to compare the e\016ciency of the algorithms   0 5 10 15 20 25 30 35 0 0.2 0.4 0.6 0.8 1 frequency weights frequency Figure 1 Distribution of W eigh ts 5.1 Generation of Syn thetic Data and W eigh ts During the exp erimen t w e will use syn thetic data to ev aluate the p erformance The data generation procedure is similar to 


Before the generation of the database the parameters D L  N  T and I are set to b e 100K 2000 1000 5 and 2 resp ectiv ely  with 2.4MB of database In the generation of the w eigh ts w e assume that the n um b er of the lo ww eigh t pro ducts will b e m uc h more than the high-w eigh t pro ducts W e generate the w eigh ts according to an exp onen tial distribution and the result is sho wn in Figure 1 5.2 P erformance Ev aluation W e study the e\013ect of di\013eren tv alues of w minsup  n um b er of transactions and items etc on the processing time for algorithms MINW AL\(O and MINW AL\(W W e use the hash-tree data structures 1  n the follo wing exp erimen ts 5.2.1 P erformance Ev aluation of the t w o algorithms The exp erimen t will b e done on the t w o algorithms MINW AL\(O and MINW AL\(W under t w o conditions the unnormalized case as describ e in Section 2 and the normalized case as discussed in Section 4 Since MINW AL\(W cannot b e applied to the unnormalized case w e need to consider only three cases MINW AL\(O for cases with and without normalization and MINW AL\(W for the normalized case The test will b e based on the syn thetic databases W e use 5 v alues of thresholds  w minsup  for eac h test W e use f 0.0016 0.0017 0.0018 0.0019 0.0020 g in the mining of unnormalized w eigh ted asso ciation rules while w minsup of f 0.0006 0.0007 0.0008 0.0009 0.0010 g are used in the mining for the normalized w eigh ted case These threshold v alues are c hosen to generate a reasonable n um b er of large itemsets and rules F or simplicit yw e use the v alues of f 1,2,3,4,5 g to represen ts these sets of threshold v alues in most of the 014gures Figure 2 sho ws the decreasing trend of the execution time when the supp ort threshold increases As the threshold increases the candidate itemsets decreases The execution time w ould decrease for the smaller resulting hash-tree b ecause of the decreasing searc hing time In Figure 3 it is noted that the time needed for eac h pass for the MINW AL\(O is m uc h less than the MINW AL\(W esp ecially from pass 2 to pass 4 F urthermore when comparing the t w o algorithms MINW AL\(W and MINW AL\(O in the normalized w eigh ted case it is noticed that the execution time of MINW AL\(O is m uc h less than the algorithm MINW AL\(W esp ecially for the cases with smaller thresh 500 1000 1500 2000 2500 3000 1 1.5 2 2.5 3 3.5 4 4.5 5 Time \(in seconds Threshold \(wminsup MINWAL\(O 500 1000 1500 2000 2500 3000 1 1.5 2 2.5 3 3.5 4 4.5 5 Time \(in seconds Threshold \(wminsup MINWAL\(O  MINWAL\(W  500 1000 1500 2000 2500 3000 1 1.5 2 2.5 3 3.5 4 4.5 5 Time \(in seconds Threshold \(wminsup MINWAL\(O  MINWAL\(W   MINWAL\(O  Figure 2 Ov erall execution time 100 200 300 400 500 600 700 800 900 1000 1 2 3 4 5 6 7 Time \(sec Pass Number MINWAL\(O 100 200 300 400 500 600 700 800 900 1000 1 2 3 4 5 6 7 Time \(sec Pass Number MINWAL\(O  MINWAL\(W  100 200 300 400 500 600 700 800 900 1000 1 2 3 4 5 6 7 Time \(sec Pass Number MINWAL\(O  MINWAL\(W   MINWAL\(O  Figure 3 Execution time for eac h pass 1 2 3 4 5 6 7 8 1 1.5 2 2.5 3 3.5 4 4.5 5 Pass Threshold \(wminsup MINWAL\(O 1 2 3 4 5 6 7 8 1 1.5 2 2.5 3 3.5 4 4.5 5 Pass Threshold \(wminsup MINWAL\(O  MINWAL\(W  1 2 3 4 5 6 7 8 1 1.5 2 2.5 3 3.5 4 4.5 5 Pass Threshold \(wminsup MINWAL\(O  MINWAL\(W   MINWAL\(O  Figure 4 P asses needed 


  0 1 2 3 4 5 6 1 2 3 4 5 6 7 Number of Candidate Itemsets \(in log Pass Number MINWAL\(O   0 1 2 3 4 5 6 1 2 3 4 5 6 7 Number of Candidate Itemsets \(in log Pass Number MINWAL\(O  MINWAL\(W    0 1 2 3 4 5 6 1 2 3 4 5 6 7 Number of Candidate Itemsets \(in log Pass Number MINWAL\(O  MINWAL\(W   MINWAL\(O  Figure 5 Size of the candidate itemsets olds It w as b ecause the n um b er of the candidate itemsets needed to b e c hec k ed in MINW AL\(W is m uc h more than in MINW AL\(O Figure 4 sho ws the n um b er of passes needed in mining of the asso ciation rules F rom the graph the n um b er of passes needed for mining the unnormalized w eigh ted case is more than the normalized case The n um b er of candidate itemsets mined is sho wn in Figure 5  where w e set w minsup 0  006 for the normalized w eigh ted case and w minw up 0  0016 for the unnormalized case The n um b er of candidate itemsets generated b y MINW AL\(W is greater than that of MINW AL\(O for the normalized w eigh ted case Based on the all the ab o v e 014gures w e can see that MINW AL\(O is in general more e\016cien t in the mining of normalized w eigh ted asso ciation rules This is b ecause that the time needed to c hec k the candidate itemsets is m uc h less than MINW AL\(W 5.2.2 Scale-Up Exp erimen t In the follo wing w e examine ho w the p erformance v aries with the n um b er of items and transactions All other things b eing equal With w minsup 0  002 for MINW AL\(O for the unnormalized case and w minsup 0  001 for other cases the execution time increases with the n um ber of items since the more the items the larger the hash tree As the hash tree gro ws directly with the items the execution time in the hash tree searc hing w ould b e increased whic hissho wn in Figure 6 In Figure 7 w e are in terested in the relation b et w een the n um b er of transactions and the execution time Keeping other things equal 1000 items and w eigh ts w e generate di\013eren t transaction database 5 5.5 6 6.5 7 7.5 8 300 400 500 600 700 800 900 1000 Time \(In Ln Sec Number of Items MINWAL\(O 5 5.5 6 6.5 7 7.5 8 300 400 500 600 700 800 900 1000 Time \(In Ln Sec Number of Items MINWAL\(O  MINWAL\(W  5 5.5 6 6.5 7 7.5 8 300 400 500 600 700 800 900 1000 Time \(In Ln Sec Number of Items MINWAL\(O  MINWAL\(W   MINWAL\(O  Figure 6 Num b er of items scale-up 5 5.5 6 6.5 7 7.5 30 40 50 60 70 80 90 100 Time \(In Ln Sec Number of Transaction \(K MINWAL\(O 5 5.5 6 6.5 7 7.5 30 40 50 60 70 80 90 100 Time \(In Ln Sec Number of Transaction \(K MINWAL\(O  MINWAL\(W  5 5.5 6 6.5 7 7.5 30 40 50 60 70 80 90 100 Time \(In Ln Sec Number of Transaction \(K MINWAL\(O  MINWAL\(W   MINWAL\(O  Figure 7 Num b er of transactions scale-up 100 150 200 250 300 350 400 1 1.5 2 2.5 3 3.5 4 Time \(sec Pass Number MINWAL\(W 100 150 200 250 300 350 400 1 1.5 2 2.5 3 3.5 4 Time \(sec Pass Number MINWAL\(W  MINWAL\(W  100 150 200 250 300 350 400 1 1.5 2 2.5 3 3.5 4 Time \(sec Pass Number MINWAL\(W  MINWAL\(W   MINWAL\(O  100 150 200 250 300 350 400 1 1.5 2 2.5 3 3.5 4 Time \(sec Pass Number MINWAL\(W  MINWAL\(W   MINWAL\(O   MINWAL\(O  Figure 8 Sp ecial case with 0/1 w eigh ts for normalized case 


with the same parameters but di\013eren t in size ranging from 25K to 100K The v alues of w minsup are set as the ab o v e scale-up exp erimen t In this 014gure the time is giv en in ln  sec  F rom the 014gure the execution time increases with the n um b er of transactions linearly with ln scale implying that the complexit yof the algorithms is exp onen tial in the n um b er of transactions 1  5.2.3 Exp erimen t for sp ecial case In this section w e are in terested in the p erformance in the sp ecial case whic h is the item w eigh ts equal to 0 or 1 only  In this case w e mak e the 014rst 900 w eigh ts b e 0 and the remaining w eigh ts b e 1 Other things including database and threshold equal as ab o v e section W e carried out the exp erimen t for the normalized w eigh ted case to compare the t w o algorithms There are t w o ma jor 014ndings 1 The p erformance of the sp ecial case is m uc h b etter than the general case where item w eigh ts follo w a distribution b et w een 0 and 1 2 Con trary to the previous cases MINW AL\(W p erforms b etter than MINW AL\(O F rom Figure 8 w e notice that the time needed in MINW AL\(W is m uc h less than the MINW AL\(O for all the thresholds This is b ecause in the joining step the n um b er of starting seed candidate itemsets in C 1 to generate itemsets in C 2  is less than MINW AL\(O case In this situation the 0/1 w eigh ts giv e the adv an tage to MINW AL\(W During the 014rst step the algorithm MINW AL\(W will easily prune all the small itemsets with 0 w eigh ts while MINW AL\(O will k eep those small itemsets with 0 w eigh ts As the starting seed is smaller in size MINW AL\(W w ould p erform w ell in this case 6 Conclusion W eha v e prop osed to study a new problem of mining w eigh ted asso ciation rule This is a generalization of the asso ciation rule mining problem In this generalization the items are assigned w eigh ts to re\015ect their imp ortance to the user The main di\013erence b et w een mining w eigh ted asso ciation rules and the mining un w eigh ted asso ciation rules is the do wn w ard closure prop ert y  W e prop osed t w o di\013eren t de\014nition of w eigh ted supp ort without normalization and with normalization W e prop osed new algorithms based on the supp ort b ounds  the algorithms MINW AL\(O and MINW AL\(W MINW AL\(O is applicable to b oth normalized and unnormalized cases and MINW AL\(W is applicable to the normalized case only  The p erformance ev aluation has b een done on these t w o algorithms W e found that MINW AL\(O outp erforms MINW AL\(W in most cases but MINW AL\(W p erforms b etter for the sp ecial case with only 0/1 item w eigh ts So far w eha v e only considered the mining of binary w eigh ted asso ciation rules Some of the researc hers did the researc h for the problem of the quan titativ e assoication rules suc has[4  3 W ema yin v estigate the problem of quan titativ e asso ciation rules with w eigh ted items whic his anin teresting topic in the future References  R Agra w al and R Srik an t F ast algorithms for mining asso ciation rules In Pr o c e e dings of the 20th VLDB Confer enc e  pages 487{499 1994  D Cheung V.T Ng A F u and Y F u Ef\014cien t mining of asso ciation rules in distributed databases In IEEE T r ansactions on Know le dge and Data Engine ering  pages 1{23 1996  T ak eshi F ukuda Y asuhik o Morimoto Shinic hi Morishita and T ak eshi T okuy ama Data mining using t w o-dimensional optimized asso ciation rules Sc heme algorithms an visualization In Pr o c e e dings of A CM SIGMOD  pages 13{23 1996  T ak eshi F ukuda Y asuhik o Morimoto Shinic hi Morishita and T ak eshi T okuy ama Mining optimized asso ciation rules for n umeric attributes T ec hnical Rep ort 1623-14 IBM T oky o Researc h Lab oratory  1996  J Han M Kam b er and J Chiang Mining m ultidimensional asso ciation rules using data cub es T ec hnical rep ort Database Systems Researc h Laboratory Sc ho ol of Science Simon F raser Univ ersit y  1997  J.S P ark M-S Chen and P S Y u An e\013ectiv e hash-based algorithm for mining asso ciation rules In Pr o c e e dings of A CM SIGMOD  pages 175{186 1995  A Sa v asere E Omiecinski and S Na v athe An e\016cien t algorithm for mining asso ciation rules in large databases In Pr o c e e dings of the 21th International Confer enc eon V ery L ar ge Data Bases  pages 432{444 1995 


expect this optimization to be of greatest bene\336t when the transaction sizes are large r example if our transaction is T 000 f A\000 B 000 C\000 D\000 E g  k 000 3 fan-out 000 2 then all the 3-subsets of T are f ABC,ABD,ABE,ACD,ACE,ADE,BCD,BCE,BDE,CDE g  Figure 2 shows the candidate hash tree C 3  We ave to increment the support of every subset of T contained in C 3  We egin with the subset AB C  and hash to node 11 and process all the itemsets In this downward path from the root we mark nodes 1 4 and 11 as visited We then process subset AD B  and mark node 10 Now consider the subset CDE  We see in this case that node 1 has already been marked and we can preempt the processing at this very stage This approach can r consume a lot of memory r a n fan-out F  for iteration k  e need additional memory of size F k to store the 337ags In the parallel implementation we have to keep a VISITED 336eld for each processor bringing the memory requirement to P\000F k  This can still get very large especially with increasing number of processors In we sho w a mechanism by which further reduces the memory requirement to only k 000F  The approach in the parallel setting yields a total requirement of k 000F 000P  5 Experimental Evaluation Database T I D Total Size T5.I2.D100K 5 2 100,000 2.6MB T10.I4.D100K 10 4 100,000 4.3MB T15.I4.D100K 15 4 100,000 6.2MB T20.I6.D100K 20 6 100,000 7.9MB T10.I6.D400K 10 6 400,000 17.1MB T10.I6.D800K 10 6 800,000 34.6MB T10.I6.D1600K 10 6 1,600,000 69.8MB Table 2 Database properties 5.1 Experimental Setup All the experiments were performed on a 12-node SGI Power Challenge shared-memory multiprocessor Each node is a MIPS processor running at 100MHz There\325s a total of 256MB of main memory The primary cache size is 16 KB 64 bytes cache line size with different instruction and data caches while the secondary cache is 1 B 128 bytes cache line size The databases are stored on an attached 2GB disk All processors run IRIX 5.3 and data is obtained from the disk via an NFS 336le server We used different synthetic databases with size ranging form 3MB to 70MB 2  and are generated using the procedure described in These databases mimic the transactions in a retailing en vironment Each transaction has a unique ID followed by a list of items bought in that transaction The 2 While results in this section are only shown for memory resident databases the concepts and optimization are equally applicable for non memory resident databases In non memory resident programs I/O becomes an important problem Solutions to the I/O problem can be applied in combination with the schemes presented in this paper These solutions are part of future research 11 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


  0 500 1000 1500 2000 2500 0 2 4 6 8 10 12 Number of Large Itemsets Iterations Large Itemset at Support = 0.5 222T5.I2.D100K\222  222T10.I4.D100K\222   222T15.I4.D100K\222   222T20.I6.D100K\222   222T10.I6.D400K\222   222T10.I6.D800K\222   222T10.I6.D1600K\222  Figure 3 Large Itemsets per Iteration data-mining provides information about the set of items generally bought together Table 2 shows the databases used and their properties The number of transactions is denoted as jD j  average transaction size as j T j  and the average maximal potentially large itemset size as j I j  The number of maximal potentially large itemsets j L j 000 2000 and the number of items N 000 1000 We refer the reader to for more detail on the database generation All the e xperiments were performed with a minimum support value of 0.5 and a leaf threshold of 2 i.e max of 2 itemsets per leaf We note that the  improvements shown in all the experiments except where indicated do not take into account initial database reading time since we speci\336cally wanted to measure the effects of the optimizations on the computation Figure 3 shows the number of iterations and the number of large itemsets found for different databases In the following sections all the results are reported for the CCPD parallelization We do not present any results for the PCCD approach since it performs very poorly and results in a speed-down on more than one processor 3  5.2 Aggregate Parallel Performance Table 3 s actual running times for the unoptimized sequential and a naive parallelization of the base algorithm Apriori for 2,4 and 8 processors without any f the techniques descibed in sections 3 and 4 In this section all the graphs showing  improvements are with respect to the data for one processor in table 3 Figure 4 presents the speedups obtained on different databases and different processors for the CCPD parallelization The results presented on CCPD use all the optimization discussed 3 Recall that in the PCCD approach every processor has to read the entire database during each iteration The resulting I/O costs on our system were too prohibitive for this method to be  12 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


