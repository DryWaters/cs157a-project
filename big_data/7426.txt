2009 Urban Remote Sensing Joint Event  978-1-4244-3461-9/09/$25.00 ©2009 IEEE  PSInSAR Analysis over the Three Gorges Dam and urban areas in China Daniele Perissin, Claudio Prati, Fabio Rocca Politecnico di Milano, POLIMI, Milan, Italy daniele.perissin@polimi.it   Teng Wang Politecnico di Milano, POLIMI, Milan, Italy LIESMARS, Wuhan university, Wuhan, China Abstract In this work we present the results achieved within the Dragon project, cooperation program between the European Space Agency \(ESA\ and the National Remote Sensing Center of 
China \(NRSCC\, about monitoring the terrain motion in urban areas, measuring the city growth rate and analyzing the stability of big manmade structures. Among the processed areas, we report here the main results we obtained in the test sites of Shanghai, Tianjin and Three Gorges. The techniques that have been used to process the data are classical SAR interferometry InSAR\, Permanent Scatterers \(PSInSAR\ and a combination of coherent-uncoherent analysis. Particular attention is worth to be paid to the analysis of the Three Gorges Dam, biggest hydroelectric plant in the world, in which stability and characteristics of its scattering structures have been studied 
I   I NTRODUCTION Aiming at solving the classical restrictions of SAR interferometry \(InSAR i  e  de c o rre l a t i on a nd a t m o s phe r i c  artifacts, the Permanent Scatterers technique \(PSInSAR\ [2  was invented and developed in Politecnico di Milano POLIMI\ in the late nineties. Instead of extracting information from the whole SAR image, PSInSAR exploits long temporal series of acquisitions to identify point-like stable reflectors PSs\. The electromagnetic stability of PSs allows obtaining around 1meter accuracy DEMs [3 an d m illim e t r i c es tim ates  o f  terrain motion [4  U s u a lly P S s co r r e s p on d to m a n m a d e  
targets, making the application of PSInSAR technology particularly appropriate in urban areas T h e  wo r k  h e r e  presented has been carried out within the Dragon I and II cooperation projects between the European space agency and the National Remote Sensing Center of China \(ESA and NRSCC respectively\. In this framework, the ‘topographic measurement’ group has been working on PS analysis in several urban test sites in China, getting ground deformation maps of wide areas, till differential movements over single constructions II  S UBSIDENCE MONITORING IN S HANGHAI  A 
 ERS processing At the beginning of the Dragon Project in 2004, Shanghai was selected as the first PS analysis test site in China for studying the subsidence caused by under-ground water pumping and by the rapid city development in the 1990s Thanks to the archived ERS data of ESA, 40 images spanning the interval 1993-2000 were processed. The results of the PS analysis allow detecting areas around the urban center of Shanghai subsiding with a linear trend of even more than 40mm per year. The PS measurements were then compared with the deformation map retrieved by means of leveling data in Shanghai, revealing quite good agreement Figure 1 shows the most significant outcomes of the 
analysis. On the left of Figure 1 the geocoded linear deformation trend in Shanghai is reported. Each point represents a permanent scatterer and  the color scale indicates the average linear motion, spanning between -40 and 40 mm/year. On the right of Figure 1 the deformation rate as estimated with optical leveling techniques is shown. The two maps are pretty in accordance, highlighting the str ongest motions nearby a bight of a branch of the mouth of the Yangtze River, with a maximum subsidence rate of -40mm/year B  Envisat parallel tracks processing The positive results obtained in Sha nghai drew the interest to monitoring its urban ground stability also after year 
2000 Considering the loss of gyroscopes of ERS-2 and the difficulty in connecting ERS and Envisat data together \(not only for the different carrier frequencies, but mostly because of the city development\, the problem of carrying out a PS analysis with few Envisat images per track was tackled. The studies on the physical nature of PSs in urban sites suggested then as solution the exploitation of multi-angle targets as dihedrals to combine data acquired from parallel tracks [6  I n th is w a y it w a s  possible to double the number of data samples to estimate height and deformation trends of dihedrals. The developed method allowed updating the subsidence monitoring in Shanghai. The results, that identify the same sinking areas of 
the ERS analysis, reveal a general decrease of deformation rate Figure 2 brings the main achievements of the study over a limited processed area. On the left of Figure 2 the deformation map estimated with ERS data along a descending track \(n. 3\ is shown in geographical coordinates. On the right the same area has been processed using Envisat data taken from two ascending parallel tracks \(n. 268 and 497\ with 14 and 12 available images respectively. As visible from Figure 2, the number of detected targets is strongly reduced than in a classical analysis, since only dihedral-like scatterers are 


2009 Urban Remote Sensing Joint Event  978-1-4244-3461-9/09/$25.00 ©2009 IEEE   Figure 1  Comparison between deformation rate measured by the PS technique \(a\ and by optical leveling \(b\ in the urban area of Shanghai    Figure 3  About 10,000 PSs detected in the Tianjin test-site. Color scale: average deformation trend \(-30  30 mm/year  Figure 2 Comparison between PS velocity field estimated with ERS data \(left\nd Envisat data acquired from the parallel track s 268 and 497 \(right  Black circles identify the two areas with the highest subsidence rate coherently imaged. Nonetheless, a reasonable agreement is found between the subsiding areas in the two datasets \(the two reported circles in Figure 2\. Moreover, in the right circle it can be noted that the average rate is lower in the map retrieved with parallel tracks than in the other one III  T IANJIN CITY GROWTH STUDY  Along with the su ccessful application of PSInSAR technique in Shanghai, Tianjin was selected as other test-bed Third direct-controlled municipality in north China, Tianjin developed rapidly in 1990s and also suffered subsidence problem due to under-ground water over-extraction and coal mining. In this test site, apart from the classical estimate of PS height and deformation trend, an investigation on the urban targets physical nature has been carried out by exploiting 23 ERS images. By analyzing the amplitude history of SAR images, birth and death days of PSs were also extracted to recognize the growth pattern of the city. The subsiding area was detected being confined to the surroundings of the city cente  From Figure 3 it is possible to notice the still rural character of the suburban area of Tianjin at the time of the acquis itions The center of the urban area appears mostly stable, while along the river and in particular in the lower right corner of the image the highest concentration of sinking targets lies. The analysis in Tianjin was performed with a very low number of images and even more with a sparse temporal sampling. Notwithstanding the outcome of the work is particularly meaningful and allows highlighting the motions affecting the imaged terrain IV  T HREE G ORGES DAM ANALYSIS  Besides Shanghai and Tianjin, the ongoing work in the 


2009 Urban Remote Sensing Joint Event  978-1-4244-3461-9/09/$25.00 ©2009 IEEE    Figure 4 History of the different phases of the Three Gorges Dam project. Blu line: water level of the Yangtze river. Red \(blu e\ stems: ER S Envisat\ data acquired over the dam frame of the second edition of the Dragon project is to monitor the stability of the Three Gorges Dam with the PS technique The Three Gorges dam is the largest hydroelectric river dam in the world. The dam wall is about 2,335 meters long and 185 meters high. The body of the dam is 115 meters wide on the bottom and 40 meters wide on top. The project used 28,000,000 m of concrete, 463,000 tons of steel, and moved about 134,000,000 m of earth. After the dam began to work in 2003, the water level of the Yangtze River in Three Gorges area raised more than 100 meters. The shape of the dam is such as to show a lot of structural details that reflect the electromagnetic signal. This allows monitoring it from the satellite. The high number of targets detected makes it possible to analyze the movements of the structure with respect to the surrounding land but also the internal stability of the dam Moreover, an accurate analysis of the signal scattered by the targets reveals if the dam is releasing water during the satellite pass or not The Three Gorges Project is still ongoing; when completed at the end of 2009 it will be the world's largest water conservation facility. The construction plan consists of three stages in 17 years 1  The preparatory and first-phase projects spanned six years from 1992-1997, its completion was marked by the damming of the Yangtze River on November 8 1997 2  The second phase ran from 1998 to 2002. It was completed when the first electrical unit in the north-bank hydropower station went on line and the permanent ship lock began operative 3  The third phase was planned for 2002-2009. It includes the completion of all 26 electrical turbo generators In Figure 4 the history of the phases of the Three Gorges project is sketched together with the height level of the Yangtze river. Red and blue stems are plotted in correspondence of the acquisition dates of ERS and Envisat images respectively. The 41 Envisa t images used in this work fall in the third phase of the project. It is worth to notice the water level increment of 20m in the middle of 2006 


2009 Urban Remote Sensing Joint Event  978-1-4244-3461-9/09/$25.00 ©2009 IEEE   Figure 5 Reflectivity map of the Three Gorges dam and different scatterer’s signatures derived from the amplitude of SAR imag es. In each matrix a pixel identifies a couple of images. Blue color means no change of amp litude among the couple, yellow and red progressively increasing changes   Figure 7. Left: time series examples of two Permanent Scatterers detected over the dam. It is possible to notice a slight seaso nal trend. Right  particular of the spill way of the dam   Figure 6 PSs detected over the dam. Color scale: estimated height of PSs. Below the dam: upriver side. Bright tagets on the le ft with no PSs: ship lift. Bright scatterers in the middle of the dam without PSs: spill way. On its right, south power plant, last par t of the dam to  be built By looking at the reflectivity map of the analyzed area in Figure 5, the structure of the dam crossing the river can be easily recognized. On the left of the dam a water channel is visible, interrupted by some very strong scatterers, that are in correspondence of the ship lift. In Figure 5 also 4 matrices are visible, reporting the amplitude behavior of 4 scatterers in the whole SAR dataset. Each pixel of a matrix identifies a pair of images. Blue indicates that the amplitude of the scatterer is similar in the two images, yellow and red mean that a change happened. Thus, from the amplitude matrix of the ship lift it is possible to recognize the two states of the lift: up and down that change the backscattered signal The structure of the Dam can be divided in three main parts. The central one, where scatterers are brighter, is the spill way, used for releasing or storing the water in the basin. A picture of the spill way can be seen in Figure 7 on the right The vertical elements on its front create strong multi-bounce scatterers when the gates are closed. But when the water flows out the scatterers become very weak. In this way, by looking at the backscattered radar signal it’s possible to know the status of the gates The other two parts of the dam aside the spill way are the 


2009 Urban Remote Sensing Joint Event  978-1-4244-3461-9/09/$25.00 ©2009 IEEE  two power plants. The left one in the reflectivity map is the north plant and was built before 2002. The amplitude matrix in Figure 5 of a scatterer in the North side shows a constant pattern in the processed time span. The right part of the dam has been under construction till year 2008. From Figure 5 also the cofferdam can be seen on the right of the image, the amplitude matrix showing its dismantling in 2006 Figure 6 shows then the permanent scatterers detected on the dam and its surroundings. No coherent scatterer is present at the moment on the ship lift and on the spill way, since their behavior in time is complex. But coherent targets are found on other temporary structure as the south plant. The color scale of Figure 6 shows the height of PS with respect to a reference point in the image. The height of the targets detected on the dam span more than 100m Figure 7 on the left shows two time series of two scatterers on the dam. The displacement measured by the radar shows an overall stability of the structure. In the first 3 years a slight seasonal trend can be noticed, likely linked to the pressure of the water, changing its level seasonally. The dispersion of the measure increases then in 2007, when the level of the basin reached its maximum height of 156m V  CONCLUSIONS  In the present work we have shown some results obtained within the cooperation program between the European Space Agency and the National Remote Sensing Center of China for what concerns urban terrain motion monitoring. PSInSAR techniques allow measuring the average deformation trend of manmade structures, revealing subsidences or the stability of single buildings. Particularly interesting results have been obtained over the Three Gorges Dam, where a slight seasonal motion has been observed and the analysis of the amplitude shows the status of different structural details \(as spill gates and ship lift\. The quality and quantity of information that can be extracted with such techniques from C-band data arouses great expectation towards the future availability of time series of Xband images as taken by TerraSAR-X or Cosmo SkyMed A CKNOWLEDGMENT The authors are very thankful to ESA for the ENVISAT and ERS data provided under the Dragon project and to T.R.E TeleRilevamento Europa for focusing and registering the SAR data R EFERENCES  1  R. Hanssen, "Radar Interferometry", Norwell, MA: Kluwer, 2001 2  A. Ferretti, C. Prati, and F. Rocca, “Permanent scatterers in SAR interferometry”, IEEE Trans. Geosci. Remote Sens., vol. 39, no. 1, pp 8–20, Jan. 2001 3  D. Perissin, "Validation of the sub-metric accuracy of vertical positioning of PS's in C band", IEEE Geoscience and Remote Sensing letters, Vol. 5, No. 3, July 2008, Pages: 502 - 506 4  A. Ferretti, G. Savio, R. Barzaghi, A. Borghi, S. Musazzi, F. Novali, C Prati, F. Rocca, "Submillimeter Accuracy of InSAR Time Series Experimental Validation", IEEE Trans. Geosci. Remote Sens., Volume 45, Issue 5, May 2007 Page\(s\:1142 - 1153 5  D. Perissin, A. Ferretti, "Urban target recognition by means of repeated spaceborne SAR images", I EEE Transactions on Geoscience and Remote Sensing, Volume 45, Issue 12, December 2007, Pages: 4043 4058 6  D. Perissin, C. Prati, F. Rocca, D. Li, M. Liao, "Multi-track PS analysis in Shanghai", Proceedings of ENVISAT 2007, Montreux \(Switzerland 23-27 April 2007 7  D. Perissin, A. Parizzi, C. Prati, F. Rocca "Monitoring Tianjin subsidence with the Permanent Scatterers technique", Proceedings of Dragon Symposium 2005, Santorini \(Greece\, 27 June - 1 July 2005 


access to the medium to transmit the packet at the head of the single send queue In 802.1 le virtual collisions can occur when each TC queue must contend for access within the station's queue manager as well as real collisions when the winning queue is allowed to transmit The results in 10 show that throughput in 802.1 le is decreased and latency is increased when compared to 802.1 la networks due to this extra contention being introduced Therefore dynamic and aggressive tuning of the network parameters is required to achieve benefit from prioritization In 6 7 the authors evaluate a mechanism which allows dynamic tuning of the timing used in the back-off algorithm in 802.11 They showed that a dynamic algorithm based on the number of currently active stations which manipulates the minimum back-off time can allow a wireless network to perform closer to the theoretical capacity of the medium The algorithm estimates the current performance and adjusts its back-off accordingly Their findings show that static network parameters lead to under-utilization of the medium and show the importance of a dynamic algorithm However the network parameters are adjusted with global knowledge of the number of nodes in the network and collision rates In 9 the authors investigate two methods of choosing CWmin in 802.11 networks based on proportional fairness and time-based fairness They conclude that proportional fairness in a network based on weights provides higher throughput than time based fairness Their work shows that CWmin can be tailored to a network to provide all nodes with fair access to the medium if priority mechanisms are used and therefore it follows that the same principles can be used to provide nodes with unfair access or differentiation using contention window parameters In 4 the authors evaluate how a network with 802.1 lb and 802.1 le nodes performs with different EDCA contention parameters and how the delay and throughput are affected by these parameters The 802.1 lb nodes model background traffic while the 802.1 le nodes model high priority traffic Four different contention parameters are tested the initial size of the contention window CWmin the maximum size of the contention window CWmax the Arbitration InterFrame Spacing interval AIFS Persistence Factor PF These parameters can be adjusted to differentiate 802.1 le traffic from 802.1 lb traffic present on the same network In 4 AIFS was shown to be the most effective contention parameter for protecting high priority traffic from background traffic However the authors show that using PF and CWmin for differentiation may have the advantage of allowing for better performance of the low priority traffic The CWmin parameter can be characterized as a compromise between AIFS which is the most effective for high priority nodes and no differentiation at all which is the least adverse towards low priority nodes The work presented in 11 is similar to our research The authors use a two-level approach to fair yet prioritized service The first level of protection for high priority services guarantees that changing network conditions do not affect data streams such as VoIP and video that have constant QoS requirements By using budgeted TxOp values for each queue new flows are not allowed to have immediate access to their share of bandwidth regardless of their TC or parameter settings This ensures that established flows are not disrupted by new flows The second level of protection called Fast-Backoff with Dynamic Adjustment when Fail or Successful is the most similar to our work due to its distributed nature Under this scheme when a station experiences a transmission failure its CW is increased by a factor greater than 2 which results in a faster than exponential backoff In addition to the CW increase the station's CWmin is increased by a factor When the station experiences a transmission success CWmin is decreased by a factor and the CW is reset to CWmin This dynamic adjustment results in a dramatic decrease in the number of collisions as well as more reliable service for the voice and video data This method adjusts the EDCA parameters on all successes and all failures with no regard for direct network performance measurements The approach in 11 differs from the approach presented here in that the Adaptive Algorithm uses direct network performance measurements as opposed to using transmission successes and failures Also 11 uses the default 802.1 1e TCs under normal circumstances and does not allow fine grained performance control Finally as shown in 9 802.1 le parameters can be tuned based on network conditions to allow better performance than a single setting Although these settings are not changed dynamically in this study they do show that changing network conditions require changing parameters to use the channel efficiently In works such as 13 the HCCA is shown to be better at efficiently using the medium than pure EDCA Contention in HCCA networks is reduced since the central coordinator controls access in an organized manner Although the stations still use EDCA at the station level queues and during CPs the central coordinator controls most of the transmission opportunities Despite this result EDCA can still be considered to be an important research topic from a reliability perspective In the HCCA scheme the AP is a single point of failure If the AP fails the network will fall back on Ad-hoc mode with EDCA controlling access 5 PROBLEM STATEMENT The first set of preliminary experiments in 12 was designed to replicate the data in 4 which showed the effect each 802.11 e contention parameter has on performance when adjusted individually As found in 4 the results 6 


suggest that prioritization based on AIFS provides the best level of service for high priority traffic since in all cases it provides the lowest access times Prioritization based on CWmax provides the next best service and prioritization based on CWmin provides the least aggressive service These finding serve as the basis for parameter adjustments that are discussed in the following section Also presented in 12 is a second set of baseline experiments that were performed to further motivate the need for tighter QoS controls in 802.1 le The primary contribution of these experiments was to show that an adaptive algorithm can provide a continuum of performance levels between the static categories of TCO and TC3 and that conversely the static categories do not allow arbitrary choices of network performance It was shown that an appropriate choice of parameters could replicate the performance of the static categories as well as the performance levels in between The experiments that were shown used a simplistic and static network model Problem Statement In prior work we have shown that an earth-bound network through a high speed Ka Band isochronous connection For these simulations the speed latency and response characteristics of the connection between the LCM and the Earth is not considered in order to focus on the local communications among the 802.1lg nodes The LCM is assumed to perform an appropriate amount of buffering and error control for this type of connection Therefore this portion of the network is only shown to demonstrate the scenario context Furthermore end-to-end connections are made only between wireless stations on the lunar surface and the LCM with the LCM acting as a data warehouse and relay station All performance metrics are assumed to be between wireless stations and the LCM 54FS n s Earh K Band LCM S 4 M9bs H 54 Mb Figure 4 Simulation network structure o It is difficult to know the optimal value for the 802.1 le contention parameters o The effective performance relative to different traffic classes depends on network dynamics o Adjusting priority dynamically is possible for simple networks however more complicated networks may present greater difficulty The work presented here furthers the work presented in 12 by using a more realistic network composed of combinations of traffic and transport types intended for actual deployment in a lunar communications system The algorithm presented in this paper attempts to solve these problems by adjusting the EDCA parameters independently at each station based on a single value assigned to each data stream This single value determines the threshold for when a stream begins to adjust its parameters or in other words how aggressive the stream is in claiming access to the wireless channel The following sections explain the methods used to accomplish this goal 6 METHODOLOGY Figure 4 shows the simulated network model used in the research reported in this paper The ns2 simulator version 2.28 was used with an EDCA add-on 5 to simulate an 802.1 le network The model represents a possible scenario for lunar communications The Lunar Communications Module LCM is an access point for the 802.1 Ig nodes to The simulated lunar surface network is composed of unmanned science stations and manned vehicles The unmanned science stations transmit telemetry and standard definition SD video represented by TS in the figure The stations might also occasionally transmit high definition video represented by H in the figure Examples of manned vehicles are humans in Extra Vehicular Activity EVA suits and humans in powered vehicles Both of these human components transmit voice command and telemetry data represented by VCT in the figure Table 1 shows the Traffic Class values used in the simulations TCO represents the highest priority while TC3 represents the lowest priority TC2 is considered the default values for traffic that is unclassified Table 2 shows the traffic characteristics of the data flows used in the simulations The workloads are based on reasonable voice video and data profiles that might be used in a lunar colony The profile of G.729 VoIP traffic is modeled with a constant bit rate CBR traffic generator configured with a sending rate of 8 Kbps In addition to VoIP the VCT generates low bandwidth but high priority command data Examples of command data include instrument commands EVA suit health status or other such mission critical operational data All command data generated by a VCT is modeled as a CBR traffic generator transmitting at a data rate of 13 Kbps SD and HD video are used at manned and unmanned stations Since not all visual applications require the higher 7 


resolution of HD video SD video is the default output for unmanned monitoring stations SD video is generated from all unmanned stations using variable bit rate VBR data at 4 Mbps HD video is only generated when higher resolution is needed for a specific application at a manned station Therefore HD video is generated only at a few nodes per simulation SD and HD video profiles used were taken from the H.264/AVC standard 15 Both profiles used in this study are not intended to specifically represent an exact combination of resolution and dimension but instead could represent the following profiles SD video at 4 Mbps could represent Extended Profile Level 2.1 while HD video could represent High Profile Level 3 These profiles can describe any number of combinations of resolution and frame rate and are only approximations of video traffic that could be used in these scenarios The video traffic model incorporates and on and an off time with durations determined by a Pareto distribution During an on state the model transmits a burst of data at a configured maximum burst rate The Pareto model's mean and shape parameter were set to 1.4 an on time of 5 seconds and an off time of 1 second Table 2 Traffic T pes and Attributes Typetiu Voice Command Telemetry SD Video HD Video Protocol UDP AR-PRESET 1.5 5.0 15 35 35 Normal TC TC C TOO Priority TC0 TC1 TC2 TC3 TC3 Metrics Throughput Round Trip Throughput Throughput Throughput Jitter Time Jitter Jitter Jitter For these simulations the EDCA code was extended to allow data collection at the transport layer The main metric used to evaluate the performance of the system with the Adaptive Algorithm is the MAC access delay which is the time from a packet's entrance to the MAC queue until the time it is successfully sent Arriving packets were time stamped at queue entry and at packet transmission so that the average delays could be calculated Using the access delays a mean access delay is accumulated in between algorithm actions Using the mean access delay as a measure of algorithm performance is discussed further in 12 In addition to mean access delay which is used in the algorithm we also obtain a number of application specific performance measures in order to evaluate the network These measures include end-to-end throughput jitter Mean Opinion Score MOS and Round Trip Time RTT The metric used for each type of data is shown in Table 2 7 THE DISTRIBUTED ALGORITHM The adaptive algorithm tunes the AIFS CWmin and CWmax parameters based on the performance of the network that is measured locally using a measure referred to as the aggressiveness ratio ar The ar is the ratio of the average access delay to an estimate of the minimal access delay In other words the ar is a measure of how aggressively the data flow should adjust its parameters An ar for each flow is calculated by measuring its current performance and comparing it to the best possible performance it can hope to achieve The ar value specifies a relative performance measure of how the station is performing compared to how well it could perform if there were no other channel contention From this point forward AR PRESET refers to the target ar value that is set before the simulation for a data flow and ar refers to the aggressiveness ratio calculated for each data flow during the algorithm operation The AR-PRESET is the overall performance goal of the data flow while the ar is the measure of the progress toward that goal at a particular point in time Figure 7 shows the pseudo-code that corresponds to the algorithm discussed in the rest of the section By dynamically adjusting contention parameters while the network is operating the high priority nodes are able to maintain their quality of service under various levels of network load This dynamic quality is important since it allows configurations for any delay requirements rather than the narrow performance characteristics of TCO The algorithm presented in Figure 7 uses a single setting called the AR PRESET that can predictably provide performance on the continuum of access delays between TCO and TC2 One of the primary concerns for the algorithm is providing a way for each node to derive stateful information about the system based on local observation rather than global knowledge In the context of space communication global knowledge is to some extend easier to obtain because the communication scenarios are usually planned out in detail Typically one knows at any given time the number of active stations and the expected traffic load from each therefore optimizing the back-off algorithm based on such 8 TCP UDP UDP UDP Profile CBR CBR CBR VBR VBR Rate kbps 8 13 248 4000 12000 Packet bytes 20 1000 1000 2048 2048 


information could be implemented without undue difficulty However lowering dependency on global knowledge is still valuable for the future of NASA missions not because such information is difficult to obtain but instead we want the network the adapt to changing mission scenario automatically therefore reducing the operational complexity and cost associated for network configuration and increase the robustness of the network during contingency situations During operation the algorithm uses mean  INTERVAL 38 interval_delay  0 more importantly knowledge of when to give up pursuit of that goal Since the only value known to the algorithm at start-up is the AR-PRESET which describes how aggressively was no contention After the start-up phase has completed the node's normal operation begins A node's normal operation consists of increase and decrease adjustments to the EDCA values The goal of the adjustments is to keep the ar in were not being shared with other nodes on comparing the current ar value to the AR PRESET for the station This derived value is called the ar ratio Table 3 shows example values that demonstrate how the ar ratio of the current ar to the AR PRESET influences parameter changes An a denotes assumes highest priority to get mean access delay is measured when the node was added The tolerance value adds padding to the decision to make parameter adjustments When the ar has been in the acceptable range for are made based can never reach comes online During Adaptive Algorithm start-up 30 ELSEIF ar 27 ELSE 28 relaxed_decrease 29 ENDIF or decreased in priority to TC3 Even though only small adjustments a stable condition and will fluctuate within the window of small adjustments For this reason a 9 1 FOR random in 0 to STARTUP 2 do nothing measure any physical 24 ELSEIF ar  AR PRESET 25 IF measure success or failure of the parameter adjustment After the change in access time and access delay to the base station For this short period the node lowers all parameters and near its AR PRESET goal volatile network conditions GOODH 23 break ar ratio  LOW 26 aggressive_decrease are made when an aggressive change while an r denotes an aggressive change and an adjustment will not be made the next time a new time period begins This feature allows the station to adapt quickly to current conditions The second important design decision involves finding a solution to this a node is given priority over all other nodes in order to estimate the minimum a relaxed change A  denotes a decrease in priority while a  denotes a relaxed change is made by the order in which the parameters a data flow is a period of time and the tolerance is high the ar must be out of the acceptable range multiple times to trigger an increase in priority The distinction between an unacceptable ar is found Table 3 Example Parameter changes can result since the ar will never be equal to the AR-PRESET In other words the algorithm by itself  AR PRESET 31 IF ar ratio  HIGH 32 aggressive_increase mean access delays gathered later This measurement is not intended to arRatio  0.5 0.5 0.8 0.8 1.2 1.2 2.0  2.0 Change arnone r access delay samples from before and after changes to the QoS values in order to access delay is assessed the delay is saved and a reliable method of providing a node with knowledge of its overall goal for its a data flow treats large ar values the node needs a way to discover the network's current performance status As a minimal a best case measurement to which it will then compare its or link layer capabilities but rather to find the performance possible if the medium or in other words if there an acceptable range Adjustments are changed For example according to the ordering explored in 4 an aggressive increase in priority would first try adjusting AIFS until exhausted then try CWmax then if the other two could not be adjusted it would try CWmin The upper and lower bounds of adjustment are the static TCO and TC3 parameters A parameter is considered exhausted when it has been increased in priority to TCO a tolerance mechanism a change Conversely when tolerance is low the ar must be in acceptable range multiple times in order to guarantee that 3 ENDFOR 4 aifs AIFS OPTIMAL 5 cwmin CWMIN OPTIMAL 6 cwmax CWMAX OPTIMAL 7 FOR counter  0 to START VAL by 1 8 optimal_delay  delay 9 ENDFOR 10 optimal delay  counter 11 aifs AIFS TC2 12 cwmin CWMIN TC2 13 cwmax CWMAX_TC2 14 15 normal_operation 16 17 interval delay  current-delay 18 IF time out  now 19 interval_delay  INTERVAL 20 ar  interval-delay  optimal-delay 21 ar_ratio  ar  AR-PRESET 22 IF ar ratio in range\(GOODL 33 ELSE 34 relaxed_increase 35 ENDIF 36 ENDIF 37 time out  now 39 ENDIF 40 41 GOTO normal_operation Figure 7 Adaptive Algorithm Pseudo-code 


2 3 4 5 6 7 Number of VCT Nodes 8 9 10 Figure 8 Total Average Throughput Voice Jitter 0.045 0.04 0.035 0.03 8 RESULTS AND DISCUSSION Figures 8 through 10 show the data collected using the network scenario described in Section 5 For each graph ten data points were measured that consist of three types of nodes Voice/Command/Telemetry VCT Telemetry/SD Video TS HD Video H The data point number designates the number of VCT nodes and TS nodes while the number of H nodes is fixed at two for all points For example data point 4 has four VCT four TS and two H nodes for a total of ten nodes Figures 8 through 10 show data compared across three simulations types simulations using no prioritization NoDiff using normal static prioritization Normal and using the Adaptive Algorithm AA In the NoDiff scenario no differentiation is used by disabling the Adaptive Algorithm and setting all priority queues to the default TC2 values shown in Table 1 According to the 802.1 le standard data flows coming into the AP that do not have TC designations are treated as TC2 In simulations using Normal prioritization each data flow is set to an appropriate TC as shown in Table 2 Voice and Command are considered most important since they represent basic communication between mission critical components Voice is the direct line of communication between EVA suits and Command data is the direct line of communication between automated systems While in many cases telemetry information can be considered mission critical here telemetry data is treated as supplemental information and is given the next highest priority Finally video has the lowest priority since it is considered an alternate form of communication to Voice and Command data There exist scenarios where each of these data flows can be mission critical and using the Adaptive Algorithm the priorities could be adjusted based on the task at hand however for these simulations it was determined that the priorities were as specified above In Figure 8 the average throughput of each node was totaled for each type of prioritization into a total throughput for the network This figure highlights an advantage of the Adaptive Algorithm The fact that the algorithm does not engage aggressively unless the AR PRESET requirement is not being met allows many situations where the data flows can coexist without the need of increased prioritization In fact the algorithm will reduce parameters to TC3 settings if the current ar is too low for its AR PRESET This feature allows better than NoDiff performance when contention in the network is low In the first data point for AA that contains one VCT one TS and two H nodes the Voice and Command data flows do not add additional contention to the network as they would under the Normal scenario Since Voice and Command use low priority parameters the low priority video flows are not impacted as greatly as they are under Normal prioritization For this same reason NoDiff performs better than Normal and AA performs better than both for the first two data points As the network load increases Normal performs better than AA and NoDiff 10000 5000 0 20000 15000 en 02 3 4 5 6 7 8 9 10 Number of VCT and TS Nodes Figure 9 Voice Jitter Figure 9 shows the average jitter for all voice flows for each of the three prioritization methods This data points out the disadvantage of dynamic prioritization By starting out with default values each data flow must find its own balance of parameters that satisfies its AR PRESET In doing so network conditions can become more volatile than Normal prioritization This can put more strain on network components such as codec buffers but as the graph shows the AR PRESET can be chosen so that the jitter still stays under an acceptable level Compared to NoDiff AA still succeeds in delivering lower jitter but it cannot be as effective as Normal prioritization 10 Total Average Throughput and TS en 0.025 0 0.02 cn 0.015 0.01 0.005 


Figure 9 also highlights the difficulty in designing a dynamic algorithm that is stable for most network configurations For data point 5 and 9 jitter is difficult to predict based on the AR PRESET This variability is most likely a result of the tolerance mechanism presenting artifacts during certain network configurations If the algorithm chooses too aggressively for too many data flows the contention parameters are set more aggressively and performance in the network suffers Although jitter remains better than NoDiff for all data points in Figure 9 providing more consistent behavior should be a primary goal for future versions of the AA SD Video Jitter 0.07 0.06 0.05  0.04  0.03 0.02 0.01 2 3 4 5 6 7 8 9 10 Number of VCT and TS Nodes Figure 10 SD Video Jitter The average jitter for SD video is shown in Figure 10 This data represents a lower priority level and as compared to voice jitter in Figure 9 is able to provide better predictability The data still shows that performance for AA experiences greater variability than Normal but over most data points is better than both NoDiff and Normal HD video performance is similar to SD but has greater overall jitter and is therefore not shown Telemetry Jitter o 045 o 04 4 NoDiff Normal 0 035 AA 0 03 025 0  1 2 3 4 5 6 7 8 9 10 Number of VCT Nodes 11 0.015 0.01 0.0050 Figure 11 Telemetry Jitter In Figure 11 which shows the jitter for Telemetry the data exhibits similar characteristics as the data for voice jitter Due to inconsistencies in the settling of the algorithm performance does not follow the same curve as NoDiff or Normal however for most data points the performance is better than NoDiff Figure 12 Command Round Trip Time The RTT for Command data is shown in Figure 12 Similar conclusions can be drawn from this data as have been discussed previously This data also exhibits greater variability than Normal and better overall performance than NoDiff The behavior of AA closely resembles that of AA in Figure 11 which reinforces the conclusion that the builtin hysteresis of the algorithm does not fully accommodate all network configurations Overall the Adaptive Algorithm is able to meet its goals of being a distributed way to dynamically manage priority and still perform similarly to static prioritization and better than no differentiation at all The self correcting characteristic of the algorithm leads to increased variability in performance which in some cases such as extreme network load causes the Adaptive Algorithm to perform significantly worse than static prioritization On the other hand network throughput is improved when the network is under-utilized because the Adaptive Algorithm is only engaged when performance requirements are not being met When viewed as a trade-off between simplicity of deployment and performance benefits over non-prioritized networks the Adaptive Algorithm can provide significant value to network architects and TS 


9 CONCLUSIONS AND FUTURE WORK We have presented an extension to 802.1l e that dynamically adapts the contention parameters to meet performance requirements of lunar communications We have explored the capacity of 802.1lg using dynamic 802.1le to handle VoIP command telemetry and video according to their respective QoS requirements The data presented here shows that the Adaptive Algorithm can be deployed in these scenarios to effectively manage priority without a complicated centralized infrastructure This capability will be essential for short-range lunar communications in that quick deployment will be a primary goal The prevalence of 802.11 hardware and the possibility of the 2.4GHz band to perform at long and short ranges make it a viable option for future lunar networks However priority will be a necessity especially if the network carries non-critical traffic alongside mission-critical VoIP and data Future work on the algorithm will include improvement to the tolerance mechanism to reduce added contention from over-adjustment It is possible that more testing with more conservative settings will result in less variable network conditions at the expense of faster convergence Although we feel that the algorithm presented here is important we believe it is not the total solution It has been shown in previous works that in general 802.1 le increases contention in the network which results in overall lower throughput The Adaptive Algorithm does alleviate this disadvantage for some scenarios but not all Other works show that 802.1 le under the HCCA can solve the problem of throughput at the expense of simplicity and rapid deployment Therefore we expect that many 802.11 e deployments will use a mix of pure EDCA and HCCA in addition to using EDCA as a backup measure for HCCA Having an efficient and simple way to use ECDA will provide network architects with greater flexibility and will further reduce dependence on proprietary network design as we move towards expansion beyond the Earth 10 ACKNOWLEDGEMENTS The research described in this paper was carried out at Clemson University under the South Carolina Space Grant Consortium Research and Education Awards Program REAP and at the Jet Propulsion Laboratory California Institute of Technology under a contract with the National Aeronautics and Space Administration 1 ANSI/IEEE Std 802.1 lb 1999 Edition IEEE Standard for Information technologyTelecommunications and Information exchange between systems Local and metropolitan area networks Specific requirements Part 11 IEEE 1999 2 ANSI/IEEE Std 802.1 le 1999 Edition IEEE Standard for Information technologyTelecommunications and Information exchange between systems Local and metropolitan area networks Specific requirements Part 11 Amendment 8 IEEE 1999 3 Wi-Fi CERTIFIED for WMM Support for Multimedia Applications with Quality of Service in Wi-Fi Networks Wi-Fi Alliance September 1 2004 4 Swaminathan Arvind and James Martin 2006 Fairness Issues in Hybrid 802.1 lb/e Networks Consumer Communications and Networking Conference 2006 3rd IEEE Vol 1 Issue 8-10 50-54 5 Wietholter Sven and Hoene Christian 2003 Design and Verification of an IEEE 802.1 le EDCA Simulation Model in ns-2.26 TKN Technical Report Series Technical University Berlin November 2003 6 Cali F M Conti E Gregori 2000 Dynamic Tuning of the IEEE 802.11 Protocol to Achieve a Theoretical Throughput Limit IEEE/ACM Transactions On Networking Vol 8 No 6 December 2000 785-799 7 Cali F M Conti E Gregori 2000 IEEE 802.11 Protocol Design and Performance Evaluation of an Adaptive Backoff Mechanism IEEE Journal on Selected Areas in Communications Vol 18 No 9 September 8 Chen D S Garg M Kappes K Trivedi 2002 Supporting VBR VoIP Traffic in IEEE 802.11 WLAN in PCF Mode Center for Advanced Computing and Communications ECE Department Duke University Durham NC Avaya Labs Research New Jersey 9 Siris Vasilios and Stamatakis George 2006 Optimal CWmin Selection for Achieving Proportional Fairness in Multi-Rate 802.1 le WLANs Test-bed Implementation and Evaluation International Conference on Mobile Computing and Networking WiNTECH 06 10 He Dajiang and Charles Shen 2003 Simulation Study of IEEE 802.1l e EDCA Vehicular Technology Conference Spring 2003 Vol 1 685-689 12 ILREFERENCES 


11 Xiao Yang L Haizhon S Choi 2004 Protection and Guarantee for Video and Voice Traffic in IEEE 802.1 le Wireless LANs INFOCOM 2004 Twenty-third Annual Joint Conference of the IEEE Computer and Communication Societies Volume 3 Issue 7-11 21522162 12 W Spearman J Martin A Distributed Adaptive Algorithm for QoS in 802.1 le Wireless Networks Proceedings of the 2007 International Symposium on Performance Evaluation of Computer and Telecommunication Systems SPECTS'07 San Diego CA July 2007 pp 379-386 13 Lim L.W Malik R Tan P.Y Apichaichalermwongse C Ando K Harada Y Panasonic Singapore Labs A QoS Scheduler for IEEE 802.1l e WLANs Consumer Communications and Networking Conference 2004 pp 199-204 14 V Vleeschauwer J Janssen G Petit and F Poppe Quality bounds for packetized voice transport Alcatel Tech Rep 1st Quarter 2000 15 ITU Series H Audiovisual and Multimedia Systems Infrastructure of audiovisual services Coding of moving video H.264 03/2005 International Telecommunication Union 12 BIOGRAPHY cooperative signal received his B.S Engineering from respectively processing and sensor networks He M.S and Ph.D degree in Electrical UCLA in 1993 1995 and 2000 Will Spearman is a Master's Candidate at Clemson University's School of Computing His work focuses on QoS in 802.cle and wireless networks His background includes a B.S in Psychology with a minor focus in Computer Science He currently is employed at Network Appliance Inc Dr Jim Martin is an Assistant Professor in the School of Computing at Clemson University His research interests include broadband access autonomic computing Internet protocols and network performance analysis He has received funding from NASA the Department of Justice BMW IBM and Cisco Dr Martin received his Ph.D from North Carolina State University Prior to joining Clemson Dr Martin was a consultant for Gartner and prior to that a software engineer for IBM Jay Gao joined the Jet Propulsion Laboratory in 2001 and is currently a senior research staff in the Communications Networks Group in the Telecommunication Research and Architecture section His research is primarily focused on space-based wireless communications and networking with emphasis on applications for the Mars Network He is currently conducting research for developing quality-of-service QoS protocols for the envisioned Interplanetary Network IPN and study optimization and protocols for deep space Ka-band communications He also supports requirements definition and interface design activities for the Department of Defense's Transformational Communications MilSatcom project and system engineering effort for NASA's Exploration System and Mission Directorate ESMD supporting the Constellation Program for return of human to the Moon and Mars Other research interests include optical-based sensorweb discrete event simulation of distributed communication/sensor systems energy efficient routing and self-organization algorithm for 13 


  14  Figure 5:  Site B1 Terrain horizon ma sk with 1 degree azimuth spacing  Figure 6:  Site B1 Terrain horizon mask with 1 de gree azimuth spacing, in e quatorial coordinates 


  15  Figure 7: Lunar South Pole Solar Illumination Yearly Average  Figure 8:  Lunar South Pole DTE Visibility Yearly Average 


  16  Figure 9: Lunar North Pole Sola r Illumination Yearly Average  Figure 10:  Lunar North Pole D TE Visibility Yearly Average 


  17  Figure 11: Site A1 Elevation Topography  Figure 12: Site A1 Yearly Average Solar Illumination and DTE visibility, Medium Resolution 


  18   Figure 13:  Site LB Te rrain Horizon Mask  Figure 14:  Theory and Computed values of Average Yearly Solar Illumination 


  19  Figure 15:  Theory and Computed values of Average Yearly DTE Communication  Figure 16:  Heliostat Mirror Design to Eliminate Cable Wrap 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


