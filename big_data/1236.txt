html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">FullCoverage Web Prediction based on Web Usage Mining and Site Topology Diamanto Oikonomopoulou 1 Maria Rigou 1,2 Spiros Sirmakessis 2 Athanasios Tsakalidis 1,2 1 Computer Engineering and Informatics Department, University of Patras, GR-26504 2 Research Academic Computer Technology Institute, 61 Riga Feraiou str., Patras, GR-26221 oikonomo@ceid.upatras.gr, rigou@cti.gr, syrma@cti.gr, tsak@cti.gr Abstract Understanding and modeling user online behavior, as well as predicting future requests remain an open challenge for researchers, analysts and marketers. In this paper, we propose an efficient prediction schema based on the extraction of sequential navigation patterns from server log files, combined with web site topology Traversed paths are monitored, internally recorded and cleaned before being completed with cashed page views After session and episode identification follows the construction of n-grams. Prediction is based upon a 5+ ngram schema with all lower level n-grams participating, a procedure that resembles the construction of an All 5thorder Markov Model. The schema achieves full coverage while maintaining competitive prediction precision 1. Introduction People display strong regularities in their cognitive behavioral model, and therefore in the actions they perform. In the web environment more specifically, there exist strong statistical regularities among the surfing patterns of a user \([1], [2 proper mechanisms that take advantage of the large volume of data users leave behind while navigating the web. Prediction models may be approached from a data mining or distributed systems perspective In the first case, prediction models can be further categorized as based on classification, based on frequent itemsets and association rules or based on clustering Classification [2] is a two-phase procedure related to the appropriate association of an object, into one of predetermined classes of common attributes \(i.e. common expected behavior interesting associations or correlations among a large dataset, proceeding in a co-occurrence based prediction Yang et al. [3], propose an association-based prediction model for caching and prefetching. In some cases, the two aforementioned techniques cooperate for the formation of Class Association Rules [4 In the distributed systems approach, the key idea relies on the construction of a predictive model that suggests future events based on past experience \(web access patterns that future actions are predicted without the concern for interactivity or immediate benefit [5]. A problem-solving framework, used for web document prediction and retrieval, is Case-based Reasoning \(CBR  propose a server-side Bayesian networks [6]. CBR application, aiming at the improvement of system performance during prefetching. In some cases, CBR techniques are combined with In statistical natural language processing, an ordered sequence of n items is defined as an n-gram. In the area of web usage mining, n-grams are correlated with timeordered sequences of user accesses, thus proper subsets of user sessions. There exist two types of n-gram based 


user sessions. There exist two types of n-gram based prediction models; point-based and path-based. The first ones rely on the currently observed action \(user request and thus suffer from low accuracy. Path-based models 7 a prediction for an ensuing request, capturing both the temporal and the sequential way web accesses are generated. Path-based models may demonstrate low applicability, due to the rarity of long-length patterns Experimental results have shown that for n?4, precision upper bound does not improve tremendously, while applicability decreases dramatically. Markov models MM processes [8], and are capable of tracking the likelihood of varying n-grams, in a state space encoding. The disadvantages of higher-order Markov Models summarize in non-negligible complexity, demanding storage requirements, insufficient coverage and in some cases poor predictive accuracy  compared to lower MM  s 5],[9 may be used, that combines different order MM  s in a way that the resulting model has a low state complexity improved prediction accuracy, while retaining the coverage of its components [9 3. The Proposed Approach to Prediction The prediction schema proposed in this paper is based on the extraction of sequential navigational patterns from Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence \(WI  04 0-7695-2100-2/04 $ 20.00 IEEE recorded server log data, integrating categorization techniques. Our goal is to encapsulate the internal motivations and ultimate objectives of varying user profiles, into corresponding access patterns that will allow for reliable predictions. The mining process uses as input page access requests sent from anonymous, non authenticated users that the system has no information regarding individual goals or characteristics. This approach fits the real life situations where commercial sites and portals choose not to make users go through registration \(a requirement that jeopardizes privacy or drives them away Apart from sequential pattern discovery, an effective categorization technique that integrates semantic information on web site content has also been implemented and has proved quite effective. More specifically, web site  s page files are categorized according to their content \(in our setup it is a manual process performed by the site administrator or owner Next, based on the assumption that interrelation of page files through links implies content interrelation, we construct a connected graph that represents the site  s internal hyperlink structure. The proposed algorithm outputs a site-map, where page files are inter-connected through links and with the home page as the starter node This semantic information is valuable in cases where mining does not return any matching patterns to be used for prediction Pattern extraction starts with data cleaning and log file parsing. Data cleaning regards stripping out requests for graphics files, as well as page misses. URI fields for all remaining page requests are normalized using a common formatting. Following executes the data processing phase comprising four successive steps: \(1 extraction of user sessions from the dataset, \(2 proper completion with cached page views, \(3 of episodes from the completed user sessions and \(4 formation of n-grams. Formally, the term session refers to a delimited, time-ordered set of user clicks. Session identification is based on either time or structure related criteria [10]. Our working assumption is that each different agent type for an IP address represents, at least 


different agent type for an IP address represents, at least one, discrete session \(as in [11] and [12 assume that the time interval spent on a single page file should not exceed a given threshold \(in which case a new session has started sessions, the assignment of requests with the same IP address and agent to one of the sessions is determined by measuring the distance between the request and each session. Distance between a request r and a session S, is defined as the number of links needed to be traversed from the last recorded page view of S, in order to obtain the referrer field of r as a request in the same session [11 A request is assigned to the session of minimum such distance The identified sessions must be complemented with cached page views that although traversed, are not recorded. Cooley in [11] tracks cashed page accesses by keeping a certain number of most recent page requests in a stack and in the case of a stack miss he resorts to a full history search. In this work we decided to perform full history search and skip the intermediate stack access since experimental results showed that the full history search does not cause noticeable performance reduction Recall that, although Cooley  s assumption supposes that cached page views are the result of user  s backtracking using the back button in the browser window- we should not neglect site  s topology, i.e. hyperlinks that allow user  s traversal between non sequential pages. Thus in many cases, full session history search is required, in order to obtain the desirable page file while it does not reduce performance, as sessions are not expected to be long. Using this technique we managed to have more accurate results without significant penalty in complexity Session completion is followed by episode identification. Episodes should be regarded as navigational subsets of significant semantic value that depict a well-formed snapshot of user  s orientation and desire. Episodes are identified according to the maximal forward reference method \(as described in [13 episode is defined as a time-ordered click sequence up to the request before a backward reference is made assuming that forward references may be used as reliable indications of what the user is looking for, while backtracking can be considered as  noise  to the semantic interpretation of user traversals The second phase of data processing proceeds with the formation of n-grams based on the extracted episodes The focus is mainly set on 3rd to 5th order n-grams and their suffices. These lengths were chosen as an equilibrium factor between precision and applicability since long n-grams tend to increase prediction accuracy with a considerable loss in applicability. This is also backed up by the observation that in most real-world implementation scenarios, long traversal sequences repeat less frequently than shorter ones. First-order n-grams are also included in order to sustain high scores in coverage The developed algorithm generates an index table T that lists all distinct n-gram couples and their subsequent request, as observed in the dataset. Each record in the index table includes an n-gram, its suffices and a corresponding support value \(indicating the specific ngram  s occurrence frequency page identifiers separated by a proper delimiter. After extracting all possible n-grams, overall mean support is computed in order to prune out of the index table rows with support below average. N-grams whose lower order proper subsets present a quite higher support value \(still over the minimum support threshold As a consequence, a matching \(n-1 preferable than a matching n-gram, in case it demonstrates Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence \(WI  04 


Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence \(WI  04 0-7695-2100-2/04 $ 20.00 IEEE better support. Even though in most cases, higher n-grams perform better in terms of precision, the prediction is based on a lower order n-gram, if the prerequisite significantly higher support are not affected from this last step, which is legitimate so that coverage maintains a satisfactory level. No n-grams are pruned out of the dataset due to their unary suffice mostly because 1-grams rely their decision on a single observed request The remaining set of n-grams provides the final patterns for obtaining prediction, concluding to a prediction table P. The prediction phase is based upon a 5+ n-gram schema with all remaining n-grams \(of length 5 down to 1 the construction of an All 5th-order Markov Model Generally, higher order n-grams receive higher priority by the prediction algorithm. In case that an observed sequence cannot be matched with some recorded n-gram the prediction algorithm searches for matching \(n-1 grams, taking into account only the n-1 last requests of the sequence. If attempts for matching n-grams \(n&gt;1 unsuccessful, the algorithm searches the prediction table P for the matching 1-gram \(corresponding to the last recorded page file in the sequence In case that there exist no matching n-gram in the table P \(n=1,2,3,4,5 examining the web site topology through the constructed site-map. Given a sequence seq that cannot be matched with any n-gram in P and the last observed request from seq is l, the prediction algorithm searches for all l  s outgoing links stored pair-wise in the site-map, in the form of \(l, p higher support value in the training dataset, that also belongs in l  s category. The rationale is based on the fact that in absence of a suitable matching pattern, we should search upon all potential single step transitions of l according to website structural constraints- and choose the most frequently observed, biased by the assumption that the user will keep on navigating through pages of the same category. This way, we are able to produce predictions even for cases that there are no recorded user patterns in the training dataset, concluding to a full coverage \(100 4. Experimental results: a case study The case study considers single action prediction, i.e prediction of the page file that will be requested immediately after a recorded sequence of requests. This approach accommodates both for evaluating prediction schemas with varying lengths and reaching comparative conclusions. The log data were recorded while monitoring users navigating a multi-topic electronic magazine. The magazine web site comprised 143 pages, categorized in several topic sections. An arbitrary set of the log data was used as a training set and the remaining as test data. We attempted single action prediction using an all 3rd, 4th and 5th order n-gram model. When existing n-grams could not provide a prediction the decision was based on the site map table, as described in the previous section Comparisons in the performance of different order models were based on the precision \(or accuracy applicability values of each schema. Precision is defined as the number of correct predictions P+ divided by the number of feasible predictions P++P- \(where P- refers to the number of unsuccessful predictions defined as the number of feasible predictions divided by all cases R that are used as input to the prediction process Note that a case in R that the algorithm failed to produce a prediction for is not assigned to neither P+ nor P Figure 1 plots precision and applicability for the 


Figure 1 plots precision and applicability for the different models under the hypothesis that the site-map was not used in the prediction process, while figure 2 presents the resulting values when the site-map was also taken into account. As depicted in figure 2, in the latter case prediction achieves full coverage. In the big majority of cases, the pruning step resulted in removing from the data set the same n-grams for both all 3rd and all 4th order models, which in turn led to the calculation of similar prediction tables P and therefore precision values 70,00 88,24 71,67 88,24 0 50 100 150   p re c is io n    a p p li c a b il it y All-5-gram All {3,4}gram Figure 1. Precision and applicability for pure all n th order n-gram model 61,76 100,00 63,24 100,00 0 50 100 150  p re c is io n    a p p li c a b il it y All-5-gram All {3,4}gram Figure 2. Precision and applicability for hybrid all n th 


order n-gram model Precision upper bounds for the all 5th order model appear higher than lower order models, because they rely their prediction decisions upon the n-grams of the lower Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence \(WI  04 0-7695-2100-2/04 $ 20.00 IEEE order models, alongside with a number of additional ones To picture that, if we restrict out prediction to using 3grams, 4-grams and 5-grams only, precision drops by approximately 20%. Another significant observation is the downfall in precision and applicability values in the hybrid approach \(figure 2 requirement for full coverage. More specifically applicability increases as its numerator P++P- increases which also results in a simultaneous decrease in precision A final remark is that the overhead caused by taking the all 5th order model approach is practically restricted in additional space requirements The proposed schema requires linear retrieval times regarding prediction decisions, while keeping space and memory requirements low through proper data tabulation The prediction table is easily updated as the training dataset increases, allowing constant learning on the part of the algorithm. Besides, the schema is flexible enough to support prediction for more than one action with minor tuning. Furthermore, in cases where the schema is used for producing recommendations or applying web prefetching, the fact that it assures full-coverage is a significant asset. Precision  s upper bound reached 71,67 overall prediction attempts, a quite competitive percentage when compared to other prediction techniques For example, the all 3-gram based model described in [7 achieves best case prediction accuracy near 63%, while applicability drops by 40%. Deshpande and Karypis [9 follow an approach similar to ours using a 5th order Markov-based prediction model that demonstrates accuracy around 50% when tested on log files coming from e-commerce sites. Davison and Hirsh in [2] propose a machine learning algorithm that accomplishes 40 precision in predicting future requests. Yang et al. [3 propose a CBR technique for web object prediction that offers a prediction accuracy upper bounded by 40 5. Conclusions and future work In this paper we have presented an efficient schema for predicting future web requests on a single site, based on the extraction of sequential navigation patterns from already recorded log data, combined with the site  s existing topology in terms of topic categories One way to improve the proposed prediction schema is to deploy a more complex categorization \(or even classification all page files similarly, regardless of whether they are media or auxiliary ones. Auxiliary pages present greater support than media pages and thus the algorithm assigns them higher priority when applying site-map based prediction, undermining the related media pages. An alternative approach may lead to increased precision upper bounds. Another refinement can be achieved during episode extraction by a using linear complexity algorithm as in [13 6. References 1] Liu, Jiming and Zhang, S. W., "Characterizing Web usage regularities with information foraging agents," in IEEE Transactions on Knowledge and Data Engineering, Vol. 16, No 4, 2004 2] B.D. Davison and H. Hirsh  Predicting Sequences of User Actions  Presented at the AAAI-98/ICML'98 Workshop on Predicting the Future: AI Approaches to Time Series Analysis Madison, WI, July 27, 1998, and published in Predicting the Future: AI Approaches to Time Series Problems, Technical 


Report WS-98-07, pp. 5-12, AAAI Press 3] Q. Yang, I.T.Y. Li and H.H. Zhang  Mining High-Quality cases for hypertext prediction and prefetching  in Proceedings of the 2001 International Conference on Case Based Reasoning ICCBR-2001, Vancouver BC, Canada, July 2001 4] B. Liu, W. Hsu and Y. Ma  Integrating Classification and Association Rule Mining  in Proceedings of KDD-98, New York, 1998 5] B.D. Davison  The Design and Evaluation of Web Prefetching and Caching Techniques  PhD thesis submitted to the Graduate School of New Brunswick Rutgers in the state University of New Jersey, 2002 6] S.N. Schiaffino and A. Amandi  User Profiling with CaseBased Reasoning and Bayesian Networks  in Proceedings of the International Joint Conference, 7th Ibero-American Conference on AI, 15th Brazilian Symposium on AI, IBERAMIASBIA 2000, Atibaia, SP, Brazil, November 19-22, 2000 7] Z. Su, Q. Yang, Y. Lu and H. Zhang  What next: A prediction System for Web Requests Using N-gram Sequence Models  in proceedings of the First International Conference on Web Information Systems and Engineering Conference, , Hong Kong, June 2000, pp 200-207 8] A. Papoulis  Probability, Random Variables and Stochastic Processes  Mc Graw Hill, 1991 9] M. Deshpande and G. Karypis  Selective Markov Models for Predicting Web-Page Accesses  in Proceedings of the 1st SIAM Data Mining Conference, 2000 10] B. Berendt, B. Mobasher, M. Spiliopoulou, J. Wiltswire Honghua Dai, T. Luo and M. Nakagawa  Measuring the Accuracy of Sessionizers for Web Usage Analysis  in Proceedings of the Web Mining Workshop at the First SIAM International Conference on Data Mining, April 2001. Chicago IL pp. 7-14 11] R.W. Cooley  Web Usage Mining: Discovery and Application of Interesting Patterns from Web Data  PhD thesis submitted to faculty of the graduate school of the University of Minnesota, 2000 12] P. Pirolli, J. Pitkow and R. Rao  Silk from a sow  s ear Extracting Usable Structures from the Web  in CHI-96 Vancouver, 1996, pp. 118-125 13] Z. Chen, R. Fowler and A. Wai-Chee Fu  Linear Time Algorithms for Finding Maximal Forward Reference  in Proceedings of the 2003 IEEE Intl Conference On Info Tech Coding and Computing \(ITCC03 Vegas, Nevada, pp. 160-164 Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence \(WI  04 0-7695-2100-2/04 $ 20.00 IEEE pre></body></html 


Table 2 All ations of Two s 000 001 and 000 000 where 000 001 000 000 000 Conbination of Patterns 1 001 001 002 002 003 003 002 001 004 005 001 001 002 002 001 004 2 001 001 002 002 003 000 002 001 004 005 001 001 002 002 003 004 3 001 001 002 002 003 003 002 001 004 4 001 001 002 002 003 003 002 001 004 005 004 5 001 001 002 002 003 001 002 001 004 005 001 001 002 002 003 003 002 001 004 6 001 001 002 002 003 000 002 001 004 005 001 001 002 002 003 003 002 001 004 005 001 001 002 002 001 004 005 001 001 002 002 001 004 7 001 001 002 002 003 003 002 001 004 005 001 001 002 002 003 000 002 001 004 005 001 001 002 002 003 004 005 001 001 002 002 003 004 8 001 001 002 002 003 000 002 001 004 005 001 001 002 002 003 003 002 001 004 005 001 001 002 002 003 003 002 001 004 9 001 001 002 002 003 000 002 001 004 005 001 001 002 002 003 003 002 001 004 005 001 001 002 002 003 003 002 001 004 005 004 10 001 001 002 002 003 001 002 001 004 005 001 001 002 002 003 001 002 001 004 005 001 001 002 002 003 003 002 001 004 References 1 A s a i  T  A b e K   K a w a s o e  S  A r i m u r a  H   Sakamoto H  Arikawa S Ef\336cient Substructure Discovery from Large Semi-structured Data Proc of the 2nd SIAM International Conference on Data Mining SDM-2002 pp.158-174 2002 2 D e R aed t  L  K r amer  S  T h e Le v e l w i s e V er s i o n Space Algorithm and its Application o ecular ment Finding Proc f the 17th International Joint erence on al Intelligence IJCAI2001 pp 853\320859 2001  G onz a l e z  J   H ol de r  L    Cook D  G r a ph-Ba s e d Relational Concept Learning Proc of the 19th International Conference on Machine Learning ICML2002 pp 219-226 2002  A IDS A nt i v i r a l S c re e n  ds/aids data.html  H ors t  R  a nd T u y  H  Global n Deterministic Approaches Springer 1996  I nokuc hi  A   W a s h i o  T    M ot oda  H  A n A pri o ri based Algorithm for Mining Frequent Substructures from Graph Data Proc of the 4th European Conference on Principles and Practice of Knowledge Discovery in Databases PKDD-2000 pp 13\32023 2000  I nokuc hi  A   W a s h i o  T   N i s h i m ura  Y    Mot oda  H  A Fast Algorithm for Mining Frequent Connected Subgraphs IBM Research Report  RT0448 February 2002  I nokuc hi  A   W a s h i o  T   Mot oda  H  C om pl e t e Mining of Frequent Patterns from Graphs Mining Graph Data Machine Learning  50 3 321-354 March 2003 9 K r a mer  S   D e R aed t  L  H e l m a C  M o l ecu l a r F eature Mining in HIV Data Proc f the 17th International Conference on Knowledge Discovery and Data Mining KDD-2001 pp 136\320143 2001  K u ra m o c h i  M   K a rypi s  G  F r e que nt S ubgra p h Discovery Proc of the 1st International Conference on Data Mining ICDM-2001 pp.313\320320 2001  L i u B  H s u  W   Ma  Y  P runi ng a n d S um m a ri z i ng the Discovered ations Proc of the 5th International Conference on Knowledge Discovery and Data Mining KDD-99 pp.125\320134 1999 12 M o tis h ita  S  a n d S e s e  J  T r a v e r s in g L a ttic e I te m set with Statistical Metric ng Proc of Symm on les f Database ystems PODS2000 pp.226\320236 2000 13 P T E  h ttp o ld w w w  c o m la b  o x  a c  u k  o u c l groups/machlearn/PTE  S e s e  J  a nd Mori s h i t a  S  A ns w e ri ng t h e M os t C or related N ation es Ef\336ciently Proc of 6th European Conference on Principles and Practice of Knowledge Discovery in Databases PKDD-02 pp.410\320422 2002  S u z uki  E  D i s c o v e ri ng U n e xpe c t e d E x c e p t i ons  A Stochastic Approach Proc f the 4th International Workshop on Rough Sets Fuzzy Sets and Machine Discovery RSFD-96 pp 225\320232 1996  S u z uki  E   K odra t of f Y  D i s c o v e ry of S u rpri s i ng Exception Rules based on Intensity of Implication Proc of Principles of Data Mining and Knowledge Discovery PKDD-98 pp 10\32018 1998  S u z uki  E  Z yt ko w  J  M U n i 336 e d A l gori t h m f or Undirected Discovery of Exception Rules Proc of Principles of Data Mining and Knowledge Discovery PKDD-2000 pp 169-180 2000  W u  X   Z h a ng C   Z h a ng S  Mi ni ng Bot h P o s itive and Negative Association Rules Proc f the 19th International Conference on Machine Learning  pp 658\320665 2002  Y a n X   H a n  J  g S p a n  G ra ph-Ba s e d S ubs t r uc t u re Pattern Mining Proc of the 3rd International Conference on a Mining  ICDM-2002 pp.721\320724 2002  Z a ki  M  E f 336 c i e n t l y Mi ni ng F r e que nt T r e e s i n a F or est Proc of the 8th International Conference on Knowledge Discovery and Data Mining KDD-2002 pp 71\32080 2002 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


Fig. 8 Runtime for GAS to find fuzzy sets for the three linguistic intervals The last experiment is dedicated to investigate the performance for the three linguistic intervals. In particular, we examined how the performance varies with the number of transactions. This is reflected in Figure 8 which shows the runtime as we increase the number of input records from 10K to 100K, for the three different cases. The results plotted in Figure 8 show that the method scales quite linearly for the census dataset used in the experiments 6. CONCLUSIONS In this paper, we proposed a clustering approach to solve the problem of interval partitioning in favor of the maximum number of large itemsets based on linguistic minimum support and confidence. The main achievement of the proposed approach is employing GAS to dynamically adjust and optimize membership functions which are essential in finding interesting weighted association rules from quantitative transactions, based on support and confidence specified as linguistic terms Compared to previous mining approaches, the proposed approach directly manages linguistic parameters, which are more natural and understandable to humans. Results of the experiments conducted on a real life census dataset demonstrated the effectiveness and applicability of the proposed approach r31 r41 r51 REFERENCES R. Agrawal, T. Imielinski and A. Swami  Mining association rules between sets of items in large databases  Proc. of ACM SIGMOD, pp.207-216, 1993 W.H. Au and K.C.C. Chan  An Effective Algorithm for Discovering Fuzzy Rules in Relational Databases  Proc C.H. Cai, et al  Mining Association Rules with Weighted Items  Proc. of IDEAS, pp.68-77, 1998 K.C.C. Chan and W.H. Au  Mining Fuzzy Association Rules  Proc. of ACM CIKM, pp.209-215, 1997 B.C. Chien, ZL. Lin and T.P. Hong  An Efficient Clustering Algorithm for Mining Fuzzy Quantitative Association Rules  IFSA World Congress and NAFIPS International Conference, Vo1.3, pp.1306-1311,2001 A.W.C. Fu, et al  Ending Fuzzy Sets for the Mining of Association Rules for Numerical Attributes  Proc. of the OfIEEE-FUZZ, pp.1314-1319,1998 115 International Symposium of Intelligent Data Engineering and Learning, pp.263-268, Oct. 1998 D.E. Goldberg, Genetic Algorithms in Search Optimization, and Machine Learning, Addison-Wesley Reading, MA, 1989 S. Guha, R. Rastogi and K. Shim  CURE: An Efficient Clustering Algorithm for Large Databases  Information Systems, Vo1.26, No.1, pp.35-58,2001 A. Gyenesei  A Fuzzy Approach for Mining Quantitative Association Rules  TUCS Technical Report No.336 2000 K. Hirota and W. Pedrycz  Linguistic Data Mining and Fuzzy Modelling  Proc. of IEEE-FUZZ, pp.1448-1496 1996 J.H. Holland, Adaptation in Natural and Artificial Systems, The MIT Press, Cambridge, MA, MIT Press edition, 1992. First edition: University of Michigan Press 1975 T.P. Hong, C.S. Kuo and S.C. Chi  A fuzzy data mining algorithm for quantitative values  Proc. of the International Conference on Knowledge-Based Intelligent Information Engineering Systems, pp.480483, 1999 T.P. Hong, C.S. Kuo and S.C. Chi  Mining Association 


Rules from Quantitative Data  Intelligent Data Analysis Vo1.3, pp.363-376, 1999 T. P. Hong, M. J. Chiang and S. L. Wang  Mining from Quantitative Data with Linguistic Minimum Supports and Confidences  Proc. of IEEE-FUZZ, pp. 494-499,2002 H. Ishibuchi, T. Nakashima and T. Yamamoto  Fuzzy Association Rules for Handling Continuous Attributes   Proc. of IEEE International Symposium on Industrial Electronics, pp. 1 1 8 - 12 1, 200 1 M. Kaya, R. Alhajj, F. Polat and A. Arslan  Efficient Automated Mining of Fuzzy Association Rules  Proc. of DEXA, 2002 C.M. Kuok, A.W. Fu and M.H. Wong  Mining fuzzy association rules in databases  SIGMOD Record, Vol. 17 No.1, pp.41-46, 1998 B. Lent, A. Swami and J. Widom  Clustering Association Rules  Proc. of IEEE ICDE!, pp.220-23 1 1997 R.J. Miller and Y. Yang  Association Rules over Interval Data  Proc. ofACM SIGMOD, pp.452-461, 1997 R. Ng and J. Han  Efficient and effective clustering methods for spatial data mining  Proc. of VLDB, 1994 W. Pedrycz  Fuzzy Sets Technology in Knowledge Discovery  Fuzzy Sets and Systems, 98, pp.279-290 1998 R. Srikant and R. Agrawal  Mining quantitative association rules in large relational tables  Proc. of ACM W. Wang and S.M. Bridges  Genetic Algorithm Optimization of Membership Functions for Mining Fuzzy Association Rules  Proc. of the International Conference on F m y  Theory &amp; Technology, pp. 13 1-134,2000 R.R. Yager  Fuzzy Summaries in Database Mining   Proc. of the Conference on Artificial Intelligence for Application, pp.265-269, 1995 S .  Yue, el al  Mining fuzzy association rules with weighted items  Proc. of IEEE SMC Conference pp.1906-1911,2000 L.A., Zadeh  Fuzzy Sets  Information and Control W. Zhang  Mining Fuzzy Quantitative Association Rules  Proc. of IEEE ICTRI, pp.99-102, 1999 SIGMOD, pp.1-12, 19 V01.8, pp.338-353, 1965 pre></body></html 


efficiency then AOFI. However utilization of fuzzy concept hierarchies provides more flexibility in reflecting expert knowledge and so allows better modeling of real-life dependencies among attribute values, which will lead to more satisfactory overall results for the induction process. The drawback of the computational cost may additionally decline when we notice that, in contrast to many other data mining algorithms, hierarchical induction algorithms need to run only once through the original \(i.e. massive dataset. We are continuing an investigation of computational costs of our approach for large datasets ACKNOWLEDGMENT Rafal Angryk would like to thank the Montana NASA EPSCoR Grant Consortium for sponsoring this research REFERENCES 1] J. Han , M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann, New York, NY 2000 2] J. Han, Y. Cai, and N. Cercone  Knowledge discovery in databases: An attribute-oriented approach  Proc. 18th Int. Conf. Ver y Large Data Bases, Vancouver, Canada, 1992, pp. 547-559 3] J. Han  Towards Efficient Induction Mechanisms in Database Systems  Theoretical Computing Science, 133, 1994, pp. 361-385 4] J. Han, Y. Fu  Discovery of Multiple-Level Association Rules from Large Databases  IEEE Trans.  on KD E, 11\(5 5] C.L. Carter, H.J. Hamilton  Efficient AttributeOriented Generalization for Knowledge Discovery from Large Databases  IEEE Trans. on KDE 10\(2 6] R.J. Hilderman, H.J. Hamilton, and N. Cercone  Data mining in large databases using domain generalization graphs  Journal of Intelligent Information Systems, 13\(3 7] C.-C. Hsu  Extending attribute-oriented induction algorithm for major values and numeric values   Expert Systems with Applications , 27, 2004, pp 187-202 8] D.H. Lee, M.H. Kim  Database summarization using fuzzy ISA hierarchies  IEEE Trans . on SMC - part B, 27\(1 9] K.-M. Lee  Mining generalized fuzzy quantitative association rules with fuzzy generalization hierarchies  20th NAFIPS Int'l Conf., Vancouver Canada, 2001, pp. 2977-2982 10] J. C. Cubero, J.M. Medina, O. Pons &amp; M.A. Vila  Data Summarization in Relational Databases through  Fuzzy Dependencies  Information Sciences, 121\(3-4 11] G. Raschia, N. Mouaddib  SAINTETIQ:a fuzzy set-based approach to database summarization   Fuzzy Sets and Systems, 129\(2 162 12] R. Angryk, F. Petry  Consistent fuzzy concept hierarchies for attribute generalization  Proceeding of the IASTED Int. Conf. on Information and Knowledge Sharing, Scottsdale AZ, USA, November 2003, pp. 158-163 13] Toxics Release Inventory \(TRI available EPA database hosted at http://www.epa.gov/tri/tridata/tri01/index.htm The 2005 IEEE International Conference on Fuzzy Systems790 pre></body></html 


the initial global candidate set would be similar to the set of global MFIs. As a result, during the global mining phase the communication and synchronization overhead is low  0 2 4 6 8 1 0 Number of Nodes Figure 5. Speedup of DMM 4.4.2 Sizeup For the sizeup test, we fixed the system to the 8-node con figuration, and distributed each database listed in Table 2 to the 8 nodes. Then, we increased the local database sire at each node from 45 MB to 215 MB by duplicating the initial database partition allocated to the node. Thus, the data distribution characteristics remained the same as the local database size was increased. This is different from the speedup test, where the database repartitioning was per formed when the number of nodes was increased. The per formance of DMM is affected by the database repartitioning to some extent, although it is usually very small. During the sizeup test, the local mining result of DMM is not changed at all at each node The results shown in Figure 6 indicate that DMM has a very good sizeup property. Since increasing the size of local database did not affect the local mining result of DMM at each node, the total execution time increased just due to more disk U 0  and computation cost which scaled almost linearly with sizeup 5 Conclusions In this paper, we proposed a new parallel maximal fre quent itemset \(MFI Max-Miner \(DMM tems. DMM is a parallel version of Max-Miner, and it re quires low synchronization and communication overhead compared to other parallel algorithms. In DMM, Max Miner is applied on each database partition during the lo 0 45 90 135 180 225 270 Amwnt of Data per Node \(ME Figure 6. Sizeup of DMM cal mining phase. Only one synchronization is needed at thc end of this phase to construct thc initial global candi date set. In the global mining phase, a top-down search is performed on the candidate set, and a prefix tree is used to count the candidates with different length efficiently. Usu ally, just a few passes are needed to find all global maximal frequent itemsets. Thus, DMM largely reduces the number of synchronizations required between processing nodes Compared with Count Distribution, DMM shows a great improvement when some frequent itemscts are large \(i.e long patterns employed by DMM for efficient communication between nodes; and global support estimation, subset-infrequency based pruning, and superset-frequency based pruning are used to reduce the size of global candidate set. DMM has very good speedup and sizeup properties References I ]  R. Agrawal and R. Srikant  FdSt Algorithms for Mining As sociation Rules  Pmc. o f f h e  ZOrh VLDB Conf, 1994, pp 487499 2] R. Agrawal and I. C. Shafer  Parallel Mining of Association Rules  IEEE Trans. on Knowledge and Dura Engineering Vol. 8, No. 6, 1996, pp. 962-969 3] R. I. Bayardo  Efficient Mining Long Patlems from Databases  Proc. ofrhe ACM SIGMOD Inf  l Conf on Man ogemenr ofDara, 1998, pp. 85-91 4] S.  M. Chung and J. Yang  A Parallel Distributive Join Al gorithm for Cube-Connected Multiprocessors  IEEE Trans on Parallel and Disrribured Systems, Vol. 7, No. 2, 1996, pp 127-137 51 M. Snir, S. Otto. S. Huss-Lederman, D. Walker, and J. Don gana, MPI: The Complete Reference, The MIT Press, 1996 


gana, MPI: The Complete Reference, The MIT Press, 1996 6] R. Rymon  Search through Systematic Set Enumeralion   Pmc. of3rd Inr  l Con$ on Principles of Knowledge Repre sentation and Reasoning, 1992, pp. 539-550 507 pre></body></html 


sketch-index in answering aggregate queries. Then Section 5.2 studies the effect of approximating spatiotemporal data, while Section 5.3 presents preliminary results for mining association rules 5.1 Performance of sketch-indexes Due to the lack of real spatio-temporal datasets we generate synthetic data in a way similar to [SJLL00 TPS03] aiming at simulation of air traffic. We first adopt a real spatial dataset [Tiger] that contains 10k 2D points representing locations in the Long Beach county \(the data space is normalized to unit length on each dimension These points serve as the  airbases  At the initial timestamp 0, we generate 100k air planes, such that each plane \(i uniformly generated in [200,300], \(ii, iii destination that are two random different airbases, and iv  the velocity direction is determined by the orientation of the line segment connecting its source and destination airbases move continually according to their velocities. Once a plane reaches its destination, it flies towards another randomly selected also uniform in [0.02, 0.04 reports to its nearest airbase, or specifically, the database consists of tuples in the form &lt;time t, airbase b, plane p passenger # a&gt;, specifying that plane p with a passengers is closest to base b at time t A spatio-temporal count/sum query has two parameters the length qrlen of its query \(square number qtlen of timestamps covered by its interval. The actual extent of the window \(interval uniformly in the data space \(history, i.e., timestamps 0,100 air planes that report to airbases in qr during qt, while a sum query returns the sum of these planes  passengers. A workload consists of 100 queries with the same parameters qrlen and qtlen The disk page size is set to 1k in all cases \(the relatively small page size simulates situations where the database is much more voluminous specialized method for distinct spatio-temporal aggregation, we compare the sketch-index to the following relational approach that can be implemented in a DBMS. Specifically, we index the 4-tuple table lt;t,b,p,a&gt; using a B-tree on the time t column. Given a count query \(with window qr and interval qt SELECT distinct p FROM &lt;t,b,p,a&gt WHERE t?qt &amp; b contained in qr The performance of each method is measured as the average number of page accesses \(per query processing a workload. For the sketch-index, we also report the average \(relative Specifically, let acti and esti be the actual and estimated results of the i-th query in the workload; then the error equals \(1/100 set the number of bits in each sketch to 24, and vary the number of sketches The first experiment evaluates the space consumption Figure 5.1 shows the sketch index size as a function of the number of sketches used \(count- and sum-indexes have the same results more sketches are included, but is usually considerably smaller than the database size \(e.g., for 16 signatures, the size is only 40% the database size 0 20 40 60 80 


80 100 120 140 160 8 16 32 number of sketches size \(mega bytes database size Figure 5.1: Size comparison Next we demonstrate the superiority of the proposed sketch-pruning query algorithm, with respect to the na  ve one that applies only spatio-temporal predicates. Figure 5.2a illustrates the costs of both algorithms for countworkloads with qtlen=10 and various qrlen \(the index used in this case has 16 sketches also illustrate the performance of the relational method which, however, is clearly incomparable \(for qrlen?0.1, it is worse by an order of magnitude we omit this technique Sketch-pruning always outperforms na  ve \(e.g., eventually two times faster for qrlen=0.25 increases with qrlen, since queries returning larger results tend to set bits in the result sketch more quickly, thus enhancing the power of Heuristics 3.1 and 3.2. In Figure 5.2b, we compare the two methods by fixing qrlen to 0.15 and varying qtlen. Similar to the findings of [PTKZ02]4 both algorithms demonstrate  step-wise  growths in their costs, while sketch-pruning is again significantly faster The experiments with sum-workloads lead to the same observations, and therefore we evaluate sketch-indexes using sketch-pruning in the rest of the experiments 4 As explained in [PTKZ02], query processing accesses at most two paths from the root to the leaf level of each B-tree regardless the length of the query interval Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE sketch-pruning naive relational 0 100 200 300 400 500 600 700 800 900 0.05 0.1 0.15 0.2 0.25 number of disk accesses query rectangle length 300 0 100 200 400 500 600 1 5 10 15 20 number of disk accesses query interval length a qtlen=10 b qrlen=0.15 Figure 5.2: Superiority of sketch-pruning \(count As discussed in Section 2, a large number of sketches reduces the variance in the resulting estimate. To verify this, Figure 5.3a plots the count-workload error of indexes 


using 8-, 16-, and 32- sketches, as a function of qrlen qtlen=10 error \(below 10 it increases slowly with qrlen used, however, the error rate is much higher \(up to 30 and has serious fluctuation, indicating the prediction is not robust. The performance of 16-sketch is in between these two extremes, or specifically, its accuracy is reasonably high \(average error around 15 much less fluctuation than 8-sketch 32-sketch 16-sketch 8-sketch relative error 0 5 10 15 20 25 30 35 0.05 0.1 0.15 0.2 0.25 query rectangle length relative error 0 5 10 15 20 25 30 35 1 5 10 15 20 query interval length a qtlen=10, count b qrlen=0.15, count relative error query rectangle length 0 5 10 15 20 25 0.05 0.1 0.15 0.2 0.25 relative error query interval length 0 5 10 15 20 25 30 1 5 10 15 20 c qtlen=10, sum d qrlen=0.15, sum Figure 5.3: Accuracy of the approximate results The same phenomena are confirmed in Figures 5.3b where we fix qrlen to 0.15 and vary qtlen 5.3d \(results for sum-workloads number of sketches improves the estimation accuracy, it also leads to higher space requirements \(as shown in Figure 5.1 Figures 5.4a and 5.4b show the number of disk accesses for the settings of Figures 5.3a and 5.3b. All indexes have almost the same behavior, while the 32-sketch is clearly more expensive than the other two indexes. The interesting observation is that 8- and 16-sketches have 


interesting observation is that 8- and 16-sketches have almost the same overhead due to the similar heights of their B-trees. Since the diagrams for sum-workloads illustrate \(almost avoid redundancy 32-sketch 16-sketch 8-sketch number of disk accesses query rectangle length 0 50 100 150 200 250 300 350 400 0.05 0.1 0.15 0.2 0.25 number of disk accesses query interval length 0 50 100 150 200 250 300 350 1 5 10 15 20 a qtlen=10 b qrlen=0.15 Figure 5.4: Costs of indexes with various signatures Summary: The sketch index constitutes an effective method for approximate spatio-temporal \(distinct aggregate processing. Particularly, the best tradeoff between space, query time, and estimation accuracy obtained by 16 sketches, which leads to size around 40 the database, fast response time \(an order of magnitude faster than the relational method average relative error 5.2 Approximating spatio-temporal data We proceed to study the efficiency of using sketches to approximate spatio-temporal data \(proposed in Section 4.1 as in the last section, except that at each timestamp all airplanes report their locations to a central server \(instead of their respective nearest bases maintains a table in the form &lt;time t, plane p, x, y&gt;, where x,y with parameters qrlen and qtlen distinct planes satisfying the spatial and temporal conditions. For comparison, we index the table using a 3D R*-tree on the columns time, x, and y. Given a query, this tree facilitates the retrieval of all qualifying tuples, after which a post-processing step is performed to obtain the Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE number of distinct planes \(in the sequel, we refer to this method as 3DR method introduces a regular res  res grid of the data space, where the resolution res is a parameter. We adopt 16 sketches because, as mentioned earlier, this number gives the best overall performance Figure 5.5 compares the sizes of the resulting sketch indexes \(obtained with resolutions res=25, 50, 100 the database size. In all cases, we achieve high compression rate \(e.g., the rate is 25% for res=25 evaluate the query efficiency, we first set the resolution to the median value 50, and use the sketch index to answer workloads with various qrlen \(qtlen=10 


workloads with various qrlen \(qtlen=10 size \(mega bytes database size 0 20 40 60 80 100 120 140 160 25 50 100 resolution Figure 5.5: Size reduction Figure 5.6a shows the query costs \(together with the error in each case method. The sketch index is faster than 3DR by an order of magnitude \(note that the vertical axis is in logarithmic scale around 15% error observations using workloads with different qtlen Finally, we examine the effect of resolution res using a workload with qrlen=0.15 and qtlen=10. As shown in Figure 5.6c, larger res incurs higher query overhead, but improves the estimation accuracy Summary: The proposed sketch method can be used to efficiently approximate spatio-temporal data for aggregate processing. It consumes significantly smaller space, and answers a query almost in real-time with low error 3D Rsketch number of disk accesses query rectangle length 1 10 100 1k 10k 0.05 0.1 0.15 0.2 0.25 16 14% 15 15% 13 relative error number of disk accesses query interval length 1 10 100 1k 10k 1 5 10 15 20 16 15% 15% 12% 11 relative error a qtlen=10, res=25 b qrlen=0.15, res=25 0 500 1000 1500 2000 2500 25 50 100 number of disk accesses resolution 20% 15% 14 relative error c qrlen=0.15, qtlen=10 


c qrlen=0.15, qtlen=10 Figure 5.6: Query efficiency \(costs and error 5.3 Mining association rules To evaluate the proposed algorithm for mining spatiotemporal association rules, we first artificially formulate 1000 association rules in the form \(r1,T,90 with 90% confidence i randomly picked from 10k ones, \(ii in at most one rule, and \(iii Then, at each of the following 100 timestamps, we assign 100k objects to the 10k regions following these rules. We execute our algorithms \(using 16 sketches these rules, and measure \(i  correct  rules divided by the total number of discovered rules, and \(ii successfully mined Figures 5.7a and 5.7b illustrate the precision and recall as a function of T respectively. Our algorithm has good precision \(close to 90 majority of the rules discovered are correct. The recall however, is relatively low for short T, but gradually increases \(90% for T=25 evaluated in the previous sections, the estimation error decreases as the query result becomes larger \(i.e., the case for higher T 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 precision HT 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 recall HT a b Figure 5.7: Efficiency of the mining algorithm Summary: The preliminary results justify the usefulness of our mining algorithm, whose efficiency improves as T increases Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE 6. Conclusions While efficient aggregation is the objective of most spatio-temporal applications in practice, the existing solutions either incur prohibitive space consumption and query time, or are not able to return useful aggregate results due to the distinct counting problem. In this paper we propose the sketch index that integrates traditional approximate counting techniques with spatio-temporal indexes. Sketch indexes use a highly optimized query algorithm resulting in both smaller database size and faster query time. Our experiments show that while a sketch index consumes only a fraction of the space required for a conventional database, it can process 


required for a conventional database, it can process queries an order of magnitude faster with average relative error less than 15 While we chose to use FM sketches, our methodology can leverage any sketches allowing union operations Comparing the efficiency of different sketches constitutes a direction for future work, as well as further investigation of more sophisticated algorithms for mining association rules. For example, heuristics similar to those used for searching sketch indexes may be applied to improve the brute-force implementation ACKNOWLEDGEMENTS Yufei Tao and Dimitris Papadias were supported by grant HKUST 6197/02E from Hong Kong RGC. George Kollios, Jeffrey Considine and were Feifei Li supported by NSF CAREER IIS-0133825 and NSF IIS-0308213 grants References BKSS90] Beckmann, N., Kriegel, H., Schneider, R Seeger, B. The R*-tree: An Efficient and Robust Access Method for Points and Rectangles. SIGMOD, 1990 CDD+01] Chaudhuri, S., Das, G., Datar, M., Motwani R., Narasayya, V. Overcoming Limitations of Sampling for Aggregation Queries. ICDE 2001 CLKB04] Jeffrey Considine, Feifei Li, George Kollios John Byers. Approximate aggregation techniques for sensor databases. ICDE, 2004 CR94] Chen, C., Roussopoulos, N. Adaptive Selectivity Estimation Using Query Feedback. SIGMOD, 1994 FM85] Flajolet, P., Martin, G. Probabilistic Counting Algorithms for Data Base Applications JCSS, 32\(2 G84] Guttman, A. R-Trees: A Dynamic Index Structure for Spatial Searching. SIGMOD 1984 GAA03] Govindarajan, S., Agarwal, P., Arge, L. CRBTree: An Efficient Indexing Scheme for Range Aggregate Queries. ICDT, 2003 GGR03] Ganguly, S., Garofalakis, M., Rastogi, R Processing Set Expressions Over Continuous Update Streams. SIGMOD, 2003 HHW97] Hellerstein, J., Haas, P., Wang, H. Online Aggregation. SIGMOD, 1997 JL99] Jurgens, M., Lenz, H. PISA: Performance Models for Index Structures with and without Aggregated Data. SSDBM, 1999 LM01] Lazaridis, I., Mehrotra, S. Progressive Approximate Aggregate Queries with a Multi-Resolution Tree Structure. SIGMOD 2001 PGF02] Palmer, C., Gibbons, P., Faloutsos, C. ANF A Fast and Scalable Tool for Data Mining in Massive Graphs. SIGKDD, 2002 PKZT01] Papadias,  D., Kalnis, P.,  Zhang, J., Tao, Y Efficient OLAP Operations in Spatial Data Warehouses. SSTD, 2001 PTKZ02] Papadias, D., Tao, Y., Kalnis, P., Zhang, J Indexing Spatio-Temporal Data Warehouses ICDE, 2002 SJLL00] Saltenis, S., Jensen, C., Leutenegger, S Lopez, M.A. Indexing the Positions of Continuously Moving Objects. SIGMOD 2000 SRF87] Sellis, T., Roussopoulos, N., Faloutsos, C The R+-tree: A Dynamic Index for MultiDimensional Objects. VLDB, 1987 TGIK02] Thaper, N., Guha, S., Indyk, P., Koudas, N Dynamic Multidimensional Histograms 


SIGMOD, 2002 Tiger] www.census.gov/geo/www/tiger TPS03] Tao, Y., Papadias, D., Sun, J. The TPR*Tree: An Optimized Spatio-Temporal Access Method for Predictive Queries. VLDB, 2003 TPZ02] Tao, Y., Papadias, D., Zhang, J. Aggregate Processing of Planar Points. EDBT, 2002 TSP03] Tao, Y., Sun, J., Papadias, D. Analysis of Predictive Spatio-Temporal Queries. TODS 28\(4 ZMT+01] Zhang, D., Markowetz, A., Tsotras, V Gunopulos, D., Seeger, B. Efficient Computation of Temporal Aggregates with Range Predicates. PODS, 2001 ZTG02] Zhang, D., Tsotras, V., Gunopulos, D Efficient Aggregation over Objects with Extent PODS, 2002 Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE pre></body></html 


