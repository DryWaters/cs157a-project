  Augmenting UML with Fact-orientation  Terry Halpin Microsoft Corporation, USA TerryHa@microsoft.com    Abstract The Unified Modeling Language \(UML\ is more useful for object-oriented code design than conceptual information analysis. Its process-centric use-cases provide an inadequate basis for specifying class diagrams, and its graphical language is incomplete, inconsistent and unnecessarily complex. For example, multiplicity constraints on n-ary associations are problematic, the constraint primitives are weak and unorthogonal, and the graphical language impedes verbalization and multiple instantiation for model validation. This paper shows how 


to compensate for these defects by augmenting UML with concepts and techniques from the Object Role Modeling ORM\ approach. It exploits "data use cases" to seed the data model, using verbalization of facts and rules with positive and negative examples to facilitate validation of business rules, and compares rule visualizations in UML and ORM. Three possible approaches are suggested: use ORM for conceptual analysis then map to UML supplement UML with population diagrams and userdefined constraints; enhance the UML metamodel   1. Introduction  The Unified Modeling Language \(UML\ was a 


dopted in 1997 by the Object Management Group \(OMG\as a language for object oriented \(OO\ analysis and design This paper is concerned with UML version 1.3, the latest approved version at the time of writing. A minor revision 1.4\hould be approved around December 2000, and a major revision \(2.0\hould be completed a few years later. Though not yet a standard, UML has been pr oposed for standardization by the International Standards Organization, with approval likely around The UML notation includes the following kinds of 


diagram for modeling different perspectives of an application: use case diagrams, class diagrams, object diagrams, statecharts, activity diagrams, sequence diagrams, collaboration diagrams, component diagrams and deployment diagrams. This paper focuses on conceptual data modeling, so considers only the static structure \(class and object\rams. Class diagrams are used for the data model, and object diagrams for data populations. Altho ugh not yet widely used for designing database applications, UML class diagrams effectively provide an extended Entity-Relationship \(ER\ notation that can be annotated with database constructs \(e.g. key 


declarations\. Background on UML may be found in its specification [31 a sim p le in tr o d u ctio n  1 3  o r a d e tailed  treatment d ept h di s c u s s i on s of U M L f o r database design may be found in an d w i t h  a s l i g h t l y  different notation\ [3   UML has become popular for designing OO program code. It is well suited for this purpose, covering data behavior, and OO-implementation details \(e.g. attribute visibility and directional navigation across associations However, UML is less suitable for developing and 


validating a conceptual data model with domain experts Its use-cases are process-centric, and in practice the move from use cases to class diagrams is often little more than a black art. Moreover, the UML notation prevents many common business rules from being diagrammed We believe these defects are best avoided by using fact-oriented modeling as a precursor to object-oriented modeling in UML. Object-Role Modeling \(ORM\ is the main exemplar of the fact-oriented approach, and is supported by CASE tools such as Microsoft Visio Enterprise F o r dat a m odel i n g  O R M s g r aph i cal  notation is more expressive and orthogonal than UML’s 


its models and queries are semantically stabler, and its design procedures fully exploit data examples using both verbalization and multiple instantiation to help capture and validate business rules with domain experts This paper identifies several weaknesses in the UML graphical language and discusses how fact-orientation can augment the object-oriented approach of UML. It shows how verbalization of facts and rules, with positive and negative examples, facilitates validation of business rules and compares rule visualizations in UML and ORM on the basis of specified modeling language criteria. The 


following three approaches are suggested as possible ways to exploit the benefits of fact-orientation: \(1\ use ORM for conceptual information analysis and map the ORM model to UML; \(2\ use UML in its current form, supplemented by informal population diagrams and user-defined constraints; \(3\d extend the UML metamodel to better support business rules 





























expect this optimization to be of greatest bene\336t when the transaction sizes are large r example if our transaction is T 000 f A\000 B 000 C\000 D\000 E g  k 000 3 fan-out 000 2 then all the 3-subsets of T are f ABC,ABD,ABE,ACD,ACE,ADE,BCD,BCE,BDE,CDE g  Figure 2 shows the candidate hash tree C 3  We ave to increment the support of every subset of T contained in C 3  We egin with the subset AB C  and hash to node 11 and process all the itemsets In this downward path from the root we mark nodes 1 4 and 11 as visited We then process subset AD B  and mark node 10 Now consider the subset CDE  We see in this case that node 1 has already been marked and we can preempt the processing at this very stage This approach can r consume a lot of memory r a n fan-out F  for iteration k  e need additional memory of size F k to store the 337ags In the parallel implementation we have to keep a VISITED 336eld for each processor bringing the memory requirement to P\000F k  This can still get very large especially with increasing number of processors In we sho w a mechanism by which further reduces the memory requirement to only k 000F  The approach in the parallel setting yields a total requirement of k 000F 000P  5 Experimental Evaluation Database T I D Total Size T5.I2.D100K 5 2 100,000 2.6MB T10.I4.D100K 10 4 100,000 4.3MB T15.I4.D100K 15 4 100,000 6.2MB T20.I6.D100K 20 6 100,000 7.9MB T10.I6.D400K 10 6 400,000 17.1MB T10.I6.D800K 10 6 800,000 34.6MB T10.I6.D1600K 10 6 1,600,000 69.8MB Table 2 Database properties 5.1 Experimental Setup All the experiments were performed on a 12-node SGI Power Challenge shared-memory multiprocessor Each node is a MIPS processor running at 100MHz There\325s a total of 256MB of main memory The primary cache size is 16 KB 64 bytes cache line size with different instruction and data caches while the secondary cache is 1 B 128 bytes cache line size The databases are stored on an attached 2GB disk All processors run IRIX 5.3 and data is obtained from the disk via an NFS 336le server We used different synthetic databases with size ranging form 3MB to 70MB 2  and are generated using the procedure described in These databases mimic the transactions in a retailing en vironment Each transaction has a unique ID followed by a list of items bought in that transaction The 2 While results in this section are only shown for memory resident databases the concepts and optimization are equally applicable for non memory resident databases In non memory resident programs I/O becomes an important problem Solutions to the I/O problem can be applied in combination with the schemes presented in this paper These solutions are part of future research 11 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


  0 500 1000 1500 2000 2500 0 2 4 6 8 10 12 Number of Large Itemsets Iterations Large Itemset at Support = 0.5 222T5.I2.D100K\222  222T10.I4.D100K\222   222T15.I4.D100K\222   222T20.I6.D100K\222   222T10.I6.D400K\222   222T10.I6.D800K\222   222T10.I6.D1600K\222  Figure 3 Large Itemsets per Iteration data-mining provides information about the set of items generally bought together Table 2 shows the databases used and their properties The number of transactions is denoted as jD j  average transaction size as j T j  and the average maximal potentially large itemset size as j I j  The number of maximal potentially large itemsets j L j 000 2000 and the number of items N 000 1000 We refer the reader to for more detail on the database generation All the e xperiments were performed with a minimum support value of 0.5 and a leaf threshold of 2 i.e max of 2 itemsets per leaf We note that the  improvements shown in all the experiments except where indicated do not take into account initial database reading time since we speci\336cally wanted to measure the effects of the optimizations on the computation Figure 3 shows the number of iterations and the number of large itemsets found for different databases In the following sections all the results are reported for the CCPD parallelization We do not present any results for the PCCD approach since it performs very poorly and results in a speed-down on more than one processor 3  5.2 Aggregate Parallel Performance Table 3 s actual running times for the unoptimized sequential and a naive parallelization of the base algorithm Apriori for 2,4 and 8 processors without any f the techniques descibed in sections 3 and 4 In this section all the graphs showing  improvements are with respect to the data for one processor in table 3 Figure 4 presents the speedups obtained on different databases and different processors for the CCPD parallelization The results presented on CCPD use all the optimization discussed 3 Recall that in the PCCD approach every processor has to read the entire database during each iteration The resulting I/O costs on our system were too prohibitive for this method to be  12 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


