Proceedings of ICCT2013 Implementation of K-Means Based on Improved Storm Model Jingling Zhao, Zhaohua Sun, Qing Liao Beijing University of Posts and Telecommunications, Beijing 100876, China zhaojingling@bupt.edu.cn, szh1124@163.com liaoqing@bupt.edu.cn Abstract  In recent years, big data processing has been a trend. Hadoop and some other cloud computing technologies make batch processing possible to process big data. Storm is like a real-time Hadoop. Storm model is easy, but it  s difficult to deal with complex Topology due to the increasing inter-dependences between 
components. This paper proposes a novel method which  simplifies the Storm Topology programming model by combining Spring with Storm. It provide a modular approach and a unified configuration model to build topologies and easy to use API for using Storm Meanwhile, this paper propose a system to process data and realize the K-Means clustering algorithm in Storm The practice results is shown and analyzed to prove the effectiveness of the system and model, at the same time it proves that Storm can improve the clustering algorithm processing speed Keywords Stream processing Storm Spring K-Means GPS Kafka 
1  Introduction As the amount of data increases, big data processing technologies are also developing. Had is a  representative distributed data processing technology for cloud computing. While people have high demand on real-time response now so that Storm   Storm is an open-source distributed realtime computation system. It is especially designed for processing endless stream data, which makes stream processing easier and reliable. The biggest difference between Storm and Hadoop is Storm never stop unless you kill it explicitly The GPS data is generated constantly over time and space from cars, which is large but reflect the temporal 
and spatial characteristics of urban traffic network making it necessary to manage this dynamic data Through the cluster analysis based on cars' location information by time, we can not only learn traffic information in real-time but provide a reasonable basis for road planning of the city Therefore, the GPS data requires effective analysis technology for meaningful information search to mirror traffic condition. However, the traditional database based real time analysis methods cannot meet the requirements because their limited performance on the speed of response for solving large scale data. As an alternative solution, realtime big data processing 
analysis methods is required. Storm is very simple in this challenge which processes data seems like a water treatment system. Spou reads data f r om e x ter n al data source and then emits the data to Bo process  until the whole Topology co m p leted A n d S t or m is  not an expensive solution to solve problems with large streaming data sets at all. However, as the Topology logic becomes complex, the coupling and the dependences among components will increase Thus, this paper aims to propose structure and computing environment needed for analysis of large-scale GPS data which was collected. The proposed system consists of three parts: data gathering, data analysis, and data storage. Our focus is on data import 
and data analysis module. The data gathering module simulates stream status by Kafka  ere are   related introduction about Kafka and Storm in     T h e K Means is used f o r t h e GP S data  analysis, which is the most important for this paper Data storage module save the results through files. In fact, files, relational databases or NoSQL databases are all acceptable data sources and result storage Meanwhile, this paper improved the traditional programming model of Storm by combining Spring with Storm which realized the definition of Topology based on XML configuration files In github, mykidong realize the Storm-spring-example, but each Spout/Bolt need its 
own ApplicationContext.xml T h is pap er reali zed all  of the components defined in one ApplicationContext.xml file. Finally, the paper implemented the GPS data analysis module using new Storm Topology programming model. In addition, the experimental results was shown and analyzed to prove the effectiveness of the model The structure of this paper is organized as below: In section 2, related technology used in the analysis system  will be introduced. In section 3, proposed the system architecture, and process the K-Means algorithm based on Storm. How to implement the system in the Storm and the evaluation will be proposed in section 4. Lastly this paper will be concluded with the suggestions for 
future research 2  Related work 2.1 Storm Model Storm is similar to Hadoop. Storm  s task is called Topology which is submitted to the master node to execute. Topology is a programming model which contains processing logic to process large-scale data in parallel. There are two kinds of components in a Topology: Spout and Bolt. How to transmit messages ____________________________ ________ 978-1-4799-0077-0/13  31 00 ©201 3 IEEE  


Proceedings of ICCT2013   among components is decided by streaming grouping  The Spouts, Bolts and the stream between components compose Topology. Figure 1 shows the structure of the Topology used in the analysis system in this paper                  Figure 1 The Storm Topology  The above illustration shows how a simple Topology would look like in this paper. There are five components in the Topologys: KafkaSpout, SplitBolt, InitBolt KmeansBolt and WriteBolt. At the KafkaSpout, it reads stream data from Kafka and emits its output in parallel to SplitBolt. At all kinds of Bolt, the tuple created at the last stage is passed to the next Bolt by processing according to user  s definition. In this process, the Storm implements Topology program model and framework so that we only focus on the implement of the Spout and Bolt function, and the rest process including distribution and parallel processing is automatically done by the Storm framework  2.2 K-Means Algorithm A GPS terminal can track where you are in real time according to the time, the longitude and the latitude information. The data of this paper comes from a northern city in China. Through analyzing these data you can learn the traffic conditions of the city and also provide a reasonable basis for road and urban planning As a classical partition clustering algorithm, K-Means partitions the samples into clusters according to Nearest Neighbor Principle between the samples and the centroid of the clusters. Because of its easy to describe and implement, and its efficient to handle large data sets the algorithm has been widely used in natural language processing and text clustering and many other fields, etc Nowadays, with the development of distributed technology, distributed technology combines with the cluster algorithm has become a research hotspot Currently, the traditional and improved clustering algorithms are rewritten into the parallel MapReduce computation model to execute, but on Storm related algorithms are not implemented, so this paper realize the K-Means algorithm based on Storm 2.3 Spring  Spring Framework is an open source, lightweight inversion of control, and aspect oriented container framework. Its architecture is based on the dependency injection design pattern. The key benefit of this approach is that it enables the developers to build loosely coupled applications. Flexible dependency injection with XML and annotation-based configuration styles bring all business bean into Spring container and manage all beans. This minimizes the dependency of application code on the container, which in turn improves productivity, maximizes the opportunity for code reuse, and improves testability. Storm Spring provides a unified configuration model to build Storm topologies in a Spring XML configuration. The Storm topology can be assembled freely by Storm developers based on the Spring Framework 2.4 Kafka Kafka is a high-throughput distributed messaging system developed for collecting and delivering high volumes of log data with low latency k a cl u s ter i s   totall y distributed, including consumer, producer and Kafka brokers.This paper uses Kafka to transform the files into streaming data to realize the real distributed computing 3  The System Architecture Based on The Storm 3.1 System Architecture  The implementation of the K-Means algorithm is important for our GPS data analysis in Storm. It decides the clustering results. However, the traditional K-Means method cannot be used in Storm directly. It need to be improved for the parallel execution. Therefore, this paper simulates a real-time system to analyze the GPS data of a northern city by Storm platform which is shown in Figure 2. It mainly includes data collecting module and analysis module, and the source data and result is stored in files     GPS  data    \(files Kafka  Cluster transform  files  into  message  queue   Nimbus  ZooKeeper Bolt  Spout Supervisor  Figure 2 The Analysis System  The collecting module collects data from external and push them into Kafka message queue. Each message includes time, longitude, latitude and some other related information. Then the data is processed by Storm. The 


Proceedings of ICCT2013   difference is the Topology is realized through the XML configuration file by combining with Spring. K-Means algorithm is used as an analysis engine for the collected large-scale GPS data. Based on such framework and algorithm, we can get the results easily and efficiently as if real time statistics  3.2 Data Collecting Storm does not define the source of data processing, so the data can be logs files, database and message queue etc. As long as Spout implements the corresponding interface it can read the data from anywhere  However there is a problem. Since the data is likely on different computers, Spout can not know which machine the data is. Even if the data exists on the same machine,  the number of tasks that should be assigned to execute this Spout can only be one which does not meet the distributed features otherwise the data will be read the same times as parallelism.This paper presents the solutions as follows                     Figure 3 Data Collection Module  Kafka middleware unify disparate data into the message queue. Storm as the consumers of message queue makes the problem of reading data in parallel solved. Meantime different Topology shared data sources become available and it improves the scalability of the system. Through messaging middleware layer, it masks external data source details and Storm-Kafka plugin for Storm is available. However, the introduction of Kafka will undoubtedly bring a certain complexity, which increase the threshold for application development on the Storm The data collection structure is as above Figure 3 3.3 K-Means Algorithm The system uses K-means algorithm for data clustering to extract meaningful information. As a traditional clustering algorithm, the standard processing flow is presented as follows. Suppose we want to divide the data into k clusters   1\oose k cluster centers randomly as the initial center u1 u2 uk 2\ the kth iteration, for each data, compute the distance to the k centers choose the most closest to and assign the data point to this cluster   3\compute centers by methods such as average  4\ the new set of k cluster centers is the same as those calculated in previous, the algorithm would be terminated. Otherwise, it should return to step 2 The clustering algorithm based on the Storm consists of four stages: pre-processing data; init stage; k-means stage and write result stage. Figure 1 shows the Topology of the GPS data analysis system based on Storm. This paper uses the same time data for clustering analysis according to the time field because our data is ordered in time 3.4 Spring For Storm The definition of the task in Storm is very easy. Take our topology for an example. The definition of Topology using  TopologyBuilder is as follows Figure 4 the Figure 5 shows the same Topology defined using Spring configuration files  Figure 4 Define Topology Using TopologyBuilder   Figure 5 Define Topology Using Spring Configuration Files  Notice that the creation and submission of the Topology is handled by the IoC container. Topology, Spout and Bolt are all seen as beans, then place them in Spring container to complete the initialization. By combining 


Proceedings of ICCT2013   Spring with Storm, the Storm components configuration and task deployment are all unified to Spring bean to manage. The Topology is assembled via the configuration file so that it reduces the dependency of application code on the container How to develop a Topology? The implementation of Topology function is mainly dependent on the Spout and Bolt. The definition of the Topology using Spring Configuration Files is processed in steps as follows  Steps 1 define the Spout and Bolt standard interfaces for using in Spring XML. Here are a few examples  a\ogy Interface: IProcessorTopology b\t Interface: IRichSpoutDef c\olt Interface: IRichBoltDef Steps 2 using the standard interfaces to develop a Topology Steps 3 assemble Topology a\ easy assembly, we give a setter method for class   Topology  s Spout and Bolt attribute b\e implementation class of Spout and Bolt c\ild Storm topology in a Spring XML configuration Steps 4 submit the Topology to Storm cluster to Run 4  Implementation  and Evaluation This section will describe the system's detail implementation, including data import, K-Means algorithm's parallelization using the new SpringStorm model, and I compared the execution time between based on SpringStorm and TopologyBuilder, between reading messages form Kafka and file, between based on Storm and a standalone Java program The system was developed on a cluster of 3 virtual machine. One is as Kafka server and one supervisor, one is Storm nimbus and zookeeper, the last one is as Storm supervisor. Every server has a 512M RAM and 20G dists. The operation system is ubuntu 12.4. The Storm is Storm-0.7.4, Kafka is Kafka-0.7.0, Zookeeper is Zookeeper-3.4.3 4.1 Implementation Data is downloaded from the datatang.com, including more than twenty thousand taxis GPS location data of a northern city in March, we use some sample data to do the experiment. The data is saved as a text file with 493MB units from 1 PM to 4 PM on March 15. First, the program will feed GPS data into Kafka through Producer program to simulate that data is the form of flow. Then write topology, this article develop the Topology using the new model according to the steps mentioned above to verify the availability of the model this paper presented. Figure 6 shows the main beans of the GPS data analysis module based on SpringStorm The whole topology was build through the idea of componentization, and it consists of five components The system injects dependencies into its topology using the dependency injection service, separating the building from using forcibly First, Storm as Kafka  s consumer feeds GPS data through Kafka and emits every message as input data of the next step. Second, we split the message into multiple fields. In this process, the time, longitude and latitude fields were emitted to the init step. Third, init the cluster state including cluster centers, cluster set and error sum of squares, then transfer these information to kmeans step. Fourth, the K-Means algorithm is calculated, the result value is transfered to the write step. Fifth, the result is written into file, because of involving writing file operation, this step can not realize parallelization   Figure 6 The Main Beans Of The Topology Based On SpringStorm  4.2 Evaluation Three experiments have been conducted on the test set. One is to evaluate the performance of the SpringStorm module, and I compared and analyzed the execution time between the proposed SpringStorm based K-Means algorithm and the traditional Storm based algorithm. One is to evaluate the performance of Storm based reading messages from Kafka and reading messages from file. The other is to evaluate the performance based on Storm and simply standalone Java program. The results are shown in table I, table II and table III  


Proceedings of ICCT2013   T ABLE I  Comparation of Execution Time Based on SpringStorm and TopologyBuilder  SpringStorm based TopologyBuilder 1m16s 1m12s T ABLE II  Comparation of Execution Time Based on Storm Reading message form Kafka and file Read message form Kafka Read message from file 1m12s 1m4s T ABLE III  Comparation of Execution Time Based on Storm and standalone Java program Storm based standalone Java program 1m12s 2m23s   Figure 7 The K-Means Result  Table I shows the comparation of execution time between based on SpringStorm and TopologyBuilder The result shows that the proposed method building topology based on Spring XML file has almost the same performance with the traditional TopologyBuilder model From Table II, we can see that reading data from Kafka costs more time than reading data directly from files. It was likely to be because the Kafka cluster would loss some performance, and our data sets and loads are not high so that the result in the performance degradation Meanwhile, we need to control the amount of messages having been emitted but not processed to prevent the Memory and CPU from becoming a bottleneck   As shown in the above Table III, performance improved almost double in executing K-Means algorithm based on Storm compared with a simply standalone Java program On mass data processing, Storm  s distributed environments offer huge advantages over similar workloads deployed in a standalone system. Figure 7 shows the clustering results for a second 5  Conclusions and Further Work Nowadays how to mining useful information from a mass of data almost as quickly as the production of information becomes a popular topic. This paper presented a massive data processing system based on Storm and improved its topology model by combing with Spring. Finally, this paper       implemented the K-means Clustering algorithm based on improved Storm model The system is used by combing Storm with Kafka and Spring. Storm reads data from a Kafka cluster. And data is processed by topology builded in a Spring XML configuration to assemble different components. From the experiments we can see that the performance was improved effectively based on Storm compared with executing a standalone java program. And using Spring has little influence on performance. However, there is performance degradation while using Kafka cluster In the area of highly-parallel batch processing, there have been quite a lot of research. However, less research in the fields of real-time stream processing especially about Storm. In the future, I will extend research based on Storm and further improving the performance of Storm  and also research how to combine other technologies with Storm References 1  http://hadoop.apache.org 2  http://Storm-project.net 3  https://github.com/nathanmarz/Storm 4  https://github.com/nathanmarz/Storm/wiki/Tutorial 5  http://Kafka.apache.org 6  Jay Kreps,Neha Narkhede,Jun Rao.Kafka: a Distributed Messaging System for Log Processing 7  https://github.com/ebottabi/Storm-spring-example 8  https://github.com/nathanmarz/Storm-contrib/tree/master Storm-Kafka 9  http://www.michael-noll.com/blog/2012/10/16/understan ding-the-parallelism-of-a-Storm-Topology 10  Kafka + Storm = Realtime Data at GumGum  http://blog.gumgum.com/2012/08/23/Kafka-Storm-realti me-data-at-gumgum 11  Storm or Apache Kafka  http://www.ymc.ch/en/Storm-or-apache-Kafka  


IV E XPERIMENTS This Section focuses on a comparative analysis between the FAIR and HFSP schedulers After evaluating the global performance of the two schedulers we focus on the estimation error as output by our size estimation module A Experimental Setup We used a cluster composed by 36 T ASK T RACKER machines with 4 CPUs and 8 GB of RAM each We congured Hadoop according to advised best practises 24 the HDFS block size is 128 MB with replication factor of 3 each T ASK T RACKER has 2 map slots with 1 GB of RAM dedicated to each and 1 reduce slots with 2 GB of RAM In total our cluster has 72 M AP slots and 36 R EDUCE slots The slowstart factor is congured to start the R EDUCE phase for a job when 95 of its M AP tasks are completed HFSP operates with the following parameters the sample set size s for both M AP and R EDUCE tasks is set to 5 the  timeout to estimate R EDUCE task size is set to 10 seconds we schedule aggressively jobs that are in the training phase setting  1 and t  100  The FAIR scheduler has been congured with a single job pool Workloads We generate workloads using PigMix a benchmarking suite used to test the performance of Apache Pig releases PigMix is appealing to us because much like its standard counterparts for traditional DB systems such as TPC it generates realistic datasets with properties such as data skew and denes queries inspired by real-world data analysis tasks We generated four datasets of sizes respectively 1 GB 10 GB 40 GB and 100 GB Job arrival follows a Poisson process and jobs are generated by choosing uniformly at random a query between the 17 dened in PigMix and applying it to one of the datasets according to a workload-dened probability distribution We evaluate two workloads  SMALL this workload is inspired by the Facebook 2009 trace observed by Chen et al  where a majority of jobs are very small The mean interval between job arrivals is  30 s   LARGE this workload is predominantly composed of relatively heavy-duty jobs In this case the mean interval between jobs is   120 s  In Table I we report the probability distribution for choosing a particular dataset size we remark that PigMix queries operate on different subsets of the generated datasets resulting in a variable number of M AP R EDUCE tasks Each workload is composed of 100 jobs and both HFSP and FAIR have been evaluated using the same jobs the same inputs and the same submission schedule We have additional results  not included here for lack of space  that conrm our results on different platforms Amazon EC2 and the Hadoop Mumak emulator and with different workloads synthetic traces generated by SWIM The y are available in a technical report TABLE I J OB SIZES IN OUR EXPERIMENTAL WORKLOADS   Dataset size Map tasks Workload SMALL LARGE 1GB  5 65 0 10 GB 10  50 20 10 40 GB 50  150 10 60 100 GB  150 5 30 TABLE II M EAN SOJOURN TIME MST AND MEAN LOAD  Workload MST s Mean Load FAIR HFSP FAIR HFSP SMALL 63 53 2.26 1.99 LARGE 2,291 544 16.80 4.60 B Macro Benchmarks In order to evaluate the overall performance of our system we compare FAIR with HFSP on sojourn time  the interval between a jobs submission and its completion  and load in terms of number of pending jobs i.e those that have been submitted and not yet completed Table II shows mean sojourn time across all jobs and mean load over the duration of the experiment for our two workloads In the SMALL workload HFSP decreases the mean sojourn time by around 16 By observing the empirical cumulative distribution function ECDF of sojourn times in Figure 3\(a we notice larger differences between FAIR and HFSP for jobs with longer sojourn times note the logarithmic scale on the x axis In this workload the system is on average loaded with around 2 pending jobs see Table II since these jobs are often small the system is generally able to allocate all tasks of pending jobs resulting in analogous scheduling choices and therefore sojourn time for both FAIR and HFSP However when system load is higher HFSP outperforms FAIR Our results are strikingly different for the LARGE workload Figure 3\(b where the mean sojourn time with HFSP is less than a quarter of the one with FAIR In this workload most jobs require several task slots and complete more quickly since HFSP awards them the entire cluster if needed when they are scheduled Instead the sharing strategy of FAIR has the drawback of increasing the sojourn time of all jobs M AP phases of most jobs complete earlier in HFSP making it possible to schedule R EDUCE phases sooner than with FAIR As a result with HFSP 30 of jobs complete within 100 seconds from their submission while in the same time window FAIR only completes 2 of them after 1,000 seconds from submission 90 of jobs are completed with HFSP while only 15 are completed with FAIR Scheduling choices are more critical when the cluster is loaded by jobs that require many resources and the difference between the SMALL and LARGE workloads exemplies this clearly Figure 3\(c shows the evolution of load run on the LARGE workload even if the job submission schedule for HFSP and FAIR is the same load is promptly decreased in HFSP by focusing resources on single jobs The fact that 56 


             10 1 10 2 10 3 Sojourn Time s 0  0 0  2 0  4 0  6 0  8 1  0 ECDF HFSP FAIR a Sojourn time for the SMALL workload                10 1 10 2 10 3 10 4 Sojourn Time s 0  0 0  2 0  4 0  6 0  8 1  0 ECDF HFSP FAIR b Sojourn time for the LARGE workload                              0  00  51  01  52  02  53  03  54  04  5 Time h 0 5 10 15 20 25 30 35 Load pending jobs HFSP FAIR c Cluster load for the LARGE workload Fig 3 Macro benchmark results scheduling becomes more critical in situations of high load is indeed conrmed by our simulation results These results allow us to conclude that HFSP performs better than FAIR in two very different workloads the advantage is more pronounced when the job and workload size is large with respect to the cluster size In that case scheduling decisions become critical and the inefciencies of simple fair sharing become apparent C Estimation Errors and Sojourn Times We have shown that HFSP outperforms FAIR in particular when applied to clusters with high load and heavy jobs Next we characterize estimation errors we measured in our experiments and discuss their impact on job sojourn times in light of our initial analysis of scheduling performance discussed in Section II-B In our experiments task times are clearly skewed Figure 4 shows the distribution of task times measured for all our experiments most tasks complete within few tens of seconds but around 10 of R EDUCE tasks and a non-negligible number of M AP tasks need orders of magnitude more time to complete As such we now characterize the job size estimation errors induced by our sampling-based technique As done in Section II-B when s i is the real size of the job and  s k i the estimated size obtained using k sample tasks we dene the estimation error as  k i  s k i s i   k j  1 means that job size is under-estimated whereas  k j  1 in case of over-estimation Figure 5 shows the ECDF of estimation errors across all our experiments for our setting of k 5 sample tasks The empirically observed error distribution maps well to the log-normal distribution we use in Section II-B Using a maximum likelihood tting method we approximate the error distribution as LogN    2   for M AP   0  0976 and  0  411  for R EDUCE   0  0878 and  0  228  The Kolmogorov-Smirnov goodness of t test does not reject the tting at a signicance level of 0.05 It is impossible to evaluate HFSP in a real deployment and in the complete absence of estimation errors since execution time of a given job in Hadoop varies at each run according to complex and rather unpredictable system properties  T o isolate the impact of errors on scheduling and sojourn                  10 0 10 1 10 2 10 3 10 4 Task Time 0  0 0  2 0  4 0  6 0  8 1  0 ECDF M AP R EDUCE Fig 4 Task time distribution                  0.25 0.5 124 Error 0  0 0  2 0  4 0  6 0  8 1  0 ECDF M AP R EDUCE Fig 5 Estimation error  k i for k 5  time we thus turn to our simulation results on SRVT which is a size-based scheduler with aging induced by fair sharing in virtual time It thus can be seen as a model for HFSP which abstracts from the intricacies of a real system deployment In our observed errors the estimation module tends to slightly over-estimate job sizes i.e  0 and error distributions are not exactly centered on 1 this results in marginally slower job aging with respect to the  0 case Moreover the  values suggest that the impact of size estimation errors on sojourn time are small as shown in Figure 2\(c on page 3 for log-normal error distributions with  0  5  SRVT achieves a mean sojourn time which is close to the one obtained with no estimation errors We believe that HFSP is likely to be similarly tolerant to such errors Indeed the main difference between SRVT and HFSP is that the latter operates in an environment that 57 


has to deal with the constraints imposed by Hadoop such as data locality task granularity and dependencies between M AP and R EDUCE phases see Section III-B These constraints limit the degrees of freedom available to the scheduler and result in cases where HFSP will take the same scheduling choices regardless of estimation errors With analogous error distributions and more possibility to deviate from optimal behavior SRVT achieves near-optimal mean sojourn time this suggests that while HFSP could certainly benet from more sophisticated and accurate size estimation methods further improvements in sojourn times are likely to be marginal V R ELATED W ORK MapReduce in general and Hadoop in particular have received considerable attention recently both from the industry and from academia Since we focus on job scheduling we consider here the literature pertaining to this domain Theoretical Approaches Several theoretical works tackle scheduling in multi-server system s  a recent example is the work by Moseley and Fox These w orks which are elegant and important contributions to the domain provide performance bounds and optimality results based on simplifying assumptions on the execution system e.g jobs with a single phase Some works provide interesting approximability results applied to simplied models of MapReduce 31 In contrast we focus on the design and implementation of a scheduling mechanism taking into account all the details and intricacies of a real system Fairness and QoS Several works take a system-based approach to scheduling on MapReduce For instance the FAIR scheduler and its enhancement with a delay scheduler is a prominent example to which we compare our results Several other works  focus on resource allocation and strive at achieving fairness across jobs but do not aim at optimizing sojourn times Sandholm and Lai study the resource assignment problem through the lenses of a bidding system to achieve a dynamic priority system and implement quality of service for jobs Kc and Anyanwu address the problem of scheduling jobs to meet user-provided deadlines but assume job runtime to be an input to the scheduler Flex is a size-based scheduler for Hadoop which is available as a proprietary commercial solution In Flex fairness is dened as avoiding job starvation and guaranteed by allocating a part of the cluster according to Hadoops FAIR scheduler size-based scheduling without aging is then performed only on the remaining set of nodes In contrast by using aging our approach can guarantee fairness while allocating all cluster resources to the highest priority job thus completing it as soon as possible Job Size Estimation Various recent approaches  propose techniques to estimate query sizes in recurring jobs Agarwal et al  report that recurring jobs are around 40 of all those running in Bings production servers Our estimation module on the other hand works on-line with any job submitted to a Hadoop cluster but it has been designed so that the estimator module can be easily plugged with other mechanisms benetting from advanced and tailored solutions Complementary approaches Task size skew is a problem in general for MapReduce applications since larger tasks delay the completion of a whole job skew also makes job size estimation more difcult The approach of SkewTune greatly mitigates the issue of skew in task processing times with a plug-in module that seamlessly integrates in Hadoop which can be used in conjunction with HFSP Tian et al 13 propose a mechanism where IO-bound and CPU-bound jobs run concurrently benetting from the absence of conicts on resources between them We remark that also in this case it is possible to benet from size-based scheduling as it can be applied separately on the IOand CPU-bound queues Tan et al  41 propose strate gies to adapti v ely start the R EDUCE phase in order to avoid starving jobs also this technique is orthogonal to the rest of scheduling choices and can be integrated in our approach Hadoop offers a Capacity Scheduler which is designed to be operated in multitenant clusters where different organizations submit jobs to the same clusters in separate queues obtaining a guaranteed amount of resources We remark that also this idea is complementary to our proposal since jobs in each queue could be scheduled according to a size-based policy such as HFSP and reap according benets Framework Schedulers Recent works have pushed the idea of sharing cluster resources at the framework level for example to enable MapReduce and Spark applications to run concurrently Monolithic schedulers such as YARN and Omega use a single component to allocate resources to each framework while two-level schedulers 47 ha v e a single manager that negotiates resources with independent framework-specic schedulers We believe that such framework schedulers impose no conceptual barriers for size-based scheduling but the implementation would require very careful engineering In particular size-based scheduling should only be limited to batch applications rather than streaming or interactive ones that require continuous progress VI C ONCLUSION Our work was motivated by the realization that MapReduce has evolved to the point where shared clusters are used for a wide range of workloads which include a non-negligible fraction of interactive data processing tasks As a consequence we have witnessed the raise of deployment best practices in which long sojourn times  due to a fair sharing of resources among competing jobs  were compensated by overdimensioned Hadoop clusters In addition we remarked that an efcient cluster utilization could be approximated through a tedious manual exercise involving the creation of static resource pools to accommodate workload diversity and an important tuning effort To overcome such limitations in this work we set off to study the benets of a new scheduling discipline that targets at the same time short sojourn times and fairness among jobs We thus proposed a size-based approach to scheduling jobs 58 


in Hadoop which we called HFSP Our work brought up several challenges evaluating job size on-line without wasting resources avoiding job starvation both on small and large jobs and guaranteeing short sojourn time despite estimation errors were the most noteworthy We solved these problems in the context of a multi-server system using virtual time and aging that is built to be tolerant to failures scale-out upgrades and supports the composite job structure of MapReduce We showed that a size-based discipline such as HFSP performs very well and that a precise job size information is not essential for the scheduler to function properly Our experimental results in which we compared HFSP to the widely used FAIR scheduler indicate that both interactivity and efciency requirements were largely met both small and large jobs do not starve and the job sojourn time distribution is consistently in favor of HFSP Our work has practical consequences as well HFSP is simple to congure and allows resource pools to be consolidated because workload diversity is intrinsically accounted for by the size-based discipline Our future work is related to job preemption We are currently investigating a novel technique to ll the gap between killing running tasks and waiting for tasks to nish Indeed killing a task too late is a huge waste of work and waiting for a task to complete when it just started is detrimental as well Our next goal is thus to provide a new set of primitives to suspend and resume tasks to achieve better preemption A CKNOWLEDGEMENTS This work has been partially supported by the EU projects BigFoot FP7-ICT-223850 and mPlane FP7-ICT-318627 R EFERENCES  J Dean and S Ghema w at MapReduce Simplied data processing on large clusters in Proc of USENIX OSDI  2004  Y  Chen S Alspaugh and R Katz Interacti v e query processing in big data systems A cross-industry study of MapReduce workloads in Proc of VLDB  2012  K Ren et al  Hadoops adolescence An analysis of Hadoop usage in scientic workloads in Proc of VLDB  2013  Apache Oozie W orko w Scheduler  http://oozie.apache.or g   Hadoop Open source implementation of MapReduce  http hadoop.apache.org  E Friedman and S Henderson F airness and ef cienc y i n web serv er protocols in Proc of ACM SIGMETRICS  2003  L E Schrage and L W  Miller  The queue m/g/1 with the shortest remaining processing time discipline Operations Research  vol 14 no 4 1966  M Harchol-Balter et al  Size-based scheduling to improve web performance ACM TOCS  vol 21 no 2 2003  A V erma L Cherkaso v a  and R H Campbell  Aria automatic resource inference and allocation for MapReduce environments in Proc of ICAC  2011   T w o sides of a coin Optimizing the schedule of MapReduce jobs to minimize their makespan and improve cluster performance in Proc of IEEE MASCOTS  2012  S Agarw al et al  Re-optimizing Data-Parallel Computing in Proc of USENIX NSDI  2012  A D Popescu et al  Same queries different data Can we predict query performance in Proc of SMDB  2012  C T ian et al  A dynamic MapReduce scheduler for heterogeneous workloads in Proc of IEEE GCC  2009  D Lu H Sheng and P  Dinda Size-based scheduling policies with inaccurate scheduling information in Proc of IEEE MASCOTS  2004  Y  Chen et al  Statistical workload injector for MapReduce https github.com/SWIMProjectUCB/SWIM  M Zaharia et al  Delay scheduling A simple technique for achieving locality and fairness in cluster scheduling in Proc of ACM EuroSys  2010  Y  Chen A Ganapathi R.Grif th and R Katz The case for e v aluating MapReduce performance using workload suites in Proc of IEEE MASCOTS  2011  M DellAmico  A simulator for data-intensi v e job scheduling  EURECOM Tech Rep RR-13-282 2013  J Nagle On pack et switches with innite storage  Communications IEEE Transactions on  vol 35 no 4 1987  S Gorinsk y and C Jechlitschek F air ef cienc y  or lo w a v erage delay without starvation in Proc of IEEE ICCCN  2007  Apache Hadoop wiki po wered by   http://wiki.apache.or g/hadoop PoweredBy  D Stiliadis and A V arma Latenc y-rate serv ers a general model for analysis of trafc scheduling algorithms IEEE/ACM TON  vol 6 no 5 1998  Apache Hadoop f air scheduler   http://hadoop.apache.or g/docs/stable fair  scheduler.html   Hadoop MapReduce JIRA 1184  https://issues.apache.or g/jira browse/MAPREDUCE-1184   PigMix  https://cwiki.apache.or g/PIG/pigmix.html  TPC Tpc benchmarks  http://www tpc.or g/information/benchmarks asp  M P astorelli et al  Practical size-based scheduling for MapReduce workloads CoRR  vol abs/1302.2749 2013  G Ananthanarayanan et al  Reining in the outliers in map-reduce clusters using mantri in Proc of USENIX OSDI  2010  K F o x and B Mosele y  Online scheduling on identical machines using SRPT in In Proc of ACM-SIAM SODA  2011  H Chang et al  Scheduling in MapReduce-like systems for fast completion time in Proc of IEEE INFOCOM  2011  B Mosele y et al  On scheduling in map-reduce and ow-shops in In Proc of ACM SPAA  2011  T  Sandholm and K  Lai MapReduce optimization using re gulated dynamic prioritization in Proc of ACM SIGMETRICS  2009  M Isard et al  Quincy fair scheduling for distributed computing clusters in Proc of ACM SOSP  2009  A Ghodsi et al  Dominant resource fairness Fair allocation of multiple resources types in Proc of USENIX NSDI  2011  B Hindman et al  Mesos A platform for ne-grained resource sharing in the data center in Proc of USENIX NSDI  2011  T  Sandholm and K Lai Dynamic proportional share scheduling in Hadoop in Proc of JSSPP  2010  K Kc and K An yanwu Scheduling Hadoop jobs to meet deadlines  in Proc of CloudCom  2010  J W olf et al  FLEX A slot allocation scheduling optimizer for MapReduce workloads in Proc of ACM MIDDLEWARE  2010  Y  Kw on et al  Skewtune mitigating skew in MapReduce applications in Proc of ACM SIGMOD  2012  J T an X Meng and L Zhang Delay tails in MapReduce scheduling  in Proc of ACM SIGMETRICS  2012   Performance analysis of coupling scheduler for MapReduce/Hadoop in Proc of IEEE INFOCOM  2012  Apache Hadoop capacity scheduler   http://hadoop.apache.or g/docs stable/capacity scheduler.html  M Zaharia et al  Resilient distributed datasets a fault-tolerant abstraction for in-memory cluster computing in Proc of USENIX NSDI  2012  Apache Hadoop ne xtgen MapReduce yarn  http://hadoop.apache org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html  M Schw arzk opf et al  Omega exible scalable schedulers for large compute clusters in Proc of EuroSys  2013  B Hindman et al  Mesos a platform for ne-grained resource sharing in the data center in Proc of USENIX NSDI  2011  Apache Hadoop on demand  http://hadoop.apache.or g/docs/stable hod scheduler.html 59 


608 


  11 that it will be able to meet all of the Van Allen Probes communications goals with its intended ground segments A CKNOWLEDGEMENTS  This work was performed with the support of the Radiation Belt Storm Probes mission under NASA\222s Living with a Star program. The authors would like to thank Rick Fitzgerald and Kim Cooper, Van Allen Probes project managers at JHU/APL for supporting this work. There are many at JHU/APL who contributed to the development and verification of the RF system. Significant technical contributions were made by: Christopher Haskins, Bob Wallis, Matthew Angert, Laurel Funk, Joe Sheehi, Wesley Millard, Norman Adams, Lloyd Ellis, Sheng Cheng, John Daniels, Phillip Huang, Avi Sharma, Carl Herrmann, David Jones, Brian Bubnash, Melanie Bell, Horace Malcom Michael Pavlick, Mark Bernacik, Christopher Deboy, Bob Bokulic, Sharon Ling, Albert Hong, Erik Hohlfeld, Judy Bitman, William Dove and Tony Garcia. Significant contributions were also made by the USN and TDRSS compatibility test teams  R EFERENCES  1 eck D. G.; Mau k  B  H.; Greb o w sk y  J  M.; Fo x  N J, \223The Living With a Star Radiation Belt Storm Probes Mission and Related Missions of Opportunity 224 American Geophysical Union, Fall Meeting 2006   h o rs k i y  A Y., Mauk B. H., Fox N. J Sibeck D G., Grebowsky, J. M., \223Radiation belt storm probes Resolving fundamental physics with practical consequences,\224 Journal of Atmospheric and SolarTerrestrial Physics Vol. 73, Issues 11-12, July 2011 Pages 1417-1424   S. Bu s h m a n M. Bu tler, R C o n d e, K. Fretz, C  Herrmann, A. Hill, R. Maurer, R. Nichols, G. Ottman M. Reid, G. Rogers, D. Srinivasan, J. Troll, B. Williams 223Radiation Belt Storm Probe Spacecraft and Impact of Environment on Spacecraft Design,\224 Proceedings of the 2012 IEEE Aerospace Conference, Big Sky Montana USA, March 3-10, 2012   opelan d D.J DeB o y C  C R o y s ter, D.W., Dov e  W.C., Srinivasan, D.K,. Bruzzi, J.R., Garcia, A., "The APL 18.3m station upgrade and its application to lunar missions," Aerospace Conference, 2010 IEEE , vol., no pp.1-10, 6-13 March 2010    Figure 10. FER/BER performance for all downlink modes for RF GSE, SCF, USN, and TDRSS 


  12  iv as a n D. K., A r ti s  D  A Bak er, R  B., Stil w e ll, R   K., Wallis, R. E., \223RF Communications Subsystem for the Radiation Belt Storm Probes,\224  Acta Astronautica vol 65, issue 11-12, December 2009, Pages 1639-1649   k i n s  C B., Mi llard, W P 223 M u l t i Ban d  So f t w a re Defined Radio for Spaceborne Communications Navigation, Radio Science, and Sensors,\224 2010 IEEE Aerospace Conference, March 2010  k i n s  C B., Mi llard, W P A d a m s  N. H Sri n i v a s a n  D. K., Angert, M. P., \223The Frontier Software-Defined Radio: Mission-Enabling, Multi-Band, Low-Power Performance,\224 61st  International Astronautical Congress IAC-10.B2.5.11, October 2011 8  Crowne, M.J.,  Haskins, C. B., Wallis, R. E.,  Royster D.W, \223Demonstrating TRL-6 on the JHU/APL Frontier Radio for the Radiation Belt Storm Probe mission,\224 2011 IEEE Aerospace Conference, March 2011  o ckw ood, M. K K i n n i s o n  J., F o x  N C o n d e, R  Driesman, A., \223Solar Probe Plus Mission Definition,\224 63rd  International Astronautical Congress, IAC 12.A3.5.2, October 2012   i t m a n J  223An I n D ept h  L o o k at t h e R a dio Freq u e n c y    Ground Support Equipment for the Radiation Belt Storm  Probes Mission,\223 IEEE Autotestcon, 2011, September 2011  d a m s  N.H., Bi t m a n J C opela n d D. J Sri n ivas a n  D  K.,  Garcia. A., \223RF Interference at Ground Stations Located in Populated Areas,\224 2013 IEEE Aerospace Conference, March 2013  B IOGRAPHY  Matthew J. Crowne is a member of the Senior Professional Staff of the RF Engineering group in JHU/APL\222s Space Department. He received his B.S from Johns Hopkins University in 2000 and his M.S. from the same university in 2009, both in electrical engineering Matthew joined JHU/APL in 2007 where he has been working on the development of radios for spaceflight communications systems. Prior to joining JHU/APL, he worked for Integrated Defense Systems Inc., where he developed solid state power amplifiers for electronic warfare and communication systems. Matthew was the integration and test lead for the Van Allen Probes RF communication system and is currently working on the Solar Probe Plus mission   Dipak K. Srinivasan is the supervisor of the RF Systems Engineering Section in the JHU/APL Space Department. He received his B.S. and M.Eng. in electrical engineering in 1999 and 2000 in electrical engineering from Cornell University, and an M.S. in applied physics from The Johns Hopkins University in 2003. Dipak joined the APL Space Department in 2000, where he has served as the lead RF Integration and Test Engineer for the CONTOUR and MESSENGER spacecraft and lead mission system verification engineer for the New Horizons project. He is currently the Lead RF Telecommunications Systems Engineer for the MESSENGER and Van Allen Probes missions and chairs technical sessions at the annual International Astronautical Congress  Darryl W. Royster is a member of the Senior Professional Staff in the RF Engineering Group at JHU/APL.  He led compatibility testing for the Van Allen Probes, STEREO, and MESSENGER missions.  Previously he was the System Engineer for the Satellite Communications Facility at JHU/ APL and the lead RF Integration and Test Engineer for the STEREO spacecraft.  Prior to joining the JHI/APL Space Department in 2001, Mr. Royster designed cellular and land mobile radio products for Ericsson, GE and Motorola.  He received his B.S. and M.S. in electrical engineering from Virginia Polytechnic Institute and State University in 1982 and 1984, respectively  Gregory L. Weaver joined the Senior Professional Staff of JHU/APL in 2003 and works within the RF Engineering Group of the Space Department.  He is a technologist with extensive background in the technical and business aspects of the frequency control industry and has held positions as a senior design engineer, technical manager and marketing strategist over a 25 year career history, including vice president positions with Bliley Technologies Inc. and the former Piezo Crystal Company. He received his M.S in Technology Management from the University of Pennsylvania in 1993 and his B.S. in Physics from Dickinson College in 1982.  He is a licensed professional engineer in the state of Pennsylvania, member of the IEEE and the UFFC Societ y.  He has contributed to the technical proceedings of the IEEE International Frequency Control Symposium, Precise Time and Time Interval Systems and Application Meeting and the European Frequency and Time Forum   


  13 Daniel Matlin is an Associate Professional Staff at JHU/APL and a member of the RF engineering group in the Space department.  He went through a dual Bachelors/Masters program at Johns Hopkins University graduating with his Bachelor of Science in Electrical Engineering in 2008 and his Masters of Science in Engineering from the Electrical Engineering department in 2009.  As a student he specialized in RF systems design.  Mr. Matlin started at the JHU/APL in February of 2010 and in his short time with the lab has been privileged to work on various tasks supporting the RBSP program, including supporting a successful launch and early operations.  Mr. Matlin assisted in the qualification testing for the flight DSP slices as well as the integrated flight transceivers.  He also carried out electrical testing and flight qualification of the newly designed Hypertronics stacking connectors as well as components and cables used for the RF subsystem  Nelli Mosavi is an EMC and RF Engineer in the JHU/APL Space Department, RF Systems Engineering section. She received a B.S. degree in Electrical Engineering from Oakland University Michigan in 2004 and an M.S. in Electrical Engineering from The Johns Hopkins University in 2010. She is currently working toward her Ph.D. at the University of Maryland Baltimore County. She joined APL in 2009 and has since been working on RF and EME issues on the Van Allen Probes mission. Nelli previously worked for SENTEL Corporation, General Motors, DENSO International, and Molex Automotive   


APPENDIX 3 RESULTS \(SEM I-PROFESSIONAL DSLRS     Run by TFDEA add-in ver 2.1 Frontier Type Orientation 2nd Goal Return to Scale Avg RoC Frontier Year MAD Dynamic OO Max CRS 1.124802 2008 1.394531 Input\(s Output\(s SOA products at Release SOA products on Frontier RoC contributors Release before forecast Release after forecast 22166527 DMU Name Date Efficiency_R Efficiency_F Effective Date Rate of Change Forecasted Date 1 Nikon D100 2002 1 1.66666667 2007.000000 1.107566 2 Olympus E1 2003 1 1.666666667 2007.000000 1.136219 3 Pentax *ist D 2003 1 1.358024691 2007.000000 1.079511 4 Nikon D20 0 2005 1 1.2 2007.000000 1.095445 5 Canon EOS 5D 2005 1 1.664796311 2007.730028 1.205269 6 Pentax K10D 2006 1 1 2006.000000  7Nikon D30 0 2007 1 1 2007.000000  8 Olympus E3 2007 1 1 2007.000000  9 Sony Alpha DSLR A70 0 2007 1 1 2007.000000  1 0 Nikon D70 0 2008 1.46 1.46 2007.000000  11 Canon EOS 5D Mark II 2008 1.065464119 1.065464119 2008.000000  12 Sony Alpha DSLR A90 0 2008 1 1 2008.000000  13 Olympus E3 0 2008 1.02 1.02 2007.000000  1 4 Pentax K20D 2008 1 1 2008.000000  15 Nikon D300s 2009 1.142857143 0.874450785 2007.000000  2008.140742 16 Canon EOS 7D 2009 1 0.754166667 2007.000000  2009.399022 17 Sony Alpha DSLR A85 0 2009 1 0.774820627 2008.000000  2010.169290 18 Pentax K-7 2009 1 0.772738276 2006.503130  2008.695302 19 Olympus E5 201 0 1.466133763 1.173333333 2007.000000   2 0 Pentax K-5 201 0 1.009024674 0.776190476 2007.000000  2009.15427 0 21 Nikon D80 0 2012 1 0.686950618 2008.000000  2011.192776 22 Canon EOS 5D Mark III 2012 1.115010291 0.930769231 2007.502755  2008.112786 23 Pentax K-5 II 2012 1 0.632075669 2006.839705  2010.740375 24 Sony Alpha SLT A99 2012 1.009662059 0.854117647 2007.640496  2008.981286 Results 2129 2013 Proceedings of PICMET '13: Technology Management for Emerging Technologies 


