Meticulous Classification Using Support Vector Machine for Brain Images Retrieval  Weijuan Li, Zhentai Lu, Qianjin Feng, Wufan Chen School of Biomedical Engineering, Southern Medical University, Guangzhou Guangdong, China   ABSTRACT  The objective of medical image retrieval system is to provide a tool for radiologists to retrieve the images similar to query image in content. Classification is an important part in retrieval system. This paper proposed a meticulous classification of MR-brain images using support vector machine \(SVM shape feature to express images, and then applied statistical association rule miner \(StARMiner to compute weight coefficient of each feature. A classifier based on SVM was trained, the parameters of which were optimized via many experiments. The result of glancing classification could achieve 92.10%. Meticulous classification can be applied in special body part retrieval system for retrieving more accurate images and reducing computational load   1. INTRODUCTION The development of digital medical imaging equipment has changed the way of diagnosis and treatment over the past decades. Nowadays, a great increasing number of digital medical images are produced in hospitals and medical research centers. Many hospitals purchase Picture Archiving and Communication Systems \(PACS manage the huge medical images. Medical images not only present the important anatomical information and the function of body tissue, but also the characteristic of health or illness in a visual way. They are of great value for diagnosis, therapy, research, and education. In clinical diagnosis process, it is beneficial and significant to find the existing images with the same modality, the same anatomical organization and the similar visual 


characteristics of disease area as the query image. Clinical decision support system and computer-aided diagnostics are on the rise and create a need of powerful data management and retrieval. Presently, PACS can only offer a text-based retrieval. Therefore, Content-based medical image retrieval \(CBMIR medical community for the inclusion into various applications [1 Categorization is an important step in large image retrieval systems. It can reduce the computational load and improve the retrieval accuracy. System of image retrieval in medical applications \(IRMA  images according to anatomical areas, modalities, view points and functional systems. The system of IRMA needs to manually determine a four-axis code. Hayit Greenspan in [3] present an automatic categorization method for X-ray images, using the Gaussian mixture modeling GMM KL which is to pre-label image categories according to body parts and view points, such as hand, skull, skull side view In this paper we tentatively put forward a meticulous classification for axial brain images in typical MR-T1 scanning modalities. The brain images have been defined into 14 categories according to the difference of anatomical structure and content of images in a sequence of axial brain images. Moreover, we have discussed how to apply the meticulous classification into our brain retrieval system so as to improve the retrieval accuracy and reduce the computational load 2. METHOD The program of meticulous classification experiment is mainly divided into three steps: feature extraction, feature optimization and classifier design using SVM 2.1. Feature Extraction The first step is to extract the features of image data Before feature extraction, images are filtered by a smoothing filter and the backgrounds of them are removed Then, firstly in the process of feature extraction, we use 3-scale SWT to decompose images into 12 sub-images The value of mean, variance, energy, consistency, contrast and correlation of each sub-image are computed. Secondly 


Fourier descriptor proposed by Zhang [4] is applied to get the shape features: the centric distance of each edge pixel is calculated; the gray value of edge pixel in the preprocessed image serves as a weight coefficient of centric distance of this edge pixel; then do discrete Fourier transform towards the centric distance of edge pixel. We selected the foremost eight Fourier coefficients as the shape features 2010 International Conference of Medical Image Analysis and Clinical Application \(MIACA 978-1-4244-8012-8/10/$26.00 2010 IEEE 99 2.2.  Feature Optimization Once the primitive features of images are extracted, the next step is to optimize them. In this process, there are two steps 1  f f m ki j i j j 1 k is an integer, here, we chose the maximum k that normalizes all features to [-0.5, 0.5 2  applied to compute weight for each feature, which aims at identifying the most relevant features and giving them large weights from each image in the original literature We use this method to weigh features as follows T is the set of all training images. Tx is the set of images from x-th class. f j is the j-th feature in the feature vector   variance in the image set Tx. Three thresholds must be given in this algorithm: \(1 between the mean of f j in the set of x-th class and the mean of f j in the set of non x-th class. \(2 variance of f j in the set of x-th class. \(3 minimal confidence level that refuse H0 min max min If the x-th class and f j satisfy the following three conditions, we say that there is an associate rule between x-th class and f j, marked it as x f j   2  3 


 0 4 x f j? means that f j distinguish well the x-th class from other classes. The feature that generates the most associate rules for classes is the most discriminative. So the weight of f j can be defined as 10w rj j q= +                    \(5 rj is the number of associate rules for f j. When q=0, it means removal of the feature that dont generate any rules when q=1, it means reserving all features 2.3. SVM for Classification SVM is a supervised learning method based on the statistical learning theory and the Vapnik-Chervonenkis dimension [6]. For a set of separable two-class objects, an SVM finds the unique separating hyperplane which has the maximum margin \(denoted with 2 / w space. The hyperplane H1 and hyperplane H2 respectively define the border of positive class objects and the border of negative class objects. Those objects that support the hyperplane H1 and hyperplane H2 are called support vectors. If there exists no hyperplane that can split the positive class objects and negative class objects, a soft margin will decide a hyperplane that splits the positive class objects and the negative class objects as clearly as possible, and while maximizing the distance to the nearest clearly split objects. The soft margin method introduces a slack variable i? , which measure the degree of misclassification of the object xi  6 The optimization becomes a trade off between a large margin and a small error penalty. The optimization problem transforms to Min  2 C ii 7 s.t  8 C is a parameter which trades classifier generality for accuracy on the training set, and input space into a high-dimensional feature space The dual of the SVM can be shown to be the following optimization problem 


Max   9 s.t. , 0 , 10 1 n yi ii Ci?? ? i n? ?   \(10 This problem has a unique solution. Given ?? is the solution of above problem, so  1 xi n w yi ii 11 We can solute b* through any i?? . The final decision function is  1  n y K x x bi i i i f x 12 K\(x, xi dot product  Some common kernels include: Linear kernel Polynomial kernel, Radial Basis Function, Hyperbolic tangent kernel Multi-class problem is usually done by reduced into multiple binary class problems. In this paper one-against-one method applies to solve this multi-class problem according to [7 3. EXPERIMENTS & DISCUSSION We collect 784 axial brain images in typical T1-MR scanning modalities from 226 patients. These images take Frankfort horizontal plane as reference plane, and are all from medical imaging department of affiliated hospital of tianjin medical university. Images are finely defined into 14 categories. The difference of vertical brain position of 100 two adjacent categories is about 1.0~2.0cm. Every 


category images has different anatomical structure, tissue shape and content from other category in a same sequence of axial brain images. A representative image of each category is shown in Fig.1. The dataset is divided into training set and test set. Each class in training set contains 15 images, and the number of testing images of each class is shown in Table 1. The images from every category are all from different patients, so as to avoid poor training caused by that the images from the same patient have a certain degree similarity According to feature extraction method previously mentioned, we selected the foremost eight Fourier coefficients as the shape features. So each image is expressed by a 80-dimensional feature vector. In the step of Gaussian normalization, we set three different value of k, 3 ,10 ,15? ?? ? ??  in order to verify the impact of values range of features on the classifier when feature dimension is higher. In StARMiner algorithm, three thresholds must be given complying with the principle that the most discriminative feature generates 14 associate rules and the least generates zero associate rules. Through a lot of experiments, we set , and set q=1 reserving all features 0.01, 0.15, 0.95maxmin minu We respectively take the polynomial, radial basis function, hyperbolic tangent function as kernel function in SVM classifier to get comparison results. What more, a comparison between a classifier using K- nearest neighbor KNN Table 2 shows the detailed classification result of the best classifier Accurate classification rate is defined as the percentage of images classified to i and rough classification rate of i-th class as the percentage of images classified to i-1, i or i +1 in testing i-th class. Specially, the rough classification of 1-th class is defined as the percentage of images classified to 1-th class or 2-th class the rough classification of 14-th class is defined as the percentage of images classified to 13-th class or 14-th class Experiment shows the range of new features after 


Gaussian normalization by 3 ,10 ,15? ?? ? ??  are respectively [-2.0196, 1.3144], [-0.4876, 0.4116 0.3572, 0.2744], and the corresponding accurate classification rates are respectively 0.17%, 62.05 59.46%. So, we let k is 10 in Gaussian normalization. The comparison between different kernel function used in SVM classifier shows that radial basis function is observably better than others. When in radial basis function, the classifier is optimal 2 1 Because distinguish between images from two adjacent classes is a little blurry and illness that appears in one class images may also appears in its two adjacent class images. Table 2 shows that images of each class in testing set are almost classified to its true class or its four most adjacent classes. The average of accurate classification rate of the SVM classifier with radial basis function as kernel is 62.05% and the average of rough classification rate 92.10 We apply this classifier to automatically classify images in our brain image database. In the process of retrieval, the images in database, whose class label are in the scope of [i-3,i+3], are selected to compare with the query image which is supposed to have been classified into i-th class. Applying this classifier in our system can effectively prevent retrieving images which are not similar to the query image on anatomical structure and content from the database   Fig.1. Representative images of each category  Table 1:  A Comparison of Classification Using KNN and SVM 12  101 Table 2: Detailed Result of Classification Using SVM with RBF kernel 4. CONCLUSION The main motivation of this work is to prevent retrieving the images not similar to the query image on anatomical structure and content in the database. Through a machine learning method, we hope to achieve better precision in 


retrieval and what is more, reduce the space of retrieval, so as to improve efficiency. In this paper, we put forward a meticulous classifier and plan to use it in our brain image retrieval system to achieve our aforementioned intent Meticulous classification will be extended to other body parts if it succeeds in brain image retrieval system. These are our future work  REFERENCES 1] Mller, H., Michoux, N., Bandon, D.,Geissbuhler, A., A review of content-based image retrieval systems in medical applicationsclinical benefits and future directions International Journal of Medical Informatics, 2004;73:1-23 2] Lehmann, T.M, Schubert, H., Keysers, D., Kohnen, M Wein, B.B, The IRMA Code for Unique Classification of Medical Images, Medical Imaging 2003: PACS and Integrated Medical Information Systems: Design and Evaluation Proceedings of SPIE Vol.5033,440-451 3] Greenspan, H., Pinhas, A.T., Medical Image Categorization and Retrieval for PACS Using the GMM-KL Framework, IEEE Transactions on Information Technology in Biomedicine, 2007; Vol. 11, No. 2 4] Gang Zhang, Z.M. Ma, Qiang Tong, Ying He, Tienan Zhao Shape Feature Extraction Using Fourier Descriptors with Brightness in Content-based Medical Image Retrieval International Conference on Intelligent Information Hiding and Multimedia Signal Proceeding, 15-17 Aug. 2008;71-74 5] Ribeiro, M X., A G R. Balan, J C. Felipe, A J M. Traina, C Traina Jr., Mining Statistical Association Rules to Select the Most Relevant Medical Image Feature, First International Workshop in Mining Complex Data \(in conjunction with The Fifth IEEE International Conference on Data Mining Houston, Texas, USA 6] Ying Tan and Jun Wang, A support vector machine with a hybrid kernel and minimal Vapnik-Chervonenkis dimension IEEE Transactions on Knowledge and Data Engineering, Apr 2004; Vol. 16, No. 4: 385-395 7] Chih-Wei Hsu, Chih-Jen Lin, A Comparison of Methods for Multi-class Support Vector Machine, IEEE Transactions on Neural Networks, Mar.2002; Vol. 13, No. 2, pp: 415-425  102 


11*]and[22*] 100100 2/6 11*]and[2**] 100100 2/6 11*]and[121] 001100 2/6 12*]and[111] 000100 1/6 12*]and[112] 001000 1/6 12*]and[22*] 000100 1/6 12*]and[3**]  010000 1/6 111]and[2**]  100100 2/6 566 111]and[22*] 100100 2/6 111]and[121] 000100 1/6 112]and[121] 001000 1/6 121]and[22*] 000100 1/6 121]and[3**] 010000 1/6 TABLE V.  TABLE TYPE STYLES The Combination of Items The Expression of Binary Information Granules Support 11*]and[12*]and[22*] 000100 1/6 11*]and[121]and[22*] 000100 1/6 12*]and[111]and[22*]  000100 1/6 12*]and[111]and[2**] 000100 1/6 121]and[111]and[22*] 000100 1/6 121]and[111]and[2**] 000100 1/6 The multiple minimum support of each item is compared with the support assembled by various items shown in the table 4, then we can get the sets of the table 5. From the results, we can get the association rules of the same layer, such as [11 and [12 *] and [22 *], and they all satisfy the minimum support MIS \([11 *] and [12 *] and [ 22  12 22 rules of different layers, such as [12 *] and [111] and [2 they also satisfy the minimum support MIS \([12 *] and [111 and [2  111 2 6, so that we achieve the multi-level association rules Different rules support of different data items produced need to meet different multiple minimum support in order to find these rules, such as the support of [12 *] and [2 **] is sup 12 *] and [2 011100 ? 100101 multiple minimum supports MIS \([12 *] and [2 


MIS \([MIS \([12 2 12 *] and 2 frequency down-generating search space of frequent itemsets V. CONCLUSIONS This paper presents a multi-level association rule mining method based on binary information granules operations and multiple minimum support constraint, with hierarchical encoding and binary granular computing of information granules operation to acquire frequent itemsets at intra level as well as inter level. We give a new definition of support and confidence based on binary information granules. And combined with the definition of multiple minimum supports we effectively restrained the generation search space of frequent itemsets, and found new rules implied in the scarce data items, achieved association rule mining of multi-layer including cross-layer. We got more meaningful rules, and avoided the generated useless rules from the high frequency data items. At last, the method is applied to mining agricultural information association rules, which has been proven to be effective and practical REFERENCES  1] Agrawal R, Imielinski T, Swami A. Mining association rules between sets of items in large databases[C] // Proceedings of the 1993 ACM SIGMOD. Washington:ACM SIGMOD, 1993: 207-216 2] Lin Q.Granular Language and Its Deductive Reasoning.[J Communications of ACM,2002,5\(2 3] Xu Jianfeng, Liu Lan, Qiu Taorong, Hu Ran. On Data Ming Algorithms Based on Binary Numeral Granular Computing. [J].Compter Science 2008,35\(3 4] Liu Qing,Jiang S L.Reasoning about Information Granules Based on Rough Logic.In:RSCTC 2002,L NA I 2475,2002.139?143 5] Ming-Cheng Tseng,Wen-Yang Lin.Efficient mining of generalized association rules with non-uniform minimum support.[J].Data knowledge engineering,62\(2007  567 


of the proposed approach are described. A simulation dataset with 64 items and 10000 transactions were used in the experiments. The dataset followed the exponential distribution. The initial population size P is set at 50, the archive size is set at 30, the crossover rate pc is set at 0.8, and the mutation rate pm is set at 0.001. The parameter d of the crossover operator is set at 0.35 according to Herrera et al.s paper [14] and the set of minimum support values is {3 4%, , 13%}. The experiments were first made for demonstrating the evolution of the Pareto fronts by the proposed approach. The evolution of the Pareto fronts of chromosomes in the archive along with different generations by the proposed approach is shown in Fig. 1 From Fig. 1, we can observe that the solutions were distributed on the Pareto fronts and the final solutions after 500 generations were better than those in different generations. Besides, we can also found that the derived solutions on a Pareto front are trade-offs between the two objectives. It thus depends on the user preference to decide which solutions on a Pareto front are desired. The experiment was then made for comparing the final Pareto front of chromosomes in the archive of the proposed approach with the previous approach [2], and is shown in Fig. 2  250 300 350 400 450 500 550 600 60 70 80 90 100 110 120 130 140 Suitability To tal N um be r o f L 1 


Generation = 0 Generation = 100 Generation = 200 Generation = 300 Generation = 400 Generation = 500  Fig. 1. The Pareto fronts derived by the proposed approach with different generations 450 500 550 600 65 70 75 80 85 90 95 Suitability To tal N um be r o f L 1 The Proposed Approach The Previous Approach  Fig. 2. Comparison results of final Pareto fronts between the proposed approach and the previous approach  From Fig. 2, it is easily to know that the Pareto front derived by using the proposed approach is better than the previous one.  From the experimental results, we thus can conclude that the proposed approach is not only effective in finding an appropriate set of solutions, but also can provide different options to users for further analysis VI. CONCLUSIONS AND FUTURE WORKS The SPEA2 adopted a fine-grained fitness assignment strategy, a density estimation technique, and an enhanced archive truncation method to derive better Pareto solutions 25]. In this paper, we have utilized it to propose a more sophisticated multi-objective approach to find the appropriate sets of membership functions for fuzzy data mining. Two objective functions are used to find the Pareto front. They are minimizing the suitability of membership functions and maximizing the total number of large 1-itemsets respectively Experiments on a simulation dataset were also made to 


show the effectiveness of the proposed approach. The results show that the proposed approach is effective in finding an appropriate set of solutions. Further, the experiments also show that the proposed approach can derive better Pareto front than the previous one [2]. In the future, we will continuously enhance the multi-objective genetic-fuzzy approach for more complex problems REFERENCES 1] C. C. Chan and W. H. Au, Mining fuzzy association rules, The Conference on Information and Knowledge Management, Las Vegas pp. 209-215, 1997 2] C. H. Chen, T. P. Hong, Vincent S. Tseng and L. C. Chen, A multi-objective genetic-fuzzy mining algorithm, The 2008 IEEE International Conference on Granular Computing, 2008 3] C. H. Chen, T. P. Hong, Vincent S. Tseng and C. S. Lee, A genetic-fuzzy mining approach for items with multiple minimum supports, Soft Computing, Vol. 13, No. 5, pp. 521-533, 2009 4] C. H. Chen, Vincent S. Tseng and T. P. Hong, Cluster-based evaluation in fuzzy-genetic data mining, IEEE Transactions on Fuzzy Systems, Vol. 16, No. 1, pp. 249-262, 2008 5] O. Cordn, F. Herrera, and P. Villar, Generating the knowledge base of a fuzzy rule-based system by the genetic learning of the data base IEEE Transactions on Fuzzy Systems, Vol. 9, No. 4, pp. 667674, 2001 6] C. A. Coello, D. A. Van Veldhuizen and G. B. Lamont, Evolutionary Algorithms for Solving Multi-objective Problems, Kluwer Academic Publishers, 2002    7] K. Deb, Multi-objective Optimization Using Evolutionary Algorithms John Wiley & Sons, 2001 8] K. Deb, S. Agrawal, A. Pratab and T. Meyarivan, A fast and elitist multiobjective genetic algorithm: NSGA-II, IEEE Transactions on Evolutionary Computation, Vol. 6, No. 2, pp. 681-695 9] C. M. Fonseca and P. J. Fleming, "Genetic algorithms for multiobjective optimization: Formulation, discussion and generalization," The International Confidence on Genetic Algorithms pp. 416-423, 1993 10] T. P. Hong, C. H. Chen, Y. L. Wu and Y. C. Lee, " Genetic-Fuzzy Data Mining with Divide-and-Conquer Strategy", IEEE Transactions on Evolutionary Computation, Vol. 12, No. 2, pp. 252-265, 2008 11] T. P. Hong, C. H. Chen, Y. L. Wu and Y. C. Lee, "A GA-based fuzzy 


mining approach to achieve a trade-off between number of rules and suitability of membership functions", Soft Computing, Vol. 10, No. 11 pp. 1091-1101. 2006 12] T. P. Hong, C. S. Kuo and S. C. Chi, "Mining association rules from quantitative data," Intelligent Data Analysis, Vol. 3, No. 5, pp 363-376, 1999 13] T. P. Hong, C. S. Kuo and S. C. Chi, "Trade-off between time complexity and number of rules for fuzzy mining from quantitative data," International Journal of Uncertainty, Fuzziness and Knowledge-based Systems, Vol. 9, No. 5, pp. 587-604, 2001 14] F. Herrera, M. Lozano and J. L. Verdegay, Fuzzy connectives based crossover operators to model genetic algorithms population diversity Fuzzy Sets and Systems, Vol. 92, No. 1, pp. 2130, 1997 15] M. Kaya and R. Alhajj, A clustering algorithm with genetically optimized membership functions for fuzzy association rules mining The IEEE International Conference on Fuzzy Systems, pp. 881-886 2003 16] M. Kaya and R. Alhaji, Utilizing genetic algorithms to optimize membership functions for fuzzy weighted association rules mining Applied Intelligence, Vol. 24 ,  No 1, pp. 7-15, 2006 17] M. Kaya and R. Alhajj, Integrating multi-objective genetic algorithms into clustering for fuzzy association rules mining, The IEEE International Conference on Data Mining, pp. 431-434, 2004 18] M. Kaya, Multi-objective genetic algorithm based approaches for mining optimized fuzzy association rules, Soft computing, Vol. 10 pp. 578-586, 2006 19] C. Kuok, A. Fu and M. Wong, Mining fuzzy association rules in databases, SIGMOD Record, Vol. 27, No. 1, pp. 41-46, 1998 20] Y. C. Lee, T. P. Hong and W. Y. Lin, Mining fuzzy association rules with multiple minimum supports using maximum constraints, Lecture Notes in Computer Science, Vol. 3214, pp. 1283-1290, 2004 21] H. Roubos and M. Setnes, Compact and transparent fuzzy models and classifiers through iterative complexity reduction, IEEE Transactions on Fuzzy Systems, Vol. 9, No. 4, pp. 516-524, 2001 22] J. D. Schaffer, Multiple objective optimization with vector evaluated genetic algorithms, The International Conference on Genetic Algorithms, pp. 93-100, 1985 23] C. H. Wang, T. P. Hong and S. S. Tseng, Integrating membership functions and fuzzy rule sets from multiple knowledge sources, Fuzzy Sets and Systems, Vol. 112, pp. 141-154, 2000 24] S. Yue, E. Tsang, D. Yeung and D. Shi, Mining fuzzy association rules with weighted items, The IEEE International Conference on Systems 


Man and Cybernetics, pp. 1906-1911, 2000 25] E. Zitzler, M. Laumanns and L. Thiele, "SPEA2: Improving the strength Pareto evolutionary algorithm for multiobjective optimization," Proc. Evolutionary Methods for Design, Optimization and Control with App. to Industrial Problems \(Barcelona, Spain, 2001 pp. 95-100 


9] Y. Gong, S. Mabu, C. Chen, Y. Wang, and K. Hirasawa, "Intrusion detection system combining misuse detection and anomaly detection using genetic network programming," in Proc. of the SICE-ICASE International Joint Conference, 2009, pp. 3463-3467 10] "Kddcupl999 data. " [Online]. Available: kdd.ics.uci.eduldatabases kddcup99/kddcup99.htrn1 11] R. P. Lippmann, D. J. Fried, I. Graf, J. Haines, K. P. Kendall, D. Mc Clung, D. Weber, S. Webster, D. Wyschogrod, R. K. Cunningham and M. A. Zissman, "Evaluating intrusion detection systems: The 1998 darpa offline intrusion detection evaluation," in Proc. of DARPA Information Survivability Conference and Exposition 2000, vol. 2 IEEE Computer Society Press, 2000 12] K. Shimada, K. Hirasawa, and J. Hu, "Class association rule mining with chi-squared test using genetic network programming," in Proc. of the IEEE International Conference on Systems, Man and Cybernetics 2006, pp. 5338-5344 


n-dimension data cube\( I1  I k Support=sup_count/total_count 2 3 4. Performance Analysis Example 2 Lets talk about a practical problem just like the status of sales. Assume that we will mine the association rules involved 4 dimension attributes of sales, the minsup=25%. First of all, using OLAP technology to build a 4-D data cube and the 4 dimension attributes are: time location, item, and supplier. For location dimension which contains area, country and so on, we choose province level We use brand level for item dimension, company level for supplier dimension. Time dimension can be divided as Q1 1-3 4-6 7-9 10-12 location\(P1,P2,L1,L2 York, item\(B1,B2,B3,B4 C1,C2,C3 sales data cube can be generalized like this Graphic 2: The 4-Dimension Data Cube of Sales The details of this sales data cube are in the table follow: The amount of cells is 100 Location Time Item Supplier Count Cell-1 P1 Q1 B1 C2 5 Cell-2 P1 Q3 B1 C1 2 Cell-99 L1 Q3 B1 C1 3 Cell-100 L2 Q4 B1 C3 11 Table 2: The details data table of sales data cube We use original Apriori_Cube Algorithm to find frequent predicate set with minsup_num= 25%*100=25 According the data table we calculate that sup_count of every member of dimension L is \(P1:8, P2:5, L1:1, L2:24 and also T, I, S. So there is the process Graphic 3: The processes of old algorithm As we know, through comparing with the minsup_num dimension location has no one frequent 1-predicate set, so that there have no frequent 4-predicate set in the output by the original algorithm. But users are interested in the Candidate 1-Predicate set L T I S P1 P2 L1 


L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Frequent 1-Predicate set T I S Q1 Q3 B2 B3 C1 C2 I1 I 2 C1              C2                 C3 2 12 1 3 5 1 6 2 12 8 11 Q 1  Q 2  Q 3  Q 4 P 1 P 2 L 1 L 2 Candidate 2-Predicate set Q1,B2},{Q1,B3 Q1,C1},{Q1,C2 Q3,B2},{Q3,B3 


Q3,C1},{Q3,C2 Candidate 3-Predicate set Q1,B2,B3}{Q 1,Q3,B2},{Q3 C1,B2 Frequent 2-Predicate set Q1,B2}{Q1,B 3},{Q3,B2 Q3,C1 Output: 1L U 2L U 3L Frequent 3-Predicate set Q1,B2,C1},{Q1,B3,B2 Q1,B2,B3  6 level from the same dimension and SP to pruning Candidate 3 Set, pruning {Q1,Q3,B2}, keep {Q1,B2,B3} due to the special requirement on location B by users 7 1 max_support of every dimension 1 frequent 4-predicate set which contains location dimension And the candidate 3-predicate set {Q1,Q3,B2} is useless because when we mine multi-dimension association rules we should follow that one predicate set contains no more than one different level from the same dimension even the sup_count >minsup_num when there are no requirements from users. But this time, users require that different members from dimension item can exist at the same time So we redo this mining process by the improved Algorithm Graphic 4: The processes of improved algorithm At last, we got the frequent 4-predicate set which the users are interested in, and then can format the rules 5. Conclusion In this paper, we have studied issues and methods on efficient mining of multi-dimension association rules based on data cube. With the development of technology, the size 


of database has become much huge than ever before unimaginable. So the multi-dimension model of database based on data cube has become useful and popular relatively how to mine useful multi-dimension association rules based on this model has been important too. Among the algorithms for the problems we choose the most classify one to improve, to make it more efficient and useful An efficient algorithm, Apriori_Cube_Improved, has been developed which explores the multi-dimension association rules. First, we use max_support and min_support of every frequent 1-predicate set to check the levels of dimensions, and then adjust the data cube by roll-up and drill-down operation immediately. This step is embed in the process of mining association rules, so it makes the rules much more useful and flexible; second applying one predicate set contains no more than one different level from the same dimension and SP pruning can help reduce the amount of predicate set, especially with the great growing of the number. So the speed of algorithm improved is faster than ever At last, there are also many other interesting issues and flaws of the algorithm which call for further study including efficient mining multi-dimension association rules of complex measures and so on. Finally, I should thank all the people who have give me help for this paper Reference 1] Jiawei Han, Micheline Kamber. Data Mining Concepts and Techniques. China Machine Press, 2007 2] Agrawal R, Imielinski T and Swami A. Mining association rules between sets of items in large database. Proc. of the ACM SIGMOD Conf., Washington DC, 1993 3] Gao Xuedong, Wang Wenxian and Wu Sen. Multidimensional Association Rule Mining Method Based on Data Cube Computer Engineering, China,2003 4] Sheng Yingying, Yan Ren, Wang Jiamin and Li Jia. Research Multi-dimensional Association Rule Ming Based on Apriori Science Technology and Engineering, China, 2009 5] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. Proc. of the 20th Int.Conf. on VLDB Santiago, Chile, 1994 6] Guozhu Dong, Jiawei Han, Joyce Lam, Jian Pei and Ke Wang Mining Multi-Dimension Constrained Gradients in Data 


Cubes. Proc. of the 27th Int.Conf. on VLDB, Roma, Italy 2001 7] M. J. Zaki. Scalable Algorithms for Association Mining. IEEE Transactions on Knowledge and Data Engineering, 2000 Candidate 1-Predicate set L T I S P1 P2 L1 L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Candidate 1-Predicate set L T I S P L Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Frequent 1-Predicate L T I S P L 


Q1 Q3 B2 B3 C1 C2 Frequent 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3 Output L1 U L2 U L3 U L4 The Dimension need to be adjust Dimension: Location Operation: Roll-up to increase the level of this dimension Result P\(P1,P2 L1,L2 2 3 2 3 The Features Of Every 1-Predicate Set Location: min_support=13 max_support=25 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 4 5 4 max_support of every dimension 5 minsup_num=25, and no one has to be adjusted Frequent 3-Predicate set Q3,C1,B2},{Q1,B2,L Q1,B3,L},{Q3,P,B2 Candidate 4-Predicate set 


Q3,C1,B2,P},{Q1,B2,B3,L}\(7 The Features of Every 1-Predicate Set Location: min_support=1 max_support=24 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 Candidate 2-Predicate set Q1,B2},{Q1,B3},{Q1,C1 Q1,C2},{Q3,B2},{Q3,B3 Q3,C1},{Q3,C2},{P,Q1 P,Q3},{P,B2},{P,B3},{P,C1 P,C2},{L,Q1},{L,Q3 L,B2},{L,B3},{L,C1},{L,C2 Frequent 2-Predicate set Q1,B2}{Q1,B3},{Q3,B2 Q3,C1},{P,Q3},{L,Q1 6 Candidate 3 -Predicate set Q3,C1,B2},{Q3,C1,P},{Q1,B 2,L},{Q1,B3,L},{Q3,P,B2},{Q 1,B2,B3  6 level from the same dimension and SP to pruning Candidate 3 Set, pruning {Q1,Q3,B2}, keep {Q1,B2,B3} due to the special requirement on location B by users 7 1 max_support of every dimension 1 frequent 4-predicate set which contains location dimension And the candidate 3-predicate set {Q1,Q3,B2} is useless because when we mine multi-dimension association rules we should follow that one predicate set contains no more than one different level from the same dimension even the sup_count >minsup_num when there are no requirements from users. But this time, users require that different 


members from dimension item can exist at the same time So we redo this mining process by the improved Algorithm Graphic 4: The processes of improved algorithm At last, we got the frequent 4-predicate set which the users are interested in, and then can format the rules 5. Conclusion In this paper, we have studied issues and methods on efficient mining of multi-dimension association rules based on data cube. With the development of technology, the size of database has become much huge than ever before unimaginable. So the multi-dimension model of database based on data cube has become useful and popular relatively how to mine useful multi-dimension association rules based on this model has been important too. Among the algorithms for the problems we choose the most classify one to improve, to make it more efficient and useful An efficient algorithm, Apriori_Cube_Improved, has been developed which explores the multi-dimension association rules. First, we use max_support and min_support of every frequent 1-predicate set to check the levels of dimensions, and then adjust the data cube by roll-up and drill-down operation immediately. This step is embed in the process of mining association rules, so it makes the rules much more useful and flexible; second applying one predicate set contains no more than one different level from the same dimension and SP pruning can help reduce the amount of predicate set, especially with the great growing of the number. So the speed of algorithm improved is faster than ever At last, there are also many other interesting issues and flaws of the algorithm which call for further study including efficient mining multi-dimension association rules of complex measures and so on. Finally, I should thank all the people who have give me help for this paper Reference 1] Jiawei Han, Micheline Kamber. Data Mining Concepts and Techniques. China Machine Press, 2007 2] Agrawal R, Imielinski T and Swami A. Mining association rules between sets of items in large database. Proc. of the ACM SIGMOD Conf., Washington DC, 1993 3] Gao Xuedong, Wang Wenxian and Wu Sen. Multidimensional Association Rule Mining Method Based on Data Cube 


Computer Engineering, China,2003 4] Sheng Yingying, Yan Ren, Wang Jiamin and Li Jia. Research Multi-dimensional Association Rule Ming Based on Apriori Science Technology and Engineering, China, 2009 5] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. Proc. of the 20th Int.Conf. on VLDB Santiago, Chile, 1994 6] Guozhu Dong, Jiawei Han, Joyce Lam, Jian Pei and Ke Wang Mining Multi-Dimension Constrained Gradients in Data Cubes. Proc. of the 27th Int.Conf. on VLDB, Roma, Italy 2001 7] M. J. Zaki. Scalable Algorithms for Association Mining. IEEE Transactions on Knowledge and Data Engineering, 2000 Candidate 1-Predicate set L T I S P1 P2 L1 L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Candidate 1-Predicate set L T I S P L Q1 Q2 Q3 Q4 B1 B2 


B3 B4 C1 C2 C3 Frequent 1-Predicate L T I S P L Q1 Q3 B2 B3 C1 C2 Frequent 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3 Output L1 U L2 U L3 U L4 The Dimension need to be adjust Dimension: Location Operation: Roll-up to increase the level of this dimension Result P\(P1,P2 L1,L2 2 3 2 3 The Features Of Every 1-Predicate Set Location: min_support=13 max_support=25 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 4 5 


4 max_support of every dimension 5 minsup_num=25, and no one has to be adjusted Frequent 3-Predicate set Q3,C1,B2},{Q1,B2,L Q1,B3,L},{Q3,P,B2 Candidate 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3,L}\(7 The Features of Every 1-Predicate Set Location: min_support=1 max_support=24 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 Candidate 2-Predicate set Q1,B2},{Q1,B3},{Q1,C1 Q1,C2},{Q3,B2},{Q3,B3 Q3,C1},{Q3,C2},{P,Q1 P,Q3},{P,B2},{P,B3},{P,C1 P,C2},{L,Q1},{L,Q3 L,B2},{L,B3},{L,C1},{L,C2 Frequent 2-Predicate set Q1,B2}{Q1,B3},{Q3,B2 Q3,C1},{P,Q3},{L,Q1 6 Candidate 3 -Predicate set Q3,C1,B2},{Q3,C1,P},{Q1,B 2,L},{Q1,B3,L},{Q3,P,B2},{Q 1,B2,B3  


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


