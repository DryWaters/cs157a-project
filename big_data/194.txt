html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">25-29 July  2004 - Budapest, Hungary MembershipMap: A Data Transformation Approach for Knowledge Discovery in Databases Hichem Frigui Department of Electrical and Computer Engineering University of Memphis Memphis, TN 38152 E-mail: hfrigui@memphis.edu Abstrad- We propose a new data transformation approach that facilitates many data mining, interpretation, and analysis tasks. Our approach, called MembershipMap, strives to extract the underlying structure or subsoncepts of each raw attribute automatically, and uses the orthogonal union of these sub concepts to define a new, semantically richer, space. The sub concept labels of each point in the original space determine the position of that point in the transformed space. Since sub-concept labels are prone to uncertainty inherent in the original data and in the initial extraction process, a combination of labeling schemes that are based on different measures of uncertainty will he presented. In particular, we introduce the CrispMap, the Fuz;yMap, and the PossibilisficMap. We outline the advantages and disadvantages of each mapping scheme, and we show that the three transformed spaces are complementary The proposed transformation is illustrated with several data sets, and we show that it can be used as a flexible pre-processing tool to support such tasks as: sampling, data cleaning, and outlier detection I. INTRODUCTION Knowledge Discovery in Databases \(KDD covering interesting and useful knowledge from large data repositories, such as patterns, trends, and associations. The KDD process consists of the following main phases[l]: . Data preparation and preprocessing: This  phase gen erally consists of cleaning of imperfect or noisy data integration of data from multiple sources, selection of relevant data, and transformation of data Data Mining: This phase consists of the application of intelligent methods to extract knowledge from data Data Mining relies on several essential tools, such as clustering, classification, and association rules. . Pattern evaluation and presentation: This  phase in volves the identification and evaluation of interesting and useful knowledge from the mined information and its presentation to the user in a suitable format The preprocessing phase, is an essential but compared to the other phases, largely ignored subject. Traditionally, it has taken a backseat to the data mining algorithms. However without adequate preparation of the data, the outcome of any data mining algorithm can he disappointing, hence the saying  garbage in garbage out   In this paper, we propose a new data transformation ap proach that facilitates many data mining, interpretation, and analysis tasks. Our approach, called MembershipMap, takes into account the intricate nature of each attribute  s distribution independently of the others and uses mixed cluster labels to create a new space. This separate treatment of the attributes allows the complete freedom to adapt different ways to com pute the concept membership functions for different attributes depending on prior knowledge and attribute type such as categorical versus numerical. The MembershipMap has several advantages including 1 complementary spaces: crisp, fuzzy, or possibilistic 2 data can be easily labeled as noise, boundary, or seeds 3 features that vary widely in scale and type 4 new records and attributes are added or removed 11. RELATED WORK A. Data Preprocessing Data preprocessing can take many forms including normal 


Data preprocessing can take many forms including normal izatiodscaling, transformatiadprojection, cleaning, and data reduction I ues to make them lie numerically in the same intervaliscale and thus have the same importance. This step is particu larly useful for distance-based methods such as clustering and nearest neighbor classification. Some data normalization techniques can be found in [2]. In general, a single linear transformation of the data to a space where all dimensions are normalized is reliable only when the data has a uni-modal distribution. If this is not the case, then simple transformations such as z-score or min-max scaling can destroy the structure shape, size, density  ghost  clusters \(when scattered data is normalized within a small interval even more difficult and less satisfactory are the presence of noise, and the presence of categorical amibutes 2 dimensional space. In general, the goals are to facilitate visualization, select a set of relevant amibutes, or map object data to an Euclidean space. Energy preserving techniques such as the Karhunen-Loeve \(KL 1147 0-7803-8353-2/04/$20.00 0 2004 IEEE FUZZ-IEEE 2004 belongingness and can handle uncertain cases. Fuzzy labels uf 0 5 \(uf points to a lower k-dimensional space using linear projections via Principal Component Analysis \(PCA large databases with too many attributes are not applicable to categorical data and may be too slow for Another class of transformations attemots to oreserve the C x \( u f 2 1-1 1-1 topology of the data by maintaining the pair-wise dissimi larities \(or their order Scaling \(MDS    s mapping [5] fall into this category. The sharpest criticism Many fuzzy partitional clustering algorithms assign this type of labels in each iteration. For instance, the Fuzzy C-Means FCM against MDS consists of their high computational and stor age requirements \( O \( N    domain experts to derive k feature extraction functions to be used to map each object into a point in k-dimensional space Faloutsos and Lin [7] generalized this approach by requiring experts to define only the objects  dissimilarity values 3 cation of outliers, irrelevant features, and identification and mending of missing data. Outlier detection is important both as a pre-requisite to later discovery tasks that cannot handle outliers gracefully, or even on its own, if outliers represent the knowledge nuggets to be discovered 4 tributes \(feature reduction data sam pling numerosity reduction through feature selection techniques [XI, [9 Data Sampling may he necessary to reduce the number of items to be mined when the data size is very large. Sampling techniques are discussed in the context of KDD in [lo]. A typical side effect of sampling when the data is distributed into clusters, is losing small clusters Attribute Numerosity Reduction is a technique that reduces the number of values taken by an attribute. It can be achieved by several techniques such as numerical attribute discretization by histogram binning B. Dura Partitioning und Labeling Clustering aims at classifying a set of N unlabeled data 


Clustering aims at classifying a set of N unlabeled data points X = { x j l j  = 1,. .. , N} into C clusters GI,. . .,Gc and assigning a label to each point to represent information about its belongingness. In general, there are three types of cluster labels depending on the uncertainty framework crisp. fizzy, and possibilisfic. Each labeling paradigm uses a different uncertainty model, and thus, have a different semantic interpretation In crisp labeling, each data point xj is assigned a binary membership value,\(u 1 if xj E Gi 0 otherwise The K-Means [ 111, K-Medoids [ 121, and other crisp partitional clustering algorithms assign this type of labels to each data sample in each iteration of the algorithm Fuzzy labeling allows for partial or gradual membership values. This type of labeling offers a richer representation of 1148 3 where dij is the distance between cluster i and point x;, and mc\(1,m Possibilistic labeling relaxes the constraint that the mem berships must sum to one. It assigns  ppicality  values ap to all clusters. As a result, if xj is a noise point, then Cgl\(up cluster, we can have CE1\(up clustering algorithms [14], [15] use this type of labeling in each iteration. For instance, the Possibilistic C-Means [I51 assigns labels using 4 where vi is a cluster-dependent resolutiodscale parameter 151. Robust statistical estimators, such M-estimators and W estimators [16] use this type of memberships to reduce the effect of noise and outliers 111. MEMBERSHIPMAPS GENERATION In this paper, we propose a data transformation approach that facilitates a variety of data interpretation, analysis, and mining tasks. Our approach maps the original feature space into a membership unit hypercube with richer information content. The proposed mapping starts by clustering each attribute independently. Each cluster, thus obtained, can be considered to form a  subspace  that reflects a  more specific   concept along that dimension. The orthogonal union of all the subspaces obtained for each attribute compose the trans formed space. The class labels \(crisp, fuzzy, or possibilistic determine the position of data points in the new space. The proposed preprocessing can be considered as a special type of  semantic mapping  that maps the data from the  raw   feature space onto a new space characterized by a semantically richer dichotomization of the original features. Our approach is outlined in Fig.1 The proposed transformation is different from other tech niques such as z-score normalization, and min-max scaling This is because it takes into account the intricate nature of each attribute  s distribution independently of the others and aggregates the mixed cluster labels to create a new space. Thus it is suitable for data sets with features that vary widely in scale and type. Moreover, our approach differs from techniques that impose a grid or a hyperbox structure on the data space. This is 25-29 July, 2004 * Budapest, Hungary Fig. I .  Illusmion of the MembenhipMap because the conceptual units in the membership space are not constrained to take any specific shape, size, or number. This information is automatically derived from the data distribution Note that the membership functions can be computed using many techniques. This task is generally trivial when performed on a univariate projection of the data \(compared to the entire data set in the original space be estimated automatically based on statistical distributions or histogram density. They can even be user-defined depending on the conceptual meaning attached to the particular attribute 


on the conceptual meaning attached to the particular attribute Most importantly, separate treatment of the attributes allows the complete freedom to adapt different ways to compute the membership functions for different attributes depending on prior knowledge and attribute type 1v. PROPERTIES OF THE MEMBERSHIP MAPS A .  n e  Crisp Map Crisp labeling assigns membership values in {O,l}. Thus the transformed vectors are binary-attribute vectors. The af vantages of this simple representation include 1 behveen two data samples can be easily assessed using their mapped vectors. This is because the number of common bits between their two corresponding binary vectors represents the number of shared concepts 2 queried easily to retrieve samples that satisfy celtain criteria. For instance  data samples that are similar from the point of view of Attribute kl and dissimilar from the point of view of Amibute k2  or  data samples that are similar in at least Nk attributes  These queries can be implemented efficiently using binary masking The limitations of the crisp Map are a direct consequence of their failure to model areas of overlap and uncertainty, as well as their inability to model outliers or noise 1 points will be mapped to the same image in the new space as points at the very core of either one of the overlapping clusters. Hence there is no way to distin guish between these points and those at the core 2 original data sample is from its cluster, it will be mapped to the same point as any point at the core of the cluster This means that points on the outskirts of a cluster are indistinguishable from points at its center B. The Fuqv and Possibilistic Maps Fuzzy and possibilistic memberships are confined to ,O, 1 Hence, the transformed vectors are continuous-valued, and may embrace more space in the MembershipMap. For the case of FuzzyMap, the constraint that the memberships along each dimension must sum to one will constrain the placement of the transformed data. The Fuzzy and Possibilistic maps offer the following advantages I Without further processing, it is possible to use the transformed space to query and retrieve data that satisfy certain properties. For example, as we will show in the next section, it is possible to identify data samples that correspond to seeds, noise, and boundary points 2 easy in this new space because all the feature vectors are normalized and each feature value has a special meaning: strength of matching between the data and one of the I-D clustersiconcepts. Moreover, fuzzy set union intersection, and other aggregation operators [ I  71 can be used to develop semantic similarity measures. This can be very helpful in case the original data has too many attributes, making the Euclidean distance unreliable v. EXPLORING THE MEMBERSHIPMAPS The MembershipMap can be mined just like the original data space, for clusters, classification models, association rules, etc. This task is expected to be easier in the membership space because all features are confined to the interval [O,l 1149 FUZZ-IEEE 2004 and have special meaning within this interval. Moreover, since different areas of the transformed spaces correspond to differ ent concepts, the MembershipMap can he explored in a pre data mining phase to uncover useful underlying information In this paper, we focus our attention in exploring the fuzzy and possibilistic Membership spaces, which we will refer to as FuzzyMap and PossibilisticMap respectively. We outline how these maps could he used to uncover seed points; noise 


how these maps could he used to uncover seed points; noise and outliers; and boundary points A. Identijfving Seed Points In applications involving huge amounts of data,"Seed points" identification may be needed to reduce the complexity of data-driven algorithms. This process can be extremely difficult, if not impossible, for data with uneven, noisy, hetero geneous distributions. In addition to sampling, seed points can offer excellent initialization for techniques that are sensitive to the initial parameters such as clustering Using the PossibilisticMap, seed points can be identified as points with high typicality, where typicality is defined as BB w I*, m Dlln i B. Identijfving Noise Points and Outliers Noise and outlier detection is a challenging problem. In fact when data has a large, unknown proportion of outliers, most learning algorithms break down and cannot yield any useful solution. Using the PossibilisticMap, Noise and outliers can be identified as those oints located of xj in the PossibilisticMap, then xj is a noise point if llxjpll is small C. Identihing Boundary Points The process of boundary points identification is paramount in many classification techniques. It can be used to reduce the training data to \(or emphasize to achieve higher focus in learning class boundaries. Using the FuzzyMap, boundary points can be identified as points that do not have a strong commitment to any cluster of a given attribute. In other words, they are points that belong to multiple units and have low punty values, where the degree of purity of xj, Pxj, is defined as words, if xp'[{u Pxj = min { m a  {uji 6 M N", ,st4 1" "km i D. Illustrative example Fig.2 displays a simple data set with 4 clusters. First, the projection of the data along each dimension is partitioned into 2 clusters using the FCM [13]. Next, possibilistic la bels were assigned to each xj in all 4 clusters using \(4 In Fig. 2\(h 1150 1 and each point xj is mapped to x,'= uIj \(1 1 2 2 a b 4 Fig. 2. Exploring the MembershipMaps. \(a b different typicality, \(c Txj 20.95 \(black squares and 0.25&lt;Txj&lt;0.50 \(squares with shade that gets lighter As can be seen, points located at the core of each cluster have the highest degree of typicality and could be treated as seeds. The degree of typicality decreases as we move away from the clusters. Points near the origin, i.e., IlxrII is small lt;0.25 Fig.Z\(b identify boundary points, fuzzy labels were assigned to each xj in all 4 clusters using eq. \(3 xf= pit c xi I Pxj &lt; 0.75 are mainly the cluster boundaries E. Discussion 1 The richness of the discovered information comes at the cost of added dimensionality. In fact, the MembershipMap can significantly increase the dimensionality of the feature space, especially if the data has a large number of clusters However, the cost of increasing the dimensionality can be offset by the significant benefits that can be reaped by putting this knowledge to use to facilitate Data Mining tasks. For example, we have shown how Fuzzy and Possibilistic Maps can easily identify seed, border, and noise points. If taken into account, these samples can be used to obtain a significantly reduced data set by sampling, noise cleaning, and removing border points to improve classicluster separation. This cleaning 


border points to improve classicluster separation. This cleaning can facilitate most subsequent supervised and unsupervised learning tasks. It is also possible, after the cleaning, to go back to the original feature space. Thus, benefiting both from lower dimensionality and lower data cardinality The cost of the high dimensional MembershipMap can also be offset by the following additional benefits: \( i values are mapped to the interval [O,l]; \(ii tributes can be mapped to numerical labels, thus, data with mixed attribute types can be mapped to numerical labels within O,l]. As a results, a larger set of DM algorithms can be investigated and \(iii interpreted and thresholded in the new space Even though the attributes are treated independently, they do get combined at a later stage. Hence, information such as correlations is not lost in the membenhip space. In fact, treat ing the attributes independently in the initial \(1-D clustering is analogous to traditional feature scaling methods such as 25-29 Ju/y, 2004 * Budapest, Hungary min-max or normalization, except that the MembershipMap approach does not destroy intricate properties of an attribute's distribution using a single global transformation. In a sense our approach applies a specialized local transformation to each from the PossibilisticMap without my knowledge about the true class labels, is higher for those samples located near the m e  class centroid subcluster along an attribute, and is thus non-linear. :~ 1.0  _ 5 .. _ Oe 0.1 VI. EXPERIMENTAL RESULTS L _ -  _ gt We illustrate the performance of the proposed transforma tion using several benchmark data sets'. These are the Iris Wisconsin Breast Cancer, Heart Disease, and the satellite data the 3 classes. The Wisconsin Breast Cancer data has 9 features O P sets. The Iris data has 4 features and 50 samples in each of 0 0  0 5  1 .o 1.5 29 D/slan- fO f r Y B  desa --an  a   and 2 classes, namely, benign and malignant. These 2 classes have 444 and 239 samples respectively. The Heart Disease data has 13 features and 2 classes. The first class, absence of heart disease, has 150 samples. and the second class, presence of heart disease, has 120 samples. The satellite data has 35 partitioned into a training set \(4,435 2,000 information extracted from the Membership maps The first step in the MembershipMap transformation con sists of a quick and rough segmentation of each feature. This is a relatively easy task that can rely on histogram thresholding _ -  _  g o a  -_ features, 6 classes, and a total of 6,435 samples. This data is L = _  I_ _ The ground truth of these sets will he used to validate the 6.4 O B  0 8  Memberohip I" erfYsl51800 b Fig. 3. Validating identified regions of interest or clustering I-D data. The results reported in this paper were obtained using the Self-Organizing Oscillators Network SOON  of finding the optimal number of clusters in an unsupervised way. We have also hied other simple clustering algorithms such as the K-Means, where the number of clusters i s  specified after inspecting the I-D projected data. The results were comparable. Next, fuzzy and possibilistic labels were assigned 


comparable. Next, fuzzy and possibilistic labels were assigned to each sample using equations \(3 4 q=1. Finally, the fuzzy \(possibilistic to create the FuzzyMap \(PossibilisticMap 15 dimensions, the Wisconsin Breast Cancer maps have 50 dimensions, the Heart Disease maps have 39 dimensions, and the Satellite maps have 107 dimensions To validate the degree of purity, we use the ground truth and compute the membership of each sample using eq.\(3 wherc the distances are computed with respect to the classes centroids. Fig. 3\(b versus their membership in the true class. The distribution shows a positive correlation indicating that the purity com puted using the FuzzyMap without any knowledge about the true class labels can estimate the degree of sharing among the multiple classes, i.e, boundary points. Moreover, we notice that all samples from class I have high purity, and the few samples with low purity belong to either class 2 or 3. This information is consistent with the known distribution of the Iris data where class 1 is well-separated while classes 2 and 3 overlap A. Identifiiing Regions of Interest B. Clustering the Membership Maps To identify seed points, noise and outliers for the Iris data This experiment illustrates the advantage of using the Mem- we compute the typicality of each  sample. Points with high bership Maps to improve clustering. We apply the Fuzzy C- typicality would correspond to seed  points, while points with Means to cluster the different data sets in the original space, low typicality would correspond  to noise points. Since the FuzzyMap, and PossibilisticMap. For all cases, we fix the Iris data has 4 dimensions, the  identified points could not be visualized as in Fig. 2. Instead, we rely on the ground truth to Of clustcrj to the actual number  Of classes, use the same random points as initial centers. validate the results. We use the class labels and compute  the centroid of each Then, we compute the distance from To assess the validity of each partition we  use the true class each point to the centroid of its class, Theoretically, points laheis to the purity Of the The  are with small distances would correspond to seeds, and points shown in Table I .  For each confusion  matrix, the column refers Fig, 3\(a  distances to a plot of the sample typicality their distance As can be seen, both maps improve the clustering  results for to the true class centroid. The distribution clearly suggests data we note here that a partition a negative Typicality, which was extracted only of the Iris data could be obtained using other  clustering algorithms and/or mnltinle clusters to renresent each class Available at "hnp: / /~~ . ics .uc i .edui  mlearn/MLRepositoryhhnP However, our goal here is to  illustrate that the Membership 1151 TABLE I CLUSTERINO RESULTS FUZZ-IEEE 2004 that is uncovered, and can even be avoided by going hack to the original feature space after data reduction and cleaning Data Set /I Originalspace 111 FutzyMap 1 1  PossMap Thus, benefiting both from lower  dimensionality and lower I 50 0 0 11 The projection and clustering steps in the MembershipMap are not restricted to the original attributes. In fact, our approach can be combined with other feature reduction techniques For instance, Principal Component Analysis can he used to Breast 422 22 Cancer 19 220 3 236 3 236 117 33 ~ ~~ ~~~ ~~~ I Disease 11 23 97 111 21 99  Eigenvectors, and then the membershir, transformation can he auulied on the Droiected TABLE II CLASSIFICATION RESULTS _ 


_ data in the same way. In fact, one can even project the data on selected 2-D or 3-D subsets of the attributes, and then apply membership transformations. I Original Space [ FuzyMap I PossMap Training I 89.29% 1 88.86% I 93.57 Testing I 85.50% I 86.30% I 87.30% ACKNOWLEDGMENTS This material  is based upon work supported by the National Science Foundation under Grant No. IIS-0133415 Maps can improve the distribution of the data so  that simple clustering algorithms are reliable. REFERENCES C. Classification using the Membership Maps This experiment uses the 36-D satellite data to illustrate the use of the Membership Maps to improve data classification Only the training data is used to learn the mapping. The test data is mapped using the same cluster parameters obtained for the training data. We train a Back Propagation Neural Networks with 36 input units, 20 hidden units, and 6 output units on the original data and two other similar nets with 107 input units, 20 hidden units, and 6 output units on the Fuzzy and possibilistic maps. Table I1 compares the classification results in the 3 spaces. As can he seen, the results on the testing data are slightly better in both transformed spaces. The above results could be improved by increasing the number of hidden nodes to compensate for the increased dimensionality in the mapped spaces. Moreover, the classifications rates could be improved if one exploits additional information that could be easily extracted from the possibilistic and fuzzy maps For instance, by using boundary points for bootstrapping and filtering noise points VII. CONCLUSIONS We have presented a new mapping that facilitates many data mining tasks. Our approach strives to extract the underlying sub-concepts of each attribute, and uses their orthogonal union to define a new space. The sub-concepts of each attribute can be easily identified using simple I-D clustering or histogram thresholding. Moreover, since fuzzy and possibilistic labeling can tolerate uncertainties and vagueness, there is no need for accurate sub-concept extraction. In addition to improving the performance of clustering and classification algorithms by taking advantage of the richer information content, the MembershipMaps could he used to formulate simple queries to extract special regions of interest, such as noise, outliers boundary, and seed points There is a natural trade-off between dimensionality and information gain. Increased dimensionality of the Member shipMap can be offset by the quality of hidden knowledge 1152 I ]  U. Fayyad, F. PiatetskqGhapiro, P. Smyth, and R. Uthurusamy. Ad wnces in Knowledge Discowr,v and Data Mining, MIT Press. 1996 Z] D. Pyle. Doto Preporrnion for Data Mining, Morgan Kaufmann 1999 131 R. N. Shepard  The analysis of proximilies: multidimensional scaling with an unknown distance function I and 11  PsychomePiko. vol. 27 pp. 125-139 219-246. 1962 4] T. Kohonen. Sel/lOqpnizotion and Associative Memoy, Springer Verlag, 1989 5] J. W. Sammon  A nonlinear mapping for data analysis  IEEE Transactions on Computms. vol. 18. pp. 401-409. 1969 6] H. V. Jagadish  A retrieval technique for similar shape  in ACM SICMOD. 1991, pp. 208-217 71 Chnstos Faloutsos and King-Ip Lin  FastMap: A fast algorithm for indexing, dala-mining and visualization of traditional and multimedia datasets  in SICMOD. 1995. 00. 163-174 SI  H. Almkllim and T. G. Dienerich  Learning with many irrelevant feamres  in Ninth National Conf AI, 1991, pp. 547-552 91 K. Kim and L. A.  Rendell  The feahrre selection problem: Traditional methds and a new algonthm  in Tenth Notional Con/ AI, 1992. pp 129-1 34 I O ]  Jyrki Kivinen and Heikki Mannilq  The power of sampling in knowledge discovety  1994. pp. 77-85 I11 R.O.Duda and P. E. Ha&amp; Patfen, Closslficnfion and Scene Analysis John Wiley and Sons. 1973 I21 L. Kaufman and P. R O U S S ~ ~ U W ,  Finding Groups in Data: An lnmducrion 


I21 L. Kaufman and P. R O U S S ~ ~ U W ,  Finding Groups in Data: An lnmducrion to Cluster Ano!vsis, John Wiley and Sons. 1990 I31 J. C. Bezdek, Pattem Recognition with Fury Objective Function Algorithms. Plenum Press, New York, 1981 1141 H.  Frigui and R. Krishapuram  A robust competitive clustering algorithm with applications in computer vision  IEEE Pons. Pan Annlwis Mach. Intell., vol. 21, no. 5. pp. 45M65, 1999 I51 R. Knshnapuram and J. Keller  A possibilistic approach lo clustering   IEEE Trans. Fuzzy Sysfems, vol. 1,  no. 2. pp. 98-110, May 1993 I61 F. R. Hampel, E. M. Roncheni, P. J. Rousseeuw. and W. A. Stahel Robust Stnti.vtics the Appmoch Based on Influence Funoions, John Wiley &amp; Sans, New York 1986 1171 2. Wang and G.  Klir. F u q  measure theo?, Plenum Press. New York 1992 I81 M.B. Rhouma and H. Frigui  Self-organization of a population of coupled oscillaton with application to clustering  IEEE Trans. Pntl Anntvsis Mach. Intell., vol. 23, no. 2. pp. 180-195, 2001 pre></body></html 


   c x n xijnx D x ij t          u    1 0 UU V where D| is the number of transactions in D, we can say that we are c confident that {i, j} isn  t frequent in D Proof: If probability policies are not considered and Sij  1, that is, while a row \(transaction according to the matrix multiplication discussed in the previous section, D'tj will be set to 0. We can view the nij original jtiD jtjtjt ijn DDD 21     f o r  e a c h  r o w  t i  c o n t a i n s   i   j   i n  D  a s  realizations of nij independent identically distributed \(i.i.d om variables ijn X X X      2 1      e a c h  w i t h  t h e  s a m e  d i s t r i b u  tion as Bernoulli distribution, B \(1 X i    1  a n d  t h e  f a i l u r e  i s  d e f i n e d  a s  X i    0     i    1  t o  n i j   T h e  p r o b  ability ? is attached to the success outcome and 1?? is attached to the failure outcome. Let X be a random variable and could be viewed as the number of transactions which include {i, j} in D such that jin     2 1    T h u s   t h e  d i s t r i b u t i o n  o f  X is the same as Binomial distribution, X?B \(nij              otherwise nx x n XP ij xnxij x ij 0 1,0     U the mean of X = nij?, and the variance of X = nij?\(1 If we want to hide {i, j} in D', we must let its support in D' be smaller than ?. Therefore, the expected value of X must be s m a l l e r  t h a n     D u V    T h a t  i s     m u s t  s a t i s f y     D n i j  u  u  V U   Moreover, the probability of the number of success which is 


Moreover, the probability of the number of success which is equal to or smaller than     1     u  D V   s h o u l d  b e  h i g h e r  t h a n  c  Therefore, U must satisfy    c x n xijnx D x ij t           u    1 0 UU V  If ? satisfies the two equations, we can say that we are c confident that {i, j} isn  t frequent in D When nij &gt; 30, the Central Limit Theorem \(C.L.T used to reduce the complexity of the equation and speed up the execution time of the sanitization process Moreover, if several entries in column j of S are equal to  1 such as Sij  1, Skj  1, Smj  1  etc, several candidate Dist ortion probabilities such as iU , kU , mU , etc are gotten. The Dis tortion probability of column j, ? j, is set to the minimal candid ate Distortion probability to guarantee that all corresponding pair-subpatterns have at least c confident to be hidden in D 3.3. Sanitization Algorithm Fig.7: Sanitization Algorithm According to the probability policies discussed above, the sanitization algorithm D'n  m = Dn  m  Sm  m is showed in Fig.7 Notice that, if the sanitization process causes all the entries in row r of D' equal to 0, randomly choose an entry from some j Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE where Drj = 1, and set D'rj = 1. It is to guarantee that the size of the sanitized database equals the size of the original database 4. Performance Evaluation 4.1. Performance Quantifying There are three potential errors in the problem. First of all some sensitive patterns are hidden unsuccessfully. Secondly some non-sensitive patterns cannot be mined from D'. And the third, new artificial patterns may be produced. However, the third condition never happens in both of our approach and SWA Besides, we also introduce the other two criteria, dissimilarity and weakness. All of the criteria are discussed as follows Criterion 1: some sensitive patterns are frequent in D'. This condition is denoted as Hiding Failure [10], and measured by     DP DP HF H H , where XPH represents the number of patterns contained in PHwhich is mined from database X Criterion 2: some non-sensitive patterns are hidden in D'. It is also denoted as Misses Cost [10], and measured by      DP DPDP MC 


MC H H H      w h e r e        X P H  r e p r e s s ents the number of patterns contained in ~PH which is mined from database X Criterion 3: the dissimilarity between the original and the sanitized database is also concerned, and it is measured by      n i m j ij n i m j ijij D DD Dis 1 1 1 1    Criterion 4: according to the previous section, Forward Inference Attack is avoided while at least one pair-subpattern of a sensitive pattern is hidden. The attack is quantified by        DPDP DPairSDPDP Weakness HH HH    where DPairS is the set of sensitive patterns whose pair subsets can be completely mined from D 4.2. Experimental Results Table 1: Experiment factor The experiment is to compare the performance between our approach and SWA which has been compare with Algo2a [4 and IGA [10] and is so far the algorithm with best performances published and presented in [12] as we know. The IBM synthetic data generator is used to generate experimental data. The dataset contains 1000 different items, with 100K transactions where the average length of each transaction is 15 items. The Apriori algorithm with minimum support = 1% is used to mine the dataset and 52964 frequent patterns are gotten Several sensitive patterns are randomly selected from the frequent patterns with length two to three items to be seeds With the several seeds, all of their supersets are included into the sensitive patterns set since any pattern which contains sensitive patterns should also be sensitive 0 0.05 0.1 0.15 0.2 0.25 0.3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern H id 


id in g F ai lu re SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern W e ak n es s SP SWA Fig.8: Rel. bet. RS and HF. Fig.9: Rel. bet. RS and weakness Fig.8, Fig.9, Fig.10, Fig.11 and Fig.12 show the effect of RS by comparing our work to SWA. Refer to Fig.8, because the level of confidence in our sanitization process takes the minim um support into account, no matter how the distribution of the sensitive patterns, we still are C confident to avoid hiding failure problems. However, there is no correlation between the disclos ure threshold in SWA and the minimum support. Under the sa me disclosure threshold, if the frequencies of the sensitive patte rns are high, the hiding failure will get high too. In Fig.9, beca use our work is to hide the sensitive patterns by decreasing the supports of the pair-subpatterns of the sensitive patterns, the val ue of weakness is related to the level of confidence. However according to the disclosure threshold of SWA, when all the pair subpatterns of the sensitive patterns have large frequencies, it may cause serious F-I Attack problems. Hiding failure and wea kness of SWA change with the distributions of sensitive patterns 0 0.1 0.2 0.3 0.4 0.5 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern M is se s C o st SP SWA 0 0.005 0.01 0.015 0.02 0.025 0.03 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern D is si m 


m il ar it y SP SWA Fig.10: Rel. bet. RS and MC.  Fig.11: Rel. bet. RS and Dis In Fig.10 and Fig.11, ideally, the misses cost and dissimilar ity increase as the RS increases in our work and SWA. However if the sensitive pattern set is composed of too many seeds, a lot of victim pair-subpatterns will be contained in Marked-Set. And as a result, cause a higher misses cost, such as x = 0.04 in Fig.10 Moreover, refer to the turning points of SWA under x = 0.0855 to 0.1006 in Fig.10 and Fig.11. The reason of violation is that the result of the experiment has strong correlation with the distri bution of the sensitive patterns. Because the sensitive patterns Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE are chosen randomly, several variant factors of the sensitive patt erns are not under control such as the frequencies of the sensiti ve patterns and the overlap between the sensitive patterns if the overlap between the sensitive patterns is high, some sensitive patterns can be hidden by removing a common item in SWA Therefore, decrease the misses cost and the dissimilarity 0 0.5 1 1.5 2 2.5 3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern E x ec u ti o n T im e h  SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set H id in g F a il u re SP SWA 


SWA Fig.12: Rel. bet. RS and time. Fig.13: Rel. bet. RL2 and HF Fig.12 shows the execution time of SWA and our approach As shown in the result, the execution time of SWA increases as RS increases. On the other hand, our approach can be separated roughly into two parts, one is to get Marked-Set which is strong dependent on the data, the other is to set sanitization matrix and execute multiplication whose execution time is dependent on the numbers of transactions and items in the database. In the experi ment, because the items and transactions are fixed, therefore, the execution time of our approach is decided by the setting of Marked-Set which makes the execution time changing slightly 0 0.05 0.1 0.15 0.2 0.25 0.3 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set W ea k n es s SP SWA 0 0.1 0.2 0.3 0.4 0.5 0.6 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set M is se s C o st SP SWA Fig.14: Rel. bet. RL2 and weakness. Fig.15: Rel. bet.RL2 and MC 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set D is si m il a ri ty SP SWA Fig.16: Rel. bet. RL2 and Dis Fig.13, Fig.14, Fig.15and Fig.16 show the effect of RL2 Refer to Fig.13 and Fig.14, our work outperforms SWA no matter what RL2 is. And our process is almost 0% hiding failure 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


