Mining Association Rules in Temporal Databases  Xinfeng Ye Department of Computer Science University of Auckland New Zealand Abstract Association rules are used to express 223interesting\224 re lationships between items of data in a standard enterprise database In a temporal database each tuple is given a start and an end time indicating the period during which the in formation recorded in the tuple is valid With a tempo ral database we may wish to discover relationships between items which satisfy certain timing constraints Existing algorithms for mining association rules cannot be applied to temporal databases directly This is because, in the existing algorithms if an itemset is supported by 
a tuple, the tuple must contain all the items in the itemset For temporal databases an itemset e.g A,B is supported as long as all the items in A,B are contained in a set of tuples which sat isfy certain timing constraint e.g the duration of the tuples containing A and B overlap each other In this paper, an al gorithm for mining association rules in temporal databases is described The algorithm allows a the itemsets to contain composite items, and \(b\the timing constraint on the tuples to be specified by the users 1 Introduction The problem of mining association rules was introduced in I Association rules can be used to express relationships between items of data An 
association rule is an expression X  Y where X and Yare sets of items X and Yare termed itemsets For a set of tuples where each tuple contains one or more items the meaning of an association rule is that the tuples which contain the items in X tend to also contain the items in Y For an itemset say X the siipporf of X denoted as s\(X is the number of tuples that contain all items in X Given a iiiiiiiiiziiiiz siipporr 6 an itemset X is large or is referred to as 
a larp iteritset if s\(X 2 6 The corzfderzce of an association s\(XUY  rule X  Y is o i.e the percentage of the tuples which contain X that also contain Y Rules are useful if their confi dence is above a nzirairiturn corzfiderzce value specified by the users An example of such a rule might be that 90 of customers who buy video players also buy video tapes Here buying This work is supported by Auckland University under grant A 
I8/XXXXX/62090/F3414079 and by ESPRIT HPCN Project No 112693 0-7803-4778-1 98 10.00 0 1998 IEEE 2803 John A Keane Department of Computation UMIST Manchester UK video players is the item in X and buying video tapes is the item in Y 90 is the confidence of the rule The problem of mining association rules is to find all rules X  Y such that XU Y is a large itemset and the confidence of X  Y is above the minimum confidence Many algorithms have been developed for mining associ 
ation rules l 2 3 4 5 61 In these algorithms if a tuple supports an itemset, the tuple must contain all the items in the itemset As explained in this requirement makes it diffi cult to discover certain useful rules in some applications In the concept of composite items is introduced A composite item consists of several items A tuple contains a composite item if the tuple contains at least one of the items in the composite item As a result the algorithm in 8 has the potential to discover rules which cannot be discovered by other algorithms However the algorithm in cannot be applied to temporal databases In a temporal database each tuple is given 
a start and an end time indicating the period during which the tuple is valid  Sometimes we might want to discover the relationships between some items or composite items recorded in the tu ples which satisfy certain timing constraints For example, in a patient database, each tuple contains a patient identifier, the disease contracted by the patient and the duration of the dis ease i.e the start and the end time of the disease Assume that we want to find out whether some diseases are likely to cause some other diseases i.e is there correlation Assume that the patient database records that a patient suffers from dis ease A and B A and B are recorded in different tuples If the duration of A 
and B overlap or intersect with each other, then A and B correlate As each tuple only records one kind of dis ease contracted by a patient, the existing algorithms cannot be used This is because in the existing algorithms if an itemset is supported by a tuple, the tuple must contain all the items in the itemset In this paper, an algorithm for mining association rules in temporal databases is described The algorithm allows \(a\the itemsets to contain composite items and b the timing con straint on the tuples to be specified by the users Mining association rules can be decomposed into two steps 1 all the tuples in the database are checked to find all large itemsets 


2 association rules are generated from these large itemsets The first step is expensive as the database needs to be scanned The second step is relatively easier 5 This paper concentrates on the first step: finding large itemsets The organisation of the paper is as follows in 2 the terminology used is defined the algorithms for finding large composite items and large itemsets in temporal databases are discussed in 53 conclusions are given in 94 2 Terminology In a temporal database each tuple has two attributes start and end which indicate the time period during which the in formation recorded in the tuple is valid A tuple might also have many other attributes It is assumed that as well as start and end each record has an attribute person ID and an attribute,,feature The person ID attribute acts as a key to distinguish the entities on whose behalf the tuples are stored in a database The value stored in attribute featiire will be used when finding the relationship between the items. That is the values of,feature are the items used for finding interesting rules Let I  U  a be the set of literals called atomic items Each atomic item is a value stored in a tuple\222s fea tiire attribute A composite item is formed by combining sev eral atomic items The general form of a composite item is cil V  Vci where a E I for 1 5 j 5 n and a  a,j for i  j A tuple contains a composite item if the tuple contains one of the atomic items which form the composite item Atomic items and composite items will be referred to as items in gen eral An item is large if the number of tuples containing the item exceeds the minimum support For a composite item consisting of i atomic items e.g a V CL~V  Vu the i-1 Va2V  Vn is acomposite item formed by selecting 1-1 atomic items from a\222 a2  a For example AV BVC has three distinct 2-subitems They are A V B,A V C and B V C An itemset is a set of items such that none of the items in the set have common items. For example A,B VC} is a valid itemset However A,AVB,CVD,CVDVE is not a valid itemset as a A and A V B have common item A and b CVDandCVDVE havecommonitemCVD person ID 1 feature I start I end RI 1 1 I5 A timing constraint specifies a condition in terms of the duration of the tuples in the database According to the tim ing constraint the tuples in a databases are used to form sev eral groups where the tuples in each group have the same per son ID For example for the tuples in Figure 1 each tuple is given a label for easy reference a timing constraint 223the tu ples whose duration intersect with each other\224 generates two groups RI,R2 and R3,R4 RI and R2, R3 and R4 are in the same group, because \(a they have the same person ID and b their duration intersect with each other RI and R3 are not in the same group as they have different person ID For the same reason RI and R4 R2 and R3 R2 and R4 cannot be in the same group In a temporal database let gl,g2 gn be the groups of tuples which are formed by applying the timing constraint to the tuples in the database such that all the tuples in gi where 1 5 i 5 n have the same person ID items\(g where 1 5 i 5 n denotes the set which records the value of the feu lure attribute of each tuple in g;\222  An itemset S is supported by g if a all the atomic items in S are in items\(g and b for each composite item in S items\(gi contains at least one of the atomic items which form the composite item2 The reason for requiring all the tuples in a group have the same person ID is because a support to an itemset must come from one en tity. For example, in Figure 1 assume that feature corresponds to disease name If we want to determine whether disease C causes disease B i.e whether B,C is a large itemset it is obvious that R2 and R3 cannot be grouped together to support B,C This is because B and C are associated with different people An association rule is an implication of the form X  Y whereX Y and XUY are itemsets X  0 Y  0 and X fly  0 An itemset consisting of i items is called an i-itemset The i-1 of an i-itemset is an \(i-1\which is formed by selecting i-1 items from the i-itemset An i-itemset has i distinct i-I For example a 3-itemset A,B C has three 2-subsets i.e A,B},{A,C and B,C 3 Algorithms for Finding Large Itemsets Finding large itemsets can be carried out in two steps 0 Step 1 Find all the large items and the large composite items 0 Step 2 Find all the large itemsets i.e the itemsets whose supports are greater than the minimum support At this step each large composite item is treated as an independent item like the atomic items It is assumed users are only interested in the composite items generated from a set of atomic items This set of atomic items will be provided by the users For example, assume that a there are five atomic items A B C D and E and b the users are only interested in the composite items gen erated from items A B and C 221For the example in Figure 1 irems\({RI,R2  A,B and itenzs\({R3,R4  C,D 222For the example in Figure 1 A,B is supported by group Rl,R2 A,B,C is neither supported by RI,R2 norby R3,R4 2804 


From A B and C four composite items AV B,A UC.BV C and A V B V C can be formed At step I the database is scanned to find the large items Assume that a the large composite items areAVB,AVCandAVBVC and b the large atomic items are A D and E At step 2 i itemsets are formed using the large items i.e A,L and AVBVC ii the tuples in the database are used to form groups according to the timing con straint and iii the database is scanned to find out which of these itemsets are large The reason that only the large atomic items and the large composite items are used in constructing itemsets is because all the items in a large itemset must be large 31 3.1 Example A patient database is shown in Figure 2\(a Each tuple contains a patient identifier, the disease contracted by the pa tient and the duration of the disease i.e the time that the diseases starts and the time when the disease is cured For easy reference each tuple is given a label In the tuples pu iicrir ID corresponds to persort ID and diseuse corresponds to itwe attribute as described earlier. Assume that the database is mined to discover if some diseases are likely to cause some other diseases It is assumed that a the minimum support is 3 b lie users are interested in the composite items generated from set A,B,C From A,B,C},fourcompositeitems,i.e AVBVC,AV A V C and 13 V C are formed c the timing constraint is 223the tiiples whose clmr-atiorz over lop n,itli ench other\224 First, the large composite items and the large atomic items are found according to the definitions in 2 The large items found arc shown in Figure 2\(b and c Then, the large atomic items in Figure 2\(c and the large composite items in Figure 2\(h arc iised to construct itemsets According to the defi nition in 2 X  0 and Y  0 in rule X 3 Y holds Thus only thc large itemsets which contain more than one item can he used to generate rules As the items in an itemset cannot have common items. from Figure 2\(b and Figure 2\(c it can hc seen that the only itemset that needs to be considered is B,A VC According to the timing constraint three groups are formed RI,R2 R3,R4 and RS,Rb All the groups support itcniset B,A V C Thus B,A V C will be used to generate rules ul Database patient ID I disease I start I end RI 1 113 0 Large Composite Items AVBVC AVC BVC 4 c Large Atomic Items I AtomicItem I SLID DO 1 d Laree Itemsets   Itemsets I Support B,AVC 1 3 Figure 2 3.2 Finding Large Items To find large atomic items is easy a counter is set up for each atomic item Then the database is scanned to check whether the atomic items are contained in the tuples and the counters of the atomic items will be increased accordingly When scanning is completed, the atomic items whose counters are greater than the minimum support are recorded as large items Finding large composite items is more complicated This is because the number of composite items generated from a set of atomic items might be very large Therefore it is not practical to generate all composite items and find the large composite items in a single database scan. Instead, large com posite items are found in several database scans During each scan only a small set of composite items are checked The large composite items are found in several steps At each step first a candidate set is formed The set contains the composite items which might be large The database is then scanned to find out which items in the candidate set are large Assume the large items found at the current step all consist of 1 atomic items The of the large items are used to form the candidate set of the next step When generating the candidate set the following principle is used If a tuple, say t does not contain a composite item say ul Vu V Vu then, according to thedefinition in 52 t does not contain any of al u2  a As a result t does not contain any composite items which are generated from the items in a q  un This means that if al V a2 V  V a is not a large item then none of the composite items generated from 2805 


11 m  n are large Thus the composite items generated from 11 CQ CL will not be included in the candidate set Initially the candidate set contains a single composite item which includes all the atomic items used to construct the com posite items. For example, assume CII cQ,LI is the set of atomic items used to generate composite items The item in the candidate set of the first step is a1 V 02 V a3 V a4 As dis cussed earlier if 11 Vu V a3 Va4 is not large, then none of the composite items generated from a m,a3,n4 are large Thus we can stop looking for large composite items after the first step Ifal Vu2 Vn Vn4 is large it is used to generate the items in step 2\222s candidate set Each item in step 2\222s candidate set is a 3-subitem of nl V c12 V uj V 4 Thus step 2\222s candi date set is NI V 02 V aj  CI V n2 V 4 al V a3 V 4 a V ai V Q The database is scanned to find the large items from the set Assume that the large items are a V ci2 V a3 and ci1 V a2 V 14 The 2-subitems of these two items form the candidate set of step 3 Thus step 3\222s candidate set is q Vn2,al Va3,ai V 04 U V 13 n2 V 04 As nl V CL V a4 and c12 V 03 V u4 are not large, from the earlier discussion CII Va3,nl Vc14,~12 VA and CQ V 04 are not large. Hence, step 3\222s candidate set is reduced to a Vn2 It can be seen that by observing the large items obtained at a step e.g step 2 it is possible to reduce the number of items in the candidate set of the next step \(e.g. step 3 The algorithm below is used to find all large composite items Assume that A  cq,q  ak is the set of atomic items used to generate the composite items CC represents the candidate set of a step LC denotes the set of large com posite items found at a step I cc  223I V a2 v  v 223k 2 for i  k;CCi  0  i  1 i   do 3 4 5 6 7 8 9 IO I I 12 13 14 IS 16 17 for each tuple t In the database do for each candidate c E CC do end-for-each if co/7tni/i\(t e then c.comf fi end-for-each if 1  2 then LC  e E cc I c COLI/Zt 2 1ZZ/Z1/?ZLII?1 support cc,_  0 for each ci E LC do CC,-l  CC,-l U JU I sa IS ciiz i-l of cl end-for-each for each c E CC 1 do if complete e LC  then fi 02-1  CC  e end-for-each fi 18 end-for 19 LCI  U LC let S  U V  Vu Va I U E A  a  a if S E LC then return 1 else return 0 fi The candidate set of the first step is given at line 1 The database is scanned in lines 3-7 corztairz\(t c is a predicate which determines whether tuple t contains the composite item e LC contains all the large items found at the current step A composite item is included in LCj if the support of the item is greater than the minimum support \(line 8 The candidate set of the next step CC 1 includes all the i-1 of the large items in LC lines 11-13 The items in candidate set CCi-1 will be checked against the items in LC to eliminate the items which are not large \(lines 14-16 The elimination is carried out according to complete line 15 The composite items in LCi have one more atomic item than the composite items in CCj-1 In complete a composite item, say c in CCj-1 is extended to include one atomic item which is not in e The extended items have the same number of atomic items as the items in LCj If all extended items are large i.e they are in LC then cremains in CCj 1 otherwise c is not large As a result c is removed from CCi-1 line 15 This is because, as described earlier if the extended item say c V a is not large, then none of c V a\222s i-I are large i.e c is not large When the candidate set becomes empty or a11 the compos ite items have been checked i.e. all the items containing more than one atomic item have been checked\\(line 2 all the large composite items have been found The result is stored in set LCI which is the union of all the set of the large items found at each step \(line 19 For example, in the patient database shown in Figure l\(a assume that a the minimum support is 4 and b the set used to generate the composite items is A,B,C The candidate set of step 1 is CC3  A V B V C line I It can be seen that the support of A V B V C is 6 Thus LC3  A V BVC The 2-subitems of A V B V C are used to form CC2 line 12 Hence CC2 AVB,AVC,BVC In complete A V B is extended with C to form a new item A VBVC As AV BVC is in LC3 A VBVC remains in CC2 For the same reason A V C and B V C also remain in CC2 At step 2 LC2 is A VB BVC As all the composite items have been checked, the algorithm terminates. Thus LCI  LC3 U LC2 3.3 Finding Large Itemsets  AV BVC,A V B,BVC Once all the large items are found a procedure based on the algorithm in 8 is used to find large itemsets In the algo rithm here each large composite item is treated as an indepen dent item like the large atomic items 2806 


The algorithm finds the large itemsets in several steps At each step it candidate set is formed first The set contains the itemsets which might be large The candidate set is generated according to the large itemsets found at the pervious step. Af ter the candidate set is formed, the database is scanned to find the large itemsets In the following A is the set of all the atomic items Lk is the sct of large itemsets obtained at step k of the algorithm Ck is the set of candidate large itemsets and Ck is generated in procedure cnrirlicl\(ite_gerz 20 LI  O 1 U t A and a is a large atomic item U a 1 CCI E LCI 21 for k=2;Lk-1 0;k do 22 Ck cLirzclidcite_ger\(Lk_l  23 24 CC  0 25 26 if iric/iic/e\(items\(g e then 21 28 fi 29 endLtor-each 30 for each itemset c t CC do COLl/?t  end-for-each for each group g formed according to the timing constraint do for each itemset c E Ck do cc,q  cc U c 3 1 end-for-each 32 Lk  C E Ck 1 c.co~irzt 2 rnirzirnurn supp-t 33 end-for 34 answer Uk Lk Initially the large itemsets are formed using the large items line 20 The database is scanned in lines 23-31 Each group formed according to the timing constraint is checked to see whether it supports the candidates in ck irzclude\(iterfzs\(g c is a predicate which checks whether group g supports c The predicate is true if and only if a all the atomic items in c are in items\(g and b for each of the composite item in c iterris\(g contains at least one of the atomic items which form the composite item Set CC contains all the itemsets being supported by group g lines 25-29 After all the groups have bccn chccked the itemsets whose support exceed the mini mum support become the large itemsets line 32 These large itemsets are used to generate the candidate large itemset for the next step line 22 At the end, the large itemsets found at different stases are joined together line 34 Procedure candidate-gerz calculates the large itemset can didates according to the large itemsets obtained at the previous step Each candidate in set Ck is a k-itemset If a k-itemset say S is large, then each of the of S should also be large 31 If one of the k-I of an k-itemset is not large, than the k-itemset is not large In candidate-gerz the large obtained at the previous step are checked to see whether they are the k-I of some k-itemsets A k-itemset becomes a candidate if all its \(k-I are large In candidate-gen firstly several sets of itemsets are formed line 35 Each set will be checked to see whether it con tains all the of a k-itemset The second con junct in the condition on line 35 means all the itemsets in SI s are the k-I of a k-itemset This is be cause each pair of the of a k-itemset are differ ent in two items In addition according to the definition in 42 each atomic item should appear in an itemset only once e.g A VB,A VC is not a valid itemset asA appears twice Thus two different items should not have common atomic items Condition  3e E S.{sl  sl C e in line 35 means the subset of any element in S should not be in S e.g if A,B},{A,C},{B,C is in S then A,B},{A,C should not be in S Each k-itemset has k distinct Hence in S only the sets which contain exactly k itemsets need to be con sidered These sets are used to form S line 36 If a set in S say ss with k elements \(itemsets\has k distinct items \(i.e I B  k in line 36 then ss must contain all the of a k-itemset As the itemsets in ss are from Lk-1 all the itemsets in ss are large As a result the k-itemset becomes a candidate line 37 Here is an example showing how candidate-gen works Assume that From line 35 L2  A A,BVC A,D BVC,Dl S  A B A D A B v C A 01 B v Cl W All A,B VC is not in S as B and BVC have B in A,D},{BVC,D is not ins as A,D common B V C,D c A,B V C A,D B V C,D holds Each candidate in C3 is a 3-itemset i.e each candidate has three items A,B},{A,D only has two elements Thus A,B},{A,D cannot contain all the 2-subsets of any 3 itemset As a result A,B A,D will not be considered further As only A,BVC},{A,D},{BVC,D has three elements 2807 


and contains three distinct items i.e A,BVC and D accord ing 10 line 36 Hence from line 37, the candidate in Ci is A,BVC,D 4 Conclusions This paper describes an algorithm for mining association rules in temporal databases The algorithm allows the user to specify the timing constraint which describes the relationships amongst the tuples in terms of time According to the timing constraint the tuples in a database are used to form several groups Then each of the groups is checked to see whether they support the itemsets which might be large The algorithm in this paper allows large itemsets to contain composite items Thus as in 8 the algorithm has the potential to discover more useful rules than the other algorithms References I R Agrawal T Imielinski and A Swami Mining associ ation rules between sets of items in large databases Proc qfACM SIGMOD Conference pp. 207-21 6 1993 2 R Agrawal and J Shafer, Parallel mining of association rules IBM Research Report RJ10004 1996 3 R Agrawal and R. Srikant Fast algorithms for mining as sociations rules Proc of 10th Irzt Coifererice on VLDB 1994 4 R Agrawal and R Srikant Mining sequential patterns PI-oc of I 1 tli Irit Coifererice on Datu Engineering 1995 5j M Holsheimer M Kersten H Mannila and H Toivonen A perspective on database and data mining Technical re port CS-R9531 CWI The Netherlands 1995 161 R Srikant and R Agrawal Mining generalised associa tion rules Proc qf 21st bit Conference on VLDB 1995 7j A Tansel J Clifford S Gadia S Jajodia A. Segev and R Snodgrass Temporal Databases Theory, Design and Implementation BenjaminKummings 1993 SI X Ye and J A Keane: Mining Composite Items in Asso ciation Rules, Proc of 1997 IEEE International Confer ence on Systems Man and Cybernetics pp 1367-1 372 2808 


 t  Fig 3 Average run time vs minp of minp e.g minp  7 there is little impact on CPU con sumption This is because minp is sufficiently large compared to the fraction of \223noise\224 transactions However, when minp is small pruning provides significant benefits In fact, when minp is 6.5 a typical run generates about 3730 candidates at the third level With pruning the number of candidates reduces to 2550 B Production Data This section applies our algorithms for discovering m patterns in data from a production computer network. Here our evaluation criteria are more subjective than the last section in that we must rely on the operations staff to detect whether we have false positives or false negatives Two temporal data sets are considered The first was collected from an insurance company that has events from over two thou sand network elements e.g routers hubs and servers The second was obtained from an outsourcing center that supports multiple application servers across a large geographical region Events in the second data set are mostly server-oriented \(e.g the CPU utilization of a server is above a threshold and those in the first relate largely to network events e.g 223link down\224 events Each data set consists of a series of records describing events received by a network console An event has three attributes of interest here host name which is the source of the event alarm type which specifies what happened e.g a connection was lost port up and the time when the event message was received at the network console We preprocess these data to convert events into items, where an item is a distinct pair of host and alarm type. The first data set contains approximately 70,000 events for which there are over 2,000 distinct items during a two week period The second data set contains over 100,000 events for which there are over 3,000 distinct items across three weeks We apply our algorithm for m-pattern discovery to both data sets, and compare the results to those for mining frequent item sets We fix minsup to be 3 so as to eliminate a pattern with only one or two instances, and we vary minp Our results are reported in Figures 4 and 5 for data sets 1 and 2 respectively These figures plot the total number of m-patterns the solid line and the number of border m-patterns the dashed line against minp Here a border pattern refers to a pattern that is not a sub set of any other pattern The x-axis is minp and the y-axis is the number of m-patterns discovered on a log scale Clearly minp provides a very effective way to select the strongest patterns in that the number of m-patters discovered drops dramatically as 14 0.1 0 0 0 0 0 J Fig 4 M-patterns of the first data set 223-\224 the number of m-patterns in the log scale 223..\224 the number of border m-patterns in the log scale x-axis is minp Fig 5 M-patterns of the second data set 222-\224 the number of m-patterns in the log scale 223..\224 the number of border m-patterns in the log scale x-axis is minp minp increases Many of these patterns have very low support levels For example we found 59 border m-patterns with length from 2 to 5 in the first data set when minp  0.7 Half of these patterns have support levels below 10 To compare with frequent patterns it suffices to set minp  0 since the algorithm reduces to mining frequent patterns Figure 6 reports frequent patterns found in the first data Here the x-axis is minsup and the y-axis is the log of the number of patterns found Note that the number of frequent patterns is huge-in ex cess of 1 veri when when minsup is 20 Examining the frequent patterns closely we find that most are related to items that occur frequently, not necessarily items that are causally re lated This is not surprise since the marginal distribution of items in our data is highly skewed Indeed a small set of items account for over 50 of total events and consequently these items tend to appear in many frequent patterns Beyond the quality of the results produced by mining for fre t no U a man Fig 6 Frequent patterns of the first data set 223-\224 the number of frequent patterns in the log scale 223..\224 the number of border frequent patterns in the log scale; x-axis is minp 415 


quent itemsets, there is an issue with scalability as well In Fig ures 4 and 5 minp 2 0.1 and minsup  3 Suppose we have minp  0 and minsup  3 so that we are mining for frequent itemsets but with a very low support threshold When we at tempt to run this case more than 30k candidates are generated at the third level. Not only does this result in very large compu tation time, we ultimately run out of memory and so are unable to process the data We reviewed the m-pattern found with the operations staff Many patterns are related to installation errors \(e.g a wrong parameter setting of a monitoring agent and redundant events e.g 11 events are generated to signal a single problem In addition a couple of correlations were discovered that are being studied for incorporating into event correlation rules for the real time monitoring We emphasize that over half of the m-patterns discovered have very low support levels Why are m-patterns common in these data One reason is a result of physical dependence that manifests itself as a set of events when a problem arises For example when a local area network LAN fails ail hosts connected to the LAN gen erate 223lost connection\224 events. Further, the same hosts generate these events if the same failure occurs This results in the mu tual dependence of these events This observation suggests that m-patterns can be used to construct signatures for problematic situations A second cause of m-patterns is redundant information For example a device may generate an event to report a problem it detects However, there may also be several management agents that monitor the same device and report the same problem This results in an m-pattern consisting of redundant events Identify ing such m-patterns can aid in constructing filtering rules that re move redundant events More details and insights can be found in 13][10 VI CONCLUSION Motivated by the need to discover infrequent but strongly correlated patterns we propose a new pattern a mutual depen dence pattern or a m-pattern M-patterns are defined in terms of minp the minimum probability of mutual occurrence of items in the pattern In contrast to one-way dependence as in asso ciation rules an m-pattern is characterized by a strong mutual dependency between any two of its subsets. That is if any part of an itemset occurs, the other part is very likely to occur as well Our results suggest that such strong mutual dependencies are common in computer networks such as due to interrelated components that are impacted by the same failure We develop an efficient algorithm for discovering m-patterns This is accomplished in three steps First we develop a linear algorithm to qualify an m-pattern based on an equivalence we prove Second we show that a level-wise search can be used for m-pattern discovery a technique that is possible since we prove that m-patterns are downward closed Last, we develop an effec tive technique for candidate pruning by establishing a necessary condition for the presence of an m-pattern A significant impact of the resulting algorithm is that it discovers strongly correlated itemsets that may occur with low support levels something that is difficult to do with existing mining algorithms Using synthetic data we demonstrate that our algorithm scales well as the data set increases in size We also show that the pruning algorithm provides considerable benefit, especially for small values of minp We apply our algorithm to data collected from two produc tion computer networks The results show that there are many m-patterns, many of which of have very low support levels \(e.g fewer than 10 occurrences\Attempting to discover these pat terns using A-priori requires a very small value for support lev els, which results in an explosion of candidates that overruns the memory of the computer we used We further develop frequent m-patterns that are defined in terms of both minsup and minp We show that this is a more general pattern That is, when minp  0 this pattern is equiv alent to frequent itemsets and when minsup  0 frequent m patterns become m-patterns ACKNOWLEDGMENT The authors would like to thank Chang-shing Pemg for help ful discussions REFERENCES C Agganval C Agganval and V.V.V Parsad Depth first generation of long patterns In lnt\222l Conf on Knowledge Discover rind Drm Mining 2000 R Agrawal T Imielinski and A Swami Mining association rules be tween sets of items in large databases In Proc fj\222VLDB pages 207-216 1993 R Agrawal and R. Srikant Fast algorithms for mining association rules In Proc of VLDB 1994 R. Agrawal and R Srikant Mining sequential patterns In Proc of the I Ith Int 221I Conference on Datu Engineering Taipei Taiwan 1995 R Bayardo, R. Agrawal and D Gunopulos Constraint-based rule mining in large dense database In ICDE 1999 R.J Bayardo. Efficiently mining long patterns from database In SIGMOD pages 85-93 1998 S Brin, R. Motiwani and C Silverstein Beyond market baskets Gen eralizing association rules to correlations Datu Mining and Knowledge Discovery pages 39-68 1998 Edith Cohen Mayur Datar Shinji Fujiwara Aristides Gionis Piotr Indyk Rajeev Motwani, Jeffrey D Ullman and Cheng Yang Finding interesting associations without support pruning In ICDE pages 489-499 2000 J Han J Pei and Y Yin Mining frequent patterns without candidate generation \(pdf In Proc 2000 ACM-SIGMOD Int Cunf on Munugement of Data SIGMOD\222OO Dallas TX 2000 J.L Hellerstein and S Ma Mining event data for actionable patterns In lnternutional Conference for the resource manugement  perfiormance evaluation of enterprive computing systems 2000 B Liu and W Hsu Post-analysis of learned rules In AAA/-96 pages 828-834 1996 Bing Liu Wynne Hsu and Yiming Ma Pruning and summarizing the dis covered associations In Proceedings of the ACM SICKDD International Conjerence on Knowledge Discovery  Datu Mining pages 15 18 1999 S Ma and J.L Hellerstein Eventbrowser A flexible tool for scalable analysis of event data In DSOM\22299 1999 S Ma and J.L Hellerstein Mining partially periodic event patterns In ICDE pages 205-214,2001 H Mannila H Toivonen and A Verkamo. Discovery of frequent episodes in event sequences Data Mining mid Knowledge Discover 1\(3 1997 B Padmanabhan and A Tuzhilin A belief-driven method for discovering unexpected patterns In KDD-98 1998 J Pei and J Han Can we push more constraints into frequent pattern mining In CorS on Knowledge Discover rind Datu Mining KDD\222OO Boston MA 2000 H Toivonen Discovery of frequent patterns in large data collections 1996 Technical Report A-1996-5 Department of Computer Science Uni versity of Helsinki 416 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


