Challenges of Privacy Protection in Big Data Analytics\nMeiko Jensen?\nIndependent Centre for Privacy Protection Schleswig-Holstein \(ULD almost every\ntype of information eventually can be derived from suf?ciently\nlarge datasets. However, in such terms, linkage of personal data\nof individuals poses a severe threat to privacy and civil rights.\nIn this position paper, we propose a set of challenges that\nhave to be addressed in order to perform big data analytics in\na privacy-compliant way.\nKeywords-privacy; big data; challenges\nI. INTRODUCTION\nThe ongoing move towards ever bigger reservoirs of data\ncollected from mostly digital sources implies tremendous\nchanges to the way we interact with online services. Given a\nsuf?cient dataset as basis, almost every single user of online\nservices can be identi?ed, leading to neatly tailored online\nservices being provided to each individual. But, while hyped\nas a fortune by many, the upcoming focus on big data and\nthe implications derived thereof also has its pitfalls.\nIn this position paper, we argue that the upcoming trend\ntowards big data analytics, and the use of the results thereof,\nleads to an erosion in terms of privacy and user’s rights.\nBased on a very generic model of big data analytics, we\nderive a set of key challenges of big data with respect to\nprivacy and civil rights of individuals. Though being far\nfrom complete, we outline the core elds of research to be\nfostered in order to reach a privacy-compliant level of big\ndata analytics.\nII. BIG DATA\nIn our model, the general term big data refers to a huge\ncollection of information \(the dataset typically stemming\nfrom more than one data source, and being processed by a\ndata analyst or data processor. We assume two modes of\nbig data analytics, depending on the intention of its use.\nThe ?rst mode consists in veri?cation of a pre-existing\nhypothesis. Here, the data analyst already has an assumption,\ne.g. about a certain property of a service’s users, that he\nwants to verify by means of data analytics. Examples range\nfrom rather trivial issues like “are most users of online\npharmacy services over 50 years of age?” to complex\nquestions like “do we achieve most of our pro?ts from\n?This work was partially funded by the European Commission, FP7\nICT program, under contract No. 257243 \(TClouds project FutureID project that saw our advertisements on external websites?”.\nIn such scenarios, the data analyst will intentionally focus\non shaping the dataset to a representation that best-possibly\nprovides the requested answer. Hence, this type of big data\nanalytics is about veri?cation of assumptions.\nIn contrast, the second mode is that of identi?cation. Here,\nthe data analyst gathered a large dataset, potentially from\nmultiple sources, and tries to identify interesting facts hidden\nwithin the dataset. A priori, the data analyst has few to\nno information about interesting aspects of the dataset, but\nhopes to stumble over interesting statistical outliers hidden\nin the overall bunch of information. An example for such\ntype of big data analytics would be “Hey, look at this Most\nplayers of online game X also bought our new mouse!”\nHere, two key concepts of aggregation of datasets within\nbig data context must be de?ned. The ?rst is the aggregation\nof schematically identical datasets, e.g joining the service\naccess logs of two different online services that are based\non the same web server implementation. This is mostly used\nto gain more information within an existing context.\nThe second type of aggregation is about linkage created\nfrom joining two datasets from disjunct contexts, based\non some key information shared in both datasets to be\naggregated. A key aspect here is that it must be feasible\nfor the data analyst to identify links in the dataset. Such\nlinks are data ?elds that have identical, similar, or other-\nwise suf?ciently related values in different datasets, such\nas user email addresses, postal codes, or combinations of\nIP addresses and timestamps. As datasets may stem from\ndifferent sources, or may contain information from different\ncontexts, a key challenge of big data analytics consists in\nidentifying such linkage.\nIn this context, the identity of service users plays a major\nrole. Given that many types of data in big data contexts\nare generated by human individuals, using their identity\nas the linking element of otherwise disjoint datasets is a\ntempting approach. Unfortunately, as we will discuss in the\nnext sections, this linkage via user identity bears some very\nchallenging pitfalls in the ?eld of privacy.\nIII. CHALLENGES TO PRIVACY\nIn this section we iterate through a set of challenges that\nmay threaten privacy of individuals in the context of big data\nanalytics.\n2013 IEEE International Congress on Big Data\n978-0-7695-5006-0/13 $26.00 © 2013 IEEE\nDOI 10.1109/BigData.Congress.2013.39\n235\nA. Interaction with Individuals\n1 most challeng-\ning issue with respect to privacy enforcement in big data\ncontexts is the involvement of individuals. On the one hand,\nin most big data contexts it is absolutely neccessary to\ncollect and process information that is bound to speci?c in-\ndividuals \(i.e. is personally identi?able information, cf. [1 the other hand, including such to the scope of big\ndata analytics automatically entitles each individual to be\ninformed about every type of data processing that involves\nits data. This right, which for European countries is ?xed in\nthe European Data Protection Directive [2], must be granted\non a per-request basis However, with huge data amounts as\nrequired by big data, it is highly challenging to even identify\nwhich 


information hidden in such piles of data actually are\nbound to an individual’s identity.\nAs an example consider one of the most common types of\nbig data sources: network traf?c. Here, in order to answer a\nrequest for transparency, it becomes necessary to crawl huge\nlists of IP addresses and timestamps, accompanied with other\ntypes of data, such as urls accessed, session cookies used,\nunique identi?ers for certain scopes, etc always looking for\ninformation that might be—or might be not—correlated to\nthe requesting individual Obviously, this is not feasible in\na trivial way, so simpli?cations have to be made.\nMoreover, a request for transparency according to Eu-\nropean law does not only cover the personal data of an\nindividual, but also involves detailed documentation of the\nprocesses by which such data is processed. Hence, it is not\nonly necessary to disclose the personal data of an individual,\nbut also provide details on the algorithms and processes\nthat are involved in the big data analytics performed with\nthem. Given that most of these processes will contain quite\ncomplex data mining algorithms, which moreover may be\nconsidered as business secrets of the big data analyst, this\nlegal obligation turns out to pose a major challenge to big\ndata processing on a broader scale.\nAll in all, providing transparency towards individuals with\nrespect to the type of processing and set of personal data\nused therein must be seen as the most challenging part of\nprivacy-compliant big data analytics.\n2 the right to refuse\nthat their data is being processed in a certain scope or\nby a certain data processor Thus, many privacy laws and\nregulations entitle individuals with the right to be asked\nprior to processing of their data. This aspect, gathering\nconsent from individuals prior to processing of their personal\ninformation suffers from the same complexity isues as listed\nin the previous section. Moreover, most regulations do not\nonly require consent, but even informed consent, meaning\nthat the individual must be able to understand what sort of\nprocessing is performed, and what may happen with their\ndata as a result. Given that many types of big data analyt-\nics are based on highly complex data mining algorithms,\ninformed consent then implies that each individual must be\nprovided with an explanation of all of these algorithms such\nthat they can understand what happens there. This, again,\nmust be considered a tremendously challenging issue with\nrespect to big data analyses to come.\n3 consent, an individual may\ndecide to revoke its given consent for processing its personal\ndata at a later stage. For instance, an individual may decide\nno longer to trust a data collector, e.g. due to assumed\nleakage of sensitive data.\nIn that case, the legal obligations—at least according to\nEuropean privacy laws—grant such individuals the general\nright to revoke their consent, implying that all processing\nof personal data of such individuals has to be stopped as\nsoon as possible, and that furthermore all personal data of\nthose individuals have to be deleted. Given that the personal\ndata of a particular individual may have been spread widely\namong data collectors and data analysts, implementing such\nrevocation requests turns out to become a highly challenging\nissue.\nB. Re-Identi?cation Attacks\nAnother major threat with respect to privacy in big data\nanalytics is the ability to perform “re-identi?cation attacks”,\nmeaning that a huge dataset available is explicitly scanned\nfor correlations that lead to a unique ?ngerprint of a single\nindividual. More precisely by linking different types of\ndatasets together, the uniqueness of each entry is increased,\nup to the point that a link back to an individual’s identity\ncan be established.\nAs an example, consider the following. As discussed\nin [3], the Internet Search Provider AOL in 2006 published\na set of search terms of their users. The assumption was\nthat the search terms themselves, correlated only by means\nof a number as a user identi?er would be anonymized\nsuf?ciently to prevent identi?cation of individuals among\nthe search query authors Unfortunately, as it turned out, re-\nidenti?cation of individuals was indeed feasible, and Thelma\nArnold was re-identi?ed by her search terms only—which\nwere assumed to have been properly anonymized. This\nanalysis which linked the search term database to other\npublicly available databases of U.S. citizens impressively\nillustrates the power of re-identi?cation attacks. However,\nit is worth noticing that a re-identi?cation attack is an\nintentional act, which starts with a dataset \(anonymized,\npseudonymized, or plain attack,\nnamely correlation attacks, arbitrary identi?cation attacks,\nand targeted identi?cation attacks. These are to be described\nnext.\n236\n1 mostly uniform data values to other\nsources in order to create more unique database entries.\nFor instance linking pseudonymized customer data of phar-\nmacies to equivalently pseudonymized data of medications\nobtained from a hospital leads to more ?ne-grained data\nper entry. More precisely, if one database lists userIDs and\npharmacies visited, and the other lists the same userIDs cor-\nrelated to medication prescriptions, the correlated database\nconsists of entries that indicate which hospital patient bought\nits medication at which pharmacies. Thus, the correlated\ndataset has more information per userID, allowing for a\nmore precise analysis 


on the individuals behind the data.\nA correlation attack, in this scenario, consists in linking\nadditional datasets in up to the point that there is either at\nleast one entry in the correlated dataset that is unique in its\ncombination of data ?elds \(despite the userID have all data ?eld\nvalues identical \(again despite the userID as a basis for a subsequent other\ntype of identi?cation attack, as discussed next.\n2 Attacks: The primary goal of\nthis type of re-identi?cation attack is to link at least one entry\nin the given dataset to the identity of a human individual.\nTypically, the attack is completed when a link between one\nentry in the aggregated dataset and one identity of a human\nbeing can be drawn, with a suf?cient level of probability.\nThis type of attack neatly illustrates the failure of\nanonymization for a certain set of anonymized data. The\nAOL example given above is a perfect example, as it\nillustrates that the anonymized dataset—in correlation with\nother datasets—was suf?cient to identify a few individuals\n\(besides Thelma Arnold among the huge set of users by\nname. A key aspect here is that is was not feasible to link\nall search terms to real-world individuals. Hence, given the\nidentity of an arbitrary human individual a priori, it is very\nunlikely to be able to ?nd an entry in the correlated dataset\nthat can clearly be attributed to that identity. In that point,\nthe arbitrary identi?cation attack differs from the targeted\nidenti?cation attack.\n3 Targeted Identi?cation Attacks: With the targeted iden-\nti?cation attack, an adversary tries to ?nd out more details\nfor a given human being. Hence, the targeted identi?cation\nattack is only successful if it is possible to link some entries\nin the database to the identity at hand, with a suf?cient level\nof probability. In that sense, the AOL search term issue\nwas not a targeted identi?cation attack, as it was not the\nadversary’s intent to ?nd out information on Thelma Arnold,\nbut to identify any person within the dataset.\nTargeted identi?cation attacks can be considered the most\nthreatening type of re-identi?cation attacks, as they are likely\nto have the largest impact to an individual’s privacy. Depend-\ning on the relation of adversary and searched individual,\nprivacy issues may rapidly come up, being performed inten-\ntional or by accident. For instance, if an employer searches\na huge dataset of pharmacy customers for occurrences of\nits employees, this may disclose information on medical\ntreatments—and hence illnesses—of its employees, which\nclearly is a violation of those individual’s privacy. The same\nobviously holds if the employer was actually looking for\ninformation on, say himself, but happened to stumble over\nthe name of an employee. Though not being intentional in\nthis case, the privacy violation remains.\nC. Probable vs. Provable Results\nA key threat to big data analytics is the validity of the\nresults gathered. Depending on the query a big data analyst\nperforms on the dataset available different types of results\ncan be produced. For instance, if the dataset was formed\nby correlation of email addresses contained in all of the\ncorrelated datasets, it may be assumed that two entries that\nheld the same email address before correlation will lead to\na single database entry that re?ects all information on the\nsame identity, i.e. the holder of the email address.\nHowever, in some cases, this assumption may not hold.\nFor instance, it might be possible that an individual managed\nto spoof its email address within one of the datasets. This\nwould cause a false correlation of dataset entries, and hence\nresult in incorrect correlated data. Additonally, some email\naddresses may be shared by two or more individuals, causing\nfalse data linkage if used for correlation to individuals.\nEither way, the use of email addresses is not 100% reliable,\nbut gives a very precise notion of “identity” in the sense of\nuniqueness and correlation within a dataset.\nThe situation gets more challenging if the type of link-\nage is not that reliable. For instance, correlating dataset\nentries by means of IP addresses and timestamps, e.g.\nwithin online service access logs, can never give a reliable,\n100% trustworthy correlation. Depending on the distribution\nof access events, and the temporal distance between two\nevents originating from the same IP address, a correlation is\npossible, maybe even likely, but never guaranteed. Given the\nhigh degree of ?exibility in terms of using IP addresses, it\nis perfectly feasible that the same IP address is used by two\ncompletely different individuals, with an almost negligible\ndelay between the two resulting events. Hence, if using\nsuch “probability-based” linkage of datasets, the general\nprobability of false correlations is increased. In contrast to\nlinkage via email addresses \(that are either identical or not\nidentical fuzzy. Hence, instead of a provable link \(in the sense\ndiscussed above probable one.\nThe threat to privacy in this terminology obviously stems\nfrom big data query results that originate in such probable\nlinks. For instance, if the big data query was to ?nd the\nexact number of users that accessed both system A and B\nwihtin the last ?ve weeks, that number may become falsi?ed\nby considering such false correlations. Depending on the\n237\nsubsequent use of such a result \(as part of another big data\nquery Effects\nDespite the threats caused by interaction with concerned\nindividuals, intentional attacks, or issues 


of false data pro-\ncessing methodology, a fourth category of threats covers the\neconomic issues of the big data paradigm. Given that most\ntypes of big data analytics require a huge set of different\ndatasets in advance, it becomes necessary to exchange such\ndatasets among business partners. This can e.g. be based\non mutual agreement, legal obligations, or by means of an\neconomic data market where data providers sell data of their\nusers to their customers. In that latter case, a lot of threats\nto privacy may arise from economic considerations in such\ndata trading. Out of these, in the following the two examples\nof confusion and distraction and context faults will be\ndiscussed. Note, however, that there are many more threats\narising in this category, ranging from fraud to censorship to\nsurveillance, which are not discussed here.\n1 and Distraction: The idea behind the ap-\nproaches of confusion and distraction is to make a sold\ndataset less useful for a customer, e.g. in order to in?uence\na competitor’s business strategies. By means of slightly\naltering the original dataset in some way, it becomes possible\nto push subsequent big data analytics towards a certain\ndirection. For instance, a company A may decide to sell their\nuser’s data to one of their competitors, company B, but does\nnot want to reveal the number of users that actually are using\nboth A and B as this may be a critical information w.r.t.\nthe business strategies of both A and B preprocess the dataset sold to B in a way that all users\nthat also use services of B are removed. In this rather trivial\ncase, B will certainly notice the lack of an intersection, and\nwill detect the alteration.\nHowever, more subtle modi?cations applied to the dataset\nby A may be less obvious, but still impact on the results\ngathered out of that data at B. Especially in situations\nwhere B regularly buys and processes datasets from A, the\nopportunities of A to shape its sold dataset in such a way\nare multifold.\nEither way, as such alteration of datasets inevitably im-\nplies modi?cation of entries of individual users \(by means of\ndeletion, alteration, or duplication users of A becomes evident. The economic\nenvironment induces a falsi?cation of big data analytics\nresults here, with all implications w.r.t. privacy, as discussed\nfor probability-based linkage in Section III-C.\n2 Context Faults: Another threat arising from the eco-\nnomic scenario of selling big data datasets consists in faults\nof context correlation. If company A sells its user’s data to\ncompany B, this is not necessarily accompanied with an in-\ndepth description of each data ?eld’s semantics. Though that\ninformation might be obvious \(e.g. for email addresses for XML data that B interprets some of the data coming from\nA different that they were contexted at A. For instance, if\nA accompanied all their user’s data with a self-calculated\n?eld activity index in %, the calculation of such an activity\nindex may be different from the calculations B performs\nfor its own users. If, in that case, B blindly correlates A’s\nactivity index ?elds and B’s activity index ?elds, this might\ne.g. result in the interesting observation that A’s users are\na little more active than B’s users—though in reality it was\njust a different type of calculation that was not spotted in\nthe big data analytics.\nThough the given example might be a little simplistic,\nmore complex data, stemming from advanced aggregation\nalgorithms, may induce such context-related misinterpreta-\ntions, which ultimately result in false results to big data\nqueries, as discussed above. Again, as big data queries often\nare about individual’s properties, this may lead to false\nstatements w.r.t. individual users \(e.g. “none of our users\nuses product X again.\nIV. CONCLUSIONS AND RESEARCH INDICATIONS\nAs can be seen, the ?eld of privacy in big data contexts\ncontains a bunch of key challenges that must be addressed\nby research. Many of these challenges do not stem from\ntechnical issues, but merely are based on legislation and or-\nganizational matters Nevertheless, it can be anticipated that\nit is feasible to meet each of the callenges discussed here by\nmeans of appropriate technical measures. For instance, keep-\ning track of a particular individual’s data throughout big data\nanalytics contexts is merely an organizational requirement\nthat can e.g. be met by means of log?les Linkage of disjoint\ndatasets can often be performed without relying on linkage\nvia user’s identities, but based on other types of data. In the\nsame direction, many types of data can be preprocessed with\nproper anonymization or pseudonymization prior to sharing,\nsuch that linkage of datasets remains feasible, but linkage to\nan individual’s identity becomes hard.\nEach of these ?elds already has a lot of pre-existing\nresearch that has already been performed. Hence, the future\ndirections for research in this red hot topic are obvious:\napplying these existing techniques to the context of big data.\nREFERENCES\n[1] E McCallister, T. Grance, and K. Scarfone, “Guide to protect-\ning the con?dentiality of personally identi?able information\n\(pii  of individuals with regard to the processing\nof personal data and on the free movement of such data 1995.\n[3] M. Barbaro and T. Zeller, “A face is ex-\nposed for aol searcher no. 4417749,” 2006. [On-\nline Available: http://select.nytimes.com/gst/abstract.html?\nres=F10612FC345B0C7A8CDDA10894DE404482\n238\n 





statistical tool and Apache \nMahout as a big data solution for machine learning. Each \nscenario was evaluated for model quality and time \nefficiency. Table III shows the general setup for each \nsoftware platform. \nTABLE III.  GENERAL SETUP FOR SOFTWARE PLATFORM \nSoftware \nSystem Details \nAppliance Available RAM \nR  3.0.0 Linux server 16GB \nMahout 0.7 \nTwo-Node Hadoop Cluster \n2GB\(each node Cluster \n \nOur original data in MHS contains only 15696 records \nfor CHF patients admission in Multicare hospitals which \ncannot be considered as a big data, however, since our aim \nis to have a general solution that can be applicable to any \ndata size for RoR prediction, we scaled up the original data \nlinearly several times to show how big data framework \noutperforms in comparison with traditional systems when \nthe training set becomes larger. The scaled data has \ncreated an increased demand for memory and processing \ntask needed to predict RoR. Table IV shows the five \ndifferent scenarios of data size. \nTABLE IV.  SCENARIOS FOR DATASET nNumber of Tuples in Dataset  \nScenario 1 Scenario 2 Scenario 3 Scenario 4 Scenario 5 \n15,696 114,862 900,000 1,665,866 3,271,716 \n \nIn order to directly compare the modeling results, \nparameters were chosen consistently across software. For \nexample, in random forest model, the number of trees \nequals to 120 and each tree has 15 variables at random. \nThe data were split into a 70/30 training and test partition, \nfor all the scenarios. In these set of experiments, we ignore \nthe logistic regression result because the execution mode \nfor logistic regression is sequential so it cannot be used to \nshow the parallelization benefit of big data solution. Table nVI shows the result of random forest model in both R and \nMahout Framework for all the scenarios. Random forest \nalgorithm trains an enormous number of simple classifiers \nand uses a voting scheme to get a single result. The \nMahout parallel implementation trains many classifiers in \nthe model in parallel.  We choose Random forest since this \napproach has somewhat unusual scaling properties. \nBecause each small classifier is trained on some of the \nfeatures of all of the training examples, the memory \nrequired on each node in the cluster will scale roughly in \nproportion to the square root. Thus the size of vectors is \nlarge and larger vectors consume more memory and slow \ndown the training. That’s the main reason that R was not \nable to run random forest in scenario 5 because the data is \ntoo large to fit into main memory. These set of experiment nshow the main advantage of Mahout, which is its robust \nhandling of extremely large and growing data sets nThe Fig. 2 shows the training time of random forest \nmodel in R and Mahout Platforms.it can be observed that nwhen the number of training examples is relatively small, \ntraditional data mining tools work even better than Mahout \n\(scenario 1 is significantly \nbetter with regard to time in comparison with traditional \nstatistical tools such as R. The increased time required by \nnon-scalable algorithms is often due to the fact that they \nrequire unbounded amounts of memory as the numbers of \ntraining examples grow. \n \n \nFigure 2. Train time for random forest model nFig. 3 compares the performance of Mahout \nFramework for different number of nodes for Hadoop \ncluster. The parallelization power of mahout can be easily \nobserved from this diagram. When the number of training nexamples is relatively small, Two-machine Hadoop cluster \nworks even better than three and four-nodes but for larger \ntraining data, the training time significantly decreases \nwhen we increase the number of nodes in Hadoop cluster. \n \n \nFigure 3. Training time for Mahout with different \nnumber of nodes \n69\nV. BACKGROUND AND SIGNIFICANCE \nPreventing hospitalization is a prominent factor to reduce \npatient morbidity, improve patient outcomes, and curb health \ncare costs. An increasing body of literature attempts to \ndevelop predictive models for hospital readmission risks \n[1,8,12,14,15,16 ,17,18,19,20,21,22,23,24]. These studies \nrange from all-cause readmissions to readmission for specific \ndiseases such as heart failure, pneumonia, stroke, and nasthma. Each of these models exploits various predictor \nvariables assessed at various times related to index nhospitalization \(admission, discharge, first follow-up visit, \netc.  In another research study [9], a real-time predictive \nmodel was developed to identify CHF patients at high risk \nfor readmission within the 30-day timeframe. In this model, \nsome clinical and social factors available within hours of \nhospital presentation are used in order to have a real-time \npredictive model. Although the model demonstrated good ndiscrimination for 30-day readmission \(AUC 0.72 1372 HF patients the \nrecent studies for predicting 30-day readmission risk for \nheart failure hospitalization is done in [15 In this work, \nadministrative claim data is used to build a regression model \non 24,163 patients from 307 hospitals. In a recent research \nstudy, we have proposed a risk calculator tool [14] that is \ncapable of calculating 30-day readmission risk for \nCongestive Heart Failure based on incomplete patient data.  \nIn a separate research effort [25], we also demonstrate the \neffectiveness of data preprocessing in 30-day readmission \nrisk prediction problem. Novel predictive modeling \ntechniques for 30-day risk-of-readmission prediction \nproblem are investigated by the authors in [25].   \nA recent study investigates the impact of big data on \nhealthcare solutions [26]. This study suggests that leveraging \nthe collection of patient and 


practitioner data could be an \nimportant way to improve quality and efficiency of health \ncare delivery. In fact, while the complexity of the domain, \ndue to very high velocity volume and variety of medical data \nis acknowledged [26], however, the necessity of enabling big \ndata solutions to these problems is mostly overlooked in the \nprevious works. To the best of our knowledge, we are the \nfirst one to propose big data solutions for information \nextraction, information integration, and predictive modeling \nfor 30-day readmission risk prediction problems.  \nFortunately there exist tools and programming paradigms \nfor such data intensive applications leveraging distributed \ncomputation model. Apache Hadoop is one such distributed nframework that implements a computational paradigm \nMapReduce, where the application is divided into many nsmall fragments of work, each of which may be executed or \nre-executed on any node in the cluster data intensive \ndistributed applications, such as Apache Hadoop.  In \nconjunction with a distributed system such as Hadoop, \nApache Mahout provides a useful set of machine learning \ntools that allow data to be classified clustered, and filtered, \nand Mahout was designed to work with Hadoop so it is \neasily scaled up to large dataset and networks. MADlib and \nBismark[27,28] are two other useful analytics tools that are \ndesigned to analyze structured and unstructured data in \nparallel. These are excellent tools to enable scalable nsophisticated in-database analytics and have been well \nadopted by the database-engine developers, data scientists, \nIT architects and academics. \nVI. CONCLUSION \nIn this work, we study the big data solution for predicting \nthe 30-day risk of readmission for the CHF patients.  Our \nproposed solution leverages big data infrastructure for both \ninformation extraction and predictive modeling. We study \nthe effectiveness of our proposed solution with a \ncomprehensive set of experiment, considering quality and \nscalability. As ongoing work, we aim at leveraging big data \ninfrastructure for our designed risk calculation tool, for \ndesigning more sophisticated predictive modeling and \nfeature extraction techniques, and extending our proposed nsolutions to predict other clinical risks. \nVII. ACKNOWLEDGMENT \nThis work is supported by MHS \(grant no A73191 time and insightful \ndiscussions during the initial stage of the study. \n \nTABLE V.  RESULT OF ROR PREDICTION ON ORIGINAL DATASET \nModels \nSupervised \nLearning \nAlgorithms \nClass \nImbalance \nSolution \nResults for Prediction \nAccuracy Precision Recall F-measure AUC \nMHS Model \nLogistic \nRegression \n- 77.88% 32% 0.69% 1.36 63.78% \nOS 58.39% 28.90% 61.41% 39.30% 63.24% \nRandom Forest - 77.90% 40.47% 1.48% 3.01% 61.04% OS 77.96 44.44% 1.7% 3.3% 62.25% \nYale Model \n\(Baseline VI.  RANDOM FOREST PREDICTION RESULT ON R AND MAHOUT \nDataset Platform Results of Prediction Accuracy Precision Recall F-measure Runtime \nScenario 1 R 77.90% 40.47% 1.48% 3.01% 18.96 sec \nMahout 78.84% 93.61 3.83% 7.35% 32.75 sec \n70\nScenario 2 R 82.35% 99.59% 17.78% 30.17% 4.01 min \nMahout 78.55% 94.51% 1.8% 3.7 36.65 sec \nScenario 3 R 86.34% 99.87% 37.49% 54.52% 1h 17m \nMahout 80.91% 91.62% 14.49% 25.02% 2m 20 sec nScenario 4 R 87.12% 99.88% 40.60% 57.73% 4h 5m \nMahout 80.98% 88.83% 15.94% 27.02% 4m 35sec \nScenario 5 R Cannot allocate vector of size 3.9 Gb after 10 hour running \nMahout 80.79% 91.48% 13.99% 24.27% 7m21sec \n nREFERENCES \n[1] Donzé J. Aujesky D., Williams D., Schnipper J.L, MD. Potentially \navoidable 30-day hospital readmissions in medical patients: \nDerivation and validation of a prediction model. JAMA Internal \nMedicine 173\(8  frontier for innovation competition and productivity, \nMcKinsey Global Institute, 2012. \n[3] The Apache Software Foundation., \nhttp://hadoop.apache.org/common/credits.html. \n[4] Ghemawat  D.J .MapReduce: simplified data processing on large \nclusters. In: Proc of OSDI, 2004. \n[5] Owen S. and Anil R. Mahout in Action. Manning Publications Co., \nGreenwich, Connecticut, 2010. \n[6] Thusoo, A., Sarma, J. S., Jain, N., Shao, Z., Chakka P., Anthony, S., \nLiu, H., Wyckoff, P., And Murthy, R .Hive—a warehousing solution \nover a Map-Reduce framework. In VLDB, 2009. \n[7] Wikipedia, http://en.wikipedia.org/wiki/Apache_Cassandra. \n[8] Adams K. F Fonarow G. C., Emerman C. L., LeJemtel T. H., \nCostanzo M. R., Abraham W. T., Berkowitz R. L., Galvao M., and nHorton D. P. Characteristics and outcomes of patients hospitalized for \nheart failure in the United States Rationale, design, and preliminary \nobservations from the first100, 000 cases in the acute decompensated nheart failure national registry \(ADHERE 2  Chen J, Lin Z, Bueno H, Curtis JP, Keenan PS, Normand \nSL, Schreiner G, Spertus JA, Vidán MT, Wang Y, Wang Y nKrumholz HM. Recent national trends in readmission rates after heart \nfailure hospitalization. Circ Heart Fail, 3:97-103, 2010. \n[10] Krumholz H. M., Normand S. L. T., Keenan P. S., Lin Z. Q., Drye E. \nE., Bhat K R., Wang Y. F., Ross J. S., Schuur J. D., and Stauer B. D.. \nHospital 30-day heart failure readmission measure methodology. \nReport prepared for the Centers for Medicare & Medicaid Services. \n[11] Amarasingham R, Moore BJ, Tabak YP, Drazner MH, Clark CA, \nZhang S, Reed WG, Swanson TS, Ma Y, Halm EA. An automated \nmodel to identify heart failure patients at risk for 30-day readmission \nor death using electronic medical record data 


Journal of Medical \nCare, 10:981-988, Feb. 2010. \n[12] MULTICARE HEALTH SYSTEM, http://www.multicare.org n[13] Han J. and Kamber M.. Data mining: concepts and techniques. \nMorgan Kaufmann, 2006. \n[14] Zolfaghar K Agarwal J., Sistla D., Chin S., Roy S. B., Verbiest N., \nTeredesai A., Hazel D., Amoroso P., and Reed L Risk-o-meter: An \nintelligent clinical risk calculator. In Proceedings of the 19th ACM \nSIGKDD Conference on Knowledge Discovery and Data Mining \n\(KDD  Yancy CW, \nPeterson ED, Hernandez AF. Incremental value of clinical data \nbeyond claims data in predicting 30-day outcomes after heart failure \nhospitalization. Cardiovasc Qual Outcomes, 4\(4  Coleman EA, Parry C, Chalmers S, Min SJ. The care transitions \nintervention: Results of a randomized controlled trial. Archives \nofInternal Medicine, 166\(17  Mari D., Tettamanti M., Djade C. D., Pasina \nL., Salerno F., Corrao S. , Marengoni A., Iorio A., Marcucci M and \nMannucci P. M. Risk factors for hospital readmission of elderly \npatients.European Journal of Internal Medicine, 24\(1  npost discharge telephonic follows up on hospital readmissions. Popul \nHealth Manag, 14:27-32, 2011. \n[19 Hunter T., Nelson J., and Birmingham J.. Preventing readmissions \nthrough comprehensive discharge planning Prof Case Manag., 18:56-\n63, 2013. \n[20] Kaur H. and Wasan S. K.. Empirical study on applications of data nmining techniques in healthcare. Journal of Computer Science, \n2\(2  Johnson M. L., Cody R. J., and Aaronson. K. D.  \nDischarge education improves clinical outcomes in patients with \nchronic heart failure. Circulation, 111\(2  applications in healthcare. \nJournal of Healthcare Information Management Vol, 19\(2  H. M., Amatruda J., Smith G. L., Mattera J. A., Roumanis \nS. A., Radford M. J., Crombie, P. and Vaccarino V Randomized trial \nof an education and support intervention to prevent readmission of \npatients with heart failure. Journal of the American College of \nCardiology, 39\(1  P., Illig S., Linn R., Fiedler R., and Granger \nC.. Comparison of logistic regression and neural networks to predict \nrehospitalization in patients with stroke. Journal of clinical \nepidemiology, 54\(11 n[25] Meadam N., Verbiest N., Zolfaghar K., Agarwal J., Chin S., Basu \nRoy S., Teredesai A., Hazel D., Reed L., Amoroso P. Exploring \nPreprocessing Techniques for Prediction of Risk of Readmission for \nCongestive Heart Failure Patients. In Data Mining and Healthcare \nWorkshop, in conjunction with the 19th ACM SIGKDD Conference non Knowledge Discovery and Data Mining \(KDD  of Big Data to \nHealth Care, JAMA.   2013; 1351- 1352. doi:10.1001/jama.2013.393. \n[27] Hellerstein, J Schoppmann F., Wang D. Z., Fratkin E., Gorajek A., \nWelton C.,  Feng X., and Kumar A. The madlib analytics library or \nmad skills. PVLDB 2012. \n[28] Feng, X., A. Kumar, B. Recht, and C. R´e, 2012: Towards a unified narchitecture for in-rdbms  analytics. In SIGMOD Conference, pp. \n325–336 \n \n71\n 


significantly \ndriven by launch vehicle fairing packaging and resulted in \nhaving no exposed cells when stowed. This iterative design \nexample is representative of the close interaction between \nscience instrument, and spacecraft design teams to arrive at \nthe least complex design to satisfy mission requirements. \nSMAP uses a single-string architecture with selective \nredundancy. Graceful degradation features have also been \ndesigned into the observatory where practical. The \ntransponders, transmitters, and inertial reference units \(IRUs redundancy and all the thrusters are placed on a single \nbranch. The reactions wheels are oriented and sized so that a \nfailure in an individual wheel can be tolerated. Each magnetic \ntorque assembly is internally redundant \(via redundant \nwindings in the case of a magnetometer failure. \nSurvival heaters and many instrument slip rings are \nredundantly wired. All actuators include redundant windings. \nParticular attention has been paid to fault protection design to \nreduce the likelihood and mission impact of specific faults \nand also to minimize the number of fault events that cause the \ninstrument to despin. The observatory is designed to robustly \nand autonomously recover attitude following the momentum \nchange associated with a despin, but the return to science noperations is a longer process resulting in undesirable science \ndata loss. For this reason, the instrument remains spinning for \nall but the most severe faults \(Figure 16 Table 5. \nFigure 15a. Early solar array configuration was constrained by \nboth instrument and telecom antenna FOVs. \nFigure 15b. Final solar array simplified to reduce panels and \ndeployments; allowed slight penetration into instrument \nantenna FOVs. \n  12\n7. ADAPTING PLANETARY AVIONICS \nSMAP’s avionics design is derived from the Mars Science \nLaboratory \(MSL selectively apply elements \nof MSL’s design to avoid unnecessary complexity for \nSMAP’s application while also minimizing the amount of \nnew design and development required. In the end, SMAP was \nable to substantially reduce the hardware complement relative \nto the MSL design and also limit the number of new board \ndesigns required. Another challenge was to adapt MSL’s fully \nredundant design to a single string application.  \nTo minimize new avionics development, the most significant \ncapability changes were addressed by the development of the \nNVM card. Specific challenges that this card addressed were \n\(1 read/write capability to \nsupport simultaneous science acquisition \(6 Mbps for \nradiometer and 40 Mbps for radar 130 Mb/s 2 data storage \(128 GB 3 BER 10-9 ndata loss. The NVM leveraged a similar architectural design \ndeveloped for the MSL MastCAM instrument however, the \nMSL NVM only had 4 GB storage and a 6 Mbps transport \nrate. Using the NVM avoided adding a separate solid state \nrecorder within the bus, which would have proved difficult to \npackage given the volume constraints. The new NVM card is \nshown in Figure 17. \nA new 128 GB flash memory chip was selected for the NVM, \nwith a layout compatible with the older chip used in MSL's \nNVM card design. A large lot of these new parts was \nprocured early in Formulation and subjected to extensive \nenvironmental, radiation, and life qualification testing. Test \nresults showed that the actual BER performance was orders of \nmagnitude better than required for the SMAP application \n[17]. This, in turn, enabled SMAP to implement the NVM \nwith a relatively simple error detection and correction \napproach rather than a more complex approach recommended \nby the manufacturer for other applications. Beyond the \nmission-specific results for SMAP, the work conducted to nqualify these parts contributed to the recently published Flash \nQualification Guideline for Space Application [18]. Early \ntesting on an MSL EM MASTCam NVM card showed the \ndata throughput capability to be 130 Mb/s. SMAP set this as a \nconstraint on the maximum downlink science data rate to \navoid further significant design effort associated with this \ncard. Further design refinements during development yielded nimprovements in data throughput; the engineering model \nNVM demonstrated a 280 Mb/s throughput capability nproviding significant margin over SMAP’s requirement.  \n8. EMI/EMC \nGreat attention was given in the design of the observatory \nfor EMC with all the expected electromagnetic \nenvironments including the self-generated environments. \nObservatory radiated emissions could adversely affect \nradiometer science performance therefore, the most severe \nEMC requirement limits \(notches Figure 18 and launch vehicle systems.  \nDuring Formulation, SMAP established an EMC Design \nControl Plan [19] to provide design guidance to ensure \nrequirements compliance. New designs incorporated best \npractices to reduce the likelihood of generating L-band \nemissions. In electronics, clock transitions were slowed and \ngrounding paths were made as short as practical. Energy \ngenerated that could become radiated is contained within a \ncomplete Faraday cage created by sealed electronics boxes \nTable 5. Observatory mass and power breakdown \(reflects \nCDR 


estimates with growth contingency applied from Table 4 nSpacecraft \(dry protection is designed to \nreduce faults that spin down the instrument. \n \nFigure 17. New NVM card for SMAP n  13\nand shielded cabling. The spacecraft structure could not be \nan adequate Faraday cage due to the large number of \npenetrations required. \nElectronics boxes were fabricated without joints where \npossible close-outs were sealed with EMI gaskets or EMI \ntape. Assemblies that were constrained to use high-density nD-connectors that leak emissions at L-band were treated \nwith conductive overwrap to provide a complete covering. \nPedestals for connectors on the C&DH assembly were \nadded to aid overwrap application, and a cover plate was \nadded to seal the gaps between faceplates of the individual \ncards in the box \(Figure 19 added other provisions to address EMC including a \ncabling design that assures a continuous Faraday cage nbetween electronics boxes using twisted wire pairs with a \nshielded jacket \(TPSJ TPSJ \nwith Laird tape provides better shielding at L-band than \nTPSJ with copper braid overwrap. Because radiated \nemissions can be directional and connectors are often a \nsource, the ICE was configured so that the connectors \npointed away from the reflector to further attenuate \nemissions that could reflect interfere with science \ninstruments. The BAPTA was identified by modeling to be a \nlikely source of leakage; therefore, the observatory structure \nabove the BAPTA was designed as a Faraday cage and the \nrotation pathway for emissions was directed into the \nspacecraft interior where any emissions would less likely \ninterfere with science measurements. \nExisting inherited designs \(most of the commercial space \nassemblies used on SMAP accepted without \nadditional modifications for EMC. EMC requirements were \nlevied on these assemblies but with the recognition some \nmay be found non-compliant. In these cases, SMAP will use \ntraditional EMI/EMC control techniques during integration \nand test in order to control emissions \(adding EMI tape or \ncloth to cover possible areas of leakage performs the \nkey on-orbit operations needed to implement the conical \nscanning scheme employed for data acquisition by the radar \nand radiometer. The observatory uses a zero momentum bias, \ndual-spin architecture to rotate its large antenna at a spin rate \nof 13–14.5 rpm, while the spacecraft bus provides a three-axis ncontrolled platform that maintains both itself and the \ninstrument section’s spin axis in a nadir-pointed orientation. \nThe major pointing and control functional aspects and design \ncharacteristics are illustrated in Figure 20. A key challenge \nFigure 18. EMC requirement in the radiometer band for \nassemblies outside the spacecraft structure. Inside the \nstructure, the lower limit is relaxed by 6 dB. \n \nFigure 20. SMAP pointing and control system functional aspects and design characteristics. \nFigure 19. Example of pedestals for the connectors on the \nC&DH assembly as well as a cover plate to seal the gaps \nbetween faceplates of the individual cards in the box. \n  14\nwith the spinning antenna stems from its large spin axis \nmoment of inertia, which at almost 240 kg-m2 is larger than \nthat of the spacecraft bus at about 190 kg-m2. \nFigure 21 shows the sensor and actuator suite and locations. \nThis control system configuration was informed by a prior ndesign concept for the Navy Remote Ocean Sensing System \n\(NROSS  similar \nsized-rotating antenna and nadir-pointing scheme \(NROSS \ndevelopment was halted after Preliminary Design Review in \nthe 1980s management, with \nmomentum compensation for the spun side and three-axis \ncontrol accomplished with a single set of four Reaction \nWheel Assemblies \(RWAs momentum compensation. In \na recent operational example of a nadir-pointing observatory \nemploying a momentum-compensated rotating antenna, \nWindSat/Coriolis, a dedicated momentum wheel was also \nused to counteract the antenna’s angular momentum, in a \nmanner similar to that planned for NROSS [21]. SMAP’s napproach provides a degree of functional redundancy for \nwheel failure and reduces control complexity nFigure 22 shows the system architecture, encompassing the \nattitude and spin rate determination functions attitude \ncontrol modes for both RWA and Reaction Control System \n\(RCS momentum \ncompensation, and the torque rod-based scheme employed \nfor RWA momentum management. For translational \nmaneuvers \(needed for orbit altitude maintenance used due to the \nmuch larger control authority offered by the thrusters. For \nnominal mapping operations, this system can control nadir \npointing errors due to precession and nutation to within \n0.5 deg, with a stability tolerance of ±0.3 deg \(3  and control is accommodating the flexible \nmodes, especially for the large antenna and its supporting \nboom while simultaneously controlling the antenna spin \nrate and nadir orientation to within the required tolerances. \nThis has been accomplished via careful engineering of the \nprimary frequencies associated with these various elements, \nto ensure adequate separation and avoid the potential for \ninterference or undesired 


resonance effects. Frequency \ndistribution of these system components is shown in Figure \n23 to illustrate this aspect of the design. The minimum 1st \nflexible mode frequencies of the solar array, as well as the nantenna and boom, became key design requirements on the \nstructure to ensure adequate separation from the spin control \nsystem and the observatory’s attitude control bandwidth. \n \nFigure 22. Pointing and control system architecture. \n \nFigure 21. Observatory sensor and actuator description.\n  15\n10. LAUNCH VEHICLE CHALLENGES \nThe launch vehicle selection process for SMAP involved \nboth programmatic and technical challenges to develop an \nobservatory design that was compatible with several \npotential medium- and large-class launch vehicles and to \naccommodate a launch vehicle selection late in the design \nlifecycle without stretching the development schedule or \ndelaying the launch ready date. SMAP’s strategy effectively \nisolated the observatory design from launch vehicle \nuncertainty by applying very conservative bounding design \nloads and environments designing to the most constraining \nfairing volume, and by developing a flexible launch vehicle \nadapter design to address vehicle-specific interface and \nmission design differences after launch vehicle selection nWhen SMAP began Formulation in September 2008, there \nwas a dearth of suitable, affordable medium-class launch \nvehicles commercially available [23]. SMAP was too \nmassive to be lofted on a Taurus XL. Delta II production \nhad been halted. The Falcon 9 had not yet flown. Evolved \nexpendable-class launch vehicles \(EELVs were costly and \ngrossly over-capable for SMAP, and SMAP’s tall stowed \nconfiguration effectively eliminated it as a co-manifest \noption on an EELV. The situation was further complicated \nbecause NASA’s Launch Services Program \(LSP NLS-2 new contract would be in \nplace and which vehicles would be initially available in the \ntimeframe SMAP expected a launch vehicle selection. \nThe Project also simultaneously explored whether a \npartnership with the DOD Space Test Program \(STP DOD agencies have high interest in \nusing SMAP data in their applications [3] and were strong \nadvocates to the STP for such a partnership. Under such a \npartnership, the DOD might have earlier access to critically nneeded soil moisture and freeze-thaw data to support their \napplications and NASA would realize a substantially lower \nmission cost. STP has occasional access to EELVs and more \nfrequent access to Minotaur IV vehicles. The Minotaur IV \nuses components with a significant flight heritage such as \nsurplus Peacekeeper stages, a Taurus fairing and attitude \ncontrol system, and a mix of Minotaur I, Pegasus, Taurus, \nand other Orbital Sciences standard avionics and software \n[24]. The use of surplus Peacekeeper stages means the nvehicle was defined by NASA to be a non-commercial \nlaunch system under the Commercial Space Act and ntherefore, it is not available for NASA acquisition. The Act \nallows for Minotaur use by DOD under certain ncircumstances that appeared potentially allowable for SMAP \nunder a DOD partnership. The Minotaur IV had not yet \nflown at that time, but it appeared to be a good potential \nmatch for SMAP. The potential cost benefit to NASA of a \nDOD partnership was substantial, especially given the \nuncertainties in the commercial acquisition process, and for \na time, the DOD partnership track seemed like the most \nlikely prospect for a launch vehicle for SMAP. For these \nreasons, SMAP strove to develop an observatory that \nmaintained Minotaur IV compatibility \(the “+” variant \nreplaces the Orion 38 upper stage with a Star 48 which \nprovides more lift capability preliminary and then \ndetailed final design until CDR in July 2012, when NASA \nannounced selection of a Delta II launch vehicle for SMAP. \nPersistent launch vehicle uncertainty drove SMAP towards \nmission and observatory designs that maintained \ncompatibility with several launch vehicles: Minotaur IV+, \nAtlas V, Falcon 9 and then later the Delta II when it was on-\nramped onto NLS-2 in October 2011. In most aspects, the \nMinotaur IV represented the most constraining vehicle \nchoice. The largest drivers were its 92-in fairing, and its lift ncapability constraint and non-restartable upper stage, which \nlimited its final orbit insertion capability for SMAP. \nTypically in early formulation before launch vehicle \nselection, missions employ enveloping and highly margined \ndesign environments for compatibility with likely launch \nvehicles. Launch vehicle selection is typically completed by \nmission PDR so that as the observatory enters the final \ndesign phase, expected launch load requirements decrease \naround the selected vehicle and maturing coupled loads and \nmission analyses. The easing of requirements allows fewer \ndesign iterations \(margin is ‘cashed in’ to address problems \n \nFigure 23. Frequency separation is the key to meet stability and performance requirements while not responding to \ndisturbances.  \n  16\nthat arise in the detailed design and highly margined loads environment \npersisted through the entire design lifecycle, which in turn \ndrove more design iterations to insure the design was \ncompatible with the higher loads cases. \nThe 92-in fairing constraint significantly drove the \nobservatory packaging to achieve a compact design. The \nspacecraft bus underwent several design iterations to package \navionics, power, and radar electronics designs as the 


nassociated electronics packaging designs matured, while also \naccommodating commercial space assemblies for other \nsubsystems [transponders, transmitters, miniature inertial \nmeasurement unit \(MIMUs etc The packaging \nrequirements contributed to decisions to develop the NVM \nslice within the C&DH to avoid having to accommodate a \nseparate solid state data recorder assembly. Harness design \nwas a challenge to accommodate difficult bends and close \nclearances. The structure design was also optimized to reduce \nmass to insure healthy margins against the Minotaur IV+ lift \ncapability. The spacecraft structure was designed with 7050 naluminum alloy and the structure includes a number of \nmachined cutouts and thin walls that are time-consuming to \nfabricate. Thermal control was also a challenge in such a \nsmall bus due to the power dissipations \(~1400 W complexity; but nearly every available exterior \nsurface is used as a radiator. \nPerhaps most significantly impacted by the 92-in fairing \nconstraint were the stowed RBA and solar array packaging. \nThe RBA has been the most sensitive assembly to launch \nvehicle uncertainty, undergoing several design iterations to \naddress stowed packaging and launch loads, and thermal \ndesign iterations to address evolving launch phase mission ndesign scenarios. The RBA along with the spin assembly is \nso intimately coupled to the observatory’s fundamental \narchitecture that these contracts were initiated early in Phase \nA; however the RBA is now the last flight assembly to be \ndelivered to observatory integration and test, less than one \nyear before launch because the start of flight manufacturing \nwas delayed until the design work could be stabilized and nconfirmed for the selected launch vehicle. \nThe solar array design was also driven by the 92-in fairing nconstraint. Unusually, SMAP’s solar array has no exposed \ncells when stowed. Because of the small bus size the \nadjacent sizes of the spacecraft to the solar array central \npanel are ‘busy’ and close clearances to the fairing envelope \nwould not allow for a ‘wrap around’ solar panel stowage \napproach. The solar array folds over on itself, placing the \noutboard cells facing inward. This feature places high \nemphasis on ensuring there are robust battery energy \nmargins available to accommodate solar array deployment \ncontingencies following launch vehicle separation. \nThe mission design for orbit insertion was also developed \ninitially around the Minotaur IV+ vehicle capability. The \nMinotaur IV+ upper stage is not restartable and to naccommodate disposal requirements, the initial target \nspecification placed the upper stage/observatory in an nelliptical orbit \(566 x 664 km timeframe, but \ndrove the spacecraft propulsion system to accommodate the \nadditional delta V \(and propellant capacity 685 km also enabled the maximum achievable \ndry mass on-orbit for the observatory, subject to the need for \n80-kg propellant tank capacity on board the spacecraft. \nTo accommodate the Delta II selection, the existing Launch nVehicle Adapter \(LVA interfaces \n\(Figure 24 upper stage is restartable, \nthe mission design was adapted to directly inject SMAP into \nits final science orbit. Additional secondary batteries were \nadded to the LVA to provide appropriate power margins \nduring the longer coast phase until separation and solar \narray deployment places the spacecraft on internal power n\(Minotaur IV+ injection time was 16 minutes, Delta II is 60 \nminutes increased mass and fairing \nvolume resources than the Minotaur IV+; unfortunately, \nwhen the Delta II was selected, the SMAP design was \n \nFigure 24. SMAP’s launch vehicle adapter design \naccommodates changes needed for late launch vehicle \nselection. \n  17\nmature and many subsystems were well into fabrication. It \nwas not cost effective to iterate through another design cycle \nto take advantage of the additional resources. The added \ndesign and development effort to remain compliant with the \nMinotaur IV+ have ultimately allowed SMAP to \naccommodate a late launch vehicle selection without \nstretching out the development schedule or launch ready \ndate, thereby allowing SMAP to retain its October 2014 \nlaunch ready date despite the launch vehicle uncertainty. \n11. LESSONS LEARNED \nSMAP has significantly benefited from the experience and \nlessons from Aquarius and SMOS for the science \nmeasurements and instrument design, and from MSL for the \navionics and power subsystem architectures. Personnel from \nthese predecessor missions were engaged early in SMAP’s ndevelopment so that their insight and lessons learned could \nbe integrated into the early architectural design stages. \nKey lessons learned from SMAP include: \n\(1 nplanned scenarios very early with regulatory functions \nand key stakeholders. Spaceborne L-band science nradars operate as secondary users within their allocated \nspectrum. The primary users, civil and defense aircraft \nterrestrial navigation systems, are increasingly wary \nabout potential interference from secondary users. \nSMAP conducted significant analysis and testing to \ndemonstrate very low interference risk potential 


and \ncoordinated the results with these users early. SMAP \nmade significant modifications to the radar design and \noperating approach to effectively eliminate risk of \ninterference to primary spectrum users. \n\(2 the launch vehicle early; this is particularly \nimportant for new observatory designs where the \nnumber of design iterations can be driven by assumed \nconstraints and conservative bounding environments \n\(especially if they persist into detail design packaging, repackaging to \nremain within fairing envelope, for instance iterations, and mission design \niterations to accommodate various launchers before a \nfinal launch selection was made. This was unavoidable \nfor SMAP given SMAP’s launch vehicle and other \ncircumstances.  \n\(3 Integrated observatory design—spacecraft and \ninstrument—SMAP is a highly integrated design that \nuniquely leverages spacecraft capabilities to simplify \ninstrument design and operation. This resulted in lower noverall design complexity and has also allowed for a \nmore reliable design by reducing the number of npossible major failure modes.  \n\(4 new design insights incrementally \nunfold and thereby often reveal new issues to be \nresolved within the design. Occasional team ‘resets’ \nwere imposed to reassess local requirements and design \ncomplexity within the context of the overall observatory \ndesign. These ‘resets’ often resulted in significant \nsimplification and in reduced development risk and \nuncertainty. \n\(5 napply resources to mature these design areas, and \nqualify critical essential electronics parts and nsubassemblies to reduce downstream risk. SMAP \nsuccessfully did this, aggressively applying resources to nradar, radiometer, avionics, spin/dynamics and control, \nand the RBA—the key development challenges for the nmission. As described earlier, the RBA and spin \nassemblies’ designs were fundamental to the \narchitectural definition and to enabling preliminary \ndesign to proceed, so these were selected early in \nFormulation and placed under contract. This reduced \nsubsystem architectural- and system-level redesign \ncycles that can set back progress. \n12. CONCLUSION \nThe SMAP observatory has been carefully designed to \naddress a number of unique challenges posed by the mission \nobjectives: \n? Achieving global coverage every 2–3 days with a single ninstrument and observatory \n? Achieving both high resolution and high soil moisture \naccuracy \n? Minimizing data loss or corruption from L-band \nterrestrial RFI \n? Using a deployable mesh reflector for L-band nradiometric measurements \n? Mechanical packaging of the large instrument antenna \nand spacecraft, with its associated structural design \ncompatibility with several small-to-medium class launch \nvehicles and to accommodate a relatively late vehicle \nselection without delaying launch. \n? Fault protection approach to minimize science \nmeasurement on-orbit down-time \n? Adapting planetary heritage avionics to an Earth science nmission application \n? Design for EMC to avoid L-band emissions that could \ndegrade science measurements \n Dynamics and pointing control of a large deployable \nspinning reflector \nThe design ensures that SMAP will provide high-quality \nscience data. \n  18\nREFERENCES \n[1] “Earth Science and Applications from Space: National nImperatives for the Next Decade and Beyond,” \nCommittee on Earth Science and Applications from \nSpace: A Community Assessment and Strategy for the \nFuture, National Research Council, The National \nAcademic Press 2007. ISBN-10: 0-309-14090-0, \nISBN-13: 987-0-309-14090-4. \n[2] JPL SMAP website: http://smap.jpl.nasa.gov n[3] Entekhabi, D., E. Njoku, P. O’Neill, K. Kellogg, et al., \n“The Soil Moisture Active Passive \(SMAP Mission,” \nProceedings of the IEEE, vol. 98, no. 5, May 2010. \n[4] Spencer, M., K. Wheeler, C. White, R. West J. \nPiepmeier, D. Hudson, and J. Medeiros, “The Soil \nMoisture Active Passive \(SMAP nRadar/Radiometer Instrument,” IEEE International \n2010 Geoscience and Remote Sensing Symposium \n\(IGARSS n[5] Camps, A., J. Gourrion, J. M. Tarongí, A. Gutiérrez, J. \nBarbosa, and R. Castro, “RFI Analysis in SMOS nImagery,” Proceedings of the 2010 IEEE International \nGeoscience and Remote Sensing Symposium, Honolulu, \nHI USA, July 2010, pp. 2007–2010. \n[6] Ruf, C., D. D. Chen, D. Le Vine, P. Matthaeis, and J. \nPiepmeier Aquarius Radiometer RFI Detection, \nMitigation and Impact Assessment,” Proceedings of the \n2012 IEEE International Geoscience and Remote \nSensing Symposium, Munich, Germany, July 2012. \n[7] Bradley, D., C Brambora, M. E. Wong, et al., “Radio-\nFrequency Interference \(RFI Active/Passive \(SMAP IGARSS International, July 2010. \n[8] Chan, S., M. Fischman, and M. Spencer, “RFI \nMitigation and Detection for the SMAP Radar,” \nGeoscience and Remote Sensing Symposium \n\(IGARSS  Spencer, M., S. Chan, E. Belz, J. Piepmeier, P. \nMohammed, and J. Johnson, “Radio Frequency \nInterference Mitigation for the Planned SMAP Radar \nand Radiometer,” Geoscience and Remote Sensing \nSymposium \(IGARSS 2011 IEEE International, pp. \n2440–2443. \n[10]  Jones, C., et al., “Analysis of Potential Interference \nfrom SMAP Radar Transmissions into FAA ARSR-3, \nARSR-4, and CARSR Radars in the 1215–1300 MHz \nBand and Interference Avoidance Strategies,” Jet \nPropulsion Laboratory internal document. \n[11] Huneycutt, B., S 


Hensley, G. Purcell, et al., “Test \nReport on the Effects of Pulsed Interference from L-\nband Spaceborne and Airborne Radars on GPS L2 \nReceivers,” Jet Propulsion Laboratory internal \ndocument, Nov. 1, 2010. \n[12 Sanders, F., et al., “NTIA Study regarding EMC \nbetween Proposed NASA SMAP Orbital Radar System \nand FAA ARSR Systems,” NTIA/ITS. \n[13] Mobrem, M., S. Kuehn, C. Spier, and E. Slimko, \n“Design and Performance of Astromesh Reflector \nOnboard Soil Moisture Active Passive Spacecraft,” \nIEEE 2012 Aerospace Conference, March 2012 n[14] Mohammed, P., SMAP L1B Radiometer Data Product \nAlgorithm Theoretical Basis Document, GSFC. \n[15 Spencer, M. W., C. W. Chen, H. Ghaemi, S. F. Chan, \nand J. E. Belz., “RFI Characterization and Mitigation \nfor the SMAP Radar,” TGARS, to be published. \n[16] West, R., SMAP L1 Radar Data Products Algorithm \nTheoretical Basis Document, Jet Propulsion Laboratory. \n[17] Heidecker, J., M. White, M. Cooper, D. Sheldon, F. \nIrom, and D. Nguyen, “Qualification of 128 Gb MLC \nNAND Flash for SMAP Space Mission,” Integrated \nReliability Workshop Final Report \(IRW  nQualification Guideline for Space Application” JPL \nPublication 12-1, Jet Propulsion Laboratory, Pasadena nCA, 2012. \n[19] Newson, S. L., and C. H. Huang, “SMAP \nElectromagnetic Compatibility \(EMC nPlan,” Jet Propulsion Laboratory Internal Project \nDocument, April 14, 2010. \n[20] Mak, P. H., “Pointing and Stabilization Issues of Large \nSpinning Antennas,” Proceedings of IEEE Position \nLocation and Navigation Symposium, Navigation into \nthe 21st Century, IEEE Plans, pp. 230–235, 1988. \n[21] Gaiser, P. W., K. M Germain, and E. M. Twarog, \n“WindSat—Spaceborne Remote Sensing of Ocean \nSurface Winds,” Proceedings of IEEE Oceans \nConference, vol. 1, p. 280, 2003. \n[22] Alvarez-Salazar, O. S., D. Adams, M. Milman, R. \nNayeri, S Ploen, L. Sievers, E. Slimko, and R. \nStephenson, “Precision Pointing Architecture of \nSMAP’s Large Spinning Antenna,” IEEE 2010 \nAerospace Conference, Big Sky, MT, to be published. \n[23] “Review of NASA’s Acquisition of Commercial \nLaunch Services,” NASA Office of Inspector General, \nFebruary 17, 2011. \n[24] “Minotaur IV Users Guide,” Release 1.1, Orbital \nSciences Corporation, January 2006. \n  19\nBIOGRAPHIES \nKent Kellogg received a B.S. in \nElectronic Engineering from \nCalifornia Polytechnic State \nUniversity, San Luis Obispo in n1983. He joined JPL’s Spacecraft \nAntenna Group in 1983 and is \ncurrently SMAP Project Manager. \nHis prior roles include managing \nJPL’s Telecommunication, Radar \nand Tracking Division, Spacecraft \nTelecommunications Equipment Section, SeaWinds/\nQuikSCAT Instrument and Project and supervising JPL’s \nSpacecraft Antenna Group nDr. Sam Thurman received B.S., \nS.M., and Ph.D. degrees in \nAerospace Engineering from \nPurdue University 1983 1985 1995 respectively. He jointed \nJPL in 1987 and is currently the \nSMAP Deputy Project Manager. \nHis prior roles include Deputy Manager of JPL’s \nAutonomous Systems Division, and several system \nengineering and management assignments in the Mars \nExploration Program. Before joining JPL, he worked as a \nmissile guidance analyst at the Charles Stark Draper \nLaboratory in Cambridge, Massachusetts.\t\nWendy Edelstein received her B.S. \ndegree in Electrical Engineering \nwith minor in Applied Mathematics \nfrom the University of California, \nSan Diego in 1988. She joined JPL \nin 1988 and is currently the SMAP \nInstrument Manager. Her prior \nroles include Deputy Section \nManager of the Radar Science & \nEngineering Section, Group \nSupervisor for the Advanced Radar Technology Group and \nradar technologist where her primary research interests \nincluded Transmit/Receive modules, RF component \nminiaturization and lightweight antenna technologies. \nDr. Michael Spencer received the nB.S. degree in Physics from the \nCollege of William and Mary, the \nM.S. degree in Planetary Science \nfrom The California Institute of \nTechnology, the M.S. degree in \nElectrical Engineering from the \nUniversity of Southern California, \nand a Ph.D. in Electrical and \nComputer Engineering from \nBrigham Young University. He joined JPL in 1990 and has\nserved as the SMAP Instrument System Engineer. He is \ncurrently the Deputy Manager of JPL’s Radar Science and \nEngineering Section; he is also the SMAP Instrument \nScientist. \nDr. Gun-Shing Chen received the \nB.S. degree in Mechanical \nEngineering from Cheng-Kung \nUniversity, Taiwan in 1977; an nM.S. degree in Aerospace \nEngineering from University of \nTexas at Austin in 1981; and a \nSc.D degree in Aeronautics and \nAstronautics from Massachusetts \nInstitute of Technology in 1986. He \njoined JPL’s Structures and Dynamics Research Group in \n1986 and is currently the SMAP Flight System Manager.\nHis prior roles include Group Supervisor, Section \nManager, Assistant Division Manager in the Mechanical \nSystem Division, Chief Engineer and Assistant Division \nManager in the Instruments and Science Data Systems \nDivision, Mechanical Project Element Manager \(PEM MICAS Microwave Limb Sounder \n\(MLS MER AMT received a \nB.S. Engineering from UCLA in \n1981 and Ph.D. in Chemical \nEngineering at the University of nPennsylvania in 1987. He joined \nJPL’s Spacecraft Power Section in \n1987 and is currently the SMAP \nMission Assurance Manager and is \nresponsible for safety, EEE parts, \nenvironments, reliability, software \nand 


hardware quality assurance. His prior roles include \nGroup Supervisor for the Power Electronics and Systems nGroup and Manager of the Power Systems Section. Prior to\njoining SMAP, he was the Deputy Mission Assurance nManager for the Juno project.  \n  20\nShawn Goodman received an M.S. \nin Mechanical Engineering from nRensselaer Polytechnic Institute. \nHe joined JPL’s Spacecraft \nMechanical Engineering Section in \n1991 and is currently SMAP \nProject System Engineer. His prior \nroles have included Group \nSupervisor, Deputy Section nmanager and Section manager of \nthe Spacecraft Mechanical Engineering Section. He has \nworked a number of flight projects including MSTI, \nSojourner, MGS, SIR-C, DS1, X2000, OPSP, MER, and \nMSL.\t\nBenhan Jai received his Masters \ndegree in Computer Engineering \nfrom the University of Southern \nCalifornia. He joined JPL in 1985 \nwhere he served in a number of \nengineering and management roles \ninvolving ground data system ndevelopment, mission operations \nsystem development, and mission \nmanagement. He is currently the \nSMAP Mission Systems Manager. His previous assignments \nhave included the development of the Ground Data System nand the Mission Operations System on many missions \nincluding NASA Scatterometer, SeaWinds, Mars Global nSurveyor, Stardust, Mars Odyssey, and Mars Surveyor \nOperations Projects. Prior to joining SMAP, Ben led the ndevelopment of mission operations and ground data \nsystems of the Mars Reconnaissance Orbiter Project \n\(MRO and led the entire flight teams of the MRO through \nlaunch, cruise, MOI, and aerobraking to the completion of nthe primary science phase of the mission.\t\nDr. Eni Njoku received the B.A. \ndegree in natural/electrical nsciences from the University of \nCambridge, U.K., in 1972 and the \nM.S. and Ph.D. degrees in \nelectrical engineering from MIT in \n1974 and 1976, respectively. He \njoined JPL in 1977 and he is \ncurrently the SMAP Project \nScientist, a JPL senior research \nscientist, and also supervisor of the Water and Carbon \nCycles Group. His prior roles have included managing \nJPL’s Geology and Planetology Section and serving as ndiscipline scientist for ocean and earth science data \nsystems at NASA. His primary research interests are in ndeveloping microwave remote sensing techniques for land \nsurface hydrology and climate. His current research nincludes microwave modeling, retrieval algorithm \ndevelopment, field experiments, and data analysis using nradiometers and radars. \nACKNOWLEDGEMENTS  \nThe work described in this paper was carried out at the Jet nPropulsion Laboratory, California Institute of Technology, \nunder a contract with the National Aeronautics and Space \nAdministration. Part of this work was also carried out at the \nNASA Goddard Space Flight Center. \n n \n 


Reinhartz-Berger, and A. Sturm, “OPCAT-\na bimodal CASE tool for object-process based system\ndevelopment,” in 5th International Conference on En-\nterprise Information Systems \(ICEIS 2003  Olsen, R. Haagmans, T. J. Sabaka, A. Kuvshinov,\nS. Maus, M. E. Purucker, M. Rother, V. Lesur, and\nM. Mandea The Swarm End-to-End mission simulator\nstudy : A demonstration of separating the various con-\ntributions to Earths magnetic field using synthetic data,”\nEarth, Planets, and Space, vol. 58, pp. 359–370, 2006.\n[10] R. M Atlas, “Observing System Simulation Exper-\niments: methodology, examples and limitations,” in\nProceedings of the WMO Workshop on the Impact of\nvarious observing systems on Numerical Weather Pre-\ndiction, Geneva Switzerland, 1997.\n[11] M. Adler, R. C. Moeller, C. S. Borden, W. D. Smythe,\nR. F. Shotwell, B. F. Cole, T. R Spilker, N. J.\nStrange, A. E. Petropoulos, D. Chattopadhyay, J. Ervin,\nE. Deems, P. Tsou, and J. Spencer Rapid Mission\nArchitecture Trade Study of Enceladus Mission Con-\ncepts,” in Proceedings of the 2011 IEEE Aerospace\nConference, Big Sky, Montana, 2011.\n[12] J. Hauser and D. Clausing, “The house of quality,”\nHarvard Business Review, no. May-June 1998, 1988.\n[13] T. L. Saaty, “Decision Making With the Analytic Hier-\narchy Process,” International Journal of Services Sci-\nences, vol. 1, no. 1, pp. 83–98, 2008.\n[14] A. M. Ross, D. E Hastings, J. M. Warmkessel, and\nN. P. Diller, “Multi-attribute Tradespace Exploration as\nFront End for Effective Space System Design,” Journal\nof Spacecraft and Rockets, vol. 41, no. 1, 2004.\n[15] W. L. Baumol On the social rate of discount,” The\nAmerican Economic Review, vol. 58, no. 4, pp. 788–\n802, 1968.\n[16] M. K Macauley, “The value of information : Measuring\nthe contribution of space-derived earth science data to\nresource management,” Journal of Environmental Eco-\nnomics and Management, vol. 22, pp. 274–282, 2006.\n[17 S. Jamieson, “Likert scales: how to \(ab Dec.\n2004.\n[18] R. C. Mitchell and R. T. Carson, Using Surveys to\nValue Public Goods: The Contingent Valuation Method,\nS. Aller, Ed. Washington DC: Library of Congress,\n1989.\n[19] O. C. Brown, P. Eremenko, and P. D Collopy, “Value-\nCentric Design Methodologies for Fractionated Space-\ncraft: Progress Summary from Phase 1 of the DARPA\nSystem F6 Program,” AIAA SPACE 2009 Conference &\nExposition, 2009.\n[20] a. Stoffelen, G. J Marseille, F. Bouttier, D. Vasiljevic,\nS. de Haan, and C. Cardinali, “ADM-Aeolus Doppler\nwind lidar Observing System Simulation Experiment,”\nQuarterly Journal of the Royal Meteorological Society,\nvol. 132, no. 619, pp 1927–1947, Jul. 2006.\n[21] A. Newell and H. A. Simon, Human Problem Solving.\nEnglewood Cliffs, NJ: Prentice Hall, 1972.\n[22] R. Lindsay, B. G. Buchanan, and E. A. Feigenbaum,\n“DENDRAL: A Case Study of the First Expert System\nfor Scientific Hypothesis Formation,” Artificial Intelli-\ngence, vol. 61, no. 2, pp. 209–261, Jun 1993.\n[23] B. G. Buchanan and E. H. Shortliffe, Rule-based Ex-\npert Systems: the MYCIN experiments of the Stanford\nHeuristic Programming Project. Addison-Wesley,\n1984.\n[24] P. Hart, R. Duda, and M. Einaudi PROSPECTORA\ncomputer-based consultation system for mineral explo-\nration,” Mathematical Geology, no. November 1977,\n1978.\n[25] J. McDermott, “R1: A Rule-Based Configurer of Com-\nputer Systems,” Artificial lntell., 19 39, vol. 19, no. 1,\npp. 39–88, Sep. 1982.\n[26] K. J. Healey, “Artificial Intelligence Research and Ap-\nplications at the NASA Johnson Space Center,” AI\nMagazine, vol. 7, no. 3, pp. 146–152, 1986.\n[27] C Forgy, “Rete: A fast algorithm for the many pat-\ntern/many object pattern match problem,” Artificial in-\ntelligence, vol. 19, no. 3597, pp. 17–37, 1982.\n[28] L. A. Zadeh, “Fuzzy Sets,” Information and Control,\nvol. 8, no. 3, pp. 338–353, Jan. 1965.\n[29] C. Haskins, “INCOSE Systems engineering handbook -\nA guide for system life cycle processes and activities,”\nSystems Engineering, 2006.\n[30] N. Das, D. Entekhabi and E. Njoku, “An Algorithm for\nMerging SMAP Radiometer and Radar Data for High-\nResolution Soil-Moisture Retrieval,” IEEE Transactions\non Geoscience and Remote Sensing, vol. 49, no. 99, pp.\n1–9, 2011.\n[31] A. Messac and A. Ismail-Yahaya, “Multiobjective ro-\nbust design using physical programming,” Structural\nand Multidisciplinary Optimization, vol. 23, no. 5, pp.\n357–371, Jun. 2002.\n20\n[32] B. Cameron, “Value flow mapping: Using networks\nto inform stakeholder analysis,” Acta Astronautica,\nvol. 62, no. 4-5, pp. 324–333 Feb. 2008.\n[33] R. Yager, “On ordered weighted averaging aggregation\noperators in multicriteria decision making,” Systems ,\nMan and Cybernetics, IEEE Transactions on, no. 1, pp.\n183–190, 1988.\n[34] J. Fortin, D Dubois, and H. Fargier, “Gradual Num-\nbers and Their Application to Fuzzy Interval Analysis,”\nIEEE Transactions on Fuzzy Systems, vol. 16, no. 2, pp.\n388–402, Apr. 2008.\n[35] H. Apgar, “Cost Estimating,” in Space Mission Engi-\nneering: The new SMAD. Hawthorne, CA: Microcosm,\n2011, ch. 11.\n[36] Chalmers University of Technology, “Use of P-band\nSAR for forest biomass and soil moisture retrieval,”\nEuropean Space Agency, Tech Rep., 2004.\n[37] D. Selva, “Rule-based system architecting of Earth\nobservation satellite systems,” PhD dissertation, Mas-\nsachusetts Institute of Technology, 2012.\n[38] H. H. Agahi, G. Ball, and G. Fox, “NICM Schedule &\nCost Rules of Thumb,” in AIAA Space Conference 2009,\nno. September, Pasadena, CA, 2009, pp 6512–6512.\nBIOGRAPHY[\nDaniel Selva received a PhD in Space\nSystems from MIT in 2012 and he is\ncurrently a post-doctoral associate in\nthe department of Aeronautics and As-\ntronautics at MIT. His research inter-\nests 


focus on the application of multi-\ndisciplinary optimization and artificial\nintelligence techniques to space systems\nengineering and architecture, in partic-\nular in the context of Earth observa-\ntion missions. Prior to MIT, Daniel worked for four years\nin Kourou \(French Guiana particular, he worked as a specialist in\noperations concerning the guidance, navigation and control\nsubsystem and the avionics and ground systems. Daniel has\na dual background in electrical engineering and aeronautical\nengineering, with degrees from Universitat Politecnica de\nCatalunya in Barcelona, Spain, and Supaero in Toulouse,\nFrance. He is a 2007 la Caixa fellow, and received the Nortel\nNetworks prize for academic excellence in 2002.\nEdward F. Crawley received an Sc.D. in\nAerospace Structures from MIT in 1981.\nHis early research interests centered on\nstructural dynamics, aeroelasticity, and\nthe development of actively controlled\nand intelligent structures. Recently, Dr.\nCrawley’s research has focused on the\ndomain of the architecture and design of\ncomplex systems. From 1996 to 2003\nhe served as the Department Head of\nAeronautics and Astronautics at MIT, leading the strategic\nrealignment of the department. Dr. Crawley is a Fellow of\nthe AIAA and the Royal Aeronautical Society \(UK is the\nauthor of numerous journal publications in the AIAA Journal,\nthe ASME Journal, the Journal of Composite Materials, and\nActa Astronautica. He received the NASA Public Service\nMedal. Recently, Prof Crawley was one of the ten members of\nthe presidential committee led by Norman Augustine to study\nthe future of human spaceflight in the US.\n21\n 


Mars program. It would be designed for low mass, lowpower and low temperature operation. The S-band antenna would be a smaller, simpler version of the antenna that flew on Deep Impact. Other antenna options would be available The UHF antennas have been flown on previous Mars lander/rover missions. There would be other alternatives for the S-band antenna and the UHF transceiver on the hub could use a larger power amplifier to talk to an orbiting asset as a backup to the S-band radio  Risk  The highest risk items for telecom would be the single string design for each element and six year design lifetime. However, the S-band radio has flight heritage. The UHF radios would be a new design but do not require new technology. They would be an engineering development 8. SYSTEM SUMMARY Mass Equipment List Table 5 shows a summary of the mass and power for each of the subsystems for the remote instrument units. The mass of one remote unit without the specified instrument is 26.6 kg with contingency specified at the subsystem level based on heritage. Table 6 shows a summary of mass and power by subsystem for the hub. The mass of the hub with contingency is 44.9 kg. Table 7 shows a mass summary for the entire package with appropriate contingencies added per the JPL  s Flight Project Practices and Design Principles Design Principles. The package totals 218.2 kg which includes four remote units, five instruments, one hub, and the carrier container\(s Table 5. Mass and power summary for remote units Remote Unit Mass CBE Contingency Total Power Power 14.3 kg 30% 18.5 kg 0.180 W 2 W Night /Day Thermal 2.0 kg 29% 2.5 kg 0 W Telecom UHF 0.2 24% 0.3 kg 2 W 40 W CDS 0.7 kg 30% 0.9 kg 1 W \(1/60th per hour 3 W Structure 3.4 kg 30% 4.4 kg 0 W Total x 1 unit 20.6 kg 29% 26.6 kg Diplexer S-Band Downconverter STDN command data to S/C CDS Pr oc es so r S-Band Exciter 9 dBi S-Band LGA UHF Downconverter Small UHF transceiver command data to S/C CDS 


to S/C CDS Pr oc es so r UHF PA UHF Monopole Command data to C&amp;DH Command dat  to C&amp;DH  Figure 7  Telecom block diagram for the S-band \(top bottom  units would be located on the hub while the remote units only contain a UHF system 15 9. OPERATIONAL SCENARIOS Daytime Operations During the day, the remote units and hub would be fully operational. The remote units would collect data from their instruments as specified by the science team and store it in the controller memory. Table 8 shows the data volume expected from each instrument. After 24 hours have passed the UHF telecom system on the hub is used to poll each of the remote units separately at the designated interval for the stored data. The hub then transmits the data direct-to-Earth using the S-band radio. This requires a maximum of eight hours at 50 kbps each day using the DSN 34 m antennas However, data rates as high as 120 kbps may be achieved reducing the downlink time. The hub has enough memory margin to accumulate data from all the instruments for three Earth days before it must downlink the data Nighttime Operations During nighttime operations, data collection at the remote units would be taking place. The magnetometer and seismometer collect data continuously. However, the seismometer operates at a reduced mode where the sampling rate is reduced to one-half of the daytime rate which has been deemed more than adequate by the science team. The remaining instruments collect data at various intervals that would be conducive to the science team  s current requirements. Telecom events would not be scheduled during the lunar night. The data accumulates in the controller memory over 16 Earth days \(~14 days at an equatorial location would be considered a worse case so two days have been added to be conservative data volume summary for each instrument during a 16 Earth day lunar night. When the sun comes up and the hub and remote units have sufficient power to run the telecom systems the hub polls each remote unit separately at a designated interval similar to operations during the day. The data would then be transmitted to Earth gradually over the next few days using the S-band radio Table 7. Mass summary for total package Unit Mass Contingency Mass + contingency 4 Remote Units 82.4 kg 29% 106.4 kg Hub 35.2 kg 27% 44.9 kg Instruments including cabling 17.3 kg 30% 22.5 kg Carrier Container\(s Total with heritage contingency 153.1 kg 29% 197.5 kg  System contingency  21.4 kg 14 Total Package  43% 218.9 kg Table 8. Instrument data volumes received at the hub over one Earth day in daylight operations Science Instrument Compressed Data Volume Received at Hub 


Volume Received at Hub Mb Seismometer 236 Magnetometer 58 Heat Flow Probe 2 Seismic Sounder 700 Instrument &amp; Hub Engineering Data 6 Total 1002 Hub Memory 5000 Margin 80  Table 6. Mass and power summary for hub Hub/Base Unit Mass CBE Contingency Total Power Power 14.3 kg 30% 18.5 kg 0.180 W 2 W Night /Day Thermal 13.4 kg 28% 17.2 kg 0 W Telecom UHF Telecom S-band 3.4 15% 3.9 kg 2 W 40 W CDS 0.7 kg 30% 0.9 kg 1 W \(1/60th per hour W \(day Structure 3.4 kg 30% 4.4 kg 0 W Total x 1 unit 35.2 kg 27% 44.9 kg 2.38 W avg at night  16 10. SUMMARY AND CONCLUSIONS The ALGEP modular design builds upon lessons learned from Apollo era ALSEP package and technology advances since that time. ALGEP meets the requirements of long lifetime survival while maintaining continuous operation of its instruments during the lunar night which can last up to 16 days at equatorial regions on the Moon. The package would be powered using solar arrays and batteries alone not requiring nuclear sources to supply power or maintain thermal control. This concept is feasible due to its lowpower operational mode at night The modular design and packaging scheme provides flexibility in deployment across all regions of the Moon including the farside pending the availability of an orbital communications asset. The relatively light ALGEP package could be accommodated on astronaut activity support vehicles, providing a method to distribute the packages across the Moon, ultimately gaining a Moon-wide understanding of lunar geophysical properties ACKNOWLEDGEMENTS This work was supported by the NASA Lunar Sortie Science Opportunities Program The work described in this publication was carried out at the Jet Propulsion Laboratory, California Institute of Technology under a contract with the National Aeronautics and Space Administration References herein to any specific commercial product process or service by trade name, trademark, manufacturer 


or otherwise does not constitute or imply its endorsement by the United States Government or the Jet Propulsion Laboratory, California Institute of Technology REFERENCES 1] NRC  Scientific Context for Exploration of the Moon   Washington D.C.: The Nat. Academies Press, 2007 2] Apollo 11 Prelim. Sci. Rept., NASA SP-214, 1969 3] Apollo 12 Prelim. Sci. Rept., NASA SP-235, 1970 4] Apollo 14 Prelim. Sci. Rept., NASA SP-272, 1971 5] Apollo 15 Prelim. Sci. Rept., NASA SP-289, 1972 6] Apollo 16 Prelim. Sci. Rept., NASA SP-315, 1972 7] Apollo 17 Prelim. Sci. Rept., NASA SP-330, 1973 8] ALSEP Termination Report, NASA RP-1036, 1979 9] NRC  New Frontiers in the Solar System: an Integrated Exploration Strategy  Decadal Survey D.C.: The Nat. Academies Press, 2003 10] International Lunar Network Science Definition Team Final Report, 2009 BIOGRAPHY Melissa Jones is a member of the technical staff in the Planetary and Lunar Mission Concepts Group at the Jet Propulsion Laboratory.  Current work includes development of small Lunar lander concepts and instrument packages to deploy on the Moon,  Report Manager for the Titan Saturn System Mission Outer Planets Flagship Mission study, and staffing various concept studies as a systems engineer on Team X, JPL  s mission design team.  Melissa graduated from Loras College with a B.S. in Chemistry and a Ph.D. in Space and Planetary Science from the University of Arkansas  Linda Herrell has a BA in math/computer science/languages \(University of Texas fluids and heat transfer \(City College of New York addition to analytical work in computer science and thermal and structural analysis, she has worked as both a payload \(instrument Earth orbiting \(Hubble Space Telescope, Earth Observing System \(EOS Cassini as Proposal Manager for several NASA science missions She currently serves as the Program Architect for NASA's New Millennium Program    Table 9. Instrument data volumes generated at the hub after 16 Earth day lunar night Science Instrument Compressed Data Volume Received at Hub Mb Seismometer 1980 Magnetometer 920 Heat Flow Probe 5 Seismic Sounder 0 Instrument &amp; Hub Engineering Data 72 Total 2977 Hub Memory 5000 Margin 40  17 Bruce Banerdt has been a research geophysicist at the California Institute of Technology's Jet Propulsion Laboratory since 1977, where he does research in planetary geophysics and instrument development for flight projects. He has been on science teams for numerous planetary missions 


on science teams for numerous planetary missions including Magellan, Mars Observer, Mars Global Surveyor and Rosetta. He was the US Project Scientist for the international Mars NetLander mission, for which he was also principal investigator of the Short-Period Seismometer experiment, and is currently the Project Scientist for the Mars Exploration Rovers. He led the Geophysics and Planetary Geology group at JPL from 1993-2005, and is the JPL Discipline Program Manager for Planetary Geosciences. He has held several visiting appointments at the Institut de Physique du Globe de Paris. He has a BS in physics and a PhD in geophysics from the University of Southern California  David Hansen is a member of the technical staff in the Communications Systems and Operations Group at the Jet Propulsion Laboratory. Current work includes the development of the telecom subsystem for the Juno project. David received a B.S. in Electrical Engineering from Cornell University and an M.S. in Electrical Engineering from Stanford University  Robert Miyake is a member of the technical staff in the Mission and Technology Development Group at the Jet Propulsion Laboratory. Current work includes the development of thermal control subsystems for interplanetary flagship missions to Jupiter and Saturn missions to Mars and the Earth Moon, and is the lead Thermal Chair for the Advanced Project Design Team Robert graduated with a B. S. from San Jose State University, with extensive graduate studies at UCLA University of Washington, and University of Santa Clara  Steve Kondos is a consultant to the Structures and Mechanisms group at the Jet Propulsion Laboratory. He currently is generating the mechanical concepts for small Lunar Landers and Lunar Science Instrument packages in support of various Lunar mission initiatives. He also provides conceptual design, mass and cost estimating support for various Team X studies as the lead for the Mechanical Subsystem Chair. Steve is also involved with various other studies and proposals and provides mentoring to several young mechanical and system engineers. He graduated with a B.S. in Mechanical Engineering from the University of California, Davis and has 28 years of experience in the aerospace field ranging from detail part design to system of systems architecture development. He has worked both in industry and in government in defense, intelligence commercial and civil activities that range from ocean and land based systems to airborne and space systems. Steve has received various NASA, Air Force, Department of Defense and other agency awards for his work on such projects as the NASA Solar Array Flight Experiment, Talon Gold, MILSTAR, Iridium, SBIRS, Mars Exploration Rovers ATFLIR, Glory Aerosol Polarimeter System and several Restricted Programs  Paul Timmerman is a senior member of technical staff in the Power Systems Group at the Jet Propulsion Laboratory Twenty-five years of experience in spacecraft design including 22 at JPL, over 250 studies in Team-X, and numerous proposals. Current assignments include a wide variety of planetary mission concepts, covering all targets within the solar system and all mission classes. Paul graduated from Loras College with a B.S. in Chemistry in 1983  Vincent Randolph is a senior engineer in the Advanced Computer Systems and 


the Advanced Computer Systems and Technologies Group at the Jet Propulsion Laboratory. Current work includes generating Command and Data Handling Subsystem conceptual designs for various proposals and Team X.  He also supports Articulation Control and Electronics design activities for the Advanced Mirror Development project. Vincent graduated from the University of California at Berkeley with a B.S. in Electrical Engineering 18  pre></body></html 


i models into time and covariate dependent dynamic counterparts  ii models and reliability analysis in a more realistic manner  iii level  whether or not functional components \(loyal generals diagnose correctly and take proper actions such as fault mask of failed components \(traitors asymmetric  iv survivability analysis. Evolutionary game modeling can derive sustainable or survivable strategies \(mapped from the ESS in EGT such as node failures such as security compromise level modeling in the so-called three-layer survivability analysis developed in Ma \(2008a this article  v offer an integrated architecture that unite reliability survivability, and fault tolerance, and the modeling approaches with survival analysis and evolutionary game theory implement this architecture. Finally, the dynamic hybrid fault models, when utilized to describe the survival of players in EGT, enhance the EGT's flexibility and power in modeling the survival and behaviors of the game players which should also be applicable to other problem domains where EGT is applicable  5. OPERATIONAL LEVEL MODELING AND DECISION-MAKING  5.1. Highlights of the Tactical and Strategic Levels  Let's first summarize what are obtainable at both tactical and strategic levels. The results at both tactical and strategic levels are precisely obtainable either via analytic or simulation optimization. With the term precisely, we mean that there is no need to assign subjective probabilities to UUUR events. This is possible because we try to assess the consequences of UUUR events \(tactical level ESS strategies \(strategic level time prediction of survivability. The following is a list of specific points. I use an assumed Wireless Sensor Network WSN  i of UUUR events: \(a actions which can be treated as censored events; \(b Cont' of Box 4.2 It can be shown that the replicator differential equations are equivalent to the classical population dynamics models such as Logistic differential equation and LotkaVolterra equation \(e.g., Kot 2001 Logistic equation, or the limited per capital growth rate is similar to the change rate of the fitness  xfxfi which can be represented with the hazard function or survivor functions introduced in the previous section on survival analysis.  This essentially connects the previous survival analysis modeling for lifetime and reliability with the EGT modeling. However, EGT provides additional modeling power beyond population dynamics or survival analysis approaches introduced in the previous section. The introduction of evolutionary theory makes the games played by a population evolvable. In other words, each player \(individual 


other words, each player \(individual agent and players interact with each other to evolve an optimized system Box 4.3. Additional Comments on DHF Models  The above introduced EGT models are very general given they are the system of ordinary differential equations. Furthermore, the choice of fitness function f\(x complexity to the differential equation system.  The system can easily be turned into system of nonlinear differential equations. The analytical solution to the models may be unobtainable when nonlinear differential equations are involved and simulation and/or numerical computation are often required  In the EGT modeling, Byzantine generals are the game players, and hybrid fault models are conveniently expressed as the strategies of players; the players may have different failure or communication behaviors Furthermore, players can be further divided into groups or subpopulations to formulate more complex network organizations. In the EGT modeling, reliability can be represented as the payoff \(fitness, the native term in EGT of the game. Because reliability function can be replaced by survivor function, survival analysis is seamlessly integrated into the EGT modeling. That is, let Byzantine generals play evolutionary games and their fitness reliability function  The evolutionary stable strategy \(ESS counterpart of Nash equilibrium in traditional games ESS corresponds to sustainable strategies, which are resistant to both internal mutations \(such as turning into treason generals or nodes such as security compromises represent survivable strategies and survivability in survivability analysis. Therefore, dynamic hybrid fault models, after the extension with EGT modeling, can be used to study both reliability and survivability 13 risks such as competing risks which can be described with CRA; \(c captured with the shard frailty.  We believe that these UUUR events are sufficiently general to capture the major factors/events in reliability, security and survivability whose occurrence probabilities are hard or impossible to obtain  Instead of trying to obtain the probabilities for these events which are infeasible in most occasions, we focus on analyzing the consequences of the events.  With survival analysis, it is possible to analyze the effects of these types of events on survivor functions. In addition, spatial frailty modeling can be utilized to capture the heterogeneity of risks in space, or the spatial distribution of risks \(Ma 2008a d UUUR events introduced previously. These approaches and models that deal with the effects of UUUR events form the core of tactical level modeling  To take advantage of the tactical level modeling approaches it is obviously necessary to stick to the survivor functions or hazard functions models. In other words, survival analysis can deal with UUUR events and offer every features reliability function provides, but reliability function cannot deal with UUUR events although survivor function and reliability function have the exactly same mathematical definition. This is the junction that survival analysis plays critical role in survivability analysis at tactical level. However, we 


recognize that it is infeasible to get a simple metric for survivability similar to reliability with tactical level modeling alone. Actually, up to this point, we are still vague for the measurement of survivability or a metric for survivability. We have not answered the question: what is our metric for survivability? We think that a precise or rigorous definition of survivability at tactical level is not feasible, due to the same reason we cited previously  the inability to determine the probabilities of UUUR events However, we consider it is very helpful to define a work definition for survivability at the tactical level  We therefore define the survivability at tactical level as a metric, Su\(t t function or reliability function with UUUR events considered. In the framework of three-layer survivability analysis, this metric is what we mean with the term survivability. The "metric" per se is not the focus of the three-layer survivability analysis. It is not very informative without the supports from the next two levels  strategic and operational models.  However, it is obvious that this metric sets a foundation to incorporate UUUR effects in the modeling at the next two levels  Due to the inadequacy of tactical level modeling, we proposed the next level approach  strategic level modeling for survivability. As expected, the tactical level is one foundation of strategic level modeling ii objectives: \(a affect survivability which survival analysis alone is not adequate to deal with; \(b survivability at tactical level is necessary but not sufficient for modeling survivability, we need to define what is meant with the term survivability at strategic level  With regard to \(a behaviors or modes which have very different consequences. These failure behaviors can be captured with hybrid fault models. However, the existing hybrid fault models in fault tolerance field are not adequate for applying to survivability analysis. There are two issues involved: one is the lack of real time notion in the constraints for hybrid fault models \(e.g., N&gt;3m+1 for Byzantine Generals problem synthesize the models after the real-time notions are introduced. The solution we proposed for the first issue is the dynamic hybrid fault models, which integrate survivor functions with traditional hybrid fault models. The solution we proposed for the second issue is the introduction of EGT modeling  With regard to \(b modeling our problem at strategic level, EGT modeling is essentially a powerful optimization algorithm.  One of the most important results from EGT modeling is the so-called evolutionary stable strategies \(ESS We map the ESS in EGT to survivable strategies in survivability analysis.   Therefore, at the strategic level, our work definition for survivability refers to the survivable strategies or sustainable strategies in the native term of EGT, which can be quantified with ESS  In addition to integrating dynamic hybrid fault models another advantage for introducing EGT modeling at strategic level is the flexibility for incorporating other node behaviors \(such as cooperative vs. non-cooperative those behaviors specified in standard hybrid fault models, as well as anthropocentric factors such as costs constraints  Without UUUR events, both tactical and strategic level 


Without UUUR events, both tactical and strategic level models default to regular reliability models. This implies that, in the absence of UUUR events, reliable strategies are sustainable or survivable.  This also implies that three-layer survivability analysis defaults to reliability analysis however, the three-layer approach does offer some significant advantages over traditional reliability analysis, as discussed in previous sections. Nevertheless, when UUUR events exist, reliable strategies and survivable strategies are different. This necessitates the next operational level modeling  5.2. Operational Level Modeling and Decision-Making  When UUUR events are involved, we cannot make real time predictions of survivability at tactical and strategic levels This implies that the implementations of survivable 14 strategies need additional measures that we develop in this section.  Box 5.1 explains the ideas involved with possibly the simplest example  Figure 4 is a diagram showing a simplified relationship between action threshold survivability \(TS survivability \(ES view since both TS and ES are multidimensional and dynamic in practice. Therefore, the sole purpose of the diagram is to illustrate the major concepts discussed above The blue curve is the survivability when survivable strategies specified by ESS are implemented at some point before time s.  The system is then guaranteed to hold survivability above ES. In contrary, if no ESS implemented before time s, then the system quickly falls below to the survivable level at around 40 time units  T i m e 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 1 0 0 Su rv iv ab ili ty M et ric S u t 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 E S S  i s  I m p lm e n t e d N o  E S S  is  I m p lm e n t e d ts E S T S  Figure 4. A Diagram Showing the Relationship Between TS and ES, as well as timing of s and t, with s &lt; t  6. SUMMARY  The previous sections discussed the major building blocks 


The previous sections discussed the major building blocks for the new life-system inspired PHM architecture. This section first identifies a few minor aspects that have not been discussed explicitly but are necessary for the implementation of the architecture, and then we summarize the major building blocks in a diagram  6.1. Missing Components and Links  Optimization Objectives  Lifetime, reliability, fault tolerance, and survivability, especially the latter two, are application dependent. Generally, the optimization of reliability and survivability are consistent; in that maximization of reliability also implies maximization of survivability. However, when application detail is considered, optimization of lifetime is not necessarily consistent with the optimization of reliability. Consider the case of the monitoring sensor network as an example. The network reliability is also dependent on connectivity coverage, etc, besides network lifetime. What may be further complicated is the time factor. All of the network metrics are time-dependent. A paradoxical situation between lifetime and reliability could be that nodes never 'sleep                                                   


          Box 5.1 Operational Level Modeling  Assuming that the ESS solution for a monitoring sensor network can be expressed with the following simple algebraic conditions: survivability metric at tactical level SU = 0.7, Router-Nodes in the WSN &gt; 10%, Selfish Nodes &lt; 40%. Even with this extremely simplified scenario, the ESS strategies cannot be implemented because we do not know when the actions should be taken to warrant a sustainable system.  These conditions lack a correlation with real time  The inability to implement ESS is rooted in our inability to assign definite probabilities to UUUR events, which implies that we cannot predict when something sufficiently bad will jeopardize the system survivability What we need at the operational level is a scheme to ensure ESS strategy is in place in advance  The fundamental idea we use to implement the ESS strategy is to hedge against the UUUR events. The similar idea has been used in financial engineering and also in integrated pest management in entomology. This can be implemented with the following scheme  Let us define a pair of survivability metrics: one is the expected survivability \(ES threshold survivability or simply threshold survivability \(TS ES is equivalent to the survivability metric at tactical level. ES corresponds to ESS at strategic level, but they are not equivalent since ESS is strategy and ES is survivability. TS is the survivability metric value \(at tactical level and TS can be obtained from strategic level models. For example, TS = SU\(s t condition for the implementation of ESS. In other words, the implementation of strategies that ensures TS at time s will guarantee the future ES level at time t.  To make the implementation more reliable and convenient multiple dynamic TSs can be computed at time s1, s2 sk, with si &lt; t for all i.  These TS at times s1, s2, ..., sk should be monitored by some evaluation systems  Unlike tactical and strategic levels, the operational level modeling is approximate. The term "approximate means that we cannot predict the real time survivability or we do not know the exact time an action should be taken. Instead, the action is triggered when the monitored survivability metric SU\(r survivability \(TS scheme of TS and ES, we ensure the ES by taking preventative actions \(prescribed by ESS and triggered by the TS consequences of UUUR events  Figure 4 is a diagram showing the above concepts and the decision-making process involved 15 This wakefulness \(never 'sleep short period but at the expense of network lifetime. Of course, when the network is running out of lifetime, network reliability ultimately crashes. This example reminds us that 


reliability ultimately crashes. This example reminds us that multi-objective optimization should be the norm rather than exception  Constraints and Extensions  Many application specific factors and constraints are ignored in this article. For example, we mentioned about spatial heterogeneity of environment, but never present a mathematical description The spatial heterogeneity can be modeled with the so-called spatial frailty in multivariate survival analysis \(Ma 2008a  Evolutionary Algorithm  Evolutionary game modeling when implemented in simulation, can be conveniently implemented with an algorithm similar to Genetic Algorithms \(GA ESS in the evolutionary game model with simulation is very similar to GA. Dynamic populations, in which population size varies from generation to generation \(Ma &amp; Krings 2008f of node failures. Another issue to be addressed is the synchronous vs. asynchronous updating when topology is considered in the simulation. This update scheme can have profound influences on the results of the simulation. Results from cellular automata computing should be very useful for getting insights on the update issue  6.2. Summary and Perspective  To recapture the major points of the article, let us revisit Figure 3, which summarizes the principal modules of the proposed life-system inspired PHM architecture. The main inspiration from life systems is the notion of individuals and their assemblage, the population. Population is an emergent entity at the next level and it has emergent properties which we are often more concerned with. Survival analysis, which has become a de facto standard in biomedicine, is particularly suitable for modeling population, although it is equally appropriate at individual level. Therefore, survival analysis \(including competing risks analysis and multivariate survival analysis comprehensively in the context of PHM in a series of four papers presented at IEEE AeroSpace 2008 \(Ma &amp; Krings 2008a, b, c, &amp; d proposed architecture. Survival analysis constitutes the major mathematical tools for analyzing lifetime and reliability, and also forms the tactical level of the three-layer survivability analysis  Besides lifetime and reliability, two other major modules in Figure 3 are fault tolerance and survivability. To integrate fault tolerance into the PHM system, Dynamic Hybrid Fault DHF 2008e, Ma 2008a make real-time prediction of reliability more realistic and make real-time prediction of fault tolerance level possible DHF models also unite lifetime, reliability and fault tolerance under a unified modeling framework that consists of survival analysis and evolutionary game theory modeling  DHG models also form the partial foundation, or strategic level, for the three-layer survivability analysis. At the strategic level, the Evolutionary Stable Strategies \(ESS which is mapped to survivable or sustainable strategies, can be obtained from the evolutionary game theory based DHF models. When there is not any UUUR event involved reliability and survivability are consistent, and reliable strategies are survivable. In this case, the strategic level modeling up to this point is sufficient for the whole PHM system modeling, and there is no need for the next level  operational level modeling  When there are UUUR events in a PHM system, the 


When there are UUUR events in a PHM system, the inability to determine the occurrence probabilities of UUUR events makes the operational level modeling necessary Then the principle of hedging must be utilized to deal with the "hanging" uncertainty from UUUR events. In this case reliability strategies are not necessarily survivable strategies At the operational level modeling, a duo of survivability metrics, expected survivability \(ES survivability \(TS the survivable strategies \(ESS level are promptly implemented based on the decisionmaking rules specified with the duo of survivability metrics then the PHM system should be able to endure the consequences of potentially catastrophic UUUR events. Of course, to endure such catastrophic events, the cost may be prohibitively high, but the PHM system will, at least, warn decision-makers for the potentially huge costs.  It might be cheap to just let it fail  Figure 3 also shows several other modules, such as security safety, application systems \(such as Automatic Logistics CBM+, RCM, Life cycle cost management, Real-time warning and alert systems architectures, but we do not discuss in this paper. Generally the new architecture should be fully compatible with existing ones in incorporating these additional modules. One point we stressed is that PHM system can be an ideal place to enforce security policies. Enforcing security policies can be mandatory for PHM systems that demand high security and safety such as weapon systems or nuclear plant facilities.  This is because maintenance, even without human-initiated security breaches, can break the security policies if the maintenance is not planned and performed properly  In perspective, although I did not discuss software issues in this paper, the introduced approaches and models should provide sufficient tools for modeling software reliability and survivability with some additional extension. Given the critical importance of software to modern PHM systems, we present the following discussion on the potential extension to software domain. Specifically, two points should be noted: \(1 architecture to software should be a metric which can 16 replace the time notion in software reliability; I suggest that the Kolmogorov complexity \(e.g., Li and Vitanyi 1997 be a promising candidate \(Ma 2008a change is because software does not wear and calendar time for software reliability usually does not make much sense 2 software reliability modeling.  Extending to general survivability analysis is not a problem either. In this article I implicitly assume that reliability and survivability are positively correlated, or reliability is the foundation of survivability. This positive correlation does not have to be the case. A simplified example that illustrates this point is the 'limit order' in online stock trading, in which limit order can be used in either direction: that stock price is rising or falling.  The solution to allow negative or uncorrelated relationships between reliability and survivability are very straightforward, and the solutions are already identified in previous discussions. Specifically, multiple G-functions and multi-stage G-functions by Vincent and Brown \(2005 very feasible solution, because lifetime, reliability and survivability may simply be represented with multiple Gfunctions. Another potential solution is the accommodation of the potential conflicts between reliability and survivability with multi-objective GA algorithms, which I previously suggested to be used as updating algorithms in the optimization of evolutionary games  


 The integration of dynamic hybrid fault models with evolutionary game modeling allows one to incorporate more realistic and detailed failure \(or survival individual players in an evolutionary game. This is because dynamic hybrid fault models are supported by survival analysis modeling, e.g., time and covariate dependent hazard or survivor functions for individual players. If necessary, more complex survival analysis modeling including competing risks analysis and multivariate survival analysis, can be introduced.  Therefore, any field to which evolutionary game theory is applicable may benefit from the increased flexibility in modeling individual players.  Two particularly interesting fields are system biology and ecological modeling.  In the former field, dynamic hybrid fault models may find important applications in the study of biological networks \(such as gene, molecular, and cell networks 2008g conjecture that explains the redundancy in the universal genetic code with Byzantine general algorithm. In addition they conducted a comparative analysis of bio-robustness with engineering fault tolerance, for example, the strong similarity between network survivability and ecological stability \(Ma &amp; Krings 2008g survivability analysis can be applied for the study of survivals or extinctions of biological species under global climate changes \(Ma 2008b  In this paper, I have to ignore much of the details related to the implementation issues to present the overall architecture and major approaches clearly and concisely. To deal with the potential devils in the implementation details, a well funded research and development team is necessary to take advantages of the ideas presented here. On the positive side I do see the great potential to build an enterprise PHM software product if there is sufficient resource to complete the implementation. Given the enormous complexity associated with the PHM practice in modern engineering fields, it is nearly impossible to realize or even demonstrate the benefits of the architecture without the software implementation. The critical importance of PHM to mission critical engineering fields such as aerospace engineering, in turn, dictates the great value of such kind software product  6.3. Beyond PHM  Finally, I would like to raise two questions that may be interested in by researchers and engineers beyond PHM community. The first question is: what can PHM offer to other engineering disciplines? The second question is: what kinds of engineering fields benefit most from PHM? Here, I use the term PHM with the definition proposed by IEEE which is quoted in the introduction section of the paper  As to the first question, I suggest software engineering and survivability analysis are two fields where PHM can play significant roles. With software engineering, I refer to applying PHM principles and approaches for dealing with software reliability, quality assurance, and even software process management, rather than building PHM software mentioned in the previous subsection. For survivability analysis, borrowing the procedures and practices of PHM should be particularly helpful for expanding its role beyond its originating domain \(network systems that control critical national infrastructures is a strong advocate for the expansion of survivability analysis to PHM. Therefore, the interaction between PHM and survivability analysis should be bidirectional. Indeed, I see the close relationships between PHM, software engineering, and survivability as well-justified because they all share some critical issues including reliability survivability, security, and dependability  


 The answer to the second question is much more elusive and I cannot present a full answer without comparative analysis of several engineering fields where PHM has been actively practiced. Of course, it is obvious that fields which demand mission critical reliability and dependability also demand better PHM solutions. One additional observation I would like to make is that PHM seems to play more crucial roles for engineering practices that depend on the systematic records of 'historical' data, such as reliability data in airplane engine manufacturing, rather than on the information from ad hoc events.  This may explain the critical importance of PHM in aerospace engineering particularly in commercial airplane design and manufacturing.  For example, comparing the tasks to design and build a space shuttle vs. to design and manufacture commercial jumbo jets, PHM should be more critical in the latter task  17    Figure 2. States of a monitoring sensor node and its failure modes \(after Ma &amp; Krings 2008e     Figure 3. Core Modules and their Relationships of the Life System Inspired PHM Architecture    REFERENCES  Adamides, E. D., Y. A. Stamboulis, A. G. Varelis. 2004 Model-Based Assessment of Military Aircraft Engine Maintenance Systems Model-Based Assessment of Military Aircraft Engine Maintenance Systems. Journal of the Operational Research Society, Vol. 55, No. 9:957-967  Anderson, R. 2001. Security Engineering. Wiley  Anderson, R. 2008. Security Engineering. 2nd ed. Wiley  Bird, J. W., Hess, A. 2007.   Propulsion System Prognostics R&amp;D Through the Technical Cooperation Program Aerospace Conference, 2007 IEEE, 3-10 March 2007, 8pp  Bock, J. R., Brotherton, T., W., Gass, D. 2005. Ontogenetic reasoning system for autonomic logistics. Aerospace Conference, 2005 IEEE 5-12 March 2005.Digital Object Identifier 10.1109/AERO.2005.1559677  Brotherton, T., P. Grabill, D. Wroblewski, R. Friend, B Sotomayer, and J. Berry. 2002. A Testbed for Data Fusion for Engine Diagnostics and Prognostics. Proceedings of the 2002 IEEE Aerospace Conference  Brotherton, T.; Grabill, P.; Friend, R.; Sotomayer, B.; Berry J. 2003. A testbed for data fusion for helicopter diagnostics and prognostics. Aerospace Conference, 2003. Proceedings 2003 IEEE  Brown, E. R., N. N. McCollom, E-E. Moore, A. Hess. 2007 Prognostics and Health Management A Data-Driven Approach to Supporting the F-35 Lightning II. 2007 IEEE AeroSpace Conference  Byington, C.S.; Watson, M.J.; Bharadwaj, S.P. 2008 Automated Health Management for Gas Turbine Engine Accessory System Components. Aerospace Conference 2008 IEEE, DOI:10.1109/AERO.2008.4526610 


2008 IEEE, DOI:10.1109/AERO.2008.4526610 Environment Covariates &amp; Spatial Frailty Applications: AL; Life Cycle Mgmt; Real-Time Alerts CBM+, RCM, TLCSM; Secret Sharing and Shared Control 18 Chen, Y. Q., S. Cheng. 2005. Semi-parametric regression analysis of mean residual life with censored survival data Biometrika \(2005  29  Commenges, D. 1999. Multi-state models in Epidemiology Lifetime Data Analysis. 5:315-327  Cook, J. 2004. Contrasting Approaches to the Validation of Helicopter HUMS  A Military User  s Perspective Aerospace Conference, 2004 IEEE  Cook, J. 2007. Reducing Military Helicopter Maintenance Through Prognostics. Aerospace Conference, 2007 IEEE Digital Object Identifier 10.1109/AERO.2007.352830  Cox, D. R. 1972. Regression models and life tables.  J. R Stat. Soc. Ser. B. 34:184-220  Crowder, M. J.  2001. Classical Competing Risks. Chapman amp; Hall. 200pp  David, H. A. &amp; M. L. Moeschberger. 1978. The theory of competing risks. Macmillan Publishing, 103pp  Ellison, E., L. Linger, and M. Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013  Hanski, I. 1999. Metapopulation Ecology. Oxford University Press  Hallam, T. G. and S. A. Levin. 1986. Mathematical Ecology. Biomathematics. Volume 17. Springer. 457pp  Hess, A., Fila, L. 2002.  The Joint Strike Fighter \(JSF concept: Potential impact on aging aircraft problems Aerospace Conference Proceedings, 2002. IEEE. Digital Object Identifier: 10.1109/AERO.2002.1036144  Hess, A., Calvello, G., T. Dabney. 2004. PHM a Key Enabler for the JSF Autonomic Logistics Support Concept. Aerospace Conference Proceedings, 2004. IEEE  Hofbauer, J. and K. Sigmund. 1998. Evolutionary Games and Population Dynamics. Cambridge University Press 323pp  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Huzurbazar, A. V. 2006. Flow-graph model for multi-state time-to-event data. Wiley InterScience  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis. Springer. 481pp  Kacprzynski, G. J., Roemer, M. J., Hess, A. J. 2002. Health management system design: Development, simulation and cost/benefit optimization. IEEE Aerospace Conference Proceedings, 2002. DOI:10.1109/AERO.2002.1036148  Kalbfleisch, J. D., and R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data. Wiley-InterScience, 2nd ed  Kalgren, P. W., Byington, C. S.   Roemer, M. J.  2006 Defining PHM, A Lexical Evolution of Maintenance and Logistics. Systems Readiness Technology Conference 


Logistics. Systems Readiness Technology Conference IEEE. DOI: 10.1109/AUTEST.2006.283685  Keller, K.; Baldwin, A.; Ofsthun, S.; Swearingen, K.; Vian J.; Wilmering, T.; Williams, Z. 2007. Health Management Engineering Environment and Open Integration Platform Aerospace Conference, 2007 IEEE, Digital Object Identifier 10.1109/AERO.2007.352919  Keller, K.; Sheahan, J.; Roach, J.; Casey, L.; Davis, G Flynn, F.; Perkinson, J.; Prestero, M. 2008. Power Conversion Prognostic Controller Implementation for Aeronautical Motor Drives. Aerospace Conference, 2008 IEEE. DOI:10.1109/AERO.2008.4526630  Klein, J. P. and M. L. Moeschberger. 2003. Survival analysis techniques for censored and truncated data Springer  Kingsland, S. E. 1995. Modeling Nature: Episodes in the History of Population Ecology. 2nd ed., University of Chicago Press, 315pp  Kot, M. 2001. Elements of Mathematical Ecology Cambridge University Press. 453pp  Krings, A. W. and Z. S. Ma. 2006. Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks Military Communications Conference, 23-25 October, 7 pages, 2006  Lamport, L., R. Shostak and M. Pease. 1982. The Byzantine Generals Problem. ACM Transactions on Programming Languages and Systems, 4\(3  Lawless, J. F. 2003. Statistical models and methods for lifetime data. John Wiley &amp; Sons. 2nd ed  Line, J. K., Iyer, A. 2007. Electronic Prognostics Through Advanced Modeling Techniques. Aerospace Conference 2007 IEEE. DOI:10.1109/AERO.2007.352906  Lisnianski, A., Levitin, G. 2003. Multi-State System Reliability: Assessment, Optimization and Applications World Scientific  Liu, Y., and K. S. Trivedi. 2006. Survivability Quantification: The Analytical Modeling Approach, Int. J of Performability Engineering, Vol. 2, No 1, pp. 29-44  19 Luchinsky, D.G.; Osipov, V.V.; Smelyanskiy, V.N Timucin, D.A.; Uckun, S. 2008. Model Based IVHM System for the Solid Rocket Booster. Aerospace Conference, 2008 IEEE.DOI:10.1109/AERO.2008.4526644  Lynch, N. 1997. Distributed Algorithms. Morgan Kaufmann Press  Ma, Z. S. 1997. Demography and survival analysis of Russian wheat aphid. Ph.D. dissertation, Univ. of Idaho 306pp  Ma, Z. S. 2008a. New Approaches to Reliability and Survivability with Survival Analysis, Dynamic Hybrid Fault Models, and Evolutionary  Game Theory. Ph.D. dissertation Univ. of Idaho. 177pp  Ma, Z. S. 2008b. Survivability Analysis of Biological Species under Global Climate Changes: A New Distributed and Agent-based Simulation Architecture with Survival Analysis and Evolutionary Game Theory. The Sixth 


International Conference on Ecological Informatics. Dec 25, 2008. Cancun, Mexico  Ma, Z. S. and E. J. Bechinski. 2008. A Survival-Analysis based  Simulation Model for Russian Wheat Aphid Population Dynamics. Ecological Modeling, 216\(2 332  Ma, Z. S. and A. W. Krings. 2008a.  Survival Analysis Approach to Reliability Analysis and Prognostics and Health Management \(PHM  AIAA AeroSpace Conference, March 1-8, 2008, Big Sky, MT, 20pp  Ma, Z. S. and A. W. Krings. 2008b. Competing Risks Analysis of Reliability, Survivability, and Prognostics and Health Management \(PHM  AIAA AeroSpace Conference, March 1-8, 2008.  Big Sky, MT. 20pp  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(I Dependence Modeling", Proc. IEEE  AIAA AeroSpace Conference, March 1-8, 2008, Big Sky, MT. 21pp  Ma, Z. S. and A. W. Krings., R. E. Hiromoto. 2008d Multivariate Survival Analysis \(II State Models in Biomedicine and Engineering Reliability IEEE International Conference of Biomedical Engineering and Informatics, BMEI 2008.  6 Pages  Ma, Z. S. and A. W. Krings. 2008e. Dynamic Hybrid Fault Models and their Applications to Wireless Sensor Networks WSNs Modeling, Analysis and Simulation of Wireless and Mobile Systems. \(ACM MSWiM 2008 Vancouver, Canada  Ma, Z. S. &amp; A. W. Krings. 2008f. Dynamic Populations in Genetic Algorithms. SIGAPP, the 23rd Annual ACM Symposium on Applied Computing, Ceara, Brazil, March 16-20, 2008. 5 Pages  Ma, Z. S. &amp; A. W. Krings. 2008g. Bio-Robustness and Fault Tolerance: A New Perspective on Reliable, Survivable and Evolvable Network Systems, Proc. IEEE  AIAA AeroSpace Conference, March 1-8, Big Sky, MT, 2008. 20 Pages  Ma, Z. S.  and A. W. Krings. 2009. Insect Sensory Systems Inspired Computing and Communications.  Ad Hoc Networks 7\(4  MacConnell, J.H. 2008. Structural Health Management and Structural Design: An Unbridgeable Gap? 2008 IEEE Aerospace Conference, DOI:10.1109/AERO.2008.4526613  MacConnell, J.H. 2007. ISHM &amp; Design: A review of the benefits of the ideal ISHM system. Aerospace Conference 2007 IEEE. DOI:10.1109/AERO.2007.352834  Marshall A. W., I. Olkin. 1967. A Multivariate Exponential Distribution. Journal of the American Statistical Association, 62\(317 Mar., 1967  Martinussen, T. and T. H. Scheike. 2006. Dynamic Regression Models for Survival Data. Springer. 466pp  Mazzuchi, T. A., R. Soyer., and R. V. Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Millar, R.C., Mazzuchi, T.A. &amp; Sarkani, S., 2007. A Survey of Advanced Methods for Analysis and Modeling of 


of Advanced Methods for Analysis and Modeling of Propulsion System", GT2007-27218, ASME Turbo Expo 2007, May 14-17, Montreal, Canada  Millar, Richard C., "Non-parametric Analysis of a Complex Propulsion System Data Base", Ph.D. Dissertation, George Washington University, June 2007  Millar, R. C. 2007. A Systems Engineering Approach to PHM for Military Aircraft Propulsion Systems. Aerospace Conference, 2007 IEEE. DOI:10.1109/AERO.2007.352840  Millar, R. C. 2008.  The Role of Reliability Data Bases in Deploying CBM+, RCM and PHM with TLCSM Aerospace Conference, 2008 IEEE, 1-8 March 2008. Digital Object Identifier: 10.1109/AERO.2008.4526633  Nowak, M. 2006. Evolutionary Dynamics: Exploring the Equations of Life. Harvard University Press. 363pp  Oakes, D. &amp; Dasu, T. 1990. A note on residual life Biometrika 77, 409  10  Pintilie, M. 2006. Competing Risks: A Practical Perspective.  Wiley. 224pp  20 Smith, M. J., C. S. Byington. 2006. Layered Classification for Improved Diagnostic Isolation in Drivetrain Components. 2006 IEEE AeroSpace Conference  Therneau, T. and P. Grambsch. 2000. Modeling Survival Data: Extending the Cox Model. Springer  Vincent, T. L. and J. L. Brown. 2005. Evolutionary Game Theory, Natural Selection and Darwinian Dynamics Cambridge University Press. 382pp  Wang. J., T. Yu, W. Wang. 2008. Research on Prognostic Health Management \(PHM on Flight Data. 2008 Int. Conf. on Condition Monitoring and Diagnosis, Beijing, China, April 21-24, 2008. 5pp  Zhang, S., R. Kang, X. He, and M. G. Pecht. 2008. China  s Efforts in Prognostics and Health Management. IEEE Trans. on Components and Packaging Technologies 31\(2             BIOGRAPHY  Zhanshan \(Sam scientist and earned the terminal degrees in both fields in 1997 and 2008, respectively. He has published more than 60 peer-refereed journal and conference papers, among which approximately 40 are journal papers and more than a third are in computer science.  Prior to his recent return to academia, he worked as senior network/software engineers in semiconductor and software industry. His current research interests include: reliability, dependability and fault tolerance of distributed and software systems behavioral and cognitive ecology inspired pervasive and 


behavioral and cognitive ecology inspired pervasive and resilient computing; evolutionary &amp; rendezvous search games; evolutionary computation &amp; machine learning bioinformatics &amp; ecoinformatics                 pre></body></html 


