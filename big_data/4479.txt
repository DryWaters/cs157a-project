This paper summarizes opportunities and challenges of big data. It identi fies important research directions and includes a number of questions that have been debated by the panel 
Abstract  KeywordsÑdata management; data analytics; data security data privacy 
 
I I NTRODUCTION  Recent technological advances and novel applications, such 
Big Data -  Opportunities and Challenges Panel Position Paper Elisa Bertino Cyber Center, CERIAS and CS Department PurdueUniversity West Lafayette, Indiana \(USA bertino@cs.purdue.edu   
as sensors, cyber-physical systems, smart mobile devices cloud systems, data analytics, and social networks, are making possible to capture, process, and share huge amounts of data referred to as big data Managed well, the data 
and to extract useful knowledge, such as patterns, from this data and predict trends and events. Big data is making possible tasks that before were impossible, like preventing disease spreading and crime, personalizing healthcare, quickly identifying business opportunities managing emergencies, protecting the homeland, and so on [1   As discussed by The Economist [2  
can be used to unlock new sources of economic value, provide fresh insights into science and hold governments to accounts 
 
 Unlocking the potential of big data requires however addressing several major challenges.  The goal of this panel is to identify and discuss research directions to address these challenges In what follows, we first discuss the notion of big data and application domains where big data is relevant. We then outline relevant challenges and summarize questions addressed by the panel II W HAT IS BIG D 
ATA  In order to discuss about research issues for big data management, it is crucial to b etter understand all dimensions related to big data. In this resp ect four characteristics define big data 
  
Volume Ö data sizes will range from terabytes to zettabytes \(that is, 10 21 bytes Variety Ö data comes in many different formats from structured data, organized ac cording to some structures like the data record, to unstr uctured data, like image 
sounds, and videos which are much more difficult to search and analyze 
  
Velocity Ö in many novel applications, like smart cities and smart planet, data conti nuously arrives at possible very high frequencies, resulting in continuous high-speed data streams. It is critical that the time required to act on this data be very small Huge number of data sources Ö the real value of data sets is when these data sets are in tegrated and cross-correlated Integration and cross-correlatio n among data sets from 
different sources allow one to uncover information and trends that often cannot be uncovered by looking at a data set in isolation. It is critical that effective automated approaches to large scale d ata integration be devised The above short discussion emphasizes that volume alone is perhaps the least difficult problem to address then dealing with big data. The real challen ge arises when we have big volumes of unstructured and structured data continuously arriving from a large number of sources. Addressing such challenge require a new generation of 
technologies and architectures, designed to eco nomically extract value from very large volumes of a wide va riety of data, by enabling high 
velocity capture, discovery and/or analysis 
FOR WHOM IS BIG DATA RELEVANT  Big data is relevant for all components of our society Industry is using big data for shifting business intelligence from reporting and decision support to prediction and nextmove decisions. This use of big data emphasizes that big data is critical for obtaining actionable knowledge. Governments are also interested in using big data and predictive analytics to improve decision making and tr ansparency, to engage citizens in public affairs, to improve national security. Healthcare represents another major area to which big data may offer 
 
 
  III 
novel opportunities e ar nin g h e al th s y s t e m s are cu rrent l y  focusing on turning health care data into knowledge translating that knowledge into practice, and creating new data by means of advanced information technology. As pointed out in e u s e o f big data tec h n o lo g ies ca n red u ce th e co st of  healthcare while improving its quality by making care more preventive and pers onalized and basing it on more extensive home-based\ntinuous monitoring Big data is also crucial for re search. Many areas of science and engineering are currently facing from a hundred to a 
2013 IEEE 37th Annual Computer Software and Applications Conference 0730-3157/13 $26.00 © 2013 IEEE DOI 10.1109/COMPSAC.2013.143 479 


E. Bertino, Data Protection from Insider Threats. Morgan&Claypool 2012 2 Data, data everywhereé, The Economist, 25 February 2010, available at http://www.economist.com/node/15557443 Downladed on April 30 2012 3 OêReilly Radar Team, Big Data Now: Current Perspeectives from OêReilly Radar. OêReilly, 2011 4 T. Murdoch, A. Detsky, çThe Inevitable Application of Big Data to Health Careé, JAMA, 2013, 309\(13\:1351-1352 5 H.V. Jagadish et al., çChallenges and Opportunities with Big Data 2012, available at http://cra.org/ccc/docs/init/bigdatawhitepaper.pdf  Downladed on April 30, 2012 6 S. Salmin, E. Bertino, çA Comprehensive Model for Provenance Invited Paper, Proceedings of the First International Workshop on Modeling Data-Intensive Computing \(MoDIC 2012\,  Florence, Italy october 15-18, 2012, LNCS 7518, Springer    
              
thousand-fold increase in the volume of data generated compared to only one decade ago. This data is produced by many sources including simulations, high-throughput scientific instruments, satellites, and telescopes. While the availability of big data is revolutionizing how research is conducted and is leading to the emergence of a new paradigm of science based on data-inten sive computing, at the same time it poses a significant challenge for scientists. In order to be able to leverage these hu ge volumes of data, new techniques and technologies are needed. A new type of einfrastructure, the Research Data Infrastructure, must be designed, implemented and optimized to support the full life cycle of scientific data, its movement across scientific disciplines, and its integration with published literature IV T EACHNICAL R ESEARCH C HALLENGES  There are many technical challenges that must be addressed to realize the full potential of big data. Jagadish et al. [5  provide a comprehensive discussion of such challenges based on the notion of data analysis pipeline Data Acquisition and Recording: it is critical to capture the context into which data has been generated, to be able to filter out non relevant data and to compress data, to automatically generate metadata supporting rich data description and to track and record provenance Information Extraction and Cleaning: data may have to be transformed in order to extract information from it and express this information in a form that is suitable for analysis. Data may also be of poor quality and/or uncertain Data cleaning and data quality verification are thus critical Data Integration, Aggregation and Representation: data can be very heterogeneous and may have different metadata Data integration, even in more conventional cases, requires huge human efforts. Novel approaches that can improve the automation of data integration are critical as manual approaches will not scale to what is required for big data Also different data aggregation and representation strategies may be needed for different data analysis tasks Query Processing, and Analysis: methods suitable for big data need to be able to deal with noisy, dynamic heterogeneous, untrustworthy data and data characterized by complex relations. However despite these difficulties big data even if noisy and uncertain can be more valuable for identifying more reliable hidden patterns and knowledge compared to tiny samples of good data. Also the often redundant\relationships existing among data can represent an opportunity for cross-checking data and thus improve data trustworthiness. Supporting query processing and data analysis requires sc alable mining algorithms and powerful computing infrastructures Interpretation: analysis results extracted from big data needs to be interpreted by decision makers and this may require the users to be able to analyze the assumptions at each stage of data processin g and possibly re-tracing the analysis.  Rich provenance is cr itical in this respect [6    In addition to the above challenges, privacy and security are critical issues. Privacy in particular raises many concern as big data could be used to re-identify privacy-sensitive data even when this data has been anonym ized. Also big data can be used to create profiles of user groups that may be used for discriminating specific groups of individuals or even single individuals. In this respect, population privacy is as crucial as personal privacy. Security also raises challenging issues including scalable security administration, management and integration of heterogeneous data security policies, and the security of data when hosted clouds V P ANEL Q UESTIONS  The panel debated several aspects of big data management and applications that are relevant from a researcher perspective. Questions asked to the panelists include Are there additional big-data applications that we should consider Which are policy issues related to big data that we should address, in addition the well-known privacy policies How can academia, industry and governments engage in projects and initiatives focusing on big data and data intensive applications Are there national and international initiatives that we should engage with What would be technology transfer and commercialization opportunities for research focusing on big data and Research Data Infrastructures Can we quantify the economic value of data Which are the incentives and impediments to data sharing A CKNOWLEDGMENT  The work reported here has been partially supported by the Purdue Cyber Center \(Discovery Park R EFERENCES  1 
      
480 


2 3 2   1 2                   
002 002 s s 
Deìnition 5 Theorem 1 Proof Theorem 2 A Index Construction 
 004 004 004 004 004 004 004 004 004 004 004 004 004 004 004 002 004 004 004 004 004 004 004 
For every out-community pair u v there must not exist two community centers and  such that u  and  v are both in-community pairs in G and has the shortest path to in  We give an example of the SPT as follows  Figure 1\(b shows a SPT of graph G Figure 1\(a with  As an example for out-community vertex pair 17,6 there is a trunk vertex 0 reached by 17 in one hop and a trunk vertex 5 that can reach 6 in one hop The shortest distance between 0 and 5 is 4 Therefore the shortest distance between 17 and 6 is 6 On the other hand if the two trunk vertices cannot reach one another then their neighbors cannot reach to each other too In other words the SPT reserve all the path information and has no false positives Apparently the size of the SPT depends on the local threshold  As is shown in the empirical result Section 4 we can reduce the graph size remarkably with and in many real world data graphs we can achieve ideal result even with  Most edges in may not be contained by E but there is no additional topology information added in besides those in the original graph G Thus we only need to focus on the discovering of the vertex set of the SPT If we can minimize the size of  the query efìciency can be improved greatly Here we give the deìnition of the minimal shortest path trunk discovery Given a directed graph and a threshold  we would like to nd the minimal SPT vertex set which can cover the original graph G and has the minimal size in all the possible results Unfortunately computing MSPT is an NP-hard problem because its corresponding decision problem is NP-complete Figure 2 The Modiìcation for Vertex 8 of G The MSPT discovery is an NP-hard optimization problem We reduce the famous NP-complete problem set-cover problem SCP to this problem Apparently when the threshold  our approach would degenerate into the SCP When  we can modify the original graph G to G as follows for each if or and u is not the end point of the path starting from in steps then delete it as an example we can see the Figure 2 After the modiìcation the MSPT problem degenerates into the SCP again Therefore we can draw a conclusion that the MSPT problem is a special case of SCP and is NP-hard too Obviously the SPT is a special case of the SCP It just increased the covered scope So we have Given any DAG the MSPT computing cost is not larger than the minimum vertex set cover MSCP i.e  III SPT DISCOVERY Since the discovery of the MSPT is NP-hard we cannot nd the optimal solution in polynomial time In this section we propose an approximate algorithm to deal this problem  Let be a directed graph we say the vertex u resp v is covered by the vertex v resp u if or and the distance between them is at most  As is given by Theorem 1 we cannot get the SPT in polynomial time Therefore an approximation scheme for this problem is presented The index construction algorithm is consists of two steps see Algorithm 1 Step 1 and 2 constitute the major part of the index construction task get the vertex set of the SPT for G and get the edge set of the SPT for G Computation of the SPT  G a directed data graph  T the SPT   output the vertex set  output the edge set The vertex set can be obtained by Algorithm 2 Its main idea derives from the 2-approximate minimum vertex cover algorithm In applications we nd a phenomenon that those vertices chosen to be community center often have much more degrees than other vertices Intuitively to minimize the size of the SPT of G we must select the vertices that can cover other vertices as much as possible Algorithm 2 rst orders the vertices by their degrees out-degree and in-degree and cover the vertices in G by bidirectionally BFS constrained by threshold  in turn until all the vertices is covered By this way we can choose those vertices to be community center which have maximum degrees as much as possible As an example we can see the graph G in Figure 1\(a The vertices 12,8,0,5 will be handled in turn because they have maximum degree Before we introduce the algorithm of the computation of the edge set for the SPT we rst give a property of community centers  The maximum distance between two community centers is 2  
u v u v u v G 002 002 002 002 E E V V G V E 002 V V 002 002 v V u N v u N v v 002 cost MSPT cost MSCP 002 G V E 002 u v v u 002 002 a threshold get vertex set G 002 V get edge set G V E T V E 002 002 002 
003 004 005 012 003\002 003\005 
1 2 3 
Example 1 The Minimal Shortest Path Trunk MSPT Discovery Vertex Set Cover Algorithm 1 input output Property 1 
012 004 011 003 003 003 012 002 002     014 
197 


003 003 
014\010 014  014  014 014 014 014 014 014\010 003 003 014 014     
Q.empty u can reach v in 2 steps and B Transitive Closure of the SPT C Complexity Theorem 3 Proof 1 Local bidirectional BFS for accessing the SPT 
1 2 3 4 for 7 8 9 while 11 12 if 14 15 16 17 1 2 for 4 if 6 7 
out in out in 002 002 002 002 u V 002 002 002 002 u V 002 
Computation of the SPT vertex set  G a directed data graph   the vertex set of SPT   Queue Q Q.enqueue\(u level  0 v  Q.dequeue return  According to the approach of our index construction this property is in evidence Take advantage of this property we propose a simple method shown in Algorithm 3 to retrieve all the edge set for the SPT The main idea of it is to do BFS from every vertex of the SPT if it can reach another vertex of the SPT in 2 steps then we create a link between them and add a cost for it Because all this link added by us are all the path information of the original graph there is no new reachability information introduced Computation of the SPT edge set  G a directed data graph  the vertex set of the SPT for G   the edge set of the SPT   do BFS from u  return  After the operation in Section 3.1 we can get a subgraph of the original graph The index size can be reduced signiìcantly The shortest path distance query efìciency closely relate to the SPT Because the total size of the SPT is comparative small we can use the most direct method that to compute all the exact shortest path distance between all the vertex pairs of the index such as the Dijksras algorithm  To improve the efìciency we can use other algorithms to construct index for the SPT too In the experiment result we can nd that our index framework can help improve the query efìciency of other approaches greatly If the size of the SPT is still a bottleneck of the query performance we can continue compute the SPT for the original SPT to further reduce the index size  For the index construction we have to order the vertices of the original graph G by their degrees use the vertices with high degree to cover other vertices generate the edges between all the SPT vertices For 1 it takes time For 2 Let denote the set of vertices that can cover all the vertices of the original graph let and denote the vertices and edges in us forward neighbors Let and denote the vertices and edges in us backward neighbors respectively then the procedure takes time  For 3 we deploy local BFS from the every SPT vertices which costs in worst case   Mostly the SPT is a really sparse graph Therefore the storage cost is very small Even if we adopt the transitive closure to store all the shortest paths between every vertices pairs of the SPT the index size is  where  IV QUERY PROCESSING We now discuss how to process the shortest path distance query by our index First we give an important property of the SPT about query processing Any vertex can reach a SPT vertex in hops According to theorem 3 the max distance between any two SPT vertices is  In the extreme case one vertex may one hop away from the nearest backward SPT vertices Therefore the maximum distance between this vertex to other SPT vertex is  The basic scheme of the shortest path distance computation based on Theorem 3 is sketched in Algorithm 4 It consists of two basic steps To get the shortest path distance of two vertices u and v we rst carry out two local BFS within hops One is forward search from u another is backward search from v If they are matched locally we directly answer the shortest path distance or else they both can reach the vertices of the SPT 
002 a threshold V V O the set of node ordered by degree each u in O visited u V u level get v s level level  002 B v s successors B v s predecessors Q B Q B V 002 V E E each u V 002 v V c the cost f rom u to v E u v c E O nlogn V 002 N u E u 002 N u E u 002 O N u E u N u E u O V E u O K 2 k V 002 002 002 002 
Algorithm 2 input output do 5 if then 6 do 10 then 13 Algorithm 3 input output do 3 then 5 Index construction time Index size 
004 004 004 002 002 002 004 004 004 004 004 004 004 004    004 002 002 005 002 002 004 005 
002 002 
                                      2 1 2 2 1 2 
198 


004 004 004 004 002 002         
1 2 1 1 2 2 1 2 
         
002 002 002 002 
Dataset Nodes Edges Diameter 
014 014 003 003 015 
1 2 3 if 5 6 7 for 11 
Query\(u,v  G a directed data graph T the SPT for G  the shortest path distance between u and v  do forward BFS from u do backward BFS from v return the distance from u to v the set of all the SPT vertices that u can reach the set of all the SPT vertices that can reach v return the distance from u to v return  If the two vertices are not in the same community we can get the shortest path distance by test the SPT The efìciency of this step is depend on how we handle the query on the SPT Usually we can answer it in constant time  For arbitrary vertex pair u v each local BFS for u and v of the original graph G takes time and the shortest path computation on the SPT can be done in constant time if we adopt a suitable index structure for it V EXPERIMENTAL EVALUATION In this section we empirically evaluate the performance of our SPTI framework on both real and synthetic datasets Particularly the following problems are considered Whether our method really reduce the index size indeed Whether the data processing speed can be increased signiìcantly by the SPTI Whether the SPTI can improve other algorithms performance To answer the questions mentioned above we study the following state-of-the-art shortest path distance query answering schemes and their SPT counterparts BFS the classical online bread-ìrst search 2HOP the 2HOP distance labeling HCL the highway-centric labeling approach using approximate set cover algorithm and directed MST with  SPT-2HOP the 2HOP approach combining with our SPT index SPT-HCL the HCL approach combining with our SPT index In each experiment we focus on measuring three important features index construction time index size and query Table I Real Datasets To validate our approaches we rst do experiments on real-world datasets For more attractive and convincible results we purposively collected 8 datasets listed in Table I with their some important characteristics All these datasets are directed sparse and unweighted graphs For those nonDAG graphs we can transform it into a DAG by coalescing strongly connected components SCC into virtual vertices This can be done simply by depth rst search DFS The size the selected graphs range from several thousand to almost two million vertices CiteSeer is collected by The Koblenz Network Collection KONECT which is about the digital library for scientiìc and academic papers primarily in the elds of computer and information science The others are all collected by Stanford Large Network Dataset Collection SNAP Their brief introductions are as follow 1 p2p-Gnutella08-31 a sequence of snapshots of the Gnutella peer-to-peer le sharing network from August 25 to 31 in 2002 2\ote a free encyclopedia written collaboratively by volunteers around the world 3\ec anonymized data of the Pokec which is the most popular online social network in Slovakia 4 cit-Patents U.S patent dataset is maintained by the National Bureau of Economic Research Table II shows the index construction time for different query answering approaches From the result we can see that our algorithm is much faster than other counterparts especially for those large graphs In addition 2HOP and HCL cannot work on the graphs which have more than hundreds of thousands of vertices due to insufìcient scalability However by combining our method both 2HOP and HCL can handle large graphs with millions of vertices 
u can reach v in 2 hops has the shortest path to 2 Answering the query by the SPT A Experimental Setup B Real Datasets 
Algorithm 4 input output then 4 do 8 for do 9 if then 10 Complexity 
002 V V each v V each v V v V O N u E u N v E v 
p2p-Gnutella08 6,301 9215 9 p2p-Gnutella09 8,114 11,717 9 p2p-Gnutella31 62,586 73702 11 p2p-Gnutella30 36,682 43,379 10 p2p-Gnutella25 22,687 23,651 11 CiteSeer 723,131 790,552 10 wiki-Vote 8297 12,374 7 soc-Pokec 1,632,803 2,964,437 11 time For query time we measure it by answering 100,000 complete random queries The index size of a SPTI approach consists of two parts the index size of the shortest path trunk and the label size of 2HOP or HCL on the trunk The construction time of a SPTI method consists of the time cost of constructing the shortest path trunk and the time cost of constructing the label of the trunk All algorithms mentioned in this work are implemented in C based on the Standard Template Library STL All experiments are performed on a Windows 7 machine with AMD-3870K 3.0GHZ and 8G RAM 
199 


E  V V E  V  E  V  E  V  V  K 
6 6 6 6 6 6 6 6 
1 31 10 4 35 10 2 58 10 1 5 10 2 32 10 2 35 10 1 11 10 1 29 10 
                    
   V  V  V    
p2p-Gnutella08 21,939 1,601 13 10 p2p-Gnutella09 27380 2,101 14 13 p2p-Gnutella31 4.75106 137,092 628 215 p2p-Gnutella30 1.43106 41,180 213 91 p2p-Gnutella25 365,157 18,647 117 45 wiki-Vote 223,179 15,760 101 20 CiteSeer   522,590 soc-Pokec   p2p-Gnutella08 4,983 5,041 9,877 p2p-Gnutella09 4,674 4,712 12868 p2p-Gnutella31 4,467 4,534 121,717 p2p-Gnutella30 5,309 5,373 65,429 p2p-Gnutella25 3,222 3,377 32,434 wiki-Vote 2,070 2,145 2235 CiteSeer 3323 3425 4505 soc-Pokec 5704 6412 66527 Table IV Result of Synthetic Datasets\(Construction Time 5K 2,595 1,260 9 10 10K 10,615 4,948 21 22 50K 348,523 132,990 699 204 100K 518,809 2,006 917 500K   173,784 39,277 Table V Result of Synthetic Datasets\(Construction Time 5K 14,547 7,591 12 10 10K 68,640 30,588 22 23 50K 980,552 637 211 100K   4,912 870 500K   490,499 52,584 Table VI Result of Synthetic Datasets\(Construction Time 5K 479,382 207,887 19 11 10K 105 60 50K   7,601 285 100K   14625 2134 500K   29,764 
C Synthetic Data 
1 5 2 0 3 0 500 
Table II Result of Real Datasets\(Construction Time Figure 3 shows the index size for different query approaches The label size for online BFS is not applicable The 2HOP and HCL can provide comparatively reasonable index size for small and sparse graph However for dense graphs such as wiki-vote the performance of them drops dramatically The biggest characteristic our SPT algorithm is that it can handle dense graphs efìciently Because the SPT is really sparse for the original graph it is very t for 2HOP and HCL to continue construct index Figure 3 The Index Size of Real Datasets For query time we can see the result in Table III Because the query time for 2HOP and HCL is nearly constant time we mainly compare our algorithm with the online BFS To make the comparison result more meaningful we do not use completely random queries which would produce too much negative queries Sometime there are even no positive queries in one experiment This is highly unlikely practical applications which pay more attention to positive queries Therefore we select the vertices whose out-degree is larger than 0 and select the target whose in-degree is larger than 0 This ensures that there must be some vertices visited in a query From the result we can conclude that our method improves the query efìciency signiìcantly in most of the dataset especially for those large and dense graphs This is mainly because the time cost of local BFS is almost negligible We further test our method on synthetic datasets too In this part we mainly investigate how density affects the algorithms performance All synthetic datasets are generated by Erdos Renyi Modes\(ER a classical random graph model We set the density from 1.5 to 3 and vary from 5K to 2,000K We still test the algorithms used above i.e 2HOP HCL STP-2HOP and STP-HCL in this experiment Table III Result of Real Datasets\(Query Time    The detailed statistics of construction time of these datasets can be seen in Table IV-VI The result shows with increasing the density and size of graphs the performance of 2HOP and HCL decrease drastically When the size of vertex comes to 100K they cannot continue to work any longer However our algorithm can work well even when the size of vertex come to 1M Especially our approach is very applicable to high density graphs The SPT itself is a very sparse subgraph which can improve the construction and query efìciency of 2HOP and HCL greately Table VII to IX report the index size of four approaches Both SPT-2HOP and SPT-HCL reduce the index size greatly The data shows our index size at least an order of magnitude less than 2HOP and HCL It is very meaningful for large graphs query In fact our approach still work efìciently even when  Due to space limitation we omit these data Figure 4 shows the query time of different query ap 
              
Dataset 2HOP HCL SPT-2HOP SPT-HCL Dataset SPT-2HOP SPT-HCL BFS 2HOP HCL SPT-2HOP SPT-HCL 2HOP HCL SPT-2HOP SPT-HCL 2HOP HCL SPT-2HOP SPT-HCL 
200 


2 
1 5 2 0 3 0 
5K 33,824 23,579 1,809 1,745 10K 69,119 47,003 3,956 3,613 50K 345,173 237,503 20,401 18,401 100K 700,927 482,481 40,676 36,856 500K   203,263 184,257 Table VIII Result of Synthetic Datasets\(Index Size 5K 84,858 60,764 1,964 1,746 10K 172,572 123,230 3,650 3,294 50K 902,141 647,649 20,100 17,854 100K   41,887 37,002 500K   202,491 180,404 proaches We vary b c 5K 421,903 312,404 2,087 1,818 10K 923,216 699,404 4,821 4,225 50K   24,290 20,905 100K   48,859 41,890 500K   241,212 207,648 threshold 5K 745 614 741 725 786 4,353 953 86 35 63 10K 1,469 1,262 1,479 1,491 1,513 7,874 1,715 3,351 140 63 50K 7,639 6,374 7,540 7,499 7,692 43,231 11,467 2,324 811 90 100K 151,151 12,495 15,162 151,491 15,796 86,840 26,371 4,342 1,489 516 500K 74,875 62,821 75,299 75,820 75,750 456,977 124,311 22,254 8,864 2,465 1000K 49,971 126,628 150,628 152,425 15,628 902,019 247,750 44,408 18,796 74,911 Table XI Size of SPT on Random Graphs 5K 610 467 612 611 606 45,521 989 75 35 18 10K 1,202 925 1,261 1,231 1,260 11,922 2,632 867 694 767 50K 5,980 4,928 6,178 6,137 6,367 60,661 15,503 1,845 1,148 452 100K 11,913 9,525 12,727 12,296 12,907 119,466 34,423 6,155 1,726 571 500K 59,729 47,091 61,648 61,865 64,159 658,330 185,288 29,017 11,674 2,448 1000K 118,876 95,193 122,709 124,457 128,875 1,290,166 36,8647 56,351 23,163 5,008 VI RELATED WORK Many approaches have been proposed for processing source-to-target shortest path and distance query and are related to several key questions i.e single-source shortest path distance computation One of the most well-known methods for this issue is Dijksras algorithm It solv es the single-source shortest path problem for a graph with non-negative edge path costs producing a shortest path tree This algorithm is often used in routing as a subroutine in other graph algorithms or in GPS Technology and can be implemented with 
             011             
E  V  E  V  V E  V E  V  002 002 002 E  V E  V O n 
Table VII Result of Synthetic Datasets\(Index Size   from 5K to 1000K so as to be more persuasive Interestingly the higher of the graphs density the more our method is efìcient When  the query time tend to constant time a Figure 4 Result of Synthetic Datasets\(Query Time In the last part of our experiment we test how the Table IX Result of Synthetic Datasets\(Index Size  affects the number of vertices and edges of the SPT Apparently if increases the size of the SPT would reduce and the local search cost would increase accordingly Therefore we do not suggest set too large In reality for many real world data graphs the index size would reduce signiìcantly even when  The result can be seen in Table X to XII We show the dataset with the density with 4 5 and 6 for space limitation Table X Size of SPT on Random Graphs   time Another famous method is Bellman-Ford algorithm  which is capable of handling graphs in which some 
                      
V V E  V  E  V  E  V  V V V 
1 5 2 0 3 3 0 2 4 5   
2HOP HCL SPT-2HOP SPT-HCL 2HOP HCL SPT-2HOP SPT-HCL 2HOP HCL SPT-2HOP SPT-HCL 2 3 4 5 6 2 3 4 5 6 
201 


out in out in out in 
5K 496 390 550 556 570 3,836 881 21 2 6 10K 987 758 1,090 1,084 1,120 11,434 2,751 175 146 109 50K 4,969 3,701 5,103 5,079 5,359 79,351 18,589 3,604 421 114 100K 9,671 7,425 10,269 10,357 10,757 165,256 40,460 7,293 1,534 494 500K 4,881 36,581 51,889 52,024 53,980 855,651 252,588 30,401 12,819 2,665 1000K 97,071 75,103 102,768 103,157 108,068 1,702,807 467,340 62,431 29,713 5,579 of the edge weight are negative numbers and can runs in 
3 
    003 
E  V O nm O n L u L u L u L v s L u L v 
Table XII Size of SPT on Random Graphs   For unweighted graphs we can deploy Breadth First Search which can compute the single-source shortest path problem in O\(m+n To compute the all-pairs shortest paths we can directly use Dijkstra n times or Floyd-Warshall algorithm which all need time These algorithms are all not suitable for large scale graphs The 2-HOP algorithm proposed by Conhen et al adopts labeling mechanism to answer reachability and distance query Each vertex u records a list of intermediate vertices which it can reach along with the shortest distance and a list of intermediate vertices which can reach it along with the shortest distance To compute the shortest distance between u and v we simply check all the common intermediate vertices between and and choose the vertex s such that s,v is minimized for all  In recent years a lot of algorithms have been proposed for graph reachability queries 6 7 8 9 10 These algorithms mainly discover the connectivity of relevant vertexes but cannot compute the shortest distance between them Vertex cover approaches has lately become the mainstream 10 Through the selecting an approximate minimum vertex cover and constructing an index the reachability query can be answered efìciently However they still do not consider the distance computing Ruoming Jin et al put forward a highway-centric labeling approach in which is our counterpart in e xperiment They deploy a novel labeling scheme which select a group of vertices to construct a tree-like highway index structure for answering distance queries in large sparse graphs It can provides better labeling size than 2-hop However in our experimental result we can nd that this method cannot handle large graphs efìciently in compare with our algorithm VII C ONCLUSION In this paper we propose a new index construction approach SPTI to answer the shortest path distance query We nd that the distribution imbalance of the edges between the vertices in the real world graphs and utilize the vertices which have more connection with others as index to answer the shortest path distance query To nd a near optimal salutation we propose to selectively choose a group of vertices from the original graph to create a trunk for preserving the shortest paths information and compact the index size Compared with existing methods such as 2HOP online BFS and HCL our algorithm can reduces the index size signiìcantly and answers the shortest path distance query efìciently In the future we plan to apply our index structure on dynamic graphs A CKNOWLEDGMENT This research was supported by the Normal Project Foundation of Education Department of LiaoNing Province Grant No L2012045 R EFERENCES  E W  Dijkstra A note on tw o problems in conne xion with graphs Numerische Mathematic 1\(1 Dec 1959  Richard Bellman On a routing problem Quarterly of Applied Mathematics 16 8790 1958  Eric W eisstein Flo yd-W arshall Algorithm W olfram MathWorld Retrieved 13 November 2009  Edith Cohen Eran Halperin Haim Kaplan and Uri Zwick Reachability and distance queries via 2-hop labels SIAM J Comput 32\(5 2003  Haixun W ang Hao He Jun Y ang Philip S Y u and Jef fre y Xu Yu Dual labeling Answering graph reachability queries in constant time In ICDE 2006  Y angjun Chen and Y ibin Chen An Ef cient Algorithm for Answering Graph Reachability queries In ICDE 2008  J Cheng J X Y u X Lin H W ang and P  S Y u F ast computing reachability labelings for large graphs with high compression rate In EDBT 2008  J Cheng J X Y u X Lin H W ang and P  S Y u F ast computation of reachability labeling for large graphs In EDBT 2006  Ruoming Jin Ning Ruan Saikat De y and Jef fre y Y u Xu SCARAB Scaling Reachability Computation on Large Graphs In SIGMOD 2012  James Cheng Zechao shang and Hong Cheng K-Reach Who is in Your Small World In VLDB 2012  Ruoming Jin Ning Ruan Y ang Xiang and V ictor E Lee A Highway-Centric Labeling Approach for Answering Distance Queries on Large Sparse Graphs In SIGMOD 2012  W asserman S and F aust K Social Netw ork Analysis Cambridge Cambridge University Press 1994  J Scott Social Netw ork Analysis A Hand-book London Sage Publications 2002  F  Chains Karinthy  Ev erything is Dif ferent Atheneum Press 1929  P  Sanders and D Schultes Highw ay hierarchies hasten e xact shortest path queries In 17th Eur Symp Algorithms\(ESA 2005  http://k onect.uni-k oblenz.de/netw orks/citeseer  accessed May 18 2013  http://snap.stanford.edu/data accessed May 18 2013 
  
V 
6                 
2 3 4 5 6 
202 


state of innovation stakeholder  node PQ It  s a balanced node Based on this, we could calculate the  node PQ Calculation process is: set different inn ovation stakeholders state i U  and j U Value of ij 000T can be get from  innovation time difference. Innovation stakeholdersí social effect and industrial effect can be obtained upon ij B  and ij G set according to relation between innovation stakeholders  Model 4.1 points out  that the value of Gij  directly affects social benefits and sector benefits. Large Gij  can lead to increasing benefits of the entire industry and the entire social growth Bij reflects big organizationís impact on businesses. Only strengthening the inter agent association within big organization and enhancing the str ategic partnership between enterprises can jointly promote the development of the entire industry, and bring more social benefits, so that each agent can be improved   5 Summary This paper puts forward the concept of the big organization based on the CSM t heory. It introduces the basic implication of the big organization and theoretical framework of the big organization including: the big organization's perspective  overall perspective, dynamic perspective, and new resource perspective; the big organizat ionís sense  the purpose of the organizational structure is innovation, organizational activities around the flow of information, breaking the traditional organizational structure, encouraging self run structure, and blurring organizational boundaries; the big organizationís platform  the platform ecosystem of the big organization ; the big organizationís operation mode  borderless learning mode, and cluster effect; the big organizationís theory  active management theory  leading consumers, and culture  entropy reduction theory  negative culture entropy and humanistic ecology theory  inspiring humanity, and circuit theory  a virtuous circle, and collaborative innovation theory  collaborative innovation stakeholder. This paper also discusses culture entropy reduction theory of the big organization  negative culture entropy, and coordinated innovation theory  innovation stakeholders collaboration. Culture entropy change model and collaborative in novation model are constructed   The research has just begun for the big organization. It also needs further improvement but remains the trend of the times   Reference  1  Gordon Pellegrinetti, Joseph Bentsman. Nonlinear Control Oriented Boiler Modeling A Benchmark Problem for Controller De sign [J  I E E E tr a n s a c tio n s o n c o n tr o l s y s te m s te c h n o lo g y 2 0 1 0  4 1\57 65  2  Klaus Kruger, Rudiger Franke, Manfred Rode Optimization of boiler start up using a nonlinear 457 


boiler model and hard constraints [J  E n e r gy 201 1 29   22 39 2251  3  K.L.Lo, Y.Rathamarit  State estimation of a boiler model using the unscented Kalman filter [J  I E T  Gener. Transm. Distrib.2008 2 6\917 931  4  Un Chul Moon, Kwang. Y.Lee. Step resonse model development for dynamic matrix control of a drum type boiler turbine system [J IE E E  T ra nsactions on Energy Conversion.2009 24 2\:423 431  5  Hacene Habbi, Mimoun Zelmat, Belkacem Ould Bouamama. A dynamic fuzzy model for a drum boiler turbine system [J  A u to m a tic a 2 0 0 9 39:1213 1219  6  Beaudreau B C. Identity, entropy and culture J   J o ur na l  o f  economic psychology, 2006, 27\(2 205 223  7  YANG M, CHEN L. Information Technique and the Entropy of Culture J  A cad e m i c E x ch a n g e  2006, 7: 048  8  ZHANG Zhi feng. Research on entropy change model for enterprise system based on dissipative structure J  Ind ustrial  Engineering and  Management 2007, 12\(1\ :15 19  9  LI Zhi qiang, LIU Chun mei Research on the Entropy Change Model for Entrepreneurs' Creative Behavior System Based on Dissipative Structure J  C h i n a S of t S c i e n c e  2009   8  1 62 166   458 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





