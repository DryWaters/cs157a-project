  ith the Pattern Repository  e  Wu  Dept of Math and Computer Science Colorado School of Mines Golden, Colorado 80401, USA Department of Computer Science University of Vermont Burlington, Vermont 05405, USA rrelue@mines.edu xwu@emba.uvm.edu  ct  Efficient algorithms for mining frequent patterns are crucial to many tasks in data mining Since the Apriori algorithm was proposed in 1994, there have been several methods developed to improve its performance. However 
most still adopt its candidate set generation-and-test approach. In addition, many methods do not generate all frequent patterns making them inadequate to derive all association rules The calculation of association rules from raw itemsets using Apriori is an intractable problem  By using a new structure called a Pattern Repository the same rules can be derived in linear-time proportional to the number of unique items found  If the selected rules are all we need the calculation can give results in realtime  In addition the calculation can easily be divided 
into subsets for distributed processing and large datasets can be stored on disk that adds to the I/O overhead, but still offers a linear time calculation of rules  n  Data mining refers to the extraction of previously unknown and potentially useful information from large amounts of data.  Three fundamental areas of data mining are association analysis clustering and classification A fundamental part of the data-mining task is finding frequent patterns in a given dataset. Frequent patterns are ones that occur at least a user specified number of times 
the minimum support in the dataset This technique allows us to perform essential tasks such as discovering association relationships among items, verify correlations and perform sequential pattern mining The Apriori algorithm finds frequent patterns by employing a bottom-up search [1   I t  g e n e r a t e s  c a n d i d a t e  sets starting at size 2 up to the maximum frequent set size At each pass, it determines which candidates are frequent by counting their occurrences that exceed the minimum support threshold  It then checks how often these candidates occur together to measure the support.  Due to 
combinatory explosion this leads to poor performance when frequent pattern sizes are large To avoid this problem some algorithms output only 223maximal frequent\224 sets 2,3,4    M a x M i n e r  u s e s  a  bottom-up search and a heuristic to try to identify frequent sets as early as possible [2    P i n c e r S e a r c h  u s e s  a bottom-up search in conjunction with top-down pruning 4    E v e n  t h o u g h  p e r f o r m a n c e  i m p r o v e m e n t s  m a y  b e  substantial the 223maximal frequent\224 sets have important limitations A complete set of valid rules cannot be extracted without complete support information of the 
frequent subsets  Almost all previous algorithms use the candidate set generate-and-test approach Frequent Pattern FP Tree based mining is an exception 5    I t  h a s  p e r f o r m a n c e  i m p r o v e m e n t s  o v e r  Apriori since it uses a compressed data representation nodes and a tree structure\ and does not need to generate candidate sets  However FP-tree-based mining uses a complex data structure and performance gains are very sensitive to the support threshold setting  In addition large databases may exceed available memory space even when the data is compressed 
In a previous paper the concept of a Pattern Repository was described 6    T h e  f u n c t i o n  o f  t h e  repository is to read and store patterns from the raw itemsets in a compact form.  Later, the information about association rules could be derived from the Pattern Repository and calculated for display  The Pattern Repository information allows pattern data to be extracted in an organized way such that calculation time is spent only on possible rules and candidate generation in Apriori is avoided.  The resulting rule generation time is linearly 
proportional to the number of rules found and is not effected by the size of the database  rk    A good example of association rule mining is market basket analysis  A transaction consists of a collection of Proceedings of the 2002 IEEE International Conference on Artificial Intelligence Systems \(ICAIS\22202 0-7695-1733-1/02 $17.00 \251 2002 IEEE 


  items purchased at a grocery store or a supermarket These are simply the items that went into the basket.  By finding association rules among the transactions the retailers can derive some useful information about the customers buying habits.  This information could be used in turn to better market and advertise what the store sells We form association rules by using the Apriori algorithm as described below An association rule is an implication of the form X  Y where X and Y are each a set of items which occur together in a significant number of baskets.  The support S is the percent of the total transactions in which X and Y occur together  The confidence C is the number of transactions where X and Y occur divided by the number of transactions where X occurs expressed as a percent An interesting rule is one where S and C meet some minimum threshold requirements referred to as the minimum support and minimum confidence respectively An example is as follows.  Assume the minimum support is 5% and minimum confidence is 50%. Bread and peanut butter are bought together in 5 of the transactions S Whenever bread appears in a transaction peanut butter also appears about 50 of the time C  Since these figures for S and C meet or exceed the minimums then there is a valid rule of the form Bread  000\002\001\004\003\006\005\b\007\b\t\013\n\f\007\b\t t\r\001\006\016\020\017 Note that when peanut butter appears in a transaction and bread is bought only 40 of the time the rule fails because C < 50 Most of the previous work in association rule development uses some form of Apriori.  The steps are as follows a Find all frequent items Scan the database to find the frequency of occurrence of each item and decide which ones are above the threshold \(S b Generate itemsets by checking all of the possible combinations of the frequent items to see if they meet the minimum support requirement. For example, if A B and C are frequent items, then check if {A, B}, {B C and A C meet the minimum support and are thus frequent itemsets c Generate longer itemsets by combining valid items from step b  If the itemsets A B and B C are valid, then they can be combined into {A, B, C}.  Note that each item can appear only once and that it would not be useful to try patterns other than those found in step b  If A B or B C were not a frequent itemset then {A, B, C} could not be one since {A, B and {B, C} are part of it d If longer combinations of rules are possible then continue until no longer combinations can be generated e Finally, check the confidence level \(C\ for each of the possible subsets of the itemsets to find the valid rules If {A, B, C} is valid then we must check A  021\023\022\025\024\027\026\031\030\023\024  B C  032\034\033\036\035  037\023\032\034\033\036 \031!\023\033"\037\023\032\034\033\036 \031  035\025\033\036  037\023\032\034\033 B A B     031   4 135/1 confidence level  The above algorithm finds all of the length 1 frequent itemsets and then proceeds to find the length 2 3 etc frequent itemsets  Note that at each level the frequency count of each frequent itemset must be retained so that the confidence level can be computed  For instance as shown in step 5 above, to compute confidence for rule A  687:9<;\002=\034>@?\025ACBEDGFIH\bJEKL>NM O\020?\004PLBE?\006JEQ\006RSK<MUTWV\006JEXC687:9<;\002=UY  If the database is large then there are many frequent items and many possible combinations.  This results in an exponential growth in the number of combinations to consider when generating candidates  The retention of previous rules and search of new rules can become very expensive.  This is especially true if there are many long rules, which pass the support test.  Length 10 rules require 10 passes through the database and a search for every possible rule combination  Some obvious ways to make this process cheaper include not scanning the entire database on each pass and eliminating the candidate generation  Tree  The FP-Tree 5  a l g o r i t h m  s c a n s  t h e  d a t a b a s e  t w i c e    The first time is to determine the frequent items that will be used to create the FP-tree and sort items in frequency order The second scan actually creates the tree as a graph  The top node of the graph is the root  The first node underneath the root is the most frequent item for each record scanned along with a count  Many records when sorted by most frequent items first, will contain the same most frequent item The basic process involves laying out each record in frequence order and creating a node for each item under the root  As more items are added, there will be common prefixes.  For instance, one record A,B,C has a common prefix with A,B,D namely {A,B}.  Nodes are not repeated, but the counts for A and B nodes are incremented When the C node is reached a new node at the same level for C is created with the value D assuming it does not already exist Note that non-frequent items are ignored in the FP-tree construction.  In addition, a linked list of frequent items is also maintained thus every occurrence of A is linked to every other node with A in the tree The inherent advantages of this structure are the relatively compact representation of the database and the exclusion of non-frequent items.  This makes it easy to fit the FP-tree into memory and thus easy to scan for rule development  After completion of construction the tree is mined for frequent patterns as follows  a Derive a set of conditional paths  These are suffix patterns from the FP-tree b Construct a conditional FP-tree for the conditional paths Proceedings of the 2002 IEEE International Conference on Artificial Intelligence Systems \(ICAIS\22202 0-7695-1733-1/02 $17.00 \251 2002 IEEE 


  c Explore the conditional tree recursively to find the frequent patterns and determine the support level for each pattern found   Note that the tree contains only frequent items No time is wasted with non-frequent items.  In addition, since the most frequent items are near the top or root of the tree the mining algorithm will find most of the valid frequent patterns early in the search The FP-tree mining algorithm works well but there are some limitations a The database must be scanned twice b Update of the database requires a complete repetition of the scan process and construction of a new tree because the frequent items may change with database update c Lowering the minimum support level requires a complete rescan and construction of a new tree d The mining algorithm is designed to work in-memory and performs poorly if a lot of memory paging is required  To achieve the fastest rule development it would be desirable to avoid scanning the database twice and eliminating the rescan requirement when an update occurs    The use of the Pattern Repository 6  a v o i d s  e x t r a  database scans and the update problems mentioned in Section 2.2.  The Pattern Repository takes the itemset data in one or more batches and stores the itemset information in a compact form  The Repository is always up to date in that any and all information inserted is immediately ready for use  Each unique symbol in the itemset is replaced with a token which is just a unique number Each itemset is replaced with a number representing its pattern in the database  The patterns contain sufficient information to reproduce the items in each itemset and statistics about each item The primary purpose of the Pattern Repository is to deliver selected patterns and statistical information to calculate the association rules attributed to them  The selected patterns refer to the ability of the Pattern Repository to deliver only those patterns, that contain one or more selected tokens  As we shall see later this greatly simplifies the association rule derivation.  We will select and count only those items that contribute to the rules instead of generating and counting candidates  Output  The basic output of the Pattern Repository is the patterns in token form  Note that the patterns can be filtered by one or more tokens that might be present in each pattern.  Thus we can specify the filter as all of the patterns that contain {12} or {2, 5, 12 This is a good starting point, but some improvements are necessary to prevent redundant processing of unnecessary items The necessary refinements are as follows a Removal of infrequent items based on the current support setting  The infrequent items do not contribute to the association rules b Removal of those items that are part of the filter Their presence is already assumed and the count is simply the number of records returned c Sorting the tokens found in ascending order  This allows elimination of duplicates when processing Otherwise the duplicates would be counted twice d Counting of the frequent tokens found e Allow specification of an extra filter parameter Removal of all tokens below a certain value which will be called the minimum value token  The above refinements facilitate counting just the tokens that are needed to derive the valid itemsets Otherwise there would extra overhead in counting tokens which do not contribute to the final rules or which have already been counted  2 Association Rule Search ith the Pattern Repository  The rule development with the Pattern Repository can be described as follows  The search starts by iterating through each frequent item in the itemset using the token number in ascending order.  That is token \2231\224 is checked to see if it qualifies as a frequent item and then token \2232\224 and so on.  As each token is determined to be frequent, it is processed as follows  a The Pattern Repository extracts records that contain the token and the counts of the other items in the transactions b The other items that meet the minimum support level are put in numerical order and paired with the original token in a list This list is permanently added to the rule list along with the counts  This will be needed later to test which rules meet the minimum confidence threshold and thus are valid c The list pairs are now processed one at a time in numerical order  Once again the Pattern Repository extracts those records that contain the token pairs and counts the tokens that meet the minimum support level d Step \(c\ continues in a depth-first manner until none of the tokens found meet the minimum support threshold Assuming that the minimum support allowed it the first rule explored would be 1,2,3,4,5,6 etc  No repeats are allowed  To prevent exploration of repeats, tokens found that are less than the highest end token are also ignored  The assumption is that numerically smaller combinations have already been Proceedings of the 2002 IEEE International Conference on Artificial Intelligence Systems \(ICAIS\22202 0-7695-1733-1/02 $17.00 \251 2002 IEEE 


  explored and thus a wasted repeat calculation  For example if 1,2,5,6 is explored then 1,2,3,4 has already been tried and failed e When no tokens are found the depth-first search returns to the previous level and processes the next itemset on the list f This continues until all of the combinations found at all levels are tested. The process stops when all of the pairs found in step \(b\ are processed g The process repeats for the next single token in the list starting with step \(a   The basic process is simply an exhaustive depth-first search of all possible combinations  Note that rules which cannot exist are not examined  For instance if 1,2} never occurs, it will never be counted or placed on a search list.  In addition, repeat rules \({1,2} is the same as 2,1 are not done again because of the numerical precedence.  Note that {1} and {2} counts are retained so we can decide if 1  ZW[L\\@Z   cE_@^`d eEd Cf\b confidence threshold  3 Time Complexity of the Pattern Repository Search  The time complexity can be numerically expressed by looking at the steps involved in the search and the operations of the Pattern Repository a If the items are maintained in a Hash table then the lookup of patterns in the Pattern Repository is O\(1 or O\(1  g depending on loading. If the lookup is done on a modern database the search would require log 2 m similar to a binary tree search where m is the number of unique items  However in the case of a database disk I/O would mask this  Effectively the overhead can be expressed simply as a constant and can be ignored b The search iterates through each frequent item. So the starting point is n  which is the number of frequent items c At each level in the depth-first search, we must assign some estimate of the percent of \223 p 224 combinations that will meet the minimum support criterion and be used for the next level  No true estimate can actually be done since the number is dependent on the actual data d The search continues to some maximum depth 223 q 224 which again must be estimated  The first level is n frequent items.  At the second level for each frequent item from level one we get some fraction of n  items that pass the minimum support level and so on.  The resulting equation is n * \(n * p 2 n * p 3 205 n * p q  The variable \223 q 224 will be less than the number of frequent items If the value for 223 p 224 is based on probability alone the following holds.  Assuming 10 items are taken 2 at a time random probability would give 1/10 * 1/10 = 1/100.  100 items taken 3 at a time would be 1/100 * 1/100 * 1/100 1/1,000,000 etc  On the average this is probably true Most of the tests for the minimum support level will fail because the appearance of items together in a transaction is in fact a result of probability.  The association rules that survive the test are in fact exceptions that exceed random probability An important point is that the n * p 2 n * p 3 205 \(n p q   part of the equation above will reduce to a constant For instance, if p is 1/10 and n is 1000 then n * p 2 10  That is there are 10 rules generated for each item in the first level on the average.  However, if we continue to n p q  at some point this part will drop below 1.  Assume that this happens at q 226 3  the equation becomes n  n  p 2 n * p 3 205 \(n * p q - 4  The parts at q - 3\, \(q \226 2 etc are not tested because the list is empty.  The portion \(n p 2 n  p 3  205 n  p q  4   is replaced with a constant C  This implies that the number of rules tested is C * n  How large is the constant It is related to how much association there is in the database For instance if no rules pass the minimum support threshold then C   1 We search for pairings at the second level, find none and stop.  In short the actual run-time is still very sensitive to the minimum support setting  A lower setting would make the C larger and the search take longer  4 An Example of Search ith Pattern Repository  Taking a look at the example database in Figure 1, the first step of rule generation with the Pattern Repository skips the frequent item determination  This information was stored when the Pattern Repository was created.  All that is required is iterating through each of the frequent items in their token order.  Thus if A, B, C, D, E, F, G are frequent items they are replaced by their tokens 1,2,3,4,5,6,7 The pattern repository also reports 250 records in the system and these items are frequent based on a minimum support of 24% or 60  Figure 1. A Sample Database Frequency List h i j\004k l\bm/n\rj\004o pLq jsr\027tsj\004osuwv x y y*z\004z   027  200 y*z 200 201 202 203 204  205/\206 p 206 y\r\202\027z 207 203 y*\203  The iteration proceeds as follows The starting tokens are processed in token order.  The patterns which contain the 2231\224 token are retrieved from the pattern repository.  Since token \2231\224 occurs 100 times the equivalent of 100 patterns are retrieved.  The Pattern Repository will remove the infrequent items in the pattern Proceedings of the 2002 IEEE International Conference on Artificial Intelligence Systems \(ICAIS\22202 0-7695-1733-1/02 $17.00 \251 2002 IEEE 


  so they will be ignored  Also the target token 2231\224 is removed since its presence is assumed in each pattern.  If an item is present in the patterns retrieved the implicit assumption is that they are frequent and are not part of the target.  The minimum value token is set to 0. This means that any tokens are allowed which are not part of the filter and are frequent A count is made of all of the other frequent items that are contained in these patterns.  Note that the occurrence the number of times that token \2233\224 occurs is counted.  If 75 occurrences of  2233\224 are counted the support of this itemset is 75/250 = 0.3 or 30%.  After all of the items are counted those meeting the minimum support are stored along with the count. 30% meet the minimum support and itemset {1,3} is stored with a count of 75.  This is called a frequent itemset If the sets {1,3}, {1,5} and {1,7} are the only itemsets that satisfy the support requirement then each is recursively explored depth-first.  Working in token order all of the patterns of any length containing 1,3 are checked first  Each successful frequent itemset and its count are recorded in a hash table  These successful itemsets will be checked as described in Section 2.1 Apriori step e to determine which of them meet the minimum confidence level and thus are valid rules In the recursive processing, we first look at all records that contain {1,3} and set the minimum token value to 4 We already know that \2232\224 failed the support test and \2233\224 is already in the pattern.  Remember that the processing is being done in token order, thus the last item in the itemset specifies the minimum token value The assumption is that lower value tokens have already been tried in order and failed.  In the perfect case the itemset would build as 1,2,3,4,5,6,7 assuming that all of the intermediate itemsets met the minimum support requirement while performing the depth-first search In this case after {1,2} is checked recursively, then {1,3} is checked  Then  1,3,2 would come up since {1,2,3} is already valid.  We do not want to check this again. This is avoided by setting the minimum token value to \2234\224 Returning to our example the tokens for the 1,3 filter are counted  The only ones left are 2234\224 2235\224 2236\224 and \2237\224, since the minimum token value is 4.  Tokens \2234\224 and 2236\224 are not counted because they are not present in the patterns  Tokens 2235\224 and 2237\224 do meet support requirements The frequent itemsets 1,3,5 and 1,3,7 are recorded Now filter 1,3,5 is checked and the only frequent itemset found is {1,3,5,7}.  This is recorded.  Then filter 1,3,5,7 is processed with no results and then filter 1,3,7 is processed with no results either  They do not have any results because the minimum token value is \2238\224 there are no tokens at or above 8 The search falls back to filter 1,5  The only possibilities at this point are 2236\224 and 2237\224 This process will find 1,5,7 a frequent itemset and {1,5,6} will fail In fact we already know it will fail since {1,6} failed.  We could have kept a list of failed tokens, but the overhead in searching would be as bad as extra counting Filter 1,5,7 will not find anything since there are no tokens above 2237\224 The search falls back to 1,7 which again finds nothing The next iteration starts with filter 2 then 3 through 7  Note that the same rules apply as shown above  We would not consider 2,1 since 1,2 has already been considered The final step would analyze the frequent itemsets for valid rules based on the minimum confidence level.  This can be done since each frequent itemset has a count The above process finds all of the association rules that the Apriori process would have found  However many of the expensive parts of the search in Apriori have been eliminated  The only rules which are present are counted and there is no candidate generation   The database records are scanned only once  The Pattern Repository handles database updating automatically.  The time complexity of this process is linear and proportional to the number of frequent items  Algorithm  Note that the search routine described in Section 3.2 can be divided into discrete independent steps  For example the search can be divided among 4 processors One processor could look for all rules containing A.  The second processor could look for all rules containing B etc The load of each search could be split among the available processors In addition the structures for the Pattern Repository can be translated to a disk based storage scheme which adds overhead but does not substantially change how it operates  A disk-based implementation of this algorithm has been tested and can handle several million itemsets with some increase in time overhead but little impact in the linear time complexity of the algorithm  A modern database or proprietary scheme can store the pattern repository information on disk and retrieve the same information using the best available indexing methods With this method, the size of the database is limited only by disk storage and indexing implementation  nd Tree  The Pattern Repository by design handles scaling and updating.  It can be divided among several processors and can be moved to disk storage for large databases  Inmemory operations are not a requirement  The updating process simply adds new data to the repository Proceedings of the 2002 IEEE International Conference on Artificial Intelligence Systems \(ICAIS\22202 0-7695-1733-1/02 $17.00 \251 2002 IEEE 


  The Frequent Pattern Tree design requires an inmemory operation and is not updateable. If support levels or new data are present, the entire database must be read twice again.  However, when all of the rules are processed with one database that fits in memory and with one minimum support value only the FP-Tree consistently works faster than the Pattern Repository based rule development The real purpose of the Pattern Repository is for 1 targeted rules of the form 223all rules that include A B or C\224 and 2 targeted rules in which the minimum support value will change.  Testing, discussed in the next section will show that for commonly available Pentium desktops all of the rules that contain 223A\224 in a 100,000 record set can be solved on the average in about 1 second  In addition, if we analyze for 3 different support levels or 3 different sets of targeted rules it will take about 3 seconds The FP-Tree approach must rescan the entire database and process all of the rules to get the same results.  When the support level decreases, the whole process will have to be repeated Since each process under test conditions takes about 20 seconds for 100,000 records it will be much slower for these types of problems    The testing platform was a Pentium 4 1.5 gigahertz with 500 megabytes of memory  The operation system was Windows 2000 professional. Java JDK 1.3 was used to implement in-memory versions of the Pattern Repository and the Frequent Pattern Tree  The FP-Tree Java implementation was based on a technical report 7  of the algorithm and was provided by Hao Huang at the Colorado School of Mines The dataset was generated using the algorithm described in 7    I t  h a s  1 0 0  0 0 0  r e c o r d s  a n d  a n  a v e r a g e  transaction size of 25 The maximal potentially frequent item size on the average was 20  The number of transaction read was varied from 10,000 to 100,000 for the test \(see Figure 2  Figure 2. Processing Time in Seconds 210\212\211 213\004\214s\215w\213s\216w\217\221\220 222/\214s\215 223s\224s\225 210\212\211 226s\226 224L\213L\217 217 226\004\211G\214\230\227\027\226\004\231s\222\020\2158\220 217 222/\211 232 233*\234\020\235 234\004\234\004\234 236 233 236\020\234\020\235 234\004\234\004\234 237 236 240\027\234\020\235 234\004\234\004\234 241 236 242 234\020\235 234\004\234\004\234 233\004\233 237 243 234\020\235 234\004\234\004\234 233L\241 241 233*\234\004\234\020\235 234\004\234\004\234 236\020\234 241  The testing involved targeted rules of the form of all rules containing \223A\224, \223B\224 or \223C\224.  The runs were repeated using a random combination of 3 frequent items and averaged  The support level was set at 2.5 of the number of transactions   There is no practical way to subdivide the problem with the FP-Tree so just the processing time for all rules is reported The advantage of the Pattern Repository is evident for targeted rules.  Note that this same advantage would apply if the support level were changed to 3 different values instead of searching for 3 different targeted rules  This would require 3 runs of the FP-tree algorithm or about 60 seconds for 100,000 records  The rules derived at 3 different support levels with the Pattern Repository would still take about 3 seconds    The proposed method of calculating association rules has several advantages.   The speed of the calculation for targeted rules is fast enough to respond to the user in real time  An even faster response can be obtained if the depth of the search is limited.  Adding new records does not create any update problems which are common in existing implementations  The system is scaleable using multiple processors or disk storage  These scaling routines exact few penalties in terms of time complexity  References  1  R   A g r a w a l  a n d  R   S r i k a n t   F a s t  a l g o r i t h m s  f o r  m i n i n g  association rules. In VLDB \22294 pp. 487-499  2  R   J   B a y a r d o   E f f i c i e n t l y  m i n i n g  l o n g  p a t t e r n s  f r o m  databases. In SIGMOD \22298 pp.85-93  3  M  J   Z a k i   S   P a r t h a s a r a t h y   M   O g i h a r a   a n d  W   L i   N e w  Algorithms for Fast Discovery of Association Rules. In Proc. of the Third Int\222l Conf. on Knowledge Discovery in Databases and Data Mining 1997, 283-286  4  D  I   L i n  a n d  Z  M   K e d e m   P i n c e r S e a r c h   A  N e w  A l g o r i t h m  for Discovering the Maximum Frequent Set In Proc of the Sixth European Conf. on Extending DatabaseTechnology 1998  5  J   H a n   J   P e i   a n d  Y   Y i n   M i n i n g  F r e q u e n t  P a t t e r n s  w i t h o u t  Candidate Generation Proc. 2000 ACM-SIGMOD Int. Conf. on Management of Data \(SIGMOD'00 Dallas, TX, May 2000  6  R  R e l u e   X  W u   a n d  H  H u a n g   E f f i c i e n t  R u n t i m e  G e n e r a t i o n  of Association Rules Proceedings of the 10th ACM International Conference on Information and Knowledge Management Atlanta, Georgia, USA, November 5-10, 2001  7  J  H a n   J  P e i   a n d  Y  Y i n   M i n i n g  P a r t i a l  P e r i o d i c i t y  U s i n g  Frequent Pattern Trees Technical Report 99-10 Simon Frasier University, 1999 Proceedings of the 2002 IEEE International Conference on Artificial Intelligence Systems \(ICAIS\22202 0-7695-1733-1/02 $17.00 \251 2002 IEEE 


and 2 since it is in the second mask meaning the cluster extends two rows This cluster is indicated by the dashed circle Finally mask identifies no clusters since the mask contains no set bits, signifying there are no clusters that be gin at I-ow l and extend through row 3 We now repeat this process beginning with the second row of the bitmap, producing the two clusters as shown Bitmap Masks starting at row 2 row3 1 0 0 mask row2 mask row 1 0 1 1 The process ends when the mask for the last row is com puted Our algorithm can be implemented efficiently since it only uses arithmetic registers, bitwise AND and bit-shift machine instructions We assume that the size of the bitmap is such that it fits in memory which is easily the case even for a 1000x1000 bitmap 3.4 Grid Smoothing As a preprocessing step to clustering we apply a 2D smoothing function to the bitmap grid In practice, we have often found that the grids contain jagged edges or small 223holes\224 of missing values where no association rule was found A typical example is shown in Figure 7\(a These features inhibit our ability to find large, complete clusters To reduce the effects caused by such anomalies we use an image processing technique known as a low-pass jilter to smooth out the grid prior to processing 8 Essentially a low-pass filter used on a two-dimensional grid replaces a value with the average value of its adjoining neighbors thereby 223smoothing\224 out large variations or inconsistencies in the grid The use of smoothing filters to reduce noise is well known in other domains such as communications and computer graphics Details of our filtering algorithm are omitted for brevity, but Figure 7\(b nicely illustrates the res ults Experiments using the association rule support values instead of binary values were also performed yielding prom ising results See Section 5 3.5 Cluster Pruning Clusters that are found by the BitOp algorithm but that do not meet certain criteria are dynamically pruned from the set of final candidate clusters Typically we have found that clusters smaller than 1 of the overall graph are not use ful in creating a generalized segmentation Pruning these smaller clusters also aids in reducing \223outliers\224 and effects from noise not eliminated by the smoothing step In the case where the BitOp algorithm finds all of the clusters to be suf ficiently large no pruning is performed. Likewise if the al Input 11 R the number of bins for attributex 12 C the number of bins for attribute Y 13 BM, the bitmap representation of the grid  is the ith row of bits in the bitmap output 01  clusters of association rules row  1 while row i R do begin RowMask  set all bits to 222 1\222 PriorMask  BM[row height=O for r=row riR r do begin RowMask  RowMask  BM[row if RowMask  0 do begin I Locate clusters in PriorMask I process_row\(PriorMask,height  break end-if if RowMask  PriorMask do begin processxo w\(PriorMask,height  PriorMask  RowMask end-if height  height+l I extend height of possible clusters I end-for processJow\(PriorMask,height end-while Figure 6 The BitOp algorithm  8 B 1 e m attribute X 4 Figure 7 A typical grid \(a\prior to smoothing b after smoothing 226 


computed cluster Function 2 false-positives 1 age  40 A 50K L salary 5 1OOK j GroupA 2 40 5 age  60 A 7511\221 5 salary 5 125K GroupA 3 age 2 60 A 25K 5 salary 5 751q 3 Group A LI B Y else  Group other 0 a E false-negatives   Y a 0 Figure 8 The function used to generate syn thetic data gorithm cannot locate a sufficiently large cluster it termin ates The idea of pruning to reduce error and to reduce the size of the result has been used in the AI community espe cially for decision trees  171 3.6 Cluster Accuracy Analysis To determine the quality of a segmentation by a set of clustered association rules we measure two quantities i the number of rules computed and ii\the summed error rate of the rules based on a sample of the data. The two quantities are combined using the minimum description length MDL principle  181 to arrive at a quantitative measure for determ ining the quality compared to an 223optimal\224 segmentation We first describe our experiments then we describe our error measure for a given rule then we describe our application of the MDL principle In 2 a set of six quantitativeattributes salary, commis sion age, hvulue hyears loan and three categorical attrib utes educationdevel cur zip code for a test database are defined. Using these attributes 10 functions of various com plexity were listed We used Function 2 shown in Figure 8 to generate the synthetic data used in our experiments. The optimal segmentation for this data would be three clustered association rules each of which represents one of the three disjuncts of the function. The clustering process is made dif ficult when we introduce noise, random perturbations of at tribute values and error due to binningof the input attributes age and salary An intuitive measure of the accuracy of the resulting clustered rules would be to see how well the rectangular clusters overlap the three precise disjuncts of the function in Figure 8 We define the notion of false-positives and fulse-negatives graphically as shown in Figure 9 and seek to minimize both sources of error In Figure 9 the light grey rectangle represents an actual cluster according to the function and the dark-grey rectangle represents a computed cluster. Note that in general an optimal cluster need not be rectangular The false-positive results are when the com puted cluster incorrectly identifies tuples outside of the op timal cluster as belonging to the specified group whereas the false-negatives are tuples that should belong to the group but are not identified as such by the computed cluster. The total summed error for a particular cluster is the total false Figure 9 Error between overlapping rc-g\222  ions positives  false-negatives However unless the optimal clustering is known beforehand such as here where a func tion is used to generate the data, this exact measure of error is not possible. Because we are interested in real-world data where the optimal clustering is not known we instead se lect a random sample of tuples from the database and use these samples to determine the relative error of the com puted clusters The relative error is only an approximation to the exact error since we are counting the number of false negatives and false-positives based only on a sample of the original database In order to get a good approximation to the actual error we use repeated k out of n sampling a stronger statistical technique The strategy we use to measure the quality of a segmenta tion given a set of clustered association rules is b<asecf on the MDL principle We are using a simplified model of MDL that has worked in practice The MDL principle states that the best model for encoding data is the one that minimizes the sum of the cost of describing the model and the cost of describing the data using that model The goal is to find a model that results in the lowest overall cost with cost typ ically measured in bits In the context of clustering, the models are the descrip tions of the clusters and the data is the sampled data de scribed above The greater the number of clusters used for segmentation, the higher the cost necessary to describe those clusters The cost of encoding the sampled data using a given set of clusters \(the model\is defined to be the sum of all errors for the clusters The intuition is that if a ruple is not an error, then it is identified by a particular cluster and hence its cost is included with the description of the cluster Otherwise if the tuple is an error, then we must specifically identify it as such and this incurs a cost We use the follow ing equation to determine the cost of a given set of clustered association rules cost  w log,\(ICI  we log,\(errors where IC is the number of clusters and errors is the sum of false-positives  false-negatives\for the clusters C The logarithmic factor is used because having more clusters re quires a logarithmically increasing number of bits to enu 227 


merate and the logarithmic factor provides a favorable non linear separation between close and near-optimal solutions Based on empirical evidence we made the simplifying as sumption the clusters themselves have a uniform encoding cost. The constants wc and we allow the user to impart a bias towards 223optimal\224 cluster selection, providing greater flex ibility in finding a representation of the segmentation that is the most usable If w is large, segmentations that have many clusters will be penalized more heavily since they will have a higher associated cost and segmentations of the data that have fewer clusters will have a greater probability of be ing 223optimal\224. Likewise if we is large, the system will favor segmentations where the error rate is lowest If both con stants are equal wc  we  1 as in thedefault case, neither term will bias the cost Our heuristic optimizer recall Figure 2 by means de scribed in the following section, seeks to minimize the MDL cost 3.7 Parameter Heuristics In this section we describe the algorithm our overall sys tem uses to adjust the minimum support and confidence thresholds based upon the accuracy analysis from the previ ous section. \(currently the number of bins for each attribute is preset at 50 We discuss this issue more in the following section We desire values for support and confidence that will result in a segmentation of the data that optimizes our MDL cost function. The search process involves successive iterations through the feedback loop shown in Figure 2 We identify the actual support and confidence values that appear in the binned data and use only these values when adjusting the ARCS parameters We begin by enumerating all unique support thresholds from the binned data with one pass and then all of the unique confidence thresholds for each of these support thresholds with a second pass A data structure sim ilar to that shown in Figure 10 is used to maintain these val ues. Note that as support increases, there become fewer and fewer cells that can support an association rule and we have found a similar decrease in the variability of the confidence values of such cells Given a choice to either begin with a low support threshold and search upwards or begin with a high sup port threshold and search downwards we chose the former since we found most 223optimal\224 segmentations were derived from grids with lower support thresholds If we were using a previous association rule mining algorithm, for efficiency it might be preferable to start at a high support threshold and work downwards, but our efficient mining algorithm al lows us to discover segmentations by starting at a low sup port threshold and working upwards. Our search starts with a low minimum support threshold so that we consider a lar ger number of association rules initially, allowing the dy namic pruning performed by the clustering algorithm to re Confidence List  I I I I I I 5 rl 12 I 25 I 40 I 56%I 90 Sumort I List\222 Wra*luoal Figure 10 Ordered support thresholds    lists of confidence and move unnecessary rules The support is gradually increased to remove background noise and outliers until there is no im provement of the clustered association rules \(within some E 4 Experimental Results The ARCS system and the BitOp algorithm have been implemented in C comprising approximately 6,300 lines of code To assess the performance and results of the al gorithms in the system we performed several experiments on an Intel Pentium workstation with a CPU clock rate of 120MHz and 32MB of main memory running Linux 1.3.48 We first describe the synthetic rules used in generating data and present our initial results We then briefly compare our results with those using a well-known classifier C4.5 to perform the segmentation task Finally we show perform ance results as the sizes of the databases scale 4.1 Generation of Synthetic Data We generated synthetic tuples using the rules of Func tion 2 from Figure 8 Several parameters affect the distri bution of the synthetic data These include the fraction of the overall number of tuples that are assigned to each value of the criterion attribute aperturbation factor to model fuzzy boundaries between disjuncts and an outlier percentage that defines how many tuples will be assigned to a given group label but do not match any of the defining rules for that group. These parameters are shown in Table 1 4.2 Accuracy and Performance Results We generated one set of data for Function 2 with ID1  50,000 and 5 perturbation and a second set of data for the same function but with 10 outliers, i.e 10 of the data are outliers that do not obey the generating rules In every ex perimental run we performed ARCS always produced three clustered association rules each very similar to the gener ating rules and effectively removed all noise and outliers 228 


Attribute 1 Value salary age 1 uniformly distributed from 20,000 to 150k uniformly distributed from 20 to 80 PI fracA fracother P U Number of tuples 20,000 to 10 million Fraction of tuples for \223Group A\224 40 Fraction of tuples for \223Group other\224 60 Perturbation factor 5 Outlier percentage 0 and 10 Table 1 Synthetic data parameters from the database. The following clustered association rules were generated for Group A clusters from the set of data containing outliers and with a minimum support threshold of 0.01 and a minimum confidence threshold of 39.0 20 5 Age 5 39 A 48601 5 Salary 5 100600 j Grp A 40 5 Age 5 59 A 74601 5 Salary 5 124000  Grp A 60 5 Age 5 80 A 25201 5 Salary 5 74600  Grp A The reader can compare the similarity of these rules with those used to generate the synthetic data in Figure 8 We measured the error rate of ARCS on these databases and compared it to rules from C4.5 C4.5 is well known for building highly accurate decision trees that are used for classifying new data and from these trees a routine called C4.5RULES constructs generalized rules  171 These rules have a form similar to our clustered association rules and we use them for comparison both in accuracy and in speed of generation. Figures 11 and 12 graph the error of both sys tems as the number of tuples scale using the two sets of gen erated data The missing bars for C4.5 on larger database sizes are due to the depletion of virtual memory for those ex periments, resulting in our inability to obtain results \(clearly C4.5 is not suited to large-scale data sets From Figure 11 we see that C4.5 rules generally have a slightly lower error rate than ARCS clustered association rules when there are no outliers in the data However, with 10 of the data appearing as outliers the error rate of C4.5 is slightly higher than ARCS as shown in Figure 12 C4.5 also comes at a cost of producing significantly more rules as shown in Figures 13 and 14 As mentioned earlier we are targeting environments where the rules will be processed by end users so keeping the number of rules small is very im portant The primary cause of error in the ARCS rules is due to the granularity of binning The coarser the granularity, the less likely it will be that the computed rules will have the same boundary as the generating rules To test this hypo thesis we performed a separate set of identical experiments using between 10 to 50 bins for each attribute We found a general trend towards more 223optimal\224 clusters as the number of bins increases 100 10002000 4000 8000 10000 Number of Tuples in 222000s Figure 15 Scalability of ARCS Table 2 Comparative execution timers sec 4.3 Scaleup Experiments To test scalability we ran ARCS on several databases with increasing numbers of tuples. Figure 15 shows that ex ecution time increases at most linearly with the size of the database Because ARCS maintains only the ElinArray and the bitmap grid ARCS requires only a constant amount of main memory regardless of the size of the database \(assum ing the same number of bins\This actually gives our system significantly better than linear performance as can be seen by close inspection of Figure 15 since some overhead ex ists initially but the data is streamed in faster from the U0 device with larger requests For example when the num ber of tuples scales from 100,000 to 10 million a factor of loo the execution time increases from 42 seconds to 420 seconds a factor of lo In comparison C4.5 requires the entire database, times some factor, to fit entirely in main memory This results in paging and eventual depletion of virtual memory which prevented us from obtaining execu tion times or accuracy results for C4.5 rules from databases greater than 100,000 tuples\Both C4.5 alone and C4.5 to gether with C4.5RULES take exponentially higher execu tion times than ARCS as shown in Table 2 229 


10 r  ______ 20 18 7 20 50 100 200 500 1000 Number of Tuples in 221000s Figure 11 Error rate with U  0 20 50 100 200 500 1000 Number of Tuples in 221000s Figure 12 Error rate with U  10 35 4 30 II 6 25  v   L 3 20  5 15  z 10  5 OT 12 16 31 3 20 50 100 200 500 1000 Number of Tuples in 221000s Figure 13 Number of rules produced U  0 5 Conclusions and Future Work In this paper we have investigated clustering two attribute association rules to identify generalized segments in large databases The contributions of this paper are sum marized here 0 We have presented an automated system to compute a clustering of the two-attribute space in large databases 0 We have demonstrated how association rule mining technology can be applied to the clustering problem We also have proposed a specialized mining algorithm that only makes one pass through the data for a given partitioning of the input attributes and allows the sup port or confidence thresholds to change without requir ing a new pass through the data e A new geometric algorithm for locating clusters in a two-dimensional grid was introduced Our approach has been shown to run in linear time with the size of 16  14  z 12 O 10 kl 5 z6 4 2 0 Lc 20 50 100 200 500 1000 Number of Tuples in 221000s Figure 14 Number of rules produced U  10 the clusters Further, parallel implementations of the algorithm would be straightforward 0 We apply the Minimum Description Length MDL principle as a means of evaluating clusters and use this metric in describing an 223optimal\224 clustering of associ ation rules 0 Experimental results show the usefulness of the clustered association rules and demonstrates how the proposed system scales in better than linear time with the amount of data The algorithm and system presented in the paper have been implemented on several platforms including Intel DEC and SGI So far we have performed tests only us ing synthetic data but intend to examine real-world demo graphic data We also plan on extending this work in the fol lowing areas 0 It may be desirable to find clusters with more than two attributes One way in which we can extend our pro posed system is by iteratively combining overlapping 230 


sets of two-attribute clustered association rules to pro duce clusters that have an arbitrary number of attrib utes Handle both categorical and quantitative attributes on the LHS of rules To obtain the best clustering we will need to consider all feasible orderings of categorical at tributes Our clustering algorithm has been extended to handle the case where one attribute is categorical and the other quantitative and we achieved good results By using the ordering of the quantitative attribute we con sider only those subsets of the categorical attribute that yield the densest clusters Preliminary experiments show that segmentation can be improved if the association rule support values rather than binary values are considered in the smooth ing filter and more advanced filters could be used for purposes of detecting edges and corners of clusters It may be beneficial to apply measures of information gain 16 such as entropy when determining which two attributes to select for segmentation or for the op timal threshold values for support and confidence The technique of factorial design by Fisher 6 41 can greatly reduce the number of experiments necessary when searching for 223optimal\224 solutions This tech nique can be applied in the heuristic optimizer to re duce the number of runs required to find good values for minimum support and minimum confidence Other search techniques such as simulated annealing can be also be used in the optimization step Acknowledgements We are grateful to Dan Liu for his work in extending functionality in the synthetic data generator and for the ex periments using C4.5 References I R Agrawal S Ghosh T Imielinski, B. Iyer and A. Swami An interval classifier for database mining applications In Proceedings of the 18th International Conference on Very Large Data Bases Vancouver Canada, 1992 2 R Agrawal T Imielinski, and A. Swami. Database mining A performance perspective In ZEEE Transactions on Know ledge and Data Engineering volume 5\(6 pages 914-925 Dec 1993 3 R Agrawal T Imielinski and A Swami Mining associ ation rules between sets of items in large databases In Pro ceedings of the I993 ACM SIGMOD International Confer ence on Management of Data Washington D.C 1993 4 G E Box W Hunter and J S Hunter Statistics for Ex perimenters An Introduction to Design, Data Analysis and Model Building John Wiley and Sons 1978 5 T Cormen, C. Lieserson and R Rivest Introduction to Al gorithms The MIT Press 1990 6 R A Fisher The Design of Experiments Hafner Publishing Company 1960 71 T Fukuda Y Morimoto S Morishita and lr Tokuyama Data mining using two-dimensional optimized association rules Scheme algorithms, and visualization In Proceed ings of the 1996ACM SIGMOD International Conference on Management of Data Montreal Canada June 1996 8 R C Gonzalez and R Woods Digital Image Processing Addison-Wesley 1992 9 M Klemettinen H Mannila P Ronkainen and H Toivonen Finding interesting rules from large sets of discovered association rules In 3rd International Confer ence on Information and Knowledge Management CIKM Nov 1994 IO J B Kruskal. Factor analysis and principle components Bi linear methods. In H Kruskal and J. Tanur editors Interna tional Encyclopedia of Statistics Free Press 1978  111 D N Lawley Factor Analysis as a Statistical Method American Elsevier Publishing second edition 1971 I21 D J Lubinsky Discovery from databases A review of ai and statistical techniques In IJCAI-89 Workshop on Know ledge Discovery in Databases pages 204218,1989 I31 M Mehta, R. Agrawal and J Rissanen Sliq A fast scal able classifier for data mining In Proceedings of the 5th In ternational Conference on Extending Databaste Technology EDBT Avignon France Mar 1996 I41 M Muralikrishna and D DeWitt Statistical pirofile estima tion in database systems ACM Computing Suweys 20\(3 Sept. 1988  151 G. Piatetsky-Shapiro Discovery, analysis and presentation of strong rules Knowledge Discovery in Databases 1991 16 J Quinlan. Induction of decision trees In MachineLearning volume 1 pages 81-106,1986  171 J Quinlan C4.5 Programs for Machine Learning Morgan Kaufmann San Mateo California 1993  J Rissanen Stochastic Complexity in Statistical Inquiry World Scientific Publishing Company, 1989 I91 H C Romesburg Cluster Analysis for Researchers Life time Learning Publications-Wadsworth Inc 1984 20 J Shafer R Agrawal and M Mehta Fast serial and paral lel classification of very large data bases In Proceedings of the 22nd International Conferenceon Very Luge Databases Bombay, India, 1996 21 H Spath Cluster AnalysisAlgorithms for data reductionand classijication ofobjects Ellis Horwood Publishers, 1980 22 R Srikant and R Agrawal Mining quantitative association rules in large relational tables In Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data Montreal Canada June 1996 23 K Whang S Kim and G Wiederhold Dynamic mainten ance of data distribution for selectivity estimation VLDB Journal 3\(1\Jan 1994 231 


