Multi-relational Algorithm for Mining Association Rules in Large Databases  Carlos Roberto ValÍncio Universidade Estadual Paulista Ö Unesp Depto. de CiÍncias de ComputaÁ„o e EstatÌstica S„o JosÈ do Rio Preto, Brazil valencio@ibilce.unesp.br Fernando Tochio Ichiba Universidade Estadual Paulista Ö Unesp Depto. de CiÍncias de ComputaÁ„o e EstatÌstica S„o JosÈ do Rio Preto, Brazil fernandoichiba@sjrp.unesp.br Fernando Takeshi Oyama Universidade Estadual Paulista Ö Unesp Depto. de CiÍncias de ComputaÁ„o e EstatÌstica S„o JosÈ do Rio Preto, Brazil ftobr@yahoo.com.br RogÈria Cristiane Grat„o de Souza Universidade Estadual Paulista Ö Unesp Depto. de CiÍncias de ComputaÁ„o e EstatÌstica 
S„o JosÈ do Rio Preto, Brazil rogeria@ibilce.unesp.br  Abstract Multi-relational data mining enables pattern mining from multiple tables. The existing multi-relational mining association rules algorithms are not able to process large volumes of data, because the amount of memory required exceeds the amount available. The proposed algorithm MRRadix presents a framework that promotes the optimization of memory usage. It also uses the concept of partitioning to handle large volumes of data. The original contribution of this proposal is enable a superior performance when compared to other related algorithms and moreover successfully concludes 
the task of mining association rules in large databases, bypass the problem of available memory. One of the tests showed that the MR-Radix presents fourteen times less memory usage than the GFP-growth Multi-relational data mining, association rules, frequent itemsets mining, relational database I   I NTRODUCTION  Traditional data mining algorithms do the processing by taking into account that the data is ordered in a single structure, generally a file or a table. This limitation makes it difficult to use those algorithms, for example, in a relational database made up of various semantically related tables [1   
Multi-relational data mining algorithms come as a viable proposal to the limitations of traditional algorithms, making it possible to extract patterns from multiple  registers in a direct and efficient manner, without the need of transferring data to a single  tabl  Ne ve rt he l e ss  a v a i l a bl e memory space of the equipment being used may not be sufficient to process the mining of large volumes of data Due to this, the performance and use of memory space of such algorithms become an inherent worry for the prospection of large repositories This work presents as an original contribution, a multirelational algorithm for the extraction of association rules 
focused on its use with large relational databases. This proposal takes into account limited available memory space since the objective of this work is the mining of large data volumes. To deal with such a restriction, the concept of partitioning the database to subdivide it into units of a size which can be allocated in memory is used to enable the processing. Moreover, the multi-relational mining algorithm had a satisfactory performance which enabled a timely analysis of the databases. The basis of this work was the PatriciaMine traditional algorithm [4   This work is organised in the following manner: section II presents the state of the art; section III details the 
development of the work, describing concepts, data structures and the rest of the information about the proposed algorithm; section IV presents the comparative tests which were done, as well as a discussion about obtained results lastly, section V presents the conclusions II  S TATE O F T HE A RT  In this section some of the principal traditional and multirelational algorithms available in literature are presented Among the association rule mining traditional algorithms A  Traditional algorithms 
The PG \(Pattern-Growth\ approach and its principal algorithm, the FP-growth, have been used in numerous studies turned to improving the original algorithm or to propose new efficient methods for the extraction of knowledge. These studies are, most of the times, focused on the development of new data structures that are more efficient than the original FP-tree, or even, turned to presenting solutions which are effective in both dense and sparse databases  The H-Mine [6 gor i t h m ha s a st r u ct ur e c a l l e d H St r u c t  which favours sparse data mining, requiring only two passes through the database for the construction of its structure 
Although the H-Mine has improved performance, mainly with sparse databases, the FP-growth is even more efficient with dense databases The Opportune Project is a hybrid algorithm which extends the functions of the FP-growth and the H-Mine processing patterns by means of a FP-tree or H-Struct according to the density of the set of data  On the other hand, the PatriciaMine algorithm has a new structure called Patricia-trie, or Radix-tree, which reduces 
2011 12th International Conference on Parallel and Distributed Computing, Applications and Technologies 978-0-7695-4564-6/11 $26.00 © 2011 IEEE DOI 10.1109/PDCAT.2011.56 269 
2011 12th International Conference on Parallel and Distributed Computing, Applications and Technologies 978-0-7695-4564-6/11 $26.00 © 2011 IEEE DOI 10.1109/PDCAT.2011.56 269 


the space needed in memory to store a tree by compressing the nodes. With this structure, the algorithm has a good performance with sparse and dense databases, being more efficient than the Opportune Project algorithm [4   B  Multi-relational algorithms Multi-relational algorithms for mining association rules use different approaches to represent and extract patterns. A first approach comprises algorithms based on logic, also known as inductive logic programming \(ILP\ algorithms The principal characteristic of this approach is that the data and patterns are represented as datalogs, which are written in first order logic [3 The most widespread ILP algorithm for mining association rules is the WARMR [8 w h ich f u n c tio n s  b a s e d  on the Apriori [9 gor i t h m   Ano t he r  I L P  a l gor i t hm fo und i n  literature is the FARMER [10 w h ich is sim ilar t o  th e  WARMR, but is more efficient in the use of ordinate trees for the organisation of frequent items There are multi-relational algorithms based on the function and structure of the data using traditional techniques, principally the Apriori and the FP-growth. The intention for that approach is to extend the traditional mining algorithms, which work adequately with the extraction of patterns from a single table, adapting them to a multirelational context so as to directly enable pattern processing on relational databases. Among the algorithms that extend the Apriori are the Apriori-Group n d  t h e Apr i o r iM R    III  T HE D EVELOPED W ORK  M R R ADIX A LGORITHM  The multi-relational algorithm proposed in this work is called MR-Radix which extends the PatriciaMine algorithm to enable the mining of association rules in large relational databases. The proposal uses the Radix-tree data structure also referenced to as Patricia-trie, to represent the database Such a structure is efficient in its mining algorithm performance and in its use of memory space since it uses 75% \(seventy-five percent\ less space than the FP-tree used in many PG approach algorithms [4   The Radix-tree structure is an alternative for mining dense or sparse relational databases since the problem of repository sparseness tends to increase the size of the structure, which is partially overcome by compressing the nodes The FP-growth and other derived algorithms use a bottom-up strategy, starting from the node sheets up to the top of the structure [13   T h e alg o r ith m p r o p o s ed in th is  work does the mining using a top-down strategy, that is frequent patterns are extracted by searching the Radix-tree from its roots to its node leaves. With this strategy a considerable performance gain is obtained, since there is only one Radix-tree and the whole process is done based on this global structure, eliminating the extra processing load of constructing intermediate and temporary structures. Besides this, the Radix-tree algorithm has an iterative strategy to explore the tree, where a performance gain is also obtained since the computational cost of successive recursive calls is eliminated A  Proposal for a relational itemset model To mine data from multiple relational tables, itemset representation becomes a bit more complex because a larger number of information has to be stored due to the multiplicity of data sources. That has motivated this proposal for a representation model called relational itemset which extends the traditional itemset concept to include information which would enable patterns from relational databases to be identified Fig. 1 shows the schema of a relational itemset containing k items with indexes which vary from 0 to k-1 Each relational item is made up of three identifying fields Table, which registers the name of that itemês source table Attributeês Name, which stores the identification of the column associated to that item; Attributeês Value, which represents the contents of the Name of the Attribute column to a given register from the source table. To facilitate the visualisation of a relational item, when convenient, the following notation will be used: Table.AttributeName AttributeValue B  Radix-tree Structure The structure of a Radix-tree or Patricia-trie \(Practical Algorithm To Retrieve Information Coded In Alphanumeric differs to the standard trie because it is capable of compressing nodes. The compression is done by clustering the nodes that share the same branch and, consequently, have the same value for the support counter C  ItemMap The structure of the ItemMap \(IM\ proposed in this work is to optimize the representation of relational items in the Radix-tree, since otherwise an item relational would need substantial memory space for its storage in the node structure An example of the above is the relational item Inpatient.Hemorrhage = Yesé. Considering that each character occupies one byte in memory, the space needed to store that item would be 22 \(twenty-two\ bytes, not including the ç.é and ç=é. As the quantity of nodes in a structure is generally high, just the space needed to store items would occupy a substantial amount of available memory. The IM is used to map a relational item in an index; as said index is stored as a whole which occupies only four bytes, a sizeable memory gain is achieved  Figure 1  Relational itemset model 
270 
270 


With that strategy, the structure does the mapping between the relational item representation and the internal representation used by the Radix-tree. The IM has a field which stores the relational item representation and another field which registers the index corresponding to that item in the ItemList \(IL\. Fig. 2 shows this relationship between the IM and the IL D  Description of the proposal The execution of this workês proposal can be divided into two stages: the construction of the Radix-tree and the mining of frequent patterns. These stages are described below 1  Constructing the Radix-tree The construction of the Radix-tree stage needs two complete sweeps through the database to obtain its frequent items to formulate their representation in the referred structure The Radix-tree construction algorithm is an iterative version for the route and extraction of frequent itemsets. The idea of that algorithm is the successive generation of patterns which share the same prefix, that is, the algorithm selects each item from the IL in a decreasing order, according to the support value, and extracts all the frequent itemsets that start with the selected item. A collection of nodes that share a given item I, which are connected by means of the linked list which comes from the IL and is called a t-list\(I\, makes up a subtree having its ascendants 2  Mining for frequent patterns A mining process first generally consists of the construction of a subtree having node leaves containing item I. This procedure can be done efficiently since the IL has a tlist\(I\ which makes up a list linking all the nodes that contain I The MR-Radix algorithm uses the top-down strategy therefore, the generation of that subtree can be done with pointer manipulation operations. Still in this stage, it is interesting to point out that the IL structure is also updated according to the adjustment of the subtree, since the ancestral nodes were previously processed, making the altering of its respective IL inclusions possible. With that, itemsets relative to the subtree are generated. The described procedures are successively repeated so that new subtrees can be generated  Figure 2  ItemMap and ItemList E  Partitioning strategy for the MR-Radix To be able to mine large databases, a strategy of partitioning the proposed algorithm was implemented. Said strategy consists of the subdivision of databases in the memory into allocatable and analysable portions, which would make possible the processing of large volumes in stages The strategy consists of the algorithm trying to read the database registers and so construct the Radix-tree, until it is identified that there is no more possible available memory to allocate the structure. In this way, a portion which is read is then considered as a partition and has its mining done individually. This procedure is repeated until the entire database has been processed, creating as many partitions as is necessary The functioning of the MR-Radix algorithm partitioning strategy is succinctly represented in Fig. 3 Phase 1 in Fig. 3 consists of the generation of frequent local itemsets, obtained from the mining of individual partitions. Still in this phase, those local itemsets are processed to form a set of global candidates. To optimise the analysis of the patterns in this set, which is done in Phase 2 a differentiated representation of said patterns is proposed Besides the actual itemset, the pattern also presents two fields: accessed partitions and accumulated support The accessed partition field stores a register of partitions in which the pattern is considered as being locally frequent The accumulated support field represents the sum of support values, corresponding to the referred pattern in the partitions in which it is frequent, that is, those indicated in the accessed partitions field The use of such fields optimises the second partitioning phase, since a pattern count is not necessary in partitions where the patterns were already frequent. The support of each pattern must be verified only in those partitions which are not in the list of accessed partitions. On concluding the count of the support in the remaining partitions, the pattern has the value of the global support in its accumulated support field. That way, frequent global itemsets are obtained delimiting only the global candidate itemsets having an accumulated support value superior to the pre-established minimum  Figure 3  Partitioning strategy 
271 
271 


IV  P ERFORMANCE S TUDY  The study comprised an analysis of the MR-Radix algorithm, comparing it to correlated algorithms, so as to verify its contribution in practical terms A  Databases and test parameters The tests consisted of applying the algorithms to relational databases, to obtain measurements of execution time and the amount of memory used in processing frequent patterns The databases that were used are characterised as being relational bases containing real data that were supplied by professionals in their respective contexts, by means of computational systems registration or similar The first database, called BT, consists of a set of registers that store information about a Tumour Bank from a hospital in the interior of the State of S„o Paulo, Brazil. Among the stored information, those related to patients and their respective extracted cancerous samples can be highlighted as well as the more detailed clinical information supplied on forms by the health professionals. Said database is the responsibility of the Database Group of the Instituto de BiociÍncias, Letras e CiÍncias Exatas da Universidade Estadual Paulista \(IBILCE/ UNESP Another database, called CENSUS, has a sample of the population census done in the United States in the year 1990 This repository is public and is available at the UCI Machine Learning Repository1. To simplify any mining work, the database had already been pre-processed, having all its data discretizised and free from imprecise and null values Table I shows information referring to the databases BT and CENSUS, including the volume of data processed in the tests B  Comparative study of the multi-relational algorithms In this section the results relative to the comparative analysis of the multi-relational algorithms MR-Radix AprioriMR and GP-growth are presented. In a previous work, the AprioriMR algorithm was implemented in a fDMMR tool [1 Th is alg o rith m  w a s  ch os en  t o en a b le a comparison of the MR-Radix to an algorithm representative of the CGT approach Based on the original GFP-growth algorithm [5 d  to  enable the preparation of the tests, this work also constructed and developed this algorithm by adding a version of it to a fDMMR tool. For the construction and mining stages of the FP-tree, part of the code-fonts from the FP-growth algorithm, implemented by Frans Coenen, were reutilised The first analysis to be discussed refers to the application of Multi-relational algorithms on a BT base, to collect respective execution times. Fig. 4 shows the values of execution times in a logarithm scale of the tests done on the BT database The graphic evidences the greater efficiency shown by the MR-Radix algorithm in comparison to the AprioriMR    1  Available at http://archive.ics.uci.edu/ml/. Accessed on 7th May 2011  The proposed algorithm is two orders of magnitude faster than the algorithm based on Apriori. The inferior performance of the AprioriMR can be explained by the fact that it is based on a CGT approach, which causes, during processing steps, a high number of patterns to be analysed. It should also be pointed out that the algorithm makes various accesses to the disc, depending on the quantity of stages needed to produce frequent patterns. On the other hand, the MR-Radix does only two of those accesses, independent of the number of patterns to be generated Execution times for that database could not be collected for the GFP-growth algorithm, since, for all the support values utilized, there was a problem of insufficient principal memory. An addendum is made that, to verify the quantity of necessary memory, a specific test was done, extending the machineês available memory From the results it was noted that, for a support value of 30% \(thirty percent\, the GFP-growth algorithm needed more than an extra 300 MB \(three-hundred megabytes\ of memory. This characteristic is due to the fact that said algorithm, as it is based on FP-growth, has a low performance when used in sparse databases, such as the BT repository. With that, it was observed that the FP-tree generated in the tests had many nodes and branches, besides requiring a considerable memory size for its representation Moreover, the MR-Radix algorithm shows little execution time variance in relation to different support values, which can be seen as a beneficial characteristic, since it has a good capacity to deal with a growing number of frequent items. The same behaviour, however, is not seen with the AprioriMR algorithm, since, as support values reduce, it takes more processing time. This characteristic is not exclusive to the AprioriMR as it occurs with other algorithms derived from the Apriori [5   TABLE I  D ATABASES U SED IN THE T ESTS  Databases Table Number of columns Number of attributes BT Sample 13,100 31 Urology_Kidney 60 60 Patient 3,647 18 CENSUS Data 2,458,359 68   Figure 4  Execution time Ö BT repository  
272 
272 


As to memory use, with performances diagrammed in Fig. 5, the MR-Radix algorithm with its use of a Radix-tree data structure manages to substantially compress the database. On the other hand, the AprioriMR algorithm, even when applied to medium sized databases Ö such as the BT produces a high number of candidate itemsets which require a large memory space to store and manipulate the patterns being processed In the extreme case of this analysis, with a support of 0.5% \(half percent\, it was verified that the algorithm proposed in this work occupied close to 1,600 KB \(one thousand, six-hundred kilobytes\ while the AprioriMR occupied more than 63,000 KB \(sixty-three thousand kilobytes With the GFP-growth algorithm, once again it was not possible to collect the values of utilised memory as the algorithm did not conclude processing. Since the amount of memory available was limited at the beginning of the test, it was not sufficient to effectively mine all the analysed support values C  Comparative study: mining large volumes of data In this subsection, the performance of multi-relational algorithms is assessed in situations that involve the mining of a large database, in which the processing frequently needs a space in the principal memory that is superior to that available in the equipment, thus needing the adoption of strategies, such as partitioning, which would enable the mining to be concluded In the following tests, it was considered that the principal memory was limited to 42 MB \(forty-two megabytes\The intention of the measurement was to verify up to which support values the multi-relational algorithms have the capacity to do the mining within that memory limit The multi-relational algorithms used in this stage are the same ones that were used in the previous tests: AprioriMR MR-Radix and GP-growth. The differential of this test is that it was opted to analyse the MR-Radix algorithm in its standard version in comparison to its version which has the strategy of partitioning the database, called MR-RadixPartition, to differentiate the reading and analysis of the tests and graphics It is worth mentioning that, to do the test, minimum support values were used, varying from 10% \(ten percent\ to up to 90% \(ninety percent\, to show the behaviour of the analysed algorithms along the whole range of support values The results obtained from this study can be seen in Fig. 6 in which is shown the execution times of the algorithms mentioned in relation to the CENSUS base As can be observed, the MR-Radix-Partition algorithm was the only one to conclude the processing of frequent patterns for all the support values that were applied. The rest of the algorithms suffered, at some moments, from the problem of insufficient memory to conclude the mining. The MR-Radix algorithm, because it did not implement the partitioning strategy, did not have its time measured for support values below 30% \(thirty percent\. GFP-growth performance measurements could not be collected for values below 80% \(eighty percent\. On the other hand, the AprioriMR concluded the processing in only one of the tests with a support of 90% \(ninety percent\. Besides that, this last one was the algorithm which took the most time to execute the mining task The performance of the MR-Radix-Partition and MRRadix was practically equal up to the support value of 40 forty percent\. This is due to the fact that the MR-RadixPartition, at the higher support values, behaves in a similar way to the MR-Radix, as the partitioning strategy is not activated in those situations. Such behaviour occurs because the version with the partitioning strategy is capable of identifying that there is sufficient memory to process the entire database so, therefore, does not do the partitioning The GFP-growth algorithm only had its performance measured with the two highest support values, 80 and 90 eighty and ninety percent\, since there was a shortage of principal memory. With these cases, their execution time was slightly less than with the MR-Radix and MR-RadixPartition algorithms The division of the CENSUS database into partitions was necessary for support values lower than 30% \(thirty percent due to insufficient memory space. In these cases, as shown in Fig. 7, only the MR-Radix-Partition algorithm succeeded in executing the conclusion. For example, for a minimum support value of 10% \(ten percent\, the algorithm subdivided the base into two partitions of a sufficient size so that their respective representation could be allocated in the principal memory. For each of the partitions, the algorithm then did the mining for frequent local itemsets and, at the end, joined such itemsets to obtain a set of frequent global patterns The second step of the scalability analysis consisted of a study of the use of memory by above mentioned algorithms when applied to the CENSUS base. Such results can be seen in Fig. 7 where collected metrics are graphically shown The AprioriMR algorithm once again had a poorer performance than the rest of the multi-relational algorithms Measuring this algorithm was only possible with the highest support value, 90% \(ninety percent\ since, for all other conditions, the processing could not be concluded due to insufficient principal memory The GFP-growth algorithm also used a larger amount of memory, even with the highest support values, than the other algorithms MR-Radix and MR-Radix-Partition. While the MR-Radix, for example, needed 2,250 KB \(two thousand two hundred and fifty kilobytes\ a support value of 80 eighty percent\, the GFP-growth needed 31,000 KB \(thirty one thousand kilobytes\. That is, the MR-Radix used fourteen times less memory than the GFP-growth The behaviour, in terms of memory use of the MR-Radix and MR-Radix-Partition algorithms, was similar at all levels of analysed supports. As already mentioned, this happens because the MR-Radix-Partition has the capacity of identifying available memory space, partitioning the database only when necessary It was seen that, for lower support values where only the MR-Radix- Partition algorithm concluded the processing, the amount of memory used remained relatively constant. This is because the algorithm creates partitions which occupy all of the available memory space. Thereby, each partition is 
273 
273 


optimised to contain the largest possible number of registers which, consequently, results in a lower number of partitions and also a lower number of accesses to the disc Compared to the GFP-growth, the MR-Radix-Partition and MR-Radix algorithms stand out, principally with the optimised use of the memory. With the execution time comparative tests, it was noted that the GFP-growth had a slightly superior performance, though memory usage was high when compared to the algorithm proposed in this work The proposed algorithm produced results that allied low execution time to a better use of the memory V  C ONCLUSION  The MR-Radix algorithm had a superior performance with execution time and used memory when compared to other multi-relational proposals. Inclusion of the database partitioning strategy in that algorithm made it possible to prospect data from large repositories, such as CENSUS, in which the principal memory space needed to process the data was higher than the space available in the equipment. Results show that, for low support values, only this approach was able to extract knowledge from that repository. That being so, this work presents a significant and original contribution as the proposed algorithm was able to overcome the available memory space problem and do the mining of multirelational data from large data repositories A CKNOWLEDGMENT  Project financed by the CAPES  Figure 5  Utilised memory Ö BT repository  Figure 6  Execution time Ö CENSUS repository  Figure 7  Utilised memory Ö CENSUS repository R EFERENCES  1  A. J. Knobbe. çMulti-Relational Data Miningé. Thesis Ph.D.\.The Netherlands, pp. 130, 2004 2  S. Dzeroski, L. D. Raedt and S. Wrobel. çMulti-Relational Data Miningé: Workshop Report. ACM SIGKDD 2003 Explorations Newsletter, vol. 5, n. 2, December 2003, pp.200202 3  P. Domingos. çProspects and challenges for multi-relational data miningé. ACM SIGKDD Explorations Newsletter, vol. 5 n. 1, July. 2003 4  A. Pietracaprina and D. Zandolin. çMining frequent itemsets using patricia triesé. Proc. IEEE ICDM Workshop Frequent Itemset Mining Implementations, vol. 80. Citeseer, 2003 5  L. C. Pizzi. çData mining in multiple tables: GFP-Growth algorithmé. Thesis \(Masters\deral University of S„o Carlos, 2006, pp. 106 [in Portuguese   6  J. Pei, J. Han, H. Lu, S. Nishio, S. Tang and D. Yang. çHmine: hyper-structure mining of frequent patterns in large databasesé. Proc. IEEE International Conference on Data Mining. IEEE Computer Society, 2001, pp.441-448 7  J. Liu, Y. Pan, K. Wang and J. Han. çMining frequent item sets by opportunistic projection.é. Proc. Eighth ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '02. New York, USA: ACM Press 2002, pp. 229 8  L. Dehaspe and L. De Raedt. çMining association rules in multiple relationsé. Proc. 7th Intl. Workshop on Inductive Logic Programming, Prague, Czech Republic, 1997, pp.125132 9  R. Agrawal and R. Srikant. çFast Algorithms for Mining Association Rules in Large Databasesé. Proc. 20th International Conference on Very Large Data Bases September 1994, pp.487-499   S. Nijssen and J. Kok. çFaster association rules for multiple relationsé. International Joint Conference on Artificial Intelligence, vol. 17. Citeseer, 2001, pp. 891Ö896   M. X. Ribeiro, M. T. P. Vieira and A. J. M. Traina. çMining Association Rules Using Clusteringé. I workshop on algorithms for data mining. Uberl‚ndia, Brazil, 2005 , pp. 916 [in Portuguese     F. T. Oyama. çExtraction of knowledge in databases using multi-relational clustering tuplesé. Monograph Undergraduate\S„o Paulo State University, 2006, pp. 51 [in Portuguese     K. Wang, L. Tang, J. Han and J. Liu. çTop Down FP-Growth for Association Rule Miningé. Lecture notes in computer science. Springer, 2002, pp.334Ö340   
274 
274 


Jr.u T  u .' ,- JIT. v j . .Al.Low Al H igh connection - ? :i:ld:e connection Low connection T:TineDe.lay N ,Tetalnum beref attnbutes o ,JudgementNode ,Proressng Node Current Consequent Fig. 3. Basic structure of class association rule mining using Generalized GNP A route with the fixed number of n attributes\(user-defined starting from the processing node is used as the possible an tecedent parts, and it is combined with the current consequent class to generate the candidate rules All the items of the consequent part are placed in Conse quent Table\(CT  individuals are evolved to extract the class association rules for each class of the attributes, respectively, i.e., for each traffic volume level of each section of the road networks, and this procedure is repeated for all the consequent items When extracting the rules, the basic procedure is like the following: firstly, the processing node randomly moves to the connecting another judgement node from the current judge ment node one after another. Then, a candidate rule of length neuser-defined candidate rule will be checked according to the database The basic checking process is shown in Fig.4, it starts from database tuple 0000, check whether attribute Ai is Middle at 0000 and whether attribute A2 is Low at 0002, . . .  , likewise if the condition is satisfied, increase the corresponding count otherwise, start the same checking process from the next time point 0001 checking whether attribute Ai is Middle at 0001 and whether attribute A2 is Low at 0003, . . .  , likewise, and get the final counts after studying all the records of the database which is the one turn of checking for the candidate rules In Fig.4, N is the total number of the searches, and a, band 2251 c are the number of searches moving to AI\(Mid Low and A3\(Mid the count c corresponds to the count for the whole candidate 


rule, the count a and b can be used for the sub candidate rules The measurements of candidate rules are calculated by these numbers After one turn of the database searching, the support confidence and chi-squared value can be calculated for the candidate rules and sub candidate rules. If the criteria are higher than their thresholds, then the time transitions and its sub time transitions become association rules and will be stored in a rule pool during the generations r 0000 0001 0002 0003 0004 0005 0006 0007 Ml M\\ -:1!"1 _   ,Low __ ,M Idle b &/c H:gh AJ CurrentConseguent T= 2  T= 2 A c Middle   I H L M L M L L L M lMj  HI M L L r-- L ""'\(M1. H M \\LJ. L ......... M M M M --CM H H L C Fig. 4. Two dimensional searching method 


D. Extraction of Rules Using the counts obtained by searching mechanisms, the support, confidence and chi-squared value can be calculated considering both the antecedent and consequent part of the candidate rules. Now, the important association rules are defined as the ones which satisfy the minimum chi-squared support and confidence value, i.e., X;'in' sUPmin and con!min respectively. Only the important association rules can be con sidered as interesting and stored in the rule pool The rules which contain more than Nthre kinds of attributes are the so called mUltiple rules, where Nthre is a user defined threshold. This is added to increase the diversity of the evolution process and the proposed method regards the rules containing many different kinds of attributes as important ones in addition to the conventional interests The interesting rules will not only be stored in the rule pool together but also be validated according to a different database, which is the validating database. As a result, both the conventional criteria of the association rules and the validation accuracy rates will be considered in the rule extraction phase thus the evolution concentrates on extracting more general rules which adapt to real-time traffic databases and avoid the over-fitting to training database during the evolution process For each interesting rule r, the validation accuracy is defined as follows where Da\(r tecedent part of rule r D\(r After the validation of the interesting rules generated by a GNP individual, their validation accuracies will be taken into the fitness value, hence can guide the evolution to generate GNP individuals which can extract more general rules adapt able to real-time databases Therefore, the fitness function of the GNP individual is now defined as F = L {x2\(r r nante\(r rER anew\(r r The symbols are as follows R : set of suffixes of important associatIOn rules which satisfy the importance requirements of chi-squared 


support and confidence value X2 \(r nante \(r rule r anew \(r r ule r is new anew = 0, otherwise amult \(r amult, if rule r has more than amult\(r 0, otherwise x2\(r r r r r cerned with the importance, accuracy, complexity, novelty and diversity of rule r, respectively From the fitness function, it can be seen that GNP indi viduals are defined as a tool to pick up candidate rules, and the proposed method does not aim at obtaining the optimal individuals, but extracting rules as many as possible. So, the individuals which can obtain many new rules with multiple attributes will have high fitness values In each generation, GNP individuals are replaced with the new ones by the selection policy and other genetic opera tions. Four kinds of genetic operators are used, i.e., uniform crossover, mutation for functions, mutation for connections and time delays of judgement nodes, respectively Classification Firstly, classify the extracted association rules in the pool into consequent classes. Every attribute has three consequent classes, i.e., attribute Ai is classified into the following conse quent classes, Ai\(Low Mid High Then, the association rules in each class are used to study whether the testing data satisfy the antecedent items. The testing data are called satisfied if they satisfy the antecedent items of the rules 2252 Unlike the validation process, the following partial match ing strategy is carried out for calculating the matching of rule r and testing data where Nk\(d, r in the antecedent part of rule r in class k 


Nk\(r r in class k The ratio of the number of satisfied rules to the total number of rules for every class is calculated considering confidence matching degree and validation accuracy, and the testing data is classified into the class whose ratio is the highest. The concrete process is like the following 1 in class k 2 Creditk = L confidence\(r Mk\(r r rERk where, a is weight of the prediction accuracy 3 Creditk Scorek T l  ota k where, Totalk is the total number of rules in class k that is, the fixed number of Nf in this paper 4 becomes the winner for the consequent attribute Ac, e.g if Ac \(Low into Low IV. S IMULAT ION In this section, the effectiveness and efficiency of the proposed methods are studied by traffic simulations. Unlike other methods in the traffic prediction of recent years, the proposed methods not only aim at predicting the traffic jam on a specific section of the road networks, but also interested in providing the whole traffic prediction for all of the interesting sections on the road networks so that the navigation system can refer to this kind of information for the calculation of the optimal route of the road networks A. Simulator Real-time simulator SOUND/4U, which is a fully customizable macroscopic free-flow traffic simulator aiming at providing efficient traffic control and management of the urban-level large scale traffic network, has been used. The SOUND/4U simulates the real-time traffic volume and trav eling speed of vehicles on the VICS[15] systems based on 


input OD\(Origin/Destination The simulation is carried out using the traffic network of Kurosaki in Kitakyusyu, Japan. As shown in Fig.5, the road model is based on the real traffic network and there are a TABLE J IJ PARAMETER SETTING FOR SIMULATION Items Values Number of sections 7941 N umber of intersections 4243 Number of traffic lights 142 Data collection interval I \(minute Total execution time 2\(hr Number of 00 points 20 Number of 00 pairs 100 Routing algorithm Logit Route Choice Logit parameter 1.0 Fig. 5. Road model used in simulations huge number of sections on the traffic network. The traffic conditions are shown in the Table III The real-time traffic volume of section s, represents as T\( s at the current time point is calculated like T\(s Nr\(s s s Cs x Ls The symbols are as follows Ls: The length of section s Cs: The capacity of section s, e.g., for sections with one lane Cs = 1, for sections with two lanes section Cs = 2 likewise Nr\(s the last time point Nin \(s sections at the current time point Nout\(s sections at the current time point After the calculation of real-time traffic volume T\( s each section, Nin\(user-defined chosen as the interesting section, here in the simulation Nin = 500 sections with heavier traffic volumes were selected and their corresponding traffic volume at every time point were discretized to LowlMiddle/High 2253 


Judging by common sense, the High threshold is defined as follows: for sections with one lane , if there exist 2 or more cars in 10 meters, then it is a High volume situation. On the other hand, the middle threshold is defined as follows: for sections with one lane, if there exist more than 1, and less than 2 cars in 10 meters, it is a Middle volume situation. The remains will be discretized to Low traffic volume situation The cars are randomly generated based on the values of the pre-defined OD\(OringinJDestination for every OD pair is changing during the execution as shown in Fig.6 lY l Jl .? 15 co ? 10 Xl 5 Q O .r:: 0 o 50 100  150 T:in e 4n ilutes Fig. 6. Change of OriginiDestination values in simulations B. Simulation Result The parameter setting of the proposed data mining during the evolution is shown in Table IV. The total number of rules stored in the rule pool is shown in Fig.7. Each round has the same number of generations of 50 and the chosen set size for AAM is 100[14 TABLE IV PARAMETER SETTING FOR EVOLUTION Items Values Number of judgment nodes 100 Number of processing nodes 10 N umber of attributes 500 Number of consequents 1500 Number of time points 120 Minimum confidence value 0.9 Minimum chi-squared value 6.63 Minimum support value 0.05 40000 Ul 35000 !Il " 30000 '" .... "0 25000 20000 ? '" 15000 j9 10000 0 5000 E 


r r  r r l o 100 200 300 400 500 ROlU1ds Fig. 7. Total number of rules extracted In order to check the effectiveness of the extracted rules we tested the classification accuracy of the proposed method using the classifier with competition The average prediction accuracy is shown in Table V. The accuracy is defined in the following: if the traffic prediction result of the section at time t is "Low" and the real traffic of this section at time t is exactly "Low", then the accuracy is 100%. The low/middle/high accuracy means the accuracy when the real traffic is low/middle/high, respectively Table V shows that the method with Accuracy Validation can improve the overall accuracy than the method without Accuracy Validation. The Middle volume of the traffic network can not be predicted accurately because the middle situations do not appear frequently enough to generate an enough number of association rules TABLE V AVERAGE PREDICTION ACCURACY FOR TESTING DATABASE Method Prediction Accuracy Overall Low Middle High With Accuracy Validation 84.82 85.84 57.71 89.71 Without Accuracy Validation 82.64 83.57 56.57 87.13 Longer step prediction is explored by studying the 2-step, 3step and 4-step prediction, where n-step means the prediction of the traffic volume at n time points later. It's results are shown in Fig. 8. It is shown from Fig.8 that the prediction accuracy decreases as the number of prediction step increases but the increase of the number of steps does not affect the overall accuracy so much, so the proposed method can do relative stable prediction even if the number of prediction steps Increases The ratio of the usable rules to the total number of rules in each prediction step is shown in Fig.9, which describes that 


how the number of the usable rules for prediction decreases by the increase of the prediction steps, considering the condition that the time delay between the antecedent part and consequent part should be bigger or equal to the prediction step. In another word, as the prediction step increases, the number of the usable rules decreases 86 if G 84 82 80 f-------?-""":__---------1 0:: 78 r-------------">.-======---1 :? 76 -------------------j 74 72 4 n-6tep I?w ihAcCUlacyValiiarnn -w ihoutAcCUlacyValiiarnn I Fig. 8. Overall accuracy for 2-step, 3-step and 4-step prediction The proposed method cannot extract all the rules meeting the given definition of importance since it uses the fixed 2254 120 II 100 80 "'<Ie o 60 ?? ? 0: 40 20 0 N Step Fig. 9. Average percentage of usable rules for each prediction step number of rules for each class of the consequent attributes but the result shows that the ability to extract important rules is sufficient enough for the purposes. The mechanism of Accuracy Validation shows more stable performances as shown in Fig. 8 V. CONCLUSION In this paper, an association rule mining method using GNP with Accuracy Validation mechanism has been proposed. It is clarified from simulations that the proposed method can extract important time-related association rules for each class of the consequent attributes efficiently. What's more important is that these rules can be used to decide to which class the time related data belong accurately. From simulations, we have also 


found that the proposed method can be used in traffic volume prediction problems. Further improvements of the proposed method is studied in terms of applying the proposed method to real world navigation systems REFERENCES I Vehicle Routing and Congestion Predictions for Real-time Driver Guid ance, Transportation Research Records, 1408, Transportation Research Board, Washington D.C., pp. 66-74, 1993 2 October 17, 1989 3 network predictions, In Proc. of the Conference of Canadian Society of Civil Engineers, Sherbrooke, Quebec, June, pp. 331-339, 1997 4 dynamic traffic networks with joint route and departure time choice. In Proc. of the 84th Annual Meeting of the Transportation Research Board Washington, DC., 2005 5 Springer, 2002 6 Related Sequential Association Rule Mining and Traffic Prediction", In Proc. of the IEEE Congress on Evolutionary Computation 2009, 2009/5 7 gorithm: Genetic Network Programming\(GNP Reinforcement Learning", Evolutionary Computation, MIT press, Vol. IS No.3, pp.369-398, 2007 8 Multiagent Models Based on Symbiosis", IEEE Trans. on Syst., Man and Cybernetics - Part B -, Vol.36, No.1, pp.179-193, 2006 9 Deck Elevator Group Supervisory Control System Using Genetic Network Programming, IEEE Trans. on Systems, Man and Cybernetics, Part C, Vol 38, No. 4, pp. 535-550, 200817 10 University of Michigan Press, 1975 11] D. E. Goldberg, Genetic Algorithm in search, optimization and machine learning, Addison -Wesley, 1989 12 means of natural selection, Cambridge, Mass., MIT Press, 1992 13 Programs, Cambridge, Mass.: MIT Press, 1994 


14 Association Rules Mining with Attribute Accumulation Mechanism and its Application to Traffic Prediction", Journal of Advanced Computational Intelligence and Intelligent Informatics, Vol. 12, No. 5, pp. 467-478 200817 15 Information and Communication System", Vehicle Navigation and Infor mation Systems Conference, 1993 2255 


intend to expand this work to involve some interesting features in each stage prediction and evaluate it on many datasets   REFERENCES  1] F. Ricardo, N. Ana, M. Paula, B. Gleidson, R. Fabiano ODE: Ontology-based software Development Environment, Proceedings of the IX Argentine Congress on Computer Science, pp. 1124-1135, 2003 2] E. Mendes, B. A. Kitchenham. Further comparison of cross-company and within-company effort estimation models for Web applications. In: Proc. 10th IEEE International Software Metrics Symposium, Chicago USA, pp.348-357, 2004 3] B. Boehm, R. Valerdi. Achievements and Challenges in Software Resource Estimation, Proceedings of ICSE 06 Shanghai, China, pp. 74-83,  2006 4] K. Molokken, M. Jorgensen. A review of software surveys on software effort estimation, Proceedings of International Symposium on Empirical Software Engineering \(ISESE 2003 5] M. Jorgensen, K. Molokken-Ostvold. How large are software cost overruns? A review of the 1994 CHAOS report, Information and Software Technology, Vol. 48 issue 4. PP. 297-301, 2006 6] X. Huanga, D. Hob, J. Rena, L. F. Capretz. Improving the COCOMO model using a neuro-Fuzzy approach Applied Soft Computing, Vol.7, issue 1, pp. 29-40, 2007 7] L. Briand, T. Langley, I. Wieczorek. A replicated assessment and comparison of common software cost modeling techniques, Proceedings of the 22nd international conference on Software Engineering, 2000 8] S.-J Huang, N. H. Chiu. Optimization of analogy weights by genetic algorithm for software effort estimation Information and Software Technology, Vol. 48, issue 11 pp. 1034-1045, 2006 9] Z. Xu, T. M. Khoshgoftaar. Identification of Fuzzy models of software cost estimation, Fuzzy Sets and Systems, Vol. 145, issue 1, pp. 141-163, 2004 10] R. Pressman. Software Engineering: practitioner 


approaches, McGraw Hill, London, 2004 11] M. Boraso, C. Montangero, H. Sedhi. Software cost estimation: an experimental study of model performance Universita di Pisa, Italy, 1996 12] Y. Wang, Q. Song, J. Shen., 2007. Grey Learning Based Software Stage-Effort Estimation. International Conference on Machine Learning and Cybernetics, pp 1470-1475, 2007 13] S. G. MacDonell, M. J. Shepperd. Using prior-phase effort records for re-estimation during software projects Ninth International, Software Metrics Symposium, pp 73- 86, 2003 14] M .C Ohlsson, C. Wohlin. An Empirical Study of Effort Estimation during Project Execution, Sixth International Software Metrics Symposium \(METRICS'99 1999 15] N. H. Chiu,,S. J. Huang.  The adjusted analogy-based software effort estimation based on similarity distances Journal of Systems and Software, Vol. 80, issue 4, pp 628-640, 2007 16] P. Sentas, L. Angelis, I. Stamelos, G.  Bleris. Software productivity and effort prediction with ordinal regression Information and Software Technology, Vol. 47, issue 1 pp. 17-29, 2005 17] E. Mendes, N. Mosley. Comparing effort prediction models for Web design and authoring using boxplots Australian Computer Science Communications,  Vol. 23 Issue 1, pp. 125-133, 2001 18] E. Mendes, N. Mosley, I. Watson. A comparison of casebased reasoning approaches, Proceedings of the 11th international conference on World Wide Web, pp. 272280, 2002 19] Q. Zhao, S. S. Bhowmick. Association Rule Mining: A Survey  http://citeseer.ist.psu.edu/734613.html, 2003 20] S. Morisak, A. Monden, H. Tamada. An Extension of association rule mining for software engineering data repositories, Information Science Technical Report NAIST, 2006 21] Q. Song, M. Shepperd.  M. Cartwright, C. Mair. Software defect association mining and defect correction effort prediction, IEEE transaction on software engineering Vol. 32, No.2, pp. 69-82, 2006 


22] R. Agrawal, T. Amielinski, A. Swami. Mining association rule between sets of items in large databases Proceedings of the ACM SIGMOD International Conference on Management of Data, pp. 207-216, 1993 23] M-J. Huang, Y-L. Tsou, S-C. Lee.  Integrating Fuzzy data mining and Fuzzy artificial neural networks for discovering implicit knowledge, J. Knowledge-Based Systems, Vol.19 \(6 24] ISBSG International Software Benchmarking standards Group, Data repository release 10, Site http://www.isbsg.org, 2007 25] L. Zadeh. Toward a theory of Fuzzy information granulation and its centrality in human reasoning and Fuzzy logic. J. Fuzzy sets and Systems 90, pp. 111-127 1997 26] I. H. Witten, E. Frank. Data Mining: Practical machine learning tools and techniques, 2nd Edition, Morgan Kaufmann, San Francisco, 2005   256 


encountering a related term, i.e. IC\(c e intuition behind the use of the negative likelihood is that the more probable a term to appear, the less information it conveys. All these features show that Jiangs measure tends to be more general and more appropriate for evaluating nontaxonomically related terms. Indeed, a high score of the relatedness measures suggests a strong relationship between terms Nevertheless, all relatedness measures have limitations because they assume that all the semantic content of a particular term is modeled by semantic links in WordNet Consequently, in many situations, truly related terms obtain a low scores even though their belongings to a certain category of tags, e.g., jargon tags Additionally, when measuring the quality of an automatically knowledge acquisition results, the typical measures used in Information Retrieval are Recall, Precision and F-Measure. However, computing Recall and F-Measure requires the availability of a Gold Standard. Hence, we will only compute the Precision which speci?es to which extent the non-taxonomic relationships is extracted correctly. In this case, the ratio between the correctly extracted relations i.e., their relatedness measures is greater than or equal to a minimum threshold, and the whole number of extracted ones is computed. Thus, we have Precision Total correctly selected entities Total selected entities 12 http://search.cpan.org/dist/WordNet-Similarity 13 A term refers to a tag subject or a tag object C. Evaluation of non-taxonomic relationships Only a percentage of the full set of non-taxonomic relationships \(89 is caused by the presence of non standard terms which are not contained in WordNet and, in consequence, cannot be evaluated using WordNet-based relatedness measures. Fig. 5 depicts the evaluation results of the extracted non-taxonomic relationships against their relatedness measures High relatedness score \(88 17% of the extracted relationships, as most of terms are strongly related with respect WordNet Null Scores were obtained for 5% of the extracted 


relationships. Analyzing this case in more detail, we have observed that the poor score is caused in many situations by the way in which Jiangs distance metric works. This latter completely depends on the distance between two terms based on the number of edges found on the path between them in WordNet. In consequence this measure returns a value that does not fully represent reality. For example, on the one hand, Jiangs distance metric returns a null value for the relationship between insurance and car, even though the ?rst is a commonly related to the second, i.e., car involved insurance Finally with a minimal Jiangs distance metric threshold, set to 46%, the computed precision of correctly extracted relationships candidates is equal to 68.8 An example of extracted non-taxonomic relationships is depicted in Table V where each relation describes the subject tag, e.g., tool, the predicate, e.g is being developed within, and the object tag, e.g mesh. Fig. 4 represent a fragment output of the extracted ontological structure where each concept de?nes a set of similar and synonym tags and labels, i.e., mentions has been, revealed, caused and is created with describe the predicates of the non-taxonomic relationships between terms Due to the limitations observed by the automatic evaluation procedure and the lack of gold standards containing non-taxonomic relationships, we have examined the extracted non-taxonomic relationships from a linguistic point 377 Top space      distance     quad great     groovy nifty caused address      addresses extension      quotation   reference  references extensions        referenz     source      refrence sources    rfrences    quotations research    search     searching searchs open-source     open_source 


opensource linux aim     design     designer      designers patern    project     patterns     projekte projects web+design    web_design webdesign internet       internetbs net          web network      networking networks      web discussion     news       password word      words community      communities is_created_with mentions revealed has_been Figure 4. A fragment output of the extracted ontological structure of view. This qualitative evaluation can bring some interesting insights about the kind of results one can expect Invalid relations are extracted: Even though a relation such as music cities skill is considered as correct one since tag subject, tag object and predicate are correctly extracted. From a semantic point of view, this relation has no meaning. Hence, a higher precision is expected Figure 5. Summary of non-taxonomic evaluation measure Table V EXAMPLES OF EXTRACTED NON TAXONOMIC RELATIONSHIPS Subject Predicate Object search has been reference reference mentions search tool is being developed within mesh security added encoding search revealed reference java provides library by performing the sense analysis on complete relations An ambiguity in the extracted predicates between terms is observed: Hence, same relations are redundant since they use a synonym predicates between terms, e.g java provides library and java yields library. Thus we expect that the redundancy removal within extracted relations will be of bene?t for the improvement of the 


obtained results VI. CONCLUSION AND FUTURE WORK The extraction of non-taxonomic relationships from folksonomies is to the best of our knowledge is the least tackled task within ontology building from folksonomy. This is why there is a need of novel and general purpose approaches covering the full process of learning relationships. In this paper, we introduced a new approach called NONTAXFOLKS that starts by pre-processing tags aiming at getting a set of frequent tagsets corresponding to an agreed representation Then, they are used to retrieve related tags using external resources such as WordNet. Thanks to the particular structure of triadic concepts, it allows grouping semantically related tags by considering the semantic relatedness embodied in the different frequencies of co-occurences among users, resources and tags in the folksonomy. Thereafter we introduced an algorithm called NTREXTRACTION for extracting non-taxonomic relationships between pair of tags picked from the triadic concepts. In summary, our approach uses several well known techniques \(such as formal concept analysis or association rule discovering the social bookmaring environnement in order to propose a new way of extracting labeled non-taxonomic relationships between tags. Currently, we are investigating the following topic concerning the discovered predicates between two terms. Indeed, in order to avoid relationships redundancy and thus a redundancy in the builded ontology. One can try to classify them into prede?ned semantic classes, detect synonyms, inverses, etc. A standard classi?cation of verbs could be used for this purpose, adding additional information about the semantic content, e.g., senses, verb types, thematic roles, etc., of predicates relationships 378 REFERENCES 1] J. Pan, S. Taylor, and E. Thomas, Reducing ambiguity in tagging systems with folksonomy search expansion, in Proceedings of the 6th Annual European Semantic Web Conference \(ESWC2009 2] V. S. M. Kavalec, A. Maedche, Discovery of lexical entries for non-taxonomic relations in ontology learning, in Proceedings of the SOFSEM 2004, LNCS, vol. 2932, 2004, pp 249256 


3] L. Specia and E. Motta, Integrating folksonomies with the semantic web, in Proceedings of the 4th European Semantic Web Conference \(ESWC 2007 Innsbruck, Austria, vol. 4519, June 2007, pp. 624639 4] P. Mika, Ontologies are us: A uni?ed model of social networks and semantics, in Proceedings of the 4th International Semantic Web Conference \(ISWC2005 3729, Galway, Ireland, June 2005, pp. 522536 5] P. Schmitz, Inducing ontology from ?ickr tags, in Proceedings of the Workshop on Collaborative Tagging \(WWW 2006 Edinburgh, Scotland, May 2006 6] M. Zhou, S. Bao, X. Wu, and Y. Yu, An unsupervised model for exploring hierarchical semantics from social annotations, in Proceedings of the 6th International Semantic Web Conference and 2nd Asian Semantic Web Conference ISWC/ASWC2007 Korea, vol. 4825, November 2006, pp. 673686 7] C. Schmitz, A. Hotho, R. Jaschke, and G. Stumme, Mining association rules in folksonomies, in Proceedings of the 10th IFCS Conference \(IFCS 2006 2006, pp. 261270 8] A. Hotho, A. Maedche, S. Staab, and V. Zacharias, On knowledgeable unsupervised text mining, in Proceedings of Text Mining Workshop, Physica-Verlag, 2003, pp. 131152 9] A. Hotho, R. Jaschke, C. Schmitz, and G. Stumme, Information retrieval in folksonomies: Search and ranking, in The Semantic Web: Research and Applications, vol. 4011 Springer, 2006, pp. 411426 10] F. Lehmann and R. Wille, A triadic approach to formal concept analysis, in Proceedings of the 3rd International Conference on Conceptual Structures: Applications, Implementation and Theory. Springer-Verlag, 1995, pp. 3243 11] R. Jaschke, A. Hotho, C. Schmitz, B. Ganter, and G.Stumme Discovering shared conceptualizations in folksonomies Web Semantics: Science, Services and Agents on the World Wide Web, vol. 6, pp. 3853, 2008 12] A. Mathes, Folksonomies - cooperative classi?cation and communication through shared metadata, Graduate School of Library and Information Science, University of Illinois Urbana-Champaign, Tech. Rep. LIS590CMC, December 2004 13] H. Lin, J. Davis, and Y. Zhou, An integrated approach 


to extracting ontological structures from folksonomies, in Proceedings of the 6th European Semantic Web Conference ESWC 2009 vol. 5554, 2009, pp. 654668 14] M. Szomszor, H. Alani, K. OHara, and N. Shadbolt, Semantic modelling of user interests based on cross-folksonomy, in Proceedings of the 7th International Semantic Web Conference \(ISWC 2008 15] G.Begelman, P. Keller, and F.Smadja, Automated tag clustering: Improving search and exploration in the tag space, in Proceedings of the the Collaborative Web Tagging Workshop WWW 2006 16] R. Jaschke, A. Hotho, C. Schmitz, B. Ganter, and G. Stumme TRIAS - an algorithm for mining iceberg tri-lattices, in Procedings of the 6th IEEE International Conference on Data Mining, \(ICDM 2006 2006, pp. 907911 17] C. Borgelt, Ef?cient implementation of APRIORI and ECLAT, in FIMI, COEUR Workshop Proceedings, COEURWS.org, vol. 126, 2003 18] J. Tang, H. Leung, Q. Luo, D. Chen, and J. Gong, Towards ontology learning from folksonomies, in Proceedings of the 21st international jont conference on Arti?cal intelligence IJCAI 2009 20892094 19] L. Ding, T. Finin, A. Joshi, R. Pan, R. Cost, Y. Peng P. Reddivari, V. Doshi, and J. Sachs, Swoogle: A search and metadata engine for the semantic web, in Proceedings of the 13th ACM Conference on Information and Knowledge Management, ACM Press, 2004, pp. 652659 20] A. Hliaoutakis, G. Varelas, E. Voutsakis, E. Petrakis, and E. E Milios, Information retrieval by semantic similarity, International Journal on Semantic Web and Information Systems IJSWIS 21] G. Pirro, M. Ruffolo, and D. Talia, Secco: On building semantic links in peer to peer networks, Journal on Data Semantics XII, LNCS 5480, pp. 136, 2009 22] C. Meilicke, H. Stuckenschmidt, and A. Tamilin, Repairing ontology mappings, in Proceedings of the International Conference AAAI 2007, Vancouver, British Columbia, Canada 2007, pp. 14081413 23] S. Ravi and M. Rada, Unsupervised graph-based word sense 


disambiguation using measures of word semantic similarity in Proceedings of the International Conference ICSC 2007 Irvine, California, USA, 2007 24] H. G. A. Budanitsky, Semantic distance in wordnet: an experimental application oriented evaluation of ?ve measures in Proceedings of the International Conference NACCL 2001 Pittsburgh, Pennsylvania, USA, 2007, pp. 2934 25] J. Jiang and D. Conrath, Semantic similarity based on corpus statistics and lexical taxonomy, in Proceedings of the International Conference ROCLING X, 1997 379 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


