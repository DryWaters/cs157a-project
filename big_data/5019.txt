Data Mining Approaches for Packaging Yield Prediction in the Post-Fabrication Process 
Seung Hwan Park, Cheong-Sool Park, Jun Seok Kim Sung-Shick Kim Jun-Geol Baek  
School of Industrial Management Engineering Korea University, Anam-dong, Sungbuk-gu, Seoul, 136701, Korea SK Hynix Semiconductor San 136-1, Ami-ri, Bubal-eup, Icheon, Gyeonggi-do 467-701, Korea 
udongpang, dumm97, bliths, sungskim jungeol}@korea.ac.kr Daewoong An daewoong.an@sk.com 
Abstract 
In the post-fabrication process for semiconductors it is critical to predict the yield. This process consists of a series of electrical and physical tests following semiconductor fabrication tests that generate a significant volume of parametric data. While past research has investigated yield prediction using parametric test data, most studies have difficulty correctly predicting the low and high yield because of 
the wide range of variables and the large data set. Also, in the case of the packaging yield pred iction is inaccurate as this yield does not directly correlate with the parametric test data Therefore, this study proposes a framework in which the packaging yield is classified using the parametric test data of the previous step of the packaging test. This study involves three stages In the first, data preprocessing is conducted due to the large data set. To learn a data mining model using much more data, parametric test data generated in the die level need to be changed into the wafer level. In the second stage a random forest algorithm is used to select significant variables affecting the packaging yield. Finally, the third stage uses a nonlinear support vector machine \(SVM to classify the low and high yield Through the three stages, this study demonstrates that this proposed algorithm has a superior performance 
Keywords-Random Forests; Ensemble Support Vector Machine; Packaging Yield Classification; Semiconductor Manufacturing Process 
I I NTRODUCTION The semiconductor manufacturing process produces core components such as memory and central processing units CPU\ Because of the high demand for the newest electronic devices such as smart phones and tablets, it is important to consistently improve the manufacturing process through state-of-the-art equipment or process management techniques In particular accurate yield prediction is imperative in managing the process Yield prediction is conducted by classifying the low and high yield groups in the final test step of the post-fabrication process. However this step in semiconductor manufacturing is complicated because there are hundreds of manufacturing processes which take several months to complete In addition, to accurately manage the yield, semiconductor manufacturing 
companies require a super-clean environment, regular equipment maintenance and comprehensive worker training 1 B u t ab o v e a l l y ie ld m a na ge m e nt thro ugh the a na ly sis of data generated during the post-fabrication process needs to be conducted Therefore yield classification is generally performed using parametric test data. In the post-fabrication process, the packaging yield is more important than other kinds of yield. Fig. 1 represents the sequence of steps and the yield in the post-fabrication process Figure 1 Flow chart of the semiconductor manufacturing process The semiconductor manufacturing process consists of about 300 to 400 units and it takes 3 to 4 months to complete a finished chip  F i r s t  w a f e r s g o thro ugh the fab r ic ati o n process for 2 to 3 months before moving on to the postfabrication stages The post-fabrication process consists of a wafer test, the assembly, and the final test [2  In  c o n j u n c t i o n  
with each of these steps the fabrication yield, the wafer test yield, the assembly yield an d the packaging y ield are all calculated. The fabrication yield refers to the ratio of wafers in to wafers out. The wafer test yield is calculated as the ratio of whole chips of wafer out and the number of non-defective chips as decided by the wafer test The assembly and 
Fab 
Wafer Test Assembly 
Final Test 
Parametric data Wafer  Wafer test yield Packaging yield Assembly yield 
Prediction Model 
Classifying high/low yield   
2013 IEEE International Congress on Big Data 978-0-7695-5006-0/13 $26.00 © 2013 IEEE DOI 10.1109/BigData.Congress.2013.55 363 


002 002 002 002 002 002 
002 003 004 005 004 002 003 005 005 006=\007  010 011 012 011  010 011 013\014 015 012 011 013  0161 1  017 011\020\021 022 006 002 023 010 012 010 024\025\010\016 026=0 2 027 024 027 2 027 024 027 027 024 027 030 031 
m 
packaging yields are the ratio of the input chips after assembly and the input chips afte r the final test respectively This study focuses primarily on the packaging yield, in particular the classification of the low and high yield based on the probe test data In order to best predict the yield in the semiconductor manufacturing process, a great deal of research has been performed. However, most research has focused on the wafer test yield prediction, rath er than the packaging yield prediction The yield prediction that uses a markov chain is conducted by calculating the probability that a chip is acceptable given defects   H o w e ver, this  p r e d ic ti o n is  limited due to the univariate analysis on which it is based So as the number of variables from the probe test is enormous multivariate analysis is esse ntial. A classic method for multivariate data analysis is multiple discriminant analysis However, this method of analysis is limited by the assumption that variables are independent, identical and normally distributed For more advanced multivariate analysis, the hybrid machine learning method is applied in the semiconductor manufacturing process, using an inductive decision tree, a neural network with a back-propagation algorithm, and a self-organizing map SOM 4 Also  the yield prediction is conducted using a fuzzy-based neural network from the multivariate parametric test data [5  However, this prior research has not focused on the packaging yield To predict the packaging yield, a stepwise support vector machine \(S-SVM algorithm is applied to the wafer test data [1 SSV M cl ass i f i es th e l ow an d h i g h y i el d  of the final test by screening for potential defects in the final test process. However, problems with parameter selection step selection and classification accuracy exist in this algorithm. Therefore, this paper proposes a data mining approach that selects significant parameters and improves classification accuracy The rest of this paper is organized as follows. Section II and Section III introduce the basic concept of the random forest algorithm and the ensemble support vector machine E-SVM\ algorithm respectively. Section IV describes the proposed algorithm based on Sections II and III. In Section V, the experimental results are explained after the proposed algorithm is applied to actual industrial data. Finally, a conclusion and further studies are described in Section VI II R ANDOM F ORESTS A LGORITHM Widely known and efficient, the random forests algorithm is used for both classification and regression Similar to the ensemble method, this algorithm is based on bagging \(bootstrap aggregating\ and is introduced by Breiman 6   B a gging is a t y p e o f  e n se m b le algor ithm a n d i s generally applied in the dec ision tree method 7  T h e advantage of random forests is that there is no need for pruning trees and it automatically generates the importance and accuracy of variables. Also, this algorithm is not sensitive to outliers in training data The basic principle of random forests is to combine many binary decision trees built by using several bootstrap samples and choosing a subset of predictors randomly at each node 8  T h e b a s i c f l ow of  ran dom f o r e s t s i s  re pr es en t e d  by u s i n g  the following algorithm    Step 1: Generate trees from the bootstrap sample data. The number of trees means the training cases let the number of variables for classification be  S tep 2 Input variables to be used to find a random split point at a node of the tree unlike an optimal split point should be far smaller than  Step 3: Determine a training subset for this tree by choosing times with replacements from all available training cases. To estimate the error of the tree, the remaining cases ar e used by predicting their classes Step 4: For each node of the tree, randomly choose variables on which to base the decision at that node Step 5: Find an optimal split point based on variables in the training set Step 6: Grow each tree completely; do not prune them The random forests algorithm is used for the selection of variables as well as classification and regression. Many selection procedures are based on the importance of the variable to ranking and model estimation so that a family of models can be generated, evaluated and compared [8 III S UPPORT V ECTOR M ACHINE The SVM is a supervised learning method that classifies data by using the hyperplane maximizing margin and is introduced by Vapnik [10 Th e S V M  p e rf o rm s  exceptionally in document classification, image recognition and text recognition [1  T he t r a i ning  d a t a s e t for the SVM can be represented as \(1\and it consists of predictors and a class label 1 The training data set includes elements, pairs of dimensional vectors known as a predictor and class  corresponding to  The hyperplane is defined as and it is known as the SVM classifier. As shown in Fig. 2 the margin should be maximized for more accurate classification. The objective function that maximizes is reformulated to minimize  However, as shown in Fig. 2 a hyperplane that can totally classify two class data does not exist Therefore, a slack variable is used for inseparable cases The slack variable adjusts the degree of misclassification and is adopted for the entire training data set 
364 


Step 1 Step 2 Step 3 
i=1,…,n  i=1,…,n  A Data Preprocessing B Variable Selection Using Random Forests 
012 031 004\032\002\032\005\032\033\034 027 035 027 036 030 037\025  031 011 022 011\020\021 026$\034\012% %&     \012 011  024\025\010 011 016\026  1\016\031 011 1  032  002 004-\010\032\005\032\033\034 037 025   011 022 011\020\021 016 021 030   011  3 012 011 012 3 022 0113\020\021 4\010 011 5 010 3 6 026$\034\012% %&    0   011 037   011 012 011 0 022 011\020\021 7\002\(\010  010 016 8  010  7\002  024 5 010+\026  7\002   011 012 011 4\010 011 5 010 3 6+\026 022 011\020\021 010 9 004-\010\032\005\032\033\034   011 022 011\020\021 016 021 030   011  3 012 011 012 3 022 0113\020\021 94\010 011 5 010 3 6 026$\034\012% %&    0   011 037   011 012 011 0 022 011\020\021 8  010  7\002   011 012 011 94\010 011 5 010 3 6+\026 022 011\020\021 
DATA PREPROCESSING RANDOM FORESTS ENSEMBLE SVM Or  OTHER ALGORITHMS 
025\010 011 016\026=1\016\031 011 031 011 
Figure 2. The concept of the SVM classifier In \(2\ which represents a modified objective function the multiplication of the penalty cost 
Raw Data Wafer Summary Data Significant Variables 
and summation of all  are added to the existing objective function. Based on the objective function, the optimization problem is formulated by adding constraints as shown in \(2 2 Also, this problem can be extended to the dual problem as shown in \(3 3   The function of the SVM linear classifier is formulated as 4\, where represents the function that returns 1 when is less than 0, otherwise 1   4 Also, the kernel trick should be applied for the SVM nonlinear classifier. Equation \(5\ is transformed by replacing variable in \(3\ with the kernel function Finally, in \(6 the non-linear classifier using the kernel trick is calculated 5     6 IV T HE P ROPOSED A LGORITHM This section describes the proposed algorithm, which consists of three parts. First, data preprocessing is performed to cover large amounts of data Second the random forests algorithm is applied to the sel ection of variables and third the E-SVM is conducted in order to classify low and high yield in the final test proces s. The outline of the proposed algorithm is shown in Fig. 3 Figure 3. Overall framework of the proposed algorithm More detailed description with regards to the proposed algorithm is as in the following In the semiconductor manufacturing process, it generally takes 3 to 4 months to complete a finished product. During this time a large amount of data is generated. In particular data at the chip level incorporates many observations making analysis difficult in establishing relation to the packaging yield. As a result we focus on data at the wafer level Using statistics such as the mean and the standard deviation of the parametric test data, we extract a new feature representing wafers. Finally, a single wafer represented as just one observation unlike including about 1500 observations in one wafer Even after data preprocessing, a large number of variables remain Because the presence of many variables reduces the learning performance it is necessary to select the significant variables. Therefore, this study proposes a selection method using the random forests algorithm The objective of this algorithm is to choose variables closely correlated with the response variable, for the purpose of identification using decision trees To identify the significant variables, four measures to assess the importance of the   margin  
Classification of High and Low Yield 
365 


n n 
A B A 
Step 1 Step 2 Step 3 Step 1 
003 
C Yield Classification Using E-SVM 
  004 011  032  1   002  006 011  004 011 002 002  
B i M i S niM i 
1 
variables are calculated [1  Fou r m eas u r es a r e as  in th e  following: \(1\easure 1 is calculated by comparing total classification error rate with the new classification error rate using th variable 2\ Measure 2 is defined as the proportion of correct classifications exceeds the proportion of the most voted misclassifications.\(3\easure 3 is calculated by the difference between the number of lowered and raised margins as described framework in Measure 2. \(4 Measure 4 is calculated through the Gini index or the Shannon entropy 1  T h es e m easu r es  ar e con s i d er ed as  points in a four-dimensional space. Fig. 4 shows the process of variable selection Figure 4. Procedure for variable selection Consequently, the input variables of the classification model are selected using the procedure above E-SVM refers to ensemble learning using a SVM. The basic concept of ensemble learning is to employ multiple learners and combine th T h e SVM is a w idel y  known classifier on the diverse domain. However, there are limitations in learning the algorithm due to the non-linearity and complexity of the data. Therefore, this study proposes an ensemble SVM to increase the classification rate Fig. 5 outlines the proposed ensemble SVM In Step 1, the given data set is separated into a training set A and a test set in the ratio of 4 to 1 Training set A is used to build an ensemble learning model using the SVM. Step 2 selects random samples for learning recursively. Generally bagging, boosting, disjunct partitioning and fold partitioning are considered when composing a samp In  this study, disjunct partitioning that randomly splits the training data is used. Following the selection of random samples, Step 3 is performed: the building and classification of the model. The SVM model using the random sample is constructed and the prediction of the test set is performed using the model  Steps 2 and 3 are repeated times and prediction results are produced The average of the prediction values is then regarded as the final predicted class for test set  Figure 5. Flow chart combining the ensemble technique and SVM V E XPERIMENTAL R ESULTS For the experiment, an actual industrial data set is used The original data set is larg e, consisting of observations at the chip level Therefore, the data set is reduced to the wafer level and each wafer is labeled as a high or low yield using specified criteria In this study, we have labeled the high yield in situation where the yield is greater than the average yield and the low yield in situation where the yield is less than the average yield Also, 500 wafers are separated into a training set and a test set in order to evaluate the performance of the model This experiment consists of two parts The first represents the results of variab le selection using the random forests algorithm and the second evaluates the performance of the E-SVM The results are compared to those of widely used classification algorithms in the data mining domain: KNearest Neighbor KNN\, Random Forests, Decision Tree Neural Network and SVM To evaluate the performance of the algorithms, we compared the low yield classification accuracy \(LCA\the high yield classification accuracy HCA and total classification accuracy \(TCA before and after applying the significant variables obtained from the random forests algorithm The LCA represents the ratio of classifying correctly the actual low yield class as the low yield class and 
Split a training set     and test set Get randomly samples from Learning of SVM using Get the SVM Model Prediction of     through Calculate the average of prediction results Repeat times 
003 
Get a four-dimension al center vector with coordinates using an average of four measures Calculate a distance between each point and the center vector and so rt the distances in decreasing order Select the variable whose the distance exceeds a given threshold 
1 
Step 2 Step 3 Step 4 
niS i 
366 


 
LCAI HCAI TCAI LCA HCA TCA 
Algorithms Classification Accuracy Increment SVM 25.53%p 14.22%p 19.87%p Algorithms Classification Accuracy Proposed Algorithms E-SVM 76.60 78.43 77.51 Proposed Algorithms E-SVM 74.47 83.33 78.90 
Random Forests 6.38%p 8.21%p 0.91%p Decision Tree 0.00%p 8.46%p 4.23%p KNN 8.51%p 11.15%p 1.32%p Nerural Network 29.79%p 24.39%p 2.70%p Average 3.83%p 13.29%p 4.73%p Random Forests 68.09 64.71 66.40 Decision Tree 55.32 68.63 61.97 KNN 65.96 45.10 55.53 Neural Network 78.72 23.53 51.13 SVM 38.30 60.78 49.54 Average 63.83 56.86 60.35 Random Forests 61.70 72.92 67.31 Decision Tree 55.32 77.08 66.20 KNN 57.45 56.25 56.85 Nerural Network 48.94 47.92 48.43 SVM 63.83 75.00 69.41 Average 57.45 65.83 61.64 
HCA means the ratio of classifying correctly the actual high yield class as the high yield class. TCA is calculated by averaging the LCA and the HCA In addition as shown in Table I the classification accuracy increment CAI is calculated to identify any differences before and after the application of the selected variables The low yield classification accuracy increment \(LCAI\ the high yield classification accuracy increment \(HCAI\d the total classification accuracy increment \(TCRI\ represent increments corresponding to LCA HCA and TCA respectively When the CAI is pos itive, the selected variables are significant. In the opposite case, the selected variables are not significant Table I is to compare LCAI, HCAI and TCAI in order to identify the performance of vari able selection. As a result the SVM algorithm demonstrates a powerful performance increment. In the case of the remaining algorithms, LCAI produces a positive number because the HCAI is greater than the absolute value of the LCAI i n spite of the negative LCAI Therefore, the variables selected by using the random forests algorithm are significant TABLE I V ARIABLE S ELECTION P ERFORMANCE Table II shows the performance of the algorithms including our proposed algori thm E-SVM First, the KNN algorithm has a difficulty in learning due to the decision of and its weakness caused by multivariate data. Also, the poor performance of the neural network algorithm occurs because of the structure of network laye rs or the sensitiv ity in setting the parameters. The SVM using a kernel trick proves suitable for classifying the non-linear data set. However, learning performance is reduced due to the small data set. Therefore the ensemble technique is applied to the classification algorithms, such as decision tree and SVM. The proposed algorithm applying the ensemble technique produces the highest LCA HCA and TCA both before and after the application of variable selection. Also the classification accuracy increases after using the proposed variable selection algorithm TABLE II T HE P ERFORMANCE OF T HE A LGORITHMS Consequently the proposed algorithm combining the ensemble technique with SVM demonstrates significance in variable selection using the random forests algorithm, and performs better than the other methods tested VI C ONCLUSION The post-fabrication process consists of many test stages and each stage generates a large amount of parametric test data. Therefore, the variable selection technique proposed in this study is applied to r eal industrial data and its performance is verified Also, the ensemble learning using SVM is a highly efficient method for predicting the packaging yield. Consequently, this study offers two data 
No Variable Selection Variable Selection 
367 


mining approaches to manage yield efficiently. In furtherance of these results, th e algorithm integrating the two data-mining approaches needs to be revised given that variable selection and ensemble learning offer superior performance in semiconductor manufacturing Also, many more data sets need to be appl ied to this proposed algorithm to test the robustness of these results A CKNOWLEDGMENTS This research was supported by the Basic Science Research Program through the National Research Foundation of Korea \(NRF\ed by the Ministry of Education, Science and Technology \(2012-0008332 This research was supported by the Ministry of Knowledge Economy MKE\, Korea, under the IT R&D Infrastructure Program supervised by the National IT Industry Promotion Agency NIPA\ \(NIPA-2012-\(B11001101-0002 R EFERENCES 1 An, D., Ko, H.H., Gulamba, T., Kim, J., Baek, J.G., and Kim S., “A Semiconductor Yields Prediction Using Stepwise Support Vector Machine”, IEEE Ineternational Symposium on Assembly and Manufacturing,  December 2009 2 Uzsoy, R., C. Lee, and L.A. Martin-Vega, “A Review of Production Planning and Scheduling models in the Semiconductor Industry PART I: System Characteristics Performance Evaluation and Production Planning,” IIE Transactions, Vol. 24, No. 4, pp. 47-60, 1992 3 Ciciani, B. and G. Jazeolla, "A Markov Chain-Based Yield Formula for VLSI Fault-Tolerant Chips," IEEE Transactions on Computer-Aided Design, Vol. 10, No.2, pp. 252-259, 1991 4 Kang, B.S., Lee, L.H., Shin, C.K., Yu, S.L., & Park, S.C Hybrid machine learning system for integrated yield management in semiconductor manufacturing," Expert Systems with Applications, Vol. 15, No. 2, pp. 123-132 August 1998 5 Wu, L., and Zhang, J., “Fuzzy neural network based yield prediction model for semiconductor manufacturing system International Journal of Production Research, Vol. 48, No. 11 pp. 3225-3243, June 2010 6 Breiman, L., “Random Forests”, Machine Learning, Vol 45 No. 1, pp. 5–32, October 2001 7 Breiman, L., “Bagging predictors”, Machine Learning, Vol 24 No. 2, pp. 123–140, August 1996 8 Genuer, R., Poggi, J.,M., and Tuleau-Malot, C., “Variable selection using random forests”, Pattern Recognition Letters Vol. 31, pp. 2225-2236, March 2010 9 Radenkovic, P., “Random Forest”, unpublished 10 Vapnik, V., The Nature of Statistical Learning theory Springer-Verlag, 1995 11 Joachims, T., "Text catergorization with support vector machines," Proceedings ofthe European Conference on Machine Learning, 10th European Conference on Machine Learning, pp. 137-142, 1998 12 Park, J., Kwon, I.H., Kim, S and Baek, J Vol. 3, No. 1 2002 13 Breiman, L Manual on setting up, using, and understanding Random Forests”, Technical Report Vol. 3, No. 1, 2002 14 Sewell, M., “Ensemble Learning”, unpublished, August 2008 15 Liaw, A., and Wiener, M., “Classification and Regression by randomForest R News Vol. 2, No. 3, pp. 18-22, D ecember 2002 
368 


002 002 
024\025\026\016\006\025\021\021\003\006\007 016\025\006\020+\003 024\013 012\021+\025\006\015\016\032\004\007  024\020\030\003\007\(\016\017\023*\007\(\031\003\026\020\006\015\025\007 007 
M\003\012\005\026\030\015\011\016\012\015\030\003\024\026\033\003\011\007\010\007\024\030\007\030\003\0073\023\010\005\014\015\030\003\017\011\005$\003 015\013\007\003\010\014-\011\024\011#\003\024\012\012\005\011\033\014\026\025\003\015\005\003\014\026\017\005\011$\024\015\014\005\026\003\011\007\012\007\014\006\007\033\003\017\011\005$\003 030\007\026\030\005\011\030,\003 003 M\003 014\030\003 023\007\007\011\003 024\030\007\033\003 024\026\033\003 015\013\007\011\007\017\005\011\007\003 030\015\005\011\007\033\003 010\005\012\024\010\010#\003\024\030\003\031\007\010\010\003\024\030\003\011\007$\005\015\007\010#\003\031\014\015\013\003\005\015\013\007\011\003\024\025\007\026\015\030,\003/\014\010\010\003 007\003 0073\015\011\007$\007\010#\003 014\026\014$\024\010\014\030\015\003 024\026\033\003 012\005\026\030\014\030\015\003 005\017\003 024\003 030\014\026\025\010\007\003 033\024\015\024\003\030\015\011\016\012\015\016\011\007\003\033\007\015\024\014\010\014\026\025\003\015\013\007\003\005\006\007\011\024\010\010\003\012\016\011\011\007\026\015\003\030\015\024\015\016\030,\003 003 035\024\012\013\003 012\005$\023\005\026\007\026\015\003 024#\003 0073\007\012\016\015\007\003 005\026\003 024\026#\003 023\010\024\015\017\005\011$\003 9\015\024-\010\007\003 003 024\030\003 024\003 030\014\026\025\010\007\003 012\005$\023\005\026\007\026\015\003 005\011\003 024\030\003 024\003 012\005\010\010\007\012\015\014\006\007,\003 002\003 033\024\015\024\003 017\010\005\031\003 033\014\024\025\011\024$\003\030\013\005\031\014\026\025\003\015\013\007\003\011\007\010\024\015\014\005\026\030\013\014\023\003-\007\015\031\007\007\026\003\012\005$\023\005\026\007\026\015\030\003\012\024\026\003 007\003\030\007\007\026\003\014\026\003\017\014\025\016\011\007\003>,\003 013\007\003\024\010\025\005\011\014\015\013$\003\025\014\006\007\026\003\017\005\011\003\015\013\007\003\032\010\005\016\033\003\031\005\011$\003\014\030\003\030\014$\014\010\024\011\003\015\005\003\015\013\024\015\003 005\017\003\024\003\015#\023\014\012\024\010\003\031\005\011$\0039\017\014\025\016\011\007\003?:\003\0073\012\007\023\015\003\017\005\011\003\024\003\017\007\031\003\012\013\024\026\025\007\030\003\011\007\030\016\010\015\014\026\025\003 017\011\005$\003 015\013\007\003 024\011\012\013\014\015\007\012\015\016\011\024\010\003 033\014\017\017\007\011\007\026\012\007\030\003 014\026\003 015\013\007\003 007\026\006\014\011\005\026$\007\026\015!\003 024\026\033\003 024\030\003 030\016\012\013\003 015\013\007\003 032\010\005\016\033\003 024\030\007\033\003 031\005\011$,\003 013\007\030\007\003 024\011\007\003 030\016$$\024\011\0142\007\033\003 024\030\003 017\005\010\010\005\031\030A\003 003 034\026\030\015\007\024\033\003 005\017\003 012\013\007\012 \014\026\025\003 010\005\012\024\010\010#\003 017\005\011\003 013\024\010\015\003 012\005\026\033\014\015\014\005\026\030!\003 015\013\007\030\007\003\024\011\007\003\033\007\015\007\011$\014\026\007\033\003\017\011\005$\003\015\013\007\003\033\014\030\015\011\014-\016\015\007\033\003\030\015\005\011\024\025\007,\003 037\007\017\005\011\007\003 024\026\003 024\015\015\024\012 \003 014\030\003 0073\007\012\016\015\007\033\003 015\013\007\003 007\026\006\014\011\005\026$\007\026\015\003 014\030\003 017\014\011\030\015\003 030\007\026\030\007\033!\003 014\026\003 015\013\007\003 005\011\014\025\014\026\024\010\003 031\005\011$\003 033\007\030\014\025\026\003 015\013\007\003 007\026\006\014\011\005\026$\007\026\015\003\031\024\030\003\024\010\031\024#\030\003\024\030\030\016$\007\033,\003 037\007\017\005\011\007\003\024\026\003\024\015\015\024\012 \003\014\030\003\0073\007\012\016\015\007\033\003\015\013\007\003$\005\030\015\003\024\023\023\011\005\023\011\014\024\015\007\003 024\015\015\024\012 \003 006\007\012\015\005\011\003 014\030\003 012\013\005\030\007\026!\003 014\026\003 015\013\007\003 005\011\014\025\014\026\024\010\003 031\005\011$\003 033\007\030\014\025\026\003\015\013\007\003\0073\023\010\005\014\015\003\015\007\026\033\007\033\003\015\005\003-\007\003\030\015\024\015\014\012,\003 032\005$\023\005\026\007\026\015\030\003$\024#\003-\007\003\033\014\030\015\011\014-\016\015\007\033,\003 003 013\007\003 017\005\010\010\005\031\014\026\025\003 014\030\003 024\003 030\014$\023\010\007\003 013#\023\005\015\013\007\015\014\012\024\010\003 0073\024$\023\010\007\003 005\017\003 013\005\031\003 015\013\007\003 031\005\011$\003$\014\025\013\015\003\005\023\007\011\024\015\007!\003\012\024\015\007\025\005\011\0142\007\033\003\014\026\003\015\013\007\003\017\005\010\010\005\031\014\026\025\003\024\010\025\005\011\014\015\013$\014\012\003 030\015\007\023\030A\003 003  002\003 031\005\011$\003 014\030\003 0073\007\012\016\015\007\033\003 031\014\015\013\014\026\003 024\003 024\010\014\012\014\005\016\030\010#\003 012\011\024\017\015\007\033!\003 012\016\030\015\005$\003 \007\011\026\007\010\003\014\026\030\014\033\007\003\024\003\025\016\007\030\015\003\014$\024\025\007,\003 6 013\007\003\031\005\011$\003\024\026\024\010#\030\007\030\003\014\015\030\003\007\026\006\014\011\005\026$\007\026\015\003\024\026\033\003\033\007\015\007\011$\014\026\007\030\003\014\015\003 014\030\003\031\014\015\013\014\026\003\024\003C\007\026\003\013#\023\007\011\006\014\030\005\011,\003  002\003\010\014-\011\024\011#\003\014\030\003;\016\007\011\014\007\033\003\024\026\033\003\025\014\006\007\030\003\015\013\007\003\031\005\011$\003\024\003\026\016$-\007\011\003\005\017\003 015\007\030\015\030\003\015\005\003\012\005\026\033\016\012\015!\003\015\013\007\003\031\005\011$\003\033\007\015\007\011$\014\026\007\030\003\015\013\024\015\003"F\002\020+\003\014\030\003 033\014\030\024-\010\007\033\003\005\026\003\015\013\007\003\013#\023\007\011\006\014\030\005\011,\003  013\007\003 031\005\011$\003 0073\007\012\016\015\007\030\003 024\026\003 024\015\015\024\012 \003 006\014\024\003 024\026\003 0073\023\010\005\014\015\024\015\014\005\026\003 005\017\003 015\013\007\003"F\002\020+\003-\016\025\003\024\026\033\003\007\010\007\006\024\015\007\030\003\003\023\011\014\006\014\010\007\025\007\030\003\015\005\003\015\013\007\003\033\005$\\003  013\007\003 031\005\011$\003 0073\007\012\016\015\007\030\003 024\003 030\007\026\030\005\011\003 016\026\033\007\011\003 015\013\007\030\007\003 026\007\031\003 023\011\014\006\014\010\007\025\007\030!\003 030\012\024\026\026\014\026 025 003 015\013\007\003 026\007\015\031\005\011 \003 017\005\011\003 010\014\006\007\003 014\025\011\024\015\014\005\026\030\003 005\012\012\016\011\011\014\026\025\003 007\015\031\007\007\026\003 013#\023\007\011\006\014\030\005\011\030,\003 004\005\003 010\014\006\007\003 014\025\011\024\015\014\005\026\030\003 005\012\012\016\011\003-\016\015\003\005\015\013\007\011\003\013#\023\007\011\006\014\030\005\011\030\003\024\011\007\003\014\033\007\026\015\014\017\014\007\033,\003 8 034\015\003 016\007\011\014\007\030\003 015\013\007\003 010\014-\011\024\011#\003 024\026\033\003 023\011\005\033\016\012\007\030\003 024\003 005\020\003 024\026\033\003 024\003 014\025\011\024\015\014\005\026\003\014\026\015\007\011\012\007\023\015\014\005\026\003\017\005\011\003\015\013\007\003\013#\023\007\011\006\014\030\005\011,\003 B 002\003 005\020\003 014\030\003 0073\007\012\016\015\007\033\003 005\026\003 024\003 013#\023\007\011\006\014\030\005\011!\003 012\024\016\030\014\026\025\003 024\003 010\014\006\007\003 014\025\011\024\015\014\005\026\003\015\005\003\005\012\012\016\011,\003  013\007\003 031\005\011$\003 014$\023\010\024\026\015\030\003 014\015\030\007\010\017\003 014\026\015\005\003 015\013\007\003 007$\005\011#\003 005\017\003 015\013\007\003 014\025\011\024\015\007\033\003$\024\012\013\014\026\007!\003\030\016\012\012\007\030\030\017\016\010\010#\003\023\011\005\023\024\025\024\015\014\026\025,\003 003 034\034 032 1\004\032F%\020\0341\004\003 K 003  0&=\0350\003  10 003 013\007\003 017\014\007\010\033\003 005\017\003 012\005$\023\016\015\014\026\025\003 014\030\003 030\007\015\003 015\005\003 012\013\024\026\025\007\003 006\014\024\003 015\013\007\003 007$\007\011\025\014\026\025\003 023\024\011\024\033\014\025$\003 005\017\003 015\013\007\003 032\010\005\016\033,\003 034\026\003 015\013\014\030\003 023\024\023\007\011!\003 005\026\007\003 005\017\003 015\013\007\003 005\030\015\003 023\011\005$\014\026\007\026\015\003\015\005\023\014\012\030\003\005\017\003\032\010\005\016\033\003\012\005$\023\016\015\014\026\025!\003\015\013\024\015\003\005\017\003\014\015\030\003\030\007\012\016\011\014\015#\003\031\024\030\003 024\033\033\011\007\030\030\007\033,\003\003\002\026\003\024\030\030\007\030\030$\007\026\015\003\005\017\003\014\015\030\003\016\026\033\007\011\010#\014\026\025\003\024\011\012\013\014\015\007\012\015\016\011\007\030\003\024\026\033\003 015\007\012\013\026\005\010\005\025\014\007\030\003\031\024\030\003\012\005\026\033\016\012\015\007\033\003\014\026\003\005\011\033\007\011\003\015\005\003\033\007\015\007\011$\014\026\007\003\015\013\007\003\015\013\011\007\024\015\003\005\017\003 024\010\031\024\011\007!\003\024\030\003\014\015\003\031\014\010\010\003\014\026\007\006\014\015\024-\010#\003\007\006\005\010\006\007\003\015\005\003-\007\012\005$\007\003$\005\011\007\003\012\005$\023\010\0073!\003 024\010\005\026\025\030\014\033\007\003\014\015\030\003\007\026\006\014\011\005\026$\007\026\015,\003&\013\007\003\030\007\012\016\011\014\015#\003\005\017\003\015\013\007\003\032\010\005\016\033\003\014\030\003\024\003\015\005\023\014\012\003 005\017\003$\016\012\013\003\033\007-\024\015\007!\003-\016\015\003\014\015\003\031\024\030\003\030\013\005\031\026\003\015\013\024\015\003$\024\026#\003\015\013\011\007\024\015\030\003\0073\014\030\015\003\024\026\033\003 005\011\007\003 011\007\010\007\006\024\026\015\010#!\003 015\013\024\015\003 024\003 026\016$-\007\011\003 005\017\003 024\015\015\024\012 \003 006\007\012\015\005\011\030\003 031\014\015\013\014\026\003 015\013\007\003 032\010\005\016\033\030\003 016\026\033\007\011\010#\014\026\025!\003 006\014\011\015\016\024\010\0142\007\033\003 014\026\017\011\024\030\015\011\016\012\015\016\011\007\003 024#\003 012\011\007\024\015\007\003 024\003 030\016\014\015\024-\010\007\003 007\026\006\014\011\005\026$\007\026\015\003 017\005\011!\003 024\030\003 007\015\003 016\026 \026\005\031\026\003 024\010\014\012\014\005\016\030\003 030\005\017\015\031\024\011\007\003 015\005\003 023\011\005\023\024\025\024\015\007,\003 007\017\007\026\030\007\030\003 033\005\003 0073\014\030\015\003 017\005\011\003 015\011\024\033\014\015\014\005\026\024\010\003 024\010\014\012\014\005\016\030\003 030\005\017\015\031\024\011\007!\003 016\015\003 015\013\007\003 007\017\017\007\012\015\014\006\007\026\007\030\030\003 005\017\003 015\013\007\030\007\003 033\007\017\007\026\030\007\030\003 014\030\003 026\007\006\007\011\003 025\016\024\011\024\026\015\007\007\033!\003 023\024\011\015\014\012\016\010\024\011\010#\003 031\013\007\026\003 012\005\026\030\014\033\007\011\014\026\025\003 015\013\007\003 007\017\017\005\011\015\003 024\026\033\003 017\014\026\024\026\012\014\024\010\003\030\016\023\023\005\011\015\003\015\013\024\015\003$\024\010\014\012\014\005\016\030\003\030\005\017\015\031\024\011\007\003$\024#\003\011\007\012\007\014\006\007\003\014\026\003\005\011\033\007\011\003 015\005\003\024\012\012\005$\023\010\014\030\013\003\014\015\030\003\025\005\024\010,\003&\013\007\011\007\017\005\011\007!\003\014\015\003\030\013\005\016\010\033\003\026\005\015\003-\007\003\012\005\026\030\014\033\007\011\007\033\003 012\005\011\011\007\012\015\003\015\005\003\024\030\030\016$\007\003\015\013\007\003\015\013\011\007\024\015\003\026\005\003\010\005\026\025\007\011\003\0073\014\030\015\030,\003 003 003\003 014\025,\003\003>\003\032\010\005\016\033\003\031\005\011$\003\024\011\012\013\014\015\007\012\015\016\011\007 014\025,\003?\003/\005\011$\003\002\010\025\005\011\014\015\013$\003 
002 002 002 002 002 002 
002 002 002 002 002 002 002 002 002 002 002 002 002 002 002 
66 
66 


006\005\010,\003??!\003\023\023,\003E'\021E>!\0036 5>7\003\003 035,\003\032\005\005 \007!\003",\003N\024\013\024\026\014\024\026\003\024\026\033\003*,\003\027\012\022\013\007\011\030\005\026!\003O&\013\007\0032\005$-\014\007\003 011\005\016\026\033\016\023A\003%\026\033\007\011\030\015\024\026\033\014\026\025!\003\033\007\015\007\012\015\014\026\025!\003\024\026\033\003\033\014\030\011\016\023\015\014\026\025\003-\005\015\026\007\015\030!O\003 014\026\003 6\\003 5?7\003\003 032,\003"\005\030\026\005\012 !\003O\032\005$\023\016\015\007\011\003\031\005\011$\030A\003\023\024\030\015!\003\023\011\007\030\007\026\015!\003\024\026\033\003 017\016\015\016\011\007!O\003 006\005\010,\003\(!\0036 5@7\003 003\037,\003D\011\005-\024\016\007\011!\003&,\003/\024\010\010\005\030\012\013\007 \003\024\026\033\003\035,\003\020\015\005\012 \007\011!\003 O%\026\033\007\011\030\015\024\026\033\014\026\025\003\012\010\005\016\033\003\012\005$\023\016\015\014\026\025\003\006\016\010\026\007\011\024-\014\010\014\015\014\007\030!O\003 006\005\010,\003E!\003\023\023,\003@\\021@B!\0036 587\003\003 003=\024\011\011\014\030\005\026!\003\037,\003\037\005\011\033-\024\011!\003\020,\003&,\003\002\010\014!\003\032,\003\034,\003*\024\010\015\005\026\003\024\026\033\003\002,\003 004\005\011$\024\026!\003O\002\003\017\011\024$\007\031\005\011 \003\017\005\011\003\033\007\015\007\012\015\014\026\025\003$\024\010\031\024\011\007\003\014\026\003\012\010\005\016\033\003-#\003\014\033\007\026\015\014\017#\014\026\025\003 030#$\023\015\005$\030!O\003\014\026\003 6\\021'B6,\003 5B7\003\003 003=\024\030\013\0142\016$\007!\003*,\003D,\0030\005\030\024\033\005\003\024\026\033\003"\007\011\026PQ\024\026\033\0072\021\027\007\033\014\026\024!\003 035\033\016\024\011\033\005\003\024\026\033\003"\007\011\026\024\026\033\0072!\035\033\016\024\011\033\005\003\037,!\003O\002\026\003\024\026\024\010#\030\014\030\003\005\017\003\030\007\012\016\011\014\015#\003\014\030\030\016\007\030\003 017\005\011\003\012\010\005\016\033\003\012\005$\023\016\015\014\026\025!O\003 006\005\010,\003?!\003\023\023,\003'\021'>!\0036 5\(7\003\003 030\014\007\026\021*\007\003=\016\024\026\025!\003\032\013\024\026\025\021\020\013\014\026\025\003F\007\007!\003=\016\026\025\021R\016\003+\024\005\003\024\026\033\003N,\003 R\014\021F\024\026\025\003&\030\024\014\003\024\026\033\003\032\013\024\026\025!\003O\027\024\010\031\024\011\007\003-\007\013\024\006\014\005\011\024\010\003\024\026\024\010#\030\014\030\003\030#\030\015\007$A\003 027\002\004!O\003\014\026\003 6\\021\(,\003 5E7\003\003 R,\003N\024\033\007\036\024\003\024\026\033\003+,\003\027\005\033\014!\003O\032\010\005\016\033\003\012\005$\023\016\015\014\026\025\021\012\005\026\012\007\023\015\030!\003 024\011\012\013\014\015\007\012\015\016\011\007\003\024\026\033\003\012\013\024\010\010\007\026\025\007\030!O\003\014\026\003 BB\021\(\(\\003 5'\\003\003 003N\024\007\025\007\011!\003+,\003+\011\007\026\0152!\003\027,\0030\014\012\013\010#!\003\032,\003/\014\010\010\007$\030!\003/,\003*\024\031\005\016\033\003 024\026\033\003\034,\003&\024 \005\016\026\024!\003OC\007\026\003\035\023\014\030\005\033\007\003\034<A\003&\013\007\003D\016\007\030\015\030\003\030\015\014\010\010\003\020\015\011\014 \007\003\037\024\012 !O\003 6\'',\003 5''7\003\003 020,\003+\024\011\026\005\016\030 \005\030!\003O\020\015\0163\026\007\015\003\031\005\011$\003\014$\023\024\012\015\003\005\026\003\014\026\033\016\030\015\011\014\024\010\003\012#-\007\011\021 023\013#\030\014\012\024\010\003\030#\030\015\007$\003\030\007\012\016\011\014\015#!O\003\014\026\003 6\\021??E?,\003 5'67\003 003\035,\003+\007\010\010\007\011!\003N,\003\0202\007\017\007\011!\003N,\0030\0073\017\005\011\033\003\024\026\033\0030,\003\037,\003F\007\007!\003O\004\005=#\023\007A\003 014\011\015\016\024\010\0142\007\033\003\012\010\005\016\033\003\014\026\017\011\024\030\015\011\016\012\015\016\011\007\003\031\014\015\013\005\016\015\003\015\013\007\003\006\014\011\015\016\024\010\0142\024\015\014\005\026!O\003\014\026\003 6\\003\023\023,\003>@\>8',\003 5'>7\003\003 003+\005\011\015\012\013\014\026\030 #!\003O=\024\012 \014\026\025\003>*\0039\024\026\033\003-\011\007\024 \014\026\025\003\005\016\015\003\005\017\003 027/\024\011\007:!O\003 6\\003 5'?7\003\003 0,\003F\024\026\025\026\007\011!\003O\020\015\0163\026\007\015A\003*\014\030\030\007\012\015\014\026\025\003\024\003\032#-\007\011\031\024\011\017\024\011\007\003 007\024\023\005\026!O\003 006\005\010,\003E!\003\023\023,\003?E\021@'!\0036 5'@7\003\003 C,\003F\014!\003R,\003/\007\026!\003\027,\003=\016\024\026\025\003\024\026\033\003S,\003F\014\016!\003O\002\026\003\005\006\007\011\006\014\007\031\003\005\017\003 005\005\015 \014\015\003\024\015\015\024\012 \014\026\025\003\024\023\023\011\005\024\012\013\007\030!O\003\014\026\003 6 023\023,\003?6\(\021?>',\003 5'87\003\003 003F\014\007-\007\011$\024\026\003\024\026\033\003\032,\003"\011#!\003O/\014\010\010\003\030\005\017\015\031\024\011\007\003\007\006\007\011\003 031\005\011 TO\003 006\005\010,\003??!\003\023\023,\003'66\021'6?!\0036\\003 5'B7\003\003 027,\003\027#\007\011\030\003\024\026\033\003\020,\003R\005\016\026\033\015!\003O\002\026\003\014\026\015\011\005\033\016\012\015\014\005\026\003\015\005\003\013\024\011\033\031\024\011\007\021 024\030\030\014\030\015\007\033\003\006\014\011\015\016\024\010\003$\024\012\013\014\026\007\0039\013\006$:\003\011\005\005\015 \014\015\030!O\003 6 5'\(7\003\003 N,\0031-\007\011\013\007\014\033\007!\003\035,\003\032\005\005 \007\003\024\026\033\003",\003N\024\013\024\026\014\024\026!\003O\035$\023\014\011\014\012\024\010\003 0073\023\010\005\014\015\024\015\014\005\026\003\005\017\003\010\014\006\007\003\006\014\011\015\016\024\010\003$\024\012\013\014\026\007\003$\014\025\011\024\015\014\005\026!O\003\014\026\003 003,\003 5'E7\003\003 003\022\007\011\0072\021\037\005\015\007\011\005!\003N,\003\0202\007\017\007\011\003\024\026\033\0030,\003\037,\003F\007\007!\003O\032\013\024\011\024\012\015\007\011\0142\014\026\025\003 013#\023\007\011\006\014\030\005\011\003\006\016\010\026\007\011\024-\014\010\014\015\014\007\030\003\014\026\003\012\010\005\016\033\003\012\005$\023\016\015\014\026\025\003\030\007\011\006\007\011\030!O\003 014\026\003 6\\021 56 003N,\0030\016\015 \005\031\030 \024!\003O\020\016-\006\007\011\015\014\026\025\003<\014\030\015\024&\027\003 \007\011\026\007\010\003\017\005\011\003\017\016\026\003\024\026\033\003 023\011\005\017\014\015!O\003 6\\003 56'7\003 003\035,\003\020 \005\016\033\014\030!\003 022\011\007\026\015\014\012\007\003 024\010\010\003\022\011\005\017\007\030\030\014\005\026\024\010!\0036 5667\003 003\020\015\024\026\014\017\005\011\033!\003\020\015\016\024\011\015\003\024\026\033\003\022\0243\030\005\026!\003<\007\011\026\003\024\026\033\003/\007\024\006\007\011!\003\004\014\012\013\005\010\024\030\003 024\026\033\003\005\015\013\007\011\030!\003O=\005\031\003\015\005\003\005\031\026\003\015\013\007\003\014\026\015\007\011\026\007\015\003\014\026\003#\005\016\011\003\030\023\024\011\007\003\015\014$\007,O\003\014\026\003 6\\021'8B,\003 56>7\003 003N,\003\0202\007\017\007\011!\003\035,\003+\007\010\010\007\011!\0030,\003\037,\003F\007\007\003\024\026\033\003N,\0030\0073\017\005\011\033!\003O\035\010\014$\014\026\024\015\014\026\025\003 015\013\007\003\013#\023\007\011\006\014\030\005\011\003\024\015\015\024\012 \003\030\016\011\017\024\012\007\003\017\005\011\003\024\003$\005\011\007\003\030\007\012\016\011\007\003\012\010\005\016\033!O\003 014\026\003 6\\021?'6,\003 56?7\003 003\020,\003&P\016\011\023\007!\003\002,\003\022\005\010\010\007\011!\003N,\003\020\015\007\017\017\024\026\003\024\026\033\003N,\003\024,\003&,\003\020\015\005\0152\003N\024\026!\003 O\002\015\015\024\012 \014\026\025\003\015\013\007\003\037\014\015F\005\012 \007\011\003-\005\005\015\003\023\011\005\012\007\030\030!O\003\014\026\003 13\017\005\011\033!\003%+!\0036 023\023,\003'\(>\021'E8,\003 56@7\003 003\027,\003/\024\015\030\005\026!\003O\027\024\010\031\024\011\007\003*\007\015\007\012\015\014\005\026\003\014\026\003\015\013\007\003\032\005\026\015\0073\015\003\005\017\003\032\010\005\016\033\003 032\005$\023\016\015\014\026\025!O\0036\\003 5687\0030,\003/\005\036\015\0122\016 !\003O\020\016-\006\007\011\015\014\026\025\003\015\013\007\003C\007\026\003\013#\023\007\011\006\014\030\005\011!O\003 003 56B7\003 003F,\003/\016\003\024\026\033\003R,\003U\013\024\026\025!\003O\002\016\015\005$\024\015\014\012\003\033\007\015\007\012\015\014\005\026\003$\005\033\007\010\003\005\017\003 024\010\031\024\011\007\003\030\014\025\026\024\015\016\011\007\003\017\005\011\003\024\026\015\014\021\006\014\011\016\030\003\012\010\005\016\033\003\012\005$\023\016\015\014\026\025!O\003\014\026\003 6\!\003\023\023,\003B>\021B@,\003 56\(7\003\003 032,\003\032,\003U\005\016!\003/,\003D\005\026\025\003\024\026\033\003*,\003&\005\031\030\010\007#!\003O\032\005\033\007\003\011\007\033\003\031\005\011$\003 023\011\005\023\024\025\024\015\014\005\026\003$\005\033\007\010\014\026\025\003\024\026\033\003\024\026\024\010#\030\014\030!O\003\014\026\003 6\!\003\023\023,\003 021'?B,\003 56E7\003\003 003\027\024\015\013\007\011!\003\020,\003+\016$\024\011\024\030\031\024$#\003\024\026\033\003\020,\003F\024\015\014\017!\003 1Q0\007\014\010\010#!\0036\\\003 5>\\003\003 003D,\0030\005\030\024\033\005!\0030,\003DV$\0072!\003*,\003\027\007\010\010\024\033\005\003\024\026\033\003\035,\003"\007\011\026W\026\033\0072\021 027\007\033\014\026\024!\003O\020\007\012\016\011\014\015#\003\024\026\024\010#\030\014\030\003\014\026\003\015\013\007\003$\014\025\011\024\015\014\005\026\003\015\005\003\012\010\005\016\033\003 007\026\006\014\011\005\026$\007\026\015\030!O\003 006\005\010,\003?!\003\023\023,\003?8E\021?\(B!\0036 003 003 003 003 003 003 003 003 003 
024\025\004.\017\016\003\006$\007 006\025\031\003\003\023\015\026+\030\007\025&\007\016\032\003\0070\(\011\036\0101\007\(!0\037\010\0072\025\006#\030\032\025.$\007 011\020\030\016\007\024\020\006\025\021\015\026\020\0070\026\015\027\003\006\030\015\016*$\007 003\031\017\006\015\016*\0073,\007 006\015\027\020\031*$\007\010\011\011\011$\007 011\026\016\003\006.\006\015\030\003\007\4\005 \003\031\016\007\024\025\004.\017\016\015\026+\007\024\025\026&\003\006\003\026\031\003\007 5\011\\024'$\0076\03376\007\010\011\011\011\00778\016\032\007\010\026\016\003\006\026\020\016\015\025\026\020\021$\007 9\025\017\006\026\020\021\007\025&\007\010\026\016\003\006\026\003\016\007\(\003\006\027\015\031\003\030\007\020\026\023\007 012..\021\015\031\020\016\015\025\026\030$\007 010\026\016\003\021\021\015+\003\026\016\007\012+\003\026\016\0075\010\012'$\0076\03377\007\010\011\011\011\007\(*\004.\025\030\015\017\004\007 025\026$\007 024\025\004.\017\016\015\026+$\007\011\021\003\031\016\006\025\026\015\031\030\007\020\026\023\007 011\021\003\031\016\006\015\031\020\021\007\037\003\031\032\026\025\021\025+\015\003\030\0075\010\024\024\011\011\037'$\0076\03376\007\010\026\016\003\006\026\020\016\015\025\026\020\021\007\024\025\026&\003\006\003\026\031\003\007 025\026$\007 010\011\0244\036\0076\03377\007"\007:;\016\032\007\012\026\026\017\020\021\007\024\025\026&\003\006\003\026\031\003\007 025\026\007\010\011\011\011\007\010\026\023\017\030\016\006\015\020\021\007\011\021\003\031\016\006\025\026\015\031\030\007\(\025\031\015\003\016*$\007 012\024\002\007 010<\012!\024=\007\024\025\004.\017\016\003\006\007\012\006\031\032\015\016\003\031\016\017\006\003\007\036\003\035\030$\007 034\021\020\031#\032\020\016\0070\030\020$\007 003\031\017\006\015\016*\007-\006\015\027\020\031*$\007\010\011\011\011$\007 002\025\005\015\021\003\007\012\023"=\025\031\007\020\026\023\007\(\003\026\030\025\006\007 036\003\016\035\025\006#\030\0075\002\(\036'$\0076\03377\007\(\003\027\003\026\016\032\007\010\026\016\003\006\026\020\016\015\025\026\020\021\007\024\025\026&\003\006\003\026\031\003\007\025\026 024\025\004\004\017\026\007\012\024\002$\007 024\006\017\031\015\020\021\007 003\031\017\006\015\016*\013>\011\021\003#\016\006\025\026\026*\015\007!\003\030\017\006\030?@\007 0 016\016.@AA\035\003\026#\017\013\034\020\015\023\017\013\031\025\004A\027\015\003\035A8\023\020B&C666&8\033\023\023\031\031\023\020:D\020\033&:\013=\016\004\021  007 006\025\031\013\007\025&\007\034\021\020\031#=\020\016\007 027\003\026\016\015\025\026$\007 006\025\031\003\003\023\015\026+\030\007\025&\007\016\032\003\0076\0337:\007\010\026\016\003\006\026\020\016\015\025\026\020\021\0072\025\006#\030\032\025.\007\025\026\007\(\003\031\017\006\015\016*\007\015\026\007 024\021\025\017\023\007\024\025\004.\017\016\015\026+$\007 034\021\020\031#\007=\020\016\007\034\006\015\003&\015\026+\030$\007 002\020\021\035\020\006\003@\007E\015+\032\016\015\026+\007\002\020\021\015\031\015\025\017\030\007\024\025\023\003\013\007 0\(\011\036\0101\007 003\031\017\006\015\016*\007\(*\004.\025\030\015\017\004$\007 006\025\031\003\003\023\015\026+\030\007\025&\007\016\032\003\0077D\016\032\007\012\024\002\007\024\025\026&\003\006\003\026\031\003\007\025\026\007\024\025\004.\017\016\003\006\007\020\026\023\007 024\025\004\004\017\026\015\031\020\016\015\025\026\030\007\(\003\031\017\006\015\016*$\007 006\025\031\003\003\023\015\026+\030\007\025&\007\016\032\003\0076\026\023\007 010\026\016\003\006\026\020\016\015\025\026\020\021\007\024\025\026&\003\006\003\026\031\003\007\025\026\007\037\006\017\030\016\003\023\007\024\025\004.\017\016\015\026+$\007 034\021\020\031#\007=\020\016\007 0\(\012$\007 024\025\004.\017\016\003\006\007\020\026\023\007 010\026&\025\006\004\020\016\015\025\026\007\(\031\015\003\026\031\003\0075\010\024\010\('$\0076\03377\007\010\011\011\011A\012\024\010\(\0077\033\016\032\007\010\026\016\003\006\026\020\016\015\025\026\020\021\007 024\025\026&\003\006\003\026\031\003\007\025\026$\007 006\025\031\003\003\023\015\026+\030\007\025&\007\016\032\003\007C\016\032\007\012\024\002\007 024\025\026&\003\006\003\026\031\003\007\025\026\007\024\025\004.\017\016\003\006\007\020\026\023\007\024\025\004\004\017\026\015\031\020\016\015\025\026\030\007\(\003\031\017\006\015\016*$\007 024\021\025\017\023\007\(\003\031\017\006\015\016*\007 020\026\023\007-\006\015\027\020\031*@\007\012\026\007\011\026\016\003\006.\006\015\030\003\007-\003\006\030.\003\031\016\015\027\003\007\025\026\007!\015\030#\030\007\020\026\023\007 024\025\004.\021\015\020\026\031\003\013\007 E\017\016\017\006\003\007\010\026\016\003\006\026\003\016$\007 
034\026\003 005\011\033\007\011\003 015\005\003 024\023\023\011\005\023\011\014\024\015\007\010#\003 024\030\030\007\030\030\003 030\016\012\013\003 024\003 015\013\011\007\024\015!\003 024\003 023\011\005\005\017\003 005\017\003 012\005\026\012\007\023\015\003 017\011\024$\007\031\005\011 \003 031\024\030\003 033\007\006\007\010\005\023\007\033\003 014\026\003 005\011\033\007\011\003 015\005\003 016\026\033\007\011\030\015\024\026\033\003 005\011\007\003 024-\005\016\015\003 030\007\010\017\021\023\011\005\023\024\025\024\015\014\026\025\003 030\005\017\015\031\024\011\007\003 031\014\015\013\014\026\003 015\013\007\003 032\010\005\016\033\003 033\005$\024\014\026,\003&\013\007\026!\003\024\003\026\005\006\007\010\003\024\011\012\013\014\015\007\012\015\016\011\007\003\031\024\030\003\023\011\005\023\005\030\007\033!\003\015\013\024\015\003\024\033\024\023\015\007\033\003 015\013\007\003\005\011\014\025\014\026\024\010\003 023\011\005\005\017\003\005\017\003\012\005\026\012\007\023\015\003\014\026\003\005\011\033\007\011\003\015\005\003\024\033\033\011\007\030\030\003\015\013\007\003\012\005$\023\010\0073\003 012\013\024\010\010\007\026\025\007\030\003 014\026\013\007\011\007\026\015\003 031\014\015\013\014\026\003 015\013\007\003 032\010\005\016\033\003 014\026\003 005\011\033\007\011\003 015\005\003 023\011\005\006\014\033\007\003 024\003 024\011\012\013\014\015\007\012\015\016\011\007\003\017\005\011\003$\024\010\014\012\014\005\016\030\003\030\005\017\015\031\024\011\007\003\023\011\005\023\024\025\024\015\014\005\026,\003&\005\003\015\013\007\003-\007\030\015\003\005\017\003 015\013\007\003 024\016\015\013\005\011G\030\003 026\005\031\010\007\033\025\007\003 015\013\014\030\003 031\005\011 \003 014\030\003 016\026\023\011\007\012\007\033\007\026\015\007\033\003 024\026\033\003 030\016\012\013\003 030\005\017\015\031\024\011\007\003\014\030\003#\007\015\003\015\005\003-\007\003\030\007\007\026\003\014\026\003\015\013\007\003\031\014\010\033,\003 013\007\011\007\017\005\011\007!\003 015\013\014\030\003 031\005\011 \003 031\014\010\010\003 010\024#\003 015\013\007\003 025\011\005\016\026\033\031\005\011 \003 017\005\011\003 017\016\015\016\011\007\003 011\007\030\007\024\011\012\013\003 014\026\015\005\003 015\013\007\003 024\011\007\024\003 005\017\003 030\007\010\017\021\023\011\005\023\024\025\024\015\014\026\025\003 030\005\017\015\031\024\011\007\003 031\014\015\013\014\026\003 032\010\005\016\033\003\024\011\012\013\014\015\007\012\015\016\011\007\030,\003"\016\015\016\011\007\003\031\005\011 \003\031\014\010\010\003\012\005\026\030\014\030\015\003\005\017\003\015\007\030\015\014\026\025\003\024\003\030$\024\010\010\003 030\012\024\010\007\003 014$\023\010\007$\007\026\015\024\015\014\005\026\003 005\017\003 015\013\007\003 032\010\005\016\033\003 024\030\007\033\003 024\010\031\024\011\007\003 024\026\033\003 015\013\007\026\003 017\016\011\015\013\007\011\003\007\006\024\010\016\024\015\014\026\025\003\014\015\030\003\007\017\017\014\012\024\012#\003\005\017\003$\024\010\014\012\014\005\016\030\003\023\016\011\023\005\030\007\030!\003$\007\015\013\005\033\030\003 005\017\003\033\007\017\007\026\030\007\003\024\026\033\003\024\026#\003\024\010\015\007\011\026\024\015\014\006\007\003\016\030\007\030,\003\003 003 0\007\017\007\011\007\026\012\007\030\003 5'7\003 003N,\003\002\011\012\013\007\011!\003*,\003\032\016\010\010\014\026\024\026\007!\003\004,\003\022\016\013\010$\024\026\026!\003\002,\003\037\005\007\013$\007!\003\022,\003 016\011\0152\003\024\026\033\003+,\0030\007\024\006\014\030!\003O&\005\023\003\015\013\011\007\024\015\030\003\015\005\003\012\010\005\016\033\003\012\005$\023\016\015\014\026\025!O\003\032\010\005\016\033\003 020\007\012\016\011\014\015#\003\002\010\010\014\024\026\012\007!\0036 567\003\003 003\027,\003\032\013\007\026\003\024\026\033\003\020,\003\002-\016\021\004\014$\007\013!\003OF\007\030\030\005\026\030\003\017\011\005$\003 020\015\0163\026\007\015!O\003 
67 
67 


state of innovation stakeholder  node PQ It  s a balanced node Based on this, we could calculate the  node PQ Calculation process is: set different inn ovation stakeholders state i U  and j U Value of ij 000T can be get from  innovation time difference. Innovation stakeholders’ social effect and industrial effect can be obtained upon ij B  and ij G set according to relation between innovation stakeholders  Model 4.1 points out  that the value of Gij  directly affects social benefits and sector benefits. Large Gij  can lead to increasing benefits of the entire industry and the entire social growth Bij reflects big organization’s impact on businesses. Only strengthening the inter agent association within big organization and enhancing the str ategic partnership between enterprises can jointly promote the development of the entire industry, and bring more social benefits, so that each agent can be improved   5 Summary This paper puts forward the concept of the big organization based on the CSM t heory. It introduces the basic implication of the big organization and theoretical framework of the big organization including: the big organization's perspective  overall perspective, dynamic perspective, and new resource perspective; the big organizat ion’s sense  the purpose of the organizational structure is innovation, organizational activities around the flow of information, breaking the traditional organizational structure, encouraging self run structure, and blurring organizational boundaries; the big organization’s platform  the platform ecosystem of the big organization ; the big organization’s operation mode  borderless learning mode, and cluster effect; the big organization’s theory  active management theory  leading consumers, and culture  entropy reduction theory  negative culture entropy and humanistic ecology theory  inspiring humanity, and circuit theory  a virtuous circle, and collaborative innovation theory  collaborative innovation stakeholder. This paper also discusses culture entropy reduction theory of the big organization  negative culture entropy, and coordinated innovation theory  innovation stakeholders collaboration. Culture entropy change model and collaborative in novation model are constructed   The research has just begun for the big organization. It also needs further improvement but remains the trend of the times   Reference  1  Gordon Pellegrinetti, Joseph Bentsman. Nonlinear Control Oriented Boiler Modeling A Benchmark Problem for Controller De sign [J  I E E E tr a n s a c tio n s o n c o n tr o l s y s te m s te c h n o lo g y 2 0 1 0  4 1\57 65  2  Klaus Kruger, Rudiger Franke, Manfred Rode Optimization of boiler start up using a nonlinear 457 


boiler model and hard constraints [J  E n e r gy 201 1 29   22 39 2251  3  K.L.Lo, Y.Rathamarit  State estimation of a boiler model using the unscented Kalman filter [J  I E T  Gener. Transm. Distrib.2008 2 6\917 931  4  Un Chul Moon, Kwang. Y.Lee. Step resonse model development for dynamic matrix control of a drum type boiler turbine system [J IE E E  T ra nsactions on Energy Conversion.2009 24 2\:423 431  5  Hacene Habbi, Mimoun Zelmat, Belkacem Ould Bouamama. A dynamic fuzzy model for a drum boiler turbine system [J  A u to m a tic a 2 0 0 9 39:1213 1219  6  Beaudreau B C. Identity, entropy and culture J   J o ur na l  o f  economic psychology, 2006, 27\(2 205 223  7  YANG M, CHEN L. Information Technique and the Entropy of Culture J  A cad e m i c E x ch a n g e  2006, 7: 048  8  ZHANG Zhi feng. Research on entropy change model for enterprise system based on dissipative structure J  Ind ustrial  Engineering and  Management 2007, 12\(1\ :15 19  9  LI Zhi qiang, LIU Chun mei Research on the Entropy Change Model for Entrepreneurs' Creative Behavior System Based on Dissipative Structure J  C h i n a S of t S c i e n c e  2009   8  1 62 166   458 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of “Daily” Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data – Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average “daily“ operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rolling… I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators – Data Element Methods – Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Today’s cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlight’s data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlight’s hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlight’s method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





