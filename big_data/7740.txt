AN IMPROVED ALGORITHM FOR MINING FREQUENT WEIGHTED ITEMSETS  Abstract  Mining frequent weighted itemsets \(FWIs\ from weighted items transaction databases \(WITDs\ has taken the interest of many researchers and there have been several works related to mining FWIs in recent years. Beside, in real world applications, sparse weighted items transaction databases SWITDs\e very popular. For example, in the super market there are many items, but in the transaction there is only a small number of items. This paper proposes an interval word segment IWS\ucture to store and process tidsets for enhancing effectiveness of mining FWIs from SWITDs. With this structure intersection operations of tidsets between two itemsets are performed blazingly fast. Experimental results obtained on a 
number of spare databases show that IWS outperforms the existing methods Keywords. Dynamic bit vector, Frequent weighted itemset, Interval-word-segment, Multi-bit segment, Tidset I  INTRODUCTION Association rule mining \(ARM\ [1-5 l a y s a n i m p o r t a nt  role in data mining. ARM is used to identify relationships among items in transaction databases. Therefore, to mine association rules, it is necessary to mine frequent itemsets FIs\ the first step. Then association rules will be generated from these FIs. Thus, mining FIs is interesting and has taken much attention in recent years [7T h ey are div i ded into  three main groups 1  Candidate generation methods: These methods use a levelwise approach for mining FIs which scan databases many 
times. First, they generate frequent 1-itemsets then used them to generate candidate 2-itemsets, and so on until no more candidates can be generated. Apriori s an ex e m pl e  2  Methods that adopt a divide-and-conquer strategy: These methods compress the database into a tree structure and mine FIs from this tree by using a divide-and-conquer strategy. FPGrowth d FP G ro w t h  3 are ex em plar al g o rithm s   3  Methods that use a hybrid approach: These methods use vertical data format to compress the database and mine frequent itemsets by using a divide-and-conquer strategy   dex B itTableFI [6 d DBVF I 7 are  some examples The first approach has fairly large processing time because of scanning database many times. The runtime of the second 
approach requires much time to travel of FP-tree to build FIs The third approach is the Eclat algorithm [9 a sed o n IT t ree  it needs to scan databases once for building tidsets of items Therefore  the runtime is sharply reduced and this is the good method. However, the main disadvantage of Eclat algorithm is the usage of much memory to store tidsets resulting in certain difficulties in calculating the intersection of tidsets between two itemsets, particularly for a big database with the number of transactions up to millions. Some reseachers have proposed solutions to improve Eclat. Zaki & Gouda in 2005 proposed dEclat algorithm w h ic h us es t h e Di f f s ets co n cept ins t e ad  of tidset. Diffset only keeps track of differences in the tids of a candidate pattern from its generating frequent patterns. Dong  os ed th e us ag e of  BitTable to store tidsets. Vo et  p r op os ed th e u s e of dy n a m i c bit v ector \(DB V  w h ic h is  
a significant improvement for BitTable Diffse able 6 an d DBV [7] pe rf orm  w e ll on dense databases such as telecommunications or census data However, sparse databases with a small number of items on each transaction such as market-basket data or bills of drug in the hospital, the performance of these methods decreases incredibly Mining FIs are typically mined from binary databases where each item in a transaction may have a different significance. However, WITDs commonly used in real-world applications, have attributes weight \(profit or benefits\f each item in a transaction. For example, an order of goods in a supermarket, each goods has its profits, etc. Thus, mining 
FWIs and weighted association rule mining \(WARM\rom WITDs has high practical applications and attracted much research attentions recently [7In this paper, we propose an improved method for using DBV, called IWS, for fast mining FWIs from SWITDs. Some concepts, definitions, theorems and corollary are proposed Based on them, we present a novel method for storing and processing tidset of itemset for fast mining FWIs using IWSTree. This method shows the effective for mining FWIs from SWITDs through experiments The rest of this paper is organized as follows. Section 2 presents background and reviews some related works. Section 3 presents the data structure of IWS. Some concepts definitions and the algorithm for computing the intersection between two IWSs are also presented in this Section. The 
usage of IWS in the mining FWIs from a weighted transaction database is presented in Section 4. Section 5 shows the results of applying IWS to some experimental databases. Section 6 gives the conclusions and suggestions for future work II  BACKGROUND AND RELATED WORK  A  Mining FWIs from WITDs A weighted items transaction database D s composed of tuples T  I  W where T    t 1  t 2 205 t m is a set of Nguyen Duy Ham Department of Math & Informatics  University of People\222s Security  Hochiminh City, Vietnam 
duyham@gmail.com  Bay Vo  Faculty of Information Technology Hochiminh City University of Technology Hochiminh City, Vietnam bayvodinh@gmail.com  Nguyen Thi Hong Minh School of Graduate Studies Ha Noi National University Hanoi, Vietnam minhnth@gmail.com  Tzung-Pei Hong Department of Computer Science and Information Engineering National University of Kaohsiung Kaohsiung, Taiwan, ROC tphong@nuk.edu.tw  
2015 IEEE International Conference on Systems, Man, and Cybernetics 978-1-4799-8697-2/15 $31.00 © 2015 IEEE DOI 10.1109/SMC.2015.451 2579 
2015 IEEE International Conference on Systems, Man, and Cybernetics 978-1-4799-8697-2/15 $31.00 © 2015 IEEE DOI 10.1109/SMC.2015.451 2579 


 transactions I    i 1  i 2 205 i n is a set of items and W  w 1  w 2 205 w n is a set of weights  that correspond to the items in set I     Example 1 Tables 1 \(A\ show a WITD. The set of I    A  B  C  D  E and there are a total of six transactions. The set of weights W 0.6, 0.1, 0.3, 0.9, 0.2}, as shown in Table 1 \(B Ramkumar et al. [10 iden t i f i ed t w o us efu l  w e i g h t ed  which are transaction weighted tw d weighted support  ws ectively shown as follows  tw  t k    1 where  tw  t k he transaction weighted of a transaction t k   w j is the weight of item i j   t k is the number of of items appearing in transaction t k  The weighted support ws f an itemset X is defined as follow An itemset X is a frequent weighted itemset if and only if ws  X   minws where minws is the given threshold which identified by users. The problem of mining FWIs from a WITDs is to find the set of all itemsets X such that X 
002  I and ws  X   minws Note that the downward closure property, that means if X 003  Y then ws  X   ws  Y also used this problem Mining FWIs from WITDs is a problem interested by many rersearches because WITDs is a form of database which have many real-world applications Ramkumar et al p o s e d t w o m easu re m e n t s of transactions weighted tw d weight support ws sed in mining FWIs Tao et al. [1 e d on t w o m easu re tw and ws proposed a framework to solve this problem. However, this approach based on the Apriori algorithm, thus need scan database many times lead to consume more time  prop os ed a s t ru ctu r e n a m e l y W I T tree, th i s is  an extension of IT-Tree Th e bas e d on W I T tree, th e au t h or  proposed an algorithm for fast mining FWIs from WITDs with scan database one time Based on WIT-tree and strategy diffse e et al. [8 proposed a the formula for fast computing ws of child item from ws of parent item on WIT-tree. Beside this approach saved memory to store tidset more than previous methods B  Some improvement for storing the tidset of itemset Zaki & Goud pro pos ed an ef f i c i en t  m e t h od t o  replace tidsets by diffset which only keeps track of differences between two tids of a candidate pattern from its generating frequent patterns. Diffsets drastically cut down \(by orders of magnitude\e size of memory required storing intermediate results. However, diffset is only effective with dense databases, but not on sparse databases. The reason is that diffset takes the offset on tidset while the number of items on transactions \(the number of elements on tidset\is less than the total number of transactions in the database  Dong os ed th e us e of B itT able  to store tidsets Tidset of items in Table 1 by BitTable  as description in Table  2 Item A appear in transactions ID 1,3,4,5}, thus BitTable of item A is  10111000 2  2 7 2 5 2 4 2 3 182 in decimal system\n which the bit position {1,3,4,5} from left to right have value 1, else have value 0. Similarly with B  C  D  E  The calculation of intersection between two tidsets is implemented by using AND bitwise operation with two corresponding bytes on two BitTable of tidsets. This is an advantage of BitTable  because bitwise operator is fast BitTable  with 16 bytes is presented in Table 3 Table 3 An example of BitTable Byte index 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Byte value 0 0 0 0 6 9 0 0 0 0 0 1 5 1 0 0 In this BitTable, there are 11 bytes which its value equal 0 0-byte\pecially on sparse databases the number of 0-byte is very large. Therefore, its potentially waste large amount of memory and lead to consume more time. This is a disadvantage of BitTable Table 4 DBV from BitTable in Table 4 Byte inde x 5 6 7 8 9 10 11 12 13 14 Byte value 6 9 0 0 0 0 0 1 5 1 DBV 5\(6, 9, 0, 0, 0, 0, 0, 1, 5, 1 Vo et al in trodu ced th e concept of DBV, which is a solution improving significantly BitTable  in performing time and memory. The authors proposed to remove all 0-bytes at the start and end of each bytelist \(no transaction is recorded in 0-bytes Using DBV structure, the bytelist of items is compacted considerably. However, DBV just only cuts 0-bytes at both the start and end of each tidset, while 0-bytes at the \223middle\224 of each bytelist are not removed. For example DBV in Table 4 there are 5 0-bytes. The disadvantage is that it consumes more memory for storing 0-byte leading to spending more time for determining intersection among two tidsets III  REPRESENTATION OF INTERVAL-WORD SEGMENT A. Structure of IWS In this paper, each IWS describes several segment of Words \(2 bytes\ferent 0-word arranging continuously on one word vector. Each segment includes two components i  Start: start index of the segment Table 1 An example of a WITD A  B ID Item  Item Weigh t  1 A  B  D  E  A 0.6  2 B  C  E  B 0.1  3 A  B  D  E  C 0.3  4 A  B  C  E  D 0.9  5 A  B  C  D  E  E 0.2  6 B  C  D  ws  X       2 Table 2 Tidset represented by BitTable Item  Tidset by BitTable Value in Decimal system A 10111000 2 2 7 2 5 2 4 2 3   182 B 11111100 2 2 7 2 6 2 5 2 4 2 3 2 2   252 C 01011100 2 2 6 2 4 2 3 2 2   92 D 10101100 2 2 7 2 5 2 3 2 2   172 E 11111000 2 2 7 2 6 2 5 2 4 2 3   248 
2580 
2580 


 ii  Word Vector Segment: A range of Words different 0word, from the start to the end in which the start is Start, the end is Start + |Word Vector Segment Example 2 An IWS improved the BitTable \(Table  3\ and DBV \(Table 4\ as shown in Table  5 In Table  5, IWS only needs 6 bytes for storing word vector while BitTable  needs 16 bytes \(Table  3\, DBV needs 10 bytes Table  4\Thus, The IWS reduced the size of memory required to store tidset of itemset Definition 1   IWS of one itemset X is a set of segments of continuous word different from 0 and described as follows IWS  X  s 1   205  s 2   205  205 s k   205   where e i   s i  
005 1 k    is the value of word in the segments Example 3 Data shown in Table  5  is an example of IWS with 2 segments 3\(1545  6\(1, 1281  Consequence 1  if k 1 IWS  X  DBV  X  Definition 2  Segment  is called a subset of segment  if it satisfies three conditions 1   2 k  t  3       205    Example 4 Segment S x  7\(1281 is a subset of segment S y  6\(1, 1281 with t 1. Because, index of 1281 in S x is 7 in word vector, this is the same index of element 1281 in S y  Definition 3  IWS  X f and only if is a subset segment of a segment in IWS  X    Example 5 Give IWS  X  3\(1545  6\(1, 1281  Subset segment 7\(1281   IWS  X se 7\(1281 is a subset segment of segment 6\(1, 1281 of IWS  X  Definition 4  The intersection among two segment S x  and S y   is a segment S   denote by S z  S x  S y where S z    and   AND     AND  205    AND  with AND is bitwise operator if     and   is subset of S x and S y  Example 6   Given two segments S x  6\(1, 1281  and  S y   5\(3220, 8726, 3104 The intersection segment of these segments is 6\(1, 1024 Because, 6\(1, 1281\d 6\(8726 3104\ubset of S x  and S y 1 AND 8726 = 1, and 1281 AND 3104 = 1024, with AND is an operator on bitwise Definition 5 The intersection between IWS  X d IWS  Y  denoted by IWS  XY  IWS  X  IWS  Y which IWS  XY  include of segments, each segment is a result of intersection segment on IWS  X d IWS  Y ollow definition 4 Theorem 1 Given a segment S i  s i  205   IWS  X  l is indices 1-bits of from left to right\ with s i s j   e i   l is mapped to Bit Vector by formula where k is index 1-bits in Bit Vector of l Proof s j is indices word of on Word Vector, there are s j  1\ords following it means s j 1  t s bef o re  on Bit Vector. Thus l is mapped on Bit Vector by calculator  s j 1 16 l  Example 7 Given an IWS  X  3\(1545  tidset  X s calculated as follow Because the start of segment in IWS  X 3, so adding 32 3-1 16 = 32\ into each bit index of segment. Beside index 1-bit of 1545 is {6, 7, 13, 16}. Hence we have 32 + 6 = 38; 32 +7 = 39; 32 + 13 = 45; 32 + 16 = 48 Therefore tidset  X 38, 39, 45, 48 B  Algorithm for determining intersection of two IWS\222s The mining FWIs on the vertical data format requests the calculation of the itemsets union to create new itemset. This is equal to determining the intersection two corresponding individual IWS\222s based on definition 5 The algorithm of calculating the intersection between two IWS\222s is described in Fig. 1 Algorithm 1 Intersection Algorithm Input IWS  X nclude n 1 segment IWS  Y nclude m 1 segment Output Z IWS  X  IWS  Y  Method name INTERSECT_IWS  IWS  X  IWS  Y  1  2 3    4 5 6 7 8 9 10 11 12 13 14 INTERSECT_IWS  IWS  X  IW S  Y   i 1 j 1 Z  
004 i 006  While \(i n 1 and j m 1  Segment in IWS  X  in IWS  Y s the first intersection segment of IWS  X  IWS  Y rom segment i th on IWS  X nd j t h on IWS  Y  S  006  Start max  IWS  X  i 1  t IWS  Y  j 1    End min  IWS  X  i 1 end IWS  Y  j 1    for all k   S  S  IWS  X  k   IWS  Y  k   Remove 0_Word from S and add S into Z  if \(End IWS  X  i 1  nd i  i 1  1 j  j 1  else j  j 1 1 i  i 1  Return Z  Fig 1 INTERSECTION_IWS algorithm The input of intersection algorithm is IWS  X d IWS  Y  where IWS  X as n 1 segment and IWS  Y as m 1 segment The output is Z which is an IWS. Line 3 determines the intersection segment on IWS  X d IWS  Y he start and the end of result segment are calculated from lines 5 - 6. Lines 8 and 9 are used to calculate word of result segment S fter that, the 0-words are removed from S  S may has many segments  and to add into Z on line 10. Lines 11 to 13 are used to recalculate the new value of i and j The last line Z is to return the result Table 5 IWS from BitTable in Table 3 Word inde x  3 6 7 Byte value 6 9 0 1 5 1 Word value 1545 1 1281 IWS  3\(1545  6\(1, 1281   k  s j 1 16 l  3 
2581 
2581 


 Algorithm INTERSECT_IWS in Fig  1  is illustrated through the following example Example 8   Let IWS  X  3\(1545  5\(1, 1281, 1030  12\(1, 1284, 1536 and IWS  Y  6\(3220, 8726, 3104  10\(1242, 8721, 6527, 6  The second segment of IWS  X  5\(1, 1281, 1030  intersects IWS  Y first segment 6\(3220, 8726, 3104 y 6 th and 7 th in words vector index. Thus, the result segment is 6\(1024, 6 because, 1281 AND 3220 = 1024, 1030 AND 8726 = 6\ Next, the segment third of IWS  X  12\(1, 1284 1536 intersection with the second segment of IWS  Y   1242, 8721, 6527,6 n word vector index are 12 and 13 Therefore, the result segment is 12\(1, 4 because, 1 AND 6527 = 1 and 1284 AND 6 = 4 The final, we have IWS  XY  IWS  X   IWS  Y   6\(1024, 6  12\(1, 4  IV  FAST ALGORITHM FOR MINING FWIs A. IWS-Tree data structure A data structure, called IWS-Tree, is used to mine FWIs from WITDs. Each node on IWS-Tree includes three components, namely X  IWS  X d ws  X here  X is an itemset  IWS  X  is the IWS of X ws  X  is the ws of X To connect nodes X and Y to create a new node X Y  X  and Y must have the same length and the prefix, with length  X  1 items, and ws  X  Y   minws where minws is set by users B. A method for fast computing the ws from IWS structure A raising problem for mining FWIs on WITDs is the ws  calculation of each itemset according to formula \(2\hus, it is necessary to determine the tidset of the itemset. This is equal to determining the index set of 1-bits in an IWS. In order to solve this problem, we define a MAP array including 65.535 elements \(Table  6\ in which each element is a connected list to record index of 1-bits from left to right \(the first bit has index 1\n each Word of the set {1, \205, 65535 Based on the index of segments in IWS, we mapped to the index set of 1-bits in the segment in bitlist The algorithm of calculating ws is expressed as in Fig  2  Algorithm 2 Compute ws Algorithm  Input  IW S  X  Output  ws of itemset X  Method name  COMPUTE_W S    1  2 3 4  5 6  7 COMPUTE_W S  IWS  X  tidset  X  
006    A  B  C  D  E add all the items A  B  C  D  E into the IWSTree. To called recursive function IWS-Tree as fowllos 
005 MAP i    tidset  X  005 I| ws  j  minws  P  
006   for all i 005  IWS  X   for all j 007  i 1 16 j    y 0 for all i  tidset  X   y  y  tw i 8 9  ws y/sum_tw return ws  Fig 2 COMPUTE_WS  algorith m The input of algorithm for computing ws is the IWS X  the output is ws  X calculate ws  X we need to build tidlist  X rom IWS  X ed on formula 3 in Theorem 1 From tidlist  X previous step, we calculate ws  X using formula 2. The first tidlist  X s empty \(line 2\nes 3 to 5 are to build tidlist  X rom IWS  X ines 6 to 8 are to calculate ws  X he last line is to return ws  X  C. Mining FWIs from a WITDs using IWS-Tree Based on WIT-FWI algorithm w e replace th e m e t h od of storing tidset by IWS and using IWS-Tree data structure We propose an algorithm for mining FWIs from WITDs namely IWS-FWI. The itemsets on an IWS-Tree are FWIs which satisfy the minws threshold The IWS_Tree have many layers. The first layer includes an equivalence class with 1-itemset which satisfies the minws  denoted by P he k layer includes many equivalence classes which are k itemsets. Each equivalence class is formed by combination of the parent node k 1\ layer\with each other node in the same layer which is after the parent node. The itemsets in an IWS-Tree are thus FWIs that satisfy the minws  The recursively algorithm for creating a IWS-Tree is expressed in Fig  3 Example 9 Consider the database D represented in Tables 1 with minws 0.4. Table  7 shows the IWS of the 1-itemsets belonging to the database D in example 1 Table 7 IWS of items from D  in example 1 Item Tidse t BitTable IWS ws A 1, 3, 4, 5 10111000 2  1\(184  0.72 B 1, 2, 3, 4, 5, 6 11111100 2  1\(252  1.00 C 2 4, 5, 6 01011100 2  1\(92  0.60 D 1, 3, 5, 6 10101100 2  1\(172  0.78 E 1, 2, 3, 4, 5 11111000 2  1\(248  0.81 In this example, each IWS of items has only 1 segment We compute ws of all 1-itemsets. The result is in column 4 in Table 7. All items satisfy the minws threshold. Thus   200  With node A Algorithm 3 IWS-FWIs Algorithm Input A Weighted database D and minws  Output A IWS-Tree containing all FWIs which satisfy minws  Method name  IWS_FWIs 1 2  3  4  5  6  7  8  9  10 11 12 13 IWS_FWIs   006   j 006  for all lj  P  th j  i do X = l i   l j  Y IWS_INTERSECT  IWS  l i  IWS  l j  ws_X   COMPUTE_WS  Y  if ws_X minws   P i     P i   X  Y  ws  X   IWS_Tree   P i   Fig. 3 IWS-FWIs algorith m  Table 6 MAP constant arra y  W ord inde x  1 2 \205 65535 B inary value 0000000000000001 0000000000000010 \205 1111111111111111 M ap array 16 15 205 1, 2, \205, 15, 16 
006   IWS_Tree   P     for all li  P    Pi   
2582 
2582 


  Consider the two items A and B  IWS  A  IWS  B    1\(184    1\(252 it implies that IWS  AB    1\(184  Based on COMPUTE_WS algorithm, we have ws  AB   ws  A   0.72  minws Thus AB is added into the IWS-Tree Next, consider items A and C. IWS  A   IWS  C    1\(184    1\(92 and it implies IWS  AC    1\(24  Because ws  AC   0.32  minws  AC is not added into the IWS-Tree Similar to node A itemsets AB  AD  AE  ABC  ABD  ABE   ABDE are added to IWS-Tree as shown in Fig. 4  Fig. 4 IWS-Tree with node A Similar to node B  C  D and E the final IWS-Tree is shown in Fig  5. The results with minws 0.4 include A  B  C  D  E  AB  AD  AE  BC  BD  BE  DE  ABD  ABE  ADE  BDE  ABDE   Fig. 5 IWS-Tree included all FWIs V  EXPERIMENTAL RESULTS In this section, we compare IWS with DBV and Diffset based methods in terms of mining time and memory usage for five sparse databases, namely Retail, Mbs-Pos, Sales_Fact_97 Sales_Fact_97_98, and Salas_Fact_Sync. Two databases RETAIL, MBS-POS are obtained from http://fimi.cs.helsinki.fi/data and three databases Sales_Fact_1997, Sales_Fact_1997_1998 and Salas_Fact_Sync are from Foodmart2000 in SQL2000. Table 8 shows the characteristics of the experimental databases. All the experiments were performed on a personal computer with an Intel Haswell Core i5 1.4-Ghz CPU and 4 GB of RAM running Microsoft Windows 8.1. The algorithms were coded by C Table 8 Characteristics of databases used in experiments Database Number of items Number of transactions Remark R ETAIL 16.470 88.162 Modified B MS-POS 1.657 515.597 Modified SALES_FACT _ 97 1.753 20.522 SALE_FACT_97 _ 98 1.753 54.537 SALES_FACT_SYNC 1.753 58.308  We modified the databases by adding a random value in the range of 1 to 10 for each item corresponding to its quantity in each transaction, and created one more table to store the   values of items \(in the range of 1 to 10 A. Mining time comparison Fig. 6 Comparison of runtime for RETAIL database Fig. 7 Comparison of runtime for MBS-POS database Fig. 8 Comparison of runtime for SALE_FACT_97 database Fig. 9 Comparison of runtime for SALE_FACT_97_98 database Fig. 10 Comparison of runtime for SALE_FACT_SYNC database    
2583 
2583 


 C  Mining memory usage comparison  Fig. 11 Comparison of memory usage for RETAIL database   Fig. 12 Comparison of memory usage for MBS-POS database  Fig. 13 Comparison of memory usage for SALE_FACT_97  database   Fig. 14 Comparison of memory usage for SALE_FACT_97_98 database   Fig. 15 Comparison of memory usage for SALE_FACT_SYNC  database   Fig. 6 15 show that IWS is very effective on sparse databases. The performances of three approaches on these databases are quite different. For example, consider RETAIL database \(Fig. 6 & 11\with minws 0.1%, the mining time of IWS is 27.69 \(s\hile that of Diffset is 64.141 \(s\d DBV is 135.576 \(s\ides, the memory usage of IWS is 24.002 MB\ while that of Diffset 48.479 \(MB\d DBV is 160.687 MB Sparse databases have a small number of items on transactions. Many 0_Word on a tidset are removed by IWS Thus n 1 and m 1 in INTERSEC function and k in WS  function are small; the total runtime is thus small. IWS is effective in both memory and processing time VI  CONCLUSIONS AND FUTURE WORK This paper has proposed an effective approach in order to reduce the memory usage for storing tidset information while mining FWIs from SWITDs. Firstly, we use the IWS concept to remove all \2230\224 Words. The IWS concept reduces the runtime and the memory usage. The experiments conducted on five databases show that our approach outperforms existing approaches in terms of memory usage and runtime. Especially when the minws threshold is small, IWS is very effective  In the future, we will further study on the problem of mining FWIs on WITDs with large databases and hierarchical databases. Besides, we will apply this method to mine frequent weighted closed itemsets and maximal frequent weighted itemsets REFERENCES 1  Agrawal, R., Mannila, H., Srikant, R., Toivonen, H., Inkeri V. A., \223Fast discovery of association rules\224 In Advances in Knowledge Discovery and Data Mining, AAAI Press, Menlo Park, CA pp. 307\226328, 1996 2  Han, J., Pei, J., Yin, Y., \223Mining frequent patterns without candidate generation\224 In ACM SIGMOD Conference Management of Data 2000 3  Grahne, G., Zhu, J., \223Fast Algorithms for Frequent Itemset Mining Using FP-Trees\224 IEEE Transactions on Knowledge And Data Engineering 17\(10\ pp.1347-1362, 2005 4  Zaki, M, J, \223Scalable algorithms for association mining\224 IEEE Transactions on Knowledge and Data Engineering 12\(3\,pp. 372-390 2000 5  Zaki, M, J., Gouda, K., \223Fast Vertical Mining Using Diffsets\224 In  KKD  pp.326-335, 2003 6  Dong, J., & Han, M., \223BitTable-FI: An efficient mining frequent itemsets algorithm\224 Knowledge Based Systems 20\(4\, pp.329\226335 2007 7  Vo, B., Hong le, & Le, B, \223DBV-Miner: A Dynamic Bit-Vector approach for fast mining frequent closed itemsets\224 Expert Systems with Applications 39\(8\, pp.7196\2267206, 2012 8  Le, B., Nguyen, H., Vo, B, \223Efficient Algorithms for Mining Frequent Weighted Itemsets from Weighted Items Databases\224 In  RIVF 2010 pp 1-6, 2010 9  Vo, B., Coenen, F., Le, B, \223A new method for mining Frequent Weighted Itemsets based on WIT-Trees\224 Expert Systems with Applications 40\(4\, pp. 1256-1264, 2013   Ramkumar, G, D., Ranka, S., Tsur, S., "Weighted Association Rules Model and Algorithm In  KDD1998 1998   Cai, C.H., Fu, A.W.C., Cheng, C.H., and Kwong, W.W., \223Mining Association Rules with Weighted Items,\224 Proc. IEEE Intelligence Database Engineering and Applications Symposium \(IDEAS \03798 pp 68-77, 1998   Tao, F., F. Murtagh, and M. Farid, \223Weighted Association Rule Mining Using Weighted Support and Significance Framework,\224 Proc. ACM SIGKDD \03703 pp.661-666, 2003   Wang, W., J. Yang and P. Yu "Efficient mining of weighted association rules \(WAR Proc. of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining pp.270-274, 2000   Lan. C.G., Hong, P. T., Lee, Y, H., \223An efficient approach for finding weighted sequential patterns from sequence databases\224 Applied Intelligence 41\(2\, pp. 439-452, 2014   Lan. C.G., Hong. P. T., Lee, Y, H., Wang, L. S., Tsai, W. C 223Enhancing the Efficiency in Mining Weighted Frequent Itemsets\224 IEEE International Conference on Systems, Man, and Cybernetics \(SMC pp. 1104-1108, 2013 
2584 
2584 


         


       0.4 0.8 1.2 1.6 250 500 750 1000 1250 1500 of Queries Average time \(sec Scalability of MISA for WKL_1 0.5 1.0 1.5 2.0 100 200 300 400 500 600 of Queries Average time \(sec Scalability of MISA for WKL_2 0 5 10 15 20 25 30 35 500 5500 10500 15500 20500 25500 30500 of Queries Average time \(sec Scalability of MISA for WKL_3 Figure 2 Scalability of Mining Index Selection Approach for Three Workloads  J Kratica I Ljubic and D T osic  A genetic algorithm for the index selection problem In Applications of Evolutionary Computing Tech Rep 2003  D Comer  The Dif culty of Optimum Inde x Selection ACM Trans Database Syst  vol 3 no 4 pp 440445 Dec 1978 A v ailable http://doi.acm.org/10.1145/320289.320296  techopedia Query Optimizer  2015 http://www.techopedia.com/denition/26224/query-optimizer Online accessed Online A v ailable http://www.techopedia.com/denition/26224/query-optimizer  J Dong and M Han Bittable An ef cient mining frequent itemsets algorithm Knowl.-Based Syst  vol 20 no 4 pp 329335 2007 A v ailable http://dblp.unitrier.de/db/journals/kbs/kbs20.html  P  Ameri U Grabo wski J Me yer  and A Streit On the Application and Performance of MongoDB for Climate Satellite Data in 13th IEEE International 2808 


Conference on Trust Security and Privacy in Computing and Communications TrustCom 2014 Beijing China September 24-26 2014  IEEE 2014 pp 652659 A v ailable http://dx.doi.org/10.1109/TrustCom.2014.84  R Lutz P  Ameri T  Latzk o and J Me yer  Management of Meteorological Mass Data with MongoDB in 28th International Conference on Informatics for Environmental Protection ICT for Energy Efeciency EnviroInfo 2014 Oldenburg Germany September 10-12 2014 J.M.G  omez M Sonnenschein U Vogel A Winter B Rapp and N Giesen Eds BIS-Verlag 2014 pp 549556 Available http://www.enviroinfo2014.org  A Caprara M Fischetti and D Maio Exact and Approximate Algorithms for the Index Selection Problem in Physical Database Design IEEE Trans Knowl Data Eng  vol 7 no 6 pp 955967 1995  K Aouiche and J Darmont Data Mining-based Materialized View and Index Selection in Data Warehouses CoRR  vol abs/0707.1548 2007 A v ailable http://arxiv.org/abs/0707.1548  N P asquier  Y  Bastide R T aouil and L Lakhal Discovering Frequent Closed Itemsets for Association Rules in Proceedings of the 7th International Conference on Database Theory  ser ICDT 99 London UK Springer-Verlag 1999 pp 398416 A v ailable http://dl.acm.org/citation.cfm?id=645503.656256  S Agra w al S Chaudhuri and V  R Narasayya  Automated Selection of Materialized Views and Indexes in SQL Databases in Proceedings of the 26th International Conference on Very Large Data Bases  ser VLDB 00 San Francisco CA USA Morgan Kaufmann Publishers Inc 2000 pp 496505 A v ailable http://dl.acm.org/citation.cfm?id=645926.671701  A Sk elle y  DB2 Advisor An Optimizer Smart Enough to Recommend Its Own Indexes in Proceedings of the 16th International Conference on Data Engineering  ser ICDE 00 Washington DC USA IEEE Computer Society 2000 pp 101 A v ailable http://dl.acm.org/citation.cfm?id=846219.847390  W  G Pedrozo and M S M G V az A T ool for Automatic Index Selection in Database Management Systems in Proceedings of the 2014 International Symposium on Computer Consumer and Control  ser IS3C 14 Washington DC USA IEEE Computer Society 2014 pp 10611064 A v ailable http://dx.doi.org/10.1109/IS3C.2014.277  M Zaman J Surabattula and L Gruenw ald An Auto-Indexing Technique for Databases Based on Clustering in Proceedings of the Database and Expert Systems Applications 15th International Workshop  ser DEXA 04 Washington DC USA IEEE Computer Society 2004 pp 776780 A v ailable http://dx.doi.org/10.1109/DEXA.2004.32  S Chaudhuri and V  Narasayya An Ef cient Cost-Driven Index Selection Tool for Microsoft SQL Server in VLDB  Very Large Data Bases Endowment Inc August 1997 A v ailable http://research.microsoft.com/apps/pubs/default.aspx?id=68349  R Agra w a l and R Srikant F ast algorithms for mining association rules in large databases in Proceedings of the 20th International Conference on Very Large Data Bases  ser VLDB 94 San Francisco CA USA Morgan Kaufmann Publishers Inc 1994 pp 487499 A v ailable http://dl.acm.org/citation.cfm?id=645920.672836  B Goethals Memory Issues in Frequent Itemset Mining  in Proceedings of the 2004 ACM Symposium on Applied Computing  ser SAC 04 New York NY USA ACM 2004 pp 530534 A v ailable http://doi.acm.org/10.1145/967900.968012  J Hipp U G  untzer and G Nakhaeizadeh Algorithms for Association Rule Mining Mdash a General Survey and Comparison SIGKDD Explor Newsl  vol 2 no 1 pp 5864 Jun 2000 A v ailable http://doi.acm.org/10.1145/360402.360421  A Rajaraman and J D Ullman Mining of Massive Datasets  New York NY USA Cambridge University Press 2011  MongoDB Introduction to MongoDB  2015 http://www.mongodb.org/about/introduction Online accessed Online A v ailable http://www.mongodb.org/about/introduction  J Pok orn y  NoSQL Databases A Step to Database Scalability in Web Environment in Proceedings of the 13th International Conference on Information Integration and Web-based Applications and Services  ser iiWAS 11 New York NY USA ACM 2011 pp 278283 Available http://doi.acm.org/10.1145/2095536.2095583  MongoDB SQL to MongoDB Mapping Chart 2015 http://docs.mongodb.org/manual/reference/sqlcomparison Online accessed Online Available http://docs.mongodb.org/manual/reference/sqlcomparison  BSON BSON Binary JSON  2015 http://bsonspec.or g Online accessed Online A v ailable http://bsonspec.org  MongoDB Inde x T ypes  2015 http://docs.mongodb.org/manual/core/index-types Online accessed Online A v ailable http://docs.mongodb.org/manual/core/index-types  R Core T eam R A Language and Environment for Statistical Computing  R Foundation for Statistical Computing Vienna Austria 2015 A v ailable http://www R-project.or g  MongoDB Inc and M Schmidber ger  rmongodb RMongoDB driver  2014 r package version 1.8.0 Available http://CRAN.R-project.org/package=rmongodb  K Bank er  MongoDB in Action  Minning Publication Co 2012 2809 


 MongoDB Geospatial Query Operators  2015 http://docs.mongodb.org/manual/reference/operator/querygeospatial Online accessed 18 Online A v ailable http://docs.mongodb.org/manual/reference/operator/querygeospatial  M K ormilitsin R Chirk o v a Y  F athi and M Stallmann View and Index Selection for Query-performance Improvement Quality-centered Algorithms and Heuristics in Proceedings of the 17th ACM Conference on Information and Knowledge Management  ser CIKM 08 New York NY USA ACM 2008 pp 13291330 A v ailable http://doi.acm.org/10.1145/1458082.1458261 2810 


