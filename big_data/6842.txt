N EW B REAKDOWN D ATA G ENERATION AND A NALYTICS M ETHODOLOGY TO A DDRESS BEOL AND MOL  D IELECTRIC TDDB  P ROCESS D EVELOPMENT AND T ECHNOLOGY Q UALIFICATION C HALLENGES  Fen Chen, Carole Graas, Michael Shinosky, Chuck Griffin, Roger Dufresne, Ronald Bolam, Cathryn Christiansen IBM Microelectronics, Essex Junction, VT 05452, USA Phone: 802-769-7917; Fax: 802-769-4139; E-mail: chenfe@us.ibm.com 
Kai Zhao, Shreesh Narasimha, Chunyan Tian IBM Microelectronics, Hopewell Junction, NY 12533, USA Choon-Leong Lou STAr Technologies, Inc., Hsinchu City, Taiwan  Abstract 227 Both MOL PC-CA spacer dielectric and BEOL lowk  dielectric breakdown data are commonly convoluted with multiple variables induced by process steps such as lithography etch, CMP, cleaning, and thin film deposition. The traditional method of stressing one DUT per die or multiple DUTs per die without careful data deconvolution, is incapable of addressing current complex MOL PC-CA and BEOL low 
k dielectric breakdown modeling challenges. In this paper, a new big data generation method plus an analytics procedure method is proposed to soundly evaluate both MOL and BEOL dielectric time-dependent-dielectric breakdown data. A new diagnostic reliability concept is for the first time proposed for comprehensive process diagnostics and more accurate reliability failure rate determination Keywords \226 lowk TDDB, lowk reliability, MOL, PC-CA breakdown, global die-to-die variation, local within chip 
variation, data deconvolution, compound Weibull distribution, compound Poisson area scaling, voltage acceleration I   I NTRODUCTION  The continuing aggressive scaling of device dimensions and the introduction of new device configurations, have progressively challenged dielectric breakdown behavior between middle-of-line \(MOL\olysilicon control gates \(PC and diffusion contacts \(CA\, and throughout the back-end-ofline \(BEOL\ lowk dielectric. Generally, both MOL PC-CA spacer dielectric and BEOL low 
k dielectric breakdown data are commonly convoluted with multiple variables originating from multiple process steps such as lithography, etch, CMP cleaning, and thin film deposition. Such variations could be significant from die to die, and even within dies. The actual die-to-die variation could be composed of several subpopulations with distinct parameters that cannot be described by a single Poisson distribution. This could be why non-Poisson area scaling is often observed from MOL PC-CA and BEOL lowk dielectric breakdown data [1  D u ri ng technology qualification, both breakdown mechanisms can 
challenge process and reliability engineers to determine the true intrinsic breakdown behavior while correctly diagnosing process variations. This is important because process issues can appear to degrade measured intrinsic behavior. The traditional method of stressing one DUT per die or multiple DUTs per die without careful data deconvolution, is incapable of addressing current complex MOL PC-CA and BEOL lowk dielectric breakdown challenges. In this paper, a new end-to-end big data generation method is proposed, together with a deconvolution 
procedure, to soundly evaluate both MOL and BEOL dielectric time-dependent-dielectric breakdown \(TDDB\ data. A new process and reliability quality index, die-to-die variation distribution, is quantitatively established. Furthermore, an aggregation of reliability stress data, diagnostics data simulation data, and yield data together with analytics is for the first time proposed as a diagnostic reliability methodology for comprehensive process diagnostics and more accurate reliability failure rate determination II  E XPERIMENTAL   The wafers investigated in this work were fabricated across 
several different technology nodes ranging from 32nm to 14nm using metal\226oxide\226semiconductor  CMOS\ses in a 300mm manufacturing line. Various BEOL and MOL test structures were studied for development and validation of our method. For the BEOL lowk TDDB study, dielectric constant k 2.7 and k 2.55 carbon doped glass films were fabricated by chemical vapor deposition. Deposition of TaN/Ta bilayer 3A.1.1 
978-1-4799-3317-4/14/$31.00 \2512014 IEEE 


barriers and Mn-doped Cu seed layers were fabricated using ionized physical vapor deposition. For the MOL PC-CA voltage-ramp-dielectric-breakdown \(VRDB\ study, SiN film was used between gate and contacts as the insulation spacer film. All MOL structures were laid on STI oxide to isolate the PC-CA breakdown from the gate dielectric breakdown. In order to investigate the local within die variation, we designed a special set of structures with 10-40 clone copies per die. All TDDB and VRDB tests were performed at wafer level by regular single site wafer probers and a special multi-site wafer prober. Multi-site probing time was minimized by simultaneously probing 16 sites on a 300mm wafer using up to 320 DUTs in parallel \(20 DUTs per die III  R ESULTS AND D ISCUSSIONS  A  Big Data Generation and Deconvolution It is well known that significant die-to-die variations cause TDDB stress fields to become a non-constant at die level which causes various forms of Weibull distributions and nonPoisson area scaling actu al B E OL a n d MO L T DDB  applications, a mixture of many Weibull distributions often seems to be a more realistic case. Distributions resulting from mixing many component distributions are designated as 223compound\224.  Figures 1 and 2 experimentally demonstrate various compound Weibull distributions \(RED distributions of data\or BEOL lowk TDDB and MOL PC-CA dielectric VRDB, respectively. Without further data deconvolution, such compound distributions could be present in all kinds of shapes such as straight line-like, bimodal-like, concave curvaturelike, and convex curvature-like shapes.  We also have found that such compound distributions exist for almost all the BEOL and MOL TDDB and VRDB datasets we have studied from 32nm to 14nm. Overall, Weibull slopes of such RED distributions were low or very low, and simply increasing the sample size without a careful deconvolution does not help to improve the overall Weibull slope at all    Figure 1: Experimental proof of various compound Weibull distributions for BEOL lowk TDDB with a k 2.7, b k 2.55 The complexity in data convolution makes it seem impossible to accurately determine a constellation of all relevant parameters just by blind data fitting. Therefore, the first step we took was to experimentally deconvolute them at die level. For this, we designed, then stressed 10-40 clone structures per die, and repeated each stress for all dies on the entire wafer. In Figures 1 and 2, the 70-90 individual black and blue distributions are the actual local distributions from each die, each data point representing the breakdown value of one of the test structure clones. Therefore, each one of these local distributions captures the within-die variation of the breakdown. Generally, such massive parallel stressing data generation can easily collect a total of about 500-1000 data points per wafer for our next step of data analysis. We refer to this methodology as \223big data generation\224   Figure 2: Experimental proof of various compound Weibull distributions for MOL PC-CA dielectric Vbd with a\ Vbd distribution, convex curvature like b\ Vbd distribution, concave curvature like   Figure 3: t63 die-to-die distributions for lowk TDDB in Figures 1a and 1b  Figure 4: V63 die-to-die distributions for PC-CA VRDB in Figures 2a and 2b By analyzing all the individual distributions separately, we can quickly construct a critical die-to-die variation index chart We do this by plotting all t63 or V63 data points from the population we want to study \(e.g. one wafer or multiple wafers together\igures 3 and 4 show such data, corresponding to the data shown in Figures 1 and 2 respectively. Since for a Weibull distribution, the characteristic scale parameter at 63.2% always has the highest confidence bounds, such t63 and V63 distributions represent the real die-to-die variation much better than the single DUT from one die approach does. In other words, the distributions of Figures 3 and 4 are much 3A.1.2 


more accurate than the red lines on Figures 1 and 2. In order to establish a quantitative assessment of any process induced dieto-die variation, this t63 or V63 across-wafer distribution is now the new standard for further process diagnostics and reliability evaluation. It should also be noted that electrically measured t63 and V63 values obtained by this method capture all relevant breakdown parameters, not just spacing Therefore, such die-to-die t63 or V63 distributions provide more comprehensive information about die-to-die variation than does a spacing distribution alone. This t63 or V63 across wafer distribution can take various shapes as shown in Figures 3 and 4. From a reliability modeling perspective, those t63 and V63 data points together with within-chip local Weibull slopes ultimately determine the true chip level failure rate and are less conservative   Figure 5: Comparison of Weibull slopes between 12-DUT and 24-DUT sample size  Furthermore, as shown on Figure 5, our big data tells us that there is always a wide Weibull slope variation \(i.e. 3-5X delta across the wafer, and that such variation is inversely proportional to the number of DUTs per die stressed. With spacing the primary driver of t63 or V63 values, one should expect the Weibull slope to be a function of t63 or V63, yet, it seems unlikely that such a large spread on the same wafer would be caused solely by this intrinsic nature. In order to explore the primary root cause, we conducted a Monte Carlo MC\ simulation study assuming a fixed Weibull slope and a fixed ample size \(i.e. 12 DUTs\If simulated Weibull slopes from 70-90 random runs could construct a wide Weibull slope variation similar to that experimentally observed, to the first order, we would conclude that our observed Weibull slope variation is simply due to the statistical variation of our relatively small sample size per die. Figure 6 does indeed show good agreement between the two. Alternatively, by plotting experimentally obtained t63 or V63 versus Weibull slope pairs, we can also directly validate whether the Weibull slope has an obvious t63 or V63 dependence. Figure 7 shows scattered data without any such obvious dependencies typical of most wafers. Since all the data points seem to come from distributions with the same Weibull slope but different t63 or V63 and in order to eliminate the substantial cross-wafer dieto-die variation but still utilize the big data we collected, we proceeded to normalize all data points with a fixed t63 or V63 point. By considering the fact of potential Weibull slope dependence on t63 or V63, we decided to use the median t63 or V63 as our normalization point. We proposed that even a minor Weibull slope deviation existed for extremely low and high t63 or V63 data points from a wafer, and such deviation effect could be cancelled out at median t63 or V63. By linking the Weibull slope obtained from normalization to the corresponding median t63 or V63, our established Weibull slope should have a better physical meaning. Figure 8 illustrates this analytics procedure in detail. After such normalization, clean mono-modal distributions with substantially improved Weibull slopes were usually observed Figures 9 and 10\s compared to the compound distributions shown in Figures 1 and 2, the Weibull slopes in Figures 9 and 10 are substantially improved \(2-4X\or lowk  TDDB k 2.7 and k 2.55, dielectrics actually exhibit a similar intrinsic local Weibull slope, although the Weibull slope from the compound distribution for k 2.55 dielectric was much worse than the compound distribution for k 2.7 dielectric This analytics suggests that although, for a given electric field the t BD of k 2.55 dielectric is slightly worse than t BD of k 2.7 dielectric, the real problem associated with the k 2.55 dielectric was really due to its higher die-to-die variability   Figure 6: Weibull slope variation comparison between measured and simulated cases  Figure 7: Experimentally measured t63 and V63 versus Weibull slopes for BEOL lowk TDDB and MOL PC-CA VRDB cases  Figure 8: Data deconvolution procedure details and its outcome of local intrinsic Weibull slope determined from large sample size data 3A.1.3 


  Figure 9: Local intrinsic TDDB distributions with improved Weibull slopes after deconvolution for BEOL lowk TDDB data in Figure 1   Figure 10: Local intrinsic Vbd distributions with improved Weibull slopes after deconvolution for MOL PC-CA VRDB data in Figure 2  Figure 11: Multiple MC simulations on to reproduce the measured BEOL lowk TDDB distribution shown in Figure 1b   Figure 12: Multiple MC simulation based on extracted parameters to reproduce the measured bimodal like MOL PC-CA Vbd distribution Lastly, with the known within-die local Weibull slope and experimentally established cross-wafer global t63 or V63 distribution, we can subjectively construct compound distributions by MC simulations as many times as we want We can then compare simulated distributions to our experimentally established distribution to carefully validate our method and the extracted parameters. We found that although the distribution per die varied every time during simulation due to sample size limited statistical variation, the combined big data distributions from MC simulations can always reproduce the details of our real data nicely as shown in Figures 11 and 12. This confidently justifies our proposed big data and analytics method Our data deconvolution method also offers a powerful way to further quantitatively establish a true Weibull slope versus t63 or V63 relation even using the data from the same wafer Two approaches can be used. The first approach is for the experimental data already exhibiting an obvious Weibull slope dependence on t63 or V63 as shown in Figure 13 as an example. In order to have a precise deconvolution, we can further group different t63s based on pre-determined t63 ranges, and then apply our proposed normalization method individually within each t63 group. As shown in Figure 14 three sections of dies with different t63 ranges were grouped to establish three Weibull slopes with three associated local mean t63 values \(Red, Magenta, and Blue distributions\n the order of low to high t63 values. The Weibull slope decreases with decreasing t63, as expected, following an empirical relation of C0+C1*t63+C2*Ln[t o r t 63 3A.1.4 002 0 \(about 30 Weibull slope reduction for a 50x t63 reduction\. Interestingly if we still use the entire wafer as a single group for normalization, the Weibull slope, at the entire wafer mean t63 fits very well within the Weibull slope versus t63 curve. This further supports our method of using median t63 as a normalization point to extract the associated Weibull slope Using this sectional grouping concept, we can adjust data points with different normalization parameters during our normalization process to further improve our intrinsic Weibull slope determination if needed. Meanwhile, a relationship of Weibull slope and t63 could also be derived from the same wafer for reliability failure rate projection. The second approach is for data not showing any obvious Weibull slope dependence on t63 or V63. In order to establish a true intrinsic Weibull slope versus t63 or V63 relation, we then have to rely on a set of structures to forcibly generate different t63 or V63 values. Let\222s use MOL PC-CA as an example. We purposely designed a set of \223wimpy\224 test structures with different PCCA spacings. The wimpy value represented how far offnominal the PC-CA spacing was designed per test structure As shown in Figure 15, by using such structures, different V63 values at different wimpy values, and therefore different Weibull slopes could be experimentally obtained by our big data analytics method. Then, by plotting Weibull slope versus V63 for all the wimpy values, a Weibull slope dependence on V63 could be quantitatively established as well. As shown in Figure 15, for PC-CA VRDB, a relatively weak dependence of Weibull slope on Vbd was found \(~30 change for ~8V Vbd delta\By using those two approaches, it was experimentally confirmed that dies with lower t63 or V63 were usually prone to exhibit higher extrinsic defect 


population as compared to the dies with higher t63 or V63. In other words, extrinsic defect eff ect could be amplified for dies with lower t63 or V63   Figure 13: A weak Weibull slope dependence on t63 observed for this particular lowk wafer  Figure 14: Sectional grouping and normalization to determine Weibull slope at different t63 values for a BEOL lowk TDDB case   Figure 15: Using wimpy structures to study MOL PC-CA Vbd Weibull slope versus V63 relation Traditionally, various data fitting methods, based on different models, are used to extract a set of relevant distribution parameters with the best fit to the data points for reliability modeling. With good confidence of our big data generation and analytics method, we can conduct a fair comparison of our method with other available fitting methods. The fitting methods we used for comparison are mono-modal MLE fitting, clustering model fitting, and bimodal 5-parameter MLE fitting. It should be noted that each of those fitting models assumes only one or two universal Weibull slopes for the entire dataset. Inherently, they cannot predict more than two Weibull slopes within the distribution and they have no capability for establishing the critical t63 or V63 die-to-die distribution. As shown in Figure 16 using a lowk TDDB distribution as an example, it is obvious that all those fitting models severely underestimate the real Weibull slope. Among those three data fitting methods, 5-parameter bimodal fitting seems to give the best Weibull slope while mono-modal fitting gives the worst Weibull slope for this specific case. By comparing those blind fitting methods to our big data generation and analytics method, it is clear that our method gives the best and also more meaningful Weibull slope. Due to the complexity in data convolution, it seems impossible for all other fitting methods to accurately determine a constellation of all relevant parameters just by blind data fitting  Figure 16: Comparison of various methods for the same wafer data. Big data method provides the best Weibull slope B  Compound Poission Area Scaling for Compound Weibull Distribtuion It has been reported that lowk TDDB could fail to follow traditional Poisson area scaling if process induced spacing variation is significant [2 T h e trad itio n a l P o isso n d i strib u ti o n  is not always adequate to pred ict TDDB area scaling in IC. As shown in Figure 17, based on traditional one DUT per die TDDB stress method, compound Weibull distributions always exhibit non-Poisson area scaling. However, Poisson area scaling is preserved for local, within-die area scaling as experimentally demonstrated in Figure 18. There is no fundamental area scaling physics change for all BEOL lowk  and MOL PC-CA TDDB, and single Poisson area scaling physics still works nicely at local per-die level. Therefore, the fatal defect concept could still be defined in terms of blocks and it could be assumed that they are distributed uniformly over each block. If a block size is equivalent to a die size then, within a die, single Poisson area scaling of course could still be preserved. Globally, due to a severe die-to-die variation, without data deconvolution, single Poisson area scaling can never be applied to any compound Weibull distribution. It is proved that the traditional Poisson distribution is not always adequate to predict TDDB area scaling if die-to-die variation is significant. Data transformed by as-designed area ratio clearly underestimated the reliability of compound Weibull distributions. It is well known that for BEOL lowk and MOL PC-CA cases, the spacing could be composed of several subpopulations with distinct parameters that cannot be described by a single Poisson distribution 3A.1.5 


007 004 004 t t Exp A tD 212 212 005 212 212  1 1  1 163 tDA t t ExptR 327 212\212 003\003 003 003  212 005 006 006   1  0  0  212\212 007 007 007 1 63 63 2 exp1 exp1 F BD n BD nt t t t tF F   a eb f baa  A A t t Therefore, the defect density in Poisson yield formula should not be a constant anymore. In order to account for this nonuniform defect distribution across the wafer, a new area scaling model, compound Poisson defect distribution model, is proposed. The expected number of fatal defects from an area is a random variable instead of being a constant when applying the compound Poisson model. The probability for a chip to have k fatal defects is [3-4   1 where 3A.1.6 004 007 004   212\212\212\212\212 2 1 2 1 1 1 A A LnFLnLnFLnLn 007 1 2 1 2 1       ak k k bak bak dfe k kP  003\003 003 003 dfe k kP k 212    212 1 1  163 1   006  212\212 003 003 003 1     DA b p a 1 1 1  1 1 1 0 4  where A 1 D = a/b and 003 is fatal defect density regarded as a random variable and f 003 s the defect density mixing function which can be in various forms such as Gamma, Seed, and triangular functions As an example, by applying gamma function as f 003  equation 2, mathematically, it can be demonstrated that non single Poisson area scaling with compound Weibull distributions could exist   2  where a and b are positive numbers, and a\ the gamma function. This distribution has the mean a/b and the variance a/b 2 The probability to have k fatal defects in Equation 2 becomes  3    The distribution in Equation 3 is commonly known as the negative binomial distribution Since yield is degraded with a single defect, we set k 0 in Equation 3 with area A 1 and defect density D as shown in Equation 4   004 004 004 a. The parameter 004 is called the cluster parameter. To incorporate the compound Poisson area scaling into TDDB data, the survival function can be equated with the reliability of a Weibull distribution with a known t63 and a single Weibull slope as shown in Equation 5   5    Solving for D\(t\ gives    6   With equation 6, the relationship of t63 and area can be determined by assuming the same defect density D\(t\ure 19 illustrates the non-Poisson area scaling behavior for a single Weibull distribution with the compound Poisson defect distribution. From Figure 19, it is obvious that single Poisson area scaling underestimates the breakdown time, which is consistent with all of our experimental observations. In order to have the same tbd ratio with the same the compound Poisson model requires a larger area ratio to achieve it as compared to the single Poisson model. In other words, the effective area ratio needed to achieve the same tbd change is smaller in the compound Poisson model as compared to the asdesigned area ratio in the single Poisson model. Furthermore the effect of decreasing is exaggerated in the single Poisson model as opposed to the compound Poisson model  Since f 003 be in various forms, it is impractical to derive the right f 003 with limited area scaling TDDB data \(i.e. 3-4 areas\d/or to determine the actual fatal area scaling ratio instead of as-designed area ratio. On the other hand, based on our big data generation and analytics method, we already demonstrated that all the TDDB distributions we have dealt with were still Weibull distributions regardless of their single Weibull or compound Weibull format. One unique characteristic of Weibull distribution is that the Weibull slope will be preserved from both horizontal \(Equation 7\d vertical \(Equation 8\ transformations. Both single and compound Weibull distributions have this characteristic  Based on this theory, a simple graphic Shift & Compare S&C\method was developed to experimentally determine the actual fatal area ratio for the compound Weibull case as shown in Equation 9 [2    7   8  9  where n F is defined as fatal area ratio, which is different from as-designed area ratio for the adjustment of different global area scaling data based on the compound Poisson area scaling model. The key foundation for using S&C method is that the Weibull shape factor will be preserved for all areas regardless of single or compound Weibull distributions. Experimentally this assumption was already proved as shown in Figure 17 Realistically, the real product TDDB distribution could have the same shape as our test structure if deconvolution is not 


performed. Therefore, there is no need to change the distribution shape in order to align the data from different areas. Figure 20 illustrates the S&C method in detail. A computer automation program was developed to minimize the delta of Weibull slopes from both horizontal shift and vertical shift to determine the best n F As shown in Figure 21, if f 3A.1.7 003  a gamma function, S&C could generate a similar n F as Equation 5 predicted. However, using S&C is a much simpler way with no guess of f 003 d its associated parameters. It should be noted that the S&C method also works naturally for single Poisson area scaling. Now the n F determined from S&C will be identical to the as-designed area ratio. In other words if the as-designed area ratio is not known for some reasons using S&C would allow one to experimentally determine the as-designed ratio for the Single Poisson case. Of course, if the big data generation method is used, and a compound Weibull distribution is carefully deconvoluted, then the single Poisson model would work flawlessly to model area scaling locally and the S&C method would no longer be needed for those compound Weibull distributions    Figure 17: Compound TDDB Weibull data transformed by Poisson area scaling    Figure 18: Local area scaling by single Poisson area scaling    Figure 19: tbd area scaling with single Poisson and compound Poisson defect distribution   Figure 20: Shift & Compare method demonstration     Figure 21: Comparison of Shift & Compare versus compound Poisson with f 003 gamma function for n F determination  C  Big Data Method Applications and Advantages With our careful data deconvolution, comprehensive process diagnostics and accurate reliability projection can be conducted. First, a t63 or V63 wafer map could be established which should provide more accurate information than does the traditional wafer map generated from one DUT per die. A t63 or V63 wafer map is important for the study of potential wafer regional dependence caused by process inhomogeneity Second, a clear separation of extrinsic early fail defects from intrinsic breakdown can be obtained as shown in Figures 10 and 22. Without this unquestionable separation, early fails simply due to narrow spacing, could be treated as extrinsic defects. Third, our big data generation and analytics method allows us to combine data from different wafers, which is not possible for the traditional method if wafer-to-wafer variation exists. Therefore, an ultra large sample size can be achieved as demonstrated in Figure 22. Also with our data deconvolution and single Poisson area transformation, we can establish an ultimate intrinsic Weibull slope determination method with a wide range span of data points down to almost 1ppm level directly with only 4000 data points \(Figure 23\astly, a more meaningful process split comparison and process diagnostics 


can be achieved. As shown in Figure 24 for a PC-CA case as an example, by comparing the raw data without deconvolution, split B is worse than split A. However, with further data deconvolution, intrinsically, both split A and B have the same and solid Weibull slope. The curvature shown from the raw compound distribution is mainly determined by the V63 distribution shape for both splits. The reason split B is worse than split A is its larger die-to-die variation and smaller absolute Vbd. Together, with our previously published diagnostic structures and meth  w e cou l d als o g o a s t ep further in determining the exact root cause, such as variation of overlay, CA size, LER or material breakdown strength to explain the observed V63 delta. Figure 25 illustrates our diagnostic method in detail. For a fixed contacted poly pitch CPP\, the highest V BD point, which is defined by the intersection of positive and negative misalignment curves should represent the most centered position of CA placement between two poly gates after patterning. This V BD point should be at 0-misalignment position if overlay is perfect. However if there is an offset of this highest V BD point from the 0misalignment position, we can conclude that there is a shift of CA placement, x OL simply due to an overlay misalignment From Figure 25, x PP is also an important parameter, which is defined as the distance between two zero breakdown positions at positive and negative sides. x pp shall represent the available electrical spacing between two poly gate edges for CA to freely move. CPP is a fairly constant number, and the spacing between two poly gates is usually also a constant. Therefore the actual CA size could be estimated by subtracting x pp from the design poly gate space. The breakdown strength of a dielectric per a specific test condition could also be determined from Figure 25 [6 T h e x a x is o f all t h e d ata points, which represents the physical spacing, shall be independent of test condition, such as voltage ramp rate However, the y-axis of all the data points, which represents the breakdown voltage, is expected to be a function of voltage ramp rate. It was reported already that the faster the ramp rate the higher the breakdown fi T h eref ore, th e E b d  dependence on voltage ramp rate is shown in Figure 25. As both Ebd and Vbd increase with increasing ramp rate, simply using Vbd/Ebd method to extrapolate the actual PC-CA spacing is also acceptable. It can generate the similar spacing as the x OL x pp method does although the x OL x pp method is recommended. Actually, with our proposed diagnostic approach, a self-consistent check can be performed using the Ebd and x OL x pp methods to accurately determine the real PC-CA spacing. After the details of various process parameters are extracted, according to Figure 26, we can correlate our established V63 die-to-die variation to overlay CA size, Ebd, and local variation separately \(i.e. all other contributors that cause an actual PC-CA spacing difference from the as-designed spacing, in addition to overlay and CA size\s a consequence, we can determine which parameter is the primary factor to cause the degradation of split B in Figure 24. By looking at all the bottom plots in Figure 26, it is obvious that only overlay and CA size could cause lower Vbd and poor Vbd distribution. Between them, overlay clearly is the primary root cause. With the same analysis of split A, and by comparing the overlay misalignment between split A and B, shown in Figure 27, it was confirmed that overlay misalignment was indeed the primary root cause as there was no obvious differences of CA size and Ebd between split A and split B. As the overlay misalignment problem was mainly a process control issue instead of a fundamental issue, the feasibility of split B was still demonstrated, as shown in Figure 24, from our careful big data generation and analytics method. Optimizing split B process control is critical and could equalize split B reliability with split A reliability    Figure 22: 11 wafers combined to generate 15000 data points     Figure 23: All dies on the same wafer from three areas after deconvolution and normalization with total 4000 DUTs     Figure 24: Raw compound distributions versus deconvoluted distributions  3A.1.8 


    Figure 25: Proposed diagnostic method for quantitative estimate of some critical process parameters   Figure 26: Combination of massive diagnostic data and big stress data for powerful reliability and process analysis     Figure 27: Overlay distribution for splits A and B  D  Failure Rate Projection for Compound Weibull Distributions Our new data generation and analytics method also has a profound impact on reliability failure rate determination Generally, the traditional "one DUT per die" stress method is only valid if all dies on the wafer are exactly identical Otherwise, the failure rate calculated from the traditional method is just a wafer-based failure rate, and has nothing to do with the required die-level failure rate. Furthermore, using a convoluted distribution at test structure level to predict a convoluted distribution at die level but without knowing the convolution details at both levels will most likely cause an erroneous projection. Based on our experimentally deconvoluted data, we proposed a probability associated TDDB concept as shown in Equation 10 and Figure 28 for TDDB failure rate calculation. The total failure rate of chips with different distribution parameters should be a sum of individual chip failure rates of all the chips and the correspondingly different probabilities over the probability density function as the following 10 where P\(x\ is a specific probability density function of parameter x. We propose two methods to calculate the die level failure rate for a compound distribution case. The first method is for the breakdowns that are solely determined by line-to-line and via-to-line space. By applying our powerful big data method, we can first use one group of 10-20 DUTs per die for VRDB to construct a Vbd die-to-die distribution on the entire wafer. As Ebd could be determined from our proposed diagnostic method, the actual spacing per each die shall be determined. Next we can use another group of 10-20 DUTs within the same die for constant voltage TDDB. By plotting the t63 from TDDB versus the spacing from VRDB on the same die, and for all the dies on the wafer, the field acceleration factor could be accurately determined as shown in Figure 29 with 50-70 t63 data points. In contrast, the traditional way to determine field acceleration relies on collecting only 3 to 5 interval t63 data points at 3 to 5 fields which will have wide error bounds. Furthermore, as such data points are potentially from convoluted distributions, more errors could be brought in. Therefore, our new method demonstrated in Figure 29 offers an improved accuracy with its larger t63 sample size. It should also be noted that the statistical variation nature of t63 induced by limited sample size per distribution is covered in our method. Using the electrically determined space to calculate the stress field has an obvious advantage as compared to using physical failure analysis or just nominal space to estimate the stress field by the traditional method. Our big data VRDB + TDDB method offers a powerful way to determine TDDB field acceleration factor with much less uncertainty. As shown in Figure 29, it is also unquestionably demonstrated that under a constant voltage of 12.5V TDDB stress, the actual stress field on each die is not a constant, and could vary from 0.47V/nm to 0.66V/nm \(~40% variation\or this case. A detailed investigation of a lowk TDDB acceleration model based on our proposed big data VRDB + TDDB method will be published later. When a space dependent failure rate \(Figure 29\ith a known intrinsic Weibull slope and its dependence on spacing is established, then, combined with a known process assumption defined spacing distribution \(usually a Normal distribution\, the total failure rate of all the chips can be calculated by Equations 11 and 12. If the calculated total failure rate is higher than an acceptable reliability target, a minimum insulator spacing \(MinIns\ should be brought in to assure that the actual failure rate integrated from MinIns to infinite in Equation 12 could meet the reliability target. With Equations 11 and 12, it is interestingly found that there is always a critical spacing that exhibits the highest failure rate This peak failure rate spacing is found to be insensitive to the sigma and the mean spacing variations. However the failure rate itself at this peak failure rate spacing is a strong function of both sigma and the mean spacing as shown in Figures 30 and 31. Mature process control can usually target the mean 3A.1.9  327   m i x x i dxxPxcdfxcdf chipCDF 1 max min  


212\212 t 007 n\t 212 212 005 327 212\212  arg 1  n x ett x x ExpxF   dse t EOL F us s use total 2 2 2 0 63 2 1 exp1  max min 14 b 007 327\212\327 327 212 5.0 1 63 s V Exp A A Ct dd ref p use 007 13  spacing fairly well. However, the sigma in the space distribution function could vary with different products, which is the most  uncertain source in this failure rate calculation method. Therefore, a careful evaluation of the peak failure rate spacing with different sigma values is extremely critical for reliability engineers. Carefully determining a MinIns to assure that product reliability can be kept away from a massive fallout condition is very useful  11   12   Figure 28: Probability associated TDDB concept  Figure 29: Combination of Vbd and TDDB big data to determine field acceleration factor with many t63 data points   Figure 30: Calculated failure rate based on Equations 9 and 10. All cases show a critical spacing with the highest failure rate  Figure 31: Examples showing the peak failure rate spacing is insensitive to sigma and mean change while peak failure rate is a strong function of both sigma and mean change On the other hand, the spacing may not be the only parameter to determine the overall TDDB. Based on Equation 13, it is well known that a Weibull CDF is a function of t63 or V63 and As a t63 or V63 distribution and an intrinsic  versus t63 or V63 relation can be exclusively determined by our proposed big data method discussed above, naturally a superposition based approach can be directly applied to calculate the realistic chip level failure rate without a bridge to the spacing. As t63 and V63 are determined by all relevant breakdown parameters, not just spacing, therefore this method should be a universal method for chip level failure rate calculation. The concept also can be applied to all reliability failure mechanisms including BEOL EM, BEOL SIV, and FEOL gate dielectric with either Lognormal or Weibull statistics. The total failure rate summing from all chips with different t63 or V63 values and correspondingly different probabilities over the probability density function could be described by Equations 14 and 15  3A.1.10    dxxFxPxF x x total 


003 003\003 t n\t 1 2 2 2 2 2 1 15  where n is the area scaling ratio, x could be either t63 or V63 k is the shape parameter and is the scale parameter of the Weibull distribution. For VRDB and TDDB, using P\(x\ in a Weibull format is recommended. As in Weibull, the distribution shape could be preserved after area scaling, which was supported by some experimental data. Also using P\(x\ in Weibull format generates a more conservative failure rate number as compared to P\(x\ in Normal format. However regardless of which P\(x\ format is used, generally, the failure rates obtained by Equations 13-15 are much smaller as compared to the numbers calculated by the traditional method Table 1 and Figure 30 illustrate one lowk TDDB with k 2.55 and 64nm pitch interconnect as an example. With our new proposed method, the summation of failure rates from chips with t63=0 hour to chips with t63=100000 hours is significantly smaller than the single failure rate produced by the traditional method. Therefore, the E max for a realistic reliability can be significantly lifted up  Table 1 Method  vs. t63 used for FR calculation t63 Failure Rate at End of Life A.U Traditional 0.62 No fixed 3050 New 1.69 Yes Varied 6.01     Figure 32: An example of calculated failure rate versus t63 based on Equations 12-14 for P\(x\ith a Weibull format   IV  D ISCUSSION AND C ONCLUSIONS  In conclusion, a new big data \(aggregation of stress data diagnostics data, simulation data, and yield data\eneration and analytics method is proposed to address MOL PC-CA and BEOL lowk TDDB challenges. With this new method without the introduction of any new TDDB acceleration model, reliability can be met with good confidence for various processes based on the square-root of E model. Since BEOL lowk TDDB and MOL PC-CA TDDB are so sensitive to processing and to structural layout, different processes and different structure layouts could potentially require different TDDB models. The breakdown mechanisms at different stress voltage regions could also be different. Therefore, unless we perform long-term TDDB stresses to validate a TDDB model at all situations such as material change at different technology nodes, different lowk used at different levels, and critical process changes, the question about the correct TDDB model would still exist from different stress voltage regions, from different test structure layouts, and from one technology node to another.  Alternatively, restoring a true Weibull slope by our data deconvolution is an easier way to improve overall TDDB projection. Without any long-term TDDB stresses, a real and improved failure rate projection can be quickly established. Furthermore, wafer processing can also be carefully diagnosed and meaningfully compared with our proposed big data generation and analytics method. A new diagnostics reliability concept is naturally embedded in our method. Therefore in addition to a simple \223pass\224 or \223fail\224 reliability judgment for qualification and process development, a precise reliability failure root cause analysis with a potential process fix guideline can also be provided by our method. As a consequence, a huge cost and time saving for new technology development and qualification can be achieved. Lastly, as mentioned above, our new method could be a prerequisite to developing a reliable TDDB acceleration model if significant die-to-die variation is present in our stress data A CKNOWLEDGMENT  The authors wish to acknowledge all the people working within IBM alliance programs for 32nm SOI and Bulk CMOS technology development. This work has been supported by the independent Bulk CMOS and SOI technology development projects at the IBM Microelectronics Division, Semiconductor Research & Development Center, Hopewell Junction, NY 12533 R EFERENCES  1 S  Y o ko g a w a S  U n o  I   K a to H   T s uch i y a T  S h im iz u an d M S a kam o to  49 th Annu. IEEE Rel. Phys Symp, 2011, pp. 149-154 2 F  C h e n  M  S h i n o s k y an d J. A i t k e n I EEE T r an s. o n El e c tr o n D e v i ce s v58, no 9, pp. 3089-3098, 2011   F e ll er  A n n   M a th Sta t   v 1 4  pp 389 40 0  1 943  4 A  T y ag i an d  M  A  B a y o u m i  I EEE T r an s o n Se mico n d u c to r  Manufacturing, v5, no 3, pp. 196-206, 1992 5 F  Ch e n e t  al I EEE I R PS 2 0 1 3  P I 1  1 1  5  6 K  Y  Y i a n g  e t al I EEE I R PS 2 0 1 0  p p  5 6 2 5 6 5      7 F  Ch e n e t  al I EEE I R PS 2 0 1 2  p p  6 A 4  1  6 A 4  9     212\212  212 k k mean x Exp xk xP or xx Exp xP    3A.1.11 212 


Figure 7  Portfolios of hosted payload and procured assets been presented in order to gain some level of con\002dence on the results that the tool outputs This discussion has focused on the validation of the scheduling algorithm by benchmarking it with operational schedules from the TDRSS Finally this paper has presented two case studies for future implementations of the TDRSS system Based on a demand forecast for the network a 002rst case study has considered the problem of selecting the frequency band to be supported and how to allocate them into the relay satellites It has been shown that maximum performance architectures require a mix of optical and RF payloads that support high throughput communications as well as reliable low data rate communications If the traditional procurement strategy is assumed 050NASA buys and operates the relay satellites\051 then monolithic architectures are preferable unless more than two high gain antennas render the resulting satellite con\002guration too complex In turn the second case study has extended this analysis by introducing architectures with hosted payloads It has been shown that according to the current available pricing model hosted payload architectures are clearly preferable than the traditional procurement strategy with cost savings between 15 to 30 for the same level of system performance It has then been discussed the advantages of having mixed procured and hosted payload architectures as a compromise to obtain networks with reduced lifecycle costs that can still address the requirements of highly sensitive and reliable applications Results have demonstrated that high data rate payloads 050speci\002cally optical payloads\051 are the best candidates to be hosted 050with savings up to 28%\051 thanks to their reduced mass and power requirement On the other hand low data rate communication payload should be allocated in privately owned satellites as they require bigger antennas and power ampli\002ers that increase the burden on the host platform Future Work The main streams of future work are as twofold On one hand additional features should be added to the model in order to better capture the complexity of the network con\002gurations 050e.g coupling between the costs of the communication payloads depending on the level of on-orbit processing the they perform\051 Additionally the size of the tradespace is currently limited to less than two thousand architectures due to computational limitations a stringent limitation given the possible combinations from the identi\002ed architectural decisions The solution currently under development is to include a genetic algorithm that alleviates this problem by iteratively generates new populations of architectures by combining the best previously evaluated networks On the other hand the other main stream of future work is related to exercising the tool in a variety of architectural decisions and mission scenarios In the presented case studies only geosynchronous constellations were considered although the tool allows comparing them with systems that place relays in MEO and LEO orbits Additionally the tool can also provide insight in valuing inter-satellite links and how the cost of putting in orbit their extra communication payloads can be leveraged by reducing the number of operating ground stations Moreover couplings between the payload allocation and fractionation strategy should also be further explored so as to understand if monolithic architectures still become preferable if relay satellites can be decomposed in clusters of independent antennas and payloads Finally supplementary what-if analysis can be conducted based on 0501\051 the demand forecast for the 2020-2030 time frame 0502\051 granted spectrum allocations to NASA and 0503\051 technology improvements that increase the spectral ef\002ciency of the communication payloads 12 


A PPENDIX Table 7  Acronyms Arch Architecture CER Cost Estimating Relationship DESDYNI Deformation Ecosystem Structure and Dynamics of Ice DSN Deep Space Network EIRP Equivalent Isotropically Radiated Power GEO Geosynchronous Orbit GRTG Guam Remote Ground Terminal GSFC Goddard Space Flight Center HD High De\002nition HP Hosted Payloads ISL Intersatellite Link ISS International Space Station LCC Life Cycle Cost LEO Low Earth Orbit MEO Medium Earth Orbit MIT Massachusetts Institute of Technology MOC Mission Operating Center MPCV Multi-Purpose Crew Vehicle NASA National Aeronautics and Space Administration NCCDS Network Control Center Data System NDA Non-Disclosure Agreement NEN Near Earth Network NISN NASA Integrated Services Network NOAA National Oceanic and Atmospheric Administration RF Radio Frequency SA Single Access SBRS Space based Relay Study SCaN Space Communication and Navigation SN Space Network STGT Second TDRSS Ground Terminal STK Systems ToolKit TDRSS Tracking and Data Relay Satellite System TT&C Telemetry Tracking and Command USGS United States Geological Survey WSGT White Sands Grount Terminal A CKNOWLEDGMENTS This project is funded by NASA under grant NNX11AR70G Special thanks for Gregory Heckler David Milliner and Catherine Barclay at NASA GSFC for their help getting the dataset and their feedback on our study R EFERENCES   National Aeronautics and Space Administration 223Space Communications and Navigation 050SCaN\051 Network Architecture De\002nition Document 050ADD\051 Volume 1  Executive Summary,\224 Tech Rep 2011   227\227 A v ailable http   www.nasa.gov/directorates/heo/scan   e a Sanchez Net Marc 223Exploring the architectural trade space of nasas space communication and navigation program,\224 in Aerospace Conference 2013 IEEE  2013   P Brown O  Eremenko 223Fractionated space architectures a vision for responsive space,\224 Tech Rep   S M V  D C E Teles J 223Overview of TDRSS,\224 Advances in Space Research  vol 16 pp 67\22676 1995   Analytical Graphics Inc A v ailable http   http://www.agi.com   D Selva Valero 223Rule-Based System Architecting of Earth Observation Satellite Systems by,\224 Ph.D dissertation Massachusetts Institute of Technology 2012   K D W  S P Davidson A 223Pricing a hosted payload,\224 in Aerospace Conference 2012 IEEE  2012   W J Larson and J R Wertz Space mission analysis and design  Microcosm Inc 1992   M Adinol\002 and A Cesta 223Contributed Paper Heuristic Scheduling of the DRS Communication System,\224 vol 8 1995   National Aeronautics and Space Administration Space Network Users Guide 050 SNUG 051  2007 no August 2007   e a Tran J J 223Evaluating cloud computing in the nasa desdyni ground data system,\224 in Proceedings of the 2nd International Workshop on Software Engineering for Cloud Computing  2011 B IOGRAPHY  Marc Sanchez Net is currently a second year M.S student in the department of Aeronautics and Astronautics at MIT His research interests include machine learning algorithms and rule-based expert systems and their suitability to the 002elds of system engineering and space communication networks Prior to his work at MIT Marc interned at Sener Ingenieria y Sistemas as a part of the team that develops and maintains FORAN a CAD/CAM/CAE commercial software for shipbuilding Marc received his degrees in both Industrial engineering and Telecommunications engineering in 2012 from Universitat Politecnica de Catalunya Barcelona Dr Daniel Selva received a PhD in Space Systems from MIT in 2012 and he is currently a post-doctoral associate in the department of Aeronautics and Astronautics at MIT and an adjunct Assistant Professor in the Sibley School of Mechanical and Aerospace Engineering at Cornell University His research interests focus on the application of multidisciplinary optimization and arti\002cial intelligence techniques to space systems engineering and architecture Prior to MIT Daniel worked for four years in Kourou 050French Guiana\051 as a member of the Ariane 5 Launch team Daniel has a dual background in electrical engineering 13 


and aeronautical engineering with degrees from Universitat Politecnica de Catalunya in Barcelona Spain and Supaero in Toulouse France He is a 2007 la Caixa fellow and received the Nortel Networks prize for academic excellence in 2002 Dr Bruce Cameron is a Lecturer in Engineering Systems at MIT and a consultant on platform strategies At MIT Dr Cameron ran the MIT Commonality study a 16 002rm investigation of platforming returns Dr Cameron's current clients include Fortune 500 002rms in high tech aerospace transportation and consumer goods Prior to MIT Bruce worked as an engagement manager at a management consultancy and as a system engineer at MDA Space Systems and has built hardware currently in orbit Dr Cameron received his undergraduate degree from the University of Toronto and graduate degrees from MIT Dr Edward F Crawley received an Sc.D in Aerospace Structures from MIT in 1981 His early research interests centered on structural dynamics aeroelasticity and the development of actively controlled and intelligent structures Recently Dr Crawleys research has focused on the domain of the architecture and design of complex systems From 1996 to 2003 he served as the Department Head of Aeronautics and Astronautics at MIT leading the strategic realignment of the department Dr Crawley is a Fellow of the AIAA and the Royal Aeronautical Society 050UK\051 and is a member of three national academies of engineering He is the author of numerous journal publications in the AIAA Journal the ASME Journal the Journal of Composite Materials and Acta Astronautica He received the NASA Public Service Medal Recently Prof Crawley was one of the ten members of the presidential committee led by Norman Augustine to study the future of human space\003ight in the US Bernard D Seery is the Assistant Director for Advanced Concepts in the Of\002ce of the Director at NASA's Goddard Space Flight Center 050GSFC\051 Responsibilities include assisting the Deputy Director for Science and Technology with development of new mission and measurement concepts strategic analysis strategy development and investment resources prioritization Prior assignments at NASA Headquarters included Deputy for Advanced Planning and Director of the Advanced Planning and Integration Of\002ce 050APIO\051 Division Director for Studies and Analysis in the Program Analysis and Evaluation 050PA&E\051 of\002ce and Deputy Associate Administrator 050DAA\051 in NASA's Code U Of\002ce of Biological and Physical Research 050OBPR\051 Previously Bernie was the Deputy Director of the Sciences and Exploration Directorate Code 600 at 050GSFC\051 Bernie graduated from Fair\002eld University in Connecticut in 1975 with a bachelors of science in physics with emphasis in nuclear physics He then attended the University of Arizona's School of Optical Sciences and obtained a masters degree in Optical Sciences specializing in nonlinear optical approaches to automated alignment and wavefront control of a large electrically-pumped CO2 laser fusion driver He completed all the course work for a PhD in Optical Sciences in 1979 with emphasis in laser physics and spectroscopy He has been a staff member in the Laser Fusion Division 050L1\051 at the Los Alamos National Laboratories 050LANL\051 managed by the University of California for the Department of Energy working on innovative infrared laser auto-alignment systems and infrared interferometry for target alignment for the HELIOS 10 kilojoule eight-beam carbon dioxide laser fusion system In 1979 he joined TRW's Space and Defense organization in Redondo Beach CA and designed and developed several high-power space lasers and sophisticated spacecraft electro-optics payloads He received the TRW Principal Investigators award for 8 consecutive years Dr Antonios A Seas is a Study Manager at the Advanced Concept and Formulation Of\002ce 050ACFO\051 of the NASA's Goddard Space Flight Center Prior to this assignment he was a member of the Lasers and Electro-Optics branch where he focused on optical communications and the development of laser systems for space applications Prior to joining NASA in 2005 he spent several years in the telecommunication industry developing long haul submarine 002ber optics systems and as an Assistant Professor at the Bronx Community College Antonios received his undergraduate and graduate degrees from the City College of New York and his doctoral degree from the Graduate Center of the City University of New York He is also a certi\002ed Project Management Professional 14 


 





 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


