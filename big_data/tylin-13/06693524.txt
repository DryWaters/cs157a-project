 
Analysis of Big Data Technologies and Methods 
Abstract 
Query Large Web Public RDF Datasets on Amazon Cloud Using Hadoop and Open Source Parsers  Ted Garcia and Taehyung \(çGeorge  Department of Computer Science California State Univ ersity, Northridge Northridge California, 91330 
Extremely large datasets found in Big Data projects are difficult to work with using conventional databases, statistical software, and visualization tools. Massively parallel software, such as Hadoop, running on tens, hund reds, or even thousands of servers is more suitab 
ted.garcia.574, twang}@csun.edu   
le for Big Data challenges. Additionally, in order to achieve the highest performance wh en querying large datasets, it is necessary to work these datasets at rest without preprocessing or moving them in to a repository Therefore, this work will analyze tools and techniques to overcome working with large datasets at rest. Parsing and querying will be done on the raw dataset - the untouched Web Data Commons RDF files. Web Data Commons comprises five billion pages of web pages crawled from the Internet. This work will analyze available tools and appropriate methods to assist the Big Data developer in working with these extremely large semantic RDF datasets. Hadoop, open source parsers, and Amazon Cloud services will be used to data mine these files. In order to assist in further discovery recommendations for future research will be included 
Keywords-big data; Hadoop; Amazon cloud computing; RDF Jena; NXParser; Any23; Semantic Web; Map/Reduce; open source software 
I 
 
 I NTRODUCTION  Querying large datasets has become easier with Big Data technologies such as Hadoop s MapReduce. Large public datasets are becoming more availa ble and can be found on the Amazon Web Service \(AWS\Cloud. In particular, Web Data Comm has extracted and posted RDF Qua ds from the Common Crawl Corpus [2 un d on AW S [3  wh ich  comprises over five billion web pages of the Internet Technologies and methods are in their infancy when attempting to process and query these large web RDF datasets. For 
example, within the last couple of years, AWS and Elastic MapReduce \(EMR provide d processing of large files with parallelization and a dist ributed file system. RDF technologies and methods have exist ed for some time and the tools are available commercially and open source. RDF Parsers and databases are being used successfully with moderately sized datasets. However, the use and analysis of RDF tools against large datasets, especially in a di stributed environment is relatively new. In order to assist the BigData developer, this work explores several open source parsing tools and how they perform in Hadoop on the Amazon Cloud. Apache Apache Jena RIOT/ARQ and Sem anticW e b s NxParse r [7 are open source parsers that can process the RDF quads 
contained in the Web Data Comm ons files. It should be known that because these parsers support limited query functions, this work will analyze extract and parse functionality only Therefore, more comprehensive tools should be used or included for complex queries Currently, using conventional databases and other software tools have not proven efficient when working with large datasets. Therefore, we suggest wo rking with these datasets at rest without preprocessing or impor ting them into a repository In order to address this challe nge this work will show how current RDF parsers can be used with Hadoop running on Amazon Cloud services. Furthermore, suggestions will be given on approaches and methods to 
allow the best use of these parsers and technologies for a variety of scenarios In Section 2, we introduce bi g data technologies along with Amazon cloud technologies, Hadoop Map/Reduce, and Parsers In Section 3, we describe related works that include NQuads SPARQL, SimpleDB, and We b Data Commons. Section 4 discusses the analysis and design of a query program developed for this study that will compare performance of the three open source parsers. In Section 5, we present the analysis of the results of testing. Finally the conclusion and discussion regarding future research is addressed in Section 6 II B 
 
IG D ATA T ECHNOLOGIES  A 2011 McKinsey paper suggests suitable technologies include A/B testing, association rule learning, classification cluster analysis, crowd sourcing, data fusion and integration ensemble learning, genetic al gorithms, machine learning natural language processing neural networks, pattern recognition, predictive mode ling, regression, sentiment analysis, signal processing supervised and unsupervised learning, simulation, time series analysis and visualization Additional technologies being applied to big data include massively parallel-processing \(MPP\databases, search-based applications, data-mining grids 
distributed file systems distributed databases, cl oud computing platforms, the Internet and scalable storage systems Even though there are many su itable technologies to solve Big Data challenges as indicated in the list above given by the McKinsey paper, this work will expl ore a handful of the more popular ones, in particular the Amazon Cloud, Hadoop's MapReduce, and three open-source parsers 
A Amazon Cloud Technologies 
Amazon Web Services \(AWS\oud platform, is one of today's most popular and is sho wn in Fig. 1. It already has Hadoop's MapReduce Framework available for use. We 
 
2013 IEEE Seventh International Conference on Semantic Computing 978-0-7695-5119-7/19 $26.00 © 5119 IEEE DOI 10.1109/ICSC.2013.49 244 
2013 IEEE Seventh International Conference on Semantic Computing 978-0-7695-5119-7/19 $26.00 © 5119 IEEE DOI 10.1109/ICSC.2013.49 244 


B Hadoop's Map/Reduce C Hadoop Distributed File System \(HDFS D Parsers 
Amazon Services Architecture  Figure 2 Map/Reduce Directed Acyclic Graph store the large RDF files in Am azon Simple Storage Service S3 azon Elastic MapReduce EMR   framework running on the Am azon Elastic Compute Cloud EC2 pa rse rs  
   
  
 Figure 1 The Hadoop Map/Reduce architecture provides a parallel processing framework that might be a better solution than multithreading. Multith reading requires adep t knowledge and skill for the programmer in that coordinating each thread and critical section can cause many problems. Multithreading requires semaphores, locks, etc. which require tedious testing to ensure there is no race or deadlock conditions. Hadoop eliminates the shared state completely and reduces the issues mentioned above This is the fundamental concept of functional programming. Data is explicitly passed between functions as parameters or return values which can only be changed by the active function at that mo ment. In this case functions are connected to each other in the shape of a directed acyclic graph. Since there is no hidden dependency \(via shared state functions in the directed acyclic graph can run anywhere in parallel as long as one is not an ancestor of the other Map/reduce is a specialized directed acyclic graph which can be used for many purposes [1  function which transforms a piece of data into some number of key/value pairs. Each of these elem ents will then be sorted by their key and reach to the same node, where a çreduce function is used to merge the values \(of the same key\to a single result as shown in Fig. 2. The code snippet below shows typical map and reduce func tions. Several may be chained together to implement a pa rallel algorithm for different use cases The distributed file system is another innovation essential to the performance of the Hadoop framework. HDFS can handle large files in the giga bytes and beyond with sequential read/write operation. These large files are broken into chunks and stored across multiple data nodes as local files There is a master çNameNodeé to keep track of overall file directory structure and the placement of chunks. This NameNode is the central contro l point and may re-distribute replicas as needed. DataNode work s all its chunks to the NameNode at bootup To read a file, the client API will calculate the chunk index based on the offset of the file pointer and make a request to the NameNode. The NameNode will reply which DataNodes has a copy of that chunk. From th is point, the client contacts the DataNode directly without going through the NameNode To write a file, client API will first contact the NameNode who will designate one of th e replicas as the primary \(by granting it a lease\The response of the NameNode contains who is the primary and who are the secondary replicas. Then the client pushes its changes to all DataNodes in any order, but this change is stored in a buffer of each DataNode. After changes are buffered at all DataNodes, the client sends a commité request to the primary, which determines an order to update and then pushes this or der to all other secondaries After all secondaries complete the commit, the primary will respond to the client about the success. Fig. 3 shows HDFS architecture The Web Data Commons files are stored in RDF NQuads  n only be parsed di rectly with a small handful of parsers. Triples, the more popular format is supported by almost all RDF parsers and tools  
 map\(input_record  emit\(k1, v1  emit\(k2, v2   reduce \(key, values aggregate = initialize while \(values.has_next aggregate = merge\(values.next  collect\(key, aggregate  
245 
245 


002 002 002 002 002 002 002 002 002 002 002 002 002 002 
HDFS Architecture \(Apache Hadoop Apache Jena ARQ/RIOT is a Java framework for building Semantic Web applications. Jena provides a collection of tools and Java libraries to help develop semantic web and linkeddata applications, tools and servers The Jena Framework includes 
 
 
 Figure 3 an API for reading, processing and writing RDF data in XML, N-triples and Turtle formats - this work is using this module An ontology API for handling OWL and RDFS ontologies A rule-based inference engine for reasoning w ith RDF and OWL data sources Stores to allow large numbers of RDF triples to be efficiently stored on disk A query engine compliant with the latest SPARQL specification Servers to allow RDF data to be published to other applications using a variety of protocols, including SPARQL In April 2012, Jena graduated from the Apache incubator process and was approved as a top-l evel Apac The Jena project has contributed extensively to the RDF movement including the following important principle. "RDF has an XML syntax and many who are familiar with XML will think of RDF in terms of that syntax. This is a mistake. RDF should be understood in terms of its data model. RDF data can be represented in XML, but un derstanding the syntax is secondary to understanding the data m   Anything To Triples \(Any23  and a command line tool that extracts structured data in RDF format from a variety of Web documents RDF/XML, Turtle, Notation 3 RDFa with RDFa 1.1 prefix mechanism Microformats: Adr, Geo, hC alendar, hCard, hListing hResume, hReview, License, XFN and Species HTML5 Microdata: \(such as Schema.org CSV: Comma Separated Values with separator autodetection Apache Any23 is used in ma jor Web of Data applications such as sindice.com and sigma. It is written in Java and licensed under the Apache License. Apache Any23 can be used in various ways As a library in Java applications that consume structured data from the Web As a command-line tool for extracting and converting between the supported formats As online service API available at any23.org Any23 is based on the Sesame Parser and appears to have ended its incubator status in August of 2012 However, at this time, it is not listed in the fo rmal lis NxParser is a Java open source, streaming, non-validating parser for the Nx format, where x = Triples, Quads, or any other quantity. For more details see the specification for the NQuads format, an extensi on of the N-Triples RDF format Note that this parser handles any combination or number of NTriples syntax terms on each line \(the number of terms per line can also vary NxParser performs very well. The following was indicated on their website. It ate 2 mi llion quads \(~4GB, \(~240MB Win7, 2.16 GHz\n approximately 1 minute and 35 seconds. Overall, it's more than twice as fast as the previous version when it comes to reading Nx It comes in two versions: lite and not-so-lite. The latter is provided "as-is" and comes with many features, most of which is not needed for th is work. This work used the lite version There is some code for batch sorting large-files and various other utilities, which some may fi nd useful. If you just want to parse Nx into memory, use the lite version The NxParser is non-validating, meaning that it will happily eat non-conformant N-Triples. Also, the NxParser will not parse certain valid N-Triples files where \(i\erms are delimited with tabs and not spaces; \(ii by a space NxParse r com es from  YARS which was created by DE RI Galway DERI Galway, the Digitial Enterprise Research Institute is an institute of the National University of Ireland, Galway which is located in Galway, Ireland. The focus of the research in DERI Galway is on the Semantic We YARS stands for "Yet another RDF store". YARS is a data store for RDF in Java and allows for querying RDF based on a declarative query language, whi ch offers a somewhat higher abstraction layer than the APIs of RDF toolkits such as Jena or Redland. YARS uses Notation3 as a way of encoding facts and queries  III R ELATED W ORKS  This work is unique. Many days of research were spent looking for exactly the same ki nd of work. The IEEE was searched as well as the Internet but nothing was found that deals with these three parsers in Amazon Hadoop 
               
246 
246 


 Figure 4 
Amazon EC2 Computing 
 
Query Program Architecture  Furthermore, there are not many RDF benchmarks for BigData technologies. Let this work be one of the first. Even though this work is unique, the fo llowing three studies have similarities to it The Data Intensive Query Processing for Large RDF Graphs Using Cloud Computing w o rk pr ocesse d RDF  with Hadoop in the Cloud like this work but it used a database and triples instead of no database and NQuads. Mohammad's work is also not concerne d with the Web Data Commons content containing most of the Internets pages of RDF Mohammad's work described an al gorithm that determines the performance cost of executing a SPARQL query on the RDF The RDF query language SPARQL is used to query an RDF repository. In Mohammad's work RDF needs to be converted and it is handed over to the Hadoop cluster to process. In some ways Mohammad's work is a performance analysis of an algorithm that uses Hadoop. On the other hand, this work compares the performance of three parsers being testing on the Amazon application services The RDF on Cloud Number Nine used SimpleDB on the Amazon Cloud. The team created an RDF store which acts as a back end for the Jena Semantic Web framework and stores its data within the SimpleDB. They stored RDF triples and queried them while checking performance against the Berlin SPARQL Benchmark to evaluate their solution and compare it to other state of the art triple stores. This work is a great example of using Jena and the Amazon Cloud to query RDF. However, this work, like the one above, use databases to store and process RDF. There are still challenges using databases when data sizes are in gigabytes, terabytes, and beyond that these other studies do not address The RDF Data Management in the Amazon Cloud  work also used SimpleDB in the Amazon Cloud. This team attempted to use various indexing techniques to try to guess the query path in order to optimize performance Finally, the Web Data Commons Ö Extracting Structured Data from Two Large Web Corp  work ed with th e content of the entire Internet represented by RDF Quads and showed the popularity of vari ous format types found within the pages IV  A NALYSIS AND D ESIGN OF THE Q UERY P ROGRAM  This section will cover the design of the program that compares performance of the three open source parsers by querying large RDF data sets. Included will be an overview of the process and methods of performing such work on the Amazon Cloud using Java t echnology. This section will describe the components of the system used to compare the parsers as shown in Fig. 4 and Table I. This work will query the Web Data Common RDF extraction from the Common Crawl Corpus which comprises ov er five billion web pages on the Internet. The query will be p erformed by three open source parsers running separately on Hadoop in the Amazon Cloud TABLE I C OMPONENT D ESCRIPTIONS  RDF Files Web Data Commons' RDF NQuad files Amazon EC2 Highly scalable, parallel computing infrastructure Amazon S3 Scalable and distributed storage running on EC2 Amazon EMR Implementation of Hadoop's MapReduce services running on EC2 Parsers Jena, NXParser, and Any23 open source parser programs used to manipulate the RDF NQuad files Java Language used to write the query program with Hadoop libraries that compares parser performance 
Amazon S3 Storage Amazon Hadoop EMR Java Application Jar RDFParseParallelMain.java All Parser Jars \(Jena NX, Any23 Hadoop Library Jars Job Data Input RDF Quads Job Data Output Summarization 
 
Component Description 
HDFS 
247 
247 


     
        
They are run separately to compare their performance extracting and parsing the RDF NQu ad files. This will allow us to perform analytics on the entire Internet by scanning the RDF files and performing query functions on the NQuads. The high level process is described below The basic steps that were conducted to obtain the results of this work are described as follows 1 Start with raw web pages containing RDFa from the Common Crawl Corpus 2 Obtain compressed RDF NQuads converted from RDFa of the Common Crawl Corpus - done by Web Data Commons \(WDC 3 Decompress WDC files and store them on Amazon S3- done by this work using standard Amazon file utilities 4 Run Amazon Elastic Map Reduce \(Hadoop\d parsers on decompressed files using this works program: RDFParseParallelMain.java 5 Combine Hadoop's distribu ted analytics output files into one file and create graphs of results  How Hadoop was used  1 Determine the number of jobs needed to answer a query 2 Minimize the size of in termediate files so that data copying and network data transfer is reduced 3 Determine number of reducers  Note that usually we run one or more MapReduce jobs to answer one query. We use the ma p phase to select data and the reduce phase to group it Common Crawl is an attempt to create an open and accessible crawl of the web. Common Crawl is a Web Scale crawl, and as such, each version of the crawl contains billions of documents from the vario us sites that were successfully able to crawl. This dataset can be tens of terabytes in size, making transfer of the crawl to interested third parties costly and impractical. In addition to this performing data processing operations on a dataset this large requires parallel processing techniques, and a potentially large com For this reason, we used the RDF subset created by Web Data Commons More and more websites embed structured data describing products, people, organizations, places, events, resumes, and cooking recipes into their HTML pages using markup formats such as RDFa, Microdata and Microformats. The Web Data Commons project extracted all Microformat, Microdata and RDFa data from the Common Crawl web corpus. The corpus is the largest and most up-to-date web corpus that is currently available to the public. The corpus provides th e extracted data for download in the form of RDF-quads and also in the form of CSV-tables for common entity types \(e.g. product organization, location, etc In addition, Web Data Commons calculates and publishes statistic s about the deployment of the different formats as well as the vocabularies that are used together with each format Web Data Commons has extracted all RDFa, Microdata and Microformats data from the August 2012 and the 2009/2010 Common Crawl corpu s. This work will use the August 2012 RDF files which is about 100 gigabytes. Web page s are included into the Common Crawl corpora based on their page rank score, thereby making the crawl's snapshots of the current popular part of the Web. For the future, Web Data Commons plans to rerun their extraction on a regular basis as new Common Crawl corpora are becoming av This work will use several services on the Amazon Cloud The Hadoop framework implemented by Amazon Elastic Map Reduce \(EMR analyze the large set of RDF data. The EMR provides fine-grai n metrics of many aspects of the running job. From these metrics, we produce the test results charts. Since WDC has done some of the difficult work for us we start with fairly structured and validated RDF NQuad files These files are stored in the Amazon S3 After "jarring" up the RDFParseParallelMain program with th e parsers jar files, this final jar file is saved to the S3 as well We also setup an output direct ory for the query result files To run RDFParseParallelMain we used the EMR control panel and start a new job. The EMR will run the RDFParseParallelMain program on as many processors as we indicate and write output files to the S3 The capability of the program RDFParseParallelMain is to twofold: count entries for each node type in the NQuad, i.e subject, predicate, object, or context or count occurrences of a query string in a specifi ed node type. The parser comparison testing was done using the former capability of counting occurrences of the node type. We have provided extensive argument passing so we could run the tests in various modes using each of the parsers separately The map and reduce functions are fairly simple. The map for the testing is key = node ty pe; value = count. The map for the query is key = query stri ng; value = count. The reduce function simply adds the reoccurr ing words and outputs the total to the output file. The comp lexity and main processing in this program is in the parsers. Each parser must break down the NQuad into separate nodes. Once that is done, the program passes the node to the reduce functi on where it eventually gets counted The final Java jar file was a jar of jars along with the main Java program. In other words the Hadoop, Jen a, NXParser and Any23 application jar files, sev eral for each, were all jarred" together with the RDFParseParallelMain Java program. This composite jar file is what is used by the Amazon EMR to run the tests 
A Process Steps B Common Crawl Corpus C Web Data Commons \(WDC D Amazon Cloud E RDFParseParallelMain 
248 
248 


The testing of each parser is done separately. This ensures there is sufficient control to yield more accurate performance results. The RDFParseParallelMain pr ogram was written such that no processing is done within it. Instead, all processing was handed over to the parsers. The RDFParseParallelMain only provided transport between the input and the Hadoop "reduce method. More specifically, the Any23 and Jena parsers used callbacks to process whereas the NXParser parser used a tight loop The RDF NQuad files sizes were 1, 4, 16, and 24 gigabytes and the CPU count is 10, 24, 64, and 90 CPU's. These quantities are not entirely random. Earlier testing, not documented in this work, gave a basis to establish amounts for this testing. The file sizes were established based on a somewhat smooth progression The last file size, which ideally would have been 32, of 24 was used to reduce the cost of processing and uploading the files. The CPU count was established with similar justificat ions since Amazon has a significant cost increase over 100 CPU's. Unfortunately, cost became an issue in determining how much of the Amazon processing power would be reasonable for this project. But it appears these constraints did not deter from a good understanding of the parsers' behaviors In the next section, we will analyze the test results and what has been concluded Hopefully, the results and testing methodology will be useful to other Big Data and Semantic Web practitioners V R ESULTS  We will analyze two aspects of the results, i.e. the performance of the parsers and the CPU cost of each parser The performance comparison will include looking at the performance curve to see where each parser did their best. For example, parser X did its best when processing the 4.7 GB moderately sized file with 64 CPU's. In the same way, we can look at the CPU cost curve and see where the best CPU cost occurs Before looking at the results, it is important to understand a few points. Statistical integrity was not achieved in this testing due to the cost to run the jo bs. However, due to the stability and consistency of the Amazon Cloud infrastructure and how the tests were performed, there is significant value in the findings. The main question is if we rerun the tests again will we get significantly different resu lts. As mentioned before prior tests were conducted and they show minimal variance Therefore, for this experiment, outcomes of the testing are taken with full face value, even though they are not statistically proven. We have suggested in the future work section to run more tests for statist ical validity In general, the testing produced unexpected results. We expected to see normal patterns in the charts. Normal curves did not show in the results. Ho wever, the results seem to have some consistency within the overall testing. In other words, the charts show unusual behavior of ma ny of the tests but there seems to be reasonable justification. Each parser seems to respond to the CPU and file size combinations in differing ways. This may suggest that the parsers can be used optimally under certain scenarios Fig. 5 allows us to compare and contrast performance among the parsers. For the most part, Jena is the slowest overall. Any23 and NXParser out-performed each other in different scenarios of file size and CPU count. For the important test where the largest size of file meets low CPU count and therefore CPU utilization, NXParser seems to have edged out the others If we look at each file size and then the CPU count we can see that there is a different winner for each scenario. For the 0.7 GB file the NXParser parser was th e fastest in the extreme cases of 10 CPU's and 90 CPU's. It was the slowest in the middle CPU counts. Then the NXParser parser was the fastest in the 24 and 90 runs of the 4 7 GB file and slowest on 64 and the middle on 10. For the final 15.9 GB file, the NXParser parser was the fastest at 10 and 24 CPU's In the following, we explore each of the parsers separately and their individualized per formance curves. As mentioned before, each parser does better or worse at a particular file size to CPU count permutation. We have taken the parser performance chart \(Fig. 5 parser to ensure the indivi dualization is apparent   Figure 5 
  
Parser Performance  Figure 6 Parser Performance - Any23 
 
F Test Design 
 
249 
249 


 
    
 Figure 7  It is not always enough to see how fast a program runs but also the cost of runn ing it. Especially when running on cloud environments where there is a fee for everything and costs can grow quickly. The next section compares the three parsers CPU usage and therefore the cost to run the parser and are shown in Fig. 9 The Parser CPU Cost by File Size chart, shown in Fig. 10 correlates with the Parser Performance chart. It shows the total time spent running the job by adding together the CPU utilization for each of the CPU's. The Fig. 9 chart shows that the parsers CPU usage is fairly close for the smaller file sizes but not for the larger files. There seems to be a dip at the 90 CPU test for all the parsers. Th e dip correlates to the performance graph of each of the processors counts. This may indicate there is a diminishing return as the CPU's increase for each file size Reviewing both charts it is very clear that Jena utilizes the most CPU than the other two The NXParser seems to be the least greedy when utilizing co mpute resources. Therefore the NXParser would be the least expensive to run in most cases Fig 10. shows CPU utilization per parser based on different number of processors. This last chart shows CPU utilization from a slightly different perspectiv e. The chart is similar to the CPU utilization by file size as it shows the same dip in the 90 CPU boundaries. This chart may help to strengthen the what is seen in Fig 9 
Parser Performance - NXParser  Figure 8 Parser Performance - Jena  Figure 9 Parser CPU Cost by File Size  Figure 10 Parser CPU Cost by File Size The Any23 parser as shown in Fig. 6 was in the middle of almost all permutations except it was faster in the 4.7 GB, 10 CPU and 15.9 GB, 64 and 90 CPU runs. Strangely, it was much slower in the 4.7 GB, 90 CPU run. For the smallest file it is slow at first, gains mo mentum and then quickly slows down again. Apparently the overhead of CPU's slows it down. For the medium file size, it optimizes performance at the 24 CPU count. For the large file, it improves over the initial progression of CPU count but falters slightly at the 90 CPU count The NXParser as shown in Fig. 7 and Any23 parser had very similar performance characteris tics which make it difficult to see which parser would work better for particular scenarios It appears the NXParser parser was faster more times than Any23. The NXParser also has the best CPU usage index as noted in the next section. The NXParser shows normal time increases for the small file but speeds up when using 90 CPU's The optimal time on the medium size file is 24 CPU's and does not take advantage of more CPU's. The large file performance improves as expected with more CPU's until the use of 90 CPU's where there is a minor dip The Jena parser as shown in Fig 8, for the most part, was the slowest of all and used the most CPU time, especially in the 15.9 GB file runs. Oddly, it was the best performer in the 0.7 GB, 64 CPU run. For the small file it progressed negatively as the CPU count increased. On the medium file size performance increased as the number of CPU's increased until the 90 CPU count. Finally, Jena took full advantage of throwing CPU's at it 
250 
250 


                    
VI C ONCLUSIONS AND F UTURE W ORK  It is clear that the NX Parser is the most efficient and may be the best for applications needi ng no validation. The Any23 parser is comparable to the NX Parser in performance but not in efficiency In order to understand why Jena is the slowest, the Jena parser has a larger codebase and is performing validations while parsing. Any23 and th e NX Parser are doing simple format validations before writing to the destination output Computing speed, dataset size and quality of output are the critical parameters to be consid ered when determining an application's needs. Usually we want processing to be instantaneous regardless of th e dataset size but this is not realistic. If the application has extremely large datasets and quality is critical, then we will have to give up speed and use the Jena processor. For some extremely large datasets, Jena may not be feasible. On the other hand, if the application with extremely large datasets is not s ubject to quality requirements or data validation has already been performed on the dataset e.g. the Web Data Commons files, then the NX Parser or Any23 are recommended. A pre-validation step using a specialized format checker may be required for some applications that require both quality and performance when using the NX Parser or Any23 It is also clear that there is acceptable speedup when using the Amazon EMR with the smaller file sizes, i.e. 0.7 and 4.7 GB. At the large 15.9 GB file size, the performance seemed to level off and the results were similar or worse than the smaller files. However, it is not clear where the best or optimal cost to performance curve might be. That is up to more testing of the RDFParseParallelMain program which could be done in some future work. But it is interesting to witness the various performance curves within the scenarios of file size and CPU count. The charts can be used for each parser as a starting point when the file size is known. Thi s could prevent overspending of CPU usage for any given test Hopefully, this work has provided a rich set of information about tackling Big Data and Semantic Web challenges. More specifically, when your challe nge is to query large RDF datasets, the determination of which parser to use may now be much easier. This work has been an exhausting challenge. But with any activity, there is always more to do. Next we will highlight some of the areas where future investigation could be focused This work did not have the time or budget to perform extensive testing. Even though the testing program and methodology are very effec tive and produced reasonable results, more testing could solidify the observed trends. Larger dataset sizes would put higher stress on the parsers and CPU's Using more and faster CPU's could result in a better understanding of the speedup curve. CPU analysis could also indicate where there is a diminishing return. In other words, the use of different CPU models coul d give the optimal approach for running certain application types Even though this work was performed on pre-validated datasets, un-validated datasets coul d be tested and, along with the above observations, a be tter heuristic could be developed for the determination of use for a variety of applications. In other words, there may be combinations of parsers and validators that could run in a sequence of steps optimized for a particular application. And, there are many CPU combinations that could be considered for the best result. For example, a very large dataset requiring an av erage level of validation might optimize at 200 large stan dard, first-generation instances. A chart could be developed based on the application or scenario type, as well R EFERENCES   
 
IEEE DanaC 2012 IEEE How Hadoop Map/Reduce works 
Retrieved October 2012, from Web Data Commons http://webdatacommons.org  Retrieved October 2012, fr om Common Crawl http://commoncrawl.org  Retrieved February 2013, from Amazon EC2 https://aws.amazon.com/ec2  Retrieved February 2013, from Amazon EMR http://aws.amazon.com/elasticmapreduce  Retrieved October 2012, from Apache Any23: http://any23.apache.org  Retrieved 02 2013, from Apache Jena:  http://jena.apache.org/index.html  Retrieved October 2012, from NX Parser https://code.google.com/p/nxparser/ redirected from http://sw.deri.org/2006/08/nxparser  Retrieved April 2013, from WikiPedia: Big Data http://en.wikipedia.org/wiki/Big_data  Retrieved February 2013, from Amazon S3: http://aws.amazon.com/s3  Retrieved October 2012, from RDF NQuads http://sw.deri.org/2008/07/n-quads  Retrieved November 2013, from Apache Jena Tutorial http://jena.apache.org/tutorials/rdf_api.html  Retrieved October 2013, from Any23 Incubator http://incubator.apache.org/projects/index.html#graduated  Retrieved October 2012, from NxParser Deri http://semanticweb.org/wiki/DERI_Galway  Retrieved October 2012, from Nxparser Yars  Mohammad Farhan Husain, e a. \(2010\Intensive Query Processing for Large RDF Graphs Using Cloud Computing   Raffael Stein, e. a. \(2009\mber Nine  Bugiotti, F. \(2012\ Data Management in the Amazon Cloud   Hannes M¸hleisen, e. a. \(2012\Web Data Commons Ö Extracting Structured Data from Two Large Web Corpora   Ho, R. \(2008, 12 16 Retrieved April 2013, from DZone: http://architects.dzone.com/articles/how-hadoopmapreduce-work  Retrieved January 2013, from Apache Hadoop http://hadoop.apache.org  
251 
251 


a Outside View b Inside View Figure 13  Mock cave test site Tunnel extends approximately 300m Cave 003oor is covered in rocky material to emulate planetary terrain Site contains surface terrain and tunnel inside building Exploration Performance Experiments A comparison of exploration performance between distributed centralized and uncoordinated task allocation was conducted using a 2-robot team of 2D mapping robots For the uncoordinated runs tasks were randomly assigned to a robot and the robot randomly decided whether to keep the task not taking into account any costs associated with that robot's performance of the task Maps at a resolution of 0.05 meters per pixel were built from 5 runs of each type Each run lasted 15 minutes The operator indicated tasks that should be performed and the system assigned these tasks to a robot In 3 out of 5 runs for each set the 002rst selected task was in the direction of the bridge in the other 2 it was in the direction of the dead-end to the right of the starting position in Figure 12 The same operator selected tasks for all runs Tables 1 2 and 3 show results for each run Percent explored is the percentage of the explorable area as determined from the ground truth model that the robot team explored Unique to total is the ratio of the area explored by a single robot to the total area explored by the team This metric gives a sense of how much overlapping work the robots are doing The average percent explored for runs with a 002rst task in the direction of the bridge was 68 and for runs with the 002rst task in the direction of the dead-end 67 The average ratio of unique to total explored for these cases was 0.45 and 0.44 respectively The average over all runs was 67 of explorable area covered and a ratio of 0.45 unique to total explored area Figure 14 shows merged maps for the runs with the largest and smallest explored areas in this experiment Table 1  2D Mapping Results Distributed Run Bridge 1st  Explored Unique:Total 1 1 80 0.57 2 1 52 0.43 3 1 94 0.45 4 0 97 0.76 5 0 60 0.37 Mean 77 0.52 Std Dev 20 0.15 Table 2  2D Mapping Results Centralized Run Bridge 1st  Explored Unique:Total 1 1 75 0.58 2 0 52 0.46 3 1 55 0.29 4 0 49 0.15 5 1 62 0.59 Mean 59 0.41 Std Dev 10 0.19 Table 3  2D Mapping Results Uncoordinated Run Bridge 1st  Explored Unique:Total 1 1 51 0.20 2 0 54 0.26 3 1 73 0.49 4 0 87 0.65 5 1 68 0.42 Mean 67 0.41 Std Dev 15 0.18 These results show high performance variation within run sets and do not show signi\002cant difference between sets The direction of the 002rst assigned task did not signi\002cantly affect results Limited navigation and path planning capabilities on individual robots common for all runs likely introduces signi\002cant randomness If the robots could more reliably complete their assigned tasks differences between task allocation strategies would likely become more evident Exploration of larger areas could also make differences clearer as more tasks would need to be assigned The lack of signi\002cant differences between allocation methods is somewhat encouraging however It indicates that distributed task allocation the method believed to be most promising for planetary missions does not perform any worse than other methods in early tests The uncoordinated method while by far the simplest would fail once robots with different capabilities are introduced Failure would occur for example if a 3D modeling task were assigned to a 2D mapping robot Mapping and Modeling Experiments An experiment including both 2D mapping and 3D modeling was conducted at the patio test site In this experiment the two 2D mapping robots were operated as described in section 6 A mapped area was then selected by the operator for 3D modeling and the 3D modeling robot was sent to complete that task There was no time limit on the run Figure 15 shows 9 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


