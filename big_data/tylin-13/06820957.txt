en-US Proceedings  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US 201 3 International Conference  on Cloud Computing and Big Data  en-US C LOUDCOM ASIA  20 13  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US 1 6 1 8  D e cem ber  201 3  en-US Fuzhou, Fujian, China  


B  Information Supression During Clerical Review What information is disclosed during clerical review has much impact on the risk of disclosure during record linkage The goal is to only display the meaningful differences between the variables needed for record linkage and suppress other information \(Fig. 1\.  For example, difference between IDs \(e.g. SSN\ are recoded into number of different digits and transposes.  DOB comparisons are made on an elementto-element basis for month, day, and year taking into account transposes both within one element and between elements Conveying meaningful similarity information in names without fully disclosing it is the most difficult.  More research on meaningful differences for record linkage is needed. In our evaluation, we start with understanding the risk of fully disclosing names.  We show in the next section that even when names are fully disclosed, privacy can still be maintained due to the non-uniqueness of names, chaffing and uncertainty in the universe around the data. Recoding can have an important side benefit of resulting in more consistent linkage decisions, both between different matches by one person as well as different people because it will help people use the same information for linkage decisions   Figure 1  Review Screen i ff M i ss in g   T X  T ra n s p o se     Sa me   C  Universe Manipulation and Chaffing Even when identifying information is separated from the sensitive information, users can still infer attributes by using background knowledge.  For example, if someone you know background knowledge\ on the cancer registry \(group disclosure\ey must have cancer \(attribute disclosure Thus, strict decoupling via encryption along cannot guarantee no attribute disclosure. We employ additional methods to interfere with such possible inferences by manipulating the universe around the data that is displayed during review and adding fake data.  In the previous example consider if you knew that the list being linked had people who did not have cancer \(i.e. fake data\r if you did not know this was a cancer registry.  Furthermore, there is no way to know how many people named ‘John Smith’ exist in the universe of the data, especially when the universe is unknown.  Our experiments confirm that even with rare names fully disclosed, when the universe is effectively manipulated people are not able to confidently infer identity The probability of attribute disclosure through group membership is dependent on a variety of factors including any pre-existing information that is known by the observer the knowledge on the nature of the list, and the uniqueness of the PII in the universe of the data. The threat model where Alice is the researcher doing the review and Bob has bribed Alice to find out the disease status of Ian is described formally in Fig. 2.  In this example, Alice could memorize the name for the target subject, Ian, and could spot the same name during clerical review. It is important to note that identifying the person is not a confidentiality violation yet Rather, the violation occurs when an identified person’s disease status becomes known.  Whether spotting the same name during clerical review can lead to confirmation that the real person of interest \(Ian Bob as cancer depends on two factors.  First, does the list represent everyone with cancer And second, how likely is it that the viewed name \(Ian PII  represents the actual real world entity \(Ian Bob hich depends on factors such as the rarity of the name.  Here we discuss methods to further introduce uncertainty in inferring that Ian Bob has cancer by intentionally manipulating the universe around the displayed data point \(Ian PII ith little impact on the matching decision.  There are three methods of modification: \(1\ chaffing: literally change the nature of the universe by adding fake data, \(2\ fabrication: change the label/name of the universe presented to the researcher to obscure inference, and \(3\ndisclosure: hide the identity of the universe from the researcher to reduce confidence Chaffing is the process of adding fake data to a dataset to enlarge the universe to such an extent where group membership no longer reveals sensitive information. It is comparable to a person concealing themself by becoming part of a large crowd.  In particular, by adding a certain type of fake data, we can fundamentally block attribute disclosure 1  Alice is a researcher who is responsible for resolving ambiguous links via clerical review for a study on linking Cancer Registry data from two hospitals, L A and L B located in NC.  During the process, she will be shown a partial list of PIIs from L A and L B to resolve the ambiguous links, denoted as L A\(PII Alice\ and L B\(PII Alice\.  The expected size of the partial list is a tiny fraction of the full lists 2  The tables are decoupled as L A L A\(PII L A\(SD L B L B\(PII L B\(SD  where L A\(PII is the PII from the hospital records in L A and L A\(SD is all columns except PII including all the sensitive data.  Alice has no access to L A\(SD and L B\(SD which specify the type of cancer diagnosis in the registry along with other information 3  Bob, who works for a health insurance company, bribes Alice to find out if Ian has cancer and gives Alice Ian’s full PII denoted as Ian PII Note that there is no guarantee that Ian PII is unique in the universe of real world entities, denoted as R A from which the data is collected.  We denote the unique real world entity that Bob is interested in as Ian Bob  4  We assume that Alice knows that L A\(PII and L B\(PII are cancer registries 5  During her clerical review process, Alice can combine her prior knowledge of Ian PII with the partial lists of PIIs given to her, L A\(PII  Alice\ and L B\(PII Alice\ to make inferences  Under simple uniform models and assuming Ian Bob has cancer Risk \(Bob finding out that Ian Bob has cancer Ian PII is on either L A\(PII or L B\(PII  Pr\(Ian PII   L A\(PII Alice\\+Pr\(Ian PII   L B\(PII Alice Pr I a n PII Ian Bob  very small since clerical review should occur in only a tiny percent of the full lists  Thus the main threat of inferring that Ian Bob has cancer results from   having seen his PII in one of the partial lists during review   knowing that L A\(PII and L B\(PII are cancer registries   AND the uniqueness of Ian PII in the universe R A  Figure 2  Formal Threat Model 8 


through group membership.  Disclosure through group membership can only occur when the list disclosed for review represents a homogenous group, such as cancer registry.  By adding real names to the list that do not have cancer, and letting the researcher know that the chaffing has occurred, the list is no longer homogenous and membership on the list has no meaning.  In sum, the researcher can no longer be certain of sensitive information based on group membership even if the identity has been disclosed.  The key to chaffing is to add the appropriate fake data so as to introduce uncertainty to attribute disclosure, but not to interfere with the matching decision.  The most effective method is to incorporate short refresher training covering how the list is chaffed, that a chaffed list effectively has an intractable universe, and thus inferences cannot be made with any reasonable certainty  Adding PIIs that the researcher is familiar with, such as their family and friends, into the list can be an effective method for subtle mental reminders that the list includes fake data obtained from the user An orthogonal method to changing the nature of the list through chaffing is to confuse the identity inference.  One method to block identity inference is to falsify the universe by presenting the list as if it came from a different data space. In our threat model, we might present the list to be from a hospital located across the country, say CA.  Such a false presentation of the list L NC as L CA would make it probabilistically impossible for them to infer that the viewed name \(Ian PII presents the real person \(Ian Bob of interest who lives in NC.  In other words, by presenting the list as if it came from a different data space \(L CA  D CA the Pr\(Ian PII Ian Bob 0 because Ian Bob  D NC Ian PII  L CA  D CA D NC  D CA   Thus, if Alice believes Ian PII  L CA we effectively block any possible correct inferences on identity Sometimes it can be difficult to totally falsify the universe to a research team member responsible for clerical review.  In such situations, not specifying the universe until after the clerical review can also be useful.  When we provide Alice with records from an undefined universe, she can be led to similar uncertain conclusions about the identity of a person because they are led to assume the whole world as their universe.  In order for Alice to make a reasonably accurate inference from prior knowledge of the subject’s name, they need two pieces of information: the existence of the same name on the list and the number of real people with the same name in the universe where the list came from, R The probability that the spotted name on the list is the same real world entity of interest is 1 in n\(same name in R\. The rate of confidence is inversely correlated with the size of n When the universe is undefined and the researcher must assume a huge universe of anyone living in the US, even for rare names it is difficult to assume n=1 with certainty. Thus even if the researcher is able to recognize a particular name there will be a constant degree of uncertainty in their judgment.  Nondisclosure is quite similar to chaffing in that the universe becomes enlarged.  This allows for an increase in probability of comparable real world entities to exist resulting in an increase in n\(same name in R\ which in turn reduces the confidence of the researcher about the identity IV  E VALUATION RE SULTS  To better understand what kinds of PII should be disclosed to the researcher during clerical review, we did an experiment by conducting an online survey. The survey simulated the situation that the insider, Alice, would be in while performing the clerical review.  Our goal was to test how well the different methods worked to reduce identity disclosure and attribute disclosure.  In this experiment, we measured \(1\he effect of chaffing, \(2\ the impact of the modification of the universe, both falsifying and nonspecification, on identity inference, \(3\ the disclosure risk of different identifying information attributes; namely, the common name, the common name and DOB pair, and the rare name, \(4\ow these attributes interact with each other when used in combination, and \(5\ the effects of missing and erroneous data on the different methods using variations on the common name and DOB pair Meet George Brown, a student at Meadowgreen HS Using only the information provided here, how likely is it that the George Brown introduced above is the same person listed on the honor roll provided here A  Highly likely to be the same person B  Moderately likely to be the same person C  Slightly likely to be the same person D  I don’t know if they are the same person or not E  Slightly likely to be two different people F  Moderately likely to be different people G  Highly likely to be two different people Meadowgreen HS Honor Roll Amanda Ward Edward Jones Hilary Ford George Brown Susan Miller David Green Alexander Parker Brian Richards Daniel Parker Alex Parker  Figure 3  Sample Survey Question for Common Name Scenario Fig. 3 is the basic question set up.  The respondents are given different identifying data about a target student and an Honor Roll list with the same identifying data. Then we ask them to select their confidence level about the likelihood that the information given in the question and the honor roll refers to the same person \(identity disclosure\ on a sevenpoint Likert scale \(i.e. three levels of yes, three levels of no or I Don’t Know\. The survey had in total 18 questions; six scenarios, each with three questions.  The six scenarios were ordered so that questions would build on each other. We started with the simple common name scenario, and then moved onto to common name and DOB.  The next two scenarios were common name and missing DOB followed by common name and transposed DOB.  The transpose was created by swapping month and date numbers in DOB \(2/5 and 5/2\    The survey questions for these cases had the missing or transposed DOB in the honor roll for the target student.  We left rare name and chaffing to the last two sections because we wanted the respondents to get used to inferring identity before giving them the chaffed list experiment for attribute inference.  In the fifth scenario for rare name, the target student had a rare name \(Rahul Ghosh and the honor rolls included other rare names such as Viswanath Sastry, Jie Lee, and Michelle Pham. In addition we changed the question slightly to ask how likely it is that the target student had made the honor roll \(attribute disclosure\ at his school.  The scale wording was also adjusted to indicate the confidence of having made the honor roll. We changed the question slightly so that we could ask 9 


the respondents to answer the same question one more time knowing that the honor roll included some fake data of students who did not make the honor roll. This is the sixth scenario, wherein we tested a given rare name as the identifying information on a chaffed list. For the three questions in each of the six scenarios, respondents were given the honor roll from the same high school \(Same HS Meadowgreen HS’\ an honor roll from a different high school \(Diff HS: ‘Valley Mountain HS’, falsified\, and finally an honor roll from an unknown high school \(No HS:‘A HS’, undefined\.  We constructed the three honor rolls so that they shared some names including the target student but were sufficiently different from each other The full results are shown as 18 stacked bar charts in Fig 4. Each of the questions corresponds to one stacked bar.  We tested the change in the confidence level of identity or attribute by comparing responses to different questions using the Wilcoxon signed rank test, a non-parametric T-test.  We mainly recruited from graduate students in various departments including public health and computer science who we anticipate will be doing the clerical reviews.  We had 59 respondents.  Although we made no attempt to recruit from students with experience in data analysis, we obtained a good mix of respondents with various experiences in data. A previous paper [6 r e por t e d t h e d e m o gr ap hi c s  a n d d a t a  experiences of the respondents from this study with selected findings\(8 of 18 questions\. Here we present complete results on the effects of manipulating the universe and the impact of missing and erroneous data on the different methods. A summary of the impact of chaffing from [6 is als o in clu d e d   A  Impact of Chaffing T-Test results between common name scenario and rare name + chaffing scenario was surprising.  There was sufficient evidence \(p-value < 0.005\ to conclude that respondents were significantly less confident in the identity of rare names on a chaffed list compared to common names on an accurate list \(Bar 1 vs. Bar 16\ Chaffing was very effective in preventing attribute disclosure of rare names during clerical review [6   B  Impact of Falsifying or Undefining the Universe We found that manipulating the label of the displayed list was quite effective in reducing confidence in identity in all cases.  In the base case for common names \(George Brown from Meadowgreen HS\ when a list was presented as being from the same high school, all but 11 answered Highly or Moderately Likely the Same Person indicating fairly high confidence in identity.  There was a dramatic shift in responses to the second question when a list was presented under a different universe \(Valley Mountain HS\ and even went on to assert that the person on the list was a different person.  Only 6 answered Highly Likely the Same Person  while the number of respondents answering Highly Likely Different People shot up from 0 to 29. The median for the falsified universe is on the other end of the spectrum at Moderately Likely Different People The results from the third question, the list with no high school defined \(A HS were quite different from the first two questions. The vast majority of the respondents fell in the middle of the spectrum with both the mode and median response being I Don’t Know The combined response to Slightly Likely Same or Different People and I Don’t Know was 56%.  We conclude that the respondents were confused and could not confidently assert a response on whether the presented person was the same person or not.  The finding supports the hypothesis that an undefined universe will effectively introduce uncertainty such that people will not be able to make an affirmative conclusion about a name they find on the list Although not as strong an effect, we see a similar trend in the impact of falsified universe and undefined universe for scenarios where we presented the common name and DOB pair and a rare name.  Given a common name and DOB pair the median response increased to Slightly Likely the Same Person for a different universe and Moderately Likely the Same Person for an undefined universe. In comparison given a rare name, the median response was I Don’t Know  for a different universe but again Moderately Likely the Same Person for an undefined universe.  Not surprisingly, the DOB and the rarity of a name serve to increase the confidence of the respondent compared to only a common name.  Nonetheless the modification of the universe still had the impact of reducing the confidence levels as indicated by the T-tests which confirmed statistically different medians in all cases when the universe was manipulated.  Most importantly, when used in combination with chaffing, the impact of manipulating the universe seems to have an additive effect \(Bars 17 and 18\.  The median response for rare names with the universe undefined dropped further to the ideal level of I Don't Know when using the chaffed list Bar 18\.  This is an important finding because it confirms that using a combination of chaffing and nondisclosure of the universe, names, including rare names, can be fully disclosed with minimum risk of attribute disclosure during the review It is quite interesting to note that given a list from a different high school, in all questions, there were some respondents who selected either Highly or Moderately Likely the Same Person as the target student.  The Pr \(target student A  L X 0 given that target student A  D A and D A  D X    regardless of how rare the identity might be Logically it would be highly unlikely that a George Brown at Meadowgreen HS is the same George Brown at the Valley Mountain HS.  However, 15% still answered that they were Highly or Moderately Likely the Same Person For common name and DOB pair it was as high as 42% and for rare name it was 19%.  In the pretest of the survey, we asked informally about such responses.  Some respondents thought that the student may have transferred from one high school to the other or that the student was simultaneously enrolled in both schools.  The respondents were unconsciously adding new dimensions, such as time, into the situation to accommodate their personal belief that two people with the same name and DOB are highly likely to be the same person \(Susan Miller DOB=4/17/1994\ Given that Susan Miller is a fairly common name, we believe such responses strongly suggest the possibility of researchers making incorrect inferences and jumping to wrong conclusions.  It could in fact be true that 10 


Ian PII  Ian Bob but when researchers make incorrect assumptions that Ian PII Ian Bob harm can still occur Thus, it seems that this result points to the need for good training before researchers are allowed to do clerical review C  Missing and Erroneous Data Of all six scenarios, respondents were most confident about the identity of the target student when presented with the common name and DOB pair where 50 out of 59 respondents answered Highly Likely the Same Person with 7 more respondents answering Moderately Likely the Same Person These results indicate that people are highly confident about a person’s identity given a pair of name and DOB.  This finding is supported empirically by Weber et al who show that the name and DOB pair was surprisingly effective for record linkage under certain circumstances [7 Thus, we recommend that the raw DOB should not be disclosed in the PII list during the clerical review process Instead, information related to DOB should be displayed as differences as shown in Fig. 1.  However, this strong confidence in identity is quickly diminished in the scenarios with missing DOB and transposed DOB \(Bar 4 vs Bars 7 10\.  In this experiment, we tested the effectiveness of the different methods in the presence of missing and erroneous data, because such data is common in real-world big data V  C ONCULSION  We are planning a second study to focus on the different methods for recoding names to understand its impact on both linkage decisions and disclosure.  Identity disclosure without sensitive attribute disclosure has little potential for harm [8   Recognizing that the desire for privacy protection is for the sensitive data rather than the identifying data, we introduce SDLink, a simple but powerful data integration system that supports safe interactive record linkage so that errors from linkage can be managed throughout the full workflow Errors that are not properly managed in machine only data integration systems propagate to subsequence data analyses 9 le a d in g t o  p o t e n tia l p r o b l em s w ith in co rr ec t an aly s es  which can ultimately result in incorrect decisions  Harm from inaccurate data and information can be as devastating as our concerns about privacy violations A CKNOWLEDGMENT  This research was supported by funding from the NCDHHS and NSF awards CNS-0915364 and OCI-1247652 R EFERENCES  1  K. Elmagarmid, G. Panagiotis, S. Verykios. Duplicate record detection: A survey. IEEE Trans. Knowl. Data Eng. 2007;19\(1\:1-16 2  H. Kang, L. Getoor, B. Shneiderman, M. Bilgic, L. Licamele Interactive Entity Resolution in Relational Data: A Visual Analytic Tool and Its Evaluation. IEEE TVCG 2008;14\(5\:999-1014 3  C. Clifton, M. Kantarcioglu, A. Doan, G. Schadow, J. Vaidya, A Elmagarmid, D. Suciu. Privacy-preserving data integration and sharing. In ACM SIGMOD  workshop DMKD '04.  2004. 19-26 4  R. Hall and S. Fienberg: Privacy-Preserving Record Linkage. Privacy in Statistical Databases 2010: Lecture Notes in Computer Science 2011, Volume 6344/2011, pp 269-283 5  D. Vatsalan, P. Christen, V. Verykios. A taxonomy of privacypreserving record linkage techniques, Info Sys.  2013;38\(6\:946-969 6  H. Kum., S. Ahalt, D. Pathak. Privacy Preserving Data Integration Using Decoupled Data. In SPSN , Springer 2013; 225-253 7  S. Weber, H. Lowe H, A. Das et al. A simple heuristic for blindfolded record linkage J Am Med Inform Assoc 2012; 19:157-161 8  S. Fienberg. Confidentiality, privacy and disclosure limitation Encyclopedia of Social Measurement, Academic Press 2005;1:463-9 9  P. Lahiri and M. Larsen. Regression analysis with linked data. Journal of the American Statistical Association, 2005;100\(469\:222-230  Figure 4  Results  11 


VII  C ONCLUSION AND FUTURE WORK  This paper highlights the problem of data transfers in the cloud, transfers that affect cloud performance by moving resources to the virtual machine on which they are needed. We described a method for big-data transfer optimization based on network characteristics and we considered the constraints introduced by SLA. We proposed two greedy scheduling algorithms and we compared them to a standard First Come First Served, packet transfer order.  Several simulation experiments highlight the obtained improvement Having a scheduler for transfers that takes into account global transfers can bring an improvement to applications and to the user experience. We believe that better algorithms may be obtained that further improves on the scheduling so that cloud resource usage is minimized. We also considered using the discussed algorithms for Cloud user data transfers, for this to be practical an API needs to be provided to the Cloud user throw which to schedule only the big-data transfers. The API should offer some sort of feedback, which can be utilized by the user to further optimize it\222s processing A CKNOWLEDGMENT  This work was partially supported by project \223ERRIC Empowering Romanian Research on Intelligent Information Technologies/FP7-REGPOT-2010-1\224, ID: 264207. The work has been co-funded by the Sectorial Operational Program Human Resources Development 2007-2013 of the Romanian Ministry of Labor, Family and Social Protection through the Financial Agreement POSDRU/89/1.5/S/62557 R EFERENCES   M i n g Z h a o and  R e n a t o J  F i gu ei r e d o 20 07 E xp e ri m e nt a l s t ud y of vi rt u a l machine migration in support of reservation of cluster resources. In Proceedings of the 2nd international workshop on Virtualization technology in distributed computing VTDC '07\. ACM, New York, NY, USA, Article 5 8 pages 2 Jo se p h Hal l  Jaso n Har t l i n e A n n a R  K a rl i n J a re d Sai a  an d Jo h n W i l k e s   2001. On algorithms for efficient data migration. In Proceedings of the twelfth annual ACM-SIAM symposium on Discrete algorithms SODA '01\. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 620-629 3 i m o t h y  W o o d P r as ha nt S h e n oy  A r un V e nkat a r a m a n i a nd Ma z i n  Yousif. 2007. Black-box and gray-box strategies for virtual machine migration. In Proceedings of the 4th USENIX conference on Networked systems design and implementation NSDI'07\. USENIX Association Berkeley, CA, USA, 17-17 4 A l e x ande r S t ag e an d T h o m as S e tz er 20 0 9 N e tw o r kaw ar e m i g r a t io n  control and scheduling of differentiated virtual machine workloads. In Proceedings of the 2009 ICSE Workshop on Software Engineering Challenges of Cloud Computing CLOUD '09\. IEEE Computer Society, Washington DC, USA, 9-14 5 G u e y ou n g J u n g N a th an G n an a s am b a nd a m an d Tr i d ib  Mu kh er j ee 2 0 1 2  Synchronous Parallel Processing of Big-Data Analytics Services to Optimize Performance in Federated Clouds. In Proceedings of the 2012 IEEE Fifth International Conference on Cloud Computing CLOUD '12\. IEEE Computer Society, Washington, DC, USA, 811-818 6 K e jiang Y e X iao ho ng J i ang  D a w e i H u a n g  J i an ha i Che n a n d Be i W a ng   2011. Live Migration of Multiple Virtual Machines with Resource Reservation in Cloud Computing Environments. In Proceedings of the 2011 IEEE 4th International Conference on Cloud Computing CLOUD '11\. IEEE Computer Society, Washington, DC, USA, 267-274 7 l be r t  G r e e nbe r g P a r a nta p L a hir i D a v i d A  Mal t z  P a r v e e n P a te l  S udi pt a Sengupta Microsoft Research, Redmond, WA, USA Towards a Next Generation Data Center Architecture: Scalability and Commoditization  


Analysis of Incrementally Generated Clusters in Biological Networks Using Graph-Theoretic Filters and Ontology Enrichment 584 Sean West, Kathryn Dempsey, Sanjukta Bhowmick, and Hesham Ali Fourth ICDM International Workshop on Knowledge Discovery Using Cloud and Distributed Computing Platforms \(KD-Cloud Accelerating Frequent Itemsets Mining on the Cloud: A MapReduce -Based Approach 592 Zahra Farzanyar and Nick Cercone Decentralized K-Means Using Randomized Gossip Protocols for Clustering Large Datasets 599 Jérôme Fellus, David Picard, and Philippe-Henri Gosselin Cloud Based Predictive Analytics: Text Classification, Recommender Systems and Decision Support 607 Klavdiya Hammond and Aparna S. Varde Decision Support in Data Centers for Sustainability 613 Michael Pawlish, Aparna S. Varde, and Stefan Robila Mining and Understanding from Big Data \(BigMUD In-Core Computation of Geometric Centralities with HyperBall: A Hundred Billion Nodes and Beyond 621 Paolo Boldi and Sebastiano Vigna Scalable Audience Reach Estimation in Real-Time Online Advertising 629 Ali Jalali, Santanu Kolay, Peter Foldes, and Ali Dasdan A Study on Privacy Preservation for Multi-user and Multi-granularity 638 Dong Li, Xiangmang He, Huahui Chen, Yihong Dong, and Yefang Chen Towards Optimal Symbolization for Time Series Comparisons 646 Gavin Smith, James Goulding, and Duncan Barrack GPU-Accelerated Query by Humming Using Modified SPRING Algorithm 654 Guangchao Yao, Yao Zheng, Limin Xiao, Li Ruan, Yongnan Li, and Zhenzhong Zhang Designing the Market of Data - for Synthesizing Data in Sciences and Businesses \(MoDAT Curating and Mining \(Big 664 Akinori Abe Quantifying and Recommending Expertise When New Skills Emerge 672 Dongping Fang, Kush R. Varshney, Jun Wang, Karthikeyan Natesan Ramamurthy Aleksandra Mojsilovi and John H. Bauer Spatiotemporal Life-Log Mining of Wheelchair Users’ Driving for Visualizing Accessibility of Roads 680 Yusuke Iwasawa and Ikuko Eguchi Yairi A Method for Generating Ontologies in Requirements Domain for Searching Data Sets in Marketplace 688 Noriyuki Kushiro 
xi 
xi 


Valuation of Data through Use-Scenarios in Innovators' Marketplace on Data Jackets 694 Chang Liu, Yukio Ohsawa, and Yoshitaka Suda Data Marketplace for Efficient Data Placement 702 Hiroshi Maruyama, Daisuke Okanohara, and Shohei Hido Acquissition of Text-Mining Skills for Beginners Using TETDM 706 Rina Nakagochi, Kayo Kawamoto, and Wataru Sunayama Detecting Topics from Twitter Posts During TV Program Viewing 714 Takanobu Nakahara and Yukinobu Hamuro Frame as a Clue to Intention of Data: Toward New Product Ideas with Framed Components 720 Jun Nakamura and Masahiko Teramoto e-Trucks Realize Four Zeros Expectations, The Challenge by Market of Data 726 Masahiko Teramoto and Jun Nakamura Valuation of Partly Disclosed Datasets for Prediction 733 Hiroe Tsubaki IdeaGraph Plus: A Topic-Based Algorithm for Perceiving Unnoticed Events 735 Chen Zhang, Hao Wang, Fanjiang Xu, and Xiaohui Hu The Eighth Workshop on Optimization Based Techniques for Emerging Data Mining Problems \(OEDM Robust Cost-Sensitive Confidence-Weighted Classification 742 Alnur Ali and Kevyn Collins-Thompson Optimal Correlation Clustering via MaxSAT 750 Jeremias Berg and Matti Järvisalo Evaluation of Session-Based Recommendation Systems for Social Networks 758 Chen-Ling Chen and Chia-Hui Chang Evaluation of Social, Geography, Location Effects for Point-of-Interest Recommendation 766 Nai-Hung Cheng and Chia-Hui Chang Fast Spectral Clustering with Landmark-Based Subspace Iteration 773 Zejun Gan, Chaofeng Sha, and Junyu Niu Item-Based Top-k Influential User Discovery in Social Networks 780 Jing Guo, Peng Zhang, Chuan Zhou, Yanan Cao, and Li Guo Unsupervised Clustering Strategy Based on Label Propagation 788 Jiguang Liang, Xiaofei Zhou, Ying Sha, Ping Liu, Li Guo, and Shuo Bai How to Improve the Quality of Pedestrian Detection Using the Priori Knowledge 795 Zhiquan Qi, Yingjie Tian, Xiaodan Yu, and Yong Shi Infinite Mixed Membership Matrix Factorization 800 Avneesh Saluja, Mahdi Pakdaman, Dongzhen Piao, and Ankur P. Parikh Resampling and Cost-Sensitive Methods for Imbalanced Multi-instance Learning 808 Xiaoguang Wang, Xuan Liu, Nathalie Japkowicz, and Stan Matwin Cost-Free Learning for Support Vector Machines with a Reject Option 817 Guibiao Xu and Bao-Gang Hu 
xii 
xii 


Dissimilarity Features in Recommender Systems 825 Christos Zigkolis, Savvas Karagiannidis, and Athena Vakali Fourth IEEE Workshop on Privacy Aspects of Data Mining \(PADM 2013 Differentially Private Anomaly Detection with a Case Study on Epidemic Outbreak Detection 833 Liyue Fan and Li Xiong A Semi-Supervised Learning Approach to Differential Privacy 841 Geetha Jagannathan, Claire Monteleoni, and Krishnan Pillaipakkamnatt The Independence of Fairness-Aware Classifiers 849 Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma Incentive-Compatible Privacy-Preserving Distributed Data Mining 859 Murat Kantarcioglu Privacy-Preserving Kernel k-Means Outsourcing with Randomized Kernels 860 Keng-Pei Lin Select-Organize-Anonymize: A Framework for Trajectory Data Anonymization 867 Giorgos Poulis, Spiros Skiadopoulos, Grigorios Loukides, and Aris Gkoulala-Divanis Data Anonymity Meets Non-discrimination 875 Salvatore Ruggieri Privacy Preserving Social Network Publication against Mutual Friend Attacks 883 Chongjing Sun, Philip S. Yu, Xiangnan Kong, and Yan Fu Adaptive Differentially Private Data Release for Data Sharing and Data Mining 891 Li Xiong Sentiment Elicitation from Natural Text for Information Retrieval and Extraction \(SENTIRE Enhancing Sentiment Classification Performance Using Bi-Tagged Phrases 892 Basant Agarwal, Namita Mittal, and Erik Cambria Dynamic Construction of Dictionaries for Sentiment Classification 896 Hanen Ameur and Salma Jamoussi Multi-Class Sentiment Analysis with Clustering and Score Representation 904 Mohsen Farhadloo and Erik Rolland Robust Language Learning Via Efficient Budgeted Online Algorithms 913 Simone Filice, Giuseppe Castellucci, Danilo Croce, and Roberto Basili Pattern-Based Topic Models for Information Filtering 921 Yang Gao, Yue Xu, and Yuefeng Li Interest Analysis Using Semantic PageRank and Social Interaction Content 929 Chung-Chi Huang and Lun-Wei Ku Joint and Pipeline Probabilistic Models for Fine-Grained Sentiment Analysis: Extracting Aspects, Subjective Phrases and their Relations 937 Roman Klinger and Philipp Cimiano 
xiii 
xiii 


Learning the Roles of Directional Expressions and Domain Concepts in Financial News Analysis 945 Pekka Malo, Ankur Sinha, Pyry Takala, Oskar Ahlgren, and Iivari Lappalainen A Framework of Review Analysis for Enhancement of Business Decision Making 955 Atika Qazi, Ram Gopal Raj, Muhammad Tahir, and Syed Ghaour Abbas Naqvi Sentiment Analysis in News Articles Using Sentic Computing 959 Prashant Raina Interpreting or Describing? Measuring Verb Abstraction 963 Dominika Rogozi ska and Aleksander Wawer Possible Usage of Sentiment Analysis for Calculating Vectors of Felific Calculus 967 Rafal Rzepka and Kenji Araki Subjective Bayes Method for Word Semantic Similarity Measurement 971 Junhua Wang, Xianglin Zuo, Wanli Zuo, and Tao Peng Eighth International Workshop on Spatial and Spatio-Temporal Data Mining SSTDM Qualitative Spatial Structure in Complex Areal Objects Using Location-Free, Mobile Geosensor Networks 978 Alan Both and Matt Duckham A Novel Approach to Trajectory Analysis Using String Matching and Clustering 986 Madhuri Debnath, Praveen Kumar Tripathi, and Ramez Elmasri Severe Hail Prediction within a Spatiotemporal Relational Data Mining Framework 994 David John Gagne II, Amy McGovern, Jerald Brotzge, and Ming Xue Blazing Fast Time Series Segmentation Based on Update Techniques for Polynomial Approximations 1002 André Gensler, Thiemo Gruber, and Bernhard Sick DLOREAN: Dynamic Location-Aware Reconstruction of Multiway Networks 1012 Fredrik Johansson, Vinay Jethava, and Devdatt Dubhashi Mining Semantic Time Period Similarity in Spatio-Temporal Climate Data 1020 Michael P. McGuire and Ziying Tang An Integer Programming Approach to Temporal Pattern Matching Queries 1028 Megan Monroe and Amol Deshpande Analyzing the Proximity and Interactions of Friends in Communities in Gowalla 1036 Tommy Nguyen, Mingming Chen, and Boleslaw K. Szymanski 4D+SNN: A Spatio-Temporal Density-Based Clustering Approach with 4D Similarity 1045 Ricardo Oliveira, Maribel Yasmina Santos, and João Moura Pires Multi-sensor Remote Sensing Image Change Detection: An Evaluation of Similarity Measures 1053 Karthik Ganesan Pillai and Ranga R. Vatsavai New Spatiotemporal Clustering Algorithms and their Applications to Ozone Pollution 1061 Sujing Wang, Tianxing Cai, and Christoph F. Eick 
xiv 
xiv 


The Passenger Demand Prediction Model on Bus Networks 1069 Chunjie Zhou, Pengfei Dai, and Renpu Li Demo Papers SaferCity: A System for Detecting and Analyzing Incidents from Social Media 1077 Michele Berlingerio, Francesco Calabrese, Giusy Di Lorenzo, Xiaowen Dong, Yiannis Gkoufas and Dimitrios Mavroeidis The MiningZinc Framework for Constraint-Based Itemset Mining 1081 Tias Guns, Anton Dries, Guido Tack, Siegfried Nijssen, and Luc De Raedt Demand Finder: Set Top Box Television Ad Targeting Using a Novel Interactive Data Visualization System 1085 Brendan Kitts, Dyng Au, Brian Burdick, Jon Borchardt, Amanda Powter, and Todd Otis An Evaluation Framework for Temporal Subspace Clustering Approaches 1089 Hardy Kremer, Stephan Günnemann, Arne Held, and Thomas Seidl Interactive Data Analysis Tool by Augmenting MATLAB with Semantic Objects 1093 Changhyun Lee, Jaegul Choo, Duen Horng \(Polo Demonstrating Interactive Multi-Resolution Large Graph Exploration 1097 Zhiyuan Lin, Nan Cao, Hanghang Tong, Fei Wang, U. Kang, and Duen Horng \(Polo NIM: Scalable Distributed Stream Process System on Mobile Network Data 1101 Lujia Pan, Jianfeng Qian, Caifeng He, Wei Fan, Cheng He, and Fan Yang PhD Forum A Multi Density-Based Clustering Algorithm for Data Stream with Noise 1105 Amineh Amini, Hadi Saboohi, and Teh Ying Wah MapReduce Based Frameworks for Classifying Evolving Data Stream 1113 Ahsanul Haque and Latifur Khan Time-Sensitive Route Planning Using Location-Based Data 1121 Hsun-Ping Hsieh, Cheng-Te Li, and Shou-De Lin Mining Discrete Patterns via Binary Matrix Factorization 1129 Peng Jiang and Michael T. Heath Mining Adverse Drug Reactions from Electronic Health Records 1137 Henry Z. Lo, Wei Ding, and Zohreh Nazeri Quantification of Financial News for Economic Surveys 1141 Mihail Minev Local Discriminative Distance Metrics and their Real World Applications 1145 Yang Mu and Wei Ding Host-Based Anomaly Detection Using Learning Techniques 1153 Ahmad Mustafa, Mohiuddin Solaimani, Latifur Khan, Ken Chiang, and Joe Ingram Rapidly Labeling and Tracking Dynamically Evolving Concepts in Data Streams 1161 Brandon S. Parker and Latifur Khan 
xv 
xv 


Author Index 1165 
xvi 
xvi 


Copyright © 2009 Boeing. All rights reserved  Historical Data Processing To load correlated data  Uncompress, unmarshall  Create a list of files containing the correlated data  Write data to warehouse 


Copyright © 2009 Boeing. All rights reserved  Live Data Processing Processed using IBM MQ IBM Message Broker and a technique called XML Shredding Message Broker Compute Nodes  Uncompress Node  Extract correlated messages  Shred Node adds to DB Stored Procedure “shreds XML docs and adds to tables 


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a “key, value” list using an XSTL  Queries made against this list of “key, value” pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


