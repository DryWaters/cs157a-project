HIG  An In-memory Database Platform Enabling Real-time Analyses of Genome Data Matthieu-P Schapranow Hasso Plattner Hasso Plattner Institute Enterprise Platform and Integration Concepts AugustÖBebelÖStr 88 14482 Potsdam Germany schapranow|plattner}@hpi.uni-potsdam.de Abstract Costs and time required for sequencing of DNA and RNA declined through use of next generation sequencing technology e.g up to 30-times coverage reads are generated in less than two days However its interpretation and analysis is still a time-consuming process potentially taking weeks In this work we present a completely new architecture for processing and analyzing genome data It builds on the in-memory database technology to eliminate time-consuming le-based data operations and to enable real-time data analysis We found out that the use of in-memory technology as an integral component for genome data processing and its analysis signiìcantly reduces time and costs to obtain relevant results e.g in the course of personalized medicine Keywords Real-Time Data Analysis In-Memory Database Technology Genome Data Personalized Medicine Next-Generation Sequencing I Introduction The Human Genome HG project ocially launched in 1990 involved thousands of worldwide research institutes and required more than a decade to sequence and decode the full HG  N o w ad a y s N e xt G en erat ion Sequencing NGS devices process whole genomes within hours at reduced costs 2  T hey a r e used in r e sea r c h a nd clinical environments to support treatment of speciìc diseases such as cancer Personalized medicine aims at treating patients speciìcally based on individual dispositions e.g genetic or environmental factors   T h i s requires tool support to identify relevant data out of the huge amount of acquired diagnostic data   The In-Memory Database IMDB technology has demonstrated major advantages in analyzing big enterprise data 5 6  In the g iv en w o r k  w e p r e sen t our ndings of applying IMDB technology to enable real-time analysis of genome data in course of our High-performance In-memory Genome HIG research project We developed a speciìc platform that combines processing and analyzing of genomic data as a holistic process based on the feedback of researchers and clinicians Our HIG architecture is designed to run on commodity hardware instead of highly specialized  HIG Web Service Ruby on Rails  Specific Cloud A pplications   Worker Node    R   Worker Python  In-Memory Database Instance  Reads and Reference Genomes    R  HIG Web Service Ruby on Rails Internet Intranet L functions SQL script procedures Master Node Primary In-Memory Database Instance R C linician w ith Mobile Device Specific Cloud Application R Researcher with Web Browser R Work List Worker Status Global State Scheduler and Coordinator Task Scheduler Python Data Layer Platform Layer Application Layer R R Worker Python R 3rd Party Tool 3rd Party Tool R S pecific C loud Applications S pecific M obile Application R R Figure 1 The HIG system architecture consist of application platform and data layer Data analysis is directly performed within the IMDB eliminating time-intensive data transfers from the le system hardware to be a cost-ecient and to b make use of existing hardware infrastructures Fig 1 depicts the system architecture of our system modeled as block diagram using the Fundamental Modeling Concepts FMC 7  The rest of the paper is structured as follows In Sect II our work is set in context of related work and we introduce selected components of our HIG system in Sect III We share our benchmarks results in Sect IV and discuss our ndings in Sect V Our work concludes with an outlook in Sect VI 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 691 


II Related Work Time consumed by sequencing and related costs dropped disruptively with the introduction of latest NGS technologies around 2007 Meanwhile algorithms used data processing of NGS data date back to early 1970s Related work in the eld of genome data processing has increased in the recent years but work focusing on implementing end-to-end processes is still rare Wandelt et al have discussed recent data management challenges for processing NGS data They observed trends towards more and more cloud-based solutions but they identiìed the ecient mapping of workîow tasks onto heterogeneous distributed compute nodes and the adjustment of a given workîow to a dynamic environment as still open issues 8 Sect 4.3 Our work contributes by deìning a system architecture that combines processing and analyzing of genome data We contribute by sharing details about our worker framework developed in Python in Sect III which enables integration of compute resources across platform and Operating System OS borders Furthermore we share details about our scheduler in Sect III which enables prioritized processing of multiple tasks in parallel e.g simultaneous users or multiple departments Pabinger et al published a survey on variant analysis tools and evaluated their functionality 9  T hey selected 32 of them for evaluation and found out that the majority is command-line based e.g ANNOVAR AnnTools or snpE 10   11   12   F o r w eb b ased t o ol s they see a drawback in data preparation before analysis can start since   l e s n e e d t o b e p a c k e d  s o r t e d a n d indexed before they can be used 9 With o u r i nco r porated IMDB technology we address time-consuming data transformation steps and replace them by native database operations as discussed in Sect V Furthermore Pabinger et al evaluated workîow systems and analysis pipeline tools They realized that these tools either miss exibility or need to have additional know-how to install speciìc tools Our work contributes by introducing a exible way to execute individual pipeline conìgurations without the need of adapting command line scripts We enable graphical modeling of individual pipeline conìgurations and its execution within a single system as presented in Sect III III Architecture Our platform consists of the following layers application platform and data as depicted in Fig 1 In the following all layers are described in detail A Application Layer The application layer consists of special purpose applications to answer speciìc medical and research questions Although these applications can only be used for limited use-cases they are highly optimized For example Fig 2 depicts our genome browser application which combines patient-speciìc data and data from various annotations databases in a single view All applications in the application layer communicate with the web service interface as depicted in Fig 1 We provide a Application Programming Interface API that can be consumed by various kinds of applications such as web browser applications or iPad and Android devices as mobile applications via asynchronous Ajax calls and JavaScript Object Notation JSON 13 1 4   As a result performing speciìc analyses is no longer limited to a speciìc location e.g the desktop computer of a clinician Instead all applications can be accessed via any internet-connected device e.g web browser or mobile tablet computer which enhances the userês productivity by having access to relevant data at any time B Platform Layer The platform layer holds the complete process logic and consists of the IMDB system for enabling realtime analysis of genomic data We developed speciìc extensions for the IMDB system to support genome data processing and its analysis In the following selected extensions and their integration in the IMDB system are describedinmoredetail 1 Scheduling of Data Processing We extended the IMDB by a worker framework which executes tasks asynchronously e.g alignment of chunks of genome data It consists of a task scheduler instance and a number of workers controlling dedicated compute resources e.g individual compute nodes Workers retrieve tasks and parameters by the scheduler instance and perform speciìc tasks such as workbench preparation task execution and maintenance of status information Thus all workers are connected to the IMDB to store status information about currently executed tasks The scheduler is responsible for managing the parallel execution of jobs on distributed compute resources It performs non-preemptive resp cooperative scheduling operations comparable to a process scheduler known from multitasking OS i.e assigning tasks to available resources without interrupting their operation at deìned times 15  W e i mplemen t ed dier e n t sc heduling p o licies which can be extended by adding new policies to the policies.d folder The default policy is random select i.e any free worker select any available job at random and starts its execution In addition we use a rst-come rst-serve policy i.e the job with the oldest creation timestamp will be executed rst and a priority scheduling policy which starts the executed of all jobs with a higher priority class before starting jobs with a lower priority class The priority scheduling also enables aboostingofverylongrunningandcomputeintensive 692 


Figure 2 Genome Browser From top to bottom comparison of reference genome and a selected cell line at a speciìc locus on chromosome 12 A Single Nucleotide Polymorphism SNP is depicted on the left Combined information from international research databases about the selected mutation are shown on the right tasks Therefore the overall job execution time for a speciìc task is supervised and evaluated corresponding to the history of already performed jobs statistics Once a long-running job has been detected is automatically assigned to a higher priority level to a nish its execution and to b free compute resources for new work as soon as possible The scheduler is contacted by the web service i.e the scheduler is the central coordinator for asynchronous job execution For keeping the global system state of all running processes the scheduler persists assigned tasks and resources and status information in the IMDB Furthermore the scheduler supervised the responsiveness of individual compute resources For example it performs checks of known worker instances using regular heartbeat calls 16  I f a pr edeìned r esp o n se b e ha vio r is no longer guaranteed e.g due to an overloaded compute node or a crashed worker process workers are marked as unresponsive As a result work-in-progress tasks of the unresponsive worker are reassigned to a new worker to guarantee their execution and the unresponsive worker is scheduled for a restart or the administrator is informed The scheduler derives the concrete tasks for a selected Genome Data Processing Pipeline GDPP model instance Models of GDPPs are modeled in BPMN in a graphical workbench The GDPP model is stored in the eXtended Markup Language Process Deìnition Language XPDL data format 17  Once a concrete GDPP model is instantiated by assigning speciìc values for all parameters it is available for genome data processing The scheduler retrieves the selected model instance from the IMDB and creates corresponding data structures representing the dependencies between individual process steps All ready-toprocess tasks are added to the work list and assigned to available worker instances The scheduler keeps track of there execution and retrieves intermediate results comparable to the MapReduce approach 18  Tasks that can be executed in parallel e.g alignment of chunks of DNA data are split into subtasks and dispatched to individual workers The scheduler combines intermediate results of individual subtasks to form the nal result set We focused on high-availability while designing important system components e.g the coordinator or the web service Each worker is equipped with the code for becoming a scheduler Once the rst worker is started it checks for existing scheduler instances and launched a new one if no scheduler is detected Thus in case of a crash or unavailability of the task scheduler any worker instance can fulìll its task After launching it performs cleanup rollback and roll-forward tasks to migrate the system to a consistent database state 2 Updater Framework We consider the use of latest international research results as enabler for evidence693 


based therapy decision 19  T he up da ter f r a mew o r k is the basis for combining international research results It periodically checks all registered Internet sources such as public FTP servers or web sites for updated and newly added versions of annotations e.g database exports as dumps or characteristic le formats such as CSV TSV and VCF If the online version is newer than the local available version the new data are automatically downloaded and imported in the IMDB to extend the knowledge base The import of newly available research databases is performed as a background job without aecting the systemês operation We import the new data without performing any data transformations 20   21   A s a result new data are available for real-time analysis without any delay For example the following selected research databases are supervised regularly by our updater framework National Center for Biotechnology Information NCBI Sangerês catalogue of somatic mutations in cancer University of California Santa Cruz UCSC    23    2 4   C Data Layer The data layer holds all required data for performing processing and analyzing of genomic data The data can be distinguished in the two categories master data and transactional data 25  F o r exa m ple h uma n r efer ence genomes and annotation data are referred to as master data whereas patient-speciìc NGS data and Electronic Medical Records EMR are referred to as transactional data 26   2 7  Its a na lysis is the b a s is fo r g a t her i ng speciìc insights e.g individual genetic dispositions and for leveraging personalized treatment decision in course of personalized medicine 3 The actual step of analyzing the genetic data requires answering very speciìc questions Thus the application layer consists of speciìc applications to answer these questions They make use of the platform layer to initialize the data processing IV Benchmarks In the following we share insights about the gathered benchmark results with multiple GDPPs processed by our HIG research prototype A Benchmark Setup All benchmarks were performed on a compute cluster consisting of 25 identical compute nodes Each of the nodes was equipped with four Intel Xeon E7-4870 Central Processing Units CPUs running at 2.40 GHz clock speed 30 MB Intel Smart cache[28  i n t er co nnected b y 6.4 GT/s Quick Path Interconnect QPI and 1 TB of main memory capacity Each CPU consisted of 10 physical cores and 20 threads running a 64-bit instruction set Exp Primary Storage Split Size IFS 1 II IMDB 1 III FS 25 IV IMDB 25 Table II Comparison of benchmark setups using BWA in HIG Split size describes the number of distributed compute nodes Exp  Experiment All compute nodes were equipped with Intel 520 series Solid State Drives SSDs of 480 GB capacity combined using a hardware raid for local le operations 29  T h e average throughput rate of the local SSDs was measured with 7.6 GB/s cached reads and 1.4 GB/s buered disk reads All nodes were interconnected via a network le system using dedicated 10 Gb/s Ethernet links and switches to share data between nodes Instead of using generated test data we only used real NGS data i.e FASTQ les from the 1,000 genome project for individual measurements 30 W e used the FASTQ le of patient HG00251 for our benchmarks It consumed 160 GB of disk space consists of approx 63 Gbp approx 695 M reads with 91 bp individual read length forming approx 20x coverage The aim of all conducted benchmarks was to minimize the overall execution time for a single GDPP run i.e to use the maximum available compute power to achieve best parallelization B Performance Key Indicators We investigated the following impact factors for controlling the overall pipeline execution 1 Integration We implemented GDPPs based on existing alignment and variant calling tools in our system architecture without any modiìcation of the established tools 2 Adaption We adapted existing GDPPs to use the IMDB as primary storage for data processing and 3 Optimization We optimized the split size parameter controlling the number of distributed compute nodes C Results For our benchmarks we integrated Burrows Wheeler Aligner BWA in our HIG research prototype We designed our benchmarks to compare the impact of the incorporated storage and the level of parallelization on the overall execution time as outlined in Tab II Exp I and III used a FS as primary storage while an IMDB was used for Exp II and IV Exp I and II were executed on a single compute node while Exp III and IV were executed on 25 compute nodes to evaluate the impact of a fully parallelized execution environment 694 


Size  t p I q s t p II q s t p III q s t p IV q s Imp p II q  Imp p III q  Imp p IV q  0.5 1,100 808 283 130 27 74 88 1.0 2,159 1,622 520 245 25 76 89 2.0 3,900 2,860 943 470 27 76 88 4.0 7,029 5,259 1,761 893 25 75 87 7.9 13,626 10,364 3,377 1,733 24 75 87 15.8 25,147 18,707 6,609 3,387 26 74 87 Table I Comparison of execution times in HIG for I BWA split size 1 II the adapted pipeline using the IMDB as primary data storage instead of the FS split size 1 III split size 25 and IV for the adapted pipeline using the IMDB as primary data storage split size 25 t p x q  Execution time for Exp x  Imp p x q  Improvement of execution time for Exp x compared to Exp I calculated as Imp p x q t p I q t p x q t p I q Exp  Experiment  0 5000 10000 15000 20000 25000 30000 0 2 4 6 8 10 12 14 16 Pipeline Execution Time [s FASTQ File Size [Gbps Exp. I Exp. II   Exp. III   Exp. IV   Figure 3 Exp II shows improvements of 24-27  on single compute node compared to Exp I Exp III and IV show improvements up 76  and 89  resp on 25 compute nodes compared to Exp I The overall pipeline execution for varying FASTQ le sizes was measured BWA version 0.6.2 was conìgured to use 80 threads to use the fully available hardware resources of our benchmark systems 31 Exp I and II describe the overall pipeline execution time on a single compute node as shown in Tab I It depicts that the use of the IMDB as primary storage for intermediate results is for all le sizes beneìcial Through this pipeline optimization the runtime can be reduced by at least 25 percent in average Exp III and IV as shown in Tab I document the impact the tuning parameter split size i.e the parallel execution of individual process steps on the overall pipeline execution time Parallel execution of selected pipeline steps reduces the overall execution time by at least 74 percent in average for BWA Additional improvement can be achieved by using the IMDB as primary storage Thus the overall execution time can by reduced by at least 87 percent in average V Evaluation and Discussion Our benchmarks verify two hypothesis Firstly the use of an IMDB as primary storage improves the overall execution time of established alignment algorithms such as BWA integrated in our HIG system architecture as depicted in Fig 3 Secondly our system supports parallel execution of intermediate process steps across multiple compute nodes which results in an additional performance improvement compared to the execution on a single compute node The result incorporating 25 compute nodes show that the system was not completely loaded by our benchmarks since we did not adapt individual processing tools which still implement single threaded execution models The best relative improvement shows the adapted pipeline using the IMDB as primary storage with at least 74 percent on single compute node and up to 89 percent on 25 compute nodes It shows that the overall pipeline execution time correlates to the number of base pairs contained in the FASTQ le in a linear way The scaling factors for the overall execution time varies between 1.80 and 1.96 across all experiments and le sizes This indicates a very constant and predictable system behavior of our HIG system for varying input le size For example a doubled number of reads results in an overall response slightly below a factor of two It helps to predict execution times and to supervise the correct system functionality e.g in case of a broken compute resource or worker process as outlined in Sect III Furthermore our results stress the beneìts of an IMDB for operating on intermediate results The pipeline optimized for the IMDB replaces individual tools operating on les for speciìc process steps such as sorting merging and indexing In contrast these operations are replaced by the IMDB operations without the need to create intermediate les in the File System FS We integrated existing alignment and variant calling tools into our HIG architecture without modifying their code Thus the speed-up documented in our benchmarks is mainly achieved by replacing selected le-based operations by IMDB operations 695 


VI Conclusion and Outlook In the given work we presented our HIG system architecture for genome data processing based on an IMDB system Based on our in-memory research activities we outlined the applicability of this technology for processing of genome data to enable its real-time analysis Furthermore we shared insights in the speciìc architecture layers from an IT perspective and outlined details about select IMDB extensions for genome data processing such as scheduling worker framework or updater framework The obtained benchmark results showed that our HIG system architecture improves overall pipeline execution time by at least 25 percent on a single compute node and up to 89 percent involving 25 compute nodes The performance boost is mainly derived from substituting intermediate processes such as sorting merging and indexing by native IMDB operations In future we will investigate the impact of optimized alignment and variant calling algorithms that directly incorporate IMDB technology We expect to further eliminate media breaks and to improve performance due to data proximity References  F  S  C ollin s et al  New Goals for the U.S Human Genome Project Science  vol 282 no 5389 pp 682 689 1998  W  J  A n s orge  N e x t g en erat ion D N A S e q u en cin g T e c h niques New Biotechn  vol 25 no 4 pp 195Ö203 2009  K  J ain  Textbook of Pers Medicine  Springer 2009 4 M  P  Sc ha pra no w et al  Mobile Real-time Analysis of Patient Data for Advanced Decision Support in Personalized Medicine in Proceedings of the 5th Intêl Conf on eHealth Telemedicine and Social Medicine  2013 5 M  P  Sc ha pra no w  In-Memory Technology Enables History-Based Access Control for RFID-Aided Supply Chains  Springer London 2013 ch 9 pp 187Ö213 6 P l a t t n e r  A Course in In-Memory Data Management The Inner Mechanics of In-Memory Databases  Springer 2013 7 A  K n pf e l  B  G ro ne  a nd P  T a b e l i n g  Fundamental Modeling Concepts Eective Communication of IT Systems  John Wiley  Sons 2006  S  W an d e lt et al  Data Management Challenges in Next Generation Sequencing Datenbank-Spektrum  vol 12 no 3 pp 161Ö171 2012  S  P ab in ger et al  A Survey of Tools for Variant Analysis of Next-generation Genome Sequencing Data Brief Bioinform  Jan 2013 10 K W a ng  M  L i  a n d H  H a k o n a r s o n  ANNO V A R Functional Annotation of Genetic Variants from Highthroughput Sequencing Data Nucleic Acids Res  vol 38 no 16 2010  V  Mak a ro v  T O Grad y  G Cai J Lih m  J  D  Buxbaum and S Yoon Anntools Bioinformatics  vol 28 no 5 pp 724Ö725 Mar 2012 1 All online references were checked on Aug 23 2013  P  Cin golan i A  Plat t s  M  C o o n  T N g u y en  L  W an g S Land X Lu and D Ruden A Program for Annotating and Predicting the Eects of Single Nucleotide Polymorphisms Fly  vol 6 no 2 pp 80Ö92 2012 13 A T  Ho l d e n e r  AJAX The Deìnitive Guide 1sted OêReilly 2008  D  Cro c k f ord  R F C 4627 Th e a p p licat ion  json Med i a Type for JavaScript Object Notation JSON http www.ietf.org/rfc/rfc4627.txt 1  July 2006  A  S  T a n e n b au m Modern Operating Systems 3rded Upper Saddle River NJ USA Prentice Hall Press 2008  L P e rk o v et al  High-Availability using Open Source Software in Proceedings of the 34th Intêl Convention MIPRO  IEEE 2011 pp 167Ö170  Th e W ork  o w Mgm t Coalit ion  Pro cess D e  n i t i on I n terface XML Process Deìnition Language http www.xpdl.org/standards/xpdl-2.2/XPDL6,2013 1 2012  J D e an an d S  G h e ma w a t  Map R e d u c e S i mp li ed Data Processing on Large Clusters in Proceedings of the 6th Symposim on Operating Systems Design and Implementation  2004 pp 137Ö150  M.P  S c h ap ran o w H  Plat t n er a n d C Mein el  A p p lied In-Memory Technology for High-Throughput Genome Data Processing and Real-time Analysis in Upgrade Devices in Tele Medicine  2013 pp 35Ö42  A  Bog K  S a c h s an d H  P lat t n er  I n t e ract iv e P erformance Monitoring of a Composite OLTP and OLAP Workload in Proceedings of the Intêl Conf on Mgmt of Data 2012  ACM 2012 pp 645Ö648  F F  rb er S  K  C h a  J  P rimsc h  C  Born h  v d  S  S igg and W Lehner SAP HANA Database Data Management for Modern Business Applications SIGMOD Rec  vol 40 no 4 pp 45Ö51 Jan 2012  N a t i on al Cen t er for B iot e c h n o logy I n format ion  A ll resources http://www.ncbi.nlm.nih.gov/guide/all 1   S  F o rb es et al  The Catalogue of Somatic Mutations in Cancer A Resource to Investigate Acquired Mutations in Human Cancer Nucleic Acids Res  vol 38 2010  L R  Mey e r et al  The UCSC Genome Browser Database Extensions and Updates 2013 Nucleic Acids Res  2012 25 T  K Da s a nd M  R M i s h r a   A S tudy o n C h a l l e ng e s and Opportunities in Master Data Management Intêl Journal of Database Mgmt Syst  vol 3 no 2 May 2011  Th e G en ome R eferen ce Con s ortiu m   Gen o me A ssemblies http://www.ncbi.nlm.nih.gov/projects/genome assembly/grc/data.shtml 1  27 A Re c t o r  W  N o l a n  a nd S Ka y   F o unda ti o n s f o r a n Electronic Medical Record Methods of Information in Medicine  pp 179Ö186 1991  I n t e l C orp o rat i on   I n t e l P ro d u c t Q u i c k R e feren c e Matrix http://cache-www.intel.com/cd/00/00/47 64/476434_476434.pdf 1  Apr 2011     I n t el S o lid S t a t e D r iv e 520 S e ries Pro d u c t Speciìcation http://www.intel.com/content/dam www/public/us/en/documents/product-speciìcations ssd-520-speciìcation.pdf 1  Feb 2012  Th e 1000 Gen o mes P ro ject Con s  A Map o f H u m an Genome Variation from Population-scale Sequencing Nature  vol 467 no 7319 pp 1061Ñ1073 Oct 2010 31 H Li a n d R  D urbi n  F a s t a nd A ccura te Sho rt Read Alignment with Burrows-Wheeler Transformation Bioinformatics  vol 25 pp 1754Ö1760 2009 696 


 7 5  S UMMARY  The purpose of this paper was to introduce a modifi ed VCM system design for near and deep space communication s Software simulation of this design was performed to  determine throughput improvements and verify the feasibility of the whole design  Problems with fra me descriptor identification were resolved by unique l ength VCM modes and inferring the VCM mode though frame marker spacing.  The simulation looked at an exampl e of a varying communications link polar orbiting CubeSat s Simulation showed that the proposed VCM system roug hly doubles throughput and slightly outperforms similar  VCM codes like SCCC and DVBS2  Future work for the proposed VCM system includes eliminating the frame descriptor from transmission developing an optimal  mode selection algorithm, and developing the proposed sy stem in hardware specifically for CubeSat application A CKNOWLEDGMENTS  The research was carried out at the Jet Propulsion Laboratory California Institute of Technology und er a contract with the National Aeronautics and Space Administration and the University of Alaska Fairban ks College of Engineering and Mines funded in part by  the Alaska Space Grant Program R EFERENCES  1  J   C u t t l e r  a n d  J   M a n n    G l o b a l  G r o u n d  S t a t i o n  Survey 5 th  Annual CubeSat Developers Workshop  August 2008 http://gs.engin.umich.edu/documents/Mann_etal_2008 pd f 2  M   C a f f r e y    E x p l o i t i n g  L i n k  D y n a m i c s  i n  L E O t o Ground Communications 6 th  Annual CubeSat Developerís Workshop  San Luis Obispo CA 2009 http://www.cubesat.org/images/cubesat/presentations Dev elopersWorkshop2009/4_Comm/2_CaffreyLink_Dynamics.pdf 3  F l e x i b l e  A d v a n c e d  C o d i n g  a n d  M o d u l a t i o n  S c h e m e  for High Rate Telemetry Applications CCSDS 131.2-B-1 Blue Book Issue 1 March 2012 http://public.ccsds.org/publications/archive/131x2b 1.pdf 4  T M  S y n c h r o n i z a t i o n  a n d  C h a n n e l  C o d i n g    C C S D S  131.0-B-2  Blue Book Issue 2 August 2011 http://public.ccsds.org/publications/archive/131x0b 2ec1.p df 5  J   H a m k i n s    P e r f o r m a n c e  o f  l o w d e n s i t y  p a r i t y check coded modulation Aerospace Conference 2010 IEEE  vol., no., pp.1-14, 6-13 March 2010 6  S   O l i v i e r i    M o d u l a r  F P G A B a s e d  S o f t w a r e  D e f i n ed Radio for CubeSats Worcester Polytechnic Institut e May 2011  7  D i g i t a l  V i d e o  B r o a d c a s t i n g   D V B    S e c o n d  g e n e r ation framing structure, channel coding and modulation sy stems for Broadcasting Interactive Services News Gather ing and other broadband satellite applications ETSI EN  302 307, v1.1.2,2006 B IOGRAPHIES   Thomas A Sielicki  received a B.S degree in electrical engineering from the University of Virginia in 2010.  He is currently a graduate student at the University of Alaska Fairbanks     Jon Hamkins Sí94 Mí96 SMí03 received a B.S degree in electrical engineering from the California Institute of Technology Caltech Pasadena in 1990 and the M.S and Ph.D degrees in electrical and computer engineering from the University of Illinois at UrbanaChampaign in 1993 and 1996 respectively  Since then he has been with the Jet Propulsion Laboratory Caltech where he is now supervisor of the Information Processing Group   His research interests include error control theory, in formation theory autonomous receivers ranging and optical communications  Denise Thorsen  Sí82 Mí97 received her B.S. \(1985\, M.S. \(1991\ and Ph.D 1996 degrees in electrical and computer engineering from the University of Illinois at UrbanaChampaign She is an Associate Professor in Electrical and Computer Engineering at the University of Alaska Fairbanks   


pp. 1-5 2009 3 S  Ag a r wa la  D Ja d a v  a n d L  A B a t h e n   i C o st a l e  Ad a p t i v e  C o st  Optimization for Storage Clouds," in pp. 436-443, 2011 4 M  Ar mb ru st  A Fo x   R  Gri f fi t h  A D  J o s e p h   R  Ka t z  A Kon wi n sk i   G. Lee, D. Patterson, A. Rabkin, I. Stoica, and M. Zaharia, "A View of Cloud Computing vol. 53, pp. 50-58, 2010 5 M   S  Av i l a Ga rc i a  X  Xi on g A E   T r e f e t h e n   C   C r i c h t on  A T s u i   a n d  P. Hu, "A Virtual Research Environment for Cancer Imaging Research in pp. 1-6 2011   J  Ba l i g a R W   A y re  K   H int o n and R  S   T u ck er  G reen c lo u d  computing: Balancing energy in processing, storage, and transport vol. 99, pp. 149-167, 2011   R. B o s e and J   F r ew   L in e a ge R e tr i e va l  for S c i e n tific Data Pro c es s ing  A  Survey vol. 37, pp. 1-28, 2005   A   Bu r t o n and A  T r elo a r P ub l i s h M y D a ta  A  C o m p os iti o n of  Serv i c e s  from ANDS and ARCS," in pp. 164-170, 2009   P. Ch e n  B Pl a l e, and M S A k tas  T em por a l  re pres e n t a ti o n  fo r s c i e n tific data provenance," in pp. 1-8, 2012   Y  Cui, H  W a ng and X   Ch e n g C hann el A llocati o n  in W i reles s  Data Center Networks," in pp. 1395-1403, 2011   E  Deelm an  G  S i n g h M L i v n y  B  B e rr im an, and J   G o o d  T h e C o s t  of  Doing Science on the Cloud: the Montage Example," in pp. 1-12, 2008   I  F o s t er J   Vo ck ler M. W i l d e   and Z. Yo n g  C him er a  A V ir tu a l  D a ta System for Representing, Querying, and Automating Data Derivation," in pp. 37-46, 2002   I  F o s t er, Z. Y o n g I   Ra icu and S  L u  C lo ud C o m putin g and G r id  Computing 360-Degree Compared," in pp. 1-10, 2008   S K  G a rg   R  Bu y y a and H   J   Si egel  T i m e and C os t T r ad e Off  Management for Scheduling Parallel Applications on Utility Grids vol. 26, pp. 1344-1355, 2010   P. K  G unda L   R a v indr an ath C A  T h ekkath, Y  Y u   and L  Z huan g  Nectar: Automatic Management of Data and Computation in Datacenters," in pp. 1-14, 2010   X. H u an g  Z  L u o and B  Yan C y b er infr a s t r uctu re  and e-S ci e n c e  Application Practices in Chinese Academy of Sciences," in pp. 348-354 2011   M  H u m p h r ey  N  B e e k w i l d er J   L   G ood a l l  and M B E r c a n Calibration of watershed models using cloud computing," in pp. 1-8, 2012   G  J u ve E. Deelm an, K  V a hi  and G   M e h t a, "D ata S h a r in g O p ti o n s  for  Scientific Workflows on Amazon EC2," in pp. 1-9, 2010   D. K ond o   B. J a v a di  P Ma lec o t, F   Capp ello and D  P  A n d e r s o n   C o s t Benefit Analysis of Cloud Computing versus Desktop Grids," in pp. 1-12, 2009 20  X  L i u  Z  Ni  D  Yu a n  Y  J i a n g Z   Wu  J  C h en   a n d  Y  Ya n g   A N o v e l Statistical Time-Series Pattern based Interval Forecasting Strategy for Activity Durations in Workflow Systems vol. 84, pp. 354-376, 2011   B  L uda s c h e r   I   A ltint as  C B e r k ley  D H i gg in s  E J a eger, M  J o n e s  and E. A. Lee, "Scientific Workflow Management and the Kepler System pp. 1039Ö1065 2005   K  K  Munis w am y R e dd y  P  Mack o  and M. Selt zer   P rove nanc e  for th e  Cloud," in pp. 197-210, 2010   H Ng u y en and D A b r a m s o n  W ork W a y s   I n t e r acti ve W o r k flow b a s e d  Science Gateways," in pp. 1-8, 2012   L  J  Os t e rw ei l  L  A   C l a r k e A   M E llis o n   R  Po d o roz hn y A  W i s e   E   Boose, and J. Hadley, "Experience in Using A Process Language to Define Scientific Workflow and Generate Dataset Provenance," in pp. 319-329, 2008   J  Q iu, J  Ekana y ak e  T  G una r a thn e   J   Y Ch o i, S  H  Ba e, H  L i  B   Zhang, Y. Ryan, S. Ekanayake, T.-L. Wu, A. Hughes, and G. Fox Hybrid Cloud and Cluster Computing Paradigms for Life Science Applications vol. 11, 2010 26  X  Su  Y M a   H  Ya n g   X C h a n g  K  N a n  J  Xu  a n d  K N i n g   An Op en Source Collaboration Environment for Metagenomics Research," in pp. 7-14, 2011 27  A   S  S z al ay a n d J   G r ay    S c i e n ce i n a n  Ex po ne n t i a l  W o r l d    vol 440, pp. 23-24, 2006   S. Toor  M. S a b e s a n, S  H o lm gre n and T   R i s c h, "A S c a l ab le A r chit ecture for e-Science Data Management," in pp. 210-217, 2011   D W a r n e k e and O  K a o  E x p loiting Dy na m i c R e s o u r c e A llocati o n  for  Efficient Parallel Data Processing in the Cloud vol. 22, pp. 985-997, 2011   Y. Y a n g K  L i u, J   Ch e n X  L i u, D   Yuan and H   J i n   A n A l gorithm  in SwinDeW-C for Scheduling Transaction-Intensive Cost-Constrained Cloud Workflows," in pp. 374-375, 2008  L   Yo un g Ch oo n and A  Y Z o m a y a   E n e rgy C o ns ci o u s Sch e du l in g for  Distributed Computing Systems under Different Operating Conditions vol. 22, pp. 13741381, 2011   D Y u an  Y   Y a n g  X   L i u  and J  Ch e n   A C o st-Effecti ve S t r a t e gy  for Intermediate Data Storage in Scientific Cloud Workflows," in  pp. 1-12, 2010   D Y u an Y   Y a n g   X   L i u and J  Ch e n  O nd e m a nd Minim um C os t  Benchmarking for Intermediate Datasets Storage in Scientific Cloud Workflow Systems vol 71, pp. 316-332, 2011 34  D  Yu a n   Y  Ya n g X L i u  W  L i  L   C u i   M  Xu  a n d  J   C h en    A Hi gh l y Practical Approach towards Achieving Minimum Datasets Storage Cost in the Cloud vol 24, pp. 1234-1244, 2012 35  D  Yu a n   Y   Ya n g  X L i u  G Z h a n g  a n d  J  C h e n    A Da t a D e p e n d e n c y  Based Strategy for Intermediate Data Storage in Scientific Cloud Workflow Systems vol. 24, pp. 956-976, 2012   M. Z a h a r i a, A  K o n w ins ki A  D   J o s e ph R. K a t z and I  S t o ica  Improving MapReduce Performance in Heterogeneous Environments," in pp. 29-42, 2008  
real world, the price of cloud storage is different according to different usages. In the future, we will incorporate more complex pricing models in our datasets storage cost model Furthermore, methods for forecasting dataset usage frequency can be further studied, with which our T-CSB algorithm can be adapted to different types of applications more easily A CKNOWLEDGMENT  The research work reported here is partly supported by Australian Research Council under DP110101340 and LP130100324, Shanghai Knowledge Service Platform Project No. ZF1213. We are also grateful for the discussions on Finite Element Modelling application with Dr. S. Xu from Faculty of Engineering and Industrial Sciences, Swinburne University of Technology R EFERENCES    A m a zo n C l o ud S e rv ic es    http://aws.amazon.com  2  I  A d a m s  D  D  E  L o n g  E  L   M i l l e r   S  P a s u p a t h y  a n d  M  W  S t o r e r   Maximizing Efficiency by Trading Storage for Computation," in 
Workshop on Hot Topics in Cloud Computing \(HotCloud'09 IEEE International Conference on Cloud Computing \(CLOUD2011 Communication of the ACM 7th International Conference on E-Science \(e-Science2011 Proceedings of the IEEE ACM Computing Surveys 5th International Conference on E-Science \(eScience'09 8th International Conference on E-Science \(eScience2012 IEEE INFOCOM 2011 ACM/IEEE Conference on Supercomputing \(SC'08 14th International Conference on Scientific and Statistical Database Management, \(SSDBM'02 Grid Computing Environments Workshop \(GCE'08 Future Generation Computer Systems 9th Symposium on Operating Systems Design and Implementation \(OSDI'2010 7th International Conference on E-Science \(e-Science2011 8th International Conference on E-Science \(e-Science2012 ACM/IEEE Conference on Supercomputing \(SC'10 23th IEEE International  Parallel & Distributed Processing Symposium IPDPS'09 Journal of Systems and Software Concurrency and Computation: Practice and Experience 8th USENIX Conference on File and Storage Technology  FAS T'10 8th International Conference on E-Science \(eScience2012 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering Journal of BMC Bioinformatics 7th International Conference on E-Science \(e-Science2011 Nature 7th International Conference on EScience \(e-Science2011 IEEE Transactions on Parallel and Distributed Systems 4th International Conference on E-Science \(eScience2008 IEEE Transactions on Parallel and Distributed Systems 24th IEEE International Parallel & Distributed Processing Symposium \(IPDPS'10 Journal of Parallel and Distributed Computing IEEE Transactions on Parallel and Distributed Systems Concurrency and Computation: Practice and Experience 8th USENIX Symposium on Operating Systems Design and Implementation \(OSDI'2008 
292 


Jorda Polo, David Carrera, Yolanda Becerra, Malgorzata Steinder  and Ian Whalley. Performance-driven task co-scheduling for  mapreduce environments. In Network Operations and Management  Symposium \(NOMS\2010 IEEE, pages 373 Ö380, 19-23 2010 12 K. Kc and K. Anyanwu, çScheduling hadoop jobs to meet deadlines  in 2nd IEEE International Conference on Cloud Computing  Technology and Science \(CloudCom\, 2010, pp. 388 Ö392 13 Xicheng Dong, Ying Wang, Huaming Liao çScheduling Mixed Real time and Non-real-time Applications in MapReduce Environment  In the proceeding of 17th International Conference on Parallel and  Distributed Systems. 2011, pp. 9 Ö 16 14 Xuan Lin, Ying Lu, J. Deogun, and S. Goddard. Real-time divisible  load scheduling for cluster computing. In Real Time and Embedded  Technology and Applications Symposium, 2007. RTAS ê07. 13th  IEEE pages 303 Ö314, 3-6 2007 15 HDFS  http://hadoop.apache.org/common/docs/current/hdfsdesign.html  16 Chen He, Ying Lu, David Swanson. çMatchmaking : A New  MapReduce Scheduling Techniqueé. In the proceeding of 2011  CloudCom, Athens, Greece, 2011, pp. 40 Ö 47 17 Matei Zaharia, Dhruba Borthakur, Joydeep Sen Sarma and Khaled  Elmeleegy, Scott Shenker, and Ion Stoica, çDelay scheduling: a  simple technique for achieving locality and fairness in cluster  schedulingé. In the proceedings of the 5th European conference on  Computer systems, 2010.  pp 265-278 18 Zhuo Tang, Junqing Zhou, Kenli Li, and Ruixuan Li "A MapReduce  task scheduling algorithm for deadline constraints.", Cluster  Computing, Vol. 15,  2012 19 Eunji Hwang, and Kyong Hoon Kim. "Minimizing Cost of Virtual  Machines for Deadline-Constrained MapReduce Applications in the  Cloud." Grid Computing \(GRID\, 2012 ACM/IEEE 13th  International Conference on. IEEE, 2012 20 Micheal Mattess, Rodrigo N. Calheiros, and Rajkumar Buyya  Scaling MapReduce Applications across Hybrid Clouds to Meet Soft  Deadlines." Technical Report CLOUDS-TR-2012-5, Cloud  Computing and Distributed Systems Laboratory, the University of  Melbourne, August 15, 2012 21 
 
11 
                
Chen He, Ying Lu, David Swanson. çReal-Time Application Scheduling in Heterogeneous MapReduce Environments Technical Report TR-UNL-CSE2012-0004, University  of Nebraska-Lincoln, 2012 Available: http://cse apps.unl.edu/facdb/publications/TR-UNL-CSE20120004.pdf 22 T. Condie, N. Conway, P. Alvaro, J. M. Hellerstein, K  Elmleegy, and R. Sears. çMapreduce Onlineé. In NSDI 2010 23 A. D. Ferguson, P. BodÌk, S. Kandula, E. Boutin, and R  Fonseca. çJockey: Guaranteed Job Latency in Data Parallel Clusters. In EuroSys, 2012 24 G. Wang, A. R. Butt, P. Pandey, and K. Gupta. çA Simulation Approach to Evaluating Design Decisions in MapReduce Setupsé. In MASCOTS 2009 25 H. Herodotou and S. Babu. Profiling, çWhat-if Analysis and Cost-based Optimization of MapReduce Programs In VLDB 2011 26 H. Herodotou, F. Dong, and S. Babu. çNo One \(Cluster Size Fits All: Automatic Cluster Sizing for Dataintensive Analyticsé. In SoCC 2011  
1544 
1544 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


