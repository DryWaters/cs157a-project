 














bins and nally either a sort of one or more bins or a re-run of the algorithm on one of these bins Many of these processing steps can be parallelized at the cost of transferring the data Pig and Terasort are signiìcantly slower than the selection algorithms as they perform a total sort on all the data Most of the Pig results are not shown on the charts as they are over 2x slower than Terasort As can be seen in Figure 4 Terasorts demarcation between the map and reduce phase is very clear In the map phase each point is assigned a bin and distributed to a reduce node During the map phase the average CPU usage is very high but starts decreasing as nodes start nishing their point distribution and become idle Once the Reduce phase starts then the CPUs are again mostly utilized until they nish their sort of the data As can be seen the data distribution during the Map phase is very low until nodes start nishing their distribution where they need to transfer their points to the Reducers The Reducer phase shows a continuous high data transfer as the results are written back to the HDFS The main problems with Terasort are during the map phase node CPU is under utilized as each node begins transferring data or nishes processing and during the reduce phase the time spent sorting the data accounts for about half of the time of the total algorithm 4 So we can expect that by simply not performing a total sort we should be able to create selection algorithms that are twice as efìcient The algorithms that only do a partial sort of the nal data are between 2x and 4x times faster than TeraSort on the 1TB dataset As can be seen in Figures 5 2 the BinMedian CPU curve during the map phase is similar to Terasort but less CPU is used and the phase is much quicker The reduced CPU comes from emitting already sorted keys to the reducers skipping the additional overhead of the default Hadoop sort This leads to both less CPU used and a quicker map phase The CPU proìles in Figure 6 and Figure 7 show the two stage approach of nding the bin rst then distributing the points First it is important to notice that both approaches show signiìcant speedups over Terasort despite performing redundant work in the form of repeating the bin calculations for each point twice More importantly despite having nearly the same algorithm for selecting the bins and distributing the points they have very different CPU and network proìles Especially noticeable is the CPU utilization and network utilization between MrS and MrT MrS is the only algorithm to have consistently high CPU utilization that does not decrease as the mapping phase completes The reason for this is that it needs no data transfer in between its stages The network overhead of transferring data inside of Hadoop is a bottleneck for CPU use that is eliminated So it appears that performance of the algorithms is tied to the data transfer As the Hadoop framework transfers data Figure 8 Scalability Results showing the decrease in the total amount of time needed for each algorithm as a function of the number of processors used for 100G of data Number of processors include the Namenode\(which doesnêt perform work directly from one node to another the CPU utilization is reduced and the time increases This effect is exacerbated on networks that utilize a replication factor for redundancy Since Hadoop places the data a priori on the nodes it is more efìcient to perform calculations on the same data if possible The MrS algorithm performs multiple passes on the same data in successive Map only phases The rst sweep through the data merely counts how many items will be in each bin and performs no other calculations so no data is transferred In the second sweep data that belongs to the appropriate bin are written out As each phase is discrete no reducers will be started until all maps are completed and all network trafìc is cabined to the end of each Map phase This approach despite more passes on the same data outperforms the others because the data transfer is minimized The speedup of the algorithms for the 100G dataset does not show a linear improvement see Figure 8 This shows a general trait of Hadoop that it is built for very large scale datasets As shown in the other results the data transfer in Hadoop can be very costly With this small size of dataset as the nodes increased the ratio of the network overhead versus the CPU time becomes a bottleneck for the algorithms In this case algorithms that minimized the amount of trafìc back and forth showed the best time improvement Runs with larger data sets would be expected to demonstrate the linear speedups provided by these algorithms because the transfer time would comprise a smaller portion of total run time V C ONCLUSION Many algorithms in the elds of indexing biology and statistics have a subcomponent that utilizes a k th point especially a median and that is often called recursively As selection nding is often a step where a total order is not needed having fast selection algorithms can greatly improve the overall running time of these algorithms The MapReduce model allows for the use of these algorithms to scale linearly on large scale datasets The open source java 417 


Figure 4 TeraSort Proìle showing the CPU usage CPU Bytes sent across the network send Bytes written to disk writ and the percentage complete d of the map/reduce phase Figure 5 BinMedian Proìle showing the CPU usage CPU Bytes sent across the network send Bytes written to disk writ and the percentage completed of the map/reduce phase implementation of MapReduce Hadoop was used as for the comparison but the underlying principles of the network and CPU utilization should generalize to other MapReduce frameworks Many selection algorithms can easily be made using the Hadoop framework but it is a non-obvious problem as to which approach will best take advantage of the Hadoop system This paper shows the implementation of several algorithms but more importantly shows the underlying reasons which explain why each algorithm performs the way it does Exposing the underlying CPU I/O access and data transfer for each of the algorithms gives concrete evidence about not only these algorithms but also ideas on how to write future Hadoop programs Several approaches to the selection problem utilizing a variety of algorithms and implementations were analyzed here in an effort to understand what is the best way to use Hadoop to solve the selection problem As with many high performance parallel systems reducing the amount of data passed between nodes and ensuring a relative load balance among processors is important Some of the approaches that can be used include naively sorting changing the binning criteria from Mappers to Partitioners moving work from the Mapper to the Reducer stage or visa versa or 418 


Figure 6 MrT Proìle showing the CPU usage CPU Bytes sent across the network send Bytes written to disk writ and the percentage completed of the map/reduce phase Figure 7 MrS Proìle showing the CPU usage CPU Bytes sent across the network send Bytes written to disk writ and the percentage completed of the map/reduce phase even eliminating one of the Map/Reduce phases A correct selection algorithm can be achieved through any of the approaches but given the multitude of options guidance is needed to direct programmers toward more optimal solutions In particular it is clear that avoiding data transfer is desirable in Hadoop even if it means performing multiple stage Map/Reduce algorithms From the results shown here it can be seen that the preferable approach is utilizing an algorithm that performs multiple Map phases over the same data to eliminate any sorting steps or data transfer until the last possible moment Using this approach selection of a point can be sped up by several times over the optimized TeraSort or other selection algorithms An efìcient and scalable selection nding method has the potential to provide general beneìt to a number of applications The results show that the methods proposed in this paper out perform several common alternatives in identifying medians with Hadoop including using sorting Pig and BinMedian methods If incorporated into some of the popular statistical packages for Hadoop all users will gain quicker access to the results they need 419 


A CKNOWLEDGEMENTS The authors acknowledge the Texas Advanced Computing Center TACC at The University of Texas at Austin for providing HPC resources that have contributed to the research results reported within this paper URL:http://www.tacc utexas.edu This research was partially supported by the National Institutes of Health NIH Grants R01GM08533703 R EFERENCES  J Dean and S Ghema w at Mapreduce simpliìed data processing on large clusters Commun ACM  vol 51 no 1 pp 107Ö113 Jan 2008  C Olston B Reed U Sri v asta v a  R  K umar  and A T omkins Pig latin a not-so-foreign language for data processing in Proceedings of the 2008 ACM SIGMOD international conference on Management of data  ser SIGMOD 08 New York NY USA ACM 2008 pp 1099Ö1110  M Blum R W  Flo yd V  Pratt R L Ri v est and R E Tarjan Time bounds for selection J Comput Syst Sci  vol 7 no 4 pp 448Ö461 Aug 1973  C A R Hoare  Algorithm 65 nd  Commun ACM  vol 4 no 7 pp 321Ö322 Jul 1961  C A R Hoare Quicksort  The Computer Journal  vol 5 no 1 pp 10Ö16 1962  M Caf aro V  De Bene and G Aloisio Deterministic par allel selection algorithms on coarse-grained multicomputers Concurr Comput  Pract Exper  vol 21 no 18 pp 2336 2354 Dec 2009  S Stadtm  uller S Speiser A Harth and R Studer Datafu a language and an interpreter for interaction with read/write linked data in Proceedings of the 22nd international conference on World Wide Web  ser WWW 13 Republic and Canton of Geneva Switzerland International World Wide Web Conferences Steering Committee 2013 pp 1225Ö1236  R J T ibshirani F ast computation of the median by successive binning Computing Research Repository CoRR  vol abs/0806.3301 2008  H Prodinger   Multiple quickselect&mdash;hoare s nd algorithm for several elements Inf Process Lett  vol 56 no 3 pp 123Ö129 Nov 1995  S G Akl  A n optimal algorithm for parallel selection  Inf Process Lett  vol 19 no 1 pp 47Ö50 Sep 1984  P  Gupta and G P  Bhattacharjee  A parallel selection algorithm BIT  vol 24 no 3 pp 274Ö287 1984  D A Bader   An impro v ed randomized algorithm for parallel selection with an experimental study J Parallel Distrib Comput  vol 64 no 9 pp 1051Ö1059 Sep 2004  S Rajasekaran Randomized parallel selection  i n Proceedings of the tenth conference on Foundations of software technology and theoretical computer science  ser FST and TC 10 New York NY USA Springer-Verlag New York Inc 1990 pp 215Ö224  L Monroe J W endelber ger  and S Michalak Randomized selection on the gpu in Proceedings of the ACM SIGGRAPH Symposium on High Performance Graphics  ser HPG 11 New York NY USA ACM 2011 pp 89Ö98  T e xas Adv anced Computing Center   T e xas adv anced computing center http://www.tacc.utexas.edu 2006 On A v ailable  http://www.tacc.utexas.edu/user-services user-guides/stampede-user-guide  420 


boiler model and hard constraints [J  E n e r gy 201 1 29   22 39 2251  3  K.L.Lo, Y.Rathamarit  State estimation of a boiler model using the unscented Kalman filter [J  I E T  Gener. Transm. Distrib.2008 2 6\917 931  4  Un Chul Moon, Kwang. Y.Lee. Step resonse model development for dynamic matrix control of a drum type boiler turbine system [J IE E E  T ra nsactions on Energy Conversion.2009 24 2\:423 431  5  Hacene Habbi, Mimoun Zelmat, Belkacem Ould Bouamama. A dynamic fuzzy model for a drum boiler turbine system [J  A u to m a tic a 2 0 0 9 39:1213 1219  6  Beaudreau B C. Identity, entropy and culture J   J o ur na l  o f  economic psychology, 2006, 27\(2 205 223  7  YANG M, CHEN L. Information Technique and the Entropy of Culture J  A cad e m i c E x ch a n g e  2006, 7: 048  8  ZHANG Zhi feng. Research on entropy change model for enterprise system based on dissipative structure J  Ind ustrial  Engineering and  Management 2007, 12\(1\ :15 19  9  LI Zhi qiang, LIU Chun mei Research on the Entropy Change Model for Entrepreneurs' Creative Behavior System Based on Dissipative Structure J  C h i n a S of t S c i e n c e  2009   8  1 62 166   458 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





