Two Layer Cache for RDF Database 
Abstract 
Xiaolin Yi Beijing University of Technology/College of Computer Science Beijing, China yixiaolin@bjut.edu.cn Fan Zheng Beijing University of Technology/College of Computer Science Beijing, China zhengfan1988@gmail.com Wei Jia Lenovo Reseach& Technology Group, Lenovo Group Limited Beijing, China jiaweia@lenovo.com 
With the Semantic Web 1 
 
has been paid more and more attention, there are more and more related applications For instance, Google has provided semantic retrieval functions you can get the information of some famous people directly RDF 2 database as the data storage tool of Semantic Web also gets many attentions. Now, there are some well-known RDF databases such as Jena 3 Sesame 4 Bigdata 5 and so on The role of RDF database is to provide data service for Semantic Web applications. The important metric of database is 
the query efficiency. Now, most of RDF databaseês efficiency h 
as not reached high level and there is a large gap between it and 
commercial demand 6   In this paper, we present a two layer cache mechanism: RC RDF database Cache\ speed up the queries and relieve the problem that the runtime of some queries is too long in RDF database. By implementing and test, the query efficiency can increase thirty-one percent or more after using this cache mechanism 
Keywords RDF\(ResourceDescriptionFramework\ Database Cache, RC\(RDF database Cache 
Heading 1 
 
I I NTRODUCTION   In 2001, Berners Lee put forward the concept of Semantic Web. It is the change and ex tension of the World Wide Web 
and its goal is to make the co mputers understand the semantic of the information and be able to provide the dynamic and active service. In the Semantic Web, the info between people and computer or computer and computer can get more fully utilization RDF database is an important part of the Semantic Web Whatever Semantic Web application you do, you can 
t complete it without it. With the Semantic Web gets a hot topic more and more applications have appeared in our life and the applications have more and more high demand for query efficiency of RDF database in the same time To use cache is the most direct way to improve the query efficiency. There is not cach e special for RDF database and the cache meet s 
 
the user's query features. So, RC was designed and realized The RC is a two layer cache for RDF database. The first layer uses ARC-Memory algorithm to improve the query efficiency and the second layer uses Key-Value DB to relieve the problem: the runtime is too long when query is too complicated or query result is too large The rest of the paper is structured as follows. Section 2 introduces related base info about RC and original intention of designing it. Section 3 describes the framework of RC. Section 4 describes the algorithm in RC. In Section 5 the performance results will be presented II B ACK G ROUND There are many cache replacement algorithms, and it can be 
A Cache Replacement Algorithm 
divided into two kinds. One is the traditional replacement algorithm, the other is self-adaptive algorithm. The traditional algorithm has FIFO, LRU, MRU and so on. They decide to replace by its call frequency and its call time. The method of judgment is very simple and most of them are especially for a fixed access model Because of the traditional algorithm is not very efficiency and comprehensive, so the researchers put attention to the selfadaptive algorithm. The most of self-adaptive algorithms use strategy based on the time an d frequency balance. There are many famous algorithm, for instance ARC 8 LRFU and so on 
Now, There are many RDF databases, we can divide them into two kinds by storage media. One is relational database 
B RDF Database 
the other is non-relational database. The non-relational database is more and more popular with the Semantic Web development and the researchers found low-performance of to store RDF data in relational database The loading rate and query efficiency are the metric of the RDF database. From the application point of view, it pays more attention to query efficiency. If the RDF database can 
t support fast concurrent query, it will be eliminated by Semantic Web after some time There are many open-source RDF database, for example Bigdata, Sesame and so on. AllegroGraph  is a famous commercial RDF database, it supports storing a large number triples and provides high-p erformance data service 
 
There is not a cache mechanism that meets user 
s query features and improves query efficiency significantly RDF database is a very important part of Semantic Web Along with the Semantic Web is more and more popular, there is more and more commercial application about Semantic Web. It proposes more stri ngent requirements about RDF database at the same time There are many tests for RDF database. LUBM  is a famous test in them. It has fourteen queries. You can get the test set from its data generator then to use the test queries to test the performance of the RDF database. If you have done it 
 
C Important Problem 
2013 10th Web Information System and Application Conference 978-0-7695-5134-0/13 $26.00 © 2013 IEEE DOI 10.1109/WISA.2013.10 11 
2013 10th Web Information System and Application Conference 978-0-7695-5134-0/13 $26.00 © 2013 IEEE DOI 10.1109/WISA.2013.10 11 
2013 10th Web Information System and Application Conference 978-0-7695-5134-0/13 $26.00 © 2013 IEEE DOI 10.1109/WISA.2013.10 11 
2013 10th Web Information System and Application Conference 978-0-7695-5134-0/13 $26.00 © 2013 IEEE DOI 10.1109/WISA.2013.10 11 
2013 10th Web Information System and Application Conference 978-1-4799-3219-1/13 $31.00 © 2013 IEEE DOI 10.1109/WISA.2013.10 11 


RC 000\035\000\003 RDF 000\003 database Cache 
INFO\\LUBM Query Query 2 Query 6 Result Triples 2,528 83,557,706 Time \(s 278.321 389.062 Query Content 
 
Using query string to Generate MD5 Key In First Floor Cache Y Y In Second Floor Cache Y Y Generate Insert First Floor Cache Thread Y Y First Floor Cache Full Generate Insert Second Floor Cache Thread Second Floor Cache Full Insert data in First Floor Cache Y Y Eliminate data Eliminate Data Insert data in Second Floor Cache N N Insert data in First Floor Cache Bigdata Get Result from Bigdata N N Key/Value database Get Rusult From Second Floor Cache Result N N Begin End Y Y N N 
First Layer:Memory This layer is using ARC-Algorithm It meets user's query features and guarantees high hit rate Second Layer:Memory  Key/Value Database This layer is using LRU-KV algorithm It is primarily responsible for storing the timeout query result to improve query efficiency When query result is removed by first layer and its runtime exceeds limit time it will be put into second layer When hitting in second layer the query result will be put into first layer 
you maybe find something: some queries spend much time to get result. For instance, the se cond query and the sixth query cost much time to run. The reason of it is the query is too complicated or the query result is too large AllegroGraph is high-performance commercial RDF database. Its LUBM8000 test result is shown in Table 001\011  TABLE I A LLEGRO G RAPH LUBM8000 T EST R ESULT From the Table 001\011 the runtime of Query2 and Query6 is very long, and the result of Query6 is very large. From the query s content, we can know Query2 is a complicated query and Query6 is a simple query Now, we find an important problem in RDF database that the runtime is very long when the query is very complex or the result set is very large. For example, the second query is very complicated and the result set of the sixth query is very big in LUBM Test III RC F RAMEWORK To improve the query efficiency and alleviate the problems caused by the query result set is too large or query is too complex of RDF database, the RC is designed and realized The Cache has two layers. The first layer implemented the ARC-memory algorithm and realize in memory. The second layer realized in Key-Value database. For more information see Figure 1 Fig. 1 RC Framework The ARC-Memory algorithm of the first layer meets query features of user and can get high level of hit ratio. It can help database getting better query ef fect. For more information about this algorithm, see Section 4 To resolve the problem caused by query too complicated or result set too big, the second layer was designed. The data that put in the second layer must meet two conditions The data must be removed by the first layer The runtime of the query must exceed the limit of time The second layer has two parts: Key-pool, KV. Key-pool keeps a fixed-size LRU queue to store the MD5 key of query KV is realized in Key/Value database to store the MD5 key and the query result. For more information, see Section 4 The detail process flow is in Figure 2 Fig. 2 RC Process Flow 
PREFIX rdf http://www.w3.org/19 99/02/22 rdf-syntax ns PREFIX ub http://www.lehigh.ed u/~zhp2/2004/0401/uni v bench.owl SELECT ?X,?Y, ?Z WHERE   Xrdf:typeub:Gradu ateStudent Yrdf:typeub:Universit y Zrdf:typeub:Departme nt Xub:memberOf ?Z Zub:subOrganization Of ?Y X ub:undergraduateDegre eFrom ?Y  PREFIX rdf http://www.w3.org/19 99/02/22 rdf-syntax ns PREFIX ub http://www.lehigh.ed u/~zhp2/2004/0401/uni v bench.owl SELECT ?X WHERE X rdf:typeub:Student 
002 002 
12 
12 
12 
12 
12 


2ÉKey 
A Cache Hit Ratio Test 
Cache size  Algorithm FIFO LRU ARCC 200 
 INPUT 
Figure 2 shows the process flow of RC. Because of the query string is not a fix-number long it can be transferred to MD5 code as the key IV ARC-M EMORY A LGORITHM Adaptive Replacement Cache \(ARC\lacement algorithm with high performance developed at the IBM ALmaden Research Center. Its h it ratio is higher than LRU algorithm. This is accomplished by keeping track of both frequently and recently used pages plus a recent eviction history for both Because of the design of first layer is realized in memory so we change the ARC algorithm into ARC-memory algorithm that make the B1 and B2 all in the memory The ARC-memory algorithm To compare hit ratio in the test, the LRU algorithm and FIFO algorithm also be realized in memory The OLTP  trace will be as the test set. Due to all algorithms realized in memory, the second field \(logical block address\ OLTP trace will be used. The logical block address will be as the query. For more information about this test, see Table 001\012 and Figure 4 TABLE II T HE R ESULT OF C ACHE H IT R ATIO In Table 001\012  ARCC is the ARC-memory algorithm. The hit ratio of every replacement alg orithm grows with the increase of the cache size. The improv e ARC algorithm is the best of three replacement algorithm. For detail trends, see Figure 3 
0.26265 0.27584 0.29442 400 0.31671 0.33767 0.35618 600 0.35272 0.37641 0.39546 800 0.38054 0.40618 0.4253 1000 0.40443 0.43134 0.44938 1200 0.42459 0.45271 0.46967 1400 0.44204 0.47148 0.48694 1600 0.45714 0.48746 0.5025 1800 0.47056 0.50158 0.5174 2000 0.48298 0.51458 0.5304 2200 0.49416 0.52713 0.54182 2400 0.50445 0.5386 0.55271 2600 0.51453 0.54895 0.56232 2800 0.52428 0.55892 0.57136 3000 0.53362 0.56888 0.58015 3200 0.54152 0.57815 0.58831 3400 0.54901 0.58666 0.59684 3600 0.55681 0.59443 0.60389 3800 0.56363 0.6017 0.61099 4000 0.57139 0.60876 0.61743 
The MD5 Key x of the Query String, Key-1,Key-n Set p=0 and set the LRU lists T1,T2,B1 and B2 to empty Key-n is in T1 or T2 Move the element of Key-n to MRU position in T2 Key-n is in B1 If |B1| >=|B2| u=1 Else u=|B2|/|B1|. End If Update p = MIN { p + u , c Key-n , p Move the element of Key-n from B1 to the MRU position in T2 Key-n is in B2 If |B2|>=|B1| u=1. Else u=|B1|/|B2|. End If Update p=MAX { p - u , 0 Key-n , p Move the element of Key-n from B1 to the MRU position in T2 Key-n is not in T1 001 T2 001 B1 001 B2 T1|+|B1| >= c If \(|T1| <c Delete LRU page in B1 Key-n , p Else Delete LRU page in T1 End If T1+|B1| <c s = |T1|+|T2|+|B1|+|B2 If\(s>=c If\(s==2c Delete LRU page in B2 End If Key-n , p End If Move the element of Key-n to MRU position in T1 If \( \(T1 is not empty\ and \(\( |T1| > p\ or \(Key-n is in B2 and T1|=p Move the LRU page in T1 to MRU position in B1 Else Move the LRU page in T2 to MRU position in B2 End If ARC-Memory algorithm has get high level hit ratio, we will test the hit ratio of it and other algorithms in Section 5 V P ERFORMANCE T EST To show RC performance result, two tests will be done. The first test will prove the high hit ratio of ARC-memory algorithm in first layer of RC. Then, the second test will put the RC into Bigdata 
INITIALIZATION Case I Case II Call ADJUST Case III ADJUST Case IV Case A ADJUST Case B ADJUST Subroutine ADJUST\(Key-n , p 
13 
13 
13 
13 
13 


4  
http://www.w3.org/RDF       http://www.w3.org/TR/rdf sparql query CyrilleNgongaNgomo. çDBpedia SPARQL Benchmark Assessment with Real Queries on Real Data   
B RC Performance Test on Bigdata Cluster 
Cache size\(1 0 Qu ery set 1 2 3 4 5 6 7 8 9 10 0 52 77 39 100 419 7 148 338 4 195 738 8 242 527 9 289 560 6 334 972 2 381 135 1 426 164 6 471 349 5 100 35 34 00 668 759 981 922 129 856 7 162 196 7 194 952 9 225 329 8 253 420 1 286 945 3 324 210 7 300 32 05 03 607 330 903 888 120 271 5 148 936 5 179 175 1 206 321 9 230 767 2 255 217 7 290 048 3 500 30 63 79 586 099 875 624 117 469 9 145 513 3 175 668 1 202 751 6 227 643 6 252 169 5 288 255 7 700 28 44 32 568 505 863 378 115 188 3 142 804 1 171 427 0 196 958 1 221 200 0 246 799 6 282 558 8 900 28 87 17 563 142 846 516 112 995 0 139 840 3 168 381 2 193 944 6 217 973 0 241 629 2 276 848 6 The situation when cache size is 0 is no-cache cluster. To be seen with the caching mechanism, the query efficiency improved significantly. And with the increase of the cache size, the runtime is less and less. When no-cache in cluster, a query would consume 47.1ms. After to add cache into cluster and set cache size is 100 queries, a query would consume 32.4ms under the same conditions. When setting cache size with 900, the runtime just needs 27.7ms. For detail trend, see Figure 4 Fig. 4 RC Test Result on Bigdata It can be seen by the results of the above two tests that the RC can ensure high hit ratio and improve query efficiency significantly A CKNOWLEDGMENT Thanks for the support of Beijing 000\003 
Fig. 3 Hit Ratio Test Result The test can prove the improve ARC algorithm has ability to maintain high hit ratio and it is better than LRU algorithm and FIFO algorithm 3 A pache   http://jena.apache.org 4 O pe nr df   http://www.openrdf.org  B i gda ta   http://www.bigdata.com 6 F lori a n St e g mai e r Ud oG rob n e r  M a ri o Dolle r Ha ra ld Kosch a nd GeroBaese Evaluation of Current RDF Database Solutions  7 K ur t Ro hl o  An evaluation of triple-store technologies for large data stores Lecture Notes in Computer Science,2007,4806\(1105-1114  N i m rod  M e gi dd o D h a r m e nd ra S  Mod h a   ARC: A SELF-TUNING LOWOVERHEAD REPLACEMENT CACHE  2nd USENIX Conference on File and Storage Technologies,2003:115-130 9 F r a nz   http://www.franz.com/agraph/allegrograph  http://swat.cse.lehigh.edu/projects/lubm 11 F U J I S U W H I T E P A PER Be ch mark  O v e r v i ew O L T P 2 0 0 3    W 3 C   13 Mo ha m e d Mo r s e y J e ns  L e hm a n n  S  r e n A ue r and A xe l Performance DBpedia: A Nucleus for a Web of Open Data  
To further illustrate the effect of RC, The second test will put the RC into Bigdata Cluster The test set also use the seco nd field \(logical block address in OLTP trace. The logical block address of OLTP trace will be as SPARQL  query. The trend of the test set is equal to the trend of logical block address in OLTP Every SPARQL query would be generated by DBpedia Random SRARQL Query Generator. It will be used to generate the query randomly for DBpedia English repository There are 241,605,993 triples of DBpedia in BigdataCluster    The test will execute 10,000 queries for different cache size In last, it will get the runtime \(millisecond\or different cache size and query set size. The detail test result is shown in Table 001\013  TABLE III T HE R ESULT OF RC T EST ON B IGDATA Key 000\003 Laboratory of 000\003 Trusted Computing, Beijing University of Technology R EFERENCES 1 B e r n e rsL e e  T i m, Jame s H e n d l e r an d O r aL assil a M ay 1 7  2 0 0 1  T h e  Semantic Web". Scientific American Magazine. Retrieved March 26 2008 2 W 3C   Soren Auer, Christian Bizer, GeorgiKobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives 
14 
14 
14 
14 
14 


Copyright © 2009 Boeing. All rights reserved  Provided by the Federal Aviation Administration FAA from the NAS to aviation industry in real-time and near real-time Product of the Enhanced Traffic Management System \(ETMS A stream of data packets containing Zlib compressed XML documents with a binary header Can be flight plan related, oceanic, or host tracks reports ASDI 


Copyright © 2009 Boeing. All rights reserved  6 ASDI Message Counts for August 8 2009 Supporting Record Counts for August 8 2009 Typical ASDI Data 


Copyright © 2009 Boeing. All rights reserved  Each ASDI message does not always provide enough information to identify the flight it is associated with Centers operate independently of each other to assign their own Computer Identification \(CID code to flight plans. So, using CID alone is not sufficient Correlator generates a new unique FLIGHT_KEY as it identifies a new unique flight plan FLIGHT_KEY is comprised of aircraft id, flight status, departure center, air craft type, destination etc ASDI Correlation 


Copyright © 2009 Boeing. All rights reserved  Architecture Data Warehouse approach implemented  IBM Infosphere Data Warehouse for data management  Backend DB2 database  IBM WebSphere Message Broker and MQ for near realtime ASDI feed consumption, message routing and transformation  SPSS Modeler for predictive analytics  IBM Cognos BI for front-end visualization 


Copyright © 2009 Boeing. All rights reserved  Architecture Server-1 Server-2 DB2 SURVDB XML Shredder WebSphere Message Broker Ext.4 H Ext.3 G Ext.2 F Ext.1 E C WebSphere MQ TCP/IP Live ASDI Stream IBM Cognos Server-3 IBM SPSS Modeler SPSS Collaboration Deployment Services 


Copyright © 2009 Boeing. All rights reserved  Database Modeling Schemas for correlated ASDI messages translated into equivalent relational schemas  Database tables generated based on classes created from schema definitions  Nine main, eleven supporting tables  Each main table contains FLIGHT_KEY 


Copyright © 2009 Boeing. All rights reserved  Database Modeling 


Copyright © 2009 Boeing. All rights reserved  Correlation Process To archive received ASDI data  Track messages must be correlated with flight plan messages FLIGHT_KEY assigned Uncorrelated data tagged Approx 30 minutes to correlate one day of data 


Copyright © 2009 Boeing. All rights reserved  Historical Data Processing To load correlated data  Uncompress, unmarshall  Create a list of files containing the correlated data  Write data to warehouse 


Copyright © 2009 Boeing. All rights reserved  Live Data Processing Processed using IBM MQ IBM Message Broker and a technique called XML Shredding Message Broker Compute Nodes  Uncompress Node  Extract correlated messages  Shred Node adds to DB Stored Procedure ìshreds XML docs and adds to tables 


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a ìkey, valueî list using an XSTL  Queries made against this list of ìkey, valueî pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


