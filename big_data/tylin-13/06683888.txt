wasted 
University of Tennessee 
program\220s mission is 
Knoxville, Tennessee  Albuquerque, New Mexico  Rutgers, The State University of New Jersey, Piscataway, NJ 246 to accelerate progress in scientific computing in order to deliver breakthroughs in various scientific topics. It does so by partnering with experts from national laboratories and universities to leverage a wide range of expertise and ensure that results from this effort benefit the wider research community. In this context we have developed an end-to-end framework to support collaboration among groups of scientists working around fusion simulations [2   W e  des c r i b e  ou r w o rk  in more detail including lessons learned from our experience that lead to a refreshed approach to collaboration around eScience data We start by exploring the related work and the current state of scientific collaboration. We then make a case for a decoupled collaboration in e-Science by illustrating common scenarios in leadership scale computing. Finally, we focus on the relationship between the application scientist and the visualization expert, presenting the ADIOS visualization schema as a facilitator for expert ise and data exchange between the two Our goal is to allow scientis ts to focus on their area of training and interest, while minimizing time 
Kenneth Moreland 
Georgia Institute of Technology 
Jingqing Mu Oak Ridge, Tennessee 
Sandia National Laboratories 
ADIOS Visualization Schema A First Step towards Improving Interdiscipl inary Collaboration in High Performance Computing Roselyne Tchoua Atlanta Georgia 
 Manish Parashar |, Norbert Podhorszki 
Abstract 
204 
I I NTRODUCTION Collaboration between scientists in leadership-scale science has grown to include a diverse collection of scientists as the computing and storage resources have increased, thus allowing researchers to tackled larger, more complex physical problems Progress in technology and computing has also increased the amount of data acquired and analyzed from sensors, medical equipment or larger instruments such as the neutron beams at the Spallation Neutron Source at Oak Ridge National Laboratory or the Tokamak Fusion Test Reactor at Princeton Plasma Physics Laboratory. Our own experience in the field of fusion, during the CPES project [1  is an ex am ple of su ch  complex collaboration between scientists from various fields and institutions with the common goal of successfully running analyzing and getting knowledge from fusion simulations. Our experience is not uncommon across universities and national laboratories. The Scientific Discovery through Advanced Computing \(SciDAC\Office of Science 
Keywords 
The most common, and perhaps the most tangible way to study collaboration in research is to measure and deduct collaborations through co-authorship  4  5 S o nne nw a l d  presents a comprehensive study of scientific collaboration and influencing factors in [6 Th e study warns of the challenges of collaboration for its own sake 
Scientific communities have benefitted from a significant increase of available computing and storage resources in the last few decades. For science projects that have access to leadership scale computing resources, the capacity to produce data has been growing exponentially. Teams working on such projects must now include, in addition to the traditional application scientists, experts in various disciplines including applied mathematicians for development of algorithms visualization specialists for la rge data, and I/O specialists Sharing of knowledge and data is becoming a requirement for scientific discovery; providing useful mechanisms to facilitate this sharing is a key challenge for e-Science. Our hypothesis is that in order to decrease the time to solution for application scientists we need to lower the barrier of entry into related computing fields 
The scientists\220 perspective in 
002 002 002 Qing Liu 002 003 002 David Pugmire 002 002 003 
A Scientific collaboration 
The key to our approach is recognizing that our users are scientists who mostly work as islands. They tend to work in very specialized environment but occasionally have to collaborate with other researchers in order to take full advantage of computing innovations and get insight from big data. We present an example of identifying the connecting elements between one of such relationships and offer a liaison schema to facilitate their collaboration 
We aim at improving users\220 experience when 
Oak Ridge National Laboratories 
Scott Kla Jeremy Logan 
204 
ky 
interacting with a vast software ecosystem and/or huge amount of data, while maintaining focus on their primary research field In this context we present our approach to bridge the gap between the application scientists and the visualization experts through a visualization schema as a first step and proof of concept for a new way to look at interdisciplinary collaboration among scientists dealing with 
201 201 201 
big data 
in acquiring skills in related fields and reducing the need for collaborators who are experts in these fields II S CIENTIFIC COLLABORATION 
Matthew Wolf \246 
Jong Choi 
collaboration schema visualization simulation experiments 
the similar work of Hara [7 a lso sh ow s th at su cc essf u l 
2013 IEEE 9th International Conference on e-Science 978-0-7695-5083-1/13 $25.00 © 2013 IEEE DOI 10.1109/eScience.2013.24 27 


Utilizing leadership scale computers certainly requires more than acquiring programming skills New software like GlobusOnline [10 an d We b g r ou ps l i k e HPCWeb 3 indicate that more scientists are getting access to large amount of data and organizing themselves for better sharing of resources, data and expertise. GlobusOnline helps researchers move, sync and share big data through web browsers and avoid time consuming, error prone IT tasks HPCWeb is a community group that is focused on making the computing and data resourc es that underlie simulation science scientific computing, and data-centric science easily accessible through web browsers In this section we describe a specific fusion project and other similar science collaboration projects around universities and national laboratories 
B Interdisciplinary collabor ation in e-Science 
of researchers\220 daily work practices 
made within a group based on previous scientists\220 
  
collaboration in research is not easily found. Indeed it is a complex problem that has to take into account collaboration between individuals, groups, institutions, sectors or nations; as well as access to required infrastructure and social networks and persona. However these studies, along with various documented success stories of scientific collaboration in domains including education, health care and computer science, indicate that collaboration has the potential to solve complex scientific problems. It can lead to new discoveries as well as new ways to conduct science. The emergence of scientific social networks is de monstrating that scientists are increasingly leveraging a wider range of tools to tap into this potential. The first attempts at establishing new networks have been largely focused on publications. Sites like LinkedIn 1 and ResearchGate 2 help researchers make professional connections and stay up-to-date on the latest publications and development in their fields. While scientists do not actually work \(online using these sites, the availability of Web 2.0 technology has also heralded the emergence of gateways and virtual research environments. These technologies foster the formation of more closely knit communities of researchers who work together using portals and common tools in their daily work routines Therefore, we note that as a research topic, scientific collaboration is a broad subject that can be tackled from various angles: statistics, quality, social character, temporal geospatial considerations, impact of technology etc Interdisciplinary collaboration adds a level of complexity as participants have different expe rtise and training. For example different perspectives on the same general science topic often result in confusion due to mismatched nomenclature and incompatible software As Hara et al. [7 su g g est g e ttin g a f u rth e r u n d e r st andin g  of scientific collaboration requires an in-depth understanding As computers are allowing for generation and exploration of larger datasets, several application scientists \(chemists physicists etc.\ have two options if they want to stay up-to-date in their respective fields: collaborate with experts in various sub-fields of computer science, or become knowledgeable in these areas themselves. The first one seems more feasible and reasonably achievable as the latter takes away from the time and energy focused on their main area of interest. Sites and software tools like MyExperiment [8  and Man y Ey es [9  m a ke  it easy for scientists to use, share and reuse workflows visualizations and datasets. Nevertheless, researchers dealing with large-scale data still face many challenges and often require some knowledge in computing and data management in order to manipulate that data. The extreme case of High  
During the five year CPES  pr oj ect th e g o al of  ou r  computer science team has been to support fusion scientists as they ran and monitored the XGC-1 code n supercomputers at Oak Ridge National Laboratory and the National Energy Research Scientific Computing Center Application scientists are often unable to take full advantage of currently available computational resources. They may lack formal training in software engineering and often get their training from other scientists in a team Code development methods, software tools and ot her such considerations are 
of scientists\220 daily lives 
1 LinkedIn: http://www.linkedin.com 2 ResearchGate: http://www.researchgate.net Performance Computing where increase in computing power has allowed scientific problems to grow in complexity and scale highlights these challenges For a theoretician, making sense of such large amount of data involves writing codes that run on supercomputers, managing data output, analyzing that data, and visualizing the results. It is becoming impossible for one single scientist to correctl y and efficiently perform all of the necessary tasks without collaborating with other experts Extracting insight from simulations running on supercomputers typically involves pen-and-paper scientists, computation scientists, applied mathematicians \(modeling algorithms, data mining algorithm etc.\, I/O and performance experts, and visualization experts. Concrete examples of this are presented in the next section III T HE H IGH P ERFORMANCE C OMPUTING HPC CASE In HPC, collaboration has inde ed become an essential piece familiarity and/or preferences A study by Basil g i v e s a  software engineering perspective on HPC common practices and discusses some of these issues. Our role in the CPES project has been to provide knowledge about the infrastructure and related software in order to bridge this knowledge gap Our close interactions with physicists have helped us identify ways in which we could not only provide them with solutions but also improve their experien ce Our resulting end-to-end technologies included three main elements: a fast Adaptable I/O System \(ADIOS a w o rkf l o w  m a n a ge m e n t s y s t e m  and an electronic simulation monitoring portal \(eSiMon 3 HPCWeb: http://www.w3.org/community/hpcweb 4 eSiMon: http://www.olcf.ornl.gov/center-projects/esimmon 
leading to an understanding of how integrative scientific collaboration may be facilitated, perhaps through supporting technologies. This paper presents an anal ysis of collaboration in e-Science. We build on the example of the High Performance Computing community \(HPC\ where interdisciplinary collaboration is increasingly required 
4 
A Example in Physics 
28 


B Similar cases and different approaches 
to projects running on the supercomputers A liaison is an application scien tist with notable computational training able both to understan d the science and also to provide valuable input on best practices for using supercomputers. Liaisons help scientists with useful software and point them to existing tools Despite this assistance, larger teams still need to develop their own data management solu tions. In the past few decades a number of large projects consisting of collaborators from both laboratories and universities have turned to gateways. A science gateway is usually a community-developed set of tools, applications, and data that are integrated via a portal to meet the needs of that community. Gateways provide easyaccess to tools and data; they enable researchers to focus more on their scientific goals and less on assembling cyber infrastructure Gateways can also foster collaboration as a core team of supercomputing-savvy developers can build and deploy applications that become services available to the 
acts as a \215 
liaison 
With ADIOS we abstract away many decisions about where and when the I/O is done and which resources are used Such decisions are important to attain satisfactory I/O performance and while achieving high performance should be a concern for end-users, they ar e generally not trained to make performance decisions ADIOS has been designed to simplify this process. First it uses a simple programming API that does not express the I/O strategy but simply declares what to output which is the primary concern of the application scientist. Second, the XML-b ased external description of output data and selection of I/O strategy allows users to make changes to their I/O choices without changing their code as they change environment or new I/O methods become available. Third, ADIOS offers multiple transport methods selectable at runtime. Finally, the self-describing, log-based file format combined with buffered writing insures excellent write performance  Th ese desig n decisio n s allo w s I O scientists to conduct research and deliver software while minimally impacting the application scientist who only wishes to output simulation results Similarly, our KEPLER monitoring workflow automates a number of mundane and repetitive data management tasks for the XGC-1 users. When users launch the fusion workflow, they are unaware of the heavy lifting done in the back-end on multiple systems; they simply launch their preferred web-browser and log in to eSiMon. While workflow tasks include moving files from computational resources to visualization clu sters, pre-processing and archiving raw data files, crea ting images and movie, eSiMon is a lightweight portal that can be launched from any machine using a web browser to monitor the initial results. Our conscious effort to help research ers think of science instead of files and directories is embedded in the eSiMon design Rather than focusing on a file browsing interface, the main simulation view displays familiar simulation variable names evolving through time as shown on Fig. 1. The tree-view on the left shows the scientific variables that can be dragged and dropped onto the canvas to display th e movies. We use a provenance system to link  g r ap h ics  to ra w data f o r do w n load s a n d analysis Fig. 1 eSiMon main simulation view 
216 
Our data management team has worked with various applications including combustion \(S3D   f u sion GTC   GTS 20 XGC1 11  an d s u pern ov a \(C h i m e ra 21  and more. We have observed a variety of different collaboration techniques Depending on the size of the project and the size of the group involved, scientists assign more or less time, energy and resources to addressing the data management issues they encounter while doing their work Smaller groups may attempt to gain some knowledge in related computing fields in order to get work done. They tend to focus on a direct solution to the task at hand, and may not take performance considerations into account, especially in the beginning phase of their projec t. Researchers are willing to live with performance problems, as long as their work does not come to a complete halt. This approach is tedious and time consuming when done correctly, but when rushed can result in fragile code and excessive depend ence on the scientists who wrote the initial program Medium-sized groups may have a staff dedicated to developing software tools and solutions for the entire team For instance Bellerophon  as been desig n e d a n d implemented to perform automated verification, visualization and management tasks while integrating with other workflow systems utilized by the CHIM  develo pm e n t tea m at ORNL. Similar to our workflow-dashboard system for fusion scientists, this approach is tailored to the needs of this specific team of astrophysicists. In an attempt to get results, developers find themselves delivering code and team specific solutions that work but that are not directly transferrable to other application teams. In these projects, like in our own, users become dependent on computer science experts and their knowledge for introducing changes to the workflow, fixing bugs or adding new functionality Larger teams with access to large infrastructure and financial resources may benefit from groups such as the Scientific Computing Group at ORNL where each member 
29 


A ADIOS Visualization Schema 
decoupled 
5 ESGF: http://esg.nersc.gov/esgf-web-fe the workflow involves a tremendous amount of effort, as a diverse set of requirements and co nstraints has to be taken into account for each application. On the other hand, making application specific workflows al so involves considerable knowledge, expertise and effort to adapt to a different group Therefore, instead of erroneously trivializing the workflow tasks, we identify points where certain workflow tasks can be more easily connected to fit different application We describe this quest for improved connections, a search for a 6 http://www.scidac.gov/fusion/fusion.html 7 VisIt: https://wci.llnl.gov/codes/visit 8 ParaView: http://www.paraview.org 9 AVS/Express: http://www.avs.com 10 HDF5: http://www.hdfgroup.org/HDF5 11 NetCDF: http://www.unidata.ucar.edu/software/netcdf 
community at large. One example is the Earth Science Grid Federation 5 ESGF\which stores and distributes terascale data sets from multiple coupled ocean-atmosphere global climate model simulations Even at large scale, gateways tend to cater to specific communities. Considerable amount of time and effort from cyber infrastructure specialists is invested to serve the need of specific domain scientists. In W ilk in s Die h r e x a m in e s t h e lessons learned from the TeraGrid  is w o rk suggests that often the greatest benefit to users was easy access to data and the tools that can filter and mine it. In other words gateways have mainly addressed the need for easy, integrated access to commonly used tools and datasets within a particular scientific field. The focus had been primarily on providing functionality; user-experience an d collaboration are still to be improved in future generation gateways. Indeed, despite the substantial effort required to build gateways end-users increasingly expect the same ease of use as commercial social networks for examples IV O UR APPROACH From professional website to complex gateways, scientists have started to leverage the advances of Web 2.0 technology and rapidly growing advances in computing resources. As a group, we have also tried to take advantage of these advances We have used our knowledge in computational science and our close relationship with app lication scientists to identify the obstacles preventing researchers from focusing on their science. Our work has resulted in the development and release of eSiMon and ADIOS. eSiMon, our one-stop-shop to fusion simulations hides the complexity of the workflow and its multiple data management tasks from the users and presents them with scientific content. Similarly ADIOS provides sophisticated techniques that take full advantage of current research and resources with out burdening the users While ADIOS is currently used in a variety of applications the workflow-dashboard system has been more difficult to generalize across applications. Building, running, debugging and modifying our Kepler workflow remains a complex task that only a workflow expert can tackle. In our experience despite training and tutorials application scientists have not easily mastered the workflows and typically rely on the assistance of an expert for any modification. This represents a single point of failure for the group; it is a dependency that can slow down, or completely stop other group members from working.  If and when the knowledgeable person leaves, time and energy has to be invested in training a new group member before usual work can continue its course. This problem caused us to reevaluate our solution, and research and implement a better way to co nnect application scientists and computer scientists The entire robust monitoring workflow with its mundane and tedious tasks cannot be easily simplified.  Workflow tasks tackle issues such as user login and credentials, data movement across platforms, f ile archiving, etc. Generalizing  In the teams we have experience with, notably the fusion scientists and their most recent project EPSi 6 Center for Edge Physics Simulation\, scientists often write their own analysis scripts or utilize a collection of scripts from the community As in computing in general, their improvised visualization skills are not always scalable hence the need for computer scientists in the project. Indeed using visualization tools such as VisIt 7 ParaView 8 AVS 9 etc. requires a certain level of expertise as well as information on the code data structures Visualization experts need to kno w where and how to access the data to be visualized. In other words they need metadata about files and physical variables to be visualized. For visualization experts, an unders tanding of the science is not essential; rather they want to know about scalars, vectors arrays and meshes for instance. This is information that application scientists are familiar wi th and can point to in their code without knowing protocols and methods expected by common visualization tools. Therefore there is a need for translation between the languag e of the application scientist and the visualization expert Matching code, data and scalable visualization software is not a new problem. HPC scientists have long used selfdescribing data formats such as HDF5 10 and NetCDF 11 to  
collaboration. Our hypothesis is that good simulation outputs \(or results from other inter-disciplinary collaboration\e product of a lot of individual knowledge, expertise, and work as well as some collaborative effort. Our interviews and meetin gs with application scientists suggest to us that it is important to identify where the work of these related but different scientists intersect and specifically target these areas for collaboration improvement. Our goal is to connect expertise where needed in order to allow each researcher to focus on their ow n area of interest One example of such a connection is that of the application scientist and the visualization specialist. We asked the question: what pieces of the workflow connect the scientist running his or her simulation code and the experts and tools that can display the large amount of output data. We build on ADIOS and eSiMon for I/O and visualization purposes to design and implement ADIOS Visualization Schema 
30 


adios-config host-language="Fortran" schema version="1.1 
data 
mesh name xgc.mesh type="unstructured file="xgc.mesh.bp" time-varying="no 
points-single-var 
adios-group 
var name="n_n" type="integer var name="n_t" type="integer var name="values" gwrite="coord path="/coordinates" type="real*8 
var name="node_connect_list" gwrite="nodeid path="/cell_set[0 ty pe   in t e g e r   dimensions="nodes_per_elem,ncells 
count="n_t 
expertise, familiarity, person al preference or availability of software packages. Furthermore we provide performance and flexibility of I/O as a bonus by embedding the schema into ADIOS XML file  
  
adios-group name="field3D 
dimensions="nspace,nnodes 
var name="dpot" type="real*8 dimensions="1,nnode 
scientists\220 eyes and brains 
mesh 
global-bounds dimensions="nphi,nnode offsets="iphi,0 var name="iden" type="real*8 dimensions="1,nnode 
ntly of users\220 
B Implementation 
offsets="iphi,0 
node_connect_list" type="triangle  
 var name="nphi" type="integer var name="iphi" type="integer global-bounds dimensions="nphi,nnode 
write large data files.  But these formats do not insure standard organization, or consistent naming convention of particular visualization elements. Therefore, straightforward visualization of the data by visualization collaborators often requires a closer connection to the science and the scientist who wrote the data. Popular tools like VisIt and ParaView develop plugins and readers for common self-describing data formats. Development of reader s for particular codes requires investment of time and energy from domain scientists and visualization/software experts. This may seem like a reasonable approach as long as codes, platforms and tools do not evolve, however this is rarely the case in bleeding edge research and development The concept of schemas, even visualization schemas, is not new. XDMF 12 eXtensible Data Model and Format\was created out of the need for a s tandardized method to exchange data between HPC codes and tools XDMF \(eXtensibleData Model an d Format\uses XML to store Light data and to describe the data Model. HDF5 is used to store Heavy data. Similarly VizSchema atte m p t s to  link common data formats to common visualization tools Both of these schemas are tightly coupled with the HDF5 file format. Even though we embed our schema as attributes in the ADIOS XML and binary files, the key difference is not the vehicle for the schema, but r ather our persis tent focus on collaboration and separation of ex pertise. Indeed the purpose of the schema is not to link the data format, or simply the metadata to the data; rather it is a description of the visual understanding of the data: content that can be processed by By focusing on meshes and physical variables, the visualization process becomes independent of code and developer. If the visual representation of a variable changes, a scientist describes that change in the ADIOS XML file. Even though the visualization expert may need to edit the reader for the resulting binary data, his or her work is thus decoupled from that of the application s cientist. Instead of having a reader for each code, programs such as VisIt and Paraview will require only a single re ader to visualize any ADIOS data file. If and when changes need to be made, and readers have to be revisited and adapted, visualization experts are able to do so without requiring an understanding of changes in the applications or discussions with domain scientists. Time and effort invested in changes are now tied to the data and no longer associated with specific codes or developers. A physicist, in our case, can simply describe and modify the mesh and the variables without having to understand the corresponding analysis and visualization scripts and software They can share scientific content with colleagues without knowledge of their preferred tools and naming conventions Data can be shared and visualized independe 
XML tags and attributes provide a convenient way to organize data in a human re adable way. XML is also an intrinsic part of ADIOS it is already the case that if users change the way they write data, they can simply edit their ADIOS XML file. We now extend this approach by giving the scientists a way to describe the me sh structure, variables that compose the mesh and related variables. The goal is to require minimal information from a domain scientist as possible and still be able to visualize the d ata on popular HPC tools. When data representation changes in the code, or new variables are added, users do not have to change their code, they simply shuffle variables and meshes around in the XML Visualization software including our own reader then interprets the changes and accurately display images on eSiMon for example Fig. 2 shows a sample of an ADIOS XML file Fig. 2 Sample ADIOS XML file with additions corresponding to the schema underlined 
xml version="1.0 
 global-bounds  global-bounds 
mesh xgc.mesh  hyperslab=": 0,: 1,: nphi mesh xgc.mesh  hyperslab=": 0,: 1,: nphi 
uniform-cell 
203 
 12 XDMF: http://www.xdmf.org/index.php/Main_Page 
value="values  
31 


C Evaluation 
Collaboration is often a critical component of research of big machines and big data; however it remains difficult to measure via usual methods of observation, interviews and questionnaires. Studies of co-autho rships offer a perspective but not a complete and general picture. In inter-disciplinary studies, particularly in field s such as HPC where diverse disciplines are required to make sense of a tsunami of data more factors need to be taken into account. We argue that connectors such as the ADIOS Visualization Schema should be components of the complex workflows that process output from large simulations. This type of facilitated juncture insures that scientists are spen ding their valuable time and energy where they see it fit Before using ADIOS schema Applications scientists and visualization experts would have to discuss and agree on data representation, metadata and I/O patterns 
xgc.mesh/time-varying 
XML file Fig. 3 Example of schema attributes generated from sample ADIOS XML 
string string 
attr   = "triangle 
tags and attributes to generate an annotated binary output file describe thereafter This file is then used by visualization experts and softwa re to locate, associate and display the meshes and variables in the output data. Fig. 3 shows the schema attributes gene rated in the output BP file inhibiting some users due to limited access to software or varying degrees of experience Without our standardized schema, modifications to software such as changes in data representations, changes in infrastructure requiring different I/O, additional variables for debugging, etc. would require modifications in the code as well as changes to visualization readers. This development work would have to be well coordinated and repeated for each change To further understand the considerable amount of development involved with implementing readers and writers for various codes and file formats we examined an assortment of VTK 13 readers We found 133 classes declaring themselves as reader and 84 declaring themselves as a writer. All together this code amounts to about 18 of the VTK code. VisIt and ParaView visualization packages are both based on VTK and expectedly have extensive lists of readers and writers in their documentation 14 15 Table 1 shows a number of simulations that have been used by members of our team, and the number of lines of codes in the corresp onding VisIt readers. As can be seen in the table, custom readers typically require thousands of lines of additional code to implement TABLE I C ODE SPECIFIC READERS AND LINES OF CODES IN V IS I T 
version_major 
dpot/adios_schema attr 
attr   = "unstructured 
attr   = "/nspace S3D 
3114 10520 4643 9888 3478 8528 
GTC 
attr   = "no 
users\220 
Pixie 
xgc.mesh/ctype 
File Format 
attr   = "xgc.mesh.bp 
xgc.mesh/cdata 
version_minor 
40171 
attr   = "/coordinates values M3DC1 
Chombo 
 attr   = "1 iden/adios_schema attr        
ncsets 
The amount of collaborative work is not only reflected through lines of code. Our grou p has been directly involved with writing VisIt readers for codes such as GTC and Pixie3D   B ased on ou r past exper ience, this work requires visualization expertise and application knowledge and would be significantly simplified using our standardized schema along with a generic reader Therefore we anticipate that the schema will consolidate numerous file formats and simulation code readers in HPC visualiza tion tools. This task has the potential to notably impact the ratio of time implementing readers over the time spent doing application and visualization research As a proof of concept, in the EPSi project we have implemented our own schema reader and plotting method for  
attr   = "1 string   /adios_schema 
attr   = 1 
xgc.mesh/points-single-var 
xgc.mesh/type xgc.mesh/nspace 
xgc.mesh/ccount 
In the case of XGC-1, variables on the mesh and variables composing the mesh are stored in separate files. Variables are also written as compounds of 2D slices of the tokomak. To support such representations and that of other common representations in HPC, the ADIOS Visualization Schema has to be flexible and expandable. Therefore we allow referring to external files for mesh variables as well as the concept of hyper slabs for super sets or su b sets of variables. For example a variable could be a subset of another variable, or as it is in the case in our example, a 3D variable could be visualized as a set of planes using a 2D mesh.  ADIOS processes the newly added 
Lines of code 
xgc.mesh xgc.mesh xgc.mesh 
attr   = " /cell_set nod e _ c onn e c t_ li st   
attr   = "n_t 
string   /adios_schema string   /adios_schema string   /adios_schema string   /adios_schema string   /adios_schema string   /adios_schema string   /adios_schema string   /adios_schema string   /adios_schema 
xgc.mesh/mesh-file 
ADIOS file format\ult to the users\220 additions in the 
205 205 
  13 VTK Visualization toolk it: http://www.vtk.org 14 VisIt list of readers http://visitusers.org/index.php?title=Detailed_list_of_file_for mats_VisIt_supports 15 ParaView list of readers http://paraview.org/Wiki/ParaVie w/Users_Guide/List_of_read ers 
Grand Total 
M3D 
double   /adios_schema 
establishing dependencies on specific persons, codes and visualization tools. Prior to using the sc hema, collaborators sharing data within the same field would also often have to impose a single tool for convenience within a team 
32 


visualize 
reported measurements of time spent outside of research V C ONCLUSION In this paper we have presented our perspective of the HPC community and its answers for inter-disciplinary collaboration around large infrastructure and big data. We have replaced part of our fusion-specific monitoring workflow by a standardized visualization schema and interpreting tool to de-couple collaboration between two types of scientists: application scientists and visualization experts. We present this work as an example of a new approach to multi-disciplinary collaboration While scientific collaboration is generally encouraged, it can be detrimental to research when time and energy are spent 
54.10 \(2003\: 952-965 8 D e R o u r e Da v i d  C a r o le G o b l e an d R o b e r t S t even s   T h e  d e s i gn and realisation of the myExperiment virtual research environment for social sharing of workflows 
to scientists\220 self scientists\220 
Future Generation Computer Systems 
write Visualization Expert Physicist  
13.6 \(2007\: 1121-1128 10 F o ster  I a n  G l o bus O n l i n e   A cce le r a ti ng a n d de mo cr atiz i n g scie n ce  through cloud-based services 
25 2009\: 561-567 9 V ie g as, F e r n an da B., e t al  M a n y ey e s a site f o r  v i sual iz at io n a t  internet scale 
Journal of the American Society for Information Science and Technology 
311.3 \(2002\: 590-614 6 
We minimize such waste of time by identifying and improving connections between two types of experts through our visualization schema We claim that the schema accelerates discovery by separating 
15.3 \(2011 70-73 11 C. S   Ch a n g  S K l ask y J. C u mm in g s, R  S a mt an e y A  Sh o s h a n i  L   Sugiyama, D. Keyes, S. Ku, G. Park, S. Parker, N. Podhorszki,H 
outside of one\220s area of training and interest 
Visualization and Computer Graphics, IEEE Transactions on 
Internet Computing, IEEE 
edit launch 
work according to their research area. The schema decreases repetitive interactions between researchers who are connected but do not speak the same scientific language.  More tangible measurements are needed for further analysis. This work advocates encouraging inter-disciplinary collaboration by emphasizing discipline-specific research and making transition from one scientific field of another seamless A CKNOWLEDGMENT The research and development of the ADIOS Schema has been partly supported by the National Center of Computation Science \(NCCS\ and by DOE SCIDAC: Center for Edge Physics Simulation \(EPSi\. We wish to thank our EPSi colleagues Choong-Seock Chang and Seung-Hoe Ku R EFERENCES 1 C e n t e r for Pla sma E dge Si mu la ti on \(C PES   http://www.scidac.gov/FES/FES_CPES.html 2 C um m i ng s  J   L o f s te ad, J   S c hw an K  S im  A  S h o s ha ni A  D o c a n   C.; Parashar, M.; Klasky, S.; Podhorszki, N.; Barreto, R., "EFFIS: An End-to-end Framework for Fusion Integrated Simulation," Parallel Distributed and Network-Based Processing \(PDP\, 2010 18th Euromicro International Conference on , vol., no., pp.428,434, 17-19 Feb. 2010 doi: 10.1109/PDP.2010.97 3 N ew man M a rk  EJ    S cie n tifi c co l l a b o rat i o n n e tw o r k s I I  Sh o r te st paths, weighted networks, and centrality" Physical Review E, Vol. 64 No. 1. \(2001\. doi:10.1103/PhysRevE.64.016132 4 N ew man M a rk E J   T h e stru ct u r e o f sci e n tif ic co l l a b o rat i o n  networks." Proceedings of the National Academy of Sciences 98.2 2001\: 404-409 5 Bar a b 341s i, A l be r t L 341szl 363, e t al  E v o l utio n o f t h e so cial  ne tw o r k o f  scientific collaborations 
XGC-1 code users. We find that the automatically generated 2D diagnostic images allow physicists to directly dig into the science. This visualization portion of the monitoring workflow replaces a painstaking process of running a series of standard analysis routines before focusi ng on interesting subsets of the data. For the applicatio n scientists the ad ded value of our modified workflow task resides in the flexibility and separation of work via the ADIOS XML file. Researchers can be enthusiastic about learning new skills outside of their area of training; however, this more in-depth section of analysis is where they would rather focus. The scripts and methods they use to make real discoveries may only be used once and cannot be automated. The intangible metrics we target are time spent outside of area of expertise/interest, and time to solution in primary research area. In other words, how quickly and efficiently can we bring scien tists to the research aspect of their work, the part that cannot be automated and leads to scientific breakthrough? In the meantime, I/O scientists and visualization scientists are also conducting research independently of the application as the collaboration has been decoupled. The visualization expert can now focus on research issues regarding visualization at scale instead of implementing and debugging readers for applications as discussed earlier Each scientist can do what they are trained to do on their own and come together only through this intermediate schema we have implemented. This scenario example is illustrated in Fig 4 Fig. 4 Decoupled collaboration between physicist and visualization expert This proof of concept is a specialized and partial evaluation. For a more in-depth assessment of our work we need measurements of the time scientists spend outside of areas of expertise before and after integrating our ADIOS schema. In addition to interviews, targets for improvements will be tailored to each type of scientists for a more complete perspective of the changes in their daily routines. For example, in the case of visualization experts, the consolidation of simulation code readers and writers gives additional support 
f information science and technology 41.1 \(2007\: 643-681 7 H ar a, N o r i ko e t  al  A n e m e r g i ng v i ew o f scie n t i f i c co l l a bo r a tio n  Scientists' perspectives on collaboration and factors that impact collaboration 
Sonnewald, Diane H. \215Scientific collaboration.\216 Annual review o 
ADIOS XML ADIOS BP file 
Monitoring workflow 
Simulation code 
Physica A: Statistical Mechanics and its Applications 
33 


Strauss, H. Abbasi, M. Adams, R. Barreto, G. Bateman, K. Bennett, Y Chen, E. D. Azevedo, C. Docan, S. Ethier, E. Feibush, L. Greengard T. Hahm, F. Hinton, C. Jin, A. Khan, A. Kritz, P. Krsti, T. Lao, W Lee, Z. Lin, J. Lofstead, P. Mouallem, M. Nagappan, A. Pankin, M Parashar, M. Pindzola, C. Reinhold, D. Schultz, K. Schwan, D. Silver A. Sim, D. Stotler, M. Vouk, M. Wolf, H. Weitzner, P. Worley, Y 
principles integrated simulation of Tokamak edge plasmas 
IO and integration for scientific codes through the Adaptable IO 
215Grid 
based parallel data streaming implemented for the gyrokinetic 13, no. 9, p. 092505, 2006. [Online  A v ail a bl e   http://link.aip.org/link/?PHP/13/092505/1  O E   B  M e s s e r S  W   B r u e nn  J   M  B l ond in W  R H i x A   
toroidal code,\216 in SC \22003: Proceedings of the 2003 ACM/IEEE 
IEEE, 2009 15 L iu, Q ing  e t al  H e l l o A D I O S   T h e Ch al l e ng e s and L e s s o ns o f  Developing Leadership Class I/O Frameworks," unpublished  L u d 344 s c h er   B e r t r a m  et a l S c i e n ti f i c p r oc es s  a u t o m a t i on and workflow management." Scientific Data Management: Challenges Existing Technology, and Deployment, Computational Science Series 2009\: 476-508 17 P i e r r e Mo ual l e m, e t al  
31pp\, 2009. [Onlin  A v a i la b l e h t tp   s t a c k s  i o p  org 174 9 4699/2/015001 19 S K l asky S Ethie r Z L i n, K M ar t i n s, D  M cC u ne a n d R S am t a n e y  
kinetic simulation of global turbulent 
Computer 
012 049 
Tracking Files Using the Kepler Provenance Framework 
205 205 
12 042, 2008  B a s i li Vic t or R   et a l  Und ers t an di n g t h e Hi gh Perform a n c e Computing Community." \(2008  conference on Supercomputing, 2003, p. 24  Jul. 2007  L i n g e r f elt  E r i c J  et a l  A Mu lt iti er S y s t em f o r t h e Ver i f i c a t i on Visualization and Management of CHIMERA 
J. Lofstead, S. Klasky, K. Schwan, N. Podhorszki, and C. Jin, \215Flexible Mezzacappa, and C. J. Dirk, \215Petascale Supernova Simulation with CHIMERA,\216 Journal of Physics Conference Series, vol. 78, no. 1, pp 
System \(ADIOS\,\216 in CLADE 2008 at HPDC. Boston, Massachusetts 
41.11 \(2008\-41  C a t l et t Cha r li e T h e p h i l os op h y of T e r a Gri d bu i l di n g  a n op en extensible, distributed TeraScale facility." Cluster Computing and the Grid 2nd IEEE/ACM International Symposium CCGRID2002. 2002 25 S S h as h ar i na J R Car y S V e itz e r P H amil l S K r ug e r M. D u r a nt   and D. A. Alexander, VizSchema - Visualization Interface for Scientific Data, IADIS International Conference, Computer Graphics Visualization, Computer Vision and Image Processing, 2009, p. 49  C h a c 363 n  L u i s  A n o n s t a ggered  c o n s erva t i v e fi ni t e volu m e s c h em e  for 3D implicit extended magnetohydrodynamics in curvilinear geometries." Computer Physics Communications 163.3 \(2004\: 143171 
J. H. Chen et al., \215Terascale direct numerical simulations of turbulent combustion using S3D,\216 Comp. Sci. & Disc., vol. 2, no. 1, p. 015001 
4 \(2011\: 2076-2085  W i lk i n s Di eh r  N a nc y  et a l  T er a G r i d s c i en c e gat e w a y s a n d  t h eir  impact on science 
Oak Ridge National Laboratory \(ORNL\; Center for Computational Sciences, 2009  
ACM, June 2008. [Online   A v ail a bl e   http://www.adiosapi.org/uploads/clade110-lofstead.pdf  B a rre t o   R o se l y n e  e t a l  C olla b o ra t i o n  p o rt a l for p e t a s c a l e  simulations 
Xiao, E. Yoon, and D. Zorin, \215Toward a first 
Procedia Computer Science 
transport properties in Tokamak experiments,\216 Physics of Plasmas, vol 
Collaborative Technologies and Systems, 2009. CTS'09 International Symposium on 
Scientific Discovery through Advanced Computing, vol. 125, pp. 12 042 
W. X. Wang and et al, \215Gyro 
art. no. 012042,\216 Scidac 2008 
34 


We now evaluate the overhead of the FChain system Table II lists the CPU cost of each key module in FChain We observe that most modules in FChain is light-weight The most computation-intensive module is the abnormal change point selection component which is triggered only when a performance anomaly occurs F Chain also distributes the change point computation load on different hosts and executes them in parallel to achieve scalability The online validation takes about 30 seconds for each component since we need some time to observe scaling impact for deciding whether we have made a pinpointing error However the online validation is only performed on those suspicious components pinpointed by the integrated fault diagnosis module The FChain daemon running inside the Domain 0 of each host imposes less than 1 CPU load and consumes about 3MB memory during normal execution IV R ELATED W ORK Our work is rst closely related to previous black-box fault localization schemes F or example NetMedic p rovided detailed application-agnostic fault diagnosis by learning inter-component impact NetMedic rst needs to assume the knowledge of the application topology To perform impact estimation NetMedic needs to nd a historical state that is similar to the current state for each component However for previously unseen anomalies we might not be able to nd a historical state that is similar to the current state for the faulty components Under those circumstances NetMedic assign a default high impact value whic h sometimes lead to inaccurate diagnosis results as shown in Section III In comparison FChain can diagnose previously unseen anomalies and does not assume any prior application knowledge Oliner et al proposed to compute anomaly scores using the histogram approach and correlates the anomaly scores of different components to infer the inter-component inuence graph As shown in Section III it is difcult for the histogram-based anomaly detection to perform online fault localization over suddenly manifesting faults Moreover unrelated components can have indirect correlations caused by workload uctuations which will cause their system to raise false alarms In comparison FChain is more robust to different types of faults and workload uctuations To achieve black-box diagnosis researchers have also explored various passive network trafc monitoring and analysis techniques such as Sherlock  O ri on 27 S N A P  28 However those analysis schemes can only achieve coarsegrained machine-level fault localization Additionally during our experiments we found that previous network trace analysis techniques cannot handle continuous data stream processing applications due to the lack of gaps between packets for extracting different network ows Project5 and E2EProf performed cros s correl a t i ons bet w een mes s a ge traces to derive causal paths in multi-tier distributed systems WAP5 e x t e nds t h e b l ack-box caus a l p at h a nal y s i s t o support wide-area distributed systems Orion di s c o v e rs dependencies from network trafc using packet headers and timing information based on the observation that the trafc delay distribution between dependent services often exhibits typical spikes LWT propos ed t o di s c o v e r t he s i mi l a ri t y of the CPU usage patterns between different VMs to extract the dependency relations hips between different VMs However as shown in our experiments dependency-based fault localization techniques are not robust which can make frequent pinpointing mistakes due to various reasons e.g the back pressure effect in distributed applications common network services pinpointed as culprits Furthermore existing dependency discovery techniques need to accumulate a large amount of trace data to achieve reasonable accuracy Particularly network trace based techniques only support requestand-reply types of applications which fail to discover any dependency in continuously running applications such as data stream processing systems In contrast FChain provides online fault localization which does not require any training data for anomalies or a large amount of training data for normal behaviors FChain is fast which can quickly localize faulty components with high accuracy af ter the performance anomaly is detected A urry of research work has proposed to use end-to-end tracing for distributed system debugging Magpie  i s a request extraction and workload modelling tool that can record ne-grained system events and correlate those events using an application specic event sch ema to capture the control ow and resource consumption of each request Pinpoint t ak es a request-oriented approach to tag each call with a request ID by modifying middleware platform and applies statistical methods to identify components that are highly correlated with failed requests Monitor t racks t he reques t s e xchanged b et ween components in the system and performs probabilistic diagnosis on the potential anomalous components X-Trace i s a n integrated cross-layer crossapplication tracing framework which tags all network operations resulting from a particular task with the same task identier to construct a task tree Spectroscope can di agnos e p erformance anomal i e s b y comparing request ows from two executions In contrast our approach does not require any instrumentation to the 
G FChain System Overhead Measurements 
System Modules CPU cost 
VM monitoring 6 attributes 1.03 0.09 m illisec onds Normal uctuation modeling 22.9 2 millis econds 1000 samples Abnormal change point selection 602.4 105.2 m illisec onds 100 samples Integrated fault diagnosis 22 1 microseconds Online validation per-component 30 1 seconds TABLE II FC HAIN OVERHEAD MEASUREMENTS  size for the DiskHog fault in Hadoop The reason has been described in Section III-A Generally the look-back window should be long enough to capture the fault manifestation We are currently investigating an adaptive look-back window conguration scheme by examining the metric changing speed 
     
206 
29 
29 


application or middleware platform to collect request ows Thus it is much easier to deploy FChain in large-scale IaaS clouds Blacksheep correl a t e s t he change poi nt of s y s t em-l e v el metrics e.g cpu usage with the change in count of Hadoop application states i.e events extracted from logs of DataNodes and TaskTrackers to detect and diagnose the anomalies in a Hadoop cluster Kahuna-BB correl a t e s b l ack-box dat a system-level metrics and white-box data Hadoop console logs across different nodes of a MapReduce cluster to identify faulty nodes In comparison FChain is a black-box fault localization system which is application-agnostic without requiring any knowledge about the application internals We believe that FChain is more practical and attractive for IaaS cloud systems than previous white-box or gray-box techniques V C ONCLUSION In this paper we have presented FChain a robust blackbox online fault localization system for IaaS cloud computing infrastructures FChain can quickly pinpoint faulty components immediately after the performance anomaly is detected FChain provides a novel predictability-based abnormal change point selection scheme that can accurately identify the onset time of the abnormal behaviors at different components processing dynamic workloads FChain combines both the abnormal change propagation knowledge and the inter-component dependency information to achieve robust fault localization FChain can further remove false alarms by performing online validation We have implemented FChain on top of the Xen platform and conducted extensive experimental evaluation using IBM System S data stream processing system Hadoop and RUBiS online auction benchmark Our experimental results show that FC hain can achieve much higher accuracy i.e up to 90 higher precision and up to 20 higher recall than existing schemes FChain is light-weight and non-intrusive which makes it practical for large-scale IaaS cloud computing infrastructures A CKNOWLEDGMENT This work was sponsored in part by NSF CNS0915567 grant NSF CNS0915861 grant NSF CAREER Award CNS1149445 U.S Army Research Ofce ARO under grant W911NF-10-1-0273 IBM Faculty Awards and Google Research Awards Any opinions expressed in this paper are those of the authors and do not necessarily reect the views of NSF ARO or U.S Government The authors would like to thank the anonymous reviewers for their insightful comments R EFERENCES   A m azon E las tic Com pute Cloud  h ttp://a w s  a m azon com ec2   V i rtual c om puting lab  http://vcl ncs u  e du  P  Barham  A  D onnelly  R I s aacs  a nd R M o rtier   U s ing m agpie f or request extraction and workload modelling in 
 2004  M  Y  Chen A  A ccardi E  K icim an J  L lo yd D  P atters on A  F ox and E Brewer Path-based failure and evolution management in  2004  R F ons eca G  P o rter  R H  K atz S  S h enk e r  and I  S toica X T race A pervasive network tracing framework in  2007  I  Cohen M  G o lds z m i dt T  K elly  J  S ym ons  a nd J  S  Chas e Correlating Instrumentation Data to System States A Building Block for Automated Diagnosis and Control in  2004  I  C ohen S  Z h ang M  G o lds z m i dt J  S ym ons  T  K elly  a nd A  F ox Capturing indexing clustering and retrieving system history in  2005  S  D uan S  Bab u  a nd K  M unagala F a A s ys tem for a utom ating failure diagnosis in  2009  S  K andula R Mahajan P  V erkaik S  A garw al J  P a dhye a nd V  Bahl Detailed diagnosis in computer networks in  2009  A  J  O liner  A  V  K ulkarni and A  A ik en  U s ing c orrelated s u rpris e to infer shared inuence in  2010  P  Bahl R Chandra A  G r eenber g  S  K andula D  A  M altz and M Zhang Towards highly reliable enterprise network services via inference of multi-level dependencies in  2007  Z  G ong X  G u  a nd J  W ilk es   P RE S S  P Redicti v e E las tic ReS ource Scaling for Cloud Systems in  2010  H  N guyen Y  T a n and X  G u P A L  P ropagation-a w are a nom aly localization for cloud hosted distributed applications in  2011  B Gedik H Andrade K L  W u P  S  Y u and M  D oo SP ADE  t he system s declarative stream processing engine in  2008  A pache H adoop S y s tem   http://hadoop apache  or g/co re   Rice uni v e rs ity bidding s y s tem   http://rubis  objectw eb  o r g   M Ben-Y e huda D  B reitgand M F actor  H  K o lodner  V  K r a v ts o v  and D Pelleg NAP a building blo ck for remediating performance bottlenecks via black box network analysis in  2009  Y  T a n X  G u  a nd H  W a ng  A dapti v e s ys tem anom aly prediction f or large-scale hosting infrastructures in  2010  D  L  M ills   A b rief his t ory o f N T P tim e m e m o irs o f a n i nternet timekeeper  2003  Y  T a n H  N guyen Z  S h en X  G u C V e nkatram ani and D  R ajan PREPARE Predictive Performance Anomaly Prevention for Virtualized Cloud Systems in  2012  M  Bas s e ville and I  V  N ikiforo v   Prentice-Hall Inc 1993  L  Cherkaso v a  K  O zonat N Mi J  S ym ons a nd E  Sm irni  Anom aly application change or workload change towards automated detection of application performance anomaly and change in  2008  P  Barham and e t al   X e n a nd the a rt of virtualization  i n  2003  T he ircache p roject  h ttp://www.ircache.net  H ttperf  h ttp://code google com  p htt p er f  S  K u llback  T h e ku llback-leibler distance  1987  X  Chen M  Z hang Z  M  M a o and P  B ahl  A utom ating n etw ork application dependency discovery experiences limitations and new solutions in  2008  M Y u  A  G reenber g  D  M altz J  Re xford L  Y u an S  K andula and C Kim Proling network performance for multi-tier data center applications in  2011  M K  A guilera J  Mogul J  W iener  P  R e ynolds  a nd A  Muthitacharoen Performance debugging for distributed systems of black boxes in  2003  S  A g arw ala F  A l e g re K  S chw a n and J  M ehalingham  E 2E P r of A utomated end-to-end performance management for enterprise systems in  2007  P  Re ynolds  J  L  W iener  J  C M ogul M  K  A guilera and A  V ahdat  WAP5 black-box performance debugging for wide-area systems in  2006  R Apte L  Hu K  S chw a n and A  G hosh L ook W ho s T alking Discovering dependencies between virtual machines using cpu utilization in  2010  G Khanna I  L aguna F  A rshad an d S Bagchi Distr ibuted diagnosis of failures in a three tier e-commerce system in  2007  R R S a m b as i v an A  X  Z heng M  D e Ros a  E  K re v at S  W h itm an M Stroucken W Wang L Xu and G R Ganger Diagnosing performance changes by com paring request ows in  2011  J  T a n a nd P  N a ras i m h an  RA M S and B lackS h eep I nferring w h ite-box application behavior using black-box techniques CMU Tech Rep 2008  J  T a n X  P a n E  Marinelli S  K a vulya R  G andhi a nd P  N a ras i m h an Kahuna Problem diagnosis for mapreduce-based cloud computing environments in  2010 
OSDI NSDI NSDI OSDI SOSP ICDE SIGCOMM DSN SIGCOMM CNSM SLAML SIGMOD ICAC PODC Computer Communication Review ICDCS Detection of abrupt changes theory and application DSN SOSP The American Statistician OSDI NSDI SOSP DSN WWW HotCloud SRDS NSDI NOMS 
207 
30 
30 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of Daily Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data  Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average daily operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rolling I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators  Data Element Methods  Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todays cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlights data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlights hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlights method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





