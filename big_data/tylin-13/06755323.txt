Stream Processing of Scientific Big Data on Heterogeneous Platforms Image Analytics on Big Data in Motion 
Abstract 
S.M. Najmabadi, M. Klaiber, Z. Wang, Y. Baroud and S Simon Institute for Parallel and Distributed Systems University of Stuttgart, Stuttgart, Germany mahdi.najmabadi@ipvs.uni-stuttgart.de 
High performance image analytics is an important challenge for big data processing as image and video data is a huge portion of big data e.g. generated by a tremendous amount of image sensors worldwide This paper presents a case study for image analytics namely the parallel connected component labeling \(CCL\which is one of the first steps of 
image analytics in general. It is shown that a high performance CCL implementation can be obtained on a heterogeneous platform if parts of the algorithm are processed on a fine grain parallel field programmable gate array \(FPGA and a multicore processor simultaneously. The proposed highly efficient architecture and implementation is suitable for the processing of big image and video data in motion and reduces the amount of memory required by the hardware architecture significantly for typical image sizes 
Keywords-component; Big data image analytic stream processing;  image processing; heterogeneous  platform 
feature  extraction; data  in  motion component  labeling 
I I NTRODUCTION One of the big data challenges are image analytic tools applicable to photos, surveillance videos and e.g. scientific data from sensors that can reach massive proportions over time 1 h o w t h at 2  5 q u intillio n b y tes are generated every day and 90% of all data was generated sin  Gen e ral l y  bi g dat a i s def i n e d bas e d on i t s  main characteristics which are growing in three dimensions volume, velocity and variety [2  In this ne w co ncep t, th e volume of unstructured data is in the scale of petabytes, and creation of them is in the fraction of the second In the literature big data in motion is defined as continues data 
streams at high data transfer rate. Such big data represent data sets that cannot be analyzed with conventional algorithms and standard hardware platforms [1 4 Du e to  the typical memory capacity and bandwidth limitations which determine the overall throughput, the processing of the continuously increasing amount of data is done online and locally on the streamed data. The new scale of volume velocity and variety requires redesigning a number of infrastructure components and algorithms to support the large-volume, complex and growing data [4 5   Modern high performance heterogeneous computing approaches based on field-programmable gate arrays FPGAs\d general-purpose computing on graphic 
processing units \(GPGPUs\ combined with general purpose processors enable the processing of large scale problems in the field of e.g. genomics, bioinformatics, graph analytics social network analytics, etc. which was not possible before s  o r i n s t an ce Con v e y  C o m p u t er propos ed a high-performance hybrid core system which pairs Intel processors with a coprocesso r of FPGAs which is able to execute data-intensive problems much more effectively IBM has also established Netezza as an advance high performance data analytical tool which has led to an exponential growth in the field of big data analytics Netezza is based on IBM blade architecture and uses FPGAs 
to filter the input data before processing it [1 Scientific data are defined in four different types: raw data, structured data, published data and data linked to the publication [5 t yp e o f  the scie nti f ic d a ta is ra w d a ta  which is generated from observation and experiment of different phenomena. For inst ance biological, climate and life sciences generate massive am ounts of scientific data [5  ag e s an d v ideos h a v e t h e h i gh es t a m oun t of  v o l u m e  among scientific data and are analytically prepared to gain additional value [8 9 T h e p r ep aratio n is d o n e b y  extracting various properties of the image such as objects 
and movements The first step in analytic image processing for many video-based applications is segmentation which is followed by connected-component labeling o n n ect edcomponent labeling performs the task of labeling all connected image pixels in a binarized image in order to identify objects or extract certain features of an object. The throughput of the CCL can strongly influence the performance of the whole image processing system as it is one of the first complex processing steps in image processing applications. For this reason, a parallel CCL algorithm with memory-efficient architecture is proposed here suited for high performance image processing 
applications such as image analytics. It is based on a scalable single pass CCL algorithm which is memoryefficient and therefore esp ecially suited for FPGAs. By using the proposed architecture and algorithm it is possible to achieve a high processing throughput for performing connected component labeling of streamed images without the need of buffering a full image, which would cause a performance limitation either due to limited FPGA-internal memory or due to limited FPGA-external memory bandwidth 
2013 IEEE 16th International Conference on Computational Science and Engineering 978-0-7695-5096-1/13 $31.00 © 2013 IEEE DOI 10.1109/CSE.2013.142 965 
2013 IEEE 16th International Conference on Computational Science and Engineering 978-0-7695-5096-1/13 $31.00 © 2013 IEEE DOI 10.1109/CSE.2013.142 965 


In the next section, the state of the art and related algorithms which cover memory-efficient and scalable CCL algorithms are presented. In Section III the video and image analytics framework are presented and the proposed algorithm and architecture are discussed. In the fourth Section experimental results are shown and compared to existing approaches in the literature. Fina lly, the paper is concluded in Section V II R ELATED W ORK Connected component labeling \(CCL\ one of the first steps in image analytics algorithms As an alternative to the classical two-pass CCL algorithm more sophisticated single pass algorithms have gained interest for reconfigurable computing recently. The classical CCL algorithms are completed after two scans of the same image and have sequential dependencies. Therefore a high amount of storage and memory for storing full images is requir  In order to store the full im age and labels, a memory with the same size as the original image is required. The labels are characterized by the position of the pixel within the image. If the pixel belongs to the background area of the image a special background label is used, otherwise its label is determined by the labels of adjacent pixels However, Baily et al. proposed FPGA based single pass architectures T h e pro posed arch itectu r e h a s t w o  drawbacks: Its worst case memory requirement is related to the height and width of the processed image, and also due to the lack of parallelism one pixel is processed per clock cycle causing a performance bottleneck in stream processing Ma et al. reduced the memory requirements significantly  t h e alg o ri t h m des c ri be d i n 14 by reu s i n g th e l a bels As a result, the required worst case memory is proportional to the width of the processed image. Kumar et al. proposed a parallel architecture and enhances the single pass algorithm used in  T h e m a in idea i s t o st ore the w h o l e i m a ges in  prior to processing. In order to gain performance speed-up the image is divided in to several slices, and different CCL units have to independently process them After that, each line from the image slice is fetched sequentially at a time in a round robin manner, and each CCL unit sends a vector to a global FIFO memory which acts as a vector collector. The vector mainly describes the regions of the processed image slice excluding the edges. In the next step a coalescing unit CU determines whether two regions of adjacent slices are connected and belong to the same object by processing one or both border of the image slice For the merge operation it is essential to connect the CCL units to the CU and perform the merging operation of neighboring image slices in a round-robin manner Additionally, to have a succes sful merging each edge region needs to have a unique, distinguishable label in its image slice. In order to overcome with the limitations of  [1 a modified version was proposed  [1 hic h r e d uc e s drastically the amount of memory required for processing by using two types of labels, slice local label and slice global labels. In this paper, a CU was introduced which improve real-time processing of CCL architecture by merging the results of image slices in a memory-efficient way III V IDEO AND I MAGE A NALYTICS F RAMEWORK Image analytics requires the processing of videos and images to transform pixel level information to object based information for the analysis of certain properties specific for the considered application domain like science industrial measurement techniques or life science applications. Videos are considered as sequence of images with a specific frame rate For high-speed scientific applications the frame rate may reach up to several hundred or thousand frames per second or more such that the video data of a single image sensor are in the range of 1 to 10 Gigabytes per second or 0.1 to 1 Petabyte per day To cope with this big data in motion, a high performance reconfigurable computing framework is proposed in this section Th e overall framework is composed of a high-speed input data stream a heterogeneous platform based on high performance reconfigurable computing devices using field programmable gate arrays \(FPGA and multi-core CPUs to which the image processing functions are mapped jointly. As shown in Figure 1, the input data stream is connected via several high speed links to the FPGAs which are able to acquire and analyze images as well as videos with a very high frame rate in real-time. Therefore, the framework based on the heterogeneous platform is equipped with integrated image processing capabilities such as segmentation component labeling and feature extraction Figure 1 High performance reconfigurable computing framework for proccessing real-time image stream processing The following image processing steps, which enable a massive data reduction from GBytes of Pixel data to only KBytes of abstract object descriptors - so called feature vectors, are transferred to the multi-core CPU. This reduction enables the framework to output information on every object in every frame even in real-time for very high frame rates The amount of data which has to be transferred from the FPGA is reduced by several orders of magnitude in this way The image processing architecture realized on the FPGA has the tasks of image segmentation and feature extraction For segmentation which is the process of separating the 
966 
966 


Figure 2 Grayscale raw image taken by image sensor and its histogram object from the background, thresholding is applied. By using this method all pixels ha ving an intensity value over a certain threshold are considered as an object and converted to black and all pixels below this threshold are converted to white and considered as background Accordingly, a binary image is generated from the origin al image. In this case, the threshold value for separating the objects from the background is of major importance. In this work the method proposed in is used f o r se gm e n ta tio n b y ge ne ra ting a histogram of the captured grayscale image of a scientific application in the field of spray process, as seen in Figure 2 The threshold value is calcula ted by using the arithmetic mean value of the two peaks detected in the histogram representing the objects and the background The next step is feature extraction based on connected component labeling The main challenge is processing high image frame rate in real-time which requires high bandwidth in the range of 10 to 100 Gbit/s. To overcome this problem, a highly optimized and sophisticated architecture is used. Due to the limited resources in embedded systems, algorithms which are especially dedicated to FPGA architectures have been proposed [12 13, 14, 16  The connected component labeling architecture proposed in this work is based on Bailey d Ma 13  w i t h a  parallel processing extension Figure 4 shows the architecture for sequential processing d i t c on s i s t s of several components which are described as follows: the neighborhood context block provides four registers A, B, C and D containing four previously processed pixels connected to the current pixel A, B and C contain the label of the previous row and D contains the previous pixel label The row buffer block is used for caching the new labels since they are not saved in a temporary image The key difference from  is t h at t h e labels are reused in every row, and cause the reduction in the need for memory The merger table decides any equality as a result of mergers on the previous row. The translation table modifies a label allocated to the pervious row to the current new label. The label for the current pixel is selected in the label selection block based on the labels of its neighborhood Figure 3 Segmentation of grayscale image with calculated threshold value The data merging unit records the features of each region by monitoring the labels of the pixels in the neighborhood context block. Each region has one entry in the data merging unit indexed by the region's label. Whenever a region is updated, its entry in the data merging unit is updated as well Figure 4 Connected Component Labeling Architecture proposed in [13  A bounding box is defined by two coordinates A and B Coordinate A indicates the upper left corner and coordinate B indicates the lower right corner. In order to extract the bounding box for each object the structure and the merging process within the data merging unit is proposed [4 ,6 The data merging unit has to be changed as shown in Figure 5 to provide bounding box extraction The modified block has two inputs a and b, one for providing the currently processed pixels coordinates and the other one for giving information on the neighbor pixe ls label. The input data is read from the corresponding data table. For each extracted object a merging unit as well as a data table is necessary. To 
967 
967 


       
002 003\004 005\006\007\(\002 003\010 002 003\011  012 003\004 005\006\007 \(\012 003\010 012 003\011  002 013\004 005\014\002\(\002 013\010 002 013\011  012 013\004 005\014\002\(\012 013\010 012 013\011  
find the bounding box for two entries of the data table the following equations are used 1 2 3 4 Figure 5 Data Merging Unit for extracting several features of the image objects simultaneously In order to handle high data throughput a parallelization approach is proposed. Thus, the image is divided into several slices, and each slice is processed separately in parallel.  In each slices the objects are identified and merged by a central unit called the coalescing unit As a result several pixels are processed simultaneously and speedup based on the number of image slices can be obtained Figure 6 through Figure 8 show the processing steps for feature extraction. Figure 6 shows the binary image after segmentation. In the next step, the image is divided into several image slices for processing in CCL processing units which are working in parallel As it is shown in Figure 7 the bounding box is extracted by each CCL proceeding unit In the last step, the bounding boxes touching the slice borders will be merged together to have the correct result This step is depicted in Figure 8 Figure 6 Binarized image after thresholding Figure 7 Extracted object features for all sub-images Figure 8 Merged object features for input image IV E XPERIMENTAL R ESULTS In the following, experime ntal data to examine the potential of heterogeneous systems consisting of reconfigurable logic devices and general purpose processors GPP\ the context of an image processing system for big data analytics is provided For this examination the approaches from  n d [16] are ev al u a t e d on bot h  reconfigurable logic devices and in software on a general purpose processor The approach in a i n s a s p eedu p f r o m di v i ding t h e  image in several vertical slices using the architecture from  s lice process o rs an d m e rg i n g th e res u lt s  u s i n g  a coalescing unit. The results for the implementation on a single FPGA where parallelism is inherently used and in software for different image sizes on a single core are given in Table 1 For the implementation of an array of slice processing units \(SPU\sisting of up to 100 SPUS on a single FPGA was realized, where all are processing an individual image in parallel. For the software implementation the classical two-pass algorithm s u s ed t o pr oces s on e im a g e on a  single core of a general purpose processor \(GPP 
968 
968 


TABLE I CCL-BENCHMARK CPU VS FPGA 256 256 / 4 6,62 1024 1024 / 8 10,51 2048 1024 / 4 13,40 Under this circumstances the coalescing unit on the GPP achieves up to 13 GPixels/s, which is in the same order of magnitude as the FPGA implementation. In general, the irregular data structures and the sequential algorithm of the coalescing process are better suited for a GPP software implementation. Considering the performance results of both slice processing units and the coalescing unit on a heterogeneous platform, the conclusion is that the slice processing units should be implemented on a FPGAs and Figure 9 Achievable frame rate for an image with 0.5 Megapixels Figure 10 Achievable frame rate for an image with 2.1 Megapixels Figure 11 Achievable frame rate of maximum 800 fps for an image with 6.3 MegaPixels and a throughput of 5 Gigapixels per second the coalescing unit should be implemented on a GPP in order to achieve maximum performance for big data image analytics. Achieving a throughput of 5 Gigapixel/s is equivalent to 0.4 Petabytes per day, so the implementation on a heterogeneous platform with a single mid-sized FPGA 
Width Height Gigapixels/s Cores Gigapixels/s Area Speedup 128 128 0,06301 1 15,3 8 242 256 256 0,06277 1 11,3 43 182 512 512 0,06286 1 12,3 81 198 1024 512 0,06189 1 9,1 93 146 2048 1024 0,06062 1 6,9 83 114 When comparing the throughput of the software implementation on a GPP and the dedicated architecture on an FPGA, the bandwidth of the FPGA architecture for CCL is one to two orders of magnitude higher. The hardware architecture is highly optimized to the FPGA structure while for the presented software solution further research has to be done to enable a proper comparison, but no decrease in speedup by less than one order of magnitude can be expected.  This enables the FPGA architecture to process an image stream consisting of several different image slices simultaneously in real-time. For the case of processing a single image, only one slice processing unit of the SPU array can be used. Still the FPGA architecture accelerates the processing by approximately a factor of 2 compared to a single GPP core. By using the parallelization scheme from  e v e ral s l i ce proces s i ng  u n i ts can be used to process a single image in parallel enabling a higher throughput. Figure 9 through Figure 11 show the results for this approach where the coalescing unit is realized for FPGAs. Depending on the image size up to 4.5 GPixels/s can be processed. To study the performance of the coalescing process on a GPP, a prototype implementation of a software version of the coalescing unit was realized. The result for the throughput given in Table 2 can be achieved for the case that the feature vectors provided by the slic e processing units are already stored in the systems RAM and can be accessed at full memory bandwidth TABLE II SOFTWARE COALESCING THROUGHPUT IN GIGAPIXEL PER SECOND 
Image Size CPU FPGA Result image size / # of slices throughput \(Gigapixels/s 
Hardware Platform 
969 
969 


pp 372377 2 T E a to n, D D e r o o s  U nde r s tan d i n g big  dat a   i n M c G r aw H il l Companies, April 2012 3 C P  A r ka dy B. Z a s l av s k y and D  G e o r g a ko po ul o s  S e n s ing  as  aservice and big data July 2012 4 H ir z e l  M A ndr a d e  H  G e dik B   J a cq ue s S il v a, G  K h an de kar  R   Kumar, V.; Mendell, M.; Nasgaard, H.; Schneider, S.; Soule, R.; Wu K.-L., "IBM Streams Processing Language: Analyzing Big Data in motion vol.57, no.3/4 pp.7:1,7:11, May-July 2013 5 Y D e m che n k o Z Z h ao P G r o sso  A W i biso no a n d C de L aat Addressing big data challenges for scientific data infrastructure 2012 IEEE pp. 614617 6 S  H K e t a n P a r a nj ap e an d B  Ma s s o n  Het er og en eou s c o m p u ti n g i n the cloud: Crunching big data and democratizing hpc access for the life sciences, in Intel White Paper 7 C. Co m p ute r   H y b r i dco r e   the  b i g da ta co m p ut ing ar c h i te c t u r e  i n Convey White Paper 8 D G a r l asu, V  S a n d ul e s cu, I  H a l c u G  N e cul o iu, O  G r ig o r iu M  Marinescu, and V. Marinescu, A big data implementation based on grid computing pp. 14 9 O ra c l e   B i g d a t a for t h e e n t e rp ri se    in w h i t e P a p e r  2 0 13  10 K   A ppi ah A  H u nte r F  Me ng  a n d P  D i c k inso n A cce l e r a te d hardware video object segmentation: From foreground detection to connected components labelling 2013, pp. 12821291 11 A  Ro s e nf e l d and J  L  P f al tz  S e que nt ial o p e r at io ns i n  dig ital  p ict ur e  processing, J. ACM, vol. 13, pp. 471494, October 1966 12 D Ba il ey and C Jo h n sto n  S i n g l e pass co n n e cte d co m p o n e n ts  analysis 2007, dec. 2007, pp. 282287 13 N M a, D B ail e y and C Jo h n sto n  O pti m ise d si ng l e pass co n n e cte d  components analysis dec. 2008, pp. 185 192 14 D  Ba il ey C. Jo hns to n a n d N   Ma  Co n ne cte d  co m p o n e n ts an al y s is of streamed images  sept. 2008, pp. 679 682 15 V K u m ar K  I r ick, A A l Maashr i a n d N V i jay k r i sh n an  A scal abl e bandwidth aware architecture for connected component labeling july 2010, pp. 116 121 16 M  K l a i b e r  L  R o c k s t r o h  Z  W a n g  Y  B a r o u d  S  S i m o n  A  Memory-Efficient Parallel Single Pass Architecture for Connected Component Labeling of Streamed Images Seoul, Korea, Dec. 2012 17 R. C  G o nz al ez and R. E  W o o d s. D i g ital I m age P r o c e ssing  3 r d   Edition\. Prentice Hall, August 2007, pp.741-76 
 
Conference on Information Communication Technologies \(ICT\, 2013 IEEE in Proceedings of the International Conference on Advances in Cloud Computing \(ACC IBM Journal of Research and Development 4th International Conference on Cloud Computing Technology and Science \(CloudCom in Roedunet International Conference \(RoEduNet 2013 11th in Computer vision and image understanding in Proceedings of Image and Vision Computing New Zealand in ICECE Technology, 2008. FPT 2008 International Conference on International Conference on Field Programmable Logic and Applications in VLSI ISVLSI\, IEEE Computer Society Annual Symposium on International Conference on Field-Programmable Technology \(FPT 
can be scaled up linearly with a several FPGAs and processor cores under the cond ition that several different images are processed in parallel. Beyond these performance considerations, mapping the irregular data structure and the sequential part of the algorithm, which is the coalescing algorithm, to the GPP is of advantage because of its higher processing frequency. For the regular data structures used in the slice processing units carryin g out the parallel part of the algorithm, the FPGA achieves a higher throughput due to its parallel architecture V C ONCLUSION Images and videos have the highest amount of volume among big data and therefore high performance image processing plays an important role for image analytics. In this paper we have investigated a parallel component labeling algorithm including feature extraction with a broad set of features as an important part of an image analytics framework. It is shown here that the performance of the proposed parallel component labeling algorithm with generalized feature extraction is accelerated and optimized if it is mapped to and executed on a heterogeneous hardware platform based on a fine-grained field-programmable gate array and a multi-core processor. For the parallelized connected component labeling \(CCL\orithm of this paper, the required memory compared to the requirement of algorithms known in the literature is reduced significantly on a heterogeneous hardware platform for typical image sizes even when compared to similar sliced parallel single pass CCL algorithms and architectures such that memory size nor memory bandwidth of the hardware platform has an impact on the performance The basic structure of the algorithm and architecture is a set of parallel CCL units generating feature data of image slices being coalesced in a separate and subsequent coalesci ng unit. In order to achieve highest performance, it was shown that the parallel CCL units should be implemented on fine-grained FPGAs and the coalescing unit should be implemented in software on a multi-core processor. With the achieved throughput of 5 Gigapixels per second or 0.4 Petabytes per day the implementation on such a heterogeneous platform with a single mid-sized FPGA can be scaled up linearly with a plurality of FPGAs. Thus, the concept has been proven to be ideally suited for high performance image analytics for big data in motion A CKNOWLEDGMENT The authors would like to thank the German Research Foundation \(DFG\or the financial support. This work has been carried out within the research project Si 586 7/1 which belongs to the priority program DFG-SPP 1423 ProzessSpray The authors also would like to thank the working group of Prof. Sommerfeld, MVT Universität HalleWittenberg for providing images of their experiments R EFERENCES 1 U Ch an dr ase k har  A  Re ddy and R Rat h  A co m p ar at iv e stu d y o f enterprise and open source big data analytical tools 
970 
970 


the corresponding sizes are 4 to 5 times larger and reach around 20 GB The training work is performed on a threelayer perceptron with the structure as 784-40-10 meaning 784 neurons in the input layer 40 neurons in the hidden layer and 10 neurons in the output layer 2 System Setup cNeural The client node and master node of cNeural both run on the master machine Each slave machine consists of 8 computing nodes and one storage node Each storage node is maintained by an RegionServer daemon in HBase Thus there are 288 computing nodes in total As each slave machine is congured 16 GB memory each computing node has 2 GB memory and it is enough for storing the training data subset in use MapReduce Approach The master machine acts as the JobTracker and NameNode And each of the rest 36 slaves both act as TaskTracker and DataNode Each machine is congured 8 slots There are also 288 slots in total and each slots Child JVM is also congured 2 GB as each computing node in cNeural B Performance of Training Data Loading In this experiment cNeural is build up by 1 2 4 up to 36 machines respectively Its inner HBase consists of 36 region servers During data loading the computing nodes access HBase concurrently The experiment is performed on cNeural with different number of machines each with 8 computing nodes and different size of dataset The curves in Fig 5 shows the load time for the different sizes of training samples respectively The experiment with 2 million training samples on 1 machine failed to proceed due to insufcient memory to load all training data Obviously as the size of input samples increases the time cost on training data loading increases too When the input sample size is xed the loading speed increases almost linearly with machine number This is because that the inner HBase provides concurrent data access and when new machines added in the system they takes a share of loading the training dataset However when the number of machines increases to some degree etc 20 in the case of test with 1 million training samples the performance of data loading can hardly be further improved This because the network transmission I/O blocking on storage nodes in HBase become bottlenecks To get valid results neural network training usually needs to run thousands of epochs And in cNeural the computing nodes only need to load data from HBase once Fig 6 illustrates the percentage of time cost on training data loading with varying training epochs The time cost on training data loading is xed Therefore the proportion of training data loading time decreases as training epochs increases With 36 machines when training epochs reach 200 the time cost in training data loading accounts only 0.3 of the total execution time The experimental results demonstrate that the time cost of data loading is very little in cNeural  0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 0 10 20 30 40 50 60 70 80     Number of machines \(each has 8 cores Load time \(seconds       Load 0.5 million training samples   Load 1 million training samples Load 2 million training samples   Figure 5 Performance of training data loading with different number of machines on different size of training samples The experiment failed to load 2 million training samples with only one machine This is the case that the training work can not be processed on a single machine 1 5 10 20 50 100 200 500 0 10 20 30 40 50 60 70 80 90 100 Percentages of time cost Number of training epoches Data loading Training Figure 6 Percentages of time cost of data loading and training with different training epochs on 1 million training samples and 36 machines C Performance of Executing Training Process The performance of executing training process is in shown in Fig 7 We can see that with the same number of machines training 0.5 million samples is always about one time faster than training 1 million samples and three times faster than training 2 million samples This indicates that time cost of our parallel algorithm increases near linear with the number of input training samples On the other perspective if the size of training data is xed the training speed of cNeural is much improved when more machines are utilized For 1 million training samples when the numbers of machines are 2 4 8 16 32 the respective time costs of one training epoch are 18.3 seconds 9.2 seconds 4.7 seconds 2.4 seconds 1.3 seconds almost getting half of time cost decrease This shows that the training process of 382 


cNeural achieves good speedup performance  0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 0 4 8 12 16 20 24 28 32 36 40     Number of machines \(each has 8 cores Training time per epoch \(seconds   Train 0.5 million training samples   Train 1 million training samples Train 2 million training samples   Figure 7 Performance of each epochs training time cost in cNeural with various numbers of machines and different sizes of training samples 124681012141618202224262830323436 0 4 8 12 16 20 24 28 32 36 40 Execution time per epoch seconds Number of machines each has 8 cores communication time computation time Figure 8 Time cost of communication and computation during a training epoch with different number of machines The histogram in Fig 8 shows the time cost by communication and computation during each epoch with different machines used respectively When more machines adopted the proportion of communication time cost increases However it always only occupies a small proportion Moreover the whole execution time still decreases D Comparative Experiments We compared the performance of cNeural with the approach proposed by as the y also tar get to w ards training neural networks with large scale training datasets This approach adopts HDFS to store large scale training data and uses MapReduce as the parallel processing engine The comparative experiments are conducted with identical number of machines and the same size of training dataset Both the execution performance and speed scalability are evaluated here Table I E XECUTION TIME  IN SECOND  ON M AP R EDUCE APPROACH AND C N EURAL WITH 1MILLION SAMPLES  Machine MapReduce appraoch cNerual Speedup ratio 1 784.0 36.3 21.6 6 177.0 6.3 28.2 12 107.0 3.1 34.1 18 95.0 2.4 40 24 82.0 1.6 50.5 30 79.0 1.4 57.9 36 60.0 1.2 52.2 1 Training Execution Performance As shown in Table I cNeural achieves around 50 times speedup over the MapReduce approach under the same conditions This improvement can be attributed to two facts First cNeural resides the training data across computer nodes memory when training however the MapReduce approach needs to reload the training data from hard disks in each training epoch Second in messaging communication cNerual adopts a event-driven model while the MapReduce approach uses the heartbeat polling model The former makes the training process more compact 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 0 5 10 15 20 25 30   Number of machines \(each has 8 cores Speed up   Hadoop MapReduce   cNeural Figure 9 Speedup salability of cNeural and Hadoop MapReduce approach with different number of machines 2 Speedup Scalability We also tested the speedup scalability of both systems with varying machine numbers The experimental results are shown in Fig 9 On one side they both achieve good speedup scalability within 10 nodes in this algorithm On the other side the speedup of the MapReduce approach failed to keep linear as cNeural because of the 383 


overhead of job initialization in Hadoop is much larger than cNeural VIII C ONCLUSION AND F UTURE W ORK The past several years have witnessed an ever-increasing growth speed of data To address large scale neural network training problems in this paper we proposed a customized parallel computing platform called cNeural Different from many previous studies cNeural is designed and built on perspective of the whole architecture from the distributed storage system at the bottom level to the parallel computing framework and algorithm on the top level Experimental results show that cNeural is able to train neural networks over millions of samples and around 50 times faster than Hadoop with dozens of machines In the future we plan to develop and add more neural network algorithms such as deep belief networks into cNeural in order to make further support training large scale neural networks for various problems Finally with more technical work such as GUI done we would like to make it as a toolbox and open source it A CKNOWLEDGMENT This work is funded in part by China NSF Grants No 61223003 the National High Technology Research and Development Program of China 863 No 2011AA01A202 and the USA Intel Labs University Research Program R EFERENCES  C Bishop Neural networks for pattern recognition  Clarendon press Oxford 1995  J Collins Sailing on an ocean of 0s and 1s  Science  vol 327 no 5972 pp 14551456 2010  S Haykin Neural networks and learning machines  Englewood Cliffs NJ Prentice Hall 2009  R Hecht-Nielsen Theory of the backpropagation neural network in Proc Int Joint Conf on Neural Networks,IJCNN IEEE 1989 pp 593605  Y  Loukas  Articial neural netw orks in liquid chromatography Efcient and improved quantitative structure-retention relationship models Journal of Chromatography A  vol 904 pp 119129 2000  N Serbedzija Simulating articial neural netw orks on parallel architectures Computer  vol 29 no 3 pp 5663 1996  M Pethick M Liddle P  W erstein and Z Huang P arallelization of a backpropagation neural network on a cluster computer in Proc Int Conf on parallel and distributed computing and systems PDCS  2003  K Ganeshamoorthy and D Ranasinghe On the performance of parallel neural network implementations on distributed memory architectures in Proc Int Symp on Cluster Computing and the Grid CCGRID  IEEE 2008 pp 9097  S Suresh S Omkar  and V  Mani P arallel implementation of back-propagation algorithm in networks of workstations IEEE Trans Parallel and Distributed Systems  vol 16 no 1 pp 2434 2005  Z Liu H Li and G Miao Mapreduce-based backpropagation neural network over large scale mobile data in Proc Int Conf on Natural Computation ICNC  vol 4 IEEE 2010 pp 17261730  M Glesner and W  P  ochm  uller Neurocomputers an overview of neural networks in VLSI  CRC Press 1994  Y  Bo and W  Xun Research on the performance of grid computing for distributed neural networks International Journal of Computer Science and Netwrok Security  vol 6 no 4 pp 179187 2006  C Chu S Kim Y  Lin Y  Y u  G  Bradski A Ng and K Olukotun Map-reduce for machine learning on multicore Advances in neural information processing systems  vol 19 pp 281288 2007  U Seif fert  Articial neural netw orks on massi v ely parallel computer hardware Neurocomputing  vol 57 pp 135150 2004  D Calv ert and J Guan Distrib uted articial neural netw ork architectures in Proc Int Symp on High Performance Computing Systems and Applications  IEEE 2005 pp 210  H Kharbanda and R Campbell F ast neural netw ork training on general purpose computers in Proc Int Conf on High Performance Computing HiPC  IEEE 2011  U Lotri  c and e a Dobnikar A Parallel implementations of feed-forward neural network using mpi and c on  net platform in Proc Int Conf on Adaptive and Natural Computing Algorithms  Coimbra 2005 pp 534537  Q V  Le R Monga and M e a De vin Building high-le v e l features using large scale unsupervised learning in Proc Int Conf on Machine Learning ICML  ACM 2012 pp 216  J Ekanayak e and H e a Li T wister a runtime for iterati v e mapreduce in Proc of the 19th ACM International Symposium on High Performance Distributed Computing  ACM 2010 pp 810818  Y  Bu B Ho we M Balazinska and M D Ernst Haloop Efcient iterative data processing on large clusters Proc of the VLDB Endowment  vol 3 no 1-2 pp 285296 2010  M Zaharia M Cho wdhury  T  Das A Da v e  J  Ma M McCauley M Franklin S Shenker and I Stoica Resilient distributed datasets A fault-tolerant abstraction for in-memory cluster computing in Proc USENIX Conf on Networked Systems Design and Implementation  USENIX Association 2012 pp 216 384 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


