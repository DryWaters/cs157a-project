Jiang Zhou Weiping Wang Xiaoyan Gu 
Computer Application Research Center Institute of Computing Technology Chinese Academy of Sciences Graduate University of Chinese Academy of Sciences Beijing China zhoujiang@ncic.ac.cn Institute of Information Engineering Chinese Academy of Sciences Beijing China wangweiping@iie.ac.cn 
OAMS A Highly Reliable Metadata Service for Big Data Storage 
Jing Guo Cuilan Du 
Computer Application Research Center Institute of Computing Technology Chinese Academy of Sciences Graduate University of Chinese Academy of Sciences Beijing China guxiaoyan@ncic.ac.cn National Computer Network Emergency Response Technical Team Coordination Center of China Beijing China guoj0000@126.com National Computer Network Emergency Response 
Technical Team Coordination Center of China Beijing China dcl@isc.org.cn 
Dan Meng 
Institute of Information Engineering Chinese Academy of Sciences Beijing China md@iie.ac.cn 
As application requirements increase in quantity and popularity big data storage is becoming an important technology which data centers and Internet companies depend on The cluster le system with centralized metadata 
Abstract 
management often encounters planned or unplanned downtime which requires higher reliability for metadata service Current paradigms use the backup server to take over as the primary when the latter is in the case of failures But if the backup crashes the le system is still in an unreliable state In this paper we present a novel primary-backup policy OAMS which ensures the availability of metadata service in cluster le system Different from traditional paradigms OAMS employs multiple standbys to tolerate the single point 
of failure It is based on the built-in shared storage pool for metadata synchronization and a series of protocols for active election active-standby switching and etc By using a prepared automatic state transition among metadata servers OAMS achieves an automatic recovery in the form of hot standby It also supports server self-recovery and dynamical addition for standbys at runtime Evaluation results show that OAMS obviously improves the reliability of metadata service while the 
average performance degradation is below 8 
I I NTRODUCTION With the expansion of information scale and emergence of massive semi-structured or unstructured data the big data period is coming Applications  in data centers and Internet companies are diversi“ed greatly and often need 24x7 hours service which requires higher reliability The 
big data cluster le system metadata service high reliability primary-backup paradigm 
Keywords 
cluster le system acting as the underlying storage for big data analysis and processing faces new challenges Current distributed le system usually adopts a centralized metadata management policy That is the metadata server manages les and directories of the whole le system and responds to client requests For example GFS uses the master to manage namespace of the cluster and provide metadata service HDFS adopts the similar idea and the namenode is also a single server As a big data cluster often encounters 
planned or unplanned downtime which may be caused by software upgrade system con“guration and hardware failures the design of centralized metadata management easily leads to the single point of failure once the master crashes the le system will be unavailable To solve this problem the primary-backup paradigm is currently widely used to improve the reliability of metadata service In this paradigm the backup server takes over as the primary and continues 
2013 IEEE 16th International Conference on Computational Science and Engineering 978-0-7695-5096-1/13 $31.00 © 2013 IEEE DOI 10.1109/CSE.2013.191 1287 
2013 IEEE 16th International Conference on Computational Science and Engineering 978-0-7695-5096-1/13 $31.00 © 2013 IEEE DOI 10.1109/CSE.2013.191 1287 


to provide service when the latter encounters failures It tolerates the single point of failure and needs no complicated state transition between metadata servers However as the primary-backup paradigm relies heavily on the backup it could lead to another single point of failure If the backup crashes the primary is still in an unreliable state There are many methods used for the primary-backup paradigm According to the backup mode it is divided into active-standby and cluster manner The paradigm can be classi“ed into three categories denoted as cold standby warm standby and hot standby with the recovery method It is also de“ned as manual and automatic mode according to the switching pattern When switching from the primary to the backup it needs to keep consistent namespace states between them Traditional log le systems combine transaction processing with f ailo v er technology to recover metadata information They adopt write-ahead logs to record metadata operations to local disks rstly and then execute them in memory If the system crashes or restarts it can recover namespace state by reading and redoing logs Namespace consistency of the primary-backup paradigm depends on log technology During running the backup synchronizes metadata operations from the primary and commits them in its own namespace With this approach the backup keeps an up-to-date namespace image with the primary and takes over the service of it in the case of failures As the backup needs to synchronize metadata information from the primary the method can be divided into two ways metadata replication and shared storage BackupNode in HDFS is a simple reliable feature The backup is a wrapper of the regular namenode instance which implements metadata replication of the primary It keeps an up-to-date namespace image with the primary but does not provide service Once the primary crashes a failover procedure will be done with the manual pattern BackupNode uses replication mechanism to synchronize metadata information When the primary stores log records to local disk it ushes them to the backup simultaneously Due to various errors in the cluster the backup may lose metadata modi“cations and cannot synchronize with the primary in time And BackupNode takes a long time for recovery as it needs to reconstruct le locations for failover Yahoo combines BackupNode with Linux tools to achieve active-standby switching It uses double network interface cards for failure detection and DRBD for metadata synchronization Yahoo uses the shared storage to keep metadata consistency between the primary and the backup With this approach it needs additional facility and has the risk of con”ict for concurrent access AvatarNode at F acebook is designed for a realtime HDFS that supports online applications There are two AvatarNodes in the cluster one providing service as the active while the other working as a standby AvatarNode uses NFS to store one cop y of the lesystem image and one copy of the transaction log Metadata operations are written by the active in NFS and read by the standby simultaneously AvatarNode acts as a hot standby because datanodes report block locations to both the active and standby However AvatarNode depends on NFS and has the problem of IO fencing when sharing transaction logs The implementation in Cloudera is similar to A v atarNode which a v oids the risk of concurrent access It employs additional tools to prevent the isolated sever from writing shared metadata logs Hadoop HA Branch uses NFS or the quorum journal manager QJM for synchronizing metadata information The further step is to use BookKeeper as the sharing logging service Though HA Branch achieves automatic active-standby switching to some extent the reliability will be greatly reduced if the standby crashes And it increases the complexity of le system with third-party software support Paca“cA HARP 14 and Echo 15 implement storage systems based on log technology By de“ning different replication strategies they ensure metadata consistency when recovery However these systems are key-value storages and cannot satisfy sophisticated operation semantics of le system Table I compares different primary-backup paradigms for metadata server Different from above paradigms we propose a novel primary-backup policy called one active multiple standbys OAMS to achieve the availability of metadata service in cluster le system With a series of protocols OAMS tolerates multiple points of failures and achieves an automatic switching in the form of hot standby It is based on the built-in shared storage pool to synchronize metadata operations Besides OAMS supports server self-recovery and dynamical addition for standbys at runtime Theoretical analysis pro v es that the reliability of metadata service can be improved by 3 to 4 orders of magnitude comparing to traditional primary-backup paradigms Performance results show that OAMS can achieve active-standby switching within several seconds while keeping high performance for metadata operations The remainder of this paper is organized as follows Section II introduces system model of our primary-backup paradigm Section III describes the OAMS policy and its three protocols In section IV we evaluate the reliability and performance of OAMS and compare it with other paradigms We conclude the paper in Section V II SYSTEM MODEL Our policy is deployed in a cluster le system with centralized metadata management which employs one metadata server as the active and multiple servers as standbys They are connected by high speed network and constitute a primary-backup cluster Besides providing metadata service the active and standby are also treated as storage nodes for 
1288 
1288 


A Synchronization Protocol 
002\003\004\005\006\007 010\004\011\012\013\014\015 010\004\011\012\013\014\015 010\016\011\017\007\013 020\004\021\017\011\022\007 023\021\021\024 025\011\026\021\020 
Table I PRIMARY-BACKUP PARADIGMS FOR METADATA SERVER Figure 1 System Model of the primary-backup cluster three protocols to achieve an automatic recovery in the case of failures 
BackupNode Warm standby Manual Replication Yahoo Warm standby Manual or automatic Shared storage AvatarNode Hot standby Manual Shared storage Cloudera Hot standby Manual Shared storage Hadoop HA Hot standby Manual or automatic Replication or shared storage a shared storage pool The pool is a built-in virtual storage which is used for metadata synchronization and persistence Fig 1 depicts the system model of the primary-backup cluster which comprises one active and two standbys OAMS acts as a highly reliable backup policy to ensure the availability of metadata service in cluster le system Under failure free cases the active responds to client requests and provides le system service When the active modi“es metadata information and updates namespace it writes logs to local disk and ushes them to standbys through the shared storage pool Standbys receive metadata operations and apply them in memory simultaneously They keep upto-date namespace states with the primary but do not provide metadata service Namespace image can also be persistent in the shared storage pool for failover after the metadata server restarts With the heartbeat mechanism a monitor is used to detect the system status in the primary-backup cluster If the active crashes one standby is elected as the new active and takes over its service To reach a consensus the Paxos protocol is used for acti v e election among multiple standbys and to prevent the so-called split-brain scenario After active-standby switching the client connects to the new active for fault tolerance and resends metadata requests The new active receives metadata operations and continues le system service The role of active and standby can be assigned from con“guration when starting up At any time there is only one metadata server which is in the active state As metadata and data management are decoupled in the cluster le system le contents are split into large blocks and replicated in datanodes For constructing le locations datanodes talk to both the active and standbys By using a prepared automatic state transition among metadata servers OAMS achieves an automatic recovery in the form of hot standby III OAMS POLICY In contrast to traditional primary-backup paradigms OAMS employs multiple standbys for the active to tolerate failures in cluster le system It provides a highly reliable metadata service with each server acting as different roles A global view is cached to maintain states of the active and standbys Combined with the shared storage pool and Paxos protocol OAMS processes metadata synchronization active election and active-standby switching It includes 
Heartbeat Heartbeat Heartbeat Active election Configuration Active-standby switching 
When using multiple standbys for backup of the active it needs to synchronize metadata operations among them Synchronization protocol is a replication method which transmits journal streams from the active to standbys Based on the shared storage pool a distributed protocol similar to the two-phase commit protocol 2PC is applied in OAMS to provide namespace consistency 2PC protocol is a simple and popular atomic commitment protocol for distributed transaction It is divided into two phases which is called preparation phase and commitment phase respectively In the preparation phase a role called the coordinator sends prepare messages to all participants to ask them whether they agree to commit it Each participant checks its status and decides if it satis“es the condition The coordinator waits and collects replies from participants in the second phase When receiving all ready messages it sends commit messages to participants for execution After all participants have completed operations they send ACK messages to the coordinator to nish the transaction If some participants send abort messages the transaction will be cancelled for all During this process logs are written in local disks for recovery 2PC protocol achieves consistency 
Paradigm Recovery method Switching pattern Metadata synchronization 
1289 
1289 


of transaction but the state is still indeterminable if there are failures in the commitment phase The process of metadata synchronization using 2PC is depicted in Fig 2a In OAMS standbys need to synchronize metadata operations from the active to keep an up-to-date namespace with it This can be regarded as a transaction because it may result in inconsistent server states if errors occur Inspiring by 2PC OAMS presents a synchronization protocol which reduces the overhead of replication and achieves namespace consistency among the active and standbys For comparison the protocol in OAMS is depicted in Fig 2b The key idea here is to combine the shared storage pool with the global log to reduce the processing latency After the coordinator decides committing or aborting the transaction it is unnecessary to wait for the operations of participants The coordinator will respond to clients when it receives ACK messages from participants and write the global log For the synchronization protocol the active is seen as the coordinator and standbys as participants A batch of metadata operations are buffered in the active and wait for being committed one time When the active writes log records in local disk it begins to ush them to standbys The standby receives metadata journals and writes prepare logs in local disk It then sends the ACK message to the active to continue these operations When receiving all ACK messages from standbys the active commits metadata modi“cations and writes a global log in the shared storage pool If some standby does not respond after a timeout period the active will cancel all operations and marks a tag in the global log For 2PC protocol it needs to notice all participants to commit or cancel operations before the active responds to clients In OAMS as the state of metadata operations has been stored in the shared storage pool the active can return to clients immediately after it writes the global log As the pool is a reliable virtual storage the standby can decide whether to commit them from the global log even if it does not receive notice from the active To nish the processing of synchronization the active still sends commit or abort messages to participants The participant completes subsequent operations and writes logs in local disk The global log can also be used for recovery after the active or standby crash With the synchronization protocol OAMS ensures the consistency of namespace state among the active and standbys while not affecting the performance of metadata operations The failover protocol is used for fault tolerance when the active crashes With the heartbeat mechanism OAMS monitors server status in the primary-backup cluster If the active crashes a new one will be elected from standbys and takes over the metadata service of it Combined with Paxos protocol for election OAMS reaches a consensus for choosing the new active among servers With more than one    
B Failover Protocol 
027\007\022\005\012 030\021\021\017\013\005\012\011\004\021\017 031\017\005\004\007 001\000 020\004\011\017\004 001\001 024\021\022\020 010\007\012\013 001\000 023\017\007\023\011\017\007 001\001 032\007\020\020\011\022\007\020 031\011\005\004 025\011\017\004\005\003\005\023\011\012\004 027\007\022\005\012 031\011\005\004  002\030\033 031\017\005\004\007 001\000 003\021\032\032\005\004 001\001 021\017 001\000 011\014\021\017\004 001\001 024\021\022\020 031\017\005\004\007 001\000 003\021\032\032\005\004 001\001 021\017 001\000 011\014\021\017\004 001\001 024\021\022\020 034\012\013 030\021\032\032\005\004 021\017 011\014\021\017\004 031\017\005\004\007 001\000 023\017\007\023\011\017\007 001\001 024\021\022\020 031\011\005\004 002\030\033 034\012\013 035\007\020\023\021\012\013 004\021 003\024\005\007\012\004\020 031\017\005\004\007 001\000 007\012\013 001\001 024\021\022\020 036\011\037 025\030!\014\011\020\007 023\017\021\004\021\003\021\024 005\004\016\021#\004 005\032\023\017\021\006\007\032\007\012\004 030\021\032\023\024\007\004\007 021\023\007\017\011\004\005\021\012\020 030\021\032\023\024\007\004\007 021\023\007\017\011\004\005\021\012\020   027\007\022\005\012 030\021\021\017\013\005\012\011\004\021\017 031\017\005\004\007 024\021\022\020 005\012 024\021\003\011\024 013\005\020 031\011\005\004 027\007\022\005\012 025\011\017\004\005\003\005\023\011\012\004 031\017\005\004\007 001\000 023\017\007\023\011\017\007 001\001 024\021\022\020 005\012 024\021\003\011\024 013\005\020 035\007\020\023\021\012\013 004\021 003\024\005\007\012\004\020 031\011\005\004 034\012\013 034\012\013 036\014\037 016\007 020\015\012\003\016\017\021\012\005&\011\004\005\021\012 023\017\021\004\021\003\021\024 005\012 002\(\010 030\021\032\032\005\004 021\017 011\014\021\017\004 031\017\005\004\007 001\000 003\021\032\032\005\004 001\001 021\017 001\000 011\014\021\017\004 001\001 024\021\022\020 010\007\012\013 001\000 023\017\007\023\011\017\007 001\001 032\007\020\020\011\022\007\020 031\017\005\004\007 022\024\021\014\011\024 024\021\022 005\012 020\016\011\017\007\013 020\004\021\017\011\022\007 023\021\021\024  002\030\033 030\021\032\023\024\007\004\007 021\023\007\017\011\004\005\021\012\020 030\021\032\023\024\007\004\007 021\023\007\017\011\004\005\021\012\020 
Figure 2 Metadata synchronization protocol standby for backup server state transition is more complicated than that of traditional primary-backup paradigm In OAMS the metadata server starts up in different states with the con“guration A global view is cached to maintain the address and state of each server It is modi“ed by the active dynamically and stored in the shared storage pool periodically The active and standby add event triggers in the view which will result in state transition when failures are detected by the monitor In the failover protocol the state of server is classi“ed into three types which are active standby and junior The active is the master server which provides metadata service for cluster le system During running only one active is allowed in the primary-backup cluster while others are in standby or junior states The standby keeps an up-to-date namespace with the active and takes over as it when necessary The synchronization protocol contributes to metadata consistency between the active and standby The junior is in an intermediate status which has a large gap in namespace state comparing to the active It may be a server which restarts after a short time or is a newly added backup node The junior needs to learn and download metadata information from the active for synchronization It will become standby through the renewing protocol see Section III C at appropriate time Under different conditions the state of server will be switched to each other for fault tolerance Fig 3 depicts the transition among three states Metadata synchronization and junior renewing are based on the shared storage pool The solid lines depict server state transition with the failover protocol Under failure free cases there are one active and multiple standbys in the primary-backup cluster OAMS monitors the cluster and judges the server failed when not receiving heartbeats from it after a timeout 
1290 
1290 


period If the active crashes OAMS starts the process of active election With the Paxos protocol one is selected as the new active among multiple standbys When reaching a consensus the candidate active enters a stage of upgrading It rstly stops receiving new metadata modi“cations and waits for current operations to be nished The candidate active also visits the global view and checks its state If the standby is in a junior state it must stop upgrading and needs the protocol to reselect active After committing latest logs in memory the candidate active updates server states in the global view It changes the state of old active to standby and set itself to active The modi“cations of the global view will trigger events which inform others to process state transition To avoid missing operations the candidate active then ushes last batch of metadata modi“cations to others with synchronization protocol As the old active may become standby and receive these logs again it needs to distinguish duplicated operations OAMS assigns a monotone increasing serial number for each batch of metadata operations The standby will decide whether to commit logs by comparing values of serial numbers Only if the value from the candidate active is larger than current serial number the standby applies logs to its own memory After the candidate active synchronizes metadata modi“cations to standbys it receives register information from all servers in the primary-backup cluster When more than one standby is registered successfully the candidate active begins receiving new client requests and providing metadata service During the process of state transition the candidate active will stop upgrading if any failures occur It will launch the reelection process with failover protocol at this time Due to block reports from datanodes the new active keeps the same up-to-date le locations as namespace state It can take over the metadata service and ensure a hot standby After active-standby switching the client reconnects to the new active and resends metadata requests to it As the process is completely transparent to upper applications the cluster le system seems to work well when failures occur Once the active is detected failures during running it will be degraded to standby and not provide metadata service If there are fatal errors such as disk faults the active will be directly switched to junior state Or the namespace state of the standby is not up-to-date comparing to the active it is degraded to junior The junior is in an intermediate status which cannot provide backup service As the active does not synchronize logs to the junior there is a large gap in namespace states between them With the reduction of standbys the cluster le system will turn into an unreliable status To avoid this risk OAMS adopts the renewing protocol to switch the junior to standby It also supports dynamical adding standbys at runtime which improves the 
C Renewing Protocol 
034\024\007\003\004 011\012\013 023\022\017\011\013\007   002\003\004\005\006\007 010\004\011\012\013\014\015  007\022\017\011\013\007  007\022\017\011\013\007 010\016\011\017\007\013 020\004\021\017\011\022\007 023\021\021\024 010\015\012\003\016\017\021\012\005&\007 017\007\023\024\005\003\011\004\007 024\021\022\020 007\022\017\011\013\007 035\007\012\007 023\017\011\013\007  030\024\005\007\012\004 017\007-#\007\020\004\020  025\011\026\021\020 024\021\014\011\024 006\005\007 
Figure 3 Server state transition in failover protocol availability of metadata service When the active or standby is degraded to junior state it enters the state of renewing The active decides the renewing strategy according to the value of serial number from the junior If the subtraction between them is large the protocol rstly launches a process of image recovery Otherwise the active ushes missing metadata modi“cations to the junior As the namespace image is stored in the shared storage pool the junior can obtain it directly from the pool and reduce the cost of downloading In the course of reading image the junior applies metadata operations in memory and reaches a consistent state with the active gradually After loading namespace image the protocol starts the procedure of synchronization At this moment the active commits current metadata operations and then stops metadata service temporarily It ushes missing logs to the junior and waits for it to reach the same state Once the junior recovers all metadata operations the active modi“es the global view and changes its state to standby Then the junior is upgraded and keeps an up-to-date namespace state with the active for a hot standby With this protocol the server can be newly added dynamically and is switched to standby nally It contributes to tolerate multiple points of failures in some scenarios such as the standby crashes A server which acts as the junior when starting up will register in the active The active veri“es its state and upgrades it to standby with the renewing protocol It also watches the global view periodically to check server states in the primary-backup cluster When the server is degraded to junior the active launches the process of renewing As OAMS uses the cluster mode for fault tolerance there may be multiple juniors in some cases The active compares values of serial numbers and selects one whose namespace state differs less with it At the same time there is only one junior which is being renewed If there are failures occur during the process the junior will give up and waits for appropriate time to upgrade To parallel execution the active 
1291 
1291 


launches the separate thread for junior renewing Based on the shared storage pool most of metadata operations can be recovered from the image and it has little effects on metadata service of the active With upper protocols OAMS achieves server state transition among three types under different conditions It ensures that there always exists one metadata server which is in the active state and provides metadata service whenever failures occur IV PERFORMANCE EVALUATION To provide a proof of concept we evaluated a prototype implementation of OAMS As HDFS is a typical distributed le system which meets the needs of big data storage and supports practical applications such as Hadoop we implements the policy based on it For failure detection and active election ZooKeeper is used to maintain small amounts of coordination data trigger of event changes in the global view and monitor metadata servers During evaluation we compare OMAS with three paradigms BackupNode Hadoop HA with NFS and Hadoop HA using the quorum journal manager QJM They are all high available solutions for HDFS BackupNode is a warm standby with the manual mode while the latter two can achieve an automatic hot standby All of them use one backup server to provide failover for the active in the case of failures Experiments are running on a cluster with ve machines Each node consists of four Intel Xeon X3320 processors 8Gbyte memory and 1-Tbyte Seagate disk All have Linux kernel 2.6.32 installed on them and are connected with gigabit Ethernet The con“guration of OAMS is one active and two standbys They also constitute the shared storage pool as storage nodes BackupNode and Hadoop HA with NFS are deployed with one active and one standby NFS is as a remote ler and mounted on each of the namenode machines For Hadoop HA with QJM the primary-backup con“guration is the same as above two while using additional three journalnode machines for sharing logs Five nodes act as datanodes in all paradigms Except for BackupNode they reports block locations to both the active and standbys for hot standby The evaluation rstly tests failover time with different paradigms To verify automatic recovery failures are caused on the cluster le system to simulate different kinds of outage such as killing the process powering cycle the machine and unplugging the network interface Time is measured for recovering metadata service in the case of failures The second set of test measures the performance comparison among OAMS and traditional primary-backup paradigms For a more detailed explanation we write special programs to request various metadata operations such as create get“leinfo delete and etc with one or more clients The evaluation is performed under both failure and failure free cases Finally an overall measurement is done to prove the high improvement on reliability and little effects on performance with OAMS in standard applications This experiment measures the ability of fault tolerance with different primary-backup paradigms in the case of failures We compute average failover time on servers for ten times when the active crashes After triggering the outage the standby takes over as the active and continues to provide metadata service As BackupNode is a warm standby with the manual mode its results do not include the time spend on switching The amount of time requires to detect a failure in ZooKeeper is heartbeat interval and session timeout with the values of 2 seconds and 5 seconds respectively Table II presents failover time comparing OAMS with traditional primary-backup paradigms The size of image le indicates the amount of metadata operations in namespace which varies from 16MB to 1GB With the growth of it the failover time of BackupNode is increased It is because BackNode does not involve modi“cations of the knowledge about le block locations and needs to reconstruct this information For OAMS and Hadoop HA there is a fast recovery as datannodes talk to both the active and standby The failover time depends on two issues One is the time it takes for the paradigm to detect a failure which is directly related to the heartbeat interval for monitor The other is the cost for switching which includes active election log synchronization and state transition Due to different approaches for sharing logs the failover time of Hadoop HA with NFS is longer than that with QJM Comparing with Hadoop HA OAMS spends less time on recovery The combination of synchronization protocol with the builtin shared storage pool and maintaining of global view in OAMS has an effect in this case It demonstrates that OAMS can achieve a faster recovery than others To compare performance overhead the evaluation is performed under both failure and failure free cases Test programs are written to produce continuous load for metadata operations The results of HDFS represent the baseline and does not involve in any failures Outage is triggered one time to make the active crash for failure test Fig 4 presents the average performance of different paradigms in failure free cases The evaluation varies the number of clients from 1 to 16 on multiple nodes to perform mixed metadata operations including create get“leinfo and etc Each client nishes 100 kilo operations for computing the average value With the increase of clients the performance is improved as the concurrent processing ability of metadata server Comparing to HDFS performance of primary-backup paradigms is respectively worse than the former This is due to the overhead of synchronizing logs between the active and standby BackupNode shows better 
A Failover Time B Performance Overhead 
1292 
1292 


6 6.5 7 7.5 8 8.5 9 9.5 10 
HDFS BackupNode OAMS HA with QJM HA with NFS Figure 4 Performance overhead under failure free cases 
  0 1 2 3 4 5 6 7   0 1 2 3 
Average Mixed Operations Per Second x10 3 Number of Clients 
0 50 100 150 200 250 300 350 
Table II FAILOVER TIME WITH DIFFERENT PRIMARY-BACKUP PARADIGMS Figure 5 Performance overhead under failure cases in the case of failures As BackupNode and Hadoop HA rely on one standby for fault tolerance outage is triggered by making the active crash for one time To OAMS the test includes three types 1 kill the active when there are one active and two standbys 1A2S 2 kill one standby under upon conditions to generate one junior 1A1S1J 3 rstly kill one active and then kill one standby which leads to two junior appear 1A2J Fig 6 presents the execution time ratio of OAMS and other paradigms comparing to HDFS in different scenarios HDFS represents the baseline which is running in failure free cases The negative value of ratios means that it needs more time to nish jobs for primary-backup paradigms Log synchronization and active-standby switching constitute the reason For OAMS the metadata service is recovered and all jobs are completed nally under three tests It proves that OAMS can tolerate multiple points of failures The performance of OAMS with 1A1S1J differences little with that of 1A2S This is because OAMS can provide metadata service as long as there are one active and one standby working And the renewing protocol for switching from junior to standby is mostly performed in the background Evaluation shows that it only costs 0.5 second to reach an up-to-date namespace state with the junior for 100 kilo operations OAMS with 1A2J is in a state of read only and cannot respond to client requests which is intelligible for the availability of cluster le system It is worthy for this cost 
16 0.762 2.586 8.395 5.375 32 1.135 5.372 10.931 8.384 64 2.416 9.541 11.834 7.533 128 1.539 21.938 9.673 6.372 256 2.394 38.346 10.817 8.081 512 2.725 80.437 9.653 7.845 1024 2.084 152.634 10.372 7.193 values than others but it cannot meet the goal for hot standby and system reliability is lower For OAMS it shows an average decrease by 7.18 in terms of performance with regard to HDFS The synchronization protocol based on the shared storage pool reduces the latency of log replication comparing to Hadoop HA Fig 5 shows the time spending on metadata operations in the case of failures For each type a total of 1 million operations are performed Failures are caused one time by killing the active process during running except for HDFS The failover time of BackupNode includes block reports and manual switching which are 8 seconds and 3 seconds respectively As the primary directly ushes logs to the standby it is intelligible that BackupNode needs less time for nishing operations than others Comparing with Hadoop HA OAMS achieves better performance with the pro“ts varying from 2.15 to 42.68 It proves the advantage of synchronization protocol and failover protocol in this policy                                                                                                                            
HDFS OAMS BackupNode HA with NFS HA with QJM 
003\017\007\011\004\007 022\007\0048\005\024\007\005\0128\021 013\007\024\007\004\007 032$\013\005\017 017\007\012\011\032\007 Time Spent for Operations \(s Metadata Operation 
As OAMS is presented to improve the reliability of metadata service for big data storage we launch standard applications to measure the overall performance Evaluation is performed in the Hadoop platform which uses HDFS or primary-backup paradigms as underlying storage Various MapReduce jobs are submitted to compare the performance        
Size of Image File MB Failover Time s OAMS BackupNode HA with NFS HA with QJM 
C Overall Measurement 
1293 
1293 


14.0 12.0 10.0 8.0 6.0 4.0 2.0 0.0 
021\017\013\003\021#\012\004 020\021\017\004 022\017\007\023 023\005 017\011\012\013\021\032 017\005\004\007\017 023\007\012\004\021\032\005\012\021 Execution Time Ratio of Primary-backup Paradigms Comparing to HDFS Mapreduce Jobs Running on OAMS and Others 
Figure 6 Overall measurement V CONCLUSION AND FUTURE WORK In this paper a highly metadata service OAMS was proposed for big data storage Different from traditional primary-backup paradigms OAMS depends on more than one standby to take over the active in cluster le system It is based on the built-in shared storage pool and employs a series of protocols to tolerate multiple points of failures OAMS achieves an automatic state transition in the form of hot standby Besides it supports server self-recovery and dynamical addition for standbys at runtime Measurements show that OAMS can obviously improve the system reliability while keeping the performance with little degradation In the future we intend to optimize and expand the policy such as supporting failover in the cluster le system with multiple metadata servers A CKNOWLEDGMENT This work is supported by the National High-Tech Research and Development Program of China under grant numbered 2011AA01A203 2013AA013204 also supported by the National HeGaoJi Key Project under grant numbered 2013ZX01039-002-001-001 and Strategic Priority Research Program of the Chinese Academy of Sciences under grant numbered XDA06030200 R EFERENCES  J Dean and S Ghema w at Mapreduce Simpli“ed data processing on large clusters in 
BackupNode HA with NFS HA with QJM OAMS with 1A2S OAMS with 1A1S1J OAMS with 1A2J 
Sixth Symposium on Operating System Design and Implementation OSDI 04 Seventh Symposium on Operating System Design and Implementation OSDI 06 19th Symposium on Operating Systems Principles SOSP 03 26th IEEE Transactions on Computing Symposium on Mass Storage Systems and Technologies Mass Storage Systems and Technologies in Cooperation with the 17th IEEE Symposium on Mass Storage Systems ACM Computing Surveys Proc of the 2011 ACM SIGMOD International Conference on Management of data Proc of the Summer USENIX conference 13th Symposium on Operating Systems Principles Research report Systems Research Center Digital Equipment Corporation Markov Chains Principles of Distributed Database Systems 
 San Francisco USA Dec 2004 pp 137…150  F  Chang J Dean S Ghema w at W  C Hsieh D A Wallach M Burrows T Chandra A Fikes and R E Gruber Bigtable A distributed storage system for structured data in  Seattle USA Nov 2006 pp 205…218  S Ghema w at H Gobiof f and S T  Leung The Google le system in  New York USA Oct 2003 pp 29…43  K Shv achk o H K uang S Radia and R Chansler  The Hadoop distributed le system in  Incline Village USA May 2010 pp 1…10  A Barry  J Brasso w  R Cattelan A Manthei E Nygaard S V Oort D Teigland M Tilstra M OKeefe G Erickson and M Agarwal Implementing journaling in a Linux shared disk le system in  Maryland USA Mar 2000 pp 351…378  T  Haerder and A Reuter  Principles of transaction-oriented database recovery  vol 15 no 4 pp 287…317 1983  F  Haas P  Reisner  and L Ellenber g The DRBD user s guide LINBIT Information Technologies GmbH 2009  D Borthakur  J S Sarma J Gray  K Muthukkaruppan N Spiegelberg H Kuang K Ranganathan D Molkov A Menon S Rash R Schmidt and A Aiyer Apache Hadoop goes realtime at Facebook in  Athens Greece Jun 2011 pp 1071…1080  R Sandber g D Goldber g S Kleiman D W alsh and B Lyon Design and implementation of the Sun network lesystem in  Portland USA Jun 1985 pp 119…130  Cloudera homepage Online A v ailable http://www.cloudera.com  Apache hadoop Online A v ailable http://hadoop.apache.or g  Apache BookK eeper  Online A v ailable http://zookeeper.apache.org/bookkeeper  W  Lin M Y ang L Zhang and L Zhou P aci“ca Replication in log-based distributed storage systems Technical Report MSR-TR-2008-25 Microsoft Research Tech Rep 2008  B Lisk o v  S Ghema w at R Gruber  P  Johnson L Shrira and M Williams Replication in the Harp le system in  California USA Oct 1991 pp 226…238  G Sw art A Birrell A Hisgen and T  Mann  A v ailability in the Echo le system in  Citeseer 1993  J R Norris  Cambridge University Press 1998  L Lamport The part-time parliament  Research Report 49 Systems Research Center Digital Equipment Corporation Tech Rep 1989  M T   Ozsu and P Valduriez  Springer 2011  Apache ZooK eeper  Online A v ailable http://zookeeper.apache.org 
since OAMS has highly improved the reliability of metadata service while having little effects on performance        
1294 
1294 


607 


608 


  11 that it will be able to meet all of the Van Allen Probes communications goals with its intended ground segments A CKNOWLEDGEMENTS  This work was performed with the support of the Radiation Belt Storm Probes mission under NASA\222s Living with a Star program. The authors would like to thank Rick Fitzgerald and Kim Cooper, Van Allen Probes project managers at JHU/APL for supporting this work. There are many at JHU/APL who contributed to the development and verification of the RF system. Significant technical contributions were made by: Christopher Haskins, Bob Wallis, Matthew Angert, Laurel Funk, Joe Sheehi, Wesley Millard, Norman Adams, Lloyd Ellis, Sheng Cheng, John Daniels, Phillip Huang, Avi Sharma, Carl Herrmann, David Jones, Brian Bubnash, Melanie Bell, Horace Malcom Michael Pavlick, Mark Bernacik, Christopher Deboy, Bob Bokulic, Sharon Ling, Albert Hong, Erik Hohlfeld, Judy Bitman, William Dove and Tony Garcia. Significant contributions were also made by the USN and TDRSS compatibility test teams  R EFERENCES  1 eck D. G.; Mau k  B  H.; Greb o w sk y  J  M.; Fo x  N J, \223The Living With a Star Radiation Belt Storm Probes Mission and Related Missions of Opportunity 224 American Geophysical Union, Fall Meeting 2006   h o rs k i y  A Y., Mauk B. H., Fox N. J Sibeck D G., Grebowsky, J. M., \223Radiation belt storm probes Resolving fundamental physics with practical consequences,\224 Journal of Atmospheric and SolarTerrestrial Physics Vol. 73, Issues 11-12, July 2011 Pages 1417-1424   S. Bu s h m a n M. Bu tler, R C o n d e, K. Fretz, C  Herrmann, A. Hill, R. Maurer, R. Nichols, G. Ottman M. Reid, G. Rogers, D. Srinivasan, J. Troll, B. Williams 223Radiation Belt Storm Probe Spacecraft and Impact of Environment on Spacecraft Design,\224 Proceedings of the 2012 IEEE Aerospace Conference, Big Sky Montana USA, March 3-10, 2012   opelan d D.J DeB o y C  C R o y s ter, D.W., Dov e  W.C., Srinivasan, D.K,. Bruzzi, J.R., Garcia, A., "The APL 18.3m station upgrade and its application to lunar missions," Aerospace Conference, 2010 IEEE , vol., no pp.1-10, 6-13 March 2010    Figure 10. FER/BER performance for all downlink modes for RF GSE, SCF, USN, and TDRSS 


  12  iv as a n D. K., A r ti s  D  A Bak er, R  B., Stil w e ll, R   K., Wallis, R. E., \223RF Communications Subsystem for the Radiation Belt Storm Probes,\224  Acta Astronautica vol 65, issue 11-12, December 2009, Pages 1639-1649   k i n s  C B., Mi llard, W P 223 M u l t i Ban d  So f t w a re Defined Radio for Spaceborne Communications Navigation, Radio Science, and Sensors,\224 2010 IEEE Aerospace Conference, March 2010  k i n s  C B., Mi llard, W P A d a m s  N. H Sri n i v a s a n  D. K., Angert, M. P., \223The Frontier Software-Defined Radio: Mission-Enabling, Multi-Band, Low-Power Performance,\224 61st  International Astronautical Congress IAC-10.B2.5.11, October 2011 8  Crowne, M.J.,  Haskins, C. B., Wallis, R. E.,  Royster D.W, \223Demonstrating TRL-6 on the JHU/APL Frontier Radio for the Radiation Belt Storm Probe mission,\224 2011 IEEE Aerospace Conference, March 2011  o ckw ood, M. K K i n n i s o n  J., F o x  N C o n d e, R  Driesman, A., \223Solar Probe Plus Mission Definition,\224 63rd  International Astronautical Congress, IAC 12.A3.5.2, October 2012   i t m a n J  223An I n D ept h  L o o k at t h e R a dio Freq u e n c y    Ground Support Equipment for the Radiation Belt Storm  Probes Mission,\223 IEEE Autotestcon, 2011, September 2011  d a m s  N.H., Bi t m a n J C opela n d D. J Sri n ivas a n  D  K.,  Garcia. A., \223RF Interference at Ground Stations Located in Populated Areas,\224 2013 IEEE Aerospace Conference, March 2013  B IOGRAPHY  Matthew J. Crowne is a member of the Senior Professional Staff of the RF Engineering group in JHU/APL\222s Space Department. He received his B.S from Johns Hopkins University in 2000 and his M.S. from the same university in 2009, both in electrical engineering Matthew joined JHU/APL in 2007 where he has been working on the development of radios for spaceflight communications systems. Prior to joining JHU/APL, he worked for Integrated Defense Systems Inc., where he developed solid state power amplifiers for electronic warfare and communication systems. Matthew was the integration and test lead for the Van Allen Probes RF communication system and is currently working on the Solar Probe Plus mission   Dipak K. Srinivasan is the supervisor of the RF Systems Engineering Section in the JHU/APL Space Department. He received his B.S. and M.Eng. in electrical engineering in 1999 and 2000 in electrical engineering from Cornell University, and an M.S. in applied physics from The Johns Hopkins University in 2003. Dipak joined the APL Space Department in 2000, where he has served as the lead RF Integration and Test Engineer for the CONTOUR and MESSENGER spacecraft and lead mission system verification engineer for the New Horizons project. He is currently the Lead RF Telecommunications Systems Engineer for the MESSENGER and Van Allen Probes missions and chairs technical sessions at the annual International Astronautical Congress  Darryl W. Royster is a member of the Senior Professional Staff in the RF Engineering Group at JHU/APL.  He led compatibility testing for the Van Allen Probes, STEREO, and MESSENGER missions.  Previously he was the System Engineer for the Satellite Communications Facility at JHU/ APL and the lead RF Integration and Test Engineer for the STEREO spacecraft.  Prior to joining the JHI/APL Space Department in 2001, Mr. Royster designed cellular and land mobile radio products for Ericsson, GE and Motorola.  He received his B.S. and M.S. in electrical engineering from Virginia Polytechnic Institute and State University in 1982 and 1984, respectively  Gregory L. Weaver joined the Senior Professional Staff of JHU/APL in 2003 and works within the RF Engineering Group of the Space Department.  He is a technologist with extensive background in the technical and business aspects of the frequency control industry and has held positions as a senior design engineer, technical manager and marketing strategist over a 25 year career history, including vice president positions with Bliley Technologies Inc. and the former Piezo Crystal Company. He received his M.S in Technology Management from the University of Pennsylvania in 1993 and his B.S. in Physics from Dickinson College in 1982.  He is a licensed professional engineer in the state of Pennsylvania, member of the IEEE and the UFFC Societ y.  He has contributed to the technical proceedings of the IEEE International Frequency Control Symposium, Precise Time and Time Interval Systems and Application Meeting and the European Frequency and Time Forum   


  13 Daniel Matlin is an Associate Professional Staff at JHU/APL and a member of the RF engineering group in the Space department.  He went through a dual Bachelors/Masters program at Johns Hopkins University graduating with his Bachelor of Science in Electrical Engineering in 2008 and his Masters of Science in Engineering from the Electrical Engineering department in 2009.  As a student he specialized in RF systems design.  Mr. Matlin started at the JHU/APL in February of 2010 and in his short time with the lab has been privileged to work on various tasks supporting the RBSP program, including supporting a successful launch and early operations.  Mr. Matlin assisted in the qualification testing for the flight DSP slices as well as the integrated flight transceivers.  He also carried out electrical testing and flight qualification of the newly designed Hypertronics stacking connectors as well as components and cables used for the RF subsystem  Nelli Mosavi is an EMC and RF Engineer in the JHU/APL Space Department, RF Systems Engineering section. She received a B.S. degree in Electrical Engineering from Oakland University Michigan in 2004 and an M.S. in Electrical Engineering from The Johns Hopkins University in 2010. She is currently working toward her Ph.D. at the University of Maryland Baltimore County. She joined APL in 2009 and has since been working on RF and EME issues on the Van Allen Probes mission. Nelli previously worked for SENTEL Corporation, General Motors, DENSO International, and Molex Automotive   


APPENDIX 3– RESULTS \(SEM I-PROFESSIONAL DSLRS     Run by TFDEA add-in ver 2.1 Frontier Type Orientation 2nd Goal Return to Scale Avg RoC Frontier Year MAD Dynamic OO Max CRS 1.124802 2008 1.394531 Input\(s Output\(s SOA products at Release SOA products on Frontier RoC contributors Release before forecast Release after forecast 22166527 DMU Name Date Efficiency_R Efficiency_F Effective Date Rate of Change Forecasted Date 1 Nikon D100 2002 1 1.66666667 2007.000000 1.107566 2 Olympus E1 2003 1 1.666666667 2007.000000 1.136219 3 Pentax *ist D 2003 1 1.358024691 2007.000000 1.079511 4 Nikon D20 0 2005 1 1.2 2007.000000 1.095445 5 Canon EOS 5D 2005 1 1.664796311 2007.730028 1.205269 6 Pentax K10D 2006 1 1 2006.000000  7Nikon D30 0 2007 1 1 2007.000000  8 Olympus E3 2007 1 1 2007.000000  9 Sony Alpha DSLR A70 0 2007 1 1 2007.000000  1 0 Nikon D70 0 2008 1.46 1.46 2007.000000  11 Canon EOS 5D Mark II 2008 1.065464119 1.065464119 2008.000000  12 Sony Alpha DSLR A90 0 2008 1 1 2008.000000  13 Olympus E3 0 2008 1.02 1.02 2007.000000  1 4 Pentax K20D 2008 1 1 2008.000000  15 Nikon D300s 2009 1.142857143 0.874450785 2007.000000  2008.140742 16 Canon EOS 7D 2009 1 0.754166667 2007.000000  2009.399022 17 Sony Alpha DSLR A85 0 2009 1 0.774820627 2008.000000  2010.169290 18 Pentax K-7 2009 1 0.772738276 2006.503130  2008.695302 19 Olympus E5 201 0 1.466133763 1.173333333 2007.000000   2 0 Pentax K-5 201 0 1.009024674 0.776190476 2007.000000  2009.15427 0 21 Nikon D80 0 2012 1 0.686950618 2008.000000  2011.192776 22 Canon EOS 5D Mark III 2012 1.115010291 0.930769231 2007.502755  2008.112786 23 Pentax K-5 II 2012 1 0.632075669 2006.839705  2010.740375 24 Sony Alpha SLT A99 2012 1.009662059 0.854117647 2007.640496  2008.981286 Results 2129 2013 Proceedings of PICMET '13: Technology Management for Emerging Technologies 


