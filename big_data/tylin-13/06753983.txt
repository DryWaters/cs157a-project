According to the wide development of the World Wide Web world many peoples have tend to mention big data and tried to make use of the big data They usually mention big data as a key to the happy future However it will be a dream which will not be realized In this paper 
Akinori Abe Faculty of Letters Chiba University 1-33 Yayoi-cho Inage-ku Chiba 263-8522 Japan Email ave@ultimaVI.arc.net.my ave@L.chiba-u.ac.jp 
Abstract 
Curating and mining big data 
I review problems in big data Then I propose a concept of curation to deal with such big and complex data 
I Introduction 
According to the wide development of the World Wide Web world many peoples have tend to mention big data and tried to make use of the big data They usually mention big data as a key to the happy future Thus in Japan usually magazines or books show very positive aspects of big data Of course some magazines such as 
W eekly T o yo K eizai 2013.4.20 also pointed out the negative aspect of big data That is big data usually contain harmful data noise In addition if data become too big it becomes rather difìcult to comprehend whole data If we simply analyze the data we will only be able to obtain poor results which are useless or harmful In Silv er sho ws both aspects of big data and illustrates actual results from both aspects Especially he claims the importance of distinguishing a true signal from a universe of noisy ever-increasing data For instance for 
the forecasting the electoral politics he points out Foxlike attitudes may be especially important when it comes to making predictions about politics Foxês strategy is to believe in a plethora of little ideas and in taking a multitude of approaches toward a problem They tend to be more tolerant of nuance uncertainty complexity and dissenting opinion On the other hand hedgehogs who have a lot of information construct stories  stories that are neater and tidier than the real world When we construct these 
stories we can lose the ability to think about the evidence critically From the viewpoint of Silver for the larger information it is necessary to divide it to small pieces and consider nuance uncertainty complexity and dissenting situation of the information For big data Fung pointed out  
numbersense 
begins with looking at the data Keep your hands off though until you have dug out excruciating details of how the data is collected ra w  untampered-with data almost 
never yields the answer Some form of adjustments  whether itês seasonality or bias correction or others are like the dressing on top of a salad The most important point is that to understand how the data is collected That is if we do not consider the origin or source of data we sometimes analyze the data wrongly Thus a strategy to review the feature of data is very important In fact the similar problems were discussed before the big data age In this paper I introduce a concept 
of curation to deal with big data First I illustrate the medical data as complex data Then I show the importance of interface between users and data For that curation is adopted to achieve the better interface system In addition I illustrate how curation functions in the context of the data market proposed by Ohsawa 
II From small data to big data A Data collection 
We proposed a framework called Cyber Integrated Medical Infrastructure CIMI which is a framework of integrated management of clinical data on computer net 
works consisting of a database a knowledge base and an inference and learning component which are connected to each other in the network In this frame w ork medical information e.g clinical data is analyzed or data mined to build a knowledge base for predicting all possible diseases and to support medical diagnosis As shown below more than 130 items are included in the medical data of each person Item sets in the data are for instance 
2013 IEEE 13th International Conference on Data Mining Workshops 978-0-7695-5109-8/13 $31.00 © 2013 IEEE DOI 10.1109/ICDMW.2013.51 664 
2013 IEEE 13th International Conference on Data Mining Workshops 978-0-7695-5109-8/13 $31.00 © 2013 IEEE DOI 10.1109/ICDMW.2013.51 664 


Fig 1 Distribution of age Clinical data 
002 003 003 004 
 and modiìed by Matsuoka the experiment In addition to the above problem we still have more serious problem 
Tumor stage 
B Data treatment C Data visualization 
2-microglobulin serum SCC antigen seminoprotein TPA TK activity NSE Ferritin BFP serum Total protein albumin GTP ZTT serum protein fraction1-globulin ALP Triglyceride LDL Cholesterol HDL Cholesterol urine sugar HbA1c White blood cell count Platelet Cl Cell immunity T Cell number and ACP Usually such type of data are collected during medial treatments However such collected data usually lack several items Because physicians tend to omit unnecessary or expensive data For instance if they do not think a patient has any problem in lung they do not ask to collect data regarding lung Accordingly the collected data do not include such data In Abe04a we tried to supplement lacking data by adding newly and computationally generated data However such data will not be the same as actual data For the treatment we assumed that the data distribution would follow the standard distribution Accordingly the generated data will follow our estimated pattern In addition it will be difìcult to collect large size of data only during medical treatment Therefore we decided to collected the complete clinical data as an experiment In the project we have intently collected more than 2200 medical data sets From certain persons the data were collected more than once Actually we could collect complete clinical data but the distribution of age is shown in Fig 1 The ratio of persons in their 50ês is very high which might become the reason for the different distribution of health levels 1  Currently we collect data from ofìce workers aged 40 to 50 years old but not from students or younger persons Thus the age distribution pattern is not similar to the Japanese standard age distribution pattern This is because we did not collect samples from persons belonging to all types of person but only from those who applied to take part in our experiments Most of our subjects are old ofìce workers in rather high position and live in or close to Tokyo Therefore our results might not reîect social tendencies Actually we can collect large size of clinical data by 1 Health levels that express the health status of patients are deìned based on We have generated the health level determination model by applying C4.5 to the collected data F or the accuracy ratio of the health level determination with the model generated by around 2000 samples 44.3  of health level can be correctly determined In fact for health level 3 it is rather difìcult to correctly determine the health level Since the data distribution is not balanced as well as does not follow standard data distribution we do not have sufìcient number of example in health level 3 to build a proper model Even if we can collect a large sized data set if it is heterogeneous it is still difìcult to conduct a proper data analysis In addition the structure of the data is complex it is rather difìcult to conduct a proper data analysis In fact our data set includes many items more than 130 Accordingly it is rather difìcult to deal with the data 2  Because in the data set data from various type of client are included For instance those who have Diabetes Mellitus and those who have Helicobacter pylori are included For each case the feature of medical data should be different Accordingly in order to explain various situations the generated decision tree becomes very complex that is it has many leaves During medical treatment physicians usually focus on necessary data for their diagnosis However computers cannot perform such a selective analysis They can deal with all data equally Accordingly analyzed data sometimes represent the different aspect of the data Therefore it will be necessary to provide a strategy to achieve a selective analysis or help such analysis In addition it is necessary to discover relationships of data which can suggest the removal of unnecessary links and can aid the recovery or supplement of missing rules or links that lead to the determination of the correct health levels Several relationships that could improve the accuracy in determining health levels could be discovered For the better analysis of data intuitively introduction of data visualization can be considered In and 8 I introduced a system called an Interface for medical diagnosis support During the procedure shown in Fig 2 the user can review a certain patient whose health status is estimated as 5a 3  In the right window of the right browser the user 2 It is sometimes called as the curse of dimensionality 3 In I described health status as health le v el F or the deìnition of health level please refer to etc 
665 
665 


A General curation 
III Curation 
Fig 2 Interactive interface for medical diagnosis support 
002 002 002 002 002 002 002 002 002 002 002 002 
can check the selected data to conìrm if the patient is in the health status of 5a Similarly the user can view all the estimated health status at a glance and check doubtful or uncertain patientês data Thus the proposed system offers a exible interface to conduct medical diagnosis and data compilation Beside the most important point of the proposed system is that all windows interfaces are automatically generated according to the data set and the analyzed results By using the interactive interface the user can discover hidden or potential and important factor of patient From this viewpoint we can say this application is chance discovery application In addition we can say it can offer curatorial situation Detailed discussions will appear in the following sections This section reviews various types of curation Actually curatorial task is usually used for tasks in art museum Many cases introduced in this section are curatorial works in gallery and art museum In addition a new type of curation digital data curation is also reviewed There is at least a person who is responsible as curator in special exhibitions galleries archive or art museums Their main task of curator is a curatorial task which is multifaceted Curator comes from a Latin word cura which means cure Then originally it used for a person who take care of a cultural heritage In the report by American Association of Museums Curators Committee AAMCC the y pointed out curators are highly knowledgeable experienced or educated in a discipline relevant to the museumês purpose or mission Curatorial roles and responsibilities vary widely within the museum community and within the museum itself and may also be fulìlled by staff members with other titles Then they showed the deìnition of curator as follows Remain current in the scholarly developments within their eld\(s conduct original research and develop new scholarship that contributes to the advancement of the body of knowledge within their eld\(s and within the museum profession as a whole Make recommendations for acquiring and deaccessioning objects in the museum collection Assume responsibility for the overall care and development of the collection which may include artifacts ne art specimens historic structures and intellectual property Advocate for and participate in the formulation of institutional policies and procedures for the care of the collection that are based on accepted professional standards and best practices as deìned by AAM CurCom and other relevant professional organizations Perform research to identify materials in the collection and to document their history Interpret the objects belonging or loaned to the museum Develop and organize exhibitions Contribute to programs and educational materials Advocate and provide for public use of the collection Develop or contribute to monographs essays research papers and other products of original thought Represent their institution in the media at public gatherings and at professional conferences and seminars Remain current on all state national and international laws as they pertain to objects in the museum collection In addition AAMCC showed curatorial responsibilities as follows A Research Scholarship and Integrity B Interpretation C Acquisition Care and Disposal D Collection Access and Use E Replication of Objects in the Collection Thus curators have responsibilities for various aspects of exhibition activities However the most important activity will be a plan of exhibition For that the above activities such as research interpretation and acquisition are necessary They should properly exhibit a truth which is result of their researches and interpretations 
666 
666 


B e-Science Data Curation C Curation in business and information market 
002 002 002 002 002 002 002 002 002 002 002 002 
Curation 
The above curation is for actual museums That is curation is conducted mainly for actual works However curation in this section is for digital data There are several differences between digital curation and analogue curation JISC pointed out an importance of curation as promoting good curation and an information infrastructure to capitalise upon and preserve expensively gathered data means bringing together varied technical and managerial resources and managing these over time This activity needs to be supported by clear strategies for resourcing and support  They compare curation with archiving and preservation Curation The activity of managing and promoting the use of data from its point of creation to ensure it is t for contemporary purpose and available for discovery and re-use For dynamic datasets this may mean continuous enrichment or updating to keep it t for purpose Archiving A curation activity which ensures that data is properly selected stored can be accessed and that its logical and physical integrity is maintained over time including security and authenticity Preservation An archiving activity in which speciìc items of data are maintained over time so that they can still be accessed and understood through successive change and obsolescence of technologies That is they pointed out that curation is more creative task Then they showed aspects of curation as follows Trust Trust can be enhanced by the existence of qualiìed domain specialists who curate the data Utility Certain information about the data  where it came from how it was generated for example is necessary to enable future users to gauge the utility and reliability of the data and indeed any annotation of the data Data utility also depends on the ability of users to manage and analyse it data mining tools and algorithms visualisation tools user interfaces and portals will play a crucial role in accelerating research Discoverability How will future users nd data in particular data they do not know exists in other domains or archived according to terminology which has fallen out of use Data access is often organised through portals how will those portals be organised What tools will users need to read or use the data and who will provide these tools Access management A signiìcant proportion of data involves conìdentiality issues Ownership and rights management also need to be taken into account Heterogeneity Not only is this data revolution creating a deluge of data the data itself comes in very many different and often specialist formats some created by the researchers themselves Complexity The data can be composite in nature with links to external objects and external dependencies such as calibration information and be highly complex in structure This complexity represents a signiìcant challenge for the preservation of data They use data curation because they think data have value Not only for keeping data but also usability of data for the public they use the word curation Actually most of data are neither art works nor archaeological artifacts However is is important to view data from the aspect of what should be preserved The main difference between data and art works or archaeological artifacts is that data do not have a shape and cannot exist alone It is necessary to prepare a container such as a cdrom and a hard disc drive system Therefore for data curation Discoverability plays a signiìcant role The aim of the data curation seems to have the similar objective as data jacket In 2011 several books on curation were published In Japan we had at least two publications which I noticed by Katsumi and Sasaki 22 In addition Rosenbaum published Curation Nation in the same year  The y discuss a curation in business and information market eld As an example I review curation by Sasaki Sasaki deìned curation in as follo ws  From huge amount of information source according to the curatorês sense of value and world view picking up information giving a new meaning to it and sharing it with many persons Sasaki used a metaphor of biotope 4 to illustrate the promotion of a unknown or less known but a good artist His main point to successful promotion is to recognize Where those who need a certain information live How to offer the information to those who need the information How to make them impressed by the information 4 Biotope is an area of uniform environmental conditions providing a living place for a speciìc assemblage of plants and animals Biotope is almost synonymous with the term habitat but while the subject of a habitat is a species or a population the subject of a biotope is a biological community from Wikipedia 
667 
667 


IV Complex data and curation A Curation in chance discovery 
002 002 002 002 002 002 002 
He gave a metaphor the place where those who need a certain information live as a biotope Actually strategies he illustrated are very intentional because they are business that should be successful and that were succeeded In addition he pointed out the importance of a human as a media That is in order to generate sympathies there should be a certain context and the context will be generated aware not only by a viewpoint such as search keyword place and program but also by the specialized personês sense of value and world view His viewpoint is based on Social Network System SNS then his curation can be regarded as a generation of explicit multi-core circle type and indeìnite relationship supported by social media A curation for business in the internet age seems an interaction between customer user and goods There will not be a system to insist on trends from big companies but trends will be constructed or selected according to customers interaction on inter\orks In addition a small company or community can use this system to give rare goods a certain trend Thus the strategy of information delivery in business has changed in recent year and they call this type of information delivery as curation Curation in business means not only an information display system but also an information delivery strategy If we interpret this type of curation as interaction between users and data this type of curation should be considered during dealing with big data In I proposed and deìned a concept of curation in chance discovery 5  Though in various articles the deìnition of a chance is described which was introduced by Ohsawa I wish to introduce it here again In fact it rather differs from the original deìnition in to reîect the recent research interests A chance is rare hidden potential or novel event\(s  situation\(s that can be conceived either as a future opportunity or risk Then chance discovery research is a type of research to establish methods strategies theories and even activities to discover a chance In addition it aims at discovering human factors for chance discoveries Accordingly a deìnition of curation in chance discovery is 5 In and 10 I e xtended the deìnition of curation in chance discovery which introduced a concept of holistic communication In this paper I used the extended version of curation Curation is a task to offer users opportunities to discover chances Curation should be conducted with considering to offer implicit and potential possibilities Chances should not be explicitly displayed to users However such chances should rather easily be discovered and arranged according to the userês interests and situations This can be achieved for instance by affordance There can be a certain holistic communication environment This type of holistic communication might function as media to discover chance for novice users There should be a certain freedom for user to interpret a key person matter thing or event which should only stimulate or assist users thinking procedure There should be a certain freedom for user to arrange chances The main point of curation is how to display data to users and how not to insist on any interpretation to users But critical point to understand the data should be presented Accordingly users can rather easily deal with data and can interpret data exibly freely and properly In the previous section I illustrated the interactive interface for medical treatment Actually it was developed for the purpose of a diagnosis support for physicians In addition it was developed by the concept of chance discovery That is it designed to provide an opportunity to discover hidden or potential and important factor of patient Therefore it provide both the original data and the analyzed result The original data and the analyzed result are linked via HTML-based browsers The interface is automatically generated from clinical data At rst as a result of C4.5 analysis Fig 3 is generated This browser provides very simple information If we click the ID shown in the lest window the analyzed decision tree is shown in the right window If we click link anchors on leaves of the decision tree another second browser will be coming up that has more detailed information on health level Fig 4 In this browser we can check the standard range of each items relating to the health level For instance the range of should be less than 5.0 The standard value is obtained by referring to the data library of Mitsubishi Chemical Medience Corporation http://www.medience.co jp Only items included in the clicked line are shown 
Curation in business 
B Application as curation 
TK Activity 
668 
668 


002 002 002 
Fig 3 Interactive interface for medical diagnosis support 1st Fig 4 Interactive interface of medical data analysis Subjects list on a leaf of the decision tree is shown Fig 5 Part of a third browser 
C Curation and Data Jacket 
By clicking at the ID in the second browser the third browser is obtained as shown in Fig.5 which presents the medical data for that ID In the browser the abnormally high values can be found as described in red and abnormally low values as described in blue For instance in this case Apolipoprotein A-I is 192.0 Apolipoprotein B is 107.0 and blood sugar is 120 That means that he/she may suffer from rather serious Lipid metabolism abnormalities or Arteriosclerosis In fact with the proposed interface it is rather easy to focus on such abnormal data Thus we can review results by C4.5 in the organic view and we can check each value relating to certain health levels in detail Of course an intelligent data base can perform such compilation of data However one of the important feature is the interface links data and analyzed data In the case the analysis is performed by C4.5 In I pointed out as follo ws Even if health levels are differently recorded when the subjects health levels are properly determined it is easy to correct them The reason for this is that differently recorded health levels are displayed in red colour and therefore the link can be followed to check the original data and to conìrm if their health levels are correct or not Exceptional abnormal or normal factors and the missing rules can be discovered with an interactive interface For instance necessary rules which are not considered during health level determination can be discovered by focusing on values that are abnormal for their health levels As shown above it is easy to detect abnormal values by their colour For the same reason it is not difìcult to discover exceptions because exceptions usually contain abnormal data for their health levels Without the proposed interface it is rather difìcult to focus on necessary and meaningful data sets because more than 100 items are contained in each individual data set and more than 2500 data have been collected By using the interface it is easy to determine which data should be checked Thus this type of interface can be considered as curation Because the interface only displays user-friendly view of a computationally analyzed result and allows exible interpretations In addition as suggested above it can also be functioned as an intelligent data base Perhaps this is also a sort of data curation Accordingly one of problems in big data analysis  how to distinguish noise from data can be solved by this type of curation In Ohsa w a pointed out to characterize a data jack et as follows In these stores 6  we often nd a poster saying this shelf has only the jacket for the content 6 shopping stores of movie DVDs and music CDs 
669 
669 


V Conclusions 
002 002 002 002 
please go ahead to buy at the register This trend implies that information more abstract of less details than a certain level can be shown open to the public whereas the rest in details tends to be hidden This idea is quite useful when we consider to create a social environment where data are accessible from those who seek and where each dataset or database can be valued or priced commercially reasonably In contrast to what are called big data in the business context which are hard for small rms to collect or manage the entry of a single participant into his/her Facebook wall may be evaluated as of a priceless value if the feature of the content is sufìciently appealed via the statement on the jacket about the wall A data jacket functions as an index or an introduction of big data It will not describe all information of the data Thus users should guess the importance of the data by reviewing the description of the data jacket For the better description of data jacket it will better to introduce the concept of curation especially chance discovery-based curation For instance an example shown in the above will suggest the formalization of a description format of data jacket That is during generating a data jacket the data can be compiled to show their structure In addition each data can be linked to the structure Then the user can understand the importance and the feature of the data In addition the user can distinguish noise from data Of course the concept of data generator or collector should not represented in the description The interface should allow the userês free thinking and selection of the data In the sense data jacket will have a certain interactive feature where the user can check the data from various aspects In the above I adopt C4.5 for the data analysis The interface is general so any analysis data mining tool can be applied to data analysis The style of the interface can be designed according to the data type and the application I used the HTML-based interface I think it will be better to use a portable interface Because the user can use in various situations and OSs Thus the procedure of data analysis can become the circulation of the followings data collection data cleaning data analysis interface data jacket generation data interpretation data cleaning Then it will be possible to deal with data very exibly and use the data in an organic way We can deal with the data according to the proper perspective and even we can change our perspective to the data In this paper I reviewed curation based on chance discovery and showed an interactive interface for medical diagnosis as an application of the curation Then I introduced the concept of curation to the data jacket generation Perhaps part of problems in dealing with big data will be How to implicitly lead user to the point which should be considered How to distinguish noise from data How not to insist on a xed interpretation to the users Actually these problems are not specialized to the big data mining They are pointed out before the big data age especially in the school of chance discovery In fact I pointed out these problems and proposed the concept of curation based on chance discovery In this paper I discuss the curation under the context of the big data mining Data jacket is an important scheme for the description of data Users can guess or consider the usage of the data via the description of data jacket However the description should not insist users on a xed interpretation of data and their usage User can freely interpret and use the data and combine several data to make them better In addition user can distinguish noise from data The bigger data become the more data contain noise In this paper I showed that by the introduction of automatic generation of interactive interface from data it is possible to provide a curated situation and it can be used as an interactive data jacket during checking and utilizing the data The format of data jacket may not be the same as that proposed by Ohsawa But an interactive data jacket will be one of the solution to deal with big data By the interactive data jacket a curatorial situation can be offered to the user then he/she can use the data in the best way Since the curation is based on chance discovery the performance can become the style of chance discovery where the user can be aware of chances and make use of them In this paper I did not discuss the knowledge level of users A strategy of data treatment will depend upon userês knowledge level It will be necessary to change the curation type according to userês knowledge level In the deìnition of curation I mentioned There can be a certain holistic communication environment This type of holistic communication might function as media to discover chance for novice users Thus a holistic communication can be changed according to the userês knowledge level 
670 
670 


Technical Report of JSAI Data Science Journal Mining Complex Data PostProceedings of the ECML/PKDDê07 Third International Workshop MCD2007 Z W Ras S Tsumoto D Zighed eds LNAI4944 Communications and Discoveries from Multidisciplinary Data S Iwata Y Ohsawa S Tsumoto N Zhong Y Shi L Magnani eds Studies in Computational Intelligence International Journal of Advanced Intelligence Paradigms Proc of KES2010 LNAI6278 Proc of ICDM2010 5th International Workshop on Chance Discovery Sequence and Genome Analysis Methods and Applications Zhongming Zhao eds Proc of IJCAI2011 6th International Workshop on Chance Discovery Ohsawa Y and Abe A eds Advances in Chance Discovery Holistic Communication A code of ethics for curators Numbersense e-Sciece Data Curation Report Curation Sense Cancer Chance Discovery Innovators Marketplace on Data Jackets P C4.5 Programs for Machine Learning Curation Nation The age of curation The Sginal and The Noize Why So Many Predictions Fail-but some Donêt 
References 
 A Abe F  Naya K K ogure and N Hagita Rule Acquisition from small and heterogeneous data set  SIG-KBS-A304-32 pp 189Ö194 2004 in Japanese  A Abe N Hagita M Furutani Y  Furutani and R Matsuoka Possibility of Integrated Data Mining of Clinical Data  Vol 6 Supplement pp S104ÖS115 2007  A Abe N Hagita M Furutani Y  Furutani and R Matsuoka Data mining of Multi-categorized Data  pp 182Ö195 Springer Verlag 2008  A Abe N Hagita M Furutani Y  Furutani and R Matsuoka Categorized and Integrated Data Mining of Clinical Data in  Vol 123 pp 315Ö330 Springer Verlag 2008  A Abe N Hagita M Furutani Y  Furutani and R Matsuoka An Interface for Medical Diagnosis Support from the viewpoint of Chance Discovery  Vol 2 No 2/3 pp 283Ö302 2010  A Abe N Hagita M Furutani Y  Furutani and R Matsuoka Categorized and Integrated Data Mining of Medical Data from the Viewpoint of Chance Discovery Part III pp 307Ö314 Springer Verlag 2010  A Abe Curation in Chance Disco v ery   pp 793Ö799 2010  A Abe N Hagita M Furutani Y  Furutani and R Matsuoka An interactive interface for medical diagnosis support in  Chap 17 pp 289Ö305 iConcept Press 2011  A Abe Curation and Communication in Chance Disco v ery   pp 3Ö8 2011  A Abe Curation in Chance Disco v ery  in  SCI 423 pp 1Ö18 Springer Verlag 2012  R Akiyama and K Sugiyama  Senden Kaigi 2004 in Japanese  American Association of Museums Curators Committee  http://www.curcom.org pdf/code ethics2009 pdf 2009  K Fung  McGraw Hill Education 2013  JISC  http://www.jisc.ac.uk e-sciencecurationreport.pdf 2004  A Katsumi  Ushio Publication 2011 in Japanese  T  K obayashi and T  Ka w akubo Prospecti v e In v estigation of Tumor Markers and Risk Assessment in Early Cancer Screening  Vol 73 No 7 pp 1946Ö1953 1994  Y  Ohsa w a and P  McBurne y eds  Springer Verlag 2003  Y  Ohsa w a  http://www panda.sys.t.u-tokyo.ac.jp/MoDAT/DJform.html  Y  Ohsa w a H Kido T  Hayashi and C Liu Data Jack ets for Synthesizing Values in the Market of Data roc of KES2013 pp 013Ö709 Elsevier 2013  J.R Quinlan Morgan Kaufman 1993  S Rosenbaum  McGrau Hill 2011  T  Sasaki  Chikuma Shinsyo 2011 in Japanese  N Silv er   The Penguin Press 2012 
671 
671 


We now evaluate the overhead of the FChain system Table II lists the CPU cost of each key module in FChain We observe that most modules in FChain is light-weight The most computation-intensive module is the abnormal change point selection component which is triggered only when a performance anomaly occurs F Chain also distributes the change point computation load on different hosts and executes them in parallel to achieve scalability The online validation takes about 30 seconds for each component since we need some time to observe scaling impact for deciding whether we have made a pinpointing error However the online validation is only performed on those suspicious components pinpointed by the integrated fault diagnosis module The FChain daemon running inside the Domain 0 of each host imposes less than 1 CPU load and consumes about 3MB memory during normal execution IV R ELATED W ORK Our work is rst closely related to previous black-box fault localization schemes F or example NetMedic p rovided detailed application-agnostic fault diagnosis by learning inter-component impact NetMedic rst needs to assume the knowledge of the application topology To perform impact estimation NetMedic needs to nd a historical state that is similar to the current state for each component However for previously unseen anomalies we might not be able to nd a historical state that is similar to the current state for the faulty components Under those circumstances NetMedic assign a default high impact value whic h sometimes lead to inaccurate diagnosis results as shown in Section III In comparison FChain can diagnose previously unseen anomalies and does not assume any prior application knowledge Oliner et al proposed to compute anomaly scores using the histogram approach and correlates the anomaly scores of different components to infer the inter-component inîuence graph As shown in Section III it is difìcult for the histogram-based anomaly detection to perform online fault localization over suddenly manifesting faults Moreover unrelated components can have indirect correlations caused by workload uctuations which will cause their system to raise false alarms In comparison FChain is more robust to different types of faults and workload uctuations To achieve black-box diagnosis researchers have also explored various passive network trafìc monitoring and analysis techniques such as Sherlock  O ri on 27 S N A P  28 However those analysis schemes can only achieve coarsegrained machine-level fault localization Additionally during our experiments we found that previous network trace analysis techniques cannot handle continuous data stream processing applications due to the lack of gaps between packets for extracting different network ows Project5 and E2EProf performed cros s correl a t i ons bet w een mes s a ge traces to derive causal paths in multi-tier distributed systems WAP5 e x t e nds t h e b l ack-box caus a l p at h a nal y s i s t o support wide-area distributed systems Orion di s c o v e rs dependencies from network trafìc using packet headers and timing information based on the observation that the trafìc delay distribution between dependent services often exhibits typical spikes LWT propos ed t o di s c o v e r t he s i mi l a ri t y of the CPU usage patterns between different VMs to extract the dependency relations hips between different VMs However as shown in our experiments dependency-based fault localization techniques are not robust which can make frequent pinpointing mistakes due to various reasons e.g the back pressure effect in distributed applications common network services pinpointed as culprits Furthermore existing dependency discovery techniques need to accumulate a large amount of trace data to achieve reasonable accuracy Particularly network trace based techniques only support requestand-reply types of applications which fail to discover any dependency in continuously running applications such as data stream processing systems In contrast FChain provides online fault localization which does not require any training data for anomalies or a large amount of training data for normal behaviors FChain is fast which can quickly localize faulty components with high accuracy af ter the performance anomaly is detected A urry of research work has proposed to use end-to-end tracing for distributed system debugging Magpie  i s a request extraction and workload modelling tool that can record ne-grained system events and correlate those events using an application speciìc event sch ema to capture the control ow and resource consumption of each request Pinpoint t ak es a request-oriented approach to tag each call with a request ID by modifying middleware platform and applies statistical methods to identify components that are highly correlated with failed requests Monitor t racks t he reques t s e xchanged b et ween components in the system and performs probabilistic diagnosis on the potential anomalous components X-Trace i s a n integrated cross-layer crossapplication tracing framework which tags all network operations resulting from a particular task with the same task identiìer to construct a task tree Spectroscope can di agnos e p erformance anomal i e s b y comparing request ows from two executions In contrast our approach does not require any instrumentation to the 
G FChain System Overhead Measurements 
System Modules CPU cost 
VM monitoring 6 attributes 1.03 0.09 m illisec onds Normal uctuation modeling 22.9 2 millis econds 1000 samples Abnormal change point selection 602.4 105.2 m illisec onds 100 samples Integrated fault diagnosis 22 1 microseconds Online validation per-component 30 1 seconds TABLE II FC HAIN OVERHEAD MEASUREMENTS  size for the DiskHog fault in Hadoop The reason has been described in Section III-A Generally the look-back window should be long enough to capture the fault manifestation We are currently investigating an adaptive look-back window conìguration scheme by examining the metric changing speed 
     
206 
29 
29 


application or middleware platform to collect request ows Thus it is much easier to deploy FChain in large-scale IaaS clouds Blacksheep correl a t e s t he change poi nt of s y s t em-l e v el metrics e.g cpu usage with the change in count of Hadoop application states i.e events extracted from logs of DataNodes and TaskTrackers to detect and diagnose the anomalies in a Hadoop cluster Kahuna-BB correl a t e s b l ack-box dat a system-level metrics and white-box data Hadoop console logs across different nodes of a MapReduce cluster to identify faulty nodes In comparison FChain is a black-box fault localization system which is application-agnostic without requiring any knowledge about the application internals We believe that FChain is more practical and attractive for IaaS cloud systems than previous white-box or gray-box techniques V C ONCLUSION In this paper we have presented FChain a robust blackbox online fault localization system for IaaS cloud computing infrastructures FChain can quickly pinpoint faulty components immediately after the performance anomaly is detected FChain provides a novel predictability-based abnormal change point selection scheme that can accurately identify the onset time of the abnormal behaviors at different components processing dynamic workloads FChain combines both the abnormal change propagation knowledge and the inter-component dependency information to achieve robust fault localization FChain can further remove false alarms by performing online validation We have implemented FChain on top of the Xen platform and conducted extensive experimental evaluation using IBM System S data stream processing system Hadoop and RUBiS online auction benchmark Our experimental results show that FC hain can achieve much higher accuracy i.e up to 90 higher precision and up to 20 higher recall than existing schemes FChain is light-weight and non-intrusive which makes it practical for large-scale IaaS cloud computing infrastructures A CKNOWLEDGMENT This work was sponsored in part by NSF CNS0915567 grant NSF CNS0915861 grant NSF CAREER Award CNS1149445 U.S Army Research Ofìce ARO under grant W911NF-10-1-0273 IBM Faculty Awards and Google Research Awards Any opinions expressed in this paper are those of the authors and do not necessarily reîect the views of NSF ARO or U.S Government The authors would like to thank the anonymous reviewers for their insightful comments R EFERENCES   A m azon E las tic Com pute Cloud  h ttp://a w s  a m azon com ec2   V i rtual c om puting lab  http://vcl ncs u  e du  P  Barham  A  D onnelly  R I s aacs  a nd R M o rtier   U s ing m agpie f or request extraction and workload modelling in 
 2004  M  Y  Chen A  A ccardi E  K icim an J  L lo yd D  P atters on A  F ox and E Brewer Path-based failure and evolution management in  2004  R F ons eca G  P o rter  R H  K atz S  S h enk e r  and I  S toica X T race A pervasive network tracing framework in  2007  I  Cohen M  G o lds z m i dt T  K elly  J  S ym ons  a nd J  S  Chas e Correlating Instrumentation Data to System States A Building Block for Automated Diagnosis and Control in  2004  I  C ohen S  Z h ang M  G o lds z m i dt J  S ym ons  T  K elly  a nd A  F ox Capturing indexing clustering and retrieving system history in  2005  S  D uan S  Bab u  a nd K  M unagala F a A s ys tem for a utom ating failure diagnosis in  2009  S  K andula R Mahajan P  V erkaik S  A garw al J  P a dhye a nd V  Bahl Detailed diagnosis in computer networks in  2009  A  J  O liner  A  V  K ulkarni and A  A ik en  U s ing c orrelated s u rpris e to infer shared inîuence in  2010  P  Bahl R Chandra A  G r eenber g  S  K andula D  A  M altz and M Zhang Towards highly reliable enterprise network services via inference of multi-level dependencies in  2007  Z  G ong X  G u  a nd J  W ilk es   P RE S S  P Redicti v e E las tic ReS ource Scaling for Cloud Systems in  2010  H  N guyen Y  T a n and X  G u P A L  P ropagation-a w are a nom aly localization for cloud hosted distributed applications in  2011  B Gedik H Andrade K L  W u P  S  Y u and M  D oo SP ADE  t he system s declarative stream processing engine in  2008  A pache H adoop S y s tem   http://hadoop apache  or g/co re   Rice uni v e rs ity bidding s y s tem   http://rubis  objectw eb  o r g   M Ben-Y e huda D  B reitgand M F actor  H  K o lodner  V  K r a v ts o v  and D Pelleg NAP a building blo ck for remediating performance bottlenecks via black box network analysis in  2009  Y  T a n X  G u  a nd H  W a ng  A dapti v e s ys tem anom aly prediction f or large-scale hosting infrastructures in  2010  D  L  M ills   A b rief his t ory o f N T P tim e m e m o irs o f a n i nternet timekeeper  2003  Y  T a n H  N guyen Z  S h en X  G u C V e nkatram ani and D  R ajan PREPARE Predictive Performance Anomaly Prevention for Virtualized Cloud Systems in  2012  M  Bas s e ville and I  V  N ikiforo v   Prentice-Hall Inc 1993  L  Cherkaso v a  K  O zonat N Mi J  S ym ons a nd E  Sm irni  Anom aly application change or workload change towards automated detection of application performance anomaly and change in  2008  P  Barham and e t al   X e n a nd the a rt of virtualization  i n  2003  T he ircache p roject  h ttp://www.ircache.net  H ttperf  h ttp://code google com  p htt p er f  S  K u llback  T h e ku llback-leibler distance  1987  X  Chen M  Z hang Z  M  M a o and P  B ahl  A utom ating n etw ork application dependency discovery experiences limitations and new solutions in  2008  M Y u  A  G reenber g  D  M altz J  Re xford L  Y u an S  K andula and C Kim Proìling network performance for multi-tier data center applications in  2011  M K  A guilera J  Mogul J  W iener  P  R e ynolds  a nd A  Muthitacharoen Performance debugging for distributed systems of black boxes in  2003  S  A g arw ala F  A l e g re K  S chw a n and J  M ehalingham  E 2E P r of A utomated end-to-end performance management for enterprise systems in  2007  P  Re ynolds  J  L  W iener  J  C M ogul M  K  A guilera and A  V ahdat  WAP5 black-box performance debugging for wide-area systems in  2006  R Apte L  Hu K  S chw a n and A  G hosh L ook W ho s T alking Discovering dependencies between virtual machines using cpu utilization in  2010  G Khanna I  L aguna F  A rshad an d S Bagchi Distr ibuted diagnosis of failures in a three tier e-commerce system in  2007  R R S a m b as i v an A  X  Z heng M  D e Ros a  E  K re v at S  W h itm an M Stroucken W Wang L Xu and G R Ganger Diagnosing performance changes by com paring request ows in  2011  J  T a n a nd P  N a ras i m h an  RA M S and B lackS h eep I nferring w h ite-box application behavior using black-box techniques CMU Tech Rep 2008  J  T a n X  P a n E  Marinelli S  K a vulya R  G andhi a nd P  N a ras i m h an Kahuna Problem diagnosis for mapreduce-based cloud computing environments in  2010 
OSDI NSDI NSDI OSDI SOSP ICDE SIGCOMM DSN SIGCOMM CNSM SLAML SIGMOD ICAC PODC Computer Communication Review ICDCS Detection of abrupt changes theory and application DSN SOSP The American Statistician OSDI NSDI SOSP DSN WWW HotCloud SRDS NSDI NOMS 
207 
30 
30 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





