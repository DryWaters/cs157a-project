Mining Repetitive Sequences Using A Big Data Ecosystem  Michael Phinney 1 Hongfei Cao 1 Andi Dhroso 1,2 Chi-Ren Shyu 1,2  1 Department of Computer Science 2 Informatics Institute, Universi ty of Missouri, Columbia, MO  Abstract Identifying repetitive gene sequences occurring within DNA sequences that span a collection of species is a challenge that is conceptually simple yet computationally challenging. Biological research suggests that certain regions within genomic sequences may be unchanged for hundreds of millions of years; understanding and identifying these highly preserved regions is a major challenge faced by bioinformaticians. Taking an evolutionary perspective on DNA, pinpointing these repetitive sequences is the first step to understanding functional simila rities and diversities. The difficulty of this problem arises from the volume of the data required for analysis; it grows with every genome that is sequenced. Traditional approaches used to identify repetitive sequences often require the pair-wise comparison of chromosomes, which takes a significant amount of time to gather results. When comparing n chromosomes n\(n-1 individual comparisons must be made. To avoid exhaustive pair-wise comparisons, we designed an algorithm that partitions genomic sequences into search key values representing potential repetitive sequences, which are hashed into bins. With the introduction of new genomes, we only process the new sequences and aggregate new results with those that were previously processed Keywords— repetitive sequences, Big Data, sequence matching MapReduce, Hadoop  I  INTRODUCTION Research on animal and plant genomes suggests that specific regions within the ge nomes are highly preserved remaining unchanged for nearly 300 m is great mystery associated with these seemingly static sequences; as an evolutionary mechanism, the exact role  o w e ver, identifying their existence is the first step to gaining insight. One of the most common approaches to identif ying ultraconcerved elements UCEs\ a pair-wise sequence alignm This method is known to be computationally expensive Modern approaches utilize information retrieval based techniques to achieve similar results and have shown to be  e suggest an a l ternative approach that utilizes the Hadoop MapReduce framework to address this issue This problem could be considered as a topic of Big Data because the data is not clean, th ere is uncertainty in the data the amount of intermediate re sults generated by pair-wise comparison approaches is difficult to manage, and the availability of genomic data is increasing rapidly. Utilizing common pair-wise comparison techniques to compare new genomes with existing genom ic datasets requires an enormous amount of redundant computation. In addition alignment algorithms may not pr ovide optimal results; they may not identify all matching sequences II  SYSTEM ARCHETECTURE  A  Framework By utilizing the Ha ew ork, w e are able to parallelize the processing across multiple machines and multiple threads. Our repetitiv e sequence algorithm utilizes a parallel, distributed progra mming model developed for use in a cluster environment, Ma be thought of as a data flow consisting of two phases: Map and Reduce conceptually, shown in Fig. 1. The map phase is responsible for sorting and filtering data. First, the largescale genomic input data is divided amongst a collection of mappers \(the number of mapper is defined based on the size of our input data manipulated and translated into a set of key-value pairs; in our case, keys are subsequences and values are their respective chromosome identifiers and starting positions in the input sequence These key-value pairs are passed through a hashing function that will deterministically map a key to one of potentially many reducers; this ensures pairs sharing a common key will be sent the same reducer In the reduce phase, reducers aggregate the output subsequences received from mappers The MapReduce programming model is appropriate for sequence matching applications. That is, by assigning a sequence to be the key, the hashing behavior of the MapReduce model may be utilized to locate iidentical sequences; exact matching sequences will be hashed to the same reduce task B  Cluster Setup For the purpose of this study, we simulate a cluster  mode. We constructed a virtual cluster by partitioning a single server’s resources and allocating them to a collection of virtual machines. In total our server has 128GB of RAM 24 2.0GHz Intel CPUs, and 6TB of disk space. For our experiments, we have configur ed our cluster to have seven data nodes and one master node. Each data node virtual machine is running CentOS Li nux, has approximately 6GB of RAM, and a single CPU. The master node has 8GB of RA M and 4 CPUs. Hadoop is running in pseudo distributed mode as opposed to fully distri buted mode, since the cluster is housed on one physical machine. The advantage to using this sort of environment, we have complete control over 


available resources and are able evaluate different cluster configuration settings such as the number of nodes and the number of CPUs and RAM to be allocated to a node III  METHOD  Common methods for identifying UCEs across a set of chromosomes utilize an exhaustive pair-wise comparative approach; given n chromosomes, identifying inter- and intra-chromosomal UCEs requires n 2 comparison. We would prefer a technique that requires n operations analyzing each chromosome once. Our approach, shown in Algorithm 1, utilizes the Hadoop MapReduce model to perform a single global aggregation to identify identical sequences across chromosomes simultaneously Algorithm1 : Repetitive Sequence in MapReduce Map Function: input k  v  k is offset for current file block \(in bytes v is a sequence in chromosome C  1 v P v ove invalid characters 2 for  i 0 to m n  do  2  TSI code v  i to i  n  3  start_pos  i  k  4 return  TSI start_pos C    Reduce Function: input k  v  k is a subsequence TSI  v  is the starting position of the subsequence w.r.t the chromosome sequence  1: if\(count v  2  uce decode k   3  pos merge v  4 return  uce  pos    As previously mentioned, the role of our Map function is to construct a set of key-value pairs; all subsequences of length n 100 base pairs in our implementation chromosome form keys and their respective starting positions along with the current chromosome identifier form the values. To avoid potential capacity issues provoked by trying to feed in data files that are too large for a single mapper, the system first splits the file into smaller chunks Hence, in the map layer, our input data will be a partition of one of the original files. In the Map function, the value in each input key-value pair is a segment of the raw chromosome sequence containing values A, C, G, T and a few others such as N, M, R, etc. A few preprocessing steps are performed before generating any key-value pairs. First only valid characters A, C, G and T are considered. That is all subsequences that contain an N, M, R, etc., are neglected because this represents some le vel of uncertainty within the sequence. Second, we convert each subsequence from a string to binary representation using the code function; we refer to this conversion as a Translated Sequence Identifier TSI by one of only four possible values, using the base-2 numeric system led to a signifi cant reduction in data size The character data type in Java requires two bytes of storage space; we reduce this to two bits. This will significantly reduce the size of intermediate results generated by Mapper In our experience using subse quences of length 100 as our key, we were able to halve the size of our intermediate results by using TSIs. The next step in the mapper is to calculate the starting position of a subsequence with respect to the raw chromosome by adding the block offset k to the current line offset i To illustrate the concept, considering a subsequence TSI starting at the 50 th position in the original file containing chromosome A; it would take the following key-value pair representation TSI 50 A  After constructing key-value pairs in the map tasks, all like keys will be mapped to the same reducer. In the Reduce function, the input key is a TSI \(potential UCE are lists of starting positions paired with a chromosome id The main job of a reducer is to count the number of positions present in the value; if there are more than two, we have found a UCE. Following our simple example above all key-value pairs with a key of TSI will arrive at the same reduce task. Upon arrival to a reducer, the positions are Figure 1 System Architecture for MapReduce setting for identifying repetitive sequences 


aggregated together. This set of positions represents the location of UCEs. A sample out put from a reducer may look as follows TSI 50 A  2300 B sing this representation, it becomes easy to see that a sequence of length 100 occurs in both chro mosome A and B starting at the 50 th and 2300 th positions respectively. The final job for the reducer is to decode the TS I, translating it back to the original character representa tion, and storing the results IV  R ESULT  Our MapReduce implementation is written in Java and utilizes the Hadoop MapReduce framework. We consider the task of identifying UCEs across six chromosomes, three human and three from rat, in an incremental fashion as to demonstrate the behavior of our run-time, intermediate data size, and final result data size as the number of base pairs contained in the input sequences is increased. We compare our results with that of an existing method described in [1 in terms of accuracy. We do not consider a direct comparison of the results in terms of runtime because both approaches require vastly different computing environments. However, we report the overall runtime for our method The first case, processing one chromosome consisting of 250 million base pairs for intra-chromosomal UCEs resulted in a runtime of appr oximately 25 minutes, 68 GB of intermediate data, and 512 MB of final data containing discovered UCEs. The subsequent cases iteratively introduced new chromosomes, increasing the number of base pairs in a roughly linear manner. Our method was able to identify all known UCEs across the six chromosomes having an average length of 250,000,000 base pairs within 4.5 hours, approximately 5.6 minutes per one million base pairs. The method scaled linearly as the number of base pairs increased \(see Fig. 2\. Our results were validated against results achieved from o re precise, every UCE identified by the prior method was also identified by our proposed method. It is im portant to note that storage space is the greatest limiting factor for our method; we generate roughly 1 GB of intermediate results per every 1 million base pairs. A fully di stributed cluster containing more nodes would certainly have improved performance V  CONCLUSIONS  This ongoing study demonstrates the potential for conducting sequence matching with the Hadoop MapReduce framework. Approaches defined in previous studies perform pair-wise comparisons, whereas our approach is comprised of a si ngle global aggregation step to identify matches across all sequences simultaneously. The simplicity of our method in combination with the scalability of Hadoop is what makes our a pproach novel. Furthermore introducing more nodes into the computational cluster will improve our performance until the total number of map and reduce tasks is exceeded by the number of nodes in our cluster. With the advent of online pay-per-use cluster services, individuals could quickly spin up a powerful environment composed of an arbitrary number of nodes to complete this task quickly. Additional improvements are being made on our met hod by employing HBase to efficiently manage memory and storage utilization; we can minimize disk I/O by accumula ting results in memory and writing them to disk in one flush ACKNOWLEDGMENTS  This material is based upon work supported by the IBM Students for a Smarter Planet Project Award. MP has been supported by the U.S. Depa rtment of Education GAANN grant no. P200A100053 REFERENCES 1  J. Reneker, E. Lyons, G. C. Cona nt, J. C. Pires, M. Freeling C. R. Shyu, D. Korkin, Long identical multispecies elements in plant and animal genomes Proc Natl Acad Sci U S A 2012 May 8;109\(19 doi: 10.1073/pnas.1121356109 Epub 2012 Apr 10 2  G. Bejerano, M. Pheasant, I. Makunin, S. Stephen, W. J Kent, J. S. Mattick et al Ultraconserved elements in the human genome Science vol. 304, pp. 1321-1325, 2004 3  T. White Hadoop: the definitive guide O'Reilly, 2012 4  J. Dean and S. Ghemawat, "MapReduce: simplified data processing on large clusters Communications of the ACM vol. 51, pp. 107-113, 2008 5  S. Ghemawat, H. Gobioff, and S.-T. Leung, "The Google file system," in ACM SIGOPS Operating Systems Review 2003 pp. 29-43 6  L. F. Lareau, M. Inada, R. E. Green, J. C. Wengrod, and S. E Brenner, "Unproductive splicing of SR genes associated with highly conserved and ultraconserved DNA elements Nature vol. 446, pp. 926-929, 2007 Figure 2 The charts display the relationship between the input size \(# of base pairs\and the following three metrics run-time, size of intermediate data generated, and size of final results generated by our method  


003 003 
014\010 014  014  014 014 014 014 014 014\010 003 003 014 014     
Q.empty u can reach v in 2 steps and B Transitive Closure of the SPT C Complexity Theorem 3 Proof 1 Local bidirectional BFS for accessing the SPT 
1 2 3 4 for 7 8 9 while 11 12 if 14 15 16 17 1 2 for 4 if 6 7 
out in out in 002 002 002 002 u V 002 002 002 002 u V 002 
Computation of the SPT vertex set  G a directed data graph   the vertex set of SPT   Queue Q Q.enqueue\(u level  0 v  Q.dequeue return  According to the approach of our index construction this property is in evidence Take advantage of this property we propose a simple method shown in Algorithm 3 to retrieve all the edge set for the SPT The main idea of it is to do BFS from every vertex of the SPT if it can reach another vertex of the SPT in 2 steps then we create a link between them and add a cost for it Because all this link added by us are all the path information of the original graph there is no new reachability information introduced Computation of the SPT edge set  G a directed data graph  the vertex set of the SPT for G   the edge set of the SPT   do BFS from u  return  After the operation in Section 3.1 we can get a subgraph of the original graph The index size can be reduced signi“cantly The shortest path distance query ef“ciency closely relate to the SPT Because the total size of the SPT is comparative small we can use the most direct method that to compute all the exact shortest path distance between all the vertex pairs of the index such as the Dijksras algorithm  To improve the ef“ciency we can use other algorithms to construct index for the SPT too In the experiment result we can nd that our index framework can help improve the query ef“ciency of other approaches greatly If the size of the SPT is still a bottleneck of the query performance we can continue compute the SPT for the original SPT to further reduce the index size  For the index construction we have to order the vertices of the original graph G by their degrees use the vertices with high degree to cover other vertices generate the edges between all the SPT vertices For 1 it takes time For 2 Let denote the set of vertices that can cover all the vertices of the original graph let and denote the vertices and edges in us forward neighbors Let and denote the vertices and edges in us backward neighbors respectively then the procedure takes time  For 3 we deploy local BFS from the every SPT vertices which costs in worst case   Mostly the SPT is a really sparse graph Therefore the storage cost is very small Even if we adopt the transitive closure to store all the shortest paths between every vertices pairs of the SPT the index size is  where  IV QUERY PROCESSING We now discuss how to process the shortest path distance query by our index First we give an important property of the SPT about query processing Any vertex can reach a SPT vertex in hops According to theorem 3 the max distance between any two SPT vertices is  In the extreme case one vertex may one hop away from the nearest backward SPT vertices Therefore the maximum distance between this vertex to other SPT vertex is  The basic scheme of the shortest path distance computation based on Theorem 3 is sketched in Algorithm 4 It consists of two basic steps To get the shortest path distance of two vertices u and v we rst carry out two local BFS within hops One is forward search from u another is backward search from v If they are matched locally we directly answer the shortest path distance or else they both can reach the vertices of the SPT 
002 a threshold V V O the set of node ordered by degree each u in O visited u V u level get v s level level  002 B v s successors B v s predecessors Q B Q B V 002 V E E each u V 002 v V c the cost f rom u to v E u v c E O nlogn V 002 N u E u 002 N u E u 002 O N u E u N u E u O V E u O K 2 k V 002 002 002 002 
Algorithm 2 input output do 5 if then 6 do 10 then 13 Algorithm 3 input output do 3 then 5 Index construction time Index size 
004 004 004 002 002 002 004 004 004 004 004 004 004 004    004 002 002 005 002 002 004 005 
002 002 
                                      2 1 2 2 1 2 
198 


004 004 004 004 002 002         
1 2 1 1 2 2 1 2 
         
002 002 002 002 
Dataset Nodes Edges Diameter 
014 014 003 003 015 
1 2 3 if 5 6 7 for 11 
Query\(u,v  G a directed data graph T the SPT for G  the shortest path distance between u and v  do forward BFS from u do backward BFS from v return the distance from u to v the set of all the SPT vertices that u can reach the set of all the SPT vertices that can reach v return the distance from u to v return  If the two vertices are not in the same community we can get the shortest path distance by test the SPT The ef“ciency of this step is depend on how we handle the query on the SPT Usually we can answer it in constant time  For arbitrary vertex pair u v each local BFS for u and v of the original graph G takes time and the shortest path computation on the SPT can be done in constant time if we adopt a suitable index structure for it V EXPERIMENTAL EVALUATION In this section we empirically evaluate the performance of our SPTI framework on both real and synthetic datasets Particularly the following problems are considered Whether our method really reduce the index size indeed Whether the data processing speed can be increased signi“cantly by the SPTI Whether the SPTI can improve other algorithms performance To answer the questions mentioned above we study the following state-of-the-art shortest path distance query answering schemes and their SPT counterparts BFS the classical online bread-“rst search 2HOP the 2HOP distance labeling HCL the highway-centric labeling approach using approximate set cover algorithm and directed MST with  SPT-2HOP the 2HOP approach combining with our SPT index SPT-HCL the HCL approach combining with our SPT index In each experiment we focus on measuring three important features index construction time index size and query Table I Real Datasets To validate our approaches we rst do experiments on real-world datasets For more attractive and convincible results we purposively collected 8 datasets listed in Table I with their some important characteristics All these datasets are directed sparse and unweighted graphs For those nonDAG graphs we can transform it into a DAG by coalescing strongly connected components SCC into virtual vertices This can be done simply by depth rst search DFS The size the selected graphs range from several thousand to almost two million vertices CiteSeer is collected by The Koblenz Network Collection KONECT which is about the digital library for scienti“c and academic papers primarily in the elds of computer and information science The others are all collected by Stanford Large Network Dataset Collection SNAP Their brief introductions are as follow 1 p2p-Gnutella08-31 a sequence of snapshots of the Gnutella peer-to-peer le sharing network from August 25 to 31 in 2002 2\ote a free encyclopedia written collaboratively by volunteers around the world 3\ec anonymized data of the Pokec which is the most popular online social network in Slovakia 4 cit-Patents U.S patent dataset is maintained by the National Bureau of Economic Research Table II shows the index construction time for different query answering approaches From the result we can see that our algorithm is much faster than other counterparts especially for those large graphs In addition 2HOP and HCL cannot work on the graphs which have more than hundreds of thousands of vertices due to insuf“cient scalability However by combining our method both 2HOP and HCL can handle large graphs with millions of vertices 
u can reach v in 2 hops has the shortest path to 2 Answering the query by the SPT A Experimental Setup B Real Datasets 
Algorithm 4 input output then 4 do 8 for do 9 if then 10 Complexity 
002 V V each v V each v V v V O N u E u N v E v 
p2p-Gnutella08 6,301 9215 9 p2p-Gnutella09 8,114 11,717 9 p2p-Gnutella31 62,586 73702 11 p2p-Gnutella30 36,682 43,379 10 p2p-Gnutella25 22,687 23,651 11 CiteSeer 723,131 790,552 10 wiki-Vote 8297 12,374 7 soc-Pokec 1,632,803 2,964,437 11 time For query time we measure it by answering 100,000 complete random queries The index size of a SPTI approach consists of two parts the index size of the shortest path trunk and the label size of 2HOP or HCL on the trunk The construction time of a SPTI method consists of the time cost of constructing the shortest path trunk and the time cost of constructing the label of the trunk All algorithms mentioned in this work are implemented in C based on the Standard Template Library STL All experiments are performed on a Windows 7 machine with AMD-3870K 3.0GHZ and 8G RAM 
199 


E  V V E  V  E  V  E  V  V  K 
6 6 6 6 6 6 6 6 
1 31 10 4 35 10 2 58 10 1 5 10 2 32 10 2 35 10 1 11 10 1 29 10 
                    
   V  V  V    
p2p-Gnutella08 21,939 1,601 13 10 p2p-Gnutella09 27380 2,101 14 13 p2p-Gnutella31 4.75106 137,092 628 215 p2p-Gnutella30 1.43106 41,180 213 91 p2p-Gnutella25 365,157 18,647 117 45 wiki-Vote 223,179 15,760 101 20 CiteSeer   522,590 soc-Pokec   p2p-Gnutella08 4,983 5,041 9,877 p2p-Gnutella09 4,674 4,712 12868 p2p-Gnutella31 4,467 4,534 121,717 p2p-Gnutella30 5,309 5,373 65,429 p2p-Gnutella25 3,222 3,377 32,434 wiki-Vote 2,070 2,145 2235 CiteSeer 3323 3425 4505 soc-Pokec 5704 6412 66527 Table IV Result of Synthetic Datasets\(Construction Time 5K 2,595 1,260 9 10 10K 10,615 4,948 21 22 50K 348,523 132,990 699 204 100K 518,809 2,006 917 500K   173,784 39,277 Table V Result of Synthetic Datasets\(Construction Time 5K 14,547 7,591 12 10 10K 68,640 30,588 22 23 50K 980,552 637 211 100K   4,912 870 500K   490,499 52,584 Table VI Result of Synthetic Datasets\(Construction Time 5K 479,382 207,887 19 11 10K 105 60 50K   7,601 285 100K   14625 2134 500K   29,764 
C Synthetic Data 
1 5 2 0 3 0 500 
Table II Result of Real Datasets\(Construction Time Figure 3 shows the index size for different query approaches The label size for online BFS is not applicable The 2HOP and HCL can provide comparatively reasonable index size for small and sparse graph However for dense graphs such as wiki-vote the performance of them drops dramatically The biggest characteristic our SPT algorithm is that it can handle dense graphs ef“ciently Because the SPT is really sparse for the original graph it is very t for 2HOP and HCL to continue construct index Figure 3 The Index Size of Real Datasets For query time we can see the result in Table III Because the query time for 2HOP and HCL is nearly constant time we mainly compare our algorithm with the online BFS To make the comparison result more meaningful we do not use completely random queries which would produce too much negative queries Sometime there are even no positive queries in one experiment This is highly unlikely practical applications which pay more attention to positive queries Therefore we select the vertices whose out-degree is larger than 0 and select the target whose in-degree is larger than 0 This ensures that there must be some vertices visited in a query From the result we can conclude that our method improves the query ef“ciency signi“cantly in most of the dataset especially for those large and dense graphs This is mainly because the time cost of local BFS is almost negligible We further test our method on synthetic datasets too In this part we mainly investigate how density affects the algorithms performance All synthetic datasets are generated by Erdos Renyi Modes\(ER a classical random graph model We set the density from 1.5 to 3 and vary from 5K to 2,000K We still test the algorithms used above i.e 2HOP HCL STP-2HOP and STP-HCL in this experiment Table III Result of Real Datasets\(Query Time    The detailed statistics of construction time of these datasets can be seen in Table IV-VI The result shows with increasing the density and size of graphs the performance of 2HOP and HCL decrease drastically When the size of vertex comes to 100K they cannot continue to work any longer However our algorithm can work well even when the size of vertex come to 1M Especially our approach is very applicable to high density graphs The SPT itself is a very sparse subgraph which can improve the construction and query ef“ciency of 2HOP and HCL greately Table VII to IX report the index size of four approaches Both SPT-2HOP and SPT-HCL reduce the index size greatly The data shows our index size at least an order of magnitude less than 2HOP and HCL It is very meaningful for large graphs query In fact our approach still work ef“ciently even when  Due to space limitation we omit these data Figure 4 shows the query time of different query ap 
              
Dataset 2HOP HCL SPT-2HOP SPT-HCL Dataset SPT-2HOP SPT-HCL BFS 2HOP HCL SPT-2HOP SPT-HCL 2HOP HCL SPT-2HOP SPT-HCL 2HOP HCL SPT-2HOP SPT-HCL 
200 


2 
1 5 2 0 3 0 
5K 33,824 23,579 1,809 1,745 10K 69,119 47,003 3,956 3,613 50K 345,173 237,503 20,401 18,401 100K 700,927 482,481 40,676 36,856 500K   203,263 184,257 Table VIII Result of Synthetic Datasets\(Index Size 5K 84,858 60,764 1,964 1,746 10K 172,572 123,230 3,650 3,294 50K 902,141 647,649 20,100 17,854 100K   41,887 37,002 500K   202,491 180,404 proaches We vary b c 5K 421,903 312,404 2,087 1,818 10K 923,216 699,404 4,821 4,225 50K   24,290 20,905 100K   48,859 41,890 500K   241,212 207,648 threshold 5K 745 614 741 725 786 4,353 953 86 35 63 10K 1,469 1,262 1,479 1,491 1,513 7,874 1,715 3,351 140 63 50K 7,639 6,374 7,540 7,499 7,692 43,231 11,467 2,324 811 90 100K 151,151 12,495 15,162 151,491 15,796 86,840 26,371 4,342 1,489 516 500K 74,875 62,821 75,299 75,820 75,750 456,977 124,311 22,254 8,864 2,465 1000K 49,971 126,628 150,628 152,425 15,628 902,019 247,750 44,408 18,796 74,911 Table XI Size of SPT on Random Graphs 5K 610 467 612 611 606 45,521 989 75 35 18 10K 1,202 925 1,261 1,231 1,260 11,922 2,632 867 694 767 50K 5,980 4,928 6,178 6,137 6,367 60,661 15,503 1,845 1,148 452 100K 11,913 9,525 12,727 12,296 12,907 119,466 34,423 6,155 1,726 571 500K 59,729 47,091 61,648 61,865 64,159 658,330 185,288 29,017 11,674 2,448 1000K 118,876 95,193 122,709 124,457 128,875 1,290,166 36,8647 56,351 23,163 5,008 VI RELATED WORK Many approaches have been proposed for processing source-to-target shortest path and distance query and are related to several key questions i.e single-source shortest path distance computation One of the most well-known methods for this issue is Dijksras algorithm It solv es the single-source shortest path problem for a graph with non-negative edge path costs producing a shortest path tree This algorithm is often used in routing as a subroutine in other graph algorithms or in GPS Technology and can be implemented with 
             011             
E  V  E  V  V E  V E  V  002 002 002 E  V E  V O n 
Table VII Result of Synthetic Datasets\(Index Size   from 5K to 1000K so as to be more persuasive Interestingly the higher of the graphs density the more our method is ef“cient When  the query time tend to constant time a Figure 4 Result of Synthetic Datasets\(Query Time In the last part of our experiment we test how the Table IX Result of Synthetic Datasets\(Index Size  affects the number of vertices and edges of the SPT Apparently if increases the size of the SPT would reduce and the local search cost would increase accordingly Therefore we do not suggest set too large In reality for many real world data graphs the index size would reduce signi“cantly even when  The result can be seen in Table X to XII We show the dataset with the density with 4 5 and 6 for space limitation Table X Size of SPT on Random Graphs   time Another famous method is Bellman-Ford algorithm  which is capable of handling graphs in which some 
                      
V V E  V  E  V  E  V  V V V 
1 5 2 0 3 3 0 2 4 5   
2HOP HCL SPT-2HOP SPT-HCL 2HOP HCL SPT-2HOP SPT-HCL 2HOP HCL SPT-2HOP SPT-HCL 2 3 4 5 6 2 3 4 5 6 
201 


out in out in out in 
5K 496 390 550 556 570 3,836 881 21 2 6 10K 987 758 1,090 1,084 1,120 11,434 2,751 175 146 109 50K 4,969 3,701 5,103 5,079 5,359 79,351 18,589 3,604 421 114 100K 9,671 7,425 10,269 10,357 10,757 165,256 40,460 7,293 1,534 494 500K 4,881 36,581 51,889 52,024 53,980 855,651 252,588 30,401 12,819 2,665 1000K 97,071 75,103 102,768 103,157 108,068 1,702,807 467,340 62,431 29,713 5,579 of the edge weight are negative numbers and can runs in 
3 
    003 
E  V O nm O n L u L u L u L v s L u L v 
Table XII Size of SPT on Random Graphs   For unweighted graphs we can deploy Breadth First Search which can compute the single-source shortest path problem in O\(m+n To compute the all-pairs shortest paths we can directly use Dijkstra n times or Floyd-Warshall algorithm which all need time These algorithms are all not suitable for large scale graphs The 2-HOP algorithm proposed by Conhen et al adopts labeling mechanism to answer reachability and distance query Each vertex u records a list of intermediate vertices which it can reach along with the shortest distance and a list of intermediate vertices which can reach it along with the shortest distance To compute the shortest distance between u and v we simply check all the common intermediate vertices between and and choose the vertex s such that s,v is minimized for all  In recent years a lot of algorithms have been proposed for graph reachability queries 6 7 8 9 10 These algorithms mainly discover the connectivity of relevant vertexes but cannot compute the shortest distance between them Vertex cover approaches has lately become the mainstream 10 Through the selecting an approximate minimum vertex cover and constructing an index the reachability query can be answered ef“ciently However they still do not consider the distance computing Ruoming Jin et al put forward a highway-centric labeling approach in which is our counterpart in e xperiment They deploy a novel labeling scheme which select a group of vertices to construct a tree-like highway index structure for answering distance queries in large sparse graphs It can provides better labeling size than 2-hop However in our experimental result we can nd that this method cannot handle large graphs ef“ciently in compare with our algorithm VII C ONCLUSION In this paper we propose a new index construction approach SPTI to answer the shortest path distance query We nd that the distribution imbalance of the edges between the vertices in the real world graphs and utilize the vertices which have more connection with others as index to answer the shortest path distance query To nd a near optimal salutation we propose to selectively choose a group of vertices from the original graph to create a trunk for preserving the shortest paths information and compact the index size Compared with existing methods such as 2HOP online BFS and HCL our algorithm can reduces the index size signi“cantly and answers the shortest path distance query ef“ciently In the future we plan to apply our index structure on dynamic graphs A CKNOWLEDGMENT This research was supported by the Normal Project Foundation of Education Department of LiaoNing Province Grant No L2012045 R EFERENCES  E W  Dijkstra A note on tw o problems in conne xion with graphs Numerische Mathematic 1\(1 Dec 1959  Richard Bellman On a routing problem Quarterly of Applied Mathematics 16 8790 1958  Eric W eisstein Flo yd-W arshall Algorithm W olfram MathWorld Retrieved 13 November 2009  Edith Cohen Eran Halperin Haim Kaplan and Uri Zwick Reachability and distance queries via 2-hop labels SIAM J Comput 32\(5 2003  Haixun W ang Hao He Jun Y ang Philip S Y u and Jef fre y Xu Yu Dual labeling Answering graph reachability queries in constant time In ICDE 2006  Y angjun Chen and Y ibin Chen An Ef cient Algorithm for Answering Graph Reachability queries In ICDE 2008  J Cheng J X Y u X Lin H W ang and P  S Y u F ast computing reachability labelings for large graphs with high compression rate In EDBT 2008  J Cheng J X Y u X Lin H W ang and P  S Y u F ast computation of reachability labeling for large graphs In EDBT 2006  Ruoming Jin Ning Ruan Saikat De y and Jef fre y Y u Xu SCARAB Scaling Reachability Computation on Large Graphs In SIGMOD 2012  James Cheng Zechao shang and Hong Cheng K-Reach Who is in Your Small World In VLDB 2012  Ruoming Jin Ning Ruan Y ang Xiang and V ictor E Lee A Highway-Centric Labeling Approach for Answering Distance Queries on Large Sparse Graphs In SIGMOD 2012  W asserman S and F aust K Social Netw ork Analysis Cambridge Cambridge University Press 1994  J Scott Social Netw ork Analysis A Hand-book London Sage Publications 2002  F  Chains Karinthy  Ev erything is Dif ferent Atheneum Press 1929  P  Sanders and D Schultes Highw ay hierarchies hasten e xact shortest path queries In 17th Eur Symp Algorithms\(ESA 2005  http://k onect.uni-k oblenz.de/netw orks/citeseer  accessed May 18 2013  http://snap.stanford.edu/data accessed May 18 2013 
  
V 
6                 
2 3 4 5 6 
202 


state of innovation stakeholder  node PQ It  s a balanced node Based on this, we could calculate the  node PQ Calculation process is: set different inn ovation stakeholders state i U  and j U Value of ij 000T can be get from  innovation time difference. Innovation stakeholders’ social effect and industrial effect can be obtained upon ij B  and ij G set according to relation between innovation stakeholders  Model 4.1 points out  that the value of Gij  directly affects social benefits and sector benefits. Large Gij  can lead to increasing benefits of the entire industry and the entire social growth Bij reflects big organization’s impact on businesses. Only strengthening the inter agent association within big organization and enhancing the str ategic partnership between enterprises can jointly promote the development of the entire industry, and bring more social benefits, so that each agent can be improved   5 Summary This paper puts forward the concept of the big organization based on the CSM t heory. It introduces the basic implication of the big organization and theoretical framework of the big organization including: the big organization's perspective  overall perspective, dynamic perspective, and new resource perspective; the big organizat ion’s sense  the purpose of the organizational structure is innovation, organizational activities around the flow of information, breaking the traditional organizational structure, encouraging self run structure, and blurring organizational boundaries; the big organization’s platform  the platform ecosystem of the big organization ; the big organization’s operation mode  borderless learning mode, and cluster effect; the big organization’s theory  active management theory  leading consumers, and culture  entropy reduction theory  negative culture entropy and humanistic ecology theory  inspiring humanity, and circuit theory  a virtuous circle, and collaborative innovation theory  collaborative innovation stakeholder. This paper also discusses culture entropy reduction theory of the big organization  negative culture entropy, and coordinated innovation theory  innovation stakeholders collaboration. Culture entropy change model and collaborative in novation model are constructed   The research has just begun for the big organization. It also needs further improvement but remains the trend of the times   Reference  1  Gordon Pellegrinetti, Joseph Bentsman. Nonlinear Control Oriented Boiler Modeling A Benchmark Problem for Controller De sign [J  I E E E tr a n s a c tio n s o n c o n tr o l s y s te m s te c h n o lo g y 2 0 1 0  4 1\57 65  2  Klaus Kruger, Rudiger Franke, Manfred Rode Optimization of boiler start up using a nonlinear 457 


boiler model and hard constraints [J  E n e r gy 201 1 29   22 39 2251  3  K.L.Lo, Y.Rathamarit  State estimation of a boiler model using the unscented Kalman filter [J  I E T  Gener. Transm. Distrib.2008 2 6\917 931  4  Un Chul Moon, Kwang. Y.Lee. Step resonse model development for dynamic matrix control of a drum type boiler turbine system [J IE E E  T ra nsactions on Energy Conversion.2009 24 2\:423 431  5  Hacene Habbi, Mimoun Zelmat, Belkacem Ould Bouamama. A dynamic fuzzy model for a drum boiler turbine system [J  A u to m a tic a 2 0 0 9 39:1213 1219  6  Beaudreau B C. Identity, entropy and culture J   J o ur na l  o f  economic psychology, 2006, 27\(2 205 223  7  YANG M, CHEN L. Information Technique and the Entropy of Culture J  A cad e m i c E x ch a n g e  2006, 7: 048  8  ZHANG Zhi feng. Research on entropy change model for enterprise system based on dissipative structure J  Ind ustrial  Engineering and  Management 2007, 12\(1\ :15 19  9  LI Zhi qiang, LIU Chun mei Research on the Entropy Change Model for Entrepreneurs' Creative Behavior System Based on Dissipative Structure J  C h i n a S of t S c i e n c e  2009   8  1 62 166   458 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of “Daily” Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data – Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average “daily“ operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rolling… I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators – Data Element Methods – Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Today’s cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlight’s data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlight’s hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlight’s method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





