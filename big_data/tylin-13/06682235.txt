000\003 
An IO optimized Data Access Method in Distributed KEY-VALUE Storage System 
Li Chao Institute of Computing Technology Chinese Academy of Sciences Beijing,China lc_lichao@126.com Wu Guangjun Institute of Information Engineering Chinese Academy of Sciences Beijing,China wuguangjun@iie.ac.cn Wang Shupeng Institute of Information Engineering Chinese 
Academy of Sciences Beijing,China wangshupeng@iie.ac.cn Li Yixi National Computer network Emergency Response technical Team/Coordination Center of China Beijing,China 15901118657@126.com 
Abstract 
Distributed KEY-VALUE storage system is a new storage framework for cloud computing. It can enable an application to dynamically adapt to growing workloads by increasing the number of 
 
servers However current distributed KEY-VALUE storage systems are still inefficient on range query for larger result set When the result set become large the file layout, cache hit rate are both key points for IO efficiency. In this paper, we will introduce our experience under the development of China Mobile Big Cloud KEY-VLAUE DB \(BC-kvDB We will discuss how we increase IO efficiency in BC-kvDB BC-kvDB is based on single-table space data model 
and provides SQL-LIKE DDL and DML language BC-kvDB’s high throughput is benefit from data locality storage, column-storage structure and multilayer caches.  Data can be accessed in local cache or local blocks through block index. Experimental results show that the random writing performance of BC-kvDB is 2.5 times better than HBase and the random reading performance is 1.8-2 times than HBase Key words: KEY-VALUE DB; Distributed Storage 
Cloud Storage Column Store 001\011 
 Introduction With the popularity of e-commerce, mobile internet, wireless sensor networks applications the amount of data is increasing rapidly. Big data has 4V characteristics \(volume, velocity, variety value\n order to explore the value of big data, we have to deal with the large-scale \(volume\d rapid-growth \(velocity\ challenges for big data Distributed Key-Value DB is an alternative storage 
system for massive structured big data storag Distributed KEY-VALUE data storage systems can achieve high throughput and scalability even in PB-scale storage. Open source softwares include Mongo ot o C a bin e t  R e di s  C a ss an dra HBas etc K EYVAL U E DB n o w h as also  been widely used in commercial system, such as PNUTS in Yahoo y n a mo in Am azon 5], an d Big Table in Google e d on t h e s c h e m a th e current key-value DBs can be divided into two 
types, point query system and range query system The point query system puts or gets item is distributed based hash function. The range query system uses column-family schema to organize data attributes. Point query system supports fast to fetch a single item. For example DynamoDB can read an item in 50ms. But the point query can only support PUT or GET operations on the given key Data mining is a kind of large-scale data 
computing and the range query systems are needed The range query systems ar e more powerful than point query systems. They are cable of query large data sets. It can provide prefix query, range query or even wildcard query. The range query systems are efficient for batch data processing. However when the result data set become large, the query efficiency decreases dramatically. HBase hypertable are typical range query KEY-VALUE 
2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber Physical and Social Computing 978-0-7695-5046-6/13 $26.00 © 2013 IEEE DOI 10.1109/GreenCom-iThings-CPSCom.2013.222 1276 


Client subsystem Metadata subsystem Region server subsystem Network communication subsystem 
A The BC-kvDB framework create table drop table add a new server load balance B The BC-kvDB data model 
DB. They face with the problem of storage locality in distributed file system. For example, HBase stores data file into HDFS When the query starts Hbase region server first get data file from HDFS do the query computing locally and then return the results to client. Usually the region server is not the server which storage data file. The network traffic cost will become significant when the return result date set get larger At present, there are two methods to improve large-scale query efficiency The first method is to optimize Map Reduce process. For example impala introduces pipeline format between Map job and Reduce job. Alough it can improve query efficiency, when the date set is large enough for a single job the IO efficiency is still very low Powe ed d i s t ribu ted colum n ar storage structure to improve efficiency for OLAP query. RCfile introduces row and column mixed storage structure and it can improve the efficiency for statistical analysis[10 The d a ta f ile la y o ut i s  the key points for block scan, but until now there is no study on multi-layered cache for KYEVALUE DB, it will also greatly improve the query efficiency In this paper, we will discuss some key methods on improving the range-query efficiency in BCkvDB. We will introduce the framework and data model in BC-kvDB in section 2. The data localization storage structure will be introduced in section 3, and multi-layered cache for point and range query in section 4. In section 5, we give concrete experiments, and the brief conclusion described in section 6 001\012  Framework and Data Model BC-kvDB is a kind of range query distributed KEY-VALUE DB. The entire system includes four subsystems. As showed in figure 1, there are client subsystem, metadata subs ystem, region server subsystem and network communication subsystem Figure 1. The BC-kvDB components provides C/C++ API. It also provide multi-language programming interface through thrift framework. Some useful tools, such as command shell, web interface, unit test are also provided at client subsystem is composed of Zookeeper cluster and multi-master cluster. The function of BC-kvDB master is to support the basic operations within the cluster scope, such as   or etc Multi-master clusters incl ude two roles. One is primary master and the others are slave masters Primary master support the basic table space operations, and the slave masters are watching the primary master working state. If the primary master is down, one of the slave masters is elected as the primary master The zookeeper cluster stores the metalog for the whole cluster reliably. The metalog is used to recover the master state when the slave masters startup. The metalog includes the current primary master address, table schema, operation logs and the ROOT region location in the cluster consists of independent PC servers. These PC servers connect each other through high-speed Ethernet. Region server\(RS\ides a key-value data read and write service. BC-kvDB uses local data storage engine. For each client write operation, it will be first write into log, then to cache. Cache refreshes at periodical time to disk and produces data files locally. Because of the read and write is implemented on RS directly and can achieve better performance can provide high-speed data communication and message service. In BC-kvDB the client and RS send and receive data block directly. But when the client and RS communicate with master, they use the RPC message service based on the network communication subsystem The data model of BC-k vDB is single table space. And the table space is further partitioned by row-key. Each range of row-key are called region and stored in a region server. Each region includes many CS files. Region is the basic unit of distribution and its location information are recorded in metatable 
1277 


RangeServer:IP  ObRootMeta2 1024X1023  server_info  server_info  server_info  ObRootMeta2  ObRootMeta2  ObRootMeta2 tabletinfo  000W\000D\000E\000O\000H\000W\000B 000Y\000H\000U\000V\000L\000R\000Q 000B Others 001Ä\001Ä\001Ä\001Ä\001 0005\000R\000R\000W\0005\000D\000Q\000J\000H  MetaRange1  MetaRange2 0000\000H\000W\000D\0005\000D\000Q\000J\000H  0007\000D\000E\000O\000H\000W  0008\000V\000H\000U\000W\000D\000E\000O\000H 0002 000E\0005 000R\000R\000W\0000 000H\000W\000D\000\025\000\003 0002\000W\000K\000H\000U\000V    0005\000R\000R\000W\000W\000D\000E\000O\000H 1024X1023    Figure 2.The metatable structure for BC-kvDB Metadata table is a kind of central indexing for region distribution The metatable is distributed as the normal user table. When a table is created, a XML schema is also created and stored into zookeeper cluster. The schema has replica policy database name, column-family name, and other basic table space information When a client writes data into a table space, it can directly sent data to the dedicated RS. The RS first accepts the updated data and writes into local log, caching the data in local memory and then responds the client. The RS will flush the write cache into local file periodically. When a region size is beyond some threshold, the region is split into two new regions. The newly produced region will move to other RS in balance period 001\013 
 0000\000H\000W\000D\000W\000D\000E\000O\000H 
0000\000H\000W\000D\0005\000D\000Q\000J\000H 
 Data File Structure for BC-kvDB When the write cache is large enough, RS will create a new write cache for the updating data and the old write cache will flush into a local data file The data file in BC-kvDB is also called cell store as hypertable. The data file in composed of data blocks, bloom filter, block index and file trailer Each part is listed in the following figure Figure 3.Data file format in BC-kvDB The trailer is the data file metadata. Its size is 1024 bytes. It includes the total records number the total file length and the magic number. The block index is the index for all blocks in the file Each item in the block index has the offset and the end row-key for each data block Bloom filter is used to predict whether a given key in the cs file. It uses the bit array succinctly represents a set, and be able to predict if an element belongs to the set in O\(1\imes. Although bloom Filter has some false positive disadvantages the correct query process will be done in the data block finally, and the correct item will be fetched The data block store the compressed key/value pairs blocks and the concrete key structure lists as figure 4 Figure 4. KEY structure  Row key is a user defined unique key, with '\\0 for the end identifier. Column family are the attributes cluster define in the table schema Column qualifier is a column name under a column family Flag item indicates that if the item is newly created or has been deleted. At query time the deleted item will not be fetched. Timestamp is the reverse order of timestamp string, and the newly created item will be fetched firstly The new data file is created at the compact or split stage. In KEY-VALUE DB the compaction 
Entry point RootRange 
 000W\000D\000E\000O\000H\000W\000B 000Y\000H\000U\000V\000L\000R\000Q 000B  000W\000D\000E\000O\000H\000W\000B 000Y\000H\000U\000V\000L\000R\000Q 000B  0003\000R\000U\000W 
1278 


Cell Cache 
A Client Cache B Storage server cache 
and splitting are common methods to produced a new data files. In BC-kvD B the new data file are all stored in local file. For detailed compaction and splitting discussion, reader can refer to Hbase for further knowledge 001\014  Multi-level cache structure In BC-kvDB, we use multi-layered cache structure to increase the efficiency of reading and writing operations Client cache module is mainly used to cache the RS location information and the table schema information which the user retrieved. Client cache module store two types of data: table cache and location cache Table cache stores the retrieval information of table schema Location cache stores region information, such as start row end row and the location information of corresponding region server It used the hash map structure. When a user makes retrieval it will find the information from local Cache at first time. If there is, then get it directly from the cache. It will speed up the query process If the table schema changes, Master will modifies the corresponding table structure. Master also informs the main RS stores of the table, the main RS will update the table structure information on locally stored region. At the same time according to the local cach e backup plan, put a schema update request to the backup node, the backup nodes at the same time update the schema cache. When RS receives a client to retrieval The Region management module on RS would compare table structure version number. If it older than local cache table structure version number then update the table structure and return to the client. That makes it possible to update all of the client's local Schema cache Because of the Metadata table changes, it will lead to failure in the lo cation cache of client cache which led to the client failed to retrieve the corresponding region. The clients read the metadata table again, and update the location cache Storage server cache is divided into three modules: Cell Cache Query Cache and Block Cache Figure 5.storage server cache mechanism structure diagram module is used to store the write data cache in order to speed up the data write operation The interior of the modu le uses the RB-Tree to save the user input data And also it can support the sequential search and random search In order to ensure the data into the cell cache is safe, the user writes the key-value data to the commit-log at first, and then written to the cell cache. If the amount of data in the cell cache reached a certain threshold, then the data in cell cache by cell store module is written to th e file Finally returned a successful response to the user Data written process  1 The user writes the key-value data to the commit-log at first 002 Then the system according to the replica strategy synchronous writes the information to Log 2 Key-Value data will be written to the cell cache 3 It determines whether cell cache reaches the set threshold. If it not reached, then return and according to the need to accept user requests to write data 4 We should create a new cell cache and accept the write data 5 The full cell cache assigned to immutable cell cache, and then refreshes data to the cell store by immutable cell cache 6 Call the CellStore module to write data 
1279 


Query Cache Block Cache 
generate related block index, and written to disk file module is to accelerate the read operation Query cache saved the user retrieve result data. Query cache uses the LRU elimination strategy When new data is written to the cell cache if it has the same data in query cache, and then deletes the same one from query cache The internal of query cache used of the B-Tree structure, each data in the B-Tree format like this md5\(Tablename, ScanSpec\eryResult\(keyvalue 001 Table name means that the user to retrieve the name of the table; ScanSpec means that the last retrieval provides the retrieval conditions. Query cache uses the B-Tree structure to support fast random search and interval search When the user needs to query the specified data the system will determine existence of the same operation in the query cache at first If query cache have the same data 002 then read the required data directly from the query cache. If not find, then insert the query results into query cache When the data update ope ration is executed, first of all, according to the table name and scan spec to determine whether it exists the update data in the query cache, if it has the same data in query cache then delete it from query cache. When the query cache reached the set threshold, system will use LRU algorithm to delete the cache data Data reading process 1 Lock operating on QueryCache 2 When it has been 1000 times query operation since the last query, output the log information and show that the hit rate of QueryCache 3 Search the corresponding key form query cache If the key does not exist, it will return False. If query cache has the key, it will be stored in result and return true is a data file for the entire storage server. When a piece of data files accessed by the user, then the data block is added to the block cache. When users query the data in the block cache, you can return immediately to the specified data block. When it is not in the block cache, adopt the method of querying the data file. At the same time, the corresponding data block in the file is added to the block cache 001\015 Experiments The purpose of the experiment is to measure the read and write efficiency with a certain amount of data in BC-kvDB, at the same time gives a comparison test in the same environment with HBase .Through the contrast test shows that the superiority of the system in the performance The experimental used 15 servers for the performance test. During the test, we mainly grasp the trend of read and write performance. There were 13 nodes as a cluster of the BC-kvDB service and 2 nodes as a client Then the following is the system information: OS was centos5.5.CPU was 2way and 4-core cups, it had Gigabit Ethernet, 21tb hard disk and 16GB memory The Experiment used an AG and a column family as the test data pattern. The key of length is set to 12 KB the value of length is set to 1 KB the value of random and sequential read and write are the same construction method The experimental method is shown as follows 1 the cluster it already had a certain amount of 2\ the client we started the random writing process, and inserted data in BC-kvDB cluster. Statistical the data insertion efficiency and then record the random wr ite efficiency. \(3\After perform the random written, we had start the random reading process Read data in the BCkvDB cluster, Statistical data read efficiency and then record random read efficiency. \(4\e should record test results in the form Experimental data and test results are showed in the following table Figure 6 The throughput of the cluster In this figure, the abscissa indicates the scale of the current cluster data \(GB\; the vertical axis represents the throughput of the entire cluster KB/s\. This experiment mean that when the data is increasing, the throughput of the entire cluster still maintain a good state Figure 7 Process efficiency of reading and writing In this figure, the abscissa indicates the process of writing / reading \(MB data, the vertical axis represents that each client's random read-write 
0 10 20 30 40 50 60 Random write Random read MB/s 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Random write 2×100GB Random read 2×1GB Random read 2×10GB Random read 2×20GB current data total 200GB current data total 800GB ms 
1280 


0 0.5 1 1.5 2 2.5 3 3.5 4 Random write 2*100GB Random read 2*1GB Random read 2*10GB Random read 2*20GB HBase BC-kvDB ms 
efficiency. The experiment data shows that the BC-kvDB storage system can able to work efficiently in big data environment. The efficiency of read and write operations is still remained high efficiency with the amount of data increases and the transformation of the data environment And then in the same environment, experiments were performed in HBase, Start single client and pre split table into 100, at the same time random write 100 records with ten threads Figure 8 BC-kvDB and HBase experimental results  Experimental results show that the random writing performance of BC-kvDB is 2.5 times of HBase and its random reading performance is 1.82 times for HBase 001\016 Conclusions This article describes a high-performance distributed KEY-VALUE storage system \(BCkvDB\d the data access method. In BC-kvDB the key-value pairs are compressed into blocks and indexed in a local data file. The region server can retrieve local data file directly for the range query This can boost the query process for larger data set greatly. Besides, the cell cache, block cache and the shadow cache make the results are mostly hit in the local cache. The multi-layered cache can further increase the query efficiency. As space is limited, the detailed data processing mechanism for BC-kvDB are not introduced in this paper we will discuss them in other papers References 1 S QL D a tab a s e s D o n t S cal e  E B O L    2 01 10309  2 F Che n Z K  Y o ng W X  Z h i I nde x f o r Cassan d r a  a novel schema for multi-dimensional range queries in Cassandra[C  S e m a n t i c s Ko n wl e d g e a n d Gr i d  S KG  2011 IEEE International Conference on :130-136 3 H B a s e h tt p  ha d o op a p a c h e org hb a s e  4 B. F  Co o p e r R. R a m a kr is hn a n U   S r iv astav a P N U T S  Yahoo!'s hosted data serving platform. In Proceedings of the VLDB Endowment 2008, 1\(2\:1277-1288 5 G  DeC a n d i a D. Ha st or u n  M  Jamp an i Dy n a mo amazon’s highly available key-value store. In Proceedings of twenty-first ACM SIGOPS symposium on Operating systems principles \(SOSP’07\, 2007:205220 6 F  Chang J  D e an, G h e m aw at S  B i g t a b l e   A distr i b u te d  storage system for structured data. Usenix Association 7th Usenix Symposium on Operating Systems Design and Implementation, 2006: 205-218 7 D G i us e ppe H  D e niz  J   Ma da n K   G unav a r d ha n L   Avinash,Dynamo Amazon’s highly available key-value store[J  A C M  20 07 205 22 0 8 H ado o p  h t t p   h a d o o p ap ac he o r g  9 A H all O Bac h m an n  R B u sso w P r o ce ssing a T r il l i o n  Cells per Mouse Click [j V L D B 20 1 2 5 1 1  14 3 61446 10 Y  Q  H e  R L cc, Y  H u a i  R CF i l e A F a st an d S p a c e efficient Data Placement Structure in MapReduce-based Warehouse Systems.IEEE 27th International Conference,2011:1199-1208 
1281 


  Fig. 6.   2 years survivability area under the curve \(AUC ison of 65 attributes, 13 attributes and 13 attributes after S MOTE class balancing Fig. 5.  2 years survivability percentage accuracy comparison of 65 attributes, 13 attributes and 13 attributes after SMOTE clas s balancing 15 


   Fig. 8.   5 years survivability area under the curve \(AUC ison of 65 attributes, 13 attributes and 13 attributes after S MOTE class balancing Fig. 7.  5 years survivability percentage accuracy comparison of 65 attributes, 13 attributes and 13 attributes after SMOTE cla ss balancing 16 


overhead of job initialization in Hadoop is much larger than cNeural VIII C ONCLUSION AND F UTURE W ORK The past several years have witnessed an ever-increasing growth speed of data To address large scale neural network training problems in this paper we proposed a customized parallel computing platform called cNeural Different from many previous studies cNeural is designed and built on perspective of the whole architecture from the distributed storage system at the bottom level to the parallel computing framework and algorithm on the top level Experimental results show that cNeural is able to train neural networks over millions of samples and around 50 times faster than Hadoop with dozens of machines In the future we plan to develop and add more neural network algorithms such as deep belief networks into cNeural in order to make further support training large scale neural networks for various problems Finally with more technical work such as GUI done we would like to make it as a toolbox and open source it A CKNOWLEDGMENT This work is funded in part by China NSF Grants No 61223003 the National High Technology Research and Development Program of China 863 No 2011AA01A202 and the USA Intel Labs University Research Program R EFERENCES  C Bishop Neural networks for pattern recognition  Clarendon press Oxford 1995  J Collins Sailing on an ocean of 0s and 1s  Science  vol 327 no 5972 pp 1455…1456 2010  S Haykin Neural networks and learning machines  Englewood Cliffs NJ Prentice Hall 2009  R Hecht-Nielsen Theory of the backpropagation neural network in Proc Int Joint Conf on Neural Networks,IJCNN IEEE 1989 pp 593…605  Y  Loukas  Arti“cial neural netw orks in liquid chromatography Ef“cient and improved quantitative structure-retention relationship models Journal of Chromatography A  vol 904 pp 119…129 2000  N Serbedzija Simulating arti“cial neural netw orks on parallel architectures Computer  vol 29 no 3 pp 56…63 1996  M Pethick M Liddle P  W erstein and Z Huang P arallelization of a backpropagation neural network on a cluster computer in Proc Int Conf on parallel and distributed computing and systems PDCS  2003  K Ganeshamoorthy and D Ranasinghe On the performance of parallel neural network implementations on distributed memory architectures in Proc Int Symp on Cluster Computing and the Grid CCGRID  IEEE 2008 pp 90…97  S Suresh S Omkar  and V  Mani P arallel implementation of back-propagation algorithm in networks of workstations IEEE Trans Parallel and Distributed Systems  vol 16 no 1 pp 24…34 2005  Z Liu H Li and G Miao Mapreduce-based backpropagation neural network over large scale mobile data in Proc Int Conf on Natural Computation ICNC  vol 4 IEEE 2010 pp 1726…1730  M Glesner and W  P  ochm  uller Neurocomputers an overview of neural networks in VLSI  CRC Press 1994  Y  Bo and W  Xun Research on the performance of grid computing for distributed neural networks International Journal of Computer Science and Netwrok Security  vol 6 no 4 pp 179…187 2006  C Chu S Kim Y  Lin Y  Y u  G  Bradski A Ng and K Olukotun Map-reduce for machine learning on multicore Advances in neural information processing systems  vol 19 pp 281…288 2007  U Seif fert  Arti“cial neural netw orks on massi v ely parallel computer hardware Neurocomputing  vol 57 pp 135…150 2004  D Calv ert and J Guan Distrib uted arti“cial neural netw ork architectures in Proc Int Symp on High Performance Computing Systems and Applications  IEEE 2005 pp 2…10  H Kharbanda and R Campbell F ast neural netw ork training on general purpose computers in Proc Int Conf on High Performance Computing HiPC  IEEE 2011  U Lotri  c and e a Dobnikar A Parallel implementations of feed-forward neural network using mpi and c on  net platform in Proc Int Conf on Adaptive and Natural Computing Algorithms  Coimbra 2005 pp 534…537  Q V  Le R Monga and M e a De vin Building high-le v e l features using large scale unsupervised learning in Proc Int Conf on Machine Learning ICML  ACM 2012 pp 2…16  J Ekanayak e and H e a Li T wister a runtime for iterati v e mapreduce in Proc of the 19th ACM International Symposium on High Performance Distributed Computing  ACM 2010 pp 810…818  Y  Bu B Ho we M Balazinska and M D Ernst Haloop Ef“cient iterative data processing on large clusters Proc of the VLDB Endowment  vol 3 no 1-2 pp 285…296 2010  M Zaharia M Cho wdhury  T  Das A Da v e  J  Ma M McCauley M Franklin S Shenker and I Stoica Resilient distributed datasets A fault-tolerant abstraction for in-memory cluster computing in Proc USENIX Conf on Networked Systems Design and Implementation  USENIX Association 2012 pp 2…16 384 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


