The Realization of Distributed Sampling Association Rule Mining Algorithm in Tourism  Du Junping  Zuo Min and Tu Xuyan  Beijing Key laboratory of Intelligent Telecommunications Software and Multimedia  School of Information Science Beijing University of Posts and Telecommunications, Beijing 100876,China University of Science and Technology Beijing, 100083, Beijing China  junpingdu@126.com tuxuyan@126.com   Abstract In this paper, we introduce the concept of the holiday tourism information data mining, improve a distributed sampling association rule mining algorithm: DS-ARM, define the realization process of the algorithm and test the capability of the algorithm and use the algorithm in the analysis of the holiday traveler destination traveling behavior    Index Terms Holiday Tourism; Data Mining; Association Rules  I  INTRODUCTION  Recently, holiday tourism has developed rapidly and become a new economic growth-point of national economy The rise of holiday tourism provides us new developing opportunity, yet, many problems remains due to the wideness and uncertain factors of holiday tourism. For instance, the reception status of tourism sites is strongly imbalanced, some hot sites being overloaded received, while few visitors in some new exploited ones; the quality and security of tourism service is getting increasingly severe, and a large rise of tourism complaints, because of no macroscopic hold and monitor of the receiving ability of hotels and tourism sites holiday tourist flow volume and passenger traffic flow, etc While tourists encounter enormous information from the internet, they are laboring under severe shortage of the needed knowledge and information. Thus, it is n ecessary to mine network mass data in knowledge discovery  In traditional network computing mode, the mining task is executed by the client, and the network volume is very large between the client and the server, which restricts the mining efficiency of network mass data seriously. At the same time the mobility and robustness of the system decreases, because of the computing mode requests the connect remains keeping of the client and the server. With the development of the internet, distributed computing mode is popular used. We can combine artificial intelligence, modeling, intelligent information pressing and other theory and technology effectively, collect, mine and analyze holiday tourism information by advanced information and technology and approaches, finish the improvement of distributed sampling association rule mining algorithm, and apply it in the holiday   This work was supported by the National Natural Science Foundation under grant no.60773112 and Beijing Natural Science foundation \(4082021 tourism data mining. Thereby, we can sort and analyze the national holiday tourism information comprehensively and systematically, steer tourist holiday consumption behavior and further promote the healthy and ordered development of holiday tourism [1   II  A SSOCIATION R ULE D ATA M INING T ECHNOLOGY  A  Basic concept of Data Mining Data Mining is extraction of interesting knowledge from data in large databases or data set. The knowledge is implicit unknown and potential useful information and the extracted knowledge is expressed as conception, rule, law, mode and other types. Besides data mining, there are other main names Knowledge Extraction, Information Discovery, Intelligent Data Analysis, Information Acquisition, and Knowledge Discovery in Database, etc [2    B  Association Rule Mining Algorithm  Association Rules reflects the reliance and association of an incident and other incidents, and the data association in database is the represent of the relevancy of things in real world. As a structural data organization form, database portrays the association of the data by its adherent data mode likelihood [3  Asso c iation rule mining is the mast common used method of association knowledge discovery, with Apriori and its improved algorithm presented by Agrawal as the most famous one. Two threshold value: minimum support and minimum confidence should be given in order to find more meaningful association rules. The mined association rule must meet the minimum support prescribed by the client which shows the lowest association degree of the association of a group of items. And the association rule also has to meet the minimum support prescribed by the client, which reflects the lowest confidence of an association rule With the rapid development and mature of relational database, especially the development of data warehouse technology, a large number of data storage in the relational database or data warehouse. The types of relational database and data warehouse are various, and numerical attribute is the most popular type of attribute in data relations, yet association rule is an important knowledge concealed in attribute of database relations. Therefore, a research on association rule mining algorithm applicable to relational 978-1-4244-2114-5/08/$25.00 © 2008 IEEE 183 Proceedings of the 7th World Congress on Intelligent Control and Automation June 25 - 27, 2008, Chongqing, China 


database, i.e. Quantitative Association Rules, QAR, has practical significance. For example, in a travel agency database, the age and vocation temporal of tourists are numerical attributes, which classic association rules can not deal with. Consider rules: \(Age v1,v2  v 1  v 2] v o c a t i on temporal [n1,n2  shows when the age of the tourist is in the rang of [v1,v2 h e m a y h a v e a t o ur i n  a pe r i o d o f n 1 n2   This rule reflects the probability of different ages travel at a certain time. Obviously, it is important to the marketing strategy of a travel agency. Therefore, association rules just can resolve problems that All Boolean Association Rules can not  III   D ISTRIBUTED SAMPLING ASSOCIATION RULE MINING ALGORITHM DS-ARM A  Specification of problem Set  m 2 1 i   i  i I   as an item of specified domain. A subset of I is an item set. Every Transaction has a unique identifier TID. Database DB is a set of transactions. Divide DB into n parts, set   n 2 1 DB   DB  DB DB   as one of them Set S as a set of transactions sampled from DB  n 2 1 S   S  S S    as one part of S which is included in DB For specified item set X and Transaction group A, support degree function  A  X  Support  is number of all items of X in A. And frequent degree function A  A  X  Support  A  X  Freq    DB  X  Freq i is called the local frequent degree of  X in I  DB  X  Freq i the  global frequent degree. In a similar way  S  X  Freq i is called the local Estimation frequent degree, and  S  X  Freq the global Estimation frequent degree For specified threshold 1 MinFreq 0   if MinFreq  A  X  Freq  item set X is considered frequent in A, On the contrary, is infrequent.. If A is a sampling, X is estimation frequent or estimation infrequent. If A is a part, X is local frequent; if A is the database, X is global frequent. So a item set may be evaluated as local frequent in k parts, while infrequent in global scope. Collect all items which frequent degrees are above or equal to r f as a set, calls  A F fr Items which are not in  v A F fr  but all of its subsets are in  A F fr  constitutes a set which is called the negative boundary Most of distributed association rules algorithms are based on Apriori algorithm, which needs to scan database multiple times. Sampling algorithm has no form of distribution because distributed algorithm would greatly increase the complexity of communication. Communication complexity highly depends on number of candidate item sets and noise grades of database segmentation. And when sampling algorithm makes a process of sampling and reduces threshold MinFreq, it also increases number of candidate item sets and noise grades, which makes normal distributed algorithm mean less [5   To solve the problem mentioned above, we bring forward DDM algorithm which makes a mining of distributed sample data, and fr _ low  takes place of MinFreq as threshold When   S F fr _ low is specified, make a scan of every part of database to find actual frequent set and negative boundary of   S F fr _ low Finally, integrate every distributed frequent set and make a rule of threshold MinFreq  The algorithm has been improved in followed ways 1\ DDM algorithm is a progressive Refinement algorithm to deal with data stored in memory. To decrease the disk I/O overhead, we improve the algorithm to generate all item sets when it starts, load them in memory just one time As every part of database is small, it will not need to read from disk when calculate the estimation frequent degree of these item sets 2\ We propose a new way to get fr _ low The method does not need heuristic function, and can produce less candidate item sets. After the database is scanned, there is no use to integrate all candidate frequent item sets, as few of sets may be frequent in MinFreq DDM algorithm is used again to confirm which sets are frequent or infrequent  B   Description of algorithm Firstly DS-ARM algorithm loads sample data to memory Sample data are stored in trie tree. Trie tree is main data structure of DS-ARM algorithm which can be accessed by all subroutines. Every node in trie tree stores structure information \(parent pointer, son node pointer est.\ and list of transactions TTD of item set related to this node. During the process of initiation, nodes of first layer are generated firstly      When new trie nodes and Corresponding item sets are produced, make TTD list of two direct parent nodes orthogonal, then get a TTD list of new node. The first step of algorithm is to run improved DDM algorithm to mine distributed samples, next, algorithm goes to next loop to specify fr _ low every loop calls routine derived from another DDM algorithm to mine next M frequent degree Estimation  item sets. M is an adjustable reference, default value is 100. When the additional sets have been found algorithm decreases value of fr _ low and evaluates frequent degree of the minimal number item sets, reappraises the error probability. If the error probability is within a cceptable scope loop stops. By negative boundary of   S F fr _ low  algorithm acquires final candidate item sets Input MinFreq  MinConf  i DB s,M   Output: association rule of global frequent items Main Set 1 error _ p   MinFreq fr _ low   Load sample data i S from i DB to memory which capacity is s Initialize trie tree with all frequency 1 items and calculate TID list of every node 184 


Acquire     MinFreq  MDDM S F fr _ low  from data mining algorithm while    error _ p   1      M  Max _ M S F S F fr _ low fr _ low    2 set fr _ low  as minimized frequent degree of   S F fr _ low  3 get erro r _ p from MinFreq  fr _ low and   S F fr _ low   Set      S F  Border _ Negative S F C fr _ low fr _ low   scan database and calculate  DB  c  Freq i for every C c  Update frequent degree of new trie tree and run  MinFreq  MDDM algorithm to calculate   DB F MinFreq   with the actual frequent degree If   DB F c MinFreq  exists and   S F c fr _ low  equal to c within negative boundary, return error information    MinConf  DB F  Rules _ Gen MinFreq algorithm is over When candidate item sets are created, scan every part of the database parallelly; calculate actual frequent degree of every candidate item set. To calculate the frequent degrees DS-ARM algorithm needs to run MDDM algorithm again with original MinFreq Except error occurs, the mining process will not produce candidate item sets beyond the negative boundary. If no error exists, all frequent item sets will be evaluated based on actual frequent degree acquired from database scanning. So which item sets in C are global frequent item sets can be acquired. Rules generated from   DB F MinFreq  have global frequent attribute If an item set belongs to negative boundary of   S F fr _ low  can be proved as frequent set, DS-ARM has mistake: A superset of candidate set may be a frequent set, but not take into account. When the matter happens, a solution put forward by Toivonen can solve the problem [6   C r ea t e a g r o u p  o f  candidate sets which compose all expected and unexpected frequent set, do another additional search. Capacity of additional candidate set should less than the composition of expected item sets and unexpected item sets product .Due to few mistake may occur, the possibility of multi-mistake decreases in exponential level. So the system cost of additional search is at the same level of first search  C  DS-ARM algorithm Performance test There are two Clusters are deployed: the first and the second experiments are operated on Cluster A. Cluster A has 15 computers which have P4 1.7G CPU and 1G memory. The computers constitute 100M Ethernet network. Cluster B runs the third algorithm extension experiment, which has 32 computers. The computer has AMD Duron600 CPU and 256M memory. Computers constitute 100M Ethernet network Data used to experiment comes from http://www.almaden.ibm.com/cs, the web site provides a tool which can produce standard test database. Database capacity is 18G, which is specified by T5.I2.D600M. The database has 600M transactions, every transaction has 5 items with average length of mode is 2.Use n  TID  method to deal database with random segmentation  1\Acceleration test The purpose designing the acceleration text is to check the parallel ability of DS-ARM. First, run DS-ARM on n=1 computer. The algorithm declines to normal Sampling algorithm at this time. Then distribute database to n computers to check the influence to the algorithm performance. As Fig.1 shows, when MinFreq is small  the acceleration of DS-ARM algorithm is basically linear. With the increase of candidate sets, the acceleration gradually develops linearly. This is because with the increase of computing nodes, the global sampling is added which results in the increase of fr _ low  s  Fig. 1  Test for DS-ARM acceleration  2\inFreq dependency test The second experiment is the test of DS-ARM algorithm performance dependency to MinFreq the same to test of dependency to number and capacity of candidate item sets Make a comparison of FDM,DDM and DS-ARM, DS-ARM is insensitive to the decrease of MinFreq which means DS-ARM algorithm only scan database for single time and is better than any progressive Refinement distributed algorithm based on Apriori algorithm 185 


  Fig.2  Test for performance comparison 3\ Extension test The third test is to check extension of DS-ARM algorithm. When database segments are fixed, a strong extension algorithm which runs on different number computers may cost about the same time, and its performance and efficiency is better than operation on single computer     Fig. 3 Test for DS-ARM Extension  When DS-ARM runs, communication burden produced by every candidate set almost the same as DDM algorithm DS-ARM produces more candidate sets, the more communication it needs. From Fig.3, DS-ARM has extension attribute in two tests. For middle scale computer cluster, the speed of DS-ARM operation is faster than single computer due to super linear acceleration. T5.I2.D1200M triangle illustration presents a bad extension because candidate sets of T5.I2.D1200M are small and average mode is short. The more candidate sets, the more advantage they brings as calculate nodes grows. If the scale of mode is big enough, it can compensate the additional burden caused by communication  D  Analysis of Algorism Test The individual attribute information of the tourists and tourism products information booked by them are distributed in many data servers. In accordance with the database produced by distributed association rule mining, DS-ARM Algorithm predicts on the tourism sites preferred by the tourists. Data have been used are as follows: the tourists information table includes: tourists ID, age, gender profession, income, education, and workload, totally seven attributes; product table includes product ID and quality product passing sites table includes product ID, and tourism sites; tourism sites table includes the sites ID, name, locus and quality; and tourists passing sites table includes tourists ID product ID, and travel temporal. To divide the tourists into different categories, age and income of the tourists should be discretized. First, divide tourists into different categories according to their age, gender, profession, income, education and workload. Then, compute different preferences of each category. Finally, we can get traveling routes of different kind of tourists by using DS-ARM Algorithm. The structure of knowledge base is as table 1 and table 2 showed  TABLE I TOURISTS INFORLMATION TABLE  Group Start Age Gender Profession 1 21 Male Sales person 2 21 Female Teacher 3 31 Male Civil servant 4 31 Female Teacher Income Upper Income Lower Workload Education 3000 2000 Medium Bachelor 3000 2000 Medium Postgraduate 3000 2000 Medium Academic 3000 2000 Medium Bachelor  TABEL   TOURISTS INFORMATION TABLE Group Tourism Nature Tourism Route 1 Landscape Tour Zhan Bridge  Qingdao Bathing Beac h Laoshan Mount Tai  Twin Peaks Mount  Qia n Mountain  Wufengshan  Yunmenshan  Laoshan 2 Cultural Relics Tour Tomb of the King of the Sulu State  Hal l Immolated Horses of the Eastern Z h Dynasty, the Lion Chamber  Tomb Dongfang shuo  Taiqing Palace  Cave of Morning Mist and Sunset Glow  Lingyan Temple 3 Folk Custom Tour Zhou Cun  Kite Workshop  The Pu SongLing Museum  Tianheng Island 4 Landscape Tour Huanglong  Jiuzaigou  Leshan  Muni Valley Confucius Temple  Lingyan Temple  Mencius Temple  Mount Tai  With the association rules the tourists behavior in their destination and the tourists preferences can be predicted thereby, hot tourism sites and peak period can be forecasted based on the tourists information. Through China tourism destination marketing system developed by our group, one can make journey layout before the trip of the tourists, arrange 186 


and recommend the journey properly, and make positive effects on discharging tourists in their tourism destination  IV  C ONCLUSION  This paper presents an improved distributed association rule mining algorithm, DS-ARM Algorithm, which is a collateral sampling algorithm. Through testing, it proved that this algorithm has ultralinear acceleration, and outperform the FDM Algorithm and DDM Algorithm. This algorithm uses single database scan, and is much superior to those level wise and multiple-scan, like FDM and DDM. The result of the test shows that DS-ARM Algorithm has preferable expansion performance. Mining tourism parameters and its development and changes concealed in mass holiday tourism information and helping tourism manager establish macroscopic concept of dynamic holiday tourism can provide important decision-making basis for managers to predict holiday tourism status accurately, discharging tourist source properly and ensure the healthy and ordered development of holiday tourism R EFERENCE  1   D u  j unp i n g  A n al ys i s of C h i n a  ho l i d a y t r ave l p h en om en on   C h i n a  travel greenbook-Analysis and expectation of China travel  situation, Dept of travel research of Chinese Academy of social sciences, 2001 2   S h i  z hon gzh i   K n ow l e dg e D i s c o v e r y  T s i n gh ua U n i v e r s i t y P r es s  2002 3 R  S r i k an t  and  R  A g r a w a l  M i n i n g gen e r al i z ed  r u l e s  I n P r o c   1995  Int  Conf.Very Large Databases. 407-419 4 Q u i n l a n  p r og r a m s  f o r  m a c h i n e l e ar ni ng  Mo r g an K a uf m a n n   C 4 5 S a n Mateo. CA.1993 5   L i n j i eb i n g  L i u m i ng de   C h en  x i ang   D a t a m i ni ng a n d O L A P T h eo r y  and  practice, Tsinghua University Press, 2002 6  M  J   Z a ki   S P A D E  A n ef f i c i en t  al go r i t h m f o r m i n i n g f r e q u e nt   sequences  Machine learning. 40:31-60, 2001      187 


 Fig 4 Decision-tree model of customer group classification The final decision-tree which is shown in Fig 4 also can be used to express implicated knowledge by using IF-THEN form. Along with the route that starts with root node and end in the final leaf node, the decision rule is extracted as follow IF 15 000 Age 000 30 AND Vocation = Student THEN S IF 15 000 Age 000 30 AND Vocation = Staff AND 600 000 Monthly income 000 3500 THEN Y IF 15 000 Age 000 30 AND Vocation = Staff AND 3500 000 Monthly income THEN C IF 30 000 Age 000 45 AND Vocation = Student THEN S IF 30 000 Age 000 45 AND Vocation = Staff AND 600 000 Monthly income 000 3500 THEN Y IF 30 000 Age 000 45 AND Vocation = Staff AND 3500 000 Monthly income THEN C It is shown in Fig 4 that China Railway Express' customers are classified into three groups. That is on Mother's Day, the business mainly concentrate on customers with higher income group, youth whose income is not very high and students. The express delivery service for flower presents health products and cosmetic is adopted mainly by a high income crowd. Youth often choose to the express delivery service for flower presents, and students often choose the Mother's Day special-purpose postcard express service  3\ Sales Strategies for China Railway Express Association analysis and customer classification is carried on by using two kinds of data mining technologies association rule and decision-tree analysis method According to the result which is given above, some sales strategies are presented for China Railway Express Enterprise in order to promote customer consumption, improve customer satisfaction, increase enterprise sales volume and set up long-term customer relationship 017p 000\002 Pay attention to customer information collection and data analysis, mining association of service and products, and realize customer group classification 017q 000\002 Provide different express delivery services to different customer groups. For example, on Mother's Day, the express delivery service of fresh flowers, cosmetic and health products is mainly provided for customers with higher income, they are the primary sales object. And the express delivery service of flowers, postcards is mainly provided for young people and students 017r 000\002 Drive cross-selling's application in China Railway Express with the association of service and products For example, if customers buy the express delivery postcard on Mother's Day, their mothers can receive the carnation flowers which represent customers compliment; if customers choose health products express delivery service, flowers express delivery service will be free for them These strategies can be used to promote sales. It not only improves the efficiency of selling, but also reflects the real value of data mining  IV  CONCLUSION For the express delivery companies carrying on cross-selling, the modern data mining technology offered a convenient technological tool which easily obtains customer's information. Cross-selling launching the popularization of pertinence can reduce the expenses on customer contacting improve the customer service level, and respond the platform of customer's demand in real time REFERENCE   ans i, L   Sargeant, A  2 0 00 Market segmentation in the Indonesian banking sector: the relation ship between demographics and desired customer benefits. International Journal of Bank Marketing, 64-74  i a n g, E n lighte n m e nt of Inter n a tional E x pre s s Ente rpris e s  Development. Logistics Technology, 2007, 18-20  Felix FO DM 9: f u zz y dec i s i on m a king with inte rac ting g o a l s  applied to cross-selling decisions in th e field of private customer banking The 10th IEEE International Conference on Fuzzy Systems, 2001, 964  967  etic algorithm  for item selection with cross-selling considerations. Proceedings of 2005 International Conference on Machine Learning and Cybernetics, 2005, 3293  3298  e i Lv, Jie C h en, Cons truction and application of cross-selling model of retail bank in China, Proceedings of ICSSSM '05. 2005 International Conference on Service Systems and Services Management, 2005, 121  125  P ang J i e   Li R uixian Hu J i a nhua The Study o f  D a t a Mining I n the  F i eld of  Telecommunication Cross-selling, Comp uter Knowledge and Technology Academic Exchange\ 2005, 112-112,126  e is ser, M. & W a ltm an R, Organizing Today for the Digital Marketing of Tom or row,Journal of Interactive Marketing, l,3 1-46  i a w e i H a n Mic h eline Ka m b e r  D a ta Mining Conce p ts a n d T e c h niq u e s   Mechanical Industry Publishi ng House, Beijing, 2001  u an R e n, Xianguo Li, Cros sselling, Chinese So cial Sciences Publishing House, 2004 10 The C R ISP-DM M o del, http www crisp-dm.org/Process/index.htm 11 Rakes h Agrawal Tom a s z  Im i e lin ski, Arun Swami, Mining association rules between sets of items in large databases  ACM SIGMOD Record   1993 pp. 207-216   


Used-for references in the LCSH into holonym/meronym relations in our WKB  In the experiments we assume that each topic comes from an individual user We attempt to evaluate our model in an environment that covers great range of topics However it is not realistic to expect a participant to hold such great range of topics in personal interests Thus for the 50 experimental topics we assume each one coming from an individual user and learn her his personalized ontology An LIR is collected through searching the subject catalogue of Queensland University of Technology QUT Library 3 by using the title of a topic Librarians have assigned title table of content summary and a list of subjects to each information item e.g a book stored in QUT library The assigned subjects are treated as the tags in Web documents that cite the knowledge in the WKB  In order to simplify the experiments we only use the librarian summarized information title table of content and summary to represent an instance in an LIR  All these information can be downloaded from QUT's Web site and are available to the public Once the WKB and an LIR are ready an ontology is learned as described in Section 3.3.1 and personalized as in Section 3.3.2 The user con\002dence rates on the subjects are speci\002ed as in Section 3.3.3 A document d i in the training set is then generated by an instance i  and its support value is determined by support  d i   X s 2 021  i  s 2S sup  s Q  14 where s 2 S in O  Q  are as de\002ned in De\002nition 5 As sup  s Q   0 for s 2 S 000 according to Eq 11 the documents with support  d   0 go to D 000  whereas those with support  d   0 go to D   4.4 Performance Measures The performance of the experimental models are measured by three methods the precision averages at eleven standard recall levels 11SPR the mean average precision MAP and the F 1 Measure They are all based on precision and recall the modern IR evaluation methods The 11SPR is reported suitable for information gathering and is used in TREC evaluations as a performance measuring standard An 11SPR v alue is computed by summing the interpolated precisions at the speci\002ed recall cutoff and then dividing by the number of topics P N i 1 precision 025 N  025  f 0  0  0  1  0  2      1  0 g  15 N is the number of topics and 025 are the cutoff points where the precisions are interpolated At each 025 point an aver3 http://library.qut.edu.au Figure 2 Experimental 11SPR Results age precision value over N topics is calculated These average precisions then link to a curve describing the recallprecision performance The MAP is a stable and discriminating choice in information gathering evaluations and is recommended for measuring general-purpose information gathering methods The average precision for each topic is the mean of the precision obtained after each relevant document is retrieved The MAP for the 50 experimental topics is then the mean of the average precision scores of each of the individual topics in the experiments The MAP re\003ects the performance in a non-interpolated recall-precision fashion F 1 Measure is also well accepted by the information gathering community which is calculated by F 1  2 002 precision 002 recall precision  recall  16 Precision and recall are evenly weighted in F 1 Measure For each topic the macro F 1 Measure averages the precision and recall and then calculates F 1 Measure whereas the micro F 1 Measure calculates the F 1 Measure for each returned result and then averages the F 1 Measure values The greater F 1 values indicate the better performance 5 Results and Discussions The experiments attempt to evaluate our proposed model by comparing to an implementation of mental model We expect that the ONTO model can achieve at least the close performance to the TREC model The experimental 11SPR results are illustrated in Fig 2 At recall point 0.3 the TREC model slightly outperformed the ONTO model but at 0.5 and 0.6 the ONTO model achieved better results than the TREC model subtly At all other points their 11SPR results are just the same For the MAP results shown on Table 1 the ONTO model achieved 0.284 which is just 0.006 below the TREC model 2 
512 
516 


TREC ONTO p-value Macro-FM 0.388 0.386 0.862 Micro-FM 0.356 0.355 0.896 MAP 0.290 0.284 0.484 Table 1 Other Experimental results downgrade For the average macroand microF 1 Measures also shown on Table 1 the TREC model only outperformed the ONTO model by 0.002 0.5 in macro F 1 and 0.001 0.2 in micro F 1  The two models achieved almost the same performance The evaluation result is promising The statistical test is also performed on the experimental results in order to analyze the evaluation's reliability As suggested by we use the Student's Paired T-Test for the signi\002cance test The null hypothesis in our T-Test is that no difference exists in two comparing models When two tests produce substantially low p-value usually  0.05 the null hypothesis can be rejected In contrast when two tests produce high p-value usually  0.1 there is not or just little practical difference between two models The T-Test results are also presented on Table 1 The pvalue s show that there is no evidence of signi\002cant difference between two experimental models as the produced pvalue s are quite high  p-value 0.484\(MAP 0.862\(macroFM and 0.896\(micro-FM far greater than 0.1 Thus we can conclude that in terms of statistics our proposed model has the same performance as the golden TREC model and the evaluation result is reliable The advantage of the TREC model is that the experimental topics and the training sets are generated by the same linguists manually They as users perfectly know their information needs and what they are looking for in the training sets Therefore it is reasonable that the TREC model performed better than the ONTO model as we cannot expect that a computational model could outperform a such perfect manual model However the knowledge contained in TREC model's training sets is well formed for human beings to understand but not for computers The contained knowledge is not mathematically formalized and speci\002ed The ONTO model on the other hand formally speci\002es the user background knowledge and the related semantic relations using the world knowledge base and local instance repositories The mathematic formalizations are ideal for computers to understand This leverages the performance of the ONTO model As a result as shown on Fig 2 and Table 1 the ONTO model achieved almost the same performance as that of the TREC model 6 Conclusions In this paper an ontology-based knowledge IR framework is proposed aiming to discover a user's background knowledge to improve IR performance The framework consists of a user's mental model a querying model a computer model and an ontology model A world knowledge base is used by the computer model to construct an ontology to simulate a user's mental model and the ontology is personalized by using the user's local instance repository The semantic relations of hypernym/hyponym holonym/meronym and synonym are speci\002ed in the ontology model The framework is successfully evaluated by comparing to a manual user model The ontology-based framework is a novel contribution to knowledge engineering and Web information retrieval References   C Buckley and E M Voorhees Evaluating evaluation measure stability In Proc of SIGIR 00  pages 3340 2000   R M Colomb Information Spaces The Architecture of Cyberspace  Springer 2002   D Dou G Frishkoff J Rong R Frank A Malony and D Tucker Development of neuroelectromagnetic ontologies\(NEMO a framework for mining brainwave ontologies In Proc of KDD 07  pages 270279 2007   S Gauch J Chaffee and A Pretschner Ontology-based personalized search and browsing Web Intelligence and Agent Systems  1\(3-4 2003   X Jiang and A.-H Tan Mining ontological knowledge from domain-speci\002c text documents In Proc of ICDM 05  pages 665668 2005   J D King Y Li X Tao and R Nayak Mining World Knowledge for Analysis of Search Engine Content Web Intelligence and Agent Systems  5\(3 2007   D D Lewis Y Yang T G Rose and F Li RCV1 A new benchmark collection for text categorization research Journal of Machine Learning Research  5:361397 2004   Y Li and N Zhong Mining Ontology for Automatically Acquiring Web User Information Needs IEEE Transactions on Knowledge and Data Engineering  18\(4 2006   H Liu and P Singh ConceptNet a practical commonsense reasoning toolkit BT Technology  22\(4 2004   A D Maedche Ontology Learning for the Semantic Web  Kluwer Academic Publisher 2002   S E Robertson and I Soboroff The TREC 2002 002ltering track report In Text REtrieval Conference  2002   M D Smucker J Allan and B Carterette A Comparison of Statistical Signi\002cance Tests for Information Retrieval Evaluation In Proc of CIKM'07  pages 623632 2007   X Tao Y Li and R Nayak A knowledge retrieval model using ontology mining and user pro\002ling Integrated Computer-Aided Engineering  15\(4 2008   X Tao Y Li N Zhong and R Nayak Ontology mining for personalzied web information gathering In Proc of WI 07  pages 351358 2007   T Tran P Cimiano S Rudolph and R Studer Ontologybased interpretation of keywords for semantic search In Proc of the 6th ICSW  pages 523536 2007   Y Y Yao Y Zeng N Zhong and X Huang Knowedge retrieval KR In Proc of WI 07  pages 729735 2007 
513 
517 


TESTS IN SECOND t INDICATES nl WAS LOWERED TO 2 Training BSTC Top-k RCBT 7 OC Holdout Validation Results RCBT outperforms BSTC on the single test it could finish by more then 7 although it should be kept in mind that RCBT's results for the 24 unfinished tests could vary widely Note that BSTC's mean accuracy increases monotonically with training set size as expected At 60 training BSTC's accuracy behaves almost identically to RCBT's 40 training accuracy see Figure 6 4 Ovarian Cancer OC Experiment For the Ovarian Cancer dataset which is the largest dataset in this collection the Top-k mining method that is used by RCBT also runs into long computational times Although Top-k is an exceptiounally fast CAR group upper bound miner it still depends on performing a pruned exponential search over the training sample subset space Thus as the number of training samples increases Top-k quickly becomes computationally challenging to tune/use Table VI contains four average classification test run times in seconds for each Ovarian Cancer\(OC training size As before the second column run times each give the average time required to build both class 0/1 BSTs and then use them to classify all test's samples with BSTC Note that BSTC was able to complete each OC classification test in about 1 minute In contrast RCBT again failed to complete processing most classification tests within 2 hours Table VI's third column gives the average times required for Top-k to mine the top 10 covering rule groups upper bouhnds for each training set test with the same 2 hour cutoff procedure as used for PC testing The fourth column gives the average run times of RCBT on the tests for which Topk finished mining rules also with a 2 hour cutoff Finally the  RCBT DNF column gives the number of tests that RCBT was unable to finish classifying in  2 hours each THE OC TESTS THAT RCBT FINISHED Training BSTC RCBT 40 92.05 97.66 60 95.75 96.73 80 94 12 98.04 1-133/077 9380 96.12 1070 cJ CZ C 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 BSTC RCBT d Median Median  Mean 260 Near outliers  Far outliers 40 Training 60 Training 0.90.80.70.6BSTC RCBT a 80 Training 1-52/0-50 Training 0.9DNFI 0.80.70.6BSTC RCBT b 1 u0.9DNFI 0.80.70.6BSTC RCBT  RCBT DNF 40 30.89 0.6186 273.37 0/25 60 61.28 41.21  5554.37 19/25 80 71.84  1421.80  7205.43 t 21/22 TIMES FOR THE OC 9 Mean 0 Near outliers  Far outliers 1.01 11 01 1.0 d Fig 6 PC Holdout Validation Results BSTC RCBT a Fig 0.80.8 0.8BSTC RCBT BSTC RCBT b c c i DNF cJ CZ C 40 Training 60 Training 80 Training 1-133/0-77 Training 0.95 DNF DNF DNF 0.9 0.90.90.90.85 0.8 BSTC RCBT TABLE VI AVERAGE RUN 1 133/0-77 70.38  1045.65  6362.86 t 20/23 over the number of tests for which Top-k finished Because RCBT couldn't finish any 80 or 1-133/0-77 tests within 2 hours with nl  20 we lowered nl to 2 Classification Accuracy Figure 7 contains boxplots for BSTC on all four OC classification test sets Boxplots were not generated for RCBT with 60 80 or 1-133/0-77 training since it was unable to finish all 25 tests for all these training set sizes in  2 hours each Table VII lists the mean accuracies of BSTC and RCBT over the tests on which RCBT was able to produce results Hence Table VII's 40 row consists of averages over 25 results Meanwhile Table VII's 60 row results are from 6 tests 80 contains a single test's result and 1-133/0-77 results from 3 tests RCBT has better mean accuracy on the 40 training size but the results are closer on the remaining sizes   4 difference over RCBT's completed tests Again RCBT's accuracy could vary widely on its uncompleted tests CAR Mining Parameter Tuning and Scalability We attempted to run Top-k to completion on the 3 OC 80 training and 2 OC 1-133/0-77 training tests However it could not finish mining rules within the 2 hour cutoff Top-k finished two of the three 80 training tests in 775 min 43.6 sec and 185 min 3.3 sec However the third test ran for over 16,000 mnm  11 days without finishing Likewise Top-k finished one of the two 1-133/0-77 tests in 126 min 45.2 sec but couldn't finish the other in 16,000 min  11 days After increasing Top-k's support cutoff from 0.7 to 0.9 it was able to finish the two unfinished 80 and 1-133/0-77 training tests in 5 min 13.8 sec and 35 min 36.9 sec respectively However RCBT with nl 2 then wasn't able to finish lower bound rule mining for either of these two tests within 1,500 min Clearly CAR-mining and parameter tuning on large training sets is TABLE VII MEAN AcCU1ACIES FOR 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


