ICICS-PCM 2003 15-18 December 2003 sioeapme 3B2.4 Learning Semantic Cluster for image retrieval using association rule hypergraph partitioning Lijuan Duan\222 Yiqiang Chen\222 Wen GaoU\224 221The College of Computer Science Beijing University of technology, Beijing 100022 China 222Institute of Computing Technology Chinese Academy of Sciences Beijing 100080 China 222Graduate School Chinese Academy of Sciences Beding 100039 China 4Department of Computer Science Harbm Institute of 
Technology, 150001, China Abstract Semantic clustering is an important and challenge task for content-based image database management This paper proposes a semantic clustering learning technique which collects the relevance feedback image retrieval mnsach 221on and uses bypergraph to represent images correlation ship then obtains the semantic clusters by bypergraph partitioning Experiments show that it is efficient and simple 1 Introduction Content-based image retrieval CBIR is a set of techniques for retrieving semantically relevant images kom 
image database based on antomaticaUy derived image features  Semantic feature extraction and description is the headstone of CBIR For human it is not ditFcult to extract the semantic information fiom an image The backgound knowledge plays an important role in human object recognition So manual or semi-automatic image annotation methods are adopted by some CBIR systems Image annotation is used in traditional image database systems It is a tedious work to annotate images in 
large databases manually However without the help of human beings, it is very difficult to extract the semantic content of an image automatically 4 examples designated by users 1,2,5 Many feedback image retrieval systems are developed to support content-based image retrieval But most feedback approaches do not have a learning mechanism to memorize the feedbacks conducted previously and reuse them in favor of future queries This paper introduces a semantic learning method by analyzing the relevance feedback image retrieval process 
which could cluster the semantic related images into same class For relevance feedback image retrieval system the images relevant to query are pointed as positive example otherwise the images irrelevant to query are pointed as negative examples It is assumed that these positive examples are related in semantic content So we can collect the correlation ships of images fiom retrieval processes Semantic retrieval and clustering is carried out based on these association relationships For a relevance feedback image retrieval system, it is 
not reality to capture the semantic relationship of all images by analyzing one query or one feedback Even by the same query requirement merent user has respective interest so the query result may not be consistent with others The 223correct\224 semantic of a multimedia is what most people but not necessarily all the people agree upon Noisy information must be identitied and eliminated So the leaming algorithm must robust and efficient Zhuang 9 proposes a graphic theoretic model for incremental relevance feedback 
in image retrieval In fact the correlation ships of images are very complex We use bypergraph to describe the relationship of images Furthermore we use association rule hypergraph partitioning method 10,11,12 to obtain the semantic clustering Association rule hypergraph partitioning algorithm ARHP is a new clustering method which is based on generalizations of graph partitioning do not require pre specified ad hoc distance functions 10,11,12 It has used to clustering related items in transaction database To resolve the problem relevance feedback is introduced The 
rest of this paper is organized as follows Section 2 into CBIR Relevance feedback is a power technique introduces the details of leaming algorithm The which improves the retrieval performance by adjusting the effectiveness of the algorithm is presented in Section 3 original query based on the relevant and irrelevant image Section 4 concludes the paper This paper is supported by the Pb D Research Fund of Beijing University of Techology 0-7803-8185-8/03/$17.00 0 2003 El33 1581 


2 Learning Algorithms The basic idea of the semantic learning algorithm is very simple It collects the relevance feedback image retrieval transaction and uses hypergraph to represent images correlation ship then obtains the semantic clusters by hypergraph partitioning Following sections 2.1 2.2 slid 2.3 describes the details of these steps 2.1 Constructing image retrieval transaction database In information system transaction is a useful concept It is the basic logical unit of execution in an information system Image retrieval transaction is a dole query process for relevance feedback image retrieval system The outcome R of relevance feedback image retrieval is represented by an image retrieval transaction Q P,N,I   Q is query example P is positive examples N={n,,n,,n,,n  n  I is the retrieved images I  i,,iz,i3,i  im  The images related in semantic level are called semantic correlation images. Such as Q and p  pn are semantic correlation images Q and N  n,,rzz,n n  n are not semantic correlation images 2.2 Representing image correlation using association rule hypergraph A hypergraph H=\(V,E consists of a set of vertices v and a set of hyperedges E 10,11,12 A hypergraph is an extension of a gmph in the sense that each hyperedge can connect more than two vertices 10,11,12 A key problem in modeling data items as a hypergraph is determining that related items can he grouped as hyperedges and determining the weights of the byperedge 10,11,12 In this case, hyperedges represent the frequent item sets found by the association rule discovery algorithm In our system the set of vertices V corresponds to the set of images being clustered and each hyperedge e E E corresponds to a set of related images A fieqnent item sets found using the association rule discovery algorithm corresponds to a set of images that have been feedback as positive examples simultaneity These fkequent images are mapped into hyperedges in a hypergraph p,,p,,p,,p  p  N is negative examples p2,p p 2.3 Learning the semantic cluster The hypergraph representation can then be used to cluster relatively large groups of related images by partitioning them into highly connected partitions We use HMETIS to partition the complex bypergraph gained fiom section 2.2 TIS is a multilevel hypergraph partitioning algorithm that has been shown to produce high quality bisections on a wide range of problems arising in scientific applications 11,12,13 such as VLSI application document category lxaffic assignment problems In the beginning HMETIS partitions the hypergraph into two parts such that the weight of the hyperedges  that are cut by the partitioning is minimized Each of these two parts can be further bisected recursively until each partition is highly connected At each recursion TIS minimizes the weighted hyperedge cut and thus tends to create partitions in which the connectivity among the vertices in each partition is high resulting in good clusters So bad clusters are eliminated by using the following cluster fitness criterion 13 The fitness function is used to measure the ratio of weights of edges that are within the partition and weights of edges involving any vertex of this partition Let e be a set of vertices representing a hyperedge and C be a set of vertices representing a partition. The fitness function that measures the goodness of partition Cis defined as follow 12,13 In formula 1 the high fitness value implies that the partition has more weights for the edges connecting vertices within the partition 12,13 To filter out vertices that are not highly co~&ed to the rest of the vertices of the partition each partition is examined Connectivity another measure function is defined in AR It measures the percentage of edges that each vertex is associated with High connectivity value suggests that the vertex has many edges connecting good proportion of the vertices in the partition The connectivity function of vertex v in C is defined as follow 12,13 In formula 2 the vertices with connectivity measure greater than a give threshold value are considered to belong to the partition and the remaining vertices are dropped fiom the partition 12,13 In AR the support criteria in the association rule discovery algorithm is also used to 6lter out of non-relevant images Depending on the support threshold images that do not meet support criteria i.e images that do not relevant with other images will he pruned 3 Experiment In order to test the efficiency of mentioned algorithm an experiment system is established which adopts the Rich Get Richer relevance feedback strategy 15 Tests are performed on commercial database Core1 Gallery It contains 1,000,000 images being classified into many 1582 


semantic groups We create a test database by randomly selecting 20 categories of Core1 Photographs \(30 images in each category We collect retrieval results of two stages In the 6rst stage 113 times retrieval processes are collected The query images are different for these retrievals In another word 113 images are point as query example to cany relevance feedback image retrieval In the second stage 7196 times retrievals are collected 600 images are pointed as query examples in these retrieval processes. The query times for eacb image of the test database are merent In section 3.1 we illustrate the semantic clustering results using algorithm mentioned in section 2 In section 3.2 we compare the results of different clustering methods In section 3.3 we illushate the semantic retrieval result 3.1 Semantic clustering results Image transaction database are constructed using the method mentioned in section 2.1 The first transaction database  called I  includes 113 transactions which involves 555 images in the test database The second transaction database called II includes 7196 transactions which involves 600 images To filter out of non-relevant images can be achieved using the support criteria in the association rule discovery process Depending on the support and confidence threshold images that don't meet threshold will be pruned In the experiment support threshold is 1 confidence threshold is IO and each hyperedge has 3-15 vertices For transaction database I the hypergraph has 2187190 hyperedges For transaction database II the hypergraph has 331102 hyperedges Fig.2 illushates the detail of hypergmph creating process for transaction database I The hypergraph representation can then be used to cluster relatively large groups of related images by partitioning them into highly connected partitions We use HMETIS to partition the complex hypergraph Fig 3 displays the semantic clustering process for transaction database JI H:Uairiori f  1 3 a15 1 10 th t-slctionll3.dat hyp-aph WF*Orl  find asxociafinn rulsah-mdgcs with apriori algorifh vsrnimn 2.9 a9.86.2881 e 1996-2881 hrintian welt rsading tmracfionll3.dar __ 1555 item 113 tmraction<r>l done 10.0lsl smting ftaa w.r.t thir frownsj  dono 10.P&1 coding loaded tr~sacfions ___ done lB.P&I chscking subsets of sire 1 2 3 4 5 6 7 8 9 10 11 12 13 I4 15 dons 1336l.80~1 niting hya-ph ___ 12lWl90 h-mdgo<r>l done 1357.33~1 Fig.2 Creating hypergraph mph lnfb-eion hyp~rgraph3 DUtxr 680 EH 331182 9Parer 28 UBfkfor 0.0 ions HPC Pn..Paconst-Palss 01 Ubcsh Ila Pixsd Uortioor s"r=iuc Parfitioning  Bizsefing a hm-aph of size luertic=s-600 hedgco*33l182 hlansc-8.581 Cut of trial 8 0 18.581 Cut Of tri*1 1 B 18.581 Cut of trial 2 8 18.581 Cut of trial 3 0 18.581 Cut Of trial 5 8 10.581 Cut of'trial 6 B 18.581 Cut bf trial 7 B 18.581 Cut of trial 8 0 18.501 Cut Of trial 9 0 18.581 lk,mincue   for the hirestion  8 avorag0  8.0 hlance  8.50 Cut or trial 4 8 ia.581  sWy for tha mV9 tition Hyperadge Cut 8 aininizs sm-of EXtsrna1 Degree 8 ninimize Sosled Cast B.&rBBB ninimizc a*orp,tion 3311q.88 iz Partition Shea 5 Extsmal Dagrrek 351 01 341 81 271 01 241 81 291 81 341 01 341 81 261 81 291 81 271 81 371 81 381 81 241 81 241 81 271 81 351 01 351 01 23t 81 381 01 281 81  ring Infc.-tion Pufitioning ria 858.523 1,d ria 2.123mce Fig.3 hypergraph partitioning 1583 


TmactimDatabase I Tnmsactions I Images 1 Support 1 Confidence I Hyperedges Precision I I 113 I 555 I 1 I 10 I 2187190 I 79.5 II I 71 I 6001 1 I 10 I 331102 I 85.8YO Tab.1 Clustering result The hypergraph is portioned into 20 classes in Fig.3 It takes 14 minutes to gain the clustering result on the computer  Pentium Pro Pentium 1.4GHz with 256M of RAM  The relevant images should be partitioned into the same cluster according to the partition rule of HMETIS The test database includes 20 categories of Corel Photographs 30 images in each category We wish that the semantic clustering results could consistent with test database categories As shown in Fig.3 the number of images for each cluster is liom 23 to 37 It is obviously there is some error in the processing To verify the clustering results each cluster is observed manually and a topic is given by the observer In factually the topic comes from the categories of Corel Photographs In another word the topic is assigned to each cluster according to the category that most images of the cluster belong to For example there are 35 images in cluster 1 and 28 images are roses The topic of the cluster is 221kose\224 It is obviously that 7 images are not rose in cluster 1 Tab.1 displays clustering precision for each database For database 4 the clustering precision is 79.5 For database I the clustering precision is up to 85.8 It shows that the clustering method is efficient 3.2 Comparing with traditional clustering method To illustrator the efficiency of semantic clustering method we compare it with the result by using traditional clustering mechanism Traditional image clustering technique is based-on visual feature such as color or texture Fig.4 displays some images with red color which are clustered into one cluster by using color feature It is obviously that these images can be partition into many semantic classes Fig.5 displays some clusters by using the mentioned algorithm in section 2 It is obviously that the images related in semantic high level fature are clustered together C Fig.5 Semantic clustering result 3.3 Experiment result of semantic retrieval The result of clustering could then be used to semantic retrieval for searching similar images Fig 6 gives an example of semantic retrieval using the cluster result mentioned in section 3.1. Fig 7 gives an example of visual feature based retrieval In Fig.6 and Fig 7 the query image is displayed at the upper left corner As these show that the query images of the two experiments are same which come from the 223sand\224 category of Corel database. The best 32 retrieved images are displayed In Figd 90 images are belonging to 223sand\224 category While in Fig.7 80 images are not belonging to the 223sand\224 category It obviously shows that the clustering method is very useful for semantic retrieval FigA Image clustering result based on visual feature 1584 


Fig.6 Retrieval result by using the cluster result Fig.7 Retrieval result based on visual feature 4 Conclusion A semantic clustering learning technique is proposed in this paper Different fiom traditional image clustering technique it exmcts image\222s semantic content by analyzing image\222s correlation ship not by image understanding techniques The basic idea of the semantic learning algorithm is very simple It collects the correlation ships of images om retrieval processes uses hypergraph to represent the images relationships and obtains images semantic clustering using bypergraph partitioning method Experiments show that it is efficient and simple Reference I Yong Rui Thomas S Huang and Shih-Fu Chang 223Image Retrieval Past Resent and Future\224 Invited paper in Int Symposium on Multimedia Information Processing Dec 11-13,1997 Taipei Taiwan Z Catherine Lee Wei-Ymg Ma Hongiiang Zhang 223Information Embedding Based on User\222s Relevance Feedback for Image Retrieval\224 Technical report HF\222 Labs 1998 3 James Z Wang JiaLi and Gio Wiederhold 223SIMpLIcity: Semantics-sensitive Integrated Matching for Picture Libraries\224 IEEE Transactions On Pattern Analysis And Machine Intelligence Vol 23 No 9 September2001 4 J R Smith Shih-Fu Chang 223Automated Image Retrieval Using Color and Texture\224 Columbia University Dept of Electrical Engineering and Center for Telecommunication Research 1995 5 Yong Rui Thomas S Huang Sharad Mehtra and Michael mega 223Relevance Feedback a power tool for interactive content-based image retrieval\224 IEEE trans Circuits and systems for video technology vol 8 no 5 pp 644-655 Sep 1998 6 Y.Lu,C.-H.Hu,X.-Q.Zhu,H.-J.ZhangandQ.Yang 223A Unified Framework for Semantics and Feature Based Relevance Feedback in Image Retrieval Systems\224 ACM Multimedia 2000 7 I J Cox U L Miller S M Omohundro P N Yianilos 223F\222ichunter Bayesian relevance feedback for image retrieval system\224 In Intl Cb On Pattern Recogoition Vienna Austria August 1996 pp.361 369 8 Jun Yang Liu Wenyin Hongjiang Zhang Yueting Zhuang  223An Approach to Semantics-Based Image Retrieval and Browsing\224 International Workshop on Multimedia Database Systems Taipei Sept 2001 9 Yueting Zhuang Jun Yang Qing Li and Yunhe Pan 223A Graphic-Theoretic Model For Incremental Relevance Feedback In Image Retrieval\224 ICE2002 lo E H Han G. Karypis V Kumar and B Mobasher 223Clustering based on association rule hypergmphs\224 In workshop on Research Issues on Data Mining and Knowledge Discovery Tucson Arizona 1997 pp.9 13 ll E H Han G Karypis V Kumar and B Mobasher 223Hypergraph based clustering in high-dimensional data sets A summary of results\224 Bulletin of the Technical Committee on Data Engineering 1998 12 D Boley M Gini R Gross EH Han K Hastings G Karypis V Kumat B Mobasher J Moore 223Partitioning-Based Clustering for Web Document Categorization\224 Decision Support Systems 1999 Vo1.27 pp.329-341 I31 E H Han G. Karypis V Kumar and B Mobasher 223Clustering In A High-Dimensional Space Using Hypergraph Models\224 Technical Report TR-97-063 Department of Computer Science University of Minnesota, Minneapolis 1997 14 Q Li J Yang Y T Zhuang 223Web-Based Multimedia Retrieval Balancing Out between Common Knowledge and Personalized Views\224 Proceedings of the 1 International conference on Web Information Systems Engineering WISE\222OI Organized by WISE Society and Kyoto University Kyoto, Japan, 3-6 December 2001, Volume 1 pp.92 101 15 L J Doan W Gao and J Y Ma 223A Rich Get Richer Strategy for Content-Based Image Retrieval\224 Fourth International Conference On Visual Information Systems, Lyon, France, November 2000 V01.21 No.1 pp.15-22 1585 


W 0 000 9 001 6 001 3 000 18 W 1 000 8 001 5 001 2 000 15 and W 2 000 7 001 4 001 1 000 12 The load imbalance is much smaller r t s still present Bitonic Partitioning Single Equivalence Class In 6 w e propose a n e w partitioning scheme called bitonic partitioning  for load balancing that can be applied to the problem here as well This scheme is based on the observation that the sum of the workload due to itemsets i and 002 2 P\000 i 000 1 003 is a constant w i 001 w 2 P\000 i 000 1 000 n 000 i 000 1 001\002 n 000 002 2 P\000 i 000 1 003 000 1 003\000 2 n 000 2 P t 000 1 We can therefore assign itemsets i and 002 n 000 i 000 1 003 as one unit with uniform work 2 n 000 2 P t 000 1 If n mod 2 P 000 0 then perfect balancing results The case n mod 2 P 001 000 0 s handled as described in The 336nal assignment is n s A 0 000 f 0 000 5 000 6 g  A 1 000 f 1 000 4 000 7 g  and A 2 000 f 2 000 3 000 8 000 9 g  with corresponding workload n s W 0 000 9 001 4 001 3 000 16 W 1 000 8 001 5 001 2 000 15 and W 2 000 7 001 6 001 1 000 14 This partition scheme is better than the d scheme and results in almost no imbalance Bitonic Partitioning Multiple Equivalence Classes Above we presented the simple case of C 1  where we only had a single equivalence class In general we may have multiple equivalence classes Observe that the bitonic scheme presented above is a greedy algorithm i.e we sort all the w i the work load due to itemset i  extract the itemset with maximum w i  and assign it to processor 0 Each time we extract the maximum of the remaining itemsets and assign it to the least loaded processor This greedy strategy generalizes to the multiple equivalence class as well 14 the major difference being work loads in different classes may not be distinct 3.1.3 Adaptive Parallelism Let n be the total number of items in the database Then there are potentially 000 n k 001 large k itemsets that we would have to count during iteration k  r in practice the number is usually much smaller as is indicated by our experimental results We found that support counting dominated the execution time to the tune of around 85 of the total computation time for the databases we considered in Section 5 On the other hand for iterations with a large number of k itemsets there was suf\336cient work in the candidate generation phase This suggests a need for some form of dynamic or adaptive parallelization based on the number of large k itemsets If there aren\325t a suf\336cient number of large itemsets then it is better not to parallelize the candidate generation 3.1.4 Parallel Hash Tree Formation We could choose to build the candidate hash tree in parallel or we could let the candidates be temporarily inserted in local lists or hash trees This would have to be followed by a step to construct the global hash tree In our implementation we build the tree in parallel We associate a lock with each leaf node in the hash tree When processor i wants to insert a candidate itemset into the hash tree it starts at the root node and hashes on successive items in the itemsets until it reaches a leaf node At this point it 6 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


acquires a lock on this leaf node for mutual exclusion while inserting the itemset r if we exceed the threshold of the leaf we convert the leaf into an internal node with the lock still set This implies that we also have to provide a lock for all the internal nodes and the processors will have to check if any node is acquired along its downward path from the root This complication only arises at the interface of the s and internal nodes With this locking mechanism each process can insert the itemsets in different parts of the hash tree in parallel r since we start with a hash tree with the root as a leaf there can be a lot of initial contention to acquire the lock at the root r we did not 336nd this to be a signi\336cant factor on 12 processors 3.2 Support Counting r this phase we could either split the database logically among the processors with a common hash tree or split the hash tree with each processor traversing the entire database We will look at each case below 3.2.1 Partitioned vs Common Candidate Hash Tree One approach in parallelizing the support counting step is to split the hash tree among the processors The decisions for computation balancing directly in\337uence the effectiveness of this approach since each processor should ideally have the same number of itemsets in its local portion of the hash tree Another approach is to keep a single common hash tree among all the processors There are several ways of incrementing the count of itemsets in the common candidate hash tree Counter per Itemset Let us assume that each itemset in the candidate hash tree has a single count 336eld associated with it Since the counts are common more than one processor may try to access the count 336eld and increment it We thus need a locking mechanism to provide mutual exclusion among the processors while incrementing the count This approach may cause contention and degrade the performance r since we are using only 12 processors and the sharing is very 336ne-grained at the itemset level we found this approach to be the better than using private or separate counters 1  3.2.2 Partitioned vs Common Database We could either choose to logically partition the database among the processors or each processor can choose to traverse the entire database for incrementing the candidate support counts Balanced Database Partitioning In our implementation we partition the database in a blocked fashion among all the processors r this strategy may not result in balanced work per processor This is because the work load is a function of the length of the transactions If l t is the length of the transaction t  then during iteration k of the algorithm we have to test whether all the 1 r all the databases we looked at on our system the overhead of contention was within 4 which leads us to conclude that contention is not a big problem Other mechanisms like separate counters to eliminate locking and local counters to eliminate false sharing were studied t not shown to be bene\336cial 14 7 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


000 l t k 001 subsets of the transaction are contained in C k  Clearly the complexity of the work load for a transaction is n s O 000 min 000 l t k 000l t l t 000 k 001\001  i.e it is polynomial in the transaction length This also implies that a static partitioning won\325t work r we could devise static heuristics to approximate a balanced partition r example one static heuristic is to estimate the maximum number of iterations we expect say T  We could then partition the database based on the mean estimated work load for each transaction r all iterations n s 000 P T k 000 1 000 l i k 001 001 001T  Another approach is to re-partition the database in each iteration In this case it is important to respect the locality of the partition by moving transactions only when it is absolutely necessary We plan to investigate different partitioning schemes as part of future work 3.3 Parallel Data Mining Algorithms Based on the discussion in the previous section we consider the following algorithms for mining association rules in parallel 000 Common Candidate Partitioned Database CCPD This algorithm uses a common candidate hash tree across all processors while the database is logically split among them The hash tree is built in parallel see section 3.1.4 Each processor then traverses its local database and counts the support see section 3.2.1 for each itemset Finally the master process selects the large itemsets 000 Partitioned Candidate Common Database PCCD This has a partitioned candidate hash tree t a common database In this approach we construct a local candidate hash tree per processor Each processor then traverses the entire database and counts support for itemsets only in its local tree Finally the master process performs the reduction and selects the large itemsets for the next iteration Note that the common candidate common database\(CCCD approach results in duplicated work while the partitioned candidate partitioned database PCPD approach is more or less equivalent to CCPD r this reason we did not implement these parallelizations 4 Optimizations In this section we present some optimizations to the association rule algorithm These optimizations are bene\336cial for both sequential and parallel implementation 4.1 Hash Tree Balancing Although the computation balancing approach results in balanced work load it does not guarantee that the resulting hash tree is balanced Balancing C 2 No Pruning  We\325ll begin by a discussion of tree balancing for C 2  since there is no pruning step in this case We can balance the hash tree by using the bitonic partitioning scheme described  We simply replace P  the number of processors with the fan-out F for 8 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


the hash table We label the n large 1-itemsets from 0 o n 000 1 n exicographical order and use P 000 F to derive the assignments A 0 000 001\001\001 000 A F\000 1 for each processor Each A i is treated as an equivalence class The hash function is based on these equivalence classes which is simply n as h 001 i 002\000 A i  for i 000 0 000 001\001\001 000 F  The equivalence classes are implemented via an indirection vector of length n  For example let L 1 000 f A\000 D 000 E 000 G\000 K 000 M 000 N 000 S\000 T 000 Z g  We 336rst label these as f 0 000 1 000 2 000 3 000 4 000 5 000 6 000 7 000 8 000 9 g  Assume that the fan-out F 000 3 We thus obtain the 3 equivalence classes A 0 000 f 0 000 5 000 6 g  A 1 000 f 1 000 4 000 7 g  and A 2 000 f 2 000 3 000 8 000 9 g  and the indirection vector is shown in table 1 Furthermore this hash function is applied at all levels of the hash tree Clearly this scheme results in a balanced hash tree as compared to the simple g 001 i 002\000 i mod F hash function which corresponds to the d partitioning scheme from section 3.1.1 Label 0 1 2 3 4 5 6 7 8 9 Hash Value 0 1 2 2 1 0 0 1 2 2 Table 1 Indirection Vector Balancing C k 001 k\001 2 002  Although items can be pruned for iteration k 002 3 we use the same bitonic partitioning scheme for C 3 and beyond Below e show that n n this general case bitonic hash function is very good as compared to the d scheme Theorem 1 below establishes an upper and lower bound on the number of itemsets per leaf for the bitonic scheme Theorem 1 Let k 002 1 denote the iteration number I 000 f 0 000 002\002\002\000 d 000 1 g the set of items F the fan-out of the hash table T 000 f 0 000 002\002\002\000 F\000 1 g the set of equivalence classes modulo F  T 000 T k the total number of leaves in C k  and G the family of all size k ordered subsets of I  i.e the set of all k itemsets that can be constructed from items in I  Suppose d 2 F is an integer and d 2 F 000 F\002 k  De\336ne the bitonic hash function h  I\003 T by h 001 i 002\000 i mod F if 0 004 001 i mod 2 F 002 003 F and 2 F\000 1 000 001 i mod 2 F 002 otherwise and the mapping H  G 003 T from k itemsets to the leaves of C k by H 001 a 1 002\002\002\000 a k 002 000 001 h 001 a 1 002 000\002\002\002\000h 001 a k 002\002  Then for every leaf B 000\001 b 1 000\002\002\002\000b k 002 005T  the ratio of the number of k itemsets in the leaf  k H 000 1 001 B 002 k  o the average number of itemsets per leaf  kG k 004 kT k  s bounded above and below by the expression e 000 k 2 d\000 F 004 k H 000 1 001 B 002 k kG k 004 kT k 004 e k 2 d\000 F 002 A proof of the above theorem can be found in W e also obtain the same lo wer and upper bound for the d hash function also r the two functions behave differently Note that the average number of k itemsets per leaf kG k 004 kT k is 000 2 w F k 001 004 F k 006 000 2 w 001 k k   Let 005 001 w 002 denote this polynomial We say that a leaf has a capacity close to the average if its capacity which is a polynomial in w of degree at most k  s f the form 000 2 w 001 k k  003 006 001 w 002  with 006 001 w 002 being a polynomial of degree at most k 000 2 9 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


r the bitonic hash function a leaf speci\336ed by the hash values 000 a 1 000\000\000\001 a k 001 has capacity close to 002 000 w 001 if and only if a i 000 002 a i 000 1 for all i 1 001 i 001 k 002 1 Thus there are F 000 F\002 1 001 k 000 1 such leaves and so 000 1 002F 000 1 001 k 000 1 fraction of the s ave capacity close to 002 000 w 001  Note also that clearly 000 1 002F 000 1 001 k 000 1 approaches 1 On the other hand for the d hash function a leaf speci\336ed by 000 a 1 000\000\000\001 a k 001 has capacity close to 002 000 w 001 if and only if a i 000 002 a i 000 1 for all i  and the number of i such that a i 003a i 000 1 is equal to 000 k 002 1 001 004 2 So there is no such leaf if k is even r odd k 003 3 the ratio of the 322good\323 s decreases as k increases achieving a maximum of 2 004 3 when k 002 3 Thus at most 2 004 3 f the s achieve the average From the above discussion it is clear that while both the simple and bitonic hash function have the same maximum and minimum bounds the distribution of the number of itemsets per leaf is quite different While a signi\336cant portion of the s are close to the average for the bitonic case only a ew are close in the simple hash function case 4.2 Short-circuited Subset Checking  Candidate Hash Tree \(C 3 Hash Function: h\(i DEPTH 0 DEPTH 1 DEPTH 2 01 35 7101113 12986 24 LEAVES ABE ADE CDE A,C,EB,D B,D B,D B,D B,D B,D B,DA,C,E A,C,E A,C,E A,C,E A,C,E A,C,E ABD ACD ACEBCEBCDBDE ABC Figure 2 Candidate Hash Tree  C 3  Recall that while counting the support once we reach a leaf node we check whether all the itemsets in the leaf are contained in the transaction This node is then marked as VISITED to avoid processing it more than once for the same transaction A further optimization is to associate a VISITED 337ag with each node in the hash tree We mark an internal node as VISITED the 336rst time we touch it This enables us to preempt the search as soon as possible We would 10 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


expect this optimization to be of greatest bene\336t when the transaction sizes are large r example if our transaction is T 000 f A\000 B 000 C\000 D\000 E g  k 000 3 fan-out 000 2 then all the 3-subsets of T are f ABC,ABD,ABE,ACD,ACE,ADE,BCD,BCE,BDE,CDE g  Figure 2 shows the candidate hash tree C 3  We ave to increment the support of every subset of T contained in C 3  We egin with the subset AB C  and hash to node 11 and process all the itemsets In this downward path from the root we mark nodes 1 4 and 11 as visited We then process subset AD B  and mark node 10 Now consider the subset CDE  We see in this case that node 1 has already been marked and we can preempt the processing at this very stage This approach can r consume a lot of memory r a n fan-out F  for iteration k  e need additional memory of size F k to store the 337ags In the parallel implementation we have to keep a VISITED 336eld for each processor bringing the memory requirement to P\000F k  This can still get very large especially with increasing number of processors In we sho w a mechanism by which further reduces the memory requirement to only k 000F  The approach in the parallel setting yields a total requirement of k 000F 000P  5 Experimental Evaluation Database T I D Total Size T5.I2.D100K 5 2 100,000 2.6MB T10.I4.D100K 10 4 100,000 4.3MB T15.I4.D100K 15 4 100,000 6.2MB T20.I6.D100K 20 6 100,000 7.9MB T10.I6.D400K 10 6 400,000 17.1MB T10.I6.D800K 10 6 800,000 34.6MB T10.I6.D1600K 10 6 1,600,000 69.8MB Table 2 Database properties 5.1 Experimental Setup All the experiments were performed on a 12-node SGI Power Challenge shared-memory multiprocessor Each node is a MIPS processor running at 100MHz There\325s a total of 256MB of main memory The primary cache size is 16 KB 64 bytes cache line size with different instruction and data caches while the secondary cache is 1 B 128 bytes cache line size The databases are stored on an attached 2GB disk All processors run IRIX 5.3 and data is obtained from the disk via an NFS 336le server We used different synthetic databases with size ranging form 3MB to 70MB 2  and are generated using the procedure described in These databases mimic the transactions in a retailing en vironment Each transaction has a unique ID followed by a list of items bought in that transaction The 2 While results in this section are only shown for memory resident databases the concepts and optimization are equally applicable for non memory resident databases In non memory resident programs I/O becomes an important problem Solutions to the I/O problem can be applied in combination with the schemes presented in this paper These solutions are part of future research 11 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


  0 500 1000 1500 2000 2500 0 2 4 6 8 10 12 Number of Large Itemsets Iterations Large Itemset at Support = 0.5 222T5.I2.D100K\222  222T10.I4.D100K\222   222T15.I4.D100K\222   222T20.I6.D100K\222   222T10.I6.D400K\222   222T10.I6.D800K\222   222T10.I6.D1600K\222  Figure 3 Large Itemsets per Iteration data-mining provides information about the set of items generally bought together Table 2 shows the databases used and their properties The number of transactions is denoted as jD j  average transaction size as j T j  and the average maximal potentially large itemset size as j I j  The number of maximal potentially large itemsets j L j 000 2000 and the number of items N 000 1000 We refer the reader to for more detail on the database generation All the e xperiments were performed with a minimum support value of 0.5 and a leaf threshold of 2 i.e max of 2 itemsets per leaf We note that the  improvements shown in all the experiments except where indicated do not take into account initial database reading time since we speci\336cally wanted to measure the effects of the optimizations on the computation Figure 3 shows the number of iterations and the number of large itemsets found for different databases In the following sections all the results are reported for the CCPD parallelization We do not present any results for the PCCD approach since it performs very poorly and results in a speed-down on more than one processor 3  5.2 Aggregate Parallel Performance Table 3 s actual running times for the unoptimized sequential and a naive parallelization of the base algorithm Apriori for 2,4 and 8 processors without any f the techniques descibed in sections 3 and 4 In this section all the graphs showing  improvements are with respect to the data for one processor in table 3 Figure 4 presents the speedups obtained on different databases and different processors for the CCPD parallelization The results presented on CCPD use all the optimization discussed 3 Recall that in the PCCD approach every processor has to read the entire database during each iteration The resulting I/O costs on our system were too prohibitive for this method to be  12 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


