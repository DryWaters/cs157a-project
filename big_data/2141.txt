Personalized E-learning System by Using Intelligent Algorithm  Mingmin Gong Information Engineering School, Wuhan University of Science and Technology Zhongnan Branch, Wuhan, 430223, Hubei, China  gmm116@126.com  Abstract Nowadays, Network and multimedia are the trend of the development of the modern education technology. With the rapid development of the network technique and the prevalence of the Internet, Elearning has become the major trend of the development of international education since 1980's and the important access for the internationalization 
and the information of education. To meet the personalized needs of learners in E-learning, a new intelligent algorithm is proposed in the paper by using personality, association ru les mining and collaborative filtering technologies. The Intelligent algorithm is composed of two phases: association rules algorithm and collaborative filtering  1. Introduction  Nowadays, Network and multimedia are the trend of the development of the modern education technology. With the rapid development of the network technique and the prevalence of the Internet 
E-learning has become the major trend of the development of international education since 1980's and the important access for the internationalization and the information of education. Although the modern distance modernization arouses a big reform on the education mode and educa tion conception. However the learners are in different age level, sex, and social role, their culture and education background, attention interest hobby also exist a great difference. Giving corresponding teaching re sources according to learners’ characteristics to implement personalized 
learning is very difficult 2   According to the reasons mentioned above the technologies of personality association rules mining and collaborative filtering are applied in the paper Based on it, a new intelligent algorithm is proposed Furthermore, a new intelligent algorithm could apply in personalized E-learning system to support personalized E-learning bette    2. Intelligent algorithm  Association mining rules and collaborative filtering 
are applied in the paper. Intelligent algorithm is composed of two phases Step1 Using association rules algorithm to mining several categories of intere sted teaching resources for users Step2 Using collaborative filtering algorithm to recommend the specific teachi ng resource of interested categories  2.1. Association rules mining  There are three steps for association rules mining algorithm   
Step1 generating frequent item-set l. if appearance frequency of item-set is less than min_sup, then the item-set is frequent item-set Step2 regarding to each frequent item-set l, all non spatial subsets are generated Step3 regarding to each non- spatial subset of frequent item-set l, if   sup _   min_ sup _   port count l conf port count s 
  1   Then the rule s  l-s is generated. min_conf represent the minimum confidence thresholds, support count\(l represents the number of transaction containing item-set l support count\(s represents the number of transaction containing item-set s Teaching resources are classified as several 
categories. Then Basing on it, using association mining algorithm generates association rules. Left side itemset of the rules is the teaching resources category Furthermore, the rules are se lected and classified. The useful rules should provide the category of recommending teaching resources. Supposed N categories are obtained though association rules recommendation, like 123  n SS SS  
2008 Workshop on Knowledge Discovery and Data Mining 0-7695-3090-7/08 $25.00 © 2008 IEEE DOI 10.1109/WKDD.2008.152 400 
2008 Workshop on Knowledge Discovery and Data Mining 0-7695-3090-7/08 $25.00 © 2008 IEEE DOI 10.1109/WKDD.2008.152 400 
2008 Workshop on Knowledge Discovery and Data Mining 0-7695-3090-7/08 $25.00 © 2008 IEEE DOI 10.1109/WKDD.2008.152 400 
2008 Workshop on Knowledge Discovery and Data Mining 0-7695-3090-7/08 $25.00 © 2008 IEEE DOI 10.1109/WKDD.2008.152 400 
2008 Workshop on Knowledge Discovery and Data Mining 0-7695-3090-7/08 $25.00 © 2008 IEEE DOI 10.1109/WKDD.2008.152 400 


123  n SSSSS is a set of all recommendation categories  n NS is all teaching reousours  set of category n S  1  N n N NS S is all commodities set of recommendation category by using association rules  2.2. Collaborative filtering algorithm  The collaborative filtering algorithm is shown as follows Step1 Representation. Supposed input data may represent mn user - item evaluating matrix R m is the number of users, n is the number of item  ij R Is the appraisal value of ith user to jth item; appraisal value is related to the content. If the item is teaching resources in E-learning, then appraisal value represents user choose or not. For example 1 represents that user choose the resources 0 represents that user does not choose the resources Step2 Searching for the nearest neighbor set Regarding to a user U, a neighbor set 123     S NN NN  is generated and arranged according to the size of similarity. Even U does not belong to 123     S NN NN are arranged form big to small according to   s SIM U N  Step3 Generating recommendation. After generating the nearest neighbor set, the interest degree of item and Top-N are calculated. Supposed user a and the corresponding option set a I the interest degree of item j is calculated according to formula 2   1   1 n au u j u u aj a n au u wr r pr w   2  Among them a r represents the average appraisal value that user a to item, U is the nearest neighbor set j w Is similarity between user and user  uj r is the appraisal value that user u to item j u r represents the average appraisal value that us er u to item. The interest degree of user i to different items is calculated separately. N items that have higher interest degree and don’t belong to item are taken as recommendation set Top-N After interested cate gories are obtained by association rules, we use the collaborative recommendation. Namely, regarding to each n SS  we use the collaborative recommendation in  n NS  Supposed, regarding to each n S  Q teaching resources are recommended. Recommendation set is 123     tttttq I II II Simultaneously the interest degree of each commodity is 123    q PIt PIt PIt PIt  We recommend resources according to below strategy. Because the user ha s different interest degree to each category, category weighting method is used to recommend commodities Regarding to each   tj PI  t 1,2…t j 1,2…q   n wS is the interest weight that the user to each category n S Interest weight is calculated though confidence degree. Confidence degree is obtained though association rules    tj n tj F wS pI   3  The size of  tj F t=1,2…t, j=1,2…q is taken as the recommendation  3. Conclusion  In summary, a new intelligent algorithm based on association rules mining a nd collaborative filtering is proposed in the paper. The al gorithm is also applied in personalized E-learning. The results manifest that the algorithm can support E-learning better  References   Divjak, B.; Begcevic, N Imaginative Acquisition of knowledge - strategic planning of E-learning”. Proceedings of 28th International Conference on Information Technology Interfaces, 2006, pp.47-52  Karunananda, Asoka S.. “A th eoretical-based approach to E-Learning Proceedings of First International Conference on Industrial and Information Systems 2006.pp.127-132  Xindong Wu. “Data m i ning artificial intelligence in data analysis Proceedings of IEEE/WIC/ACM International Conference on Intelligent Agent Technology 2004.pp.7  Li Dun,Cao Yuanda  A  New Weighted Text Filtering Method Proceedings of International Conference on Natural Language Processing and Knowledge Engineering  Wuhan,2005,pp.695-698  
401 
401 
401 
401 
401 


037\005\003\0169D\006\023\013\024\006\012G4\006\005\013\006\004<4\006 006 006 006 023\013\024\0061G\012W4\006\005\013\006\004\006 006 006 006 H\006 006  012   023\012 021 010  1   023\012 021 011 006 006 006 006 006 006 006 023\013\024\0062G4\006\005\013\006\004\006 006 006 006 006 006 006 032 \006H 012  1 J   0232 021 011 006 006 006 006 006 006 006 H 012  1 J\012\014\006\011\004\006\012\004\005\003\004\005\006\024\003\022\033\027\005\012\013\004 010 006 006 006 006 006 006 007 012 H 012  1 J 011 006 006 006 006 006 J 011 006 037\005\003\016;D\006\024\003\005\033\024\004\006\007!\006\005\015\012\014\006\011\010\036\013\024\012\005\015\025\006\012\014\006 \012\004\012\014\015\003\022#\006 035\014\012\004\036\006 012\004\005\003\004\005\006 024\003\022\033\027\005\012\013\004\006 013 \006 027\013\004\027\003\016\005\014!\006 0\003\006 027\011\004\006 036\003\005\006 011\014\014\013\027\012\011\005\012\013\004\006 024\033\010\003\014#\006 032 \006 007\006 012\014\006 011\004\006 012\004\005\003\004\005\006 024\003\022\033\027\005\012\013\004\006 013 \006 027 013\004\027\003\016\005\006 026G>+!\001?\006 011\004\022\006 007  001!\006 005\015\003\004\006 0\003\006 027\011\004\006 036\003\005\006 011\004\006 011\014\014\013\027\012\011\005\012\013\004\006 024\033\010\003\006 007 001 001<\007#\006\017\004\006\011\014\014\013\027\012\011\005\012\013\004\006\024\033\010\003\006\025\003\011\004\014\006\012 \006\003\002\003\004\005\006\007\006\013\027\027\033\024\014!\006\003 002\003\004\005\006 001<\007\0060\012\010\010\006\013\027\027\033\024#\006 035\014\012\004\036\006\026\032\007\006\011\010\036\013\024\012\005\015\025!\0060\003\006\027\011\004\006\036\003\005\006\005\015\003\006 \013\010\010\0130\012\004\036\006\024\003\014\033\010 005\014\006 012\004\006\001@\011\025\016\010\003\0064#\006 006 V8\006 027\013\004\027\003\016\005>H4!$!9!;!7J!H\027J?\006 D\006 012\004\005\003\004\005\006 024\003\022\033\027\005\012\013\004\006 012\014\006 H 027J!\006 011\004\022\006\004\013\006\011\014\014\013\027\012\011\005\012\013\004\006\012\014\006\036\003\004\003\024\011\005\003\022#\006 V9\006 027\013\004\027\003\016\005>H$!9!;J!H/!\027!\003J?D\006 012\004\005\003\004\005\006 024\003\022\033\027\005\012\013\004\006 011\024\003\006 H/!\027J\011\004\022\006 H\027!\003J!\006 011\004\022\006 011\014\014\013\027\012\011\005\012\013\004\006 024\033\010\003\014\006 027 001 003\006 011\004\022\006 027\003 001 006 011\024\003\006 036\003\004\003\024\011\005\003\022#\006 VA\006\027\013\004\027\003\016\005>H$!9J!>\011!/!\027!\003!\036??D\006\012\004\005\003\004\005\006\024\003\022\033\027\005\012\013\004\006\011\024\003 006H\011!/J!\006 H\011!\003J!\006 H\011!\036J!\006 H/!\036J!\011\004\022\006 H\003!\036J!\006 011\014\014\013\027\012\011\005\012\013\004\006 024\033\010\003\014\006 011 001 027\003\036!\006 011\003 001 027\036!\006\011\036 001 027\003!\006/\036 001 011\027\003!\006\003\036 001 011/\027#\006 015\003\006 013\005\015\003\024\006 027\013\004\027\003\016\005\014:\006 012\004\005\003\004\005\006 024\003\022\033\027\005\012\013\004\006 011\004\022\006 003@\005\024\011\027\005\003\022\006 011\014\014\013\027\012\011\005\012\013\004\006\024\033\010\003\014\006\011\024\003\006\013\025\012\005\005\003\022#\006\035\014\012\004\036\006Q\012\003:\014\006\011\010\036\013\024\012\005\015 025\0060\003\006\027\011\004\006 036\003\005\006\005\015\003\006\014\011\025\003\006\024\003\014\033\010\005#\006 011\012\011\001\030\024\014\001\002\016\031\021\006\027\005\032\033\001\034\015\006\035\021\006\033\007\025\010\015\001\002\025\007\016\036\004\015\004\001 017\010\036\013\024\012\005\015\025\006 016\003\024 \013\024\025\011\004\027\003\006 012\014\006 003\014\005\012\025\011\005\003\022\006 020\006 005\015\003\006 005\012\025\003\014\006 013 \006 027\011\024\024\020\012\004\036\006 027\013\025\016\011\024\012\014\013\004\006 013 \006 014\012\004\036\010\003\006 003\010\003\025\003\004\005#\006 C\015\003\004\006 011\002\003\024\011\036\003 010\020\006 003\002\003\024\020\006\027\013\004\027\003\016\005\006\015\011\014\006\004\006\014\033\016\003\024\006\027\013\004\027\003\016\005\014\006\011\004\022\006\003\002\003\024\020\006\027\013\004\027\003\016 005\006\015\011\014\006\025\006 011\005\005\024\012/\033\005\003\014!\006 005\015\003\004\006 026\032\007\006 017\010\036\013\024\012\005\015\025\006 027\013\025\016\010\003@\012\005\020\006 012\014\006 025\004 006 032\004\006 013\024\022\003\024\006 005\013\006 027\013\025\016\011\024\003\006 026\032\007\006 011\010\036\013\024\012\005\015\025\006 011\004\022\006 Q\012\003:\014\006 011\010\036\013\024\012\005\015\025\006 3445!\006 0\003\006\022\013\006\005\015\003\006 \013\010\010\0130\012\004\036\006\003@\016\003\024\012\025\003\004\005#\006 007\011\004\022\013\025\010\020\006 036\003\004\003\024\011\005\003\006 011\006 027\013\004\005\003@\005!\006 0\015\012\027\015\006 015\011\014\006 8%%\006 024\0130\014\006 013/1\003\027\005?\006 011\004\022\006 8%\006 027\013\010\033\025\004\014\006 011\005\005\024\012/\033\005\003?!\006 011\004\022\006 016\024\013/\011/\012\010\012\005 020\006 013 \006 003@\012\014\005\012\004\036\006 024\003\010\011\005\012\013\004\014\015\012\016\006 003\0050\003\003\004\006 013/1\003\027\005\006 011\004\022\006 011\005\005\024\012/\033\005\003\006 012\014\006 006 017\022\013\016\005\006 O\013\022\012\004\006 017\010\036\013\024\012\005\015\025\006 34$5\006 005\013\006 036\003\004\003\024\011\005\003\006 027\013\004\027\003\016\005\006 010\011\005\005 012\027\003#\006 015\003\004\006 024\011\004\022\013\025\010\020\006 014\003\010\003\027\005\006 4%%!4;%!$%%!$;%\006 027\013\004\027\003\016\005\014\006 005\013 006 027\013\025\016\033\005\003\006 012\004\005\003\004\005\006 024\003\022\033\027\005\012\013\004#\006 015\003\006 003@\016\003\024\012\025\003\004\005\006 012\014\006 027\011\024\024\012\003\022 006 013\004\006 011\006 025\012\027\024\013\006 027\013\025\016\033\005\003\024\006 0\012\005\015\006 9\006 4#&O\031'\006 026"\035\006 011\004\022\006 7.\006 025\003\025\013\024\020#\006 015\003\006 003@\016\003\024\012\025\003\004\005\006 024\003\014\033\010\005\006 012\014\006 014\015\0130\004\006 012\004\006 012\036\033\024\003 003 006 012\004\006 0\015\012\027\015\006 034 4 006 014\015\0130\014\006 Q\012\003:\014\006 011\010\036\013\024\012\005\015\025:\014\006 016\003\024 \013\024\025\011\004\027\003!\006 011\004\022\006 034  006 014\015\0130\014\006 026\032\007\006 011\010\036\013\024\012\005\015\025:\014\006\016\003\024 \013\024\025\011\004\027\003#\006 006 006 001 001\002\003 004 004\002\003 005 005\002\003 006 006\002\003 007 007\002\003 003 003\001 004\001\001 004\003\001 005\001\001 005\003\001 010 001 002 003 004 011\004 011\005 006 023\012\036\033\024\003 004\002 006\011\010\036\013\024\012\005\015\025\006\027\013\025\016\011\024\012\014\013\004\006 015\003\006\027\015\012\003 \006\024\003\011\014\013\004\006 \013\024\006/\003\005\005\003\024\006\026\032\007\006\011\010\036\013\024\012\005\015\025\006\016\003\024 \013\024\025\011\004 027\003\006 012\014\006 005\015\011\005D\006 005\015\003\013\024\003\025\006 4\006 011\004\022\006 022\003\022\033\027\005\012\013\004\006 4\006 016\033\005\006 013\0240\011\024\022\006 011\004\022\006 016\024\013\002\003\006 012\004\005\003\004\005\006\024\003\022\033\027\005\012\013\004\006\015\011\014\006\0050\013\006\003\010\003\025\003\004\005\014\006\011\005\006\025\013\014\005!\006\011\004\022\006\015\011\014\006 004\013\006\013\005\015\003\024\006 027\013\025/\012\004\011\005\012\013\004\014!\006\014\013\006\024\003\022\033\027\003\014\006\005\015\003\006\027\013\025\016\033\005\012\004\036\006\014\027\013\016\003#\006 013 026 026\034\035\037\032 006 015\012\014\006 016\011\016\003\024\006 012\024\014\005\010\020\006 012\004\005\024\013\022\033\027\003\014\006 014\003\002\003\024\011\010\006 002\012\0030\016\013\012\004\005\014\006 013\004 006 003\002\003\004\005#\006+\015\003\004\006\016\024\013\022\033\027\003\014\006\011\006\025\013\022\003\010\006/\011\014\003\022\006\023\026\017\006\005\013\006\024\003\016\024\003\014\003\004\005 006\003\002\003\004\005!\006 0\015\012\027\015\006 013\027\033\014\006 013\004\006 003\002\003\004\005\006 011\004\022\006 012\005\014\006 013\027\027\033\024<\005\012\025\003#\006 004\006 005\015\012\014\006 025\013 022\003\010!\006 011\014\014\013\027\012\011\005\012\013\004\006 024\033\010\003\006 003@\005\024\011\027\005\012\013\004\006 012\014\006 014\005\033\022\012\003\022!\006 011\004\022\006 027\013\025\016\011\024\003 014\006 0050\013\006 022\012  \003\024\003\004\005\0060\011\020\006\005\013\006\027\013\025\016\033\005\003\006\012\004\005\003\004\005\006\024\003\022\033\027\005\012\013\004#\006 017\0272\004\0130\010\003\022\036\025\003\004\005 006 015\003\006 0\013\0242\006 016\024\003\014\003\004\005\003\022\006 012\004\006 005\015\012\014\006 016\011\016\003\024\006 012\014\006 014\033\016\016\013\024\005\003\022\006 020\006 011\005\012\013\004\011\010\006\037\027\012\003\004\027\003\006\023\013\033\004\022\011\005\012\013\004\006\013 \006\026\015\012\004\011\006>7%;&;%8;?#\006 007 001\023\001\007\001*\026\001\037 006 345\006 Q#\006 026\015\003\004!\006 C\015\020\006 022\012\022\006 X\013\015\004\006 031\003\024\014\027\015\003\010\006 011\012\010\006 005\013\006 033\004\022\003\024\014\005\011 004\022\006 016\013\010\011\024\012'\011\005\012\013\004Y\006 015\003\006\022\012  \003\024\003\004\027\003\014\006/\003\0050\003\003\004\006\013/1\003\027\005\006\011\004\022\006\003\002\003\004\005\006\027\013\004\027\003\016\005\014#\006 037\005\033\022\012\003\014\006\012\004\006\031\012\014\005\013\024\020\006\011\004\022\006 015\012\010\013\014\013\016\015\020\006\013 \006\037\027\012\003\004\027\003!\006\002\013\010#89!\006\016\016#964<;48!\006$%%8#\006 3$5\006 011\004\006 Z\033\004<\015\003!\006 O\003\004\036\006 C\003\012<\022\013\004\036!\006 031\003\006 030\015\012<X\033\004#\006 017\004\006 012\004\005\024\013 022\033\027\005\012\013\004\006 005\013\006 012\004\005\003\010\010\012\036\003\004\027\003<\027\013\025\016\033\005\012\004\036\006 013\024\012\003\004\005\003\022\006 025\003\025\013\024\020\006 005\015\003\013\024\020#\006 026\013\025\016 033\005\003\024\006 007\003\014\003\011\024\027\015\006 011\004\022\006 003\002\003\010\013\016\025\003\004\005!\0064669!\00684>4$?!\006\016\016#8&<9$>\012\004\006\026\015\012\004\003\014\003\0060\012\005\015 006\001\004\036\010\012\014\015\006\011/\014\005\024\011\027\005?\006 385\006 006 007#\006 037\005\033\022\003\024!\006 B#\006 007#\006 021\003\0041\011\025\012\004\014!\006 011\004\022\006 006 023\003\004\014\003\010!\006 N\004\013 0\010\003\022\036\003\006 003\004\036\012\004\003\003\024!\006 016\024\012\004\027\012\016\010\003\014\006 011\004\022\006 025\003\005\015\013\022\014#\006 011\005\011\006 011\004\022\006 N\004\0130\010\003\022\036\003\006 001\004\036\012\004\003\003\024 012\004\036!\006 002\013\010#\006 006 016\016#\006 474<46&!\006466A#\006 395\006 017\004\022\024\0030\006 021#\006 021\0112\003\024#\006 013\004\025\013\004\013\005\013\004\012\027\006 024\003\011\014\013\004\012\004\036\006 012\004\006 005\015\003\006 024\011\025\0030\013\0242\006 013 \006 014\012\005\033\011\005\012\013\004\006\027\011\010\027\033\010\033\014#\006\017\024\005\012 \012\027\012\011\010\006\032\004\005\003\010\010\012\036\003\004\027\003!\00696>4<8 006\016\016#;<$8!\0064664#\006 3;5\006 017\004\022\024\0030\006 006 031#\006 023\011\024\024\003\010\010!\006 011\024\0032\006 X#\006 037\003\024\036\013\005!\006 011\005\015\012\011\014\006 037\011\010\010[\003!\006 011\004\022\006 026\010\011\033\022\012\013\006 021\011\024\005\013\010\012\004\012#\006 035\014\012\004\036\006 005\015\003\006 003\002\003\004\005\006 027\011\010\027\033\010\033\014\006 013\024\006 005\024\011\0272\012\004\036\006 005\015 003\006 004\013\024\025\011\005\012\002\003\006 014\005\011\005\003\006 013 \006 027\013\004\005\024\011\027\005\014#\006 032\004\005\003\024\004\011\005\012\013\004\011\010\006 X\013\033\024\004\011\010\006 013 \006 026\013\013\016\003\024\011\005\012\002\003\006 032\004 013\024\025\011\005\012\013\004\006 037\020\014\005\003\025\014!\006 49>$<8?!\006\016\016#66<4$6!\006$%%;#\006 375\006 032\010\012\011\004\013\006 026\003\024\002\003\014\011\005\013!\006 011\014\014\012\025\013\006 023\024\011\004\027\003\014\027\015\003\005!\006 011\004\022\006 017\004\036\003 010\013\006 013\004\005\011\004\011\024\012#\006 017\006 036\033\012\022\003\022\006\005\013\033\024\006\005\015\024\013\033\036\015\006\014\013\025\003\006\003@\005\003\004\014\012\013\004\014\006\013 \006\005\015\003\006\003\002\003\004\005\006\027\011 010\027\033\010\033\014#\006\026\013\025\016\033\005\011\005\012\013\004\011\010\006 032\004\005\003\010\010\012\036\003\004\027\003!47>$?!\006\016\016#8%&<89&!\006$%%%#\006 3&5\006 Z#\006 Z\011\004\036!\006 006 012\003\024\027\003!\006 011\004\022\006 X#\006 026\011\024/\013\004\003\010\010!\006 017\006 014\005\033\022\020\006 013 006 024\003\005\024\013\014\016\003\027\005\012\002\003\006 011\004\022\006 013\004<\010\012\004\003\006\003\002\003\004\005\006\022\003\005\003\027\005\012\013\004#\006"\024\013\027\003\003\022\012\004\036\014\006\013 \006\005\015\003\006$4\014\005\006\011\004 004\033\011\010\006\012\004\005\003\024\004\011\005\012\013\004\011\010\006\017\026.\006 037\032O\032\007\006 027\013\004 \003\024\003\004\027\003\006 013\004\006 007\003\014\003\011\024\027\015\006 011\004\022\006\022\003\002\003\010\013\016\025\003\004\005\006 012\004\006 012\004 013\024\025\011\005\012\013\004\006 024\003\005\024\012\003\002\011\010!\006 016\016#$A<87!\006466A#\006 3A5\006 006 006 031#\006 Z\011\004\036!\006 037#\006 026\015\033\011!\006 037#\006 C\011\004\036!\006 011\004\022\006 026#<N#\006 N\013\015!\006 037\005\024\033\027\005\033\024\003\022\006 033\014\003\006 013 \006 003@\005\003\024\004\011\010\006 2\004\0130\010\003\022\036\003\006 013\024\006 003\002\003\004\005</\011\014\003\022\006 013\016\003\004\006 022\013\025\011\012\004\006 033\003\014 005\012\013\004\006 011\004\0140\003\024\012\004\036#\006 024\013\027\003\003\022\012\004\036\014\006 013 \006 005\015\003\006 7\005\015\006 011\004\004\033\011\010\006 012\004\005\003\024\004\011\005\012\013\004\011\010\006 017\026.\006 037\032 O\032\007\006 027\013\004 \003\024\003\004\027\003\006 013\004\006 007\003\014\003\011\024\027\015\006 011\004\022\006 022\003\002\003\010\013\016\025\003\004\005\006 012\004\006 012\004 \013\024\025\011\005\012\013\004\006 024\003\005\024\012\003\002\011\010!\006 013\024\013\004\005\013!\006 026\011\004\011\022\011!\006 017\026.\006"\024\003\014\014!\006$%%8#\006 365\006 O\011\004\005\003\024\006 021!C\012\010\010\003\006 007#\006 023\013\024\025\011\010\006 026\013\004\027\003\016\005\006 017\004\011\010\020\014\012\014 001 011\005\015\003\025\011\005\012\027\011\010\006 023\013\033\004\022\011\005\012\013\004#\006*\0030\006Z\013\0242 001 037\016\024\012\004\036\003\024<B\003\024\010\011\036 002 010666\006 34%5\006\007#\006\001#\006.\013\013\024\003\006\011\004\022\006\023#\006\021\012\003\024/\011\033\025!\006.\003\005\015\013\022\014\006\011\004\022\006\017\016\016\010\012 027\011\005\012\013\004\014\006\013 \006 032\004\005\003\024\002\011\010\006 017\004\011\010\020\014\012\014#\006 006 006 006 006 006 037\013\027\006 \013\024\006\032\004\022\033\014\005\024\012\011\010\006 006 017 016 016 010 012 003 022 006  011 005 015  006 4 6&6#\006 3445\006Q\012\003\006\030\015\012<\016\003\004\036!\006\034\012\033\006\030\013\004\036<\005\012\011\004#\006\032\004\005\003\004\005\006\024\003\022\033\027\005\006\013 \006 027\013\004\027\003\016\005\006\010\011\005\005\012\027\003\006\004\013\022\003\006\011\004\022\006 012\005\014\006 027\013\025\016\033\005\012\004\036#\006 026\013\025\016\033\005\003\024\006 001\004\036\012\004\003\003\024\012\004\036!\006 4!\006 8?!\006 016 016#6<44#\006 012\004\006 026\015\012\004\003\014\003\006 0\012\005\015\006\001\004\036\010\012\014\015\006\011/\014\005\024\011\027\005?\006 34$5\006 O\013\022\012\004\006 007!\006 012\014\014\011\013\033\012\006 007!\006 017\010\011\013\033\012\006 031#\006 032\004\027\024\003\025\003\004\005\011\010\006 027\013\004 027\003\016\005\006 013\024\025\011\005\012\013\004\006 011\010\036\013\024\012\005\015\025\014\006/\011\014\003\022\006\013\004\006O\011\010\013\012\014\006>\027\013\004\027\003\016\005?\006\010\011\005\005\012\027\003\014#\006\026\013\025\016 033\005\011\005\012\013\004\011\010\006\032\004\005\003\010\010\012\036\003\004\027\003!\006 466;!\00644>$?!\006\016\016#$97<$7&\006 006 006 006 006 
1116 
1116 


3 Add\(A1.1\,A dd\(A1.2\,Add A1.3 T800 T900 3 Add\(A1.1\,A dd\(A1.2\,Del A3.2 T100 T800  5. Conclusion  Frequent item sets acquisition based on the set operations of item-transaction list transforms the transactions list into item-transaction list. By doing some set operations on this list, the final frequent item sets is gotten. The analysis of algorithm shows that it is simple and costs little in space & time. Also , the actual application proves it is easy and worthy to be programmed. Moreover, the final results gotten by the algorithm not only include the frequent number and frequent items but also the transactions list corresponding to the frequent item sets, which can be applied to study whether the association rules are ordinary rules or individual rules, and it would be our future task  Acknowledgements  This paper is supported by the National Natural Science Foundation of P.R. China \(Grant no. 60474022 the specialized Research Fund for the Doctoral Program of Higher Education of China under Grant No 20060613007, and the Youth Found of Education Office in Sichuan Province, P.R. China \(Grant no. 2006B044  References  1  Agrawal R, Imielinski T, Swami A. Mining Association Rules Between Sets of Items in Very Large Databases[C In  Proc eedi n gs of  t h e A C M  SIGMOD Conference on Management of Data Washington, USA, 1993.05: p207-p216 2  Rakesh Agrawal, Ramakrishnan Srikant. Fast Algorithms for Mining Association Rules C In Proceedings of the 20th VLDB Conference, Santiago Chile, 1994, p487-499 3  H.Mannila, H.Toivonen, A.I.Verkamo. Efficient algorithms for discovering association rules C  In Proc. AAAI'94 Workshop Knowledge Discovery in Databases KDD'94 Seattle, WA, July 1994 pp181- 192 4  Park J S, Chen M S, Yu P S. An effective Hash-based algorithm for mining association rules [A  Proceedings of 1995 ACM-SIGMOD int’l Conf. on Management of Data \(SIGMOD’95 a n Jos e  CA: 1995:p175-p186 5  Savasere A , Omiecinski E, Navathe S. An efficient algorithm for mining association rules in large databases[A V L D B 95[C  1995 p 432p4 43  6  Toivonen H. Sampling Large Databases for Association Rules[A Proce e di n g s of 22t h V L D B  Conf[C m b ay I n di a, 1996  p134p14 5  7  Brin S, Motwani R, Ullman J D, et al. Dynamic item set counting and implication rules for market basket data[J ecord A C M Special I n teres t  Group on Management of Data\97, 26\(2\ 255 8  Agarwal R C, Aggarwal C C, Prasad V V V. A tree projection algorithm for generation of frequent item sets r n a l of P a ral l el an d D i s t ributed Computing, 2001, 61\(3\ p350-p371 9  Han J., Pei J., Yin Y., Mining frequent patterns without candidate generation, Proc. of ACM Int Conf. on Management of Data, Dallas, TX, May 2000, p3-p12 10  Fan Ming, Li Chuan. Mining frequent patterns in an FP-tree without conditional FP-tree generation  J Journal of Computer Research and Development 2003, 40\(8\ p1216-p1222 11  Grahne G, Zhu J. Efficiently using prefix-trees in mining frequent item sets[Z  I n  1 s t W o r k s h o p o n  Frequent Item set Mining Implementation\(FIMI’03 2003 12  Liu J, Pan Y, Wang K, et al. Mining frequent item sets by opportunistic projection [A h e 8t h  A C M  SIGKDD International Conference on Knowledge Discovery and Data Mining[C A l bert a C a n a da  2002: p229-p238 13  Zaki M J, Gouda K. Fast vertical mining using diffsets[Z P r o c Of ACM SIGKDD  0 3   Washington, DC, 2003   
175 
175 
175 


The KDD Cup 99 dataset includes a set of 41 features derived from each connection and a label which speci  es the status of connection records as either normal or speci  c attack type. These features had all forms of continuous, discrete, and symbolic with signi  cantly varying ranges falling in four categories[17 KDD d a taset is d i v i d e d in to tr ain i n g  and testing record sets. Total number of connection records in the training dataset is about 5 million records. This is too large for our purpose; as such only concise training dataset of KDD, known as 10 training dataset, was employed here In computer simulation, we have to consider the following assumption: population size N pop 100 replacement percentage P rep 20%. Crossover rate P c 80%, Mutation rate P m 5%, Termination condition Maximum generation=100\Table 1 compare the different algorithm performances, just as shown in Table 1, the total performance of our algorithm is better than other algorithm Table 1. Different algorithms performances Algorithm Detection rate False alarm rate Our alghrithm 97.29 0.27 Ishibuchi[13 95 02  0.2 4  EFRID[14 98 15  7.0  5. Conclusions  In this work, a genetic fuzzy rule-based classifier has been designed by using fuzzy rule iterative learning algorithm in instruction detection system. To reduce the search space of fuzzy rule candidate, the population is initialized with the individuals randomly chosen among the pre-screened rules. The pre-screening process is completed by the usage of support and confidence. A new fitness function based on NC s pres e n t e d i n  t h i s paper, In f act t h e  performance of the final classification which was constructed according to the new fitness function was compared to several classi fication algorithms. Result showed that the presented algorithm was capable of increasing the detection rate and decreasing the false alarm rate simultaneously Acknowledgments This work was supported by the FuJian Province Department of Education \(JA05300 References  1 Mur a li.A R a o.M 2 0 0 5 A sur v ey on intr us i on de te c t i o n  approaches. In: First International Conference on Information and Communication Technologies .pp.233-240  Non g  Y   Qi a n g C  B o rro r C  M   2 0 0 4  E W M A f o recast  of normal system activity for computer intrusion Detection IEEE Trans, Reliab. 53\(4\557-566  A x elsso n  S   20 00  In tr u s ion d e tectio n sy ste m s: a su rve y  and taxonomy. Technical report no. 99-15, Department of Computer Engineering . Chalmers University of Technology Sewden 4 T i a n J F Fu.Y   W a ng J L  20 05. I n tr us ion de te c tio n  combining multiple decision trees by fuzzy logic. In: Sixth International Conference on Parallel and Distributed Computing. Application an Technologies,5-8 December 2005. pp.256-258 5 L  A  Za de l, Role of so f t c o m puting a nd f u z z y log i c in the  conception design and development of information intelligent systems, Lecture Notes in Computer Science 695\(1998\1-9 6 M.s  A b a d e h  J  H a bi bi, C  L u c a s  I n tr us i o n de te c tio n us i n g  a fuzzy genetics-based learning algorithm. Journal of Network and Computer Applications\(2005  7 J  E.D i c k e r s on, J  J u s lin O  K ouk ous oula  J  A  D i c k e r s on Fuzzy intrusion detection, in: Proceedings of IFSA World Congress and 20 th North American Fuzzy Information Processing Society Conference, NAFIPS2001, Vancouver British Columbia,July2001,pp.1506–1510  H Ish i b u c hi  T  Nakash i m a  T  M u rat a  A f u zz y cl assi f i er  system that generates fuzzy if-then rules for pattern classification problems, In: Proceedings of 2nd IEEE International Conference on Evolutionary Computation 
918 


Perth, Australia, 29 November-1 December 1995, IEEE vol.2, 1995, pp. 759-764 9 J  L i u, J  K w ok  A n e x te nde d g e ne tic r u le induc ti on  algorithm, In:Proceedings of the Congress on Evolutionary Computation Conference, 16-19 July 2000, La Jolla, CA USA, vol. 1, 2000,pp. 458-463 10 J.H  H o lla n d A d a p ta tio n in N a tura l a n d A r ti  cial Systems, University of Michigan Press, Ann Arbor, 1975 1 J G o m e z D Dasgu p t a E v o l vi n g f u zz y cl as si  ers for intrusion detection, in: Proceedings of IEEE Works hop on Information Assurance, United States Military Academy West Point, New York, June 2001, pp. 68–75  12 D  S ong M.I  H e y w ood, A  N. Zinc ir H e y w ood, T r a i ning  genetic programming on half a million patterns: an example from anomaly detection, IEEE Transactions on Evolutionary Computation 9\(3\ \(2005\ 225–239, doi: 10.1109/TEVC. 2004 841683 13 G  Flor e z  S  M.B r i d g e s  R  B  Va ug hn, A n im pr ov e d  algorithm for fuzzy data mining for intrusion detection, in Proceedings of North American Fuzzy Information Processing Society Conference, NAFIPS 2000, New Orleans LA, June 2002, pp. 457–462 14 Y  J i n  W  v on Se e l e n B  Se ndh of f  O n g e ne r a ting FC 3 fuzzy rule systems with data using evolution strategies, IEEE Trans. Syst. Man Cybern.—Part B: Cybernetics 29 \(6\ \(1999 829–845 15 Y  J i n Fuz z y  m ode ling of hig h d im e n s i ona l s y s t em s   complexity reduction and interpretability improvement, IEEE Trans. Fuzzy Syst. 8 \(2\ \(2000\ 212–221 1 H Rou b o s  M  S e t n es Co m p act an d t r an sp aren t f u zz y models and classi  ers through interactive complexity reduction, IEEE Trans. Fuzzy Syst. 9 \(4\ \(2001\ 516–524 1 KDD Cu p  1 9 9 9 I n t r u s i o n  d e t ect i o n d a t a set   http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html  
919 


                                                                                                                 
456 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efÞcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efÞciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efÞciency When the clustering model is available it is a signiÞcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques SufÞcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efÞcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efÞcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conÞdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conÞdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conÞdence The sufÞcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efÞcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efÞciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufÞciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70Ð81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207Ð216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487Ð499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145Ð154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146Ð153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9Ð15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597Ð600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern ClassiÞcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155Ð162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512Ð521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1Ð12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265Ð278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283Ð304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476Ð482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559Ð563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220Ð231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13Ð24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10Ð17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549Ð550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188Ð201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259Ð283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909Ð921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305Ð345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432Ð444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conÞdence Intelligent Data Analysis  9\(4\:381Ð395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49Ð73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407Ð419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1Ð12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki EfÞciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642Ð658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483Ð490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194Ð203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344Ð353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efÞcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103Ð114 1996 
618 
618 


