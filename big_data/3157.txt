The Development and Application of Qu ick Search Engine Based on Special Fields  Shouning Qu 1 Bing Zhang 2 Wei Wang 3 Xinsheng Yu 4  1School of Electrical Engineering and Automation, Harbin Institute of Technology, qsn@ujn.edu.cn 2School of Information Science and Engineering, University of Jinan, tanghulu116@163.com 3School of Electrical Engineering and Automation, Harbin Institute of Technology wangwei602@hit.edu.cn 4Shaoxing Electric Power Bureau, Zhejiang Electric Power Corporation, xinshengyu2007@163.com  Abstract  This paper analyzed some problems existing in the business search engine when it searched the special fields, and then put forward a set of search engine scheme about data warehouse design based on data mining. It applied the improved association rules algorithm to text clustering algorithm. At the same time in combination with the advantages of J2EE architecture in system development, this paper described how to apply J2EE platform to form an exact and efficient intelligent search engine system by the technologies such as JSP, Servlets EJB, JDBC and so on  1. Introduction Along with the development of Internet and the increase of all kinds of network information, it becomes more and more difficult for people to find out the useful information which is really needed by them. So we have to do our research by dint of search engine. Now there are some existent search engines, such as Google, Baidu etc The technologies of them have become mature, but they can’t meet the need of users’ search in special fields There are many existing problems. For example, the veracity is low, the information returned back is too much, the classification to information is too general some results returned back is in disorder and so on. So an intelligent search engine based on special fields is demanded urgently At present, there are mainly two information search methods in the search engine: catalog search and full-text search. The former searches the information by manual work. It is correct comparatively, but it usually leads to some problems. Such as information in background database can’t update in time and so on. The latter processes the text by full-text searching technology Users search the documents or web pages by keywords and it could return back all information which contains these keywords, so it usually leads to such problem as that the information returned back is too many 1 Upon that, this paper put forward a set of search engine scheme about data warehouse design based on data mining and it applied the improved association rules algorithm to text clustering algorithm. By this way the system could draw out information intelligently and update the database automatically. At the same time, J2EE technology is used to develop the system to make up the defects of current the business search engine when it is used to search the special fields 2. Relevant algorithm and key technology The performance of intelligent search engine is mainly evaluated by its accuracy and speed. The tolerable time of users waiting in front of the browser is generally limited so the speed of the system responding to the users should be fast. On the other hand, users are usually anxious to receive the accurate information, so the accuracy of system should be high 2.1. The improvement and application of association rule algorithm Along with the increase and accumulation of vast data association rule has become an important aspect to the field of data mining. Apriori association rule mining algorithm is one of the classical algorithms, but it has some defects 8 According to these defects, the paper puts forward an improved association rule algorithm. The details are described as follows Input: Database D of transaction; minimum support threshold min-sup Output: the frequent sets L of D L 1 find_frequent_l-itemsets\(D\     k=1 do while\(L k-1 001 002 k find_frequent_itemsets\(min_sup move record from D where itemnum=k-1 loop   end 
2008 International Symposiums on Information Processing 978-0-7695-3151-9/08 $25.00 © 2008 IEEE DOI 10.1109/ISIP.2008.13 265 
2008 International Symposiums on Information Processing 978-0-7695-3151-9/08 $25.00 © 2008 IEEE DOI 10.1109/ISIP.2008.13 265 


In order to compress C k the Apriori algorithm uses the prune step. The prune codes are described as follows Procedure has_infrequent_subset\(c:candidatekitemset L k-1 frequent\(k-1\_itemset for each \(k-1\ubset s of c if s  L k-1 then  return true;   else  return false 2.2. Text clustering As the text clustering here is the classical short document clustering the paper puts forward an improved k-means clustering algorithm. The detailed process of this algorithm is shown as Figure1  Figure 1  Flow chart of clustering Firstly, the documents should be processed by the Chinese word segmentation technology, which is used in the paper is Maximum Matching Method. Chinese word segmentation technology belongs to the category of natural language processing technology 9 The flow of the thought is described as follows: calculate the length of the document firstly, choose n characters from the left to right within the range of the document’s length. Here n should be determined by the length of the longest word in the background words database. Then it matches the n characters with the words in database, judging it matches successfully whether or not. If it dose, segment a word and cut the word from the documents; if not, subtract a character from the right of the n characters, continue matching. If it doesn’t match successfully until there is only one character left in the n characters, it implies that this word isn’t contained in database. Next, begin from the second character of the document, choose n characters again, and then continue matching with the words in database. If it matches successfully, the next step should begin from the character after the last character of the word. The word segmentation algorithm is applied to every document and the results are saved in a table The following step is separated into two parts: on the one hand, weight calculation is applied to every word of the document. Considering the effect of the length of the document to weight, the paper uses TFC calculation method which is normalizable. The formula is shown as 1   ij i 2 kj k 1 TF log\(D/DF  weight\(i TF log\(D/DF  M K    1 In the formula, TF ij is the frequency of the key word i in document j. D is the number of all documents. DF i is the number of documents that have the key word i. Then use the Vector Space Model to denote the text eigenvector by weight On the other hand, all documents are regarded as transaction database, each document is regarded as a transaction and the key words in documents are regarded as a group of items. In this way, the problem of association analyzing between the key words in text database turns to the association mining between the items in transaction database. At the same time, the problem of similarity analyzing between the questions turns to be the problem of association rule between the items in transaction database. It calculates the correlation value between key words in documents to form the association matrix. Here, when calculating the correlation between key words, the association front and rear are all written in the association table, the format of the association table is described as \(no, front, rear, S, C The meaning of it is \(the serial number of association rule, association front, association rear, support confidence 2.3. Similarity computation  The paper computes the similarity degree based on association rule. It assumes that one document in the database is composed by m words W 1 W 2 W m   another one is composed by n words W 1  W 2 W n   then the relationship matrix between the two documents can be expressed as follows         11 12 13 1m 21 22 23 2m n1 n2 n3 nm RRR R RRR R RRR R R nm is the correlation value between the keywords W n  and W m The detailed algorithm is described as following if W m W n then R nm 1 else for each record of association table if W 1 and W 2   record  then i++; R nm i-a\/\(i +a\ // a is a default parameter else R nm b // b is a default parameter The value of a \(or b\ lies on the numbers of databases They are generated randomly at first and finally determined through testing repeatedly After calculating the correlation value between key words, the similarity degree between sentences can be computed. The weight of document 1 W 1 W 2 W m  can 
266 
266 


be described as T 1 T 2 T m  and the weight of document 2 W 1 W 2 W m  can be described as T T 1 T 2 T n  The algorithm of similarity degrees calculation between the two documents shows as follows S i S i sum1=sum2=0 For \(i=1; i<=n; i For \(j=1; j<=m; j S i S i T i R[i    S i is the correlation value of Wi' and the document 1 S i S i T i R[i    S i is the correlation value of W i and the document 2 000\003 sum1=sum1+S i sum2=sum2+ S i end i sim=\(sum1+sum2\/2;//sim is the similarity degree between the document 1 and the document 2 After computing the similarity degree, the clustering algorithm can be executed to gain some cluster of subsets then the association rule algorithm is executed on every subset, viz. the clustering algorithm is used as the pretreatment of association rule. This step can be executed for n times to improve the veracity of clustering 2  3. Structure design of system data warehouse The process of the data warehouse mainly includes three steps: marshal all kinds of original data, manage the data and obtain the information which is needed 3 The original database is composed of the relative stable text information on the network connected with special fields and their URL. The original format of the information is the description of out-of-order and no classified text information, the description of keywords and the description of its URL. As it is a search engine based on special fields, every field should have its special words database. Here, the special words database is the aggregate of all keywords in the fields and it should update with the change of the information. In order to improve the recall ratio, the thesaurus database is needed It organizes the database by k-means text clustering algorithm to classify these information. When users raising a question, there are two methods can be chosen One method is as follows: the user should input the nature language description of the question or the keywords of the question, and the user should fix the question to a certain field or more than one field to help the system to search the question in some certain fields fleetly by similarity computing, and then users could get the best information or the reference information from this certain field. In this way the speed of information searching can be advanced consumedly. Another method is that the users don’t choose the fields that questions belong to, then the system will traversing all special words databases of fields to search As most of questions raised by users are similar, if the information warehouse is designed successfully enough at the beginning, lots of questions can be included in. The system could return the correlative information in time After the system used for period of time, the background data warehouse will be extended and updated automatically due to the new information on the web site The practicality of the system will be better and better The flow chart of the data warehouse formation is as figure2  Figure 2  The formation of data warehouse 4. Implementation of system J2EE four tiers  The system’s main function structure is as figure 3  Figure 3  System function structure Th e main purpose of the system setting to save users information is to track and record the information interested by them. It can be considered as a referenced modulus to help the system to return the information which users really need as exactly as possible, and to filtrate the irrelevant information as much as possible The classical J2EE application structure includes four tiers: Client tier, Web tier, Business tier and Enterprise information system tier \(EIS tier\. The second tier and the 
267 
267 


third tier here are called by a joint name Middle tier 4  The J2EE structure figure of this system is as Figure 4  Figure 4  J2EE structure of search engine Client tier applied to this system is composed of two parts: one is the Client based on browser, it orients the numerous users who are dispersive and change frequently The function of this part is difficult to deploy by the conventional Client. Besides, the privilege of users is limited to raising a question or submitting the information which is not included in database. It doesn’t relate to the matter such as modifying the system information directly so it affects the system security little by network. The flow is described as follows: users start up the browser at Client, connect with web server which could create the dynamic HTML information by networking, then transmit the service required by users to Business tier to do some interrelated analysis, finally get the correlative information by querying the database in EIS tier. The system will return the information in the format of web pages to users, i.e. users input the question at Client, the question will be processed by search engine server, the correlative information can be returned by querying the system database at last. The other one is the special Client based on C/S pattern, it orients the administrators Applying the special Client to system maintenance and management is mainly for the sake of system security The design of Presentation layer based on Web Client mainly includes the register JSP and login JSP. The paper applies the MVC pattern which is popular in web applications to develop this part of the system. Here MVC is Model-View-Controller. In this pattern, View is the interface used to communicate among users 5 The application of it in this system is some dynamic JSP pages, such as the interface used to login, the interface used to raise a question and so on. These interfaces are deployed in Web Server. Model is the main body of the application, it is used to express business data and business logic. The application of it in this system is the EJB groupware \(the primary groupware is Entity Bean running in EJB container, including users entity administrator entity etc. In addition, all kinds of data manipulation limited in these entities could be regarded as a part of Model; Just as its name implies, Controller is used to control. It receives the user’s input and responds the user’s requests by transferring Model and View When users submit the form, Controller doesn’t do any process or output, what it done is only to receive the diversified requests of all users. Then the system processes these requests by transferring Model groupware, such as browsing the userinfo, querying database etc. Finally it displays the data processed by Model through transferring View. The application of View in this system is the Servlet designed by relevant JSP .When users give the request, JSP will transfer the request to Servlet, and then Servlet could help to accomplish the functions such as raising a question or querying database by transferring the data processing through the relevant EJB groupware found by JNDI The following describes the system processing flow through the request of user raising a question. The user should choose the function of raising a question firstly then the system will return the correlative information to JSP by transferring the Servlet in background database according to the information of user’s question. The detailed flow is described as following: when users submit the question to system by network randomly, the system should comprehend this question firstly, and then matches the question to the information in database by computing the similarity between the question described in nature language and the information in background database. If matching successfully, the correlative information will be returned to users, ending the information extracting process; if matching unsuccessfully, it means there is no correlative information in database, and then the question will be submitted to the database as an unsolved question waiting the information on web site updates later. Here the system intercalates a prohibitive words database used to save the key words which is forbidden to be discussed in this system. The flow chart is as Figure 5 The purpose of Presentation layer development is to provide users with dynamic Web pages when they visit the system. Meanwhile it is in charge of communicating with EJB in Business, so that the user’s request can be responded. EJB groupware is the architecture of J2EE platform 6 Session Bean in J2EE application is applied to do some processes which are in server, for example accessing the database, transferring other EJB module 4  It is mainly used to communicate with fore-end Presentation layer, such as saving the user’s login information, recording the data that user’s operation to system \(information such as writing the question into 
268 
268 


unsolved question database etc.\, transferring Entity Bean to respond the user’s request to Information layer and so on. Entity Bean represents the data that preserved permanently, the typical type is the data saved in database. It can meet the user’s demands by accessing those data, and it defines a series of data operation to fulfill all functions that system demand. The kind of entity is mainly determined by the condition of information table in background database, for example, this system includes the user information table, information table words table and so on; Message-Driven Bean is just like an asynchronous information receiver who has realized some business logic, it is mainly used to process the asynchronous information. When JMS \(Java information queue\ receives a message, Message-Driven bean is transferred by EJB container. Message-Driven bean in this system is mainly in charge of sending important information to each function module entities, for example opening the individual information of users, amending the individual information and so on  Figure 5  Flow chart of search engine The last step is the design of Information layer. The J2EE application groupware often need to access the EIS tier to obtain the required data information 7 The data information of this system includes fundamental information of users, information of keywords information of the background information database and information of the unsolved questions etc In the system exploitation and deployment, WebLogic Server in BEA Company is used as the J2EE application server. It can not only provide the application function of managing the J2EE application and other unattached application in the way of local mode and long-distance mode, but also can provide the application function of simplifying the structure of these applications 5. Conclusion The architecture of J2EE is core when designing and developing this search engine. It uses JSP, Servlets, EJB and JDBC database connectivity technologies. Compared with  others, the most obvious advantage of this intelligent quick search engine is that it can return accurate correlative information to users timely. In the other words, it can extract the optimal information in the shortest time from the background database and it embodies the intelligence of extracting information well References 1  Junping Qiu, Yisheng Yu, “The Research of Intelligent Search Engine Based on KBS Journal Information science, 2006, 3, pp. 413-416 2  Shouning Qu, Qin Wang, “Intelligent Question Answering System Based on Data Mining Journal J.of ZhengzhouUniv.\(Nat.Sci.Ed.\, 2007, 6, pp. 50-54 3  Shan Wang, Database technologies and On-Line analytical processing, TECHNOLOGY PRESS, Beijing, 1998 4  Qiang Zhao, Xinliang Qiao, J2EE Application development Weblogic+JBuilder\, PUBLISHING HOUSE OF ELECTRONICS INDUSTRY, Beijing, 2003 5  Lei Ji, Li Li, and Wei Zhou, Master J2EE—Eclipse, Struts Hibernate, Spring Conformity Application Cases POSTS&TELECOM PRESS, Beijing, 2006 6  Vlada Matena, Sanjeev Krishnan, Enchiridion of EJB Application—Development Based On Groupware, QING HUA UNIVERSITY PRESS, Beijing, 2004 7  Yulong Hao, Technology of J2EE Programming QINGHUA UNIVERSITY PRESS, Beijing, 2005 8  U. Savasere A, “An efficient algorithm for mining association rules Journal  Proceedings of the 21st International Conference on VLDB 1995, 9, pp. 432–444 9  I. Fabrizio Sebastiani, “Machine learning in automated text categorization Journal  ACM Computing Surveys pp. 11 12 
269 
269 


  Proceedings of the Sev e nth Inter n ational Conference on Ma c h ine Lear ning and Cyber n etics  K u nming, 12-15 J u ly 2008          In fi g u re 1, t h e y ax is represen ts t h e tim e spen t in  readi ng i n a bat c h of st r e am const r uc t i ng i t s l o cal  DSC F C I t ree  up dat i n g t h e gl o b al DSC F C I-t ree  an d pr o duci n g f r e que nt cl o s ed i t e m s et s sati sfy i ng i t e m  constraints B The y axis i n figure 2 re presents the  me m o ry capacity used for storing the gl obal  DSCFCI-tree As ca n be see n fr om t h e t w o f i gu res whe n t h e val u e of s u pp o r t   i s fi xe d, t h e t i m e and space re qui rem e nt s of t h e algorithm grow as th e stream progre sses, but tend to sta b ilize. For e x am ple, the m e m o ry re qui red by the  global DSC F CI-tree with su ppo r t 0  00 15  in cr eases relativ ely q u i ck ly as th e first sev e n  b a tch e s o f st ream arriv e and tend s to stab ilize at ro ugh ly 3  8MB with sm a l l  bum ps. T h is is because the num b er of pote ntial fre que nt clo s ed item s e t s satisfyin g ite m co n s trai n t s g o es up  relativ ely q u i ck ly at first, and b e co m e s relativ ely stead y  later th ro ugh in tr odu cing p r un ing tech no log y Th e stab ility resu lts are qu ite n i ce as th ey p r o v id e ev id en ce th at th e algorith m can  h a n d l e lon g  d a ta strea m s. As t h e v a lu e o f suppo r t   dec r eases the e x ecution tim e in fig u re 1 a n d t h e m e m o ry requi red by  DS CFCI-tree in  figure 2 inc r ea ses quickly  This is because t h ere are m o re p ot ent i a l  f r e que nt cl ose d i t e m s et s sati sfy i ng i t e m  const r ai nt s wi t h sl ower support    s s Fi gu re 1 an d F i gu re 2 sh o w t h e e x peri m e nt al res u l t s  o f  d a ta sets T1 5I7 D 1 000k  Th e ex per i m e n t al r e su lts of  o t h e rs two d a ta sets are sim ilar to th ese fig u r es                           Fig u re 3 shows th e av erag e t i m e sp en t i n  up d a ting  t h e gl obal DS C F C I-t ree o n c e  with three dif f eren t ite m con s t r ai nt s c onst r ai nt  0 n o c onst r ai nt s co nst r ai nt  1  co nst r ai nt 2   e  can dra w a c o nclusi on that w ith t h e reinforcem en t o f th e constraints 222  degree the num b er of fre que nt close d  ite m s ets satis fyin g co rrespo nd ing item co n s t r ain t s d ecreases, and th e tim e sp en t i n  up d a ti ng  th e g l o b a l DSCFCI-tree dec r eases t o o. M o re ove r  the space  requirem ent of t h e global DSCFCI-tree go es d o wn  sim u l t a neousl y The r ef ore  i n t e g r at i ng i t e m const r ai nt s in to t h e m i n i n g algorith m can  reduce t h e e x ecution tim e  and space com p lexity of the algorithm    4 3   2 1  005 005 005 5.   Concl u si ons In this pa per   we propose a n ef ficient algorith m for m i ni ng fre q u e n t cl ose d i t e m s et s i n dat a st ream s. The co n t r i bu tio n s of ou r stud y in clu d e 1 pro posin g a no v e l  dat a  st r u ct ure  DSC F C I t re e, fo r st ori n g pot e n t i a l   fre que nt cl ose d i t e m s et s, and de vel o pi n g  a new m e t hod  for increm ental updating DSCFCI-t ree ef ficiently 2 appl y i n g a  pr u n i n g t e c hni que f o r ef fi ci ent  p r u n i n g gl obal  DSCFCI-tree  to reduce the s p ace re quire m e nt of t h e  DSCFCI-tree an d th e tim e sp en t in trav ersi n g the DSCFCI-tree dram atica l l y 3  teg r ating ite m con s t r ai nt s i n t o t h e m i ni ng a l go ri t h m and t hus  red u ci n g  furthe r t h e e x ecution tim e and sp ace co m p lex ity of th e const r ai nt 0 S=0.001 T ime\(s S=0.0015  const r ai nt 1 const r ai nt 2 S=0.002  N umber of data stream segments Fi gu re 3 c o m p ari s on of e x ecut i o n  t i m e wi t h di f f erent const r ai nt s\(s=0.0015 Figure 2.The m e m o ry usage of DSCFCI-tree   279 005 004 005 4 3 2 1 


  Proceedings of the Sev e nth Inter n ational Conference on Ma c h ine Lear ning and Cyber n etics  K u nming, 12-15 J u ly 2008  al gori t h m   Our performance st udy shows  that DSCFCI  algorithm is ef ficient and ef fective  Refer e nces 1] C Gia nnel l a, J Ha n, J  Pei, et al. Mi ning fre quent p a ttern s i n  d a t a stream s at mu ltip le tim e g r an u l arities  G]. In  H Kar g up ta, A Jo sh i, K Siv a ku m a r  et al, ed s Next  Ge nerat i on Dat a M i ni ng C a m b ri dg e, M a ss  M I T Press, 2003  2 G S Mank u, R Mo t w an i. App r ox im a t e f r e q u e n c y cou n t s ove r st ream i ng dat a  C    The 28 th Int 222 l Confere n ce on V e ry Lar g e Data Bases \(VL D B 2002 HongKong, 2002 3  Aras u A M a nk u G S  A p pr oxi m a t e co unt s a n d  q u a n tiles ov er slid in g wi n d o w s. In Pro c eed ing s  o f  th e 23 rd ACM S I G M OD SI G AC T S I G A R T  Sym posi u m on Pri n ci pl es o f  Dat a base Sy st e m s. Pari s France  AC M Press, 2004 4 Datar M, Gio n i s A, In d y k P   Mo t w an i  R. Main tain in g str eam stat is tics o v e r slid in g  w i ndow s In  Proceedi ngs of the 13 th A nnu al ACMSIA M  Sy m p o s iu m on  Discrete Al g o rith m s San Fran cisco   USA  AC M Press, 2002 5 N Pas quier  Y Bastide  R T a ouil, et al Discoveri n g freq u e n t clo s ed item s e t s fo r asso ciatio n ru les[C]. In   Beeri C, et al, eds  Proc of the 17 th I n t 222 l Co nf  on Dat a base Theory B e rl i n  Spri nger V e rl ag, 1999  6  W a n g Jian yon g. Clo s et sear ch ing fo r t h e b e st st rat e gi es f o r m i ni ng f r e que nt cl ose d i t e m s et s. In  Pr oc. N i n t h A C M SIGK DD In t\222 1 Co nf  on  K now ledg e D i scov er y an d  D a t a Min i n g  W a shi ngt on,DC Aug.2003    280 


association mining The Likelihood Ratio Test fails to extract features also belonging to common vocabulary and it makes the extraction dependent on the feature position in the sentence leading to low recall The dBNP and bBNP based methods yield low recall due to the fact that the product features do not occur with the article the in front of them very often The Association Mining approach returns all frequent nouns which decreases precision Our results suggest that the choice of algorithm to use depends on the targeted dataset If it consists of mainly on-topic content the results of Table 10 indicate that the Association Mining algorithm is better suited for this task due to its high recall If the dataset consists of a mixture of onand off-topic content our results suggest that the Likelihood Ratio Test based algorithm would perform better due to its ability to distinguish and 002lter out the off-topic features For future work we plan to extend the Likelihood Ratio Test methods especially the dBNP based approach by other determiners such as a or this  which should increase the recall of this method Another possibility which we will investigate regards the BNP patterns The current Likelihood Ratio Test approach is not capable of dealing with discontinuous feature phrases for example in 5 the quality of the pictures is great the feature would be picture quality  This problem could be addressed by introducing wildcards in the BNP patterns We will also investigate whether there are any methods in order to calculate an optimal threshold for the candidate feature extraction in order to increase the recall of the Likelihood Ratio Test based algorithm We plan to investigate whether a deeper linguistic analysis e.g with a dependency parser can improve the feature extraction Acknowledgements The project was funded by means of the German Federal Ministry of Economy and Technology under the promotional reference 01MQ07012 The authors take the responsibility for the contents The information in this document is proprietary to the following Theseus Texo consortium members Technische Universit  at Darmstadt The information in this document is provided as is and no guarantee or warranty is given that the information is 002t for any particular purpose The above referenced consortium members shall have no liability for damages of any kind including without limitation direct special indirect or consequential damages that may result from the use of these materials subject to any liability which is mandatory due to applicable law Copyright 2008 by Technische Universit  at Darmstadt References  R Agra w al and R Srikant F ast algorithms for mining association rules Proc 20th Int Conf Very Large Data Bases VLDB  1215:487–499 1994  K Bloom N Gar g and S Ar g amon Extracting a ppraisal expressions In HLT-NAACL 2007  pages 308–315 2007  R Bruce and J W iebe Recognizing subjecti vity a case study in manual tagging Natural Language Engineering  5\(02 1999  K Da v e S La wrence and D Pennock Mi ning the peanut gallery opinion extraction and semantic classi\002cation of product reviews In Proceedings of the 12th International Conference on World Wide Web  pages 519–528 New York NY USA 2003 ACM  T  Dunning Accurate methods for the statistics of surprise and coincidence Computational Linguistics  19\(1 1993  O Feiguina and G Lapalme Query-based summ arization of customer reviews In Canadian Conference on AI  pages 452–463 2007  C Fellbaum Wordnet An Electronic Lexical Database  MIT Press 1998  A Ferraresi Building a v ery lar ge corpus of english obtained by web crawling ukwac Master's thesis University of Bologna Italy 2007  M Gamon A Aue S Corston-Oli v er  and E Ringger  Pulse Mining customer opinions from free text In Proceedings of the 6th International Symposium on Intelligent Data Analysis IDA-2006  Springer-Verlag 2005  N Glance M Hurst K Nig am M Sie gler  R Stockton and T Tomokiyo Deriving marketing intelligence from online discussion In Proceedings of the 11th ACM SIGKDD International Conference on Knowledge Discovery in Data Mining  pages 419–428 New York USA 2005 ACM  M Hu and B Liu Mining opinion features in customer reviews In Proceedings of 9th National Conference on Arti\002cial Intelligence  2004  N K obayashi K Inui K T atei shi and T  Fukushima Collecting evaluative expressions for opinion extraction In Proceedings of IJCNLP 2004  pages 596–605 2004  S Morinag a K Y amanishi K T ateishi and T  Fukushima Mining product reputations on the Web In Proceedings of KDD-02 8th ACM International Conference on Knowledge Discovery and Data Mining  pages 341–349 Edmonton CA 2002 ACM Press  A.-M Popescu and O Etzioni Extracting product features and opinions from reviews In Proceedings of HLT-EMNLP-05 the Human Language Technology Conference/Conference on Empirical Methods in Natural Language Processing  pages 339–346 Vancouver CA 2005  H Schmid T reetagger a language independent part-ofspeech tagger Institut fur Maschinelle Sprachverarbeitung Universitat Stuttgart  1995  J W iebe R Bruce and T  O'Hara De v elopment and use of a gold-standard data set for subjectivity classi\002cations In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics  pages 246–253 Association for Computational Linguistics Morristown NJ USA 1999  J Y i T  Nasuka w a R Bunescu and W  Niblack Sentiment analyzer Extracting sentiments about a given topic using natural language processing techniques In Proceeding of ICDM-03 the 3ird IEEE International Conference on Data Mining  pages 427–434 Melbourne US 2003 IEEE Computer Society 
160 
151 


Figure 4 Expected and real number of extracted patterns using two promoter sequence datasets Horizontal axis minimum support vertical axis number of patterns 
85 
85 


a frequency constraint and according to the structure of the dataset These proposals are all based on a global analytical model i.e an interesting approach that needs however to develop complex and speci\036c models As a result they cannot be easily extended to handle complex conjunctions of constraints to incorporate different symbol distributions or different semantics for pattern occurrences To the best of our knowledge no method has been proposed to estimate the number of patterns satisfying a constraint while avoiding to develop a global analytical model Our approach requires only to know how to compute for a given pattern its probability to satisfy the constraint this can be obtained in many situations and it remains ef\036cient in practice by adopting a pattern space sampling scheme 6 Conclusion Using constraints to specify subjective interestingness issues and to support actionable pattern discovery has become popular Constraint-based mining techniques are now well studied for many pattern domains but one of the bottlenecks for using them within Knowledge Discovery processes is the extraction parameter tuning This is especially true in the context of differential mining where domain knowledge is used to provide different datasets to support the search of truly interesting patterns From a user perspective a simple approach would be to get graphics that depict the extraction landscape i.e the number of extracted patterns for many points in the parameter space We developed an ef\036cient technique based on pattern space sampling that provides an estimate on the number of extracted patterns This has been applied to non trivial substring pattern mining tasks and we demonstrated by means of many experiments that the technique is effective It provides reasonable estimates given execution times that enable to probe a large number of points in the parameter space Notice that domain knowledge is also exploited here when selecting the distribution model Future directions of work include to adapt the approach to other pattern domains and to different constraints Another interesting aspect to investigate is the use of more sophisticated sampling schemes e.g that could b e incorporated in the approach when more complex syntactical constraints are handled e.g a grammar to specify the shape of the patterns Acknowledgments This work is partly funded by EU contract IQ FP6-516169 Inductive Queries for Mining Patterns and Models and by the French contract ANR-MDCO14 Bingo2 Knowledge Discovery For and By Inductive Queries We thank Dr Olivier Gandrillon from the Center for Molecular and Cellular Genetics CNRS UMR 5534 who provided the DNA promoter sequences References  J F  Boulicaut L De Raedt and H  M annila e ditors Constraint-Based Mining and Inductive Databases  volume 3848 of LNCS  Springer 2005  C  B resson C K e ime C F a ure Y  Letrillard M  B arbado S San\036lippo N Benhra O Gandrillon and S GoninGiraud Large-scale analysis by SAGE revealed new mechanisms of v-erba oncogene action BMC Genomics  8\(390 2007  L  C ao and C  Z hang Domain-dri v e n actionable kno wledge discovery in the real world In Proceedings PAKDDÕ06 volume 3918 of LNCS  pages 821–830 Springer 2006  G  D ong and J  L i Ef 036cient mining of emer ging patterns discovering trends and differences In Proceedings ACM SIGKDDÕ99  pages 43–52 1999  F  Geerts B  G oethals and J  V  d en Bussche T ight upper bounds on the number of candidate patterns ACM Trans on Database Systems  30\(2 2005  U  K eich and P  A  P e vzner  S ubtle motifs de\036ning the limits of motif 036nding algorithms Bioinformatics  18\(10 2002  S  K ramer  L De Raedt and C  Helma M olecular f eature mining in HIV data In Proceedings KDDÕ01  pages 136 143 2001  L  L hote F  Rioult and A  S oulet A v e rage number of frequent closed patterns in bernouilli and markovian databases In Proceedings IEEE ICDMÕ05  pages 713–716 2005  I  M itasiunaite a nd J.-F  B oulicaut Looking for monotonicity properties of a similarity constraint on sequences In Proceedings of ACM SACÕ06 Data Mining  pages 546–552 2006  I Mitasiunaite and J F  Boulicaut Introducing s oftness i nto inductive queries on string databases In Databases and Information Systems IV  pages 117–132 IOS Press 2007  I Mitasiunaite C Rigotti S Schicklin L  M e yniel J F  Boulicaut and O Gandrillon Extracting signature motifs from promoter sets of differentially expressed genes Technical report LIRIS CNRS UMR 5205 INSA Lyon France 2008 23 pages Submitted  G Ramesh W  M aniatty  a nd M J Zaki F easible itemset distributions in data mining theory and application In Proceedings ACM PODSÕ03  pages 284–295 2003  F  Zelezn  y Ef\036cient sampling in relational feature spaces In Proceedings ILPÕ05  volume 3625 of LNCS  pages 397 413 Springer 2005 
86 
86 


