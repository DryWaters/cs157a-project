Big Data Solutions for Predicting Risk-of-Readmission for Congestive Heart Failure \nPatients  \n \nKiyana Zolfaghar, Naren Meadem, Ankur \nTeredesai, Senjuti Basu Roy, Si-Chi Chin \n Institute of Technology, CWDS, UW Tacoma \n{kiyana,mnaren,ankurt, senjutib, \nscchin}@u.washington.edu \nBrian Muckian \nMulticare Health System nTacoma, Washington \nbrian.Muckian@multicare.org\nAbstract—Developing holistic predictive modeling solutions nfor risk prediction is extremely challenging in healthcare \ninformatics. Risk prediction involves integration of clinical \nfactors with socio-demographic factors, health conditions, \ndisease parameters, hospital care quality parameters, and a \nvariety of variables specific to each health care provider \nmaking the task increasingly complex. Unsurprisingly, many \nof such factors need to be extracted independently from \ndifferent sources, and integrated back to improve the quality \nof predictive modeling. Such sources are typically nvoluminous, diverse, and vary significantly over the time. \nTherefore, distributed and parallel computing tools \ncollectively termed big data have to be developed. In this \nwork, we study big data driven solutions to predict the 30-\nday risk of readmission for congestive heart failure \(CHF useful factors from National \nInpatient Dataset \(NIS Health System \(MHS integrated dataset. We demonstrate \nthe effectiveness and efficiency of the open-source predictive \nmodeling framework we used, describe the results from \nvarious modeling algorithms we tested, and compare the nperformance against baseline non-distributed, non-parallel, \nnon-integrated small data results previously published to \ndemonstrate comparable accuracy over millions of records.  \nKeywords: Healthcare Knowledge-Discovery; Risk \nPrediction; \nI.  INTRODUCTION  \nHospital readmission is expensive and generally npreventable [1].  Reducing preventable readmission is \nconsidered a key quality of care parameter that is deemed \nmeasurable. Yet, it is still challenging to develop accurate \npredictive models to predict such risk and the importance \nof factors that contribute to readmission due to the \ndiversity of data sources even within a single large \nhospital. Add to this the aspiration of obtaining a holistic \nview of cause for readmissions by integrating socio-\neconomic parameters and external data with existing \nclinical data, and this problem becomes even more \nchallenging and complex requiring significant advances in \ndata integration discretization, normalization and data \norganization to name a few.  \nAt the same time, predicting risk of re-hospitalization \nfor chronic and potentially fatal diseases such as \ncongestive heart failure can result in significant cost \nsavings and improvement of care at many hospitals. A key \nquestion being asked today in health informatics is how \nbig data healthcare implementations can help correlate and \ncollate insights across various heterogeneous data sources \nto enable a better understanding of issues such as quality \nof care particularly for chronic conditions that lead to \nrepetitive readmissions.  \nIt was prohibitively difficult to store, manage and mine \nlarge volumes of structured and semi-structured health \nrecord datasets prior to the recent advances in big data \ninfrastructure [2]. One of the emergent abilities of new \nshared nothing distributed, and parallel computing \ninfrastructure is the ability to perform similar operations \non large amounts \(petabytes high \nvelocity, and diverse types of data \(variety of data computation closer to \nwhere the data is which is unlike the prior paradigm of \nhaving to move data around for large computations to \nhappen. Within the healthcare informatics setting, this \nability to process large amount of diverse unstructured, \nsemi-structured, and structured data enables clinical \ninformatics to develop new insights and discovery new \nknowledge by combining data from various sources. Such \nsources can be internal as well as external to the electronic \nhealth record \(EHR nhundreds of attributes, which can be leveraged for \npredictive modeling.  \nNew tools and programming paradigms for such data \nintensive applications leverage the distributed computation \nmodel. Apache Hadoop [3 is one such distributed \nframework that implements a computational paradigm \nMapReduce[4], where the application is divided into many \nsmall fragments of work, each of which may be executed \nor re-executed on a number of compute nodes in  a cluster \nof data intensive distributed applications.  In conjunction \nwith a distributed system such as Hadoop, the Apache \nMahout [5] framework provides a useful set of machine \nlearning libraries for implementing modeling tasks such as \nclassification and clustering though there is significant nneed to develop advanced domain specific \nimplementations of these algorithms. Mahout was \ndesigned to work in conjunction with Hadoop to scale for \nlarge datasets and compute clusters. Furthermore, \ndistributed query processing solutions such as Hive [6] and \nCassandra [7] are now available for distributed query \nprocessing and exploratory analyses, though very few case \nstudies are available that demonstrate their use in the nhealthcare setting. \n2013 IEEE International Conference on Big Data\n978-1-4799-1293-3/13/$31.00 ©2013 IEEE 64\nSuch tools enable important quality of care metrics to \nbe developed across hundreds of dimensions. In this 


work \nthe focus is on demonstrating how our implementation \nusing current big data tools leveraged this ability to \nmanage millions of events efficiently to develop accurate \npredictive models for estimating the risk of readmission of \ncongestive heart failure patients.   \nCongestive Heart Failure \(CHF identified as \none of the leading causes of hospitalization, especially for \nadults older than 65 years of age 8]. Furthermore, studies \nshow that CHF is one of the primary reasons behind \nreadmission within a short time-span [9]. Based on the \n2005 data of Medicare beneficiaries, it has been estimated \nthat 12.5% of Medicare admissions due to CHF were \nfollowed by readmission within 15 days, accounting for \nabout $590 million in health care costs [10]. The Center \nfor Medicare and Medicaid Services \(CMS 30 day all cause heart failure readmission rate as \na publicly reported efficiency metric. All cause 30 day nreadmission rates for patients with CHF have increased by \n11% between 1992 and 2001 [11]. \n \nIn collaboration with Multicare Health Systems \(MHS  npredicting risk of readmission for the real world CHF \npatient records provided by MHS. The necessity of nadopting big data solutions in this work is two fold – 1 extracting and \nintegrating additional important factors \(such as income NIS multiple classification \nalgorithms [13] for readmission prediction using \ndistributed analytics platform Mahout6.  In our \nexperimental analysis, we observe that a 16000 patient \ndataset with 76 attributes take 32.75 seconds to run giving \naccuracy of about 79% in Mahout, whereas a 1.5 million \npatient dataset with the same set of attributes takes 4 \nminutes 35 seconds \(compared to 4 hours 5 minutes, when \nrun in a non-distributed analytics platform nminutes 21 seconds to run \(it does not even run in a non-\ndistributed environment accuracy,  \ndemonstrating the effectiveness of our proposed solutions. \nOur primary contributions are: \na initiate the study of predicting 30-day risk-of \nreadmission problem for CHF patients using big \ndata solutions. \nb solutions \nfor predictive modeling using distributed \nclassification models. \nc set of experiments \nthat demonstrate the quality of the obtained \nsolutions for the risk prediction problem as well \nas its scalability aspects on large datasets. \nThe rest of the paper is organized as follows: In the nnext section, we describe our proposed big data solutions \nfor information extraction and integration. Then we \ndiscuss our predictive modeling techniques in conjunction \nwith the big data infrastructure. Our comprehensive \nexperimental results are presented in the experiments and \nresults section after that. An overview of current state of \nthe art approaches for risk of readmission and healthcare \ndata integration within this context are covered in the \nrelated work section followed by our conclusion and a \nbrief description of some future research directions. \nII. DATA EXTRACTION AND PREPROCESSING USING \nBIG DATA FRAMEWORK \nA. Data Extraction  \nReal world clinical data is noisy and heterogeneous in \nnature, severely skewed, and contains hundreds of relevant \nyet sometimes correlated attributes. This data resides in \nmultiple databases such as individual EMRs, lab and \nimaging systems, physician notes, medical \ncorrespondences claims, CRM systems, and hospital \nfinance department servers. The collection, integration, \nand analysis of such big, complex, and noisy data in \nhealthcare are a challenging task. For this reason, \nhealthcare information systems can be considered as a \nform of big data not only for its sheer volume, but also for \nits complexity and diversity which makes traditional data \nwarehousing solutions prohibitively cumbersome and ill-\nsuited for large scale data exploration and modeling. \n In this section, we study how a big data framework \ncan be leveraged to extract and preprocess data. The    \nfocus of the next section will be subsequent predictive \nmodeling. We will leverage Hadoop as our big data \nframework to archive performance scalability and fault \ntolerance for our task at hand. Hadoop is a popular open-\nsource map-reduce implementation, which is being used as \nan alternative to store and process extremely large data \nsets on commodity hardware. Hadoop is designed to scale \nup from single servers to hundreds of compute nodes, each noffering local computation and storage capabilities within \nHadoop. \nHowever, Hadoop provides no query functionality. In \naddition, selection methods in Hadoop are comparatively \nslower than in most DBMS. Thus a processing framework \non top of MapReduce solution is also needed to simulate a \nscalable data warehouse. To achieve this goal, we use Hive \n[6] as an open-source data warehousing solution built on \ntop of Hadoop. Hive supports queries expressed in a SQL-\nlike declarative language - HiveQL, which are compiled \ninto map-reduce jobs that are executed using Hadoop. In \naddition, HiveQL enables users to plug in custom map-\nreduce scripts into queries. Hive has 2 main user interfaces \nof CLI \(command line a SQL like construct. The process is as follows: first \nthe healthcare data such as raw patient event logs, or 


nstructured electronic medical records can be stored as flat \nfiles on various nodes. These will then become accessible \n\(i.e loaded Hadoop File System create \nappropriate tables and develop the schema so that data can \nbe structured and appropriately queried The meta-data of \n65\nHive includes the name, column, partition, and properties \nof the tables, and it is usually stored in a small relational \ndatabase table using MySQL. To extract relevant \ninformation from the Hive schema, we can run queries \nusing an interface similar to standard SQL which actually \ngets converted to programmatic constructs and executes as \nmultiple MapReduce jobs. \nIn addition to Hive, Cassandra is another popular open \nsource big data tool for distributed data management that \nwe leverage in order to make the data extraction process \nfaster. It can handle very large amounts of data spread out \nacross many commodity servers with no single point of \nfailure \(due to replication structured key-value store with \ntunable consistency. Due to its ring architecture, it is \nmassively scalable Data is replicated to multiple nodes to \nprotect from loss during node failure. Cassandra also \noffers flexible schema-less data modeling by offering the \norganization of a traditional RDBMS table layout \ncombined with the flexibility and power of no stringent \nstructure requirements. \nB. Data Integration \nMany measures of healthcare delivery or quality are not \npublicly available at the individual patient or hospital \nlevel largely due to privacy restrictions, legal issues or \nreporting norms. Instead, such measures are provided at naggregate level with varying granularity such as state-\nlevel, county-level or city-level. For example average \nincome is typically available by zip-code, whereas death \nratio is available by city, or average smoking rate by \ncountry, through a variety of publicly available datasets. \nAlthough these aggregated statistics cannot reconstruct \nthe underlying individual-level data, these aggregated \ndata can be combined with individual data to produce \nmore informative models. To integrate such data from \ndifferent sources, in this paper we propose a simple but \neffective clustering based technique. For example suppose \nwe have two datasets A and B. The dataset A contains \nincome data based on the zip-code, and we want to add \nthis factor to dataset B. To achieve this, the dataset A \n\(including income data based on the zip-code set of clusters using clustering method based on \nsome common features between dataset A and B. Then, \nthe average income is calculated for each cluster. In the \nnext step, each record of B dataset is assigned to a cluster \nthat is most similar to it \(based on distance function on \ncommon set of features values of the \nrecords in B are patched up with the plausible values \ngenerated from its respective cluster Average value appropriate for various kinds of \nclinical risk assessments in health care domain. Clinical \nrisk calculators and risk assessment tools provide \ninformation about a person's chance of having a disease or \nencountering a clinical event [14]. Such tools are useful to \neducate patients as well as healthcare providers to monitor nthe development of health conditions. Risk calculators are \ncommonly used for diseases like cancer, diabetes heart \ndisease, and stroke etc. Developing predictive modeling \nsolutions for such disease related risk of readmissions is \nextremely challenging in healthcare informatics due to \nhigh dimensionality and large volume of the data that is \nincreasingly becoming available within hospital systems. \nIn this paper, the focus is on demonstrating how clinical \nrisk calculator tools can be augmented and scaled using a \nbig data infrastructure implementation.  \n As mentioned earlier, Risk of Readmission \(RoR challenging [15]. \nCongestive Heart Failure \(CHF studies show that many of \nthese admissions are readmissions within a short window \nof time. Identifying CHF patients who are at a greater risk \nof hospitalization can guide implementation of appropriate \nplans to prevent these readmissions. During the initial \nhospitalization \(either during admission or discharge npatient, if her risk of readmission \(RoR such as, within 30 days or 60 days could be \ncalculated, it may in turn lead to developing improved \npost-discharge planning for the patient Furthermore, such \ninsights may guide health care providers to develop \nprograms to improve the quality of care and administer \ntargeted interventions - thus reducing the readmission rate \nand the cost incurred in these readmissions. This can also \nfacilitate proper resource utilization within the hospitals.  \nFormally the problem is formulated as a supervised \nlearning problem, especially as a binary classification task. \nThe class of a patient is Readmission if the elapsed period \nbetween the last discharge and next admission is smaller or \nequal to 30, and No Readmission else. Developing \npredictive modeling solutions for ROR prediction is \nextremely challenging. It involves integration of socio-\ndemographic factors, health conditions, disease nparameters, hospital care quality parameters, and a variety \nof variables specific to health care providers making the \ntask immensely complex. To tackle this complexity, we \nleverage the power of Mahout as a big data solution for \ndata analytic tasks to archive better scalability in term of \ntime and resources.  \nMahout is a 


machine learning based algorithm library \nintended to run as Apache MapReduce jobs on the Hadoop \ncluster in order to be scalable to reasonably large data sets. \nThe advantage of Mahout over other approaches and nmachine learning tools such as R becomes striking as the \nnumber of training examples gets extremely large This is \nthe scenario, we face in healthcare domain collecting large \namount of data 24/7 in an organization's health \ninformation and clinical systems. The reason that Mahout \nhas an advantage with larger data sets is that as input data \nincreases; the time or memory requirements for training \nmay not increase linearly in a non-scalable system. In \ngeneral, the classification algorithms in Mahout require \nresources that increase no faster than the number of \ntraining or test examples, and in most cases the computing \n66\nresources required can be parallelized. This allows us to \ntrade off the number of computers used against the time \nthe problem takes to solve. \nFig. 1 shows the framework of using Mahout for RoR \nprediction. The training data should first load into Hadoop \nfile system \(HDFS classifiable data then \nclassifiable data by selecting predictor and target variables \nand identifying each variable type \(numeric, categorical, \ntext and so on required by Mahout classifiers. In the third step, the \nclassification algorithm should be selected. The algorithms \nin Mahout all share scalability. In this paper, we first use \nrandom forest because it can work with all types of \npredictor variables. Moreover, It has high overhead for \ntraining thus costly for traditional tools such as R but \noffers complex and interesting classifications and can \nhandle nonlinear and conditional relationships in data \nbetter than other techniques. \n \n \nFigure 1. Predictive modeling using Mahout Framework \nIV. RESULTS AND DISCUSSION \nIn this Section we present our comprehensive \nexperimental results. We investigate both quality and the \nscalability aspects of our proposed solutions for predicting nrisk of readmission for CHF patients. \nA. Datasets Description \nOur experiments are mainly conducted using dataset \nfrom Multicare Health System \(MHS CHF \n\(either primary or secondary primarily \nconsider patients with a discharge diagnosis of ICD9-CM \nfor this purpose, as listed in Table I nTABLE I.  THE ICD-9 CM CODES FOR CHF \nICD-9 CM codes Description \n402.01 Malignant hypertensive heart disease with heart failure \n402.11 Benign hypertensive heart disease with heart failure \n402.91 Unspecified hypertensive heart disease with heart failure \n404.01 \nMalignant hypertensive heart and kidney disease \nwith heart failure and with chronic kidney disease \nstage I through stage IV, or unspecified \n404.03 \nMalignant hypertensive heart and kidney disease \nwith heart failure and chronic kidney disease \nstage V or end stage renal disease \n404.11 \nBenign hypertensive heart and kidney disease \nwith heart failure and with chronic kidney disease \nstage I through stage IV, or unspecified \n404.13 \nBenign hypertensive heart and kidney disease nwith heart failure and chronic kidney disease \nstage V or end stage renal disease \n404.91 \nUnspecified hypertensive heart and kidney \ndisease with heart failure and with chronic kidney \ndisease stage I through stage IV, or unspecified \n404.93 \nUnspecified hypertensive heart and kidney \ndisease with heart failure and chronic kidney \ndisease stage V or end stage renal disease \n428.XX Heart Failure codes \n \nOur entity of observation is each CHF hospital \nencounter and we consider only the admissions when a \npatient is discharged to home to exclude inter hospital \ntransfers. Admissions encountering in-hospital deaths are \nnot included in our analysis because we are more \ninterested in predicting readmissions.  We calculate the \ndays elapsed between the last discharge due to CHF and \nnext admission in order to identify if the readmission has noccurred within 30 days. The dataset consists of CHF \nhospitalization for patients discharged since 2009.  It nprovides information of 6739 patients diagnosed with CHF \nand number of hospital encounters generated by these \npatients during 2009-2013 is 15696 \nGiven the hospital encounter records of every patients, \na record has been labeled as \\readmission= yes" \(or class \n1 of \nan earlier index hospitalization due to CHF, or \n`readmission= no" \(or class 0 earlier, \n30 day is chosen because it is a clinically meaningful time-\nframe for hospitals and medical communities to take \naction to reduce the probability of readmission. \nHospital readmission due to CHF is a complex \nphenomenon governed by multiple factors \(i.e, \nattributes/variables nuniqueness of the domain. One of our major challenges \nbefore classification task is to determine the subset of \nattributes \(i.e., predictor variables myriad of \nattributes present in the data set. Based on inputs from \ndomain experts and literature review Predictor variables \ncan be divided into clinically relevant categories: socio-\ndemographic, vital signs laboratory tests, discharge \n67\ndisposition, medical comorbidity and other cost related \nfactors, like length of stay. Many of the vital signs and \neven laboratory tests are considered both at admission and \ndischarge time.  \nThe Nationwide Inpatient Sample \(NIS 


MHS \ndataset. NIS dataset, which developed as part of the \nHealthcare Cost and Utilization Project \(HCUP is the \nlargest publicly available all-payer inpatient care database \nin the United States. It contains data from approximately 8 \nmillion hospital stays each year; however, it excludes data \nelements that could directly or indirectly identify \nindividuals so it can not directly integrated to MHS \ndataset. \nThe NIS includes more than 100 clinical and \nnonclinical features for each hospital stay  including \nPrimary and secondary diagnoses and procedures, \nAdmission and discharge status Patient demographics \n\(e.g., gender, age, race, median income for ZIP Code e.g ownership, size, \nteaching status data, it is a prominent \nexample of Big data in healthcare domain. \nB. Data Extraction and Integration \nAs mentioned in previous section, NIS data with 8 \nmillion records and more than 100 features is consider as \na big data which selecting and extracting data from that is \ntime-consuming. To extract data from NIS, Hive nframework is leveraged. The main goal is to integrate \nincome value which is available in the NIS data to MHS ndata. However due to security reason, it is not possible to \nmap patients in NIS to patients in MHS data Moreover, \nthe medium income is available based on zip code patient, \nwhich is also masked. The only information to find MHS \npatients in NIS data is the hospital zip code. So in the first \nstep we select the patients in NIS that are hospitalized in \nMHS facilities using MHS hospitals zip code and Hive \nframework.it took only 55 seconds for MapReduce job in \nhive to complete the query and 25 seconds in Cassandra, \nhowever running the same query in traditional RDBMS \nwill take significantly longer. In the next step we used a \nset of common factors between NIS dataset and MHS that \ncan be correlated to income. We use these factors for the nclustering method mentioned in data integration of section \nII. We choose the age, gender and also elective nhospitalization as three variables that are highly correlated \nto income and can be used for clustering purpose. The \ntable II shows the correlation of these factors to income \nusing chi-square test.  \nTABLE II CORRELATION ANALYSIS OF COMMON FACTORS AND \nINCOME \nFactors \nMetrics for Correlation Analysis \nX-squared p-value \nAge 447.5904 < 2.2e-16 \nFactors \nMetrics for Correlation Analysis \nX-squared p-value \nGender 17.2379 0.001738 \nElective \nHospitalization 359.0019 < 2.2e-16 \n \nKmeans [13] is used as clustering method to segment nselected data from NIS. Then the average income is \ncalculated for each cluster. Now we are able to map each nrecord of data in MHS to closest cluster based on \nEuclidian distance function nthe records in MHS dataset are patched up with the \nplausible values generated from its respective cluster n\(Average value the \naverage income and readmission risk. Now average \nincome can be used as a predictor variable for RoR nprediction. This generic scenario can be used for other \nattributes in order to augment the original dataset MHS quality was assessed through common model \nquality measures such as Accuracy, Precision, Recall and \nArea Under the Curve \(AUC nmeasures are less or more appropriate. The precision is \nimportant if there is a high cost related to falsely predicting \npatients to belong to the class Readmission. Recall is \nrelevant if the detection of patients that belong to \nReadmission is the main goal. The accuracy is the \ntraditional evaluation measure that gives a global insight in \nthe performance of the model. The AUC measure is \ntypically interesting when the problem is imbalanced such \nthe situation we deal with in RoR prediction \( it is \nobserved that the labeled dataset is highly skewed - i.e., \nthe number of instances with No Readmission label \nsignificantly outnumbers the number of instances with \nclass label Readmission. \nTable V shows the result of RoR prediction on original \ndataset Logistic regression and random forests are applied \nto data as two common classification models. We also ncompare all results with the Yale model as baseline \nmethod. In this work, a hierarchical logistic regression nmodel was developed to predict 30-day readmission risk \nfor patients hospitalized with heart failure using some \ndemographic and comorbidity variables. As our data at \nhand is different from the one considered in the Yale \nModel, our comparison primarily relies on the basis of the \nattributes suggested by the Yale model. To circumvent the \nimbalanced problem, we leverage over sampling method. \nThis technique alters the class distribution of the training \ndata so that both the classes are well represented. \nOversampling works by resampling the rare class records \nso that the resulting training set has an equal number of \nrecords for each. The positive effect of over-sampling can \nbe observed in improving the recall metric for logistic nregression model. \n68\nD. Predictive Model Result for Scalability \nThe objective of the experiments in this section is to \nshow to what extent big data solution can lead to time \nefficiency and scalability. This paper describes a \nbenchmark study of various scenarios that are created by \napplying random forest as classification algorithm to \ncompare the performance of two-open source software \npackages: R as traditional 


statistical tool and Apache \nMahout as a big data solution for machine learning. Each \nscenario was evaluated for model quality and time \nefficiency. Table III shows the general setup for each \nsoftware platform. \nTABLE III.  GENERAL SETUP FOR SOFTWARE PLATFORM \nSoftware \nSystem Details \nAppliance Available RAM \nR  3.0.0 Linux server 16GB \nMahout 0.7 \nTwo-Node Hadoop Cluster \n2GB\(each node Cluster \n \nOur original data in MHS contains only 15696 records \nfor CHF patients admission in Multicare hospitals which \ncannot be considered as a big data, however, since our aim \nis to have a general solution that can be applicable to any \ndata size for RoR prediction, we scaled up the original data \nlinearly several times to show how big data framework \noutperforms in comparison with traditional systems when \nthe training set becomes larger. The scaled data has \ncreated an increased demand for memory and processing \ntask needed to predict RoR. Table IV shows the five \ndifferent scenarios of data size. \nTABLE IV.  SCENARIOS FOR DATASET nNumber of Tuples in Dataset  \nScenario 1 Scenario 2 Scenario 3 Scenario 4 Scenario 5 \n15,696 114,862 900,000 1,665,866 3,271,716 \n \nIn order to directly compare the modeling results, \nparameters were chosen consistently across software. For \nexample, in random forest model, the number of trees \nequals to 120 and each tree has 15 variables at random. \nThe data were split into a 70/30 training and test partition, \nfor all the scenarios. In these set of experiments, we ignore \nthe logistic regression result because the execution mode \nfor logistic regression is sequential so it cannot be used to \nshow the parallelization benefit of big data solution. Table nVI shows the result of random forest model in both R and \nMahout Framework for all the scenarios. Random forest \nalgorithm trains an enormous number of simple classifiers \nand uses a voting scheme to get a single result. The \nMahout parallel implementation trains many classifiers in \nthe model in parallel.  We choose Random forest since this \napproach has somewhat unusual scaling properties. \nBecause each small classifier is trained on some of the \nfeatures of all of the training examples, the memory \nrequired on each node in the cluster will scale roughly in \nproportion to the square root. Thus the size of vectors is \nlarge and larger vectors consume more memory and slow \ndown the training. That’s the main reason that R was not \nable to run random forest in scenario 5 because the data is \ntoo large to fit into main memory. These set of experiment nshow the main advantage of Mahout, which is its robust \nhandling of extremely large and growing data sets nThe Fig. 2 shows the training time of random forest \nmodel in R and Mahout Platforms.it can be observed that nwhen the number of training examples is relatively small, \ntraditional data mining tools work even better than Mahout \n\(scenario 1 is significantly \nbetter with regard to time in comparison with traditional \nstatistical tools such as R. The increased time required by \nnon-scalable algorithms is often due to the fact that they \nrequire unbounded amounts of memory as the numbers of \ntraining examples grow. \n \n \nFigure 2. Train time for random forest model nFig. 3 compares the performance of Mahout \nFramework for different number of nodes for Hadoop \ncluster. The parallelization power of mahout can be easily \nobserved from this diagram. When the number of training nexamples is relatively small, Two-machine Hadoop cluster \nworks even better than three and four-nodes but for larger \ntraining data, the training time significantly decreases \nwhen we increase the number of nodes in Hadoop cluster. \n \n \nFigure 3. Training time for Mahout with different \nnumber of nodes \n69\nV. BACKGROUND AND SIGNIFICANCE \nPreventing hospitalization is a prominent factor to reduce \npatient morbidity, improve patient outcomes, and curb health \ncare costs. An increasing body of literature attempts to \ndevelop predictive models for hospital readmission risks \n[1,8,12,14,15,16 ,17,18,19,20,21,22,23,24]. These studies \nrange from all-cause readmissions to readmission for specific \ndiseases such as heart failure, pneumonia, stroke, and nasthma. Each of these models exploits various predictor \nvariables assessed at various times related to index nhospitalization \(admission, discharge, first follow-up visit, \netc.  In another research study [9], a real-time predictive \nmodel was developed to identify CHF patients at high risk \nfor readmission within the 30-day timeframe. In this model, \nsome clinical and social factors available within hours of \nhospital presentation are used in order to have a real-time \npredictive model. Although the model demonstrated good ndiscrimination for 30-day readmission \(AUC 0.72 1372 HF patients the \nrecent studies for predicting 30-day readmission risk for \nheart failure hospitalization is done in [15 In this work, \nadministrative claim data is used to build a regression model \non 24,163 patients from 307 hospitals. In a recent research \nstudy, we have proposed a risk calculator tool [14] that is \ncapable of calculating 30-day readmission risk for \nCongestive Heart Failure based on incomplete patient data.  \nIn a separate research effort [25], we also demonstrate the \neffectiveness of data preprocessing in 30-day readmission \nrisk prediction problem. Novel predictive modeling \ntechniques for 30-day risk-of-readmission prediction \nproblem are investigated by the authors in [25].   \nA recent study investigates the impact of big data on \nhealthcare solutions [26]. This study suggests that leveraging \nthe collection of patient and 


practitioner data could be an \nimportant way to improve quality and efficiency of health \ncare delivery. In fact, while the complexity of the domain, \ndue to very high velocity volume and variety of medical data \nis acknowledged [26], however, the necessity of enabling big \ndata solutions to these problems is mostly overlooked in the \nprevious works. To the best of our knowledge, we are the \nfirst one to propose big data solutions for information \nextraction, information integration, and predictive modeling \nfor 30-day readmission risk prediction problems.  \nFortunately there exist tools and programming paradigms \nfor such data intensive applications leveraging distributed \ncomputation model. Apache Hadoop is one such distributed nframework that implements a computational paradigm \nMapReduce, where the application is divided into many nsmall fragments of work, each of which may be executed or \nre-executed on any node in the cluster data intensive \ndistributed applications, such as Apache Hadoop.  In \nconjunction with a distributed system such as Hadoop, \nApache Mahout provides a useful set of machine learning \ntools that allow data to be classified clustered, and filtered, \nand Mahout was designed to work with Hadoop so it is \neasily scaled up to large dataset and networks. MADlib and \nBismark[27,28] are two other useful analytics tools that are \ndesigned to analyze structured and unstructured data in \nparallel. These are excellent tools to enable scalable nsophisticated in-database analytics and have been well \nadopted by the database-engine developers, data scientists, \nIT architects and academics. \nVI. CONCLUSION \nIn this work, we study the big data solution for predicting \nthe 30-day risk of readmission for the CHF patients.  Our \nproposed solution leverages big data infrastructure for both \ninformation extraction and predictive modeling. We study \nthe effectiveness of our proposed solution with a \ncomprehensive set of experiment, considering quality and \nscalability. As ongoing work, we aim at leveraging big data \ninfrastructure for our designed risk calculation tool, for \ndesigning more sophisticated predictive modeling and \nfeature extraction techniques, and extending our proposed nsolutions to predict other clinical risks. \nVII. ACKNOWLEDGMENT \nThis work is supported by MHS \(grant no A73191 time and insightful \ndiscussions during the initial stage of the study. \n \nTABLE V.  RESULT OF ROR PREDICTION ON ORIGINAL DATASET \nModels \nSupervised \nLearning \nAlgorithms \nClass \nImbalance \nSolution \nResults for Prediction \nAccuracy Precision Recall F-measure AUC \nMHS Model \nLogistic \nRegression \n- 77.88% 32% 0.69% 1.36 63.78% \nOS 58.39% 28.90% 61.41% 39.30% 63.24% \nRandom Forest - 77.90% 40.47% 1.48% 3.01% 61.04% OS 77.96 44.44% 1.7% 3.3% 62.25% \nYale Model \n\(Baseline VI.  RANDOM FOREST PREDICTION RESULT ON R AND MAHOUT \nDataset Platform Results of Prediction Accuracy Precision Recall F-measure Runtime \nScenario 1 R 77.90% 40.47% 1.48% 3.01% 18.96 sec \nMahout 78.84% 93.61 3.83% 7.35% 32.75 sec \n70\nScenario 2 R 82.35% 99.59% 17.78% 30.17% 4.01 min \nMahout 78.55% 94.51% 1.8% 3.7 36.65 sec \nScenario 3 R 86.34% 99.87% 37.49% 54.52% 1h 17m \nMahout 80.91% 91.62% 14.49% 25.02% 2m 20 sec nScenario 4 R 87.12% 99.88% 40.60% 57.73% 4h 5m \nMahout 80.98% 88.83% 15.94% 27.02% 4m 35sec \nScenario 5 R Cannot allocate vector of size 3.9 Gb after 10 hour running \nMahout 80.79% 91.48% 13.99% 24.27% 7m21sec \n nREFERENCES \n[1] Donzé J. Aujesky D., Williams D., Schnipper J.L, MD. Potentially \navoidable 30-day hospital readmissions in medical patients: \nDerivation and validation of a prediction model. JAMA Internal \nMedicine 173\(8  frontier for innovation competition and productivity, \nMcKinsey Global Institute, 2012. \n[3] The Apache Software Foundation., \nhttp://hadoop.apache.org/common/credits.html. \n[4] Ghemawat  D.J .MapReduce: simplified data processing on large \nclusters. In: Proc of OSDI, 2004. \n[5] Owen S. and Anil R. Mahout in Action. Manning Publications Co., \nGreenwich, Connecticut, 2010. \n[6] Thusoo, A., Sarma, J. S., Jain, N., Shao, Z., Chakka P., Anthony, S., \nLiu, H., Wyckoff, P., And Murthy, R .Hive—a warehousing solution \nover a Map-Reduce framework. In VLDB, 2009. \n[7] Wikipedia, http://en.wikipedia.org/wiki/Apache_Cassandra. \n[8] Adams K. F Fonarow G. C., Emerman C. L., LeJemtel T. H., \nCostanzo M. R., Abraham W. T., Berkowitz R. L., Galvao M., and nHorton D. P. Characteristics and outcomes of patients hospitalized for \nheart failure in the United States Rationale, design, and preliminary \nobservations from the first100, 000 cases in the acute decompensated nheart failure national registry \(ADHERE 2  Chen J, Lin Z, Bueno H, Curtis JP, Keenan PS, Normand \nSL, Schreiner G, Spertus JA, Vidán MT, Wang Y, Wang Y nKrumholz HM. Recent national trends in readmission rates after heart \nfailure hospitalization. Circ Heart Fail, 3:97-103, 2010. \n[10] Krumholz H. M., Normand S. L. T., Keenan P. S., Lin Z. Q., Drye E. \nE., Bhat K R., Wang Y. F., Ross J. S., Schuur J. D., and Stauer B. D.. \nHospital 30-day heart failure readmission measure methodology. \nReport prepared for the Centers for Medicare & Medicaid Services. \n[11] Amarasingham R, Moore BJ, Tabak YP, Drazner MH, Clark CA, \nZhang S, Reed WG, Swanson TS, Ma Y, Halm EA. An automated \nmodel to identify heart failure patients at risk for 30-day readmission \nor death using electronic medical record data 


Journal of Medical \nCare, 10:981-988, Feb. 2010. \n[12] MULTICARE HEALTH SYSTEM, http://www.multicare.org n[13] Han J. and Kamber M.. Data mining: concepts and techniques. \nMorgan Kaufmann, 2006. \n[14] Zolfaghar K Agarwal J., Sistla D., Chin S., Roy S. B., Verbiest N., \nTeredesai A., Hazel D., Amoroso P., and Reed L Risk-o-meter: An \nintelligent clinical risk calculator. In Proceedings of the 19th ACM \nSIGKDD Conference on Knowledge Discovery and Data Mining \n\(KDD  Yancy CW, \nPeterson ED, Hernandez AF. Incremental value of clinical data \nbeyond claims data in predicting 30-day outcomes after heart failure \nhospitalization. Cardiovasc Qual Outcomes, 4\(4  Coleman EA, Parry C, Chalmers S, Min SJ. The care transitions \nintervention: Results of a randomized controlled trial. Archives \nofInternal Medicine, 166\(17  Mari D., Tettamanti M., Djade C. D., Pasina \nL., Salerno F., Corrao S. , Marengoni A., Iorio A., Marcucci M and \nMannucci P. M. Risk factors for hospital readmission of elderly \npatients.European Journal of Internal Medicine, 24\(1  npost discharge telephonic follows up on hospital readmissions. Popul \nHealth Manag, 14:27-32, 2011. \n[19 Hunter T., Nelson J., and Birmingham J.. Preventing readmissions \nthrough comprehensive discharge planning Prof Case Manag., 18:56-\n63, 2013. \n[20] Kaur H. and Wasan S. K.. Empirical study on applications of data nmining techniques in healthcare. Journal of Computer Science, \n2\(2  Johnson M. L., Cody R. J., and Aaronson. K. D.  \nDischarge education improves clinical outcomes in patients with \nchronic heart failure. Circulation, 111\(2  applications in healthcare. \nJournal of Healthcare Information Management Vol, 19\(2  H. M., Amatruda J., Smith G. L., Mattera J. A., Roumanis \nS. A., Radford M. J., Crombie, P. and Vaccarino V Randomized trial \nof an education and support intervention to prevent readmission of \npatients with heart failure. Journal of the American College of \nCardiology, 39\(1  P., Illig S., Linn R., Fiedler R., and Granger \nC.. Comparison of logistic regression and neural networks to predict \nrehospitalization in patients with stroke. Journal of clinical \nepidemiology, 54\(11 n[25] Meadam N., Verbiest N., Zolfaghar K., Agarwal J., Chin S., Basu \nRoy S., Teredesai A., Hazel D., Reed L., Amoroso P. Exploring \nPreprocessing Techniques for Prediction of Risk of Readmission for \nCongestive Heart Failure Patients. In Data Mining and Healthcare \nWorkshop, in conjunction with the 19th ACM SIGKDD Conference non Knowledge Discovery and Data Mining \(KDD  of Big Data to \nHealth Care, JAMA.   2013; 1351- 1352. doi:10.1001/jama.2013.393. \n[27] Hellerstein, J Schoppmann F., Wang D. Z., Fratkin E., Gorajek A., \nWelton C.,  Feng X., and Kumar A. The madlib analytics library or \nmad skills. PVLDB 2012. \n[28] Feng, X., A. Kumar, B. Recht, and C. R´e, 2012: Towards a unified narchitecture for in-rdbms  analytics. In SIGMOD Conference, pp. \n325–336 \n \n71\n 


significantly \ndriven by launch vehicle fairing packaging and resulted in \nhaving no exposed cells when stowed. This iterative design \nexample is representative of the close interaction between \nscience instrument, and spacecraft design teams to arrive at \nthe least complex design to satisfy mission requirements. \nSMAP uses a single-string architecture with selective \nredundancy. Graceful degradation features have also been \ndesigned into the observatory where practical. The \ntransponders, transmitters, and inertial reference units \(IRUs redundancy and all the thrusters are placed on a single \nbranch. The reactions wheels are oriented and sized so that a \nfailure in an individual wheel can be tolerated. Each magnetic \ntorque assembly is internally redundant \(via redundant \nwindings in the case of a magnetometer failure. \nSurvival heaters and many instrument slip rings are \nredundantly wired. All actuators include redundant windings. \nParticular attention has been paid to fault protection design to \nreduce the likelihood and mission impact of specific faults \nand also to minimize the number of fault events that cause the \ninstrument to despin. The observatory is designed to robustly \nand autonomously recover attitude following the momentum \nchange associated with a despin, but the return to science noperations is a longer process resulting in undesirable science \ndata loss. For this reason, the instrument remains spinning for \nall but the most severe faults \(Figure 16 Table 5. \nFigure 15a. Early solar array configuration was constrained by \nboth instrument and telecom antenna FOVs. \nFigure 15b. Final solar array simplified to reduce panels and \ndeployments; allowed slight penetration into instrument \nantenna FOVs. \n  12\n7. ADAPTING PLANETARY AVIONICS \nSMAP’s avionics design is derived from the Mars Science \nLaboratory \(MSL selectively apply elements \nof MSL’s design to avoid unnecessary complexity for \nSMAP’s application while also minimizing the amount of \nnew design and development required. In the end, SMAP was \nable to substantially reduce the hardware complement relative \nto the MSL design and also limit the number of new board \ndesigns required. Another challenge was to adapt MSL’s fully \nredundant design to a single string application.  \nTo minimize new avionics development, the most significant \ncapability changes were addressed by the development of the \nNVM card. Specific challenges that this card addressed were \n\(1 read/write capability to \nsupport simultaneous science acquisition \(6 Mbps for \nradiometer and 40 Mbps for radar 130 Mb/s 2 data storage \(128 GB 3 BER 10-9 ndata loss. The NVM leveraged a similar architectural design \ndeveloped for the MSL MastCAM instrument however, the \nMSL NVM only had 4 GB storage and a 6 Mbps transport \nrate. Using the NVM avoided adding a separate solid state \nrecorder within the bus, which would have proved difficult to \npackage given the volume constraints. The new NVM card is \nshown in Figure 17. \nA new 128 GB flash memory chip was selected for the NVM, \nwith a layout compatible with the older chip used in MSL's \nNVM card design. A large lot of these new parts was \nprocured early in Formulation and subjected to extensive \nenvironmental, radiation, and life qualification testing. Test \nresults showed that the actual BER performance was orders of \nmagnitude better than required for the SMAP application \n[17]. This, in turn, enabled SMAP to implement the NVM \nwith a relatively simple error detection and correction \napproach rather than a more complex approach recommended \nby the manufacturer for other applications. Beyond the \nmission-specific results for SMAP, the work conducted to nqualify these parts contributed to the recently published Flash \nQualification Guideline for Space Application [18]. Early \ntesting on an MSL EM MASTCam NVM card showed the \ndata throughput capability to be 130 Mb/s. SMAP set this as a \nconstraint on the maximum downlink science data rate to \navoid further significant design effort associated with this \ncard. Further design refinements during development yielded nimprovements in data throughput; the engineering model \nNVM demonstrated a 280 Mb/s throughput capability nproviding significant margin over SMAP’s requirement.  \n8. EMI/EMC \nGreat attention was given in the design of the observatory \nfor EMC with all the expected electromagnetic \nenvironments including the self-generated environments. \nObservatory radiated emissions could adversely affect \nradiometer science performance therefore, the most severe \nEMC requirement limits \(notches Figure 18 and launch vehicle systems.  \nDuring Formulation, SMAP established an EMC Design \nControl Plan [19] to provide design guidance to ensure \nrequirements compliance. New designs incorporated best \npractices to reduce the likelihood of generating L-band \nemissions. In electronics, clock transitions were slowed and \ngrounding paths were made as short as practical. Energy \ngenerated that could become radiated is contained within a \ncomplete Faraday cage created by sealed electronics boxes \nTable 5. Observatory mass and power breakdown \(reflects \nCDR 


estimates with growth contingency applied from Table 4 nSpacecraft \(dry protection is designed to \nreduce faults that spin down the instrument. \n \nFigure 17. New NVM card for SMAP n  13\nand shielded cabling. The spacecraft structure could not be \nan adequate Faraday cage due to the large number of \npenetrations required. \nElectronics boxes were fabricated without joints where \npossible close-outs were sealed with EMI gaskets or EMI \ntape. Assemblies that were constrained to use high-density nD-connectors that leak emissions at L-band were treated \nwith conductive overwrap to provide a complete covering. \nPedestals for connectors on the C&DH assembly were \nadded to aid overwrap application, and a cover plate was \nadded to seal the gaps between faceplates of the individual \ncards in the box \(Figure 19 added other provisions to address EMC including a \ncabling design that assures a continuous Faraday cage nbetween electronics boxes using twisted wire pairs with a \nshielded jacket \(TPSJ TPSJ \nwith Laird tape provides better shielding at L-band than \nTPSJ with copper braid overwrap. Because radiated \nemissions can be directional and connectors are often a \nsource, the ICE was configured so that the connectors \npointed away from the reflector to further attenuate \nemissions that could reflect interfere with science \ninstruments. The BAPTA was identified by modeling to be a \nlikely source of leakage; therefore, the observatory structure \nabove the BAPTA was designed as a Faraday cage and the \nrotation pathway for emissions was directed into the \nspacecraft interior where any emissions would less likely \ninterfere with science measurements. \nExisting inherited designs \(most of the commercial space \nassemblies used on SMAP accepted without \nadditional modifications for EMC. EMC requirements were \nlevied on these assemblies but with the recognition some \nmay be found non-compliant. In these cases, SMAP will use \ntraditional EMI/EMC control techniques during integration \nand test in order to control emissions \(adding EMI tape or \ncloth to cover possible areas of leakage performs the \nkey on-orbit operations needed to implement the conical \nscanning scheme employed for data acquisition by the radar \nand radiometer. The observatory uses a zero momentum bias, \ndual-spin architecture to rotate its large antenna at a spin rate \nof 13–14.5 rpm, while the spacecraft bus provides a three-axis ncontrolled platform that maintains both itself and the \ninstrument section’s spin axis in a nadir-pointed orientation. \nThe major pointing and control functional aspects and design \ncharacteristics are illustrated in Figure 20. A key challenge \nFigure 18. EMC requirement in the radiometer band for \nassemblies outside the spacecraft structure. Inside the \nstructure, the lower limit is relaxed by 6 dB. \n \nFigure 20. SMAP pointing and control system functional aspects and design characteristics. \nFigure 19. Example of pedestals for the connectors on the \nC&DH assembly as well as a cover plate to seal the gaps \nbetween faceplates of the individual cards in the box. \n  14\nwith the spinning antenna stems from its large spin axis \nmoment of inertia, which at almost 240 kg-m2 is larger than \nthat of the spacecraft bus at about 190 kg-m2. \nFigure 21 shows the sensor and actuator suite and locations. \nThis control system configuration was informed by a prior ndesign concept for the Navy Remote Ocean Sensing System \n\(NROSS  similar \nsized-rotating antenna and nadir-pointing scheme \(NROSS \ndevelopment was halted after Preliminary Design Review in \nthe 1980s management, with \nmomentum compensation for the spun side and three-axis \ncontrol accomplished with a single set of four Reaction \nWheel Assemblies \(RWAs momentum compensation. In \na recent operational example of a nadir-pointing observatory \nemploying a momentum-compensated rotating antenna, \nWindSat/Coriolis, a dedicated momentum wheel was also \nused to counteract the antenna’s angular momentum, in a \nmanner similar to that planned for NROSS [21]. SMAP’s napproach provides a degree of functional redundancy for \nwheel failure and reduces control complexity nFigure 22 shows the system architecture, encompassing the \nattitude and spin rate determination functions attitude \ncontrol modes for both RWA and Reaction Control System \n\(RCS momentum \ncompensation, and the torque rod-based scheme employed \nfor RWA momentum management. For translational \nmaneuvers \(needed for orbit altitude maintenance used due to the \nmuch larger control authority offered by the thrusters. For \nnominal mapping operations, this system can control nadir \npointing errors due to precession and nutation to within \n0.5 deg, with a stability tolerance of ±0.3 deg \(3  and control is accommodating the flexible \nmodes, especially for the large antenna and its supporting \nboom while simultaneously controlling the antenna spin \nrate and nadir orientation to within the required tolerances. \nThis has been accomplished via careful engineering of the \nprimary frequencies associated with these various elements, \nto ensure adequate separation and avoid the potential for \ninterference or undesired 


resonance effects. Frequency \ndistribution of these system components is shown in Figure \n23 to illustrate this aspect of the design. The minimum 1st \nflexible mode frequencies of the solar array, as well as the nantenna and boom, became key design requirements on the \nstructure to ensure adequate separation from the spin control \nsystem and the observatory’s attitude control bandwidth. \n \nFigure 22. Pointing and control system architecture. \n \nFigure 21. Observatory sensor and actuator description.\n  15\n10. LAUNCH VEHICLE CHALLENGES \nThe launch vehicle selection process for SMAP involved \nboth programmatic and technical challenges to develop an \nobservatory design that was compatible with several \npotential medium- and large-class launch vehicles and to \naccommodate a launch vehicle selection late in the design \nlifecycle without stretching the development schedule or \ndelaying the launch ready date. SMAP’s strategy effectively \nisolated the observatory design from launch vehicle \nuncertainty by applying very conservative bounding design \nloads and environments designing to the most constraining \nfairing volume, and by developing a flexible launch vehicle \nadapter design to address vehicle-specific interface and \nmission design differences after launch vehicle selection nWhen SMAP began Formulation in September 2008, there \nwas a dearth of suitable, affordable medium-class launch \nvehicles commercially available [23]. SMAP was too \nmassive to be lofted on a Taurus XL. Delta II production \nhad been halted. The Falcon 9 had not yet flown. Evolved \nexpendable-class launch vehicles \(EELVs were costly and \ngrossly over-capable for SMAP, and SMAP’s tall stowed \nconfiguration effectively eliminated it as a co-manifest \noption on an EELV. The situation was further complicated \nbecause NASA’s Launch Services Program \(LSP NLS-2 new contract would be in \nplace and which vehicles would be initially available in the \ntimeframe SMAP expected a launch vehicle selection. \nThe Project also simultaneously explored whether a \npartnership with the DOD Space Test Program \(STP DOD agencies have high interest in \nusing SMAP data in their applications [3] and were strong \nadvocates to the STP for such a partnership. Under such a \npartnership, the DOD might have earlier access to critically nneeded soil moisture and freeze-thaw data to support their \napplications and NASA would realize a substantially lower \nmission cost. STP has occasional access to EELVs and more \nfrequent access to Minotaur IV vehicles. The Minotaur IV \nuses components with a significant flight heritage such as \nsurplus Peacekeeper stages, a Taurus fairing and attitude \ncontrol system, and a mix of Minotaur I, Pegasus, Taurus, \nand other Orbital Sciences standard avionics and software \n[24]. The use of surplus Peacekeeper stages means the nvehicle was defined by NASA to be a non-commercial \nlaunch system under the Commercial Space Act and ntherefore, it is not available for NASA acquisition. The Act \nallows for Minotaur use by DOD under certain ncircumstances that appeared potentially allowable for SMAP \nunder a DOD partnership. The Minotaur IV had not yet \nflown at that time, but it appeared to be a good potential \nmatch for SMAP. The potential cost benefit to NASA of a \nDOD partnership was substantial, especially given the \nuncertainties in the commercial acquisition process, and for \na time, the DOD partnership track seemed like the most \nlikely prospect for a launch vehicle for SMAP. For these \nreasons, SMAP strove to develop an observatory that \nmaintained Minotaur IV compatibility \(the “+” variant \nreplaces the Orion 38 upper stage with a Star 48 which \nprovides more lift capability preliminary and then \ndetailed final design until CDR in July 2012, when NASA \nannounced selection of a Delta II launch vehicle for SMAP. \nPersistent launch vehicle uncertainty drove SMAP towards \nmission and observatory designs that maintained \ncompatibility with several launch vehicles: Minotaur IV+, \nAtlas V, Falcon 9 and then later the Delta II when it was on-\nramped onto NLS-2 in October 2011. In most aspects, the \nMinotaur IV represented the most constraining vehicle \nchoice. The largest drivers were its 92-in fairing, and its lift ncapability constraint and non-restartable upper stage, which \nlimited its final orbit insertion capability for SMAP. \nTypically in early formulation before launch vehicle \nselection, missions employ enveloping and highly margined \ndesign environments for compatibility with likely launch \nvehicles. Launch vehicle selection is typically completed by \nmission PDR so that as the observatory enters the final \ndesign phase, expected launch load requirements decrease \naround the selected vehicle and maturing coupled loads and \nmission analyses. The easing of requirements allows fewer \ndesign iterations \(margin is ‘cashed in’ to address problems \n \nFigure 23. Frequency separation is the key to meet stability and performance requirements while not responding to \ndisturbances.  \n  16\nthat arise in the detailed design and highly margined loads environment \npersisted through the entire design lifecycle, which in turn \ndrove more design iterations to insure the design was \ncompatible with the higher loads cases. \nThe 92-in fairing constraint significantly drove the \nobservatory packaging to achieve a compact design. The \nspacecraft bus underwent several design iterations to package \navionics, power, and radar electronics designs as the 


nassociated electronics packaging designs matured, while also \naccommodating commercial space assemblies for other \nsubsystems [transponders, transmitters, miniature inertial \nmeasurement unit \(MIMUs etc The packaging \nrequirements contributed to decisions to develop the NVM \nslice within the C&DH to avoid having to accommodate a \nseparate solid state data recorder assembly. Harness design \nwas a challenge to accommodate difficult bends and close \nclearances. The structure design was also optimized to reduce \nmass to insure healthy margins against the Minotaur IV+ lift \ncapability. The spacecraft structure was designed with 7050 naluminum alloy and the structure includes a number of \nmachined cutouts and thin walls that are time-consuming to \nfabricate. Thermal control was also a challenge in such a \nsmall bus due to the power dissipations \(~1400 W complexity; but nearly every available exterior \nsurface is used as a radiator. \nPerhaps most significantly impacted by the 92-in fairing \nconstraint were the stowed RBA and solar array packaging. \nThe RBA has been the most sensitive assembly to launch \nvehicle uncertainty, undergoing several design iterations to \naddress stowed packaging and launch loads, and thermal \ndesign iterations to address evolving launch phase mission ndesign scenarios. The RBA along with the spin assembly is \nso intimately coupled to the observatory’s fundamental \narchitecture that these contracts were initiated early in Phase \nA; however the RBA is now the last flight assembly to be \ndelivered to observatory integration and test, less than one \nyear before launch because the start of flight manufacturing \nwas delayed until the design work could be stabilized and nconfirmed for the selected launch vehicle. \nThe solar array design was also driven by the 92-in fairing nconstraint. Unusually, SMAP’s solar array has no exposed \ncells when stowed. Because of the small bus size the \nadjacent sizes of the spacecraft to the solar array central \npanel are ‘busy’ and close clearances to the fairing envelope \nwould not allow for a ‘wrap around’ solar panel stowage \napproach. The solar array folds over on itself, placing the \noutboard cells facing inward. This feature places high \nemphasis on ensuring there are robust battery energy \nmargins available to accommodate solar array deployment \ncontingencies following launch vehicle separation. \nThe mission design for orbit insertion was also developed \ninitially around the Minotaur IV+ vehicle capability. The \nMinotaur IV+ upper stage is not restartable and to naccommodate disposal requirements, the initial target \nspecification placed the upper stage/observatory in an nelliptical orbit \(566 x 664 km timeframe, but \ndrove the spacecraft propulsion system to accommodate the \nadditional delta V \(and propellant capacity 685 km also enabled the maximum achievable \ndry mass on-orbit for the observatory, subject to the need for \n80-kg propellant tank capacity on board the spacecraft. \nTo accommodate the Delta II selection, the existing Launch nVehicle Adapter \(LVA interfaces \n\(Figure 24 upper stage is restartable, \nthe mission design was adapted to directly inject SMAP into \nits final science orbit. Additional secondary batteries were \nadded to the LVA to provide appropriate power margins \nduring the longer coast phase until separation and solar \narray deployment places the spacecraft on internal power n\(Minotaur IV+ injection time was 16 minutes, Delta II is 60 \nminutes increased mass and fairing \nvolume resources than the Minotaur IV+; unfortunately, \nwhen the Delta II was selected, the SMAP design was \n \nFigure 24. SMAP’s launch vehicle adapter design \naccommodates changes needed for late launch vehicle \nselection. \n  17\nmature and many subsystems were well into fabrication. It \nwas not cost effective to iterate through another design cycle \nto take advantage of the additional resources. The added \ndesign and development effort to remain compliant with the \nMinotaur IV+ have ultimately allowed SMAP to \naccommodate a late launch vehicle selection without \nstretching out the development schedule or launch ready \ndate, thereby allowing SMAP to retain its October 2014 \nlaunch ready date despite the launch vehicle uncertainty. \n11. LESSONS LEARNED \nSMAP has significantly benefited from the experience and \nlessons from Aquarius and SMOS for the science \nmeasurements and instrument design, and from MSL for the \navionics and power subsystem architectures. Personnel from \nthese predecessor missions were engaged early in SMAP’s ndevelopment so that their insight and lessons learned could \nbe integrated into the early architectural design stages. \nKey lessons learned from SMAP include: \n\(1 nplanned scenarios very early with regulatory functions \nand key stakeholders. Spaceborne L-band science nradars operate as secondary users within their allocated \nspectrum. The primary users, civil and defense aircraft \nterrestrial navigation systems, are increasingly wary \nabout potential interference from secondary users. \nSMAP conducted significant analysis and testing to \ndemonstrate very low interference risk potential 


and \ncoordinated the results with these users early. SMAP \nmade significant modifications to the radar design and \noperating approach to effectively eliminate risk of \ninterference to primary spectrum users. \n\(2 the launch vehicle early; this is particularly \nimportant for new observatory designs where the \nnumber of design iterations can be driven by assumed \nconstraints and conservative bounding environments \n\(especially if they persist into detail design packaging, repackaging to \nremain within fairing envelope, for instance iterations, and mission design \niterations to accommodate various launchers before a \nfinal launch selection was made. This was unavoidable \nfor SMAP given SMAP’s launch vehicle and other \ncircumstances.  \n\(3 Integrated observatory design—spacecraft and \ninstrument—SMAP is a highly integrated design that \nuniquely leverages spacecraft capabilities to simplify \ninstrument design and operation. This resulted in lower noverall design complexity and has also allowed for a \nmore reliable design by reducing the number of npossible major failure modes.  \n\(4 new design insights incrementally \nunfold and thereby often reveal new issues to be \nresolved within the design. Occasional team ‘resets’ \nwere imposed to reassess local requirements and design \ncomplexity within the context of the overall observatory \ndesign. These ‘resets’ often resulted in significant \nsimplification and in reduced development risk and \nuncertainty. \n\(5 napply resources to mature these design areas, and \nqualify critical essential electronics parts and nsubassemblies to reduce downstream risk. SMAP \nsuccessfully did this, aggressively applying resources to nradar, radiometer, avionics, spin/dynamics and control, \nand the RBA—the key development challenges for the nmission. As described earlier, the RBA and spin \nassemblies’ designs were fundamental to the \narchitectural definition and to enabling preliminary \ndesign to proceed, so these were selected early in \nFormulation and placed under contract. This reduced \nsubsystem architectural- and system-level redesign \ncycles that can set back progress. \n12. CONCLUSION \nThe SMAP observatory has been carefully designed to \naddress a number of unique challenges posed by the mission \nobjectives: \n? Achieving global coverage every 2–3 days with a single ninstrument and observatory \n? Achieving both high resolution and high soil moisture \naccuracy \n? Minimizing data loss or corruption from L-band \nterrestrial RFI \n? Using a deployable mesh reflector for L-band nradiometric measurements \n? Mechanical packaging of the large instrument antenna \nand spacecraft, with its associated structural design \ncompatibility with several small-to-medium class launch \nvehicles and to accommodate a relatively late vehicle \nselection without delaying launch. \n? Fault protection approach to minimize science \nmeasurement on-orbit down-time \n? Adapting planetary heritage avionics to an Earth science nmission application \n? Design for EMC to avoid L-band emissions that could \ndegrade science measurements \n Dynamics and pointing control of a large deployable \nspinning reflector \nThe design ensures that SMAP will provide high-quality \nscience data. \n  18\nREFERENCES \n[1] “Earth Science and Applications from Space: National nImperatives for the Next Decade and Beyond,” \nCommittee on Earth Science and Applications from \nSpace: A Community Assessment and Strategy for the \nFuture, National Research Council, The National \nAcademic Press 2007. ISBN-10: 0-309-14090-0, \nISBN-13: 987-0-309-14090-4. \n[2] JPL SMAP website: http://smap.jpl.nasa.gov n[3] Entekhabi, D., E. Njoku, P. O’Neill, K. Kellogg, et al., \n“The Soil Moisture Active Passive \(SMAP Mission,” \nProceedings of the IEEE, vol. 98, no. 5, May 2010. \n[4] Spencer, M., K. Wheeler, C. White, R. West J. \nPiepmeier, D. Hudson, and J. Medeiros, “The Soil \nMoisture Active Passive \(SMAP nRadar/Radiometer Instrument,” IEEE International \n2010 Geoscience and Remote Sensing Symposium \n\(IGARSS n[5] Camps, A., J. Gourrion, J. M. Tarongí, A. Gutiérrez, J. \nBarbosa, and R. Castro, “RFI Analysis in SMOS nImagery,” Proceedings of the 2010 IEEE International \nGeoscience and Remote Sensing Symposium, Honolulu, \nHI USA, July 2010, pp. 2007–2010. \n[6] Ruf, C., D. D. Chen, D. Le Vine, P. Matthaeis, and J. \nPiepmeier Aquarius Radiometer RFI Detection, \nMitigation and Impact Assessment,” Proceedings of the \n2012 IEEE International Geoscience and Remote \nSensing Symposium, Munich, Germany, July 2012. \n[7] Bradley, D., C Brambora, M. E. Wong, et al., “Radio-\nFrequency Interference \(RFI Active/Passive \(SMAP IGARSS International, July 2010. \n[8] Chan, S., M. Fischman, and M. Spencer, “RFI \nMitigation and Detection for the SMAP Radar,” \nGeoscience and Remote Sensing Symposium \n\(IGARSS  Spencer, M., S. Chan, E. Belz, J. Piepmeier, P. \nMohammed, and J. Johnson, “Radio Frequency \nInterference Mitigation for the Planned SMAP Radar \nand Radiometer,” Geoscience and Remote Sensing \nSymposium \(IGARSS 2011 IEEE International, pp. \n2440–2443. \n[10]  Jones, C., et al., “Analysis of Potential Interference \nfrom SMAP Radar Transmissions into FAA ARSR-3, \nARSR-4, and CARSR Radars in the 1215–1300 MHz \nBand and Interference Avoidance Strategies,” Jet \nPropulsion Laboratory internal document. \n[11] Huneycutt, B., S 


Hensley, G. Purcell, et al., “Test \nReport on the Effects of Pulsed Interference from L-\nband Spaceborne and Airborne Radars on GPS L2 \nReceivers,” Jet Propulsion Laboratory internal \ndocument, Nov. 1, 2010. \n[12 Sanders, F., et al., “NTIA Study regarding EMC \nbetween Proposed NASA SMAP Orbital Radar System \nand FAA ARSR Systems,” NTIA/ITS. \n[13] Mobrem, M., S. Kuehn, C. Spier, and E. Slimko, \n“Design and Performance of Astromesh Reflector \nOnboard Soil Moisture Active Passive Spacecraft,” \nIEEE 2012 Aerospace Conference, March 2012 n[14] Mohammed, P., SMAP L1B Radiometer Data Product \nAlgorithm Theoretical Basis Document, GSFC. \n[15 Spencer, M. W., C. W. Chen, H. Ghaemi, S. F. Chan, \nand J. E. Belz., “RFI Characterization and Mitigation \nfor the SMAP Radar,” TGARS, to be published. \n[16] West, R., SMAP L1 Radar Data Products Algorithm \nTheoretical Basis Document, Jet Propulsion Laboratory. \n[17] Heidecker, J., M. White, M. Cooper, D. Sheldon, F. \nIrom, and D. Nguyen, “Qualification of 128 Gb MLC \nNAND Flash for SMAP Space Mission,” Integrated \nReliability Workshop Final Report \(IRW  nQualification Guideline for Space Application” JPL \nPublication 12-1, Jet Propulsion Laboratory, Pasadena nCA, 2012. \n[19] Newson, S. L., and C. H. Huang, “SMAP \nElectromagnetic Compatibility \(EMC nPlan,” Jet Propulsion Laboratory Internal Project \nDocument, April 14, 2010. \n[20] Mak, P. H., “Pointing and Stabilization Issues of Large \nSpinning Antennas,” Proceedings of IEEE Position \nLocation and Navigation Symposium, Navigation into \nthe 21st Century, IEEE Plans, pp. 230–235, 1988. \n[21] Gaiser, P. W., K. M Germain, and E. M. Twarog, \n“WindSat—Spaceborne Remote Sensing of Ocean \nSurface Winds,” Proceedings of IEEE Oceans \nConference, vol. 1, p. 280, 2003. \n[22] Alvarez-Salazar, O. S., D. Adams, M. Milman, R. \nNayeri, S Ploen, L. Sievers, E. Slimko, and R. \nStephenson, “Precision Pointing Architecture of \nSMAP’s Large Spinning Antenna,” IEEE 2010 \nAerospace Conference, Big Sky, MT, to be published. \n[23] “Review of NASA’s Acquisition of Commercial \nLaunch Services,” NASA Office of Inspector General, \nFebruary 17, 2011. \n[24] “Minotaur IV Users Guide,” Release 1.1, Orbital \nSciences Corporation, January 2006. \n  19\nBIOGRAPHIES \nKent Kellogg received a B.S. in \nElectronic Engineering from \nCalifornia Polytechnic State \nUniversity, San Luis Obispo in n1983. He joined JPL’s Spacecraft \nAntenna Group in 1983 and is \ncurrently SMAP Project Manager. \nHis prior roles include managing \nJPL’s Telecommunication, Radar \nand Tracking Division, Spacecraft \nTelecommunications Equipment Section, SeaWinds/\nQuikSCAT Instrument and Project and supervising JPL’s \nSpacecraft Antenna Group nDr. Sam Thurman received B.S., \nS.M., and Ph.D. degrees in \nAerospace Engineering from \nPurdue University 1983 1985 1995 respectively. He jointed \nJPL in 1987 and is currently the \nSMAP Deputy Project Manager. \nHis prior roles include Deputy Manager of JPL’s \nAutonomous Systems Division, and several system \nengineering and management assignments in the Mars \nExploration Program. Before joining JPL, he worked as a \nmissile guidance analyst at the Charles Stark Draper \nLaboratory in Cambridge, Massachusetts.\t\nWendy Edelstein received her B.S. \ndegree in Electrical Engineering \nwith minor in Applied Mathematics \nfrom the University of California, \nSan Diego in 1988. She joined JPL \nin 1988 and is currently the SMAP \nInstrument Manager. Her prior \nroles include Deputy Section \nManager of the Radar Science & \nEngineering Section, Group \nSupervisor for the Advanced Radar Technology Group and \nradar technologist where her primary research interests \nincluded Transmit/Receive modules, RF component \nminiaturization and lightweight antenna technologies. \nDr. Michael Spencer received the nB.S. degree in Physics from the \nCollege of William and Mary, the \nM.S. degree in Planetary Science \nfrom The California Institute of \nTechnology, the M.S. degree in \nElectrical Engineering from the \nUniversity of Southern California, \nand a Ph.D. in Electrical and \nComputer Engineering from \nBrigham Young University. He joined JPL in 1990 and has\nserved as the SMAP Instrument System Engineer. He is \ncurrently the Deputy Manager of JPL’s Radar Science and \nEngineering Section; he is also the SMAP Instrument \nScientist. \nDr. Gun-Shing Chen received the \nB.S. degree in Mechanical \nEngineering from Cheng-Kung \nUniversity, Taiwan in 1977; an nM.S. degree in Aerospace \nEngineering from University of \nTexas at Austin in 1981; and a \nSc.D degree in Aeronautics and \nAstronautics from Massachusetts \nInstitute of Technology in 1986. He \njoined JPL’s Structures and Dynamics Research Group in \n1986 and is currently the SMAP Flight System Manager.\nHis prior roles include Group Supervisor, Section \nManager, Assistant Division Manager in the Mechanical \nSystem Division, Chief Engineer and Assistant Division \nManager in the Instruments and Science Data Systems \nDivision, Mechanical Project Element Manager \(PEM MICAS Microwave Limb Sounder \n\(MLS MER AMT received a \nB.S. Engineering from UCLA in \n1981 and Ph.D. in Chemical \nEngineering at the University of nPennsylvania in 1987. He joined \nJPL’s Spacecraft Power Section in \n1987 and is currently the SMAP \nMission Assurance Manager and is \nresponsible for safety, EEE parts, \nenvironments, reliability, software \nand 


hardware quality assurance. His prior roles include \nGroup Supervisor for the Power Electronics and Systems nGroup and Manager of the Power Systems Section. Prior to\njoining SMAP, he was the Deputy Mission Assurance nManager for the Juno project.  \n  20\nShawn Goodman received an M.S. \nin Mechanical Engineering from nRensselaer Polytechnic Institute. \nHe joined JPL’s Spacecraft \nMechanical Engineering Section in \n1991 and is currently SMAP \nProject System Engineer. His prior \nroles have included Group \nSupervisor, Deputy Section nmanager and Section manager of \nthe Spacecraft Mechanical Engineering Section. He has \nworked a number of flight projects including MSTI, \nSojourner, MGS, SIR-C, DS1, X2000, OPSP, MER, and \nMSL.\t\nBenhan Jai received his Masters \ndegree in Computer Engineering \nfrom the University of Southern \nCalifornia. He joined JPL in 1985 \nwhere he served in a number of \nengineering and management roles \ninvolving ground data system ndevelopment, mission operations \nsystem development, and mission \nmanagement. He is currently the \nSMAP Mission Systems Manager. His previous assignments \nhave included the development of the Ground Data System nand the Mission Operations System on many missions \nincluding NASA Scatterometer, SeaWinds, Mars Global nSurveyor, Stardust, Mars Odyssey, and Mars Surveyor \nOperations Projects. Prior to joining SMAP, Ben led the ndevelopment of mission operations and ground data \nsystems of the Mars Reconnaissance Orbiter Project \n\(MRO and led the entire flight teams of the MRO through \nlaunch, cruise, MOI, and aerobraking to the completion of nthe primary science phase of the mission.\t\nDr. Eni Njoku received the B.A. \ndegree in natural/electrical nsciences from the University of \nCambridge, U.K., in 1972 and the \nM.S. and Ph.D. degrees in \nelectrical engineering from MIT in \n1974 and 1976, respectively. He \njoined JPL in 1977 and he is \ncurrently the SMAP Project \nScientist, a JPL senior research \nscientist, and also supervisor of the Water and Carbon \nCycles Group. His prior roles have included managing \nJPL’s Geology and Planetology Section and serving as ndiscipline scientist for ocean and earth science data \nsystems at NASA. His primary research interests are in ndeveloping microwave remote sensing techniques for land \nsurface hydrology and climate. His current research nincludes microwave modeling, retrieval algorithm \ndevelopment, field experiments, and data analysis using nradiometers and radars. \nACKNOWLEDGEMENTS  \nThe work described in this paper was carried out at the Jet nPropulsion Laboratory, California Institute of Technology, \nunder a contract with the National Aeronautics and Space \nAdministration. Part of this work was also carried out at the \nNASA Goddard Space Flight Center. \n n \n 


Reinhartz-Berger, and A. Sturm, “OPCAT-\na bimodal CASE tool for object-process based system\ndevelopment,” in 5th International Conference on En-\nterprise Information Systems \(ICEIS 2003  Olsen, R. Haagmans, T. J. Sabaka, A. Kuvshinov,\nS. Maus, M. E. Purucker, M. Rother, V. Lesur, and\nM. Mandea The Swarm End-to-End mission simulator\nstudy : A demonstration of separating the various con-\ntributions to Earths magnetic field using synthetic data,”\nEarth, Planets, and Space, vol. 58, pp. 359–370, 2006.\n[10] R. M Atlas, “Observing System Simulation Exper-\niments: methodology, examples and limitations,” in\nProceedings of the WMO Workshop on the Impact of\nvarious observing systems on Numerical Weather Pre-\ndiction, Geneva Switzerland, 1997.\n[11] M. Adler, R. C. Moeller, C. S. Borden, W. D. Smythe,\nR. F. Shotwell, B. F. Cole, T. R Spilker, N. J.\nStrange, A. E. Petropoulos, D. Chattopadhyay, J. Ervin,\nE. Deems, P. Tsou, and J. Spencer Rapid Mission\nArchitecture Trade Study of Enceladus Mission Con-\ncepts,” in Proceedings of the 2011 IEEE Aerospace\nConference, Big Sky, Montana, 2011.\n[12] J. Hauser and D. Clausing, “The house of quality,”\nHarvard Business Review, no. May-June 1998, 1988.\n[13] T. L. Saaty, “Decision Making With the Analytic Hier-\narchy Process,” International Journal of Services Sci-\nences, vol. 1, no. 1, pp. 83–98, 2008.\n[14] A. M. Ross, D. E Hastings, J. M. Warmkessel, and\nN. P. Diller, “Multi-attribute Tradespace Exploration as\nFront End for Effective Space System Design,” Journal\nof Spacecraft and Rockets, vol. 41, no. 1, 2004.\n[15] W. L. Baumol On the social rate of discount,” The\nAmerican Economic Review, vol. 58, no. 4, pp. 788–\n802, 1968.\n[16] M. K Macauley, “The value of information : Measuring\nthe contribution of space-derived earth science data to\nresource management,” Journal of Environmental Eco-\nnomics and Management, vol. 22, pp. 274–282, 2006.\n[17 S. Jamieson, “Likert scales: how to \(ab Dec.\n2004.\n[18] R. C. Mitchell and R. T. Carson, Using Surveys to\nValue Public Goods: The Contingent Valuation Method,\nS. Aller, Ed. Washington DC: Library of Congress,\n1989.\n[19] O. C. Brown, P. Eremenko, and P. D Collopy, “Value-\nCentric Design Methodologies for Fractionated Space-\ncraft: Progress Summary from Phase 1 of the DARPA\nSystem F6 Program,” AIAA SPACE 2009 Conference &\nExposition, 2009.\n[20] a. Stoffelen, G. J Marseille, F. Bouttier, D. Vasiljevic,\nS. de Haan, and C. Cardinali, “ADM-Aeolus Doppler\nwind lidar Observing System Simulation Experiment,”\nQuarterly Journal of the Royal Meteorological Society,\nvol. 132, no. 619, pp 1927–1947, Jul. 2006.\n[21] A. Newell and H. A. Simon, Human Problem Solving.\nEnglewood Cliffs, NJ: Prentice Hall, 1972.\n[22] R. Lindsay, B. G. Buchanan, and E. A. Feigenbaum,\n“DENDRAL: A Case Study of the First Expert System\nfor Scientific Hypothesis Formation,” Artificial Intelli-\ngence, vol. 61, no. 2, pp. 209–261, Jun 1993.\n[23] B. G. Buchanan and E. H. Shortliffe, Rule-based Ex-\npert Systems: the MYCIN experiments of the Stanford\nHeuristic Programming Project. Addison-Wesley,\n1984.\n[24] P. Hart, R. Duda, and M. Einaudi PROSPECTORA\ncomputer-based consultation system for mineral explo-\nration,” Mathematical Geology, no. November 1977,\n1978.\n[25] J. McDermott, “R1: A Rule-Based Configurer of Com-\nputer Systems,” Artificial lntell., 19 39, vol. 19, no. 1,\npp. 39–88, Sep. 1982.\n[26] K. J. Healey, “Artificial Intelligence Research and Ap-\nplications at the NASA Johnson Space Center,” AI\nMagazine, vol. 7, no. 3, pp. 146–152, 1986.\n[27] C Forgy, “Rete: A fast algorithm for the many pat-\ntern/many object pattern match problem,” Artificial in-\ntelligence, vol. 19, no. 3597, pp. 17–37, 1982.\n[28] L. A. Zadeh, “Fuzzy Sets,” Information and Control,\nvol. 8, no. 3, pp. 338–353, Jan. 1965.\n[29] C. Haskins, “INCOSE Systems engineering handbook -\nA guide for system life cycle processes and activities,”\nSystems Engineering, 2006.\n[30] N. Das, D. Entekhabi and E. Njoku, “An Algorithm for\nMerging SMAP Radiometer and Radar Data for High-\nResolution Soil-Moisture Retrieval,” IEEE Transactions\non Geoscience and Remote Sensing, vol. 49, no. 99, pp.\n1–9, 2011.\n[31] A. Messac and A. Ismail-Yahaya, “Multiobjective ro-\nbust design using physical programming,” Structural\nand Multidisciplinary Optimization, vol. 23, no. 5, pp.\n357–371, Jun. 2002.\n20\n[32] B. Cameron, “Value flow mapping: Using networks\nto inform stakeholder analysis,” Acta Astronautica,\nvol. 62, no. 4-5, pp. 324–333 Feb. 2008.\n[33] R. Yager, “On ordered weighted averaging aggregation\noperators in multicriteria decision making,” Systems ,\nMan and Cybernetics, IEEE Transactions on, no. 1, pp.\n183–190, 1988.\n[34] J. Fortin, D Dubois, and H. Fargier, “Gradual Num-\nbers and Their Application to Fuzzy Interval Analysis,”\nIEEE Transactions on Fuzzy Systems, vol. 16, no. 2, pp.\n388–402, Apr. 2008.\n[35] H. Apgar, “Cost Estimating,” in Space Mission Engi-\nneering: The new SMAD. Hawthorne, CA: Microcosm,\n2011, ch. 11.\n[36] Chalmers University of Technology, “Use of P-band\nSAR for forest biomass and soil moisture retrieval,”\nEuropean Space Agency, Tech Rep., 2004.\n[37] D. Selva, “Rule-based system architecting of Earth\nobservation satellite systems,” PhD dissertation, Mas-\nsachusetts Institute of Technology, 2012.\n[38] H. H. Agahi, G. Ball, and G. Fox, “NICM Schedule &\nCost Rules of Thumb,” in AIAA Space Conference 2009,\nno. September, Pasadena, CA, 2009, pp 6512–6512.\nBIOGRAPHY[\nDaniel Selva received a PhD in Space\nSystems from MIT in 2012 and he is\ncurrently a post-doctoral associate in\nthe department of Aeronautics and As-\ntronautics at MIT. His research inter-\nests 


focus on the application of multi-\ndisciplinary optimization and artificial\nintelligence techniques to space systems\nengineering and architecture, in partic-\nular in the context of Earth observa-\ntion missions. Prior to MIT, Daniel worked for four years\nin Kourou \(French Guiana particular, he worked as a specialist in\noperations concerning the guidance, navigation and control\nsubsystem and the avionics and ground systems. Daniel has\na dual background in electrical engineering and aeronautical\nengineering, with degrees from Universitat Politecnica de\nCatalunya in Barcelona, Spain, and Supaero in Toulouse,\nFrance. He is a 2007 la Caixa fellow, and received the Nortel\nNetworks prize for academic excellence in 2002.\nEdward F. Crawley received an Sc.D. in\nAerospace Structures from MIT in 1981.\nHis early research interests centered on\nstructural dynamics, aeroelasticity, and\nthe development of actively controlled\nand intelligent structures. Recently, Dr.\nCrawley’s research has focused on the\ndomain of the architecture and design of\ncomplex systems. From 1996 to 2003\nhe served as the Department Head of\nAeronautics and Astronautics at MIT, leading the strategic\nrealignment of the department. Dr. Crawley is a Fellow of\nthe AIAA and the Royal Aeronautical Society \(UK is the\nauthor of numerous journal publications in the AIAA Journal,\nthe ASME Journal, the Journal of Composite Materials, and\nActa Astronautica. He received the NASA Public Service\nMedal. Recently, Prof Crawley was one of the ten members of\nthe presidential committee led by Norman Augustine to study\nthe future of human spaceflight in the US.\n21\n 


Mars program. It would be designed for low mass, lowpower and low temperature operation. The S-band antenna would be a smaller, simpler version of the antenna that flew on Deep Impact. Other antenna options would be available The UHF antennas have been flown on previous Mars lander/rover missions. There would be other alternatives for the S-band antenna and the UHF transceiver on the hub could use a larger power amplifier to talk to an orbiting asset as a backup to the S-band radio  Risk  The highest risk items for telecom would be the single string design for each element and six year design lifetime. However, the S-band radio has flight heritage. The UHF radios would be a new design but do not require new technology. They would be an engineering development 8. SYSTEM SUMMARY Mass Equipment List Table 5 shows a summary of the mass and power for each of the subsystems for the remote instrument units. The mass of one remote unit without the specified instrument is 26.6 kg with contingency specified at the subsystem level based on heritage. Table 6 shows a summary of mass and power by subsystem for the hub. The mass of the hub with contingency is 44.9 kg. Table 7 shows a mass summary for the entire package with appropriate contingencies added per the JPL  s Flight Project Practices and Design Principles Design Principles. The package totals 218.2 kg which includes four remote units, five instruments, one hub, and the carrier container\(s Table 5. Mass and power summary for remote units Remote Unit Mass CBE Contingency Total Power Power 14.3 kg 30% 18.5 kg 0.180 W 2 W Night /Day Thermal 2.0 kg 29% 2.5 kg 0 W Telecom UHF 0.2 24% 0.3 kg 2 W 40 W CDS 0.7 kg 30% 0.9 kg 1 W \(1/60th per hour 3 W Structure 3.4 kg 30% 4.4 kg 0 W Total x 1 unit 20.6 kg 29% 26.6 kg Diplexer S-Band Downconverter STDN command data to S/C CDS Pr oc es so r S-Band Exciter 9 dBi S-Band LGA UHF Downconverter Small UHF transceiver command data to S/C CDS 


to S/C CDS Pr oc es so r UHF PA UHF Monopole Command data to C&amp;DH Command dat  to C&amp;DH  Figure 7  Telecom block diagram for the S-band \(top bottom  units would be located on the hub while the remote units only contain a UHF system 15 9. OPERATIONAL SCENARIOS Daytime Operations During the day, the remote units and hub would be fully operational. The remote units would collect data from their instruments as specified by the science team and store it in the controller memory. Table 8 shows the data volume expected from each instrument. After 24 hours have passed the UHF telecom system on the hub is used to poll each of the remote units separately at the designated interval for the stored data. The hub then transmits the data direct-to-Earth using the S-band radio. This requires a maximum of eight hours at 50 kbps each day using the DSN 34 m antennas However, data rates as high as 120 kbps may be achieved reducing the downlink time. The hub has enough memory margin to accumulate data from all the instruments for three Earth days before it must downlink the data Nighttime Operations During nighttime operations, data collection at the remote units would be taking place. The magnetometer and seismometer collect data continuously. However, the seismometer operates at a reduced mode where the sampling rate is reduced to one-half of the daytime rate which has been deemed more than adequate by the science team. The remaining instruments collect data at various intervals that would be conducive to the science team  s current requirements. Telecom events would not be scheduled during the lunar night. The data accumulates in the controller memory over 16 Earth days \(~14 days at an equatorial location would be considered a worse case so two days have been added to be conservative data volume summary for each instrument during a 16 Earth day lunar night. When the sun comes up and the hub and remote units have sufficient power to run the telecom systems the hub polls each remote unit separately at a designated interval similar to operations during the day. The data would then be transmitted to Earth gradually over the next few days using the S-band radio Table 7. Mass summary for total package Unit Mass Contingency Mass + contingency 4 Remote Units 82.4 kg 29% 106.4 kg Hub 35.2 kg 27% 44.9 kg Instruments including cabling 17.3 kg 30% 22.5 kg Carrier Container\(s Total with heritage contingency 153.1 kg 29% 197.5 kg  System contingency  21.4 kg 14 Total Package  43% 218.9 kg Table 8. Instrument data volumes received at the hub over one Earth day in daylight operations Science Instrument Compressed Data Volume Received at Hub 


Volume Received at Hub Mb Seismometer 236 Magnetometer 58 Heat Flow Probe 2 Seismic Sounder 700 Instrument &amp; Hub Engineering Data 6 Total 1002 Hub Memory 5000 Margin 80  Table 6. Mass and power summary for hub Hub/Base Unit Mass CBE Contingency Total Power Power 14.3 kg 30% 18.5 kg 0.180 W 2 W Night /Day Thermal 13.4 kg 28% 17.2 kg 0 W Telecom UHF Telecom S-band 3.4 15% 3.9 kg 2 W 40 W CDS 0.7 kg 30% 0.9 kg 1 W \(1/60th per hour W \(day Structure 3.4 kg 30% 4.4 kg 0 W Total x 1 unit 35.2 kg 27% 44.9 kg 2.38 W avg at night  16 10. SUMMARY AND CONCLUSIONS The ALGEP modular design builds upon lessons learned from Apollo era ALSEP package and technology advances since that time. ALGEP meets the requirements of long lifetime survival while maintaining continuous operation of its instruments during the lunar night which can last up to 16 days at equatorial regions on the Moon. The package would be powered using solar arrays and batteries alone not requiring nuclear sources to supply power or maintain thermal control. This concept is feasible due to its lowpower operational mode at night The modular design and packaging scheme provides flexibility in deployment across all regions of the Moon including the farside pending the availability of an orbital communications asset. The relatively light ALGEP package could be accommodated on astronaut activity support vehicles, providing a method to distribute the packages across the Moon, ultimately gaining a Moon-wide understanding of lunar geophysical properties ACKNOWLEDGEMENTS This work was supported by the NASA Lunar Sortie Science Opportunities Program The work described in this publication was carried out at the Jet Propulsion Laboratory, California Institute of Technology under a contract with the National Aeronautics and Space Administration References herein to any specific commercial product process or service by trade name, trademark, manufacturer 


or otherwise does not constitute or imply its endorsement by the United States Government or the Jet Propulsion Laboratory, California Institute of Technology REFERENCES 1] NRC  Scientific Context for Exploration of the Moon   Washington D.C.: The Nat. Academies Press, 2007 2] Apollo 11 Prelim. Sci. Rept., NASA SP-214, 1969 3] Apollo 12 Prelim. Sci. Rept., NASA SP-235, 1970 4] Apollo 14 Prelim. Sci. Rept., NASA SP-272, 1971 5] Apollo 15 Prelim. Sci. Rept., NASA SP-289, 1972 6] Apollo 16 Prelim. Sci. Rept., NASA SP-315, 1972 7] Apollo 17 Prelim. Sci. Rept., NASA SP-330, 1973 8] ALSEP Termination Report, NASA RP-1036, 1979 9] NRC  New Frontiers in the Solar System: an Integrated Exploration Strategy  Decadal Survey D.C.: The Nat. Academies Press, 2003 10] International Lunar Network Science Definition Team Final Report, 2009 BIOGRAPHY Melissa Jones is a member of the technical staff in the Planetary and Lunar Mission Concepts Group at the Jet Propulsion Laboratory.  Current work includes development of small Lunar lander concepts and instrument packages to deploy on the Moon,  Report Manager for the Titan Saturn System Mission Outer Planets Flagship Mission study, and staffing various concept studies as a systems engineer on Team X, JPL  s mission design team.  Melissa graduated from Loras College with a B.S. in Chemistry and a Ph.D. in Space and Planetary Science from the University of Arkansas  Linda Herrell has a BA in math/computer science/languages \(University of Texas fluids and heat transfer \(City College of New York addition to analytical work in computer science and thermal and structural analysis, she has worked as both a payload \(instrument Earth orbiting \(Hubble Space Telescope, Earth Observing System \(EOS Cassini as Proposal Manager for several NASA science missions She currently serves as the Program Architect for NASA's New Millennium Program    Table 9. Instrument data volumes generated at the hub after 16 Earth day lunar night Science Instrument Compressed Data Volume Received at Hub Mb Seismometer 1980 Magnetometer 920 Heat Flow Probe 5 Seismic Sounder 0 Instrument &amp; Hub Engineering Data 72 Total 2977 Hub Memory 5000 Margin 40  17 Bruce Banerdt has been a research geophysicist at the California Institute of Technology's Jet Propulsion Laboratory since 1977, where he does research in planetary geophysics and instrument development for flight projects. He has been on science teams for numerous planetary missions 


on science teams for numerous planetary missions including Magellan, Mars Observer, Mars Global Surveyor and Rosetta. He was the US Project Scientist for the international Mars NetLander mission, for which he was also principal investigator of the Short-Period Seismometer experiment, and is currently the Project Scientist for the Mars Exploration Rovers. He led the Geophysics and Planetary Geology group at JPL from 1993-2005, and is the JPL Discipline Program Manager for Planetary Geosciences. He has held several visiting appointments at the Institut de Physique du Globe de Paris. He has a BS in physics and a PhD in geophysics from the University of Southern California  David Hansen is a member of the technical staff in the Communications Systems and Operations Group at the Jet Propulsion Laboratory. Current work includes the development of the telecom subsystem for the Juno project. David received a B.S. in Electrical Engineering from Cornell University and an M.S. in Electrical Engineering from Stanford University  Robert Miyake is a member of the technical staff in the Mission and Technology Development Group at the Jet Propulsion Laboratory. Current work includes the development of thermal control subsystems for interplanetary flagship missions to Jupiter and Saturn missions to Mars and the Earth Moon, and is the lead Thermal Chair for the Advanced Project Design Team Robert graduated with a B. S. from San Jose State University, with extensive graduate studies at UCLA University of Washington, and University of Santa Clara  Steve Kondos is a consultant to the Structures and Mechanisms group at the Jet Propulsion Laboratory. He currently is generating the mechanical concepts for small Lunar Landers and Lunar Science Instrument packages in support of various Lunar mission initiatives. He also provides conceptual design, mass and cost estimating support for various Team X studies as the lead for the Mechanical Subsystem Chair. Steve is also involved with various other studies and proposals and provides mentoring to several young mechanical and system engineers. He graduated with a B.S. in Mechanical Engineering from the University of California, Davis and has 28 years of experience in the aerospace field ranging from detail part design to system of systems architecture development. He has worked both in industry and in government in defense, intelligence commercial and civil activities that range from ocean and land based systems to airborne and space systems. Steve has received various NASA, Air Force, Department of Defense and other agency awards for his work on such projects as the NASA Solar Array Flight Experiment, Talon Gold, MILSTAR, Iridium, SBIRS, Mars Exploration Rovers ATFLIR, Glory Aerosol Polarimeter System and several Restricted Programs  Paul Timmerman is a senior member of technical staff in the Power Systems Group at the Jet Propulsion Laboratory Twenty-five years of experience in spacecraft design including 22 at JPL, over 250 studies in Team-X, and numerous proposals. Current assignments include a wide variety of planetary mission concepts, covering all targets within the solar system and all mission classes. Paul graduated from Loras College with a B.S. in Chemistry in 1983  Vincent Randolph is a senior engineer in the Advanced Computer Systems and 


the Advanced Computer Systems and Technologies Group at the Jet Propulsion Laboratory. Current work includes generating Command and Data Handling Subsystem conceptual designs for various proposals and Team X.  He also supports Articulation Control and Electronics design activities for the Advanced Mirror Development project. Vincent graduated from the University of California at Berkeley with a B.S. in Electrical Engineering 18  pre></body></html 


i models into time and covariate dependent dynamic counterparts  ii models and reliability analysis in a more realistic manner  iii level  whether or not functional components \(loyal generals diagnose correctly and take proper actions such as fault mask of failed components \(traitors asymmetric  iv survivability analysis. Evolutionary game modeling can derive sustainable or survivable strategies \(mapped from the ESS in EGT such as node failures such as security compromise level modeling in the so-called three-layer survivability analysis developed in Ma \(2008a this article  v offer an integrated architecture that unite reliability survivability, and fault tolerance, and the modeling approaches with survival analysis and evolutionary game theory implement this architecture. Finally, the dynamic hybrid fault models, when utilized to describe the survival of players in EGT, enhance the EGT's flexibility and power in modeling the survival and behaviors of the game players which should also be applicable to other problem domains where EGT is applicable  5. OPERATIONAL LEVEL MODELING AND DECISION-MAKING  5.1. Highlights of the Tactical and Strategic Levels  Let's first summarize what are obtainable at both tactical and strategic levels. The results at both tactical and strategic levels are precisely obtainable either via analytic or simulation optimization. With the term precisely, we mean that there is no need to assign subjective probabilities to UUUR events. This is possible because we try to assess the consequences of UUUR events \(tactical level ESS strategies \(strategic level time prediction of survivability. The following is a list of specific points. I use an assumed Wireless Sensor Network WSN  i of UUUR events: \(a actions which can be treated as censored events; \(b Cont' of Box 4.2 It can be shown that the replicator differential equations are equivalent to the classical population dynamics models such as Logistic differential equation and LotkaVolterra equation \(e.g., Kot 2001 Logistic equation, or the limited per capital growth rate is similar to the change rate of the fitness  xfxfi which can be represented with the hazard function or survivor functions introduced in the previous section on survival analysis.  This essentially connects the previous survival analysis modeling for lifetime and reliability with the EGT modeling. However, EGT provides additional modeling power beyond population dynamics or survival analysis approaches introduced in the previous section. The introduction of evolutionary theory makes the games played by a population evolvable. In other words, each player \(individual 


other words, each player \(individual agent and players interact with each other to evolve an optimized system Box 4.3. Additional Comments on DHF Models  The above introduced EGT models are very general given they are the system of ordinary differential equations. Furthermore, the choice of fitness function f\(x complexity to the differential equation system.  The system can easily be turned into system of nonlinear differential equations. The analytical solution to the models may be unobtainable when nonlinear differential equations are involved and simulation and/or numerical computation are often required  In the EGT modeling, Byzantine generals are the game players, and hybrid fault models are conveniently expressed as the strategies of players; the players may have different failure or communication behaviors Furthermore, players can be further divided into groups or subpopulations to formulate more complex network organizations. In the EGT modeling, reliability can be represented as the payoff \(fitness, the native term in EGT of the game. Because reliability function can be replaced by survivor function, survival analysis is seamlessly integrated into the EGT modeling. That is, let Byzantine generals play evolutionary games and their fitness reliability function  The evolutionary stable strategy \(ESS counterpart of Nash equilibrium in traditional games ESS corresponds to sustainable strategies, which are resistant to both internal mutations \(such as turning into treason generals or nodes such as security compromises represent survivable strategies and survivability in survivability analysis. Therefore, dynamic hybrid fault models, after the extension with EGT modeling, can be used to study both reliability and survivability 13 risks such as competing risks which can be described with CRA; \(c captured with the shard frailty.  We believe that these UUUR events are sufficiently general to capture the major factors/events in reliability, security and survivability whose occurrence probabilities are hard or impossible to obtain  Instead of trying to obtain the probabilities for these events which are infeasible in most occasions, we focus on analyzing the consequences of the events.  With survival analysis, it is possible to analyze the effects of these types of events on survivor functions. In addition, spatial frailty modeling can be utilized to capture the heterogeneity of risks in space, or the spatial distribution of risks \(Ma 2008a d UUUR events introduced previously. These approaches and models that deal with the effects of UUUR events form the core of tactical level modeling  To take advantage of the tactical level modeling approaches it is obviously necessary to stick to the survivor functions or hazard functions models. In other words, survival analysis can deal with UUUR events and offer every features reliability function provides, but reliability function cannot deal with UUUR events although survivor function and reliability function have the exactly same mathematical definition. This is the junction that survival analysis plays critical role in survivability analysis at tactical level. However, we 


recognize that it is infeasible to get a simple metric for survivability similar to reliability with tactical level modeling alone. Actually, up to this point, we are still vague for the measurement of survivability or a metric for survivability. We have not answered the question: what is our metric for survivability? We think that a precise or rigorous definition of survivability at tactical level is not feasible, due to the same reason we cited previously  the inability to determine the probabilities of UUUR events However, we consider it is very helpful to define a work definition for survivability at the tactical level  We therefore define the survivability at tactical level as a metric, Su\(t t function or reliability function with UUUR events considered. In the framework of three-layer survivability analysis, this metric is what we mean with the term survivability. The "metric" per se is not the focus of the three-layer survivability analysis. It is not very informative without the supports from the next two levels  strategic and operational models.  However, it is obvious that this metric sets a foundation to incorporate UUUR effects in the modeling at the next two levels  Due to the inadequacy of tactical level modeling, we proposed the next level approach  strategic level modeling for survivability. As expected, the tactical level is one foundation of strategic level modeling ii objectives: \(a affect survivability which survival analysis alone is not adequate to deal with; \(b survivability at tactical level is necessary but not sufficient for modeling survivability, we need to define what is meant with the term survivability at strategic level  With regard to \(a behaviors or modes which have very different consequences. These failure behaviors can be captured with hybrid fault models. However, the existing hybrid fault models in fault tolerance field are not adequate for applying to survivability analysis. There are two issues involved: one is the lack of real time notion in the constraints for hybrid fault models \(e.g., N&gt;3m+1 for Byzantine Generals problem synthesize the models after the real-time notions are introduced. The solution we proposed for the first issue is the dynamic hybrid fault models, which integrate survivor functions with traditional hybrid fault models. The solution we proposed for the second issue is the introduction of EGT modeling  With regard to \(b modeling our problem at strategic level, EGT modeling is essentially a powerful optimization algorithm.  One of the most important results from EGT modeling is the so-called evolutionary stable strategies \(ESS We map the ESS in EGT to survivable strategies in survivability analysis.   Therefore, at the strategic level, our work definition for survivability refers to the survivable strategies or sustainable strategies in the native term of EGT, which can be quantified with ESS  In addition to integrating dynamic hybrid fault models another advantage for introducing EGT modeling at strategic level is the flexibility for incorporating other node behaviors \(such as cooperative vs. non-cooperative those behaviors specified in standard hybrid fault models, as well as anthropocentric factors such as costs constraints  Without UUUR events, both tactical and strategic level 


Without UUUR events, both tactical and strategic level models default to regular reliability models. This implies that, in the absence of UUUR events, reliable strategies are sustainable or survivable.  This also implies that three-layer survivability analysis defaults to reliability analysis however, the three-layer approach does offer some significant advantages over traditional reliability analysis, as discussed in previous sections. Nevertheless, when UUUR events exist, reliable strategies and survivable strategies are different. This necessitates the next operational level modeling  5.2. Operational Level Modeling and Decision-Making  When UUUR events are involved, we cannot make real time predictions of survivability at tactical and strategic levels This implies that the implementations of survivable 14 strategies need additional measures that we develop in this section.  Box 5.1 explains the ideas involved with possibly the simplest example  Figure 4 is a diagram showing a simplified relationship between action threshold survivability \(TS survivability \(ES view since both TS and ES are multidimensional and dynamic in practice. Therefore, the sole purpose of the diagram is to illustrate the major concepts discussed above The blue curve is the survivability when survivable strategies specified by ESS are implemented at some point before time s.  The system is then guaranteed to hold survivability above ES. In contrary, if no ESS implemented before time s, then the system quickly falls below to the survivable level at around 40 time units  T i m e 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 1 0 0 Su rv iv ab ili ty M et ric S u t 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 E S S  i s  I m p lm e n t e d N o  E S S  is  I m p lm e n t e d ts E S T S  Figure 4. A Diagram Showing the Relationship Between TS and ES, as well as timing of s and t, with s &lt; t  6. SUMMARY  The previous sections discussed the major building blocks 


The previous sections discussed the major building blocks for the new life-system inspired PHM architecture. This section first identifies a few minor aspects that have not been discussed explicitly but are necessary for the implementation of the architecture, and then we summarize the major building blocks in a diagram  6.1. Missing Components and Links  Optimization Objectives  Lifetime, reliability, fault tolerance, and survivability, especially the latter two, are application dependent. Generally, the optimization of reliability and survivability are consistent; in that maximization of reliability also implies maximization of survivability. However, when application detail is considered, optimization of lifetime is not necessarily consistent with the optimization of reliability. Consider the case of the monitoring sensor network as an example. The network reliability is also dependent on connectivity coverage, etc, besides network lifetime. What may be further complicated is the time factor. All of the network metrics are time-dependent. A paradoxical situation between lifetime and reliability could be that nodes never 'sleep                                                   


          Box 5.1 Operational Level Modeling  Assuming that the ESS solution for a monitoring sensor network can be expressed with the following simple algebraic conditions: survivability metric at tactical level SU = 0.7, Router-Nodes in the WSN &gt; 10%, Selfish Nodes &lt; 40%. Even with this extremely simplified scenario, the ESS strategies cannot be implemented because we do not know when the actions should be taken to warrant a sustainable system.  These conditions lack a correlation with real time  The inability to implement ESS is rooted in our inability to assign definite probabilities to UUUR events, which implies that we cannot predict when something sufficiently bad will jeopardize the system survivability What we need at the operational level is a scheme to ensure ESS strategy is in place in advance  The fundamental idea we use to implement the ESS strategy is to hedge against the UUUR events. The similar idea has been used in financial engineering and also in integrated pest management in entomology. This can be implemented with the following scheme  Let us define a pair of survivability metrics: one is the expected survivability \(ES threshold survivability or simply threshold survivability \(TS ES is equivalent to the survivability metric at tactical level. ES corresponds to ESS at strategic level, but they are not equivalent since ESS is strategy and ES is survivability. TS is the survivability metric value \(at tactical level and TS can be obtained from strategic level models. For example, TS = SU\(s t condition for the implementation of ESS. In other words, the implementation of strategies that ensures TS at time s will guarantee the future ES level at time t.  To make the implementation more reliable and convenient multiple dynamic TSs can be computed at time s1, s2 sk, with si &lt; t for all i.  These TS at times s1, s2, ..., sk should be monitored by some evaluation systems  Unlike tactical and strategic levels, the operational level modeling is approximate. The term "approximate means that we cannot predict the real time survivability or we do not know the exact time an action should be taken. Instead, the action is triggered when the monitored survivability metric SU\(r survivability \(TS scheme of TS and ES, we ensure the ES by taking preventative actions \(prescribed by ESS and triggered by the TS consequences of UUUR events  Figure 4 is a diagram showing the above concepts and the decision-making process involved 15 This wakefulness \(never 'sleep short period but at the expense of network lifetime. Of course, when the network is running out of lifetime, network reliability ultimately crashes. This example reminds us that 


reliability ultimately crashes. This example reminds us that multi-objective optimization should be the norm rather than exception  Constraints and Extensions  Many application specific factors and constraints are ignored in this article. For example, we mentioned about spatial heterogeneity of environment, but never present a mathematical description The spatial heterogeneity can be modeled with the so-called spatial frailty in multivariate survival analysis \(Ma 2008a  Evolutionary Algorithm  Evolutionary game modeling when implemented in simulation, can be conveniently implemented with an algorithm similar to Genetic Algorithms \(GA ESS in the evolutionary game model with simulation is very similar to GA. Dynamic populations, in which population size varies from generation to generation \(Ma &amp; Krings 2008f of node failures. Another issue to be addressed is the synchronous vs. asynchronous updating when topology is considered in the simulation. This update scheme can have profound influences on the results of the simulation. Results from cellular automata computing should be very useful for getting insights on the update issue  6.2. Summary and Perspective  To recapture the major points of the article, let us revisit Figure 3, which summarizes the principal modules of the proposed life-system inspired PHM architecture. The main inspiration from life systems is the notion of individuals and their assemblage, the population. Population is an emergent entity at the next level and it has emergent properties which we are often more concerned with. Survival analysis, which has become a de facto standard in biomedicine, is particularly suitable for modeling population, although it is equally appropriate at individual level. Therefore, survival analysis \(including competing risks analysis and multivariate survival analysis comprehensively in the context of PHM in a series of four papers presented at IEEE AeroSpace 2008 \(Ma &amp; Krings 2008a, b, c, &amp; d proposed architecture. Survival analysis constitutes the major mathematical tools for analyzing lifetime and reliability, and also forms the tactical level of the three-layer survivability analysis  Besides lifetime and reliability, two other major modules in Figure 3 are fault tolerance and survivability. To integrate fault tolerance into the PHM system, Dynamic Hybrid Fault DHF 2008e, Ma 2008a make real-time prediction of reliability more realistic and make real-time prediction of fault tolerance level possible DHF models also unite lifetime, reliability and fault tolerance under a unified modeling framework that consists of survival analysis and evolutionary game theory modeling  DHG models also form the partial foundation, or strategic level, for the three-layer survivability analysis. At the strategic level, the Evolutionary Stable Strategies \(ESS which is mapped to survivable or sustainable strategies, can be obtained from the evolutionary game theory based DHF models. When there is not any UUUR event involved reliability and survivability are consistent, and reliable strategies are survivable. In this case, the strategic level modeling up to this point is sufficient for the whole PHM system modeling, and there is no need for the next level  operational level modeling  When there are UUUR events in a PHM system, the 


When there are UUUR events in a PHM system, the inability to determine the occurrence probabilities of UUUR events makes the operational level modeling necessary Then the principle of hedging must be utilized to deal with the "hanging" uncertainty from UUUR events. In this case reliability strategies are not necessarily survivable strategies At the operational level modeling, a duo of survivability metrics, expected survivability \(ES survivability \(TS the survivable strategies \(ESS level are promptly implemented based on the decisionmaking rules specified with the duo of survivability metrics then the PHM system should be able to endure the consequences of potentially catastrophic UUUR events. Of course, to endure such catastrophic events, the cost may be prohibitively high, but the PHM system will, at least, warn decision-makers for the potentially huge costs.  It might be cheap to just let it fail  Figure 3 also shows several other modules, such as security safety, application systems \(such as Automatic Logistics CBM+, RCM, Life cycle cost management, Real-time warning and alert systems architectures, but we do not discuss in this paper. Generally the new architecture should be fully compatible with existing ones in incorporating these additional modules. One point we stressed is that PHM system can be an ideal place to enforce security policies. Enforcing security policies can be mandatory for PHM systems that demand high security and safety such as weapon systems or nuclear plant facilities.  This is because maintenance, even without human-initiated security breaches, can break the security policies if the maintenance is not planned and performed properly  In perspective, although I did not discuss software issues in this paper, the introduced approaches and models should provide sufficient tools for modeling software reliability and survivability with some additional extension. Given the critical importance of software to modern PHM systems, we present the following discussion on the potential extension to software domain. Specifically, two points should be noted: \(1 architecture to software should be a metric which can 16 replace the time notion in software reliability; I suggest that the Kolmogorov complexity \(e.g., Li and Vitanyi 1997 be a promising candidate \(Ma 2008a change is because software does not wear and calendar time for software reliability usually does not make much sense 2 software reliability modeling.  Extending to general survivability analysis is not a problem either. In this article I implicitly assume that reliability and survivability are positively correlated, or reliability is the foundation of survivability. This positive correlation does not have to be the case. A simplified example that illustrates this point is the 'limit order' in online stock trading, in which limit order can be used in either direction: that stock price is rising or falling.  The solution to allow negative or uncorrelated relationships between reliability and survivability are very straightforward, and the solutions are already identified in previous discussions. Specifically, multiple G-functions and multi-stage G-functions by Vincent and Brown \(2005 very feasible solution, because lifetime, reliability and survivability may simply be represented with multiple Gfunctions. Another potential solution is the accommodation of the potential conflicts between reliability and survivability with multi-objective GA algorithms, which I previously suggested to be used as updating algorithms in the optimization of evolutionary games  


 The integration of dynamic hybrid fault models with evolutionary game modeling allows one to incorporate more realistic and detailed failure \(or survival individual players in an evolutionary game. This is because dynamic hybrid fault models are supported by survival analysis modeling, e.g., time and covariate dependent hazard or survivor functions for individual players. If necessary, more complex survival analysis modeling including competing risks analysis and multivariate survival analysis, can be introduced.  Therefore, any field to which evolutionary game theory is applicable may benefit from the increased flexibility in modeling individual players.  Two particularly interesting fields are system biology and ecological modeling.  In the former field, dynamic hybrid fault models may find important applications in the study of biological networks \(such as gene, molecular, and cell networks 2008g conjecture that explains the redundancy in the universal genetic code with Byzantine general algorithm. In addition they conducted a comparative analysis of bio-robustness with engineering fault tolerance, for example, the strong similarity between network survivability and ecological stability \(Ma &amp; Krings 2008g survivability analysis can be applied for the study of survivals or extinctions of biological species under global climate changes \(Ma 2008b  In this paper, I have to ignore much of the details related to the implementation issues to present the overall architecture and major approaches clearly and concisely. To deal with the potential devils in the implementation details, a well funded research and development team is necessary to take advantages of the ideas presented here. On the positive side I do see the great potential to build an enterprise PHM software product if there is sufficient resource to complete the implementation. Given the enormous complexity associated with the PHM practice in modern engineering fields, it is nearly impossible to realize or even demonstrate the benefits of the architecture without the software implementation. The critical importance of PHM to mission critical engineering fields such as aerospace engineering, in turn, dictates the great value of such kind software product  6.3. Beyond PHM  Finally, I would like to raise two questions that may be interested in by researchers and engineers beyond PHM community. The first question is: what can PHM offer to other engineering disciplines? The second question is: what kinds of engineering fields benefit most from PHM? Here, I use the term PHM with the definition proposed by IEEE which is quoted in the introduction section of the paper  As to the first question, I suggest software engineering and survivability analysis are two fields where PHM can play significant roles. With software engineering, I refer to applying PHM principles and approaches for dealing with software reliability, quality assurance, and even software process management, rather than building PHM software mentioned in the previous subsection. For survivability analysis, borrowing the procedures and practices of PHM should be particularly helpful for expanding its role beyond its originating domain \(network systems that control critical national infrastructures is a strong advocate for the expansion of survivability analysis to PHM. Therefore, the interaction between PHM and survivability analysis should be bidirectional. Indeed, I see the close relationships between PHM, software engineering, and survivability as well-justified because they all share some critical issues including reliability survivability, security, and dependability  


 The answer to the second question is much more elusive and I cannot present a full answer without comparative analysis of several engineering fields where PHM has been actively practiced. Of course, it is obvious that fields which demand mission critical reliability and dependability also demand better PHM solutions. One additional observation I would like to make is that PHM seems to play more crucial roles for engineering practices that depend on the systematic records of 'historical' data, such as reliability data in airplane engine manufacturing, rather than on the information from ad hoc events.  This may explain the critical importance of PHM in aerospace engineering particularly in commercial airplane design and manufacturing.  For example, comparing the tasks to design and build a space shuttle vs. to design and manufacture commercial jumbo jets, PHM should be more critical in the latter task  17    Figure 2. States of a monitoring sensor node and its failure modes \(after Ma &amp; Krings 2008e     Figure 3. Core Modules and their Relationships of the Life System Inspired PHM Architecture    REFERENCES  Adamides, E. D., Y. A. Stamboulis, A. G. Varelis. 2004 Model-Based Assessment of Military Aircraft Engine Maintenance Systems Model-Based Assessment of Military Aircraft Engine Maintenance Systems. Journal of the Operational Research Society, Vol. 55, No. 9:957-967  Anderson, R. 2001. Security Engineering. Wiley  Anderson, R. 2008. Security Engineering. 2nd ed. Wiley  Bird, J. W., Hess, A. 2007.   Propulsion System Prognostics R&amp;D Through the Technical Cooperation Program Aerospace Conference, 2007 IEEE, 3-10 March 2007, 8pp  Bock, J. R., Brotherton, T., W., Gass, D. 2005. Ontogenetic reasoning system for autonomic logistics. Aerospace Conference, 2005 IEEE 5-12 March 2005.Digital Object Identifier 10.1109/AERO.2005.1559677  Brotherton, T., P. Grabill, D. Wroblewski, R. Friend, B Sotomayer, and J. Berry. 2002. A Testbed for Data Fusion for Engine Diagnostics and Prognostics. Proceedings of the 2002 IEEE Aerospace Conference  Brotherton, T.; Grabill, P.; Friend, R.; Sotomayer, B.; Berry J. 2003. A testbed for data fusion for helicopter diagnostics and prognostics. Aerospace Conference, 2003. Proceedings 2003 IEEE  Brown, E. R., N. N. McCollom, E-E. Moore, A. Hess. 2007 Prognostics and Health Management A Data-Driven Approach to Supporting the F-35 Lightning II. 2007 IEEE AeroSpace Conference  Byington, C.S.; Watson, M.J.; Bharadwaj, S.P. 2008 Automated Health Management for Gas Turbine Engine Accessory System Components. Aerospace Conference 2008 IEEE, DOI:10.1109/AERO.2008.4526610 


2008 IEEE, DOI:10.1109/AERO.2008.4526610 Environment Covariates &amp; Spatial Frailty Applications: AL; Life Cycle Mgmt; Real-Time Alerts CBM+, RCM, TLCSM; Secret Sharing and Shared Control 18 Chen, Y. Q., S. Cheng. 2005. Semi-parametric regression analysis of mean residual life with censored survival data Biometrika \(2005  29  Commenges, D. 1999. Multi-state models in Epidemiology Lifetime Data Analysis. 5:315-327  Cook, J. 2004. Contrasting Approaches to the Validation of Helicopter HUMS  A Military User  s Perspective Aerospace Conference, 2004 IEEE  Cook, J. 2007. Reducing Military Helicopter Maintenance Through Prognostics. Aerospace Conference, 2007 IEEE Digital Object Identifier 10.1109/AERO.2007.352830  Cox, D. R. 1972. Regression models and life tables.  J. R Stat. Soc. Ser. B. 34:184-220  Crowder, M. J.  2001. Classical Competing Risks. Chapman amp; Hall. 200pp  David, H. A. &amp; M. L. Moeschberger. 1978. The theory of competing risks. Macmillan Publishing, 103pp  Ellison, E., L. Linger, and M. Longstaff. 1997.  Survivable Network Systems: An Emerging Discipline, Carnegie Mellon, SEI, Technical Report CMU/SEI-97-TR-013  Hanski, I. 1999. Metapopulation Ecology. Oxford University Press  Hallam, T. G. and S. A. Levin. 1986. Mathematical Ecology. Biomathematics. Volume 17. Springer. 457pp  Hess, A., Fila, L. 2002.  The Joint Strike Fighter \(JSF concept: Potential impact on aging aircraft problems Aerospace Conference Proceedings, 2002. IEEE. Digital Object Identifier: 10.1109/AERO.2002.1036144  Hess, A., Calvello, G., T. Dabney. 2004. PHM a Key Enabler for the JSF Autonomic Logistics Support Concept. Aerospace Conference Proceedings, 2004. IEEE  Hofbauer, J. and K. Sigmund. 1998. Evolutionary Games and Population Dynamics. Cambridge University Press 323pp  Hougaard, P. 2000. Analysis of Multivariate Survival Data Springer. 560pp  Huzurbazar, A. V. 2006. Flow-graph model for multi-state time-to-event data. Wiley InterScience  Ibrahim, J. G., M. H. Chen and D. Sinha. 2005. Bayesian Survival Analysis. Springer. 481pp  Kacprzynski, G. J., Roemer, M. J., Hess, A. J. 2002. Health management system design: Development, simulation and cost/benefit optimization. IEEE Aerospace Conference Proceedings, 2002. DOI:10.1109/AERO.2002.1036148  Kalbfleisch, J. D., and R. L. Prentice, 2002. The Statistical Analysis of Failure Time Data. Wiley-InterScience, 2nd ed  Kalgren, P. W., Byington, C. S.   Roemer, M. J.  2006 Defining PHM, A Lexical Evolution of Maintenance and Logistics. Systems Readiness Technology Conference 


Logistics. Systems Readiness Technology Conference IEEE. DOI: 10.1109/AUTEST.2006.283685  Keller, K.; Baldwin, A.; Ofsthun, S.; Swearingen, K.; Vian J.; Wilmering, T.; Williams, Z. 2007. Health Management Engineering Environment and Open Integration Platform Aerospace Conference, 2007 IEEE, Digital Object Identifier 10.1109/AERO.2007.352919  Keller, K.; Sheahan, J.; Roach, J.; Casey, L.; Davis, G Flynn, F.; Perkinson, J.; Prestero, M. 2008. Power Conversion Prognostic Controller Implementation for Aeronautical Motor Drives. Aerospace Conference, 2008 IEEE. DOI:10.1109/AERO.2008.4526630  Klein, J. P. and M. L. Moeschberger. 2003. Survival analysis techniques for censored and truncated data Springer  Kingsland, S. E. 1995. Modeling Nature: Episodes in the History of Population Ecology. 2nd ed., University of Chicago Press, 315pp  Kot, M. 2001. Elements of Mathematical Ecology Cambridge University Press. 453pp  Krings, A. W. and Z. S. Ma. 2006. Fault-Models in Wireless Communication: Towards Survivable Ad Hoc Networks Military Communications Conference, 23-25 October, 7 pages, 2006  Lamport, L., R. Shostak and M. Pease. 1982. The Byzantine Generals Problem. ACM Transactions on Programming Languages and Systems, 4\(3  Lawless, J. F. 2003. Statistical models and methods for lifetime data. John Wiley &amp; Sons. 2nd ed  Line, J. K., Iyer, A. 2007. Electronic Prognostics Through Advanced Modeling Techniques. Aerospace Conference 2007 IEEE. DOI:10.1109/AERO.2007.352906  Lisnianski, A., Levitin, G. 2003. Multi-State System Reliability: Assessment, Optimization and Applications World Scientific  Liu, Y., and K. S. Trivedi. 2006. Survivability Quantification: The Analytical Modeling Approach, Int. J of Performability Engineering, Vol. 2, No 1, pp. 29-44  19 Luchinsky, D.G.; Osipov, V.V.; Smelyanskiy, V.N Timucin, D.A.; Uckun, S. 2008. Model Based IVHM System for the Solid Rocket Booster. Aerospace Conference, 2008 IEEE.DOI:10.1109/AERO.2008.4526644  Lynch, N. 1997. Distributed Algorithms. Morgan Kaufmann Press  Ma, Z. S. 1997. Demography and survival analysis of Russian wheat aphid. Ph.D. dissertation, Univ. of Idaho 306pp  Ma, Z. S. 2008a. New Approaches to Reliability and Survivability with Survival Analysis, Dynamic Hybrid Fault Models, and Evolutionary  Game Theory. Ph.D. dissertation Univ. of Idaho. 177pp  Ma, Z. S. 2008b. Survivability Analysis of Biological Species under Global Climate Changes: A New Distributed and Agent-based Simulation Architecture with Survival Analysis and Evolutionary Game Theory. The Sixth 


International Conference on Ecological Informatics. Dec 25, 2008. Cancun, Mexico  Ma, Z. S. and E. J. Bechinski. 2008. A Survival-Analysis based  Simulation Model for Russian Wheat Aphid Population Dynamics. Ecological Modeling, 216\(2 332  Ma, Z. S. and A. W. Krings. 2008a.  Survival Analysis Approach to Reliability Analysis and Prognostics and Health Management \(PHM  AIAA AeroSpace Conference, March 1-8, 2008, Big Sky, MT, 20pp  Ma, Z. S. and A. W. Krings. 2008b. Competing Risks Analysis of Reliability, Survivability, and Prognostics and Health Management \(PHM  AIAA AeroSpace Conference, March 1-8, 2008.  Big Sky, MT. 20pp  Ma, Z. S. and A. W. Krings. 2008c. Multivariate Survival Analysis \(I Dependence Modeling", Proc. IEEE  AIAA AeroSpace Conference, March 1-8, 2008, Big Sky, MT. 21pp  Ma, Z. S. and A. W. Krings., R. E. Hiromoto. 2008d Multivariate Survival Analysis \(II State Models in Biomedicine and Engineering Reliability IEEE International Conference of Biomedical Engineering and Informatics, BMEI 2008.  6 Pages  Ma, Z. S. and A. W. Krings. 2008e. Dynamic Hybrid Fault Models and their Applications to Wireless Sensor Networks WSNs Modeling, Analysis and Simulation of Wireless and Mobile Systems. \(ACM MSWiM 2008 Vancouver, Canada  Ma, Z. S. &amp; A. W. Krings. 2008f. Dynamic Populations in Genetic Algorithms. SIGAPP, the 23rd Annual ACM Symposium on Applied Computing, Ceara, Brazil, March 16-20, 2008. 5 Pages  Ma, Z. S. &amp; A. W. Krings. 2008g. Bio-Robustness and Fault Tolerance: A New Perspective on Reliable, Survivable and Evolvable Network Systems, Proc. IEEE  AIAA AeroSpace Conference, March 1-8, Big Sky, MT, 2008. 20 Pages  Ma, Z. S.  and A. W. Krings. 2009. Insect Sensory Systems Inspired Computing and Communications.  Ad Hoc Networks 7\(4  MacConnell, J.H. 2008. Structural Health Management and Structural Design: An Unbridgeable Gap? 2008 IEEE Aerospace Conference, DOI:10.1109/AERO.2008.4526613  MacConnell, J.H. 2007. ISHM &amp; Design: A review of the benefits of the ideal ISHM system. Aerospace Conference 2007 IEEE. DOI:10.1109/AERO.2007.352834  Marshall A. W., I. Olkin. 1967. A Multivariate Exponential Distribution. Journal of the American Statistical Association, 62\(317 Mar., 1967  Martinussen, T. and T. H. Scheike. 2006. Dynamic Regression Models for Survival Data. Springer. 466pp  Mazzuchi, T. A., R. Soyer., and R. V. Spring. 1989. The proportional hazards model in reliability. IEEE Proceedings of Annual Reliability and Maintainability Symposium pp.252-256  Millar, R.C., Mazzuchi, T.A. &amp; Sarkani, S., 2007. A Survey of Advanced Methods for Analysis and Modeling of 


of Advanced Methods for Analysis and Modeling of Propulsion System", GT2007-27218, ASME Turbo Expo 2007, May 14-17, Montreal, Canada  Millar, Richard C., "Non-parametric Analysis of a Complex Propulsion System Data Base", Ph.D. Dissertation, George Washington University, June 2007  Millar, R. C. 2007. A Systems Engineering Approach to PHM for Military Aircraft Propulsion Systems. Aerospace Conference, 2007 IEEE. DOI:10.1109/AERO.2007.352840  Millar, R. C. 2008.  The Role of Reliability Data Bases in Deploying CBM+, RCM and PHM with TLCSM Aerospace Conference, 2008 IEEE, 1-8 March 2008. Digital Object Identifier: 10.1109/AERO.2008.4526633  Nowak, M. 2006. Evolutionary Dynamics: Exploring the Equations of Life. Harvard University Press. 363pp  Oakes, D. &amp; Dasu, T. 1990. A note on residual life Biometrika 77, 409  10  Pintilie, M. 2006. Competing Risks: A Practical Perspective.  Wiley. 224pp  20 Smith, M. J., C. S. Byington. 2006. Layered Classification for Improved Diagnostic Isolation in Drivetrain Components. 2006 IEEE AeroSpace Conference  Therneau, T. and P. Grambsch. 2000. Modeling Survival Data: Extending the Cox Model. Springer  Vincent, T. L. and J. L. Brown. 2005. Evolutionary Game Theory, Natural Selection and Darwinian Dynamics Cambridge University Press. 382pp  Wang. J., T. Yu, W. Wang. 2008. Research on Prognostic Health Management \(PHM on Flight Data. 2008 Int. Conf. on Condition Monitoring and Diagnosis, Beijing, China, April 21-24, 2008. 5pp  Zhang, S., R. Kang, X. He, and M. G. Pecht. 2008. China  s Efforts in Prognostics and Health Management. IEEE Trans. on Components and Packaging Technologies 31\(2             BIOGRAPHY  Zhanshan \(Sam scientist and earned the terminal degrees in both fields in 1997 and 2008, respectively. He has published more than 60 peer-refereed journal and conference papers, among which approximately 40 are journal papers and more than a third are in computer science.  Prior to his recent return to academia, he worked as senior network/software engineers in semiconductor and software industry. His current research interests include: reliability, dependability and fault tolerance of distributed and software systems behavioral and cognitive ecology inspired pervasive and 


behavioral and cognitive ecology inspired pervasive and resilient computing; evolutionary &amp; rendezvous search games; evolutionary computation &amp; machine learning bioinformatics &amp; ecoinformatics                 pre></body></html 


