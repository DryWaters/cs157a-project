An Improved Apriori Algorithm Weixiao LIU*, Junli CHEN, Shifu QU, Wanggen WAN  School of Communication and Information Engineering, Shanghai University, Shanghai 200072, China  Email:lweixiao2008@yahoo.cn  Keywords Data Mining; Association Rule; Interest Items Interest Measure Abstract Apriori algorithm has some abuses, such as too many scans of the database, large load of systemês I/O and vast unrelated middle itemsets. This paper proposes an improved Apriori algorithm to overcome these abuses. The improved algorithm reduces the set of candidates and accelerated the speed of the algorithm by adding the interest items. Breaking the traditional steps of the algorithm to reduce the database scans and bring down the load of systemês I/O. The algorithm improves the readability of the strong association by constructing the model of the interest measure. Experimental 
results show that the algorithm can improve the speed and efficiency of operation effectively 1 Introduction Apriori algorithm is the most classic algorithm of Association  ori algorithm needs to scan the database many times to find candidates of frequent itemsets and generates great amount of unrelated middle itemsets during the iterative process. With the evaluation criteria of support and confidence the algorithm may generate many uninteresting strong association rules. The drawbacks that proposed above increase the load of system I/O and greatly affect the efficiency of the algorithm Against the drawbacks of the Apriori algorithm, this paper presents a fast and efficient Apriori algorithm. The algorithm introduces the concept of user interest items and with the user interest items to reduce the emergence of the candidate items realization steps to reduce the number of database scans and the load of system I/O. In the 
stage of strong association rules evaluation, more useful strong association rules are mined with the establishment of the user interest model 2 Apriori algorithm 2.1 The steps of the Apriori algorithm Apriori algorithm uses iterative method layer by layer to find candidate itemsets. The main steps of the Apriori algorithm are showed as follows Step 1 Producing one frequent itemset Scaning the database D to find the frequency itemsets L 1  Step 2: Connection Using the connection method to have L k K>1\rating candidate itemsets k with itemsets L k-1 and itself Step 3: Pruning Supposed: c k C k that c k 
is a itemsets of candidate k, c k +1 is a \(k-1\set of c k if 1 1 k k L c that the itemsets of candidate c k should be deleted from the itemsets of candidate C k  Step 4: Generating strong association rules Strong association rules are mined according to the minimum confidence level and then end of the algorithm 2.2 Apriori algorithm analysis The Apriori algorithm has certain limitation on the speed and efficiency as follows 1\Apriori algorithm will produce a large number of middleitemsets. Candidate itemsets C k 
are generated by Apriori-Gen function with itemsets L k-1 The number of itemsets C k is k L k C 1 Obviously that the number of candidate itemsets Ck will be in large quantity if the k is big enough 2 Scaning the database too many times. Apriori algorithm needs to scan the database every time when it generates candidate itemsets. It will increase the load of system I/O and impact the speed of the algorithm when it scans massive database 3\ssociation rules are mined. Apriori algorithm uses evaluation criteria of support and confidence Not all of the strong association rules are useful to the users For example, transaction database is showed as follows. We use TID to mark the transaction, Items to stand for itemsets in 
the transaction, Num to stand for the times of the itemsets and L stand for the length of the itemsets Tab.1 Example of transaction database TID Items Num L  1 I 1 I 2 20 2 2 I 1 I 3 I 4 5 3 3 I 2 I 5 I 6 70 3 
4 I 6 I 7 5 3 For itemsets \(I 1 I 2 pport of \(I 1 I 2  Sup\(I 1 I 2 20/100=0.2 and the confidence of \(I 1 I 2  conf\(I 1 I 
2 20/25=0.8. When the min_sup is not bigger than 0.2 and the min_con is not bigger than 0.8, Itemsets \(I 1 I 2  is choosed as strong association rules. However, when I 1 does not appear, the confidence of I 2 is conf = 70/75 = 0.93. In other words, if we suppose I 1 and I 2 as products the effect of putting itemsets\(I 1 I 2 o put them separately. So we need to find another evaluation criteria interest measure 221 


3 Improved Apriori algorithm The improved Apriori algorithm made the following adjustments according to the three drawbacks 3.1 Constraints of the user interest itemsets Interest items are selected by users. We use Its n to stand for them. Interest itemsets are collection of interest Its n and Its  its 1 its 2 its n  The improved algorithm uses interest itemsets to exclude nonrelevant items in the transaction database D and the length of the itemsets will be changes too. For example: the number of uninterested items in itemsets whose length is n, then the length of the excluded itemsets is Ln =L-n. Shown in Table 1 as an example of transaction database, the itemsets of transaction database is I={I 1 I 2 I 3 I 4 I 5 I 6 I 7 We want to know the strong association rules of I 1 I 2 I 3 I 4 I 5 so the interest itemsets is Its={I 1 I 2 I 3 I 4 I 5 The excluded transaction database is shown in table 2 Tab.2 New example of transaction database TID Items Num L  1 I1,I2 20 2 2 I1,I3,I4 5 3 3 I2,I5 70 2 Compared the new transaction database with the original we know 1\ number of database scans that the new transaction database compared to the original is seven to five. The efficiency improves about 30 2\We assume that I 1 I 2 I 3 I 4 I 5 I 6 I 7 are frequent items. The number of candidate three sets is about 1330 which generate by the original transaction database however the number of candidate three sets is about 120 which generate by the new The efficiency improves about 91 3.2 The steps of the improved algorithm Against too many times of database scans, the improved Apriori algorithm changes the steps of the traditional algorithm. We use arrays to store the datas and reduce the number of the database scans. We introduce the steps of the improved algorithm with table 2 Step 1: Traverse the new database The existence form of transaction itemsets in table 2 is shown as follows Tab.3 The existence form of transaction itemsets in table 2 TID Items Num 1 I 1 20 1 I 2 20 2 I 1 5 2 I 3 5 2 I 4 5 3 I 2 70 3 I 5 70 The unique identifier of itemsets in transaction database is TID. We put the itemsets into array A according to the TID We use ç,éto separate the items under the same TID with different items and ç;é to separate itemsets with different TID The organizational form is 1 I 2 I 1 I 3 I 4 I 2 I 5 I 6  Step 2:Traverse the array A and generate frequent itemsets The improved algorithm generates frequent itemsets according to the min_sup by traversing the array A. We put the frequent itemsets and the corresponding support into A 1 A 2 A n n stands for the length of the itemsets. We use to separate the itemsets and the corresponding support and to separate the different itemsets. If we set the min_sup is 0.2 then we have frequent itemsets as follows Tab.4 Frequent itemsets Items I 1 I 2 I 5 I 1 I 2 I 2 I 5 Number 25 90 70 20 70 Sup 0.25 0.9 0.7 0.2 0.7 The organizational form of frequent itemsets in array is A1[]={I 1 0.25; I 2 0.9; I 5 0.7 1 I 2 0.2 I 2 I 5 0.7 Step 3: Candidate frequent itemsets The improved algorithm generates frequent itemsets k by using A k-1 and itself to do a connection. Compared the generated frequent itemsets with the itemsets in array A k then add the non-existed itemsets in array orderly Compared the implementation steps of the improved Apriori algorithm with the traditional algrorithm we know 1\The improved algorithm greatly reduces the number of database traversal and the system I / O load because it scans the database only once 2\proved algorithm puts the frequent itemsets and support into one array. With this array we can generate strong association rules rapidly 3\We can use the sequence of the frequent itemsets in corresponding array to know which frequent itemsets are generated by itemsets in database and which are generated by candidate itemsets. This improvement will help users to achieve practical applications 3.3 Model of interest The evaluation criteria which based on the measure of support and confidence will not reflect the real strong association rules in some cases. So we need to build a new model of interest The interest measure mainly includes the subjective interest measure and the objective interest measure. Now studies mainly focused on the objective interest measure. Some models of the objective interest measure are shown as follows 1\he interest model of Gray and Orlows The interest model of Gray and Orlowska is used to assess the associated degree between the itemsets. The definition is shown as follows m k y p x p y p x p xy p I       1          1       y p x p xy p is the deviation k and m stand for factor parameters 2\rule template of The rule template is an extension of syntax. It is used to bound which properties can appear on the left side and which properties can appear on the right side. The definition of the 222 


rule template is A 1 A 2 A k A m A j may be a name of property or class. The model is proposed by users and if a rule matchs this model then the rule is interest 3\he function of J-measure  The function of J-measure is proposed by Gray and Orlowska It is the average information of probability of classification rules and it is the best rule to find the properties of the discrete rules. The definition of function is         1   1 log   1       log      X P Y X P Y X p X P Y X p Y X P Y P  2 Close analysis of the interest model that proposed above we know 1\he parameters k and m in the Gray and Orlowska's interest mode does not have determined computation rule. It is easy to add subjective factors and affect the accuracy of the evaluation model 2\ettinenês rule template is a rule-based method Although the rule template can select the valid association rules accordance with the rule template, it does not give the effective template to suit every rule 3\he model of function J-measure considers the coupling degree of the probability distribution between X and Y. But the function does not consider the impact of P\(Y So the interest model that we build must meet the following requirements 1\model of Y X must meet that if the probability of X is big then the same to the model 2\The new model must consider the coupling degree between X and Y. The value of the model is proportional to the coupling degree 3\ the analysis of examples in chapter 3 we know if we do not consider the impact of post item I 2 we will elicit misleading strong association rules. So we must pay attention to the post item Y 4\he interest model of Y X should be proportional to the support of X and Y Consider the analysis above we set the interest model as follows      1    1    1   Y X Sup Y X conf X Sup Y Sup Y X Inte  3       1    1    1   XY P X Y P x P Y P Y X Inte  4 The interest model that we proposed above meets the premise that X is a big probability event and consider the coupling degree between X and Y. The model also adds the factor of probability Y. The practical application value of the strong association rules is proportional to the interest measure We use the interest model to verify the example of chapter 2.2      1    1    1   2 1 2 1 1 2 2 1 I I Sup I I conf I Sup I Sup I I Inte 5  13  0 2  0  8  0 1   25  0 1  9  0 1   2 1 I I Inte  6 From the result we know that the interest measure of I 1 I 2 is 0.13. It is too low to let us trust this association rule According to the new interest model and combining the traditional evaluation criteria of support and confident we make the new definition of strong association rules. We assume that the minimum support threshold is min_sup, the minimum confidence threshold is min_con and the minimum interest measure is min_int. We call it strong association rules if the association rules meet that sup min_   Y X Sup  con Y X Con min_   and int min_   Y X Int  4 Experiment results The algorithm uses C # as a code development platform and Microsoft SQL Server2005 as a database development platform. The number of test record is 52761 which exist in the vAssocSeqLineItems of AdventureWorksDW sample database. OrderNumber and Model will be used as identifiers and items of the transaction. The operation platform of the algorithm is shown in table 5 Tab.5 Experiment platform Experiment platform  CPU Pentium\(R\ 2.2GHz Memory DDR2, 2048MB Hard Disk ST3160815AS,160G The runtime that the improved Apriori algorithm compares with the traditional is shown in figure 1. The abscissa of the figure stands for the threshold of support \(100 /%\nd the ordinate stands for the runtime Fig.1 The time ratio of two ways to find frequent itemsets We assume the min_sup is 0.01, the min_con is 0.2 and the min_int is 1.45. The result that the improved algorithm compares with the traditional is shown in table 6 Tab.6 Compared the improved algorithm with the traditional Apriori Improved Apriori Efficiency improvement Frequent itemsets 114 44 61.4 Strong association rules 51 20 62 Time 79 24 69.62 The experiment results show that the improved Apriori algorithm can quickly and efficiently get the useful strong association rules. Figure 1 shows that the improved algorithm finds frequent itemsets faster than the traditional under different levels of support. It reflects that the rapid characteristics of the improved algorithms. As can be seen 223 


from Table 6, the number ratio of frequent itemsets that the improved algorithm compared with the traditional is 114:44 This stands for that the number of superfluous itemsets is 70 And the number ration of strong association rules is 51:20. It shows that the improved algorithm can get scientific strong association rules accurately 5 Conclusions This paper proposes an improved algorithm of Apriori. User interest items and a new model of interest measure are added into the algorithm and the steps of the algorithm are restructured. Compared with the traditional algorithm the improved algorithm has a great advantage in both speed and efficiency. But there is also room for improvement, such as improving the integrity of the frequent itemsets and doing more experiments  to verify the scientificalness of the interest model Association Rules algorithm has broad application prospects of the market. How to optimize the efficiency of database operations, how to select the more scientific min_sup min_con and min_int, how to further explore the practical application of the algorithm will be the future research work Acknowledgements This research is supported by National Natural Science Fund for Nature Program \(60872115\Shanghaiês Key Discipline Development Program \(J50104 References   R. AGRAWAL, R.SRIKAN. çFast algorithm s for mining association rules in lager databasesé, Santiago Proceedings of the Twentieth International Conference on Very Large Databases , pp. 487-499,\(1994   CHAOHUI L I U JIANC HE NG AN. çFast Mining a n d Updating Frequent Itemsetsé,Volume 1, 3-4 ,pp365 368, \(2008   S.MITR A, S   K. R A L, a n d P.MITR A D ata m i ning in soft computing framework: a surveyé. In IEEE Transactions on Neural Networks, Vo1.13, No. 1, \(2002   GRAY,ORLOWSKA.éClustering Categori cal Attributes into Interesting Association Rulesé, Proceedings of the 2nd Pacific-Asia Conference on Knowledge Discovery and Data Mining, Berlin: Springer,1998   M Klem ettinen, H Ma nnila R Ronkaine n. çFindi ng intrersting rules from large sets of discovered association rulesé New York, USA:ACM,1994   SYMT H P,GOODM AN R M. çAn inform ation the oretic approach to rule induction from databasesé,IEEE Trans on Knowledge and Data Engineering, 4\(4 1992 224 


  Fig. 6.  Other supporting graphics for the rule identified in Fig. 4   violated, but that is less critical than say, looking only a layer at C.  Discussion of Resu lts for Competing Methods In general, these methods fail to consistently rank the correct tool at the top of the list for at least one of the primary reasons in the list below.  There are secondary reasons as well such as some of the assumptions of the statistical tests are a time for a multi layer yield signal i Method searches a layer at a time The ANOVA and single layer rule methods look at the data split into a layer at a time.  As demonstrated in the simple example in Fig. 1  and 2, this can hide an otherwise obvious signal ii Method searches for a difference in the mean number of bad die \(or specific sort fail bins This is why ANOVA tends to fail on these data sets.  At high yielding factories many of the yield issues affe ct only a small number of lots and only a small number of die per wafer on affected lots Too few lots and die per wafer affected results in a small, if any difference, between the corr ect tools and other tools of its kind iii Method does not effectiv ely restrict the data set to approximately the affected time period only ANOVA calculates the means for each tool on all data provided This means it mixes unaffected and affected time periods diluting the signal.  Continge ncy tables also tabulate all the data provided, again mixing affected and unaffected time  periods and also diluting the signal iv Method uses a statistic for ranking which is sensitive to widely differing run rates and number of layers processed  between tool types This is the case with the most-lots-in least-time method.  Both Al gorithm 1 and most-lots-inleast time use conditional support as part of the ranking criterion.  However, Algorithm 1 also uses hit rate, which normalizes for differences in run rates and layers proc by dividing by the number of lo ts during the time period Most-lots-in-least time does not normalize for these differences, rather it simply searches for the smallest time window that includes the largest number of affected lot This leaves it vulnerable to ranking tools like wet benches consistently near the top of all the data sets  since wet benches typically have high run rates and are used at many layers.  A tool with a high run rate that is in use for many layers is more likely to essed s   have a small time window for the affected lots simply because it can run more lots in less time more often  in  e time d, rather than on constructing a sample.  Also, it enables greater efficiency in that more signals can be found in less time  IV.  CONCLUSIONS  In [2   we de m onstrated how m e thods bas e d on m oder n statistical learning algorithms can greatly reduce the time required to do the typical analysis done by yield analysts fabs.  Here, we have focused specifically on how a modern method generally referred to as association rules can be adapted to out perform traditi onal methods for doing multi layer tool commonality.  While any of the traditional methods could perform better with more human intervention, such as preparing well groomed data sets with carefully constructed Goodî lot lists or time periods to compare against, thes consuming efforts are made unnecessary by Algorithm 1.   We have coded the algorithm to run in seconds on a laptop computer against data sets of 500 lots and no time consuming sample selection is required fro m the user in order for the method to find the correct tool commonality, just run the past n days of data through the algorithm.  The benefit for the analyst is that more time can be spent following up on the top few commonalities identifie    326 ASMC 2010 


 IEEE/SEMI International vol., no pp.85-89, 21-23 May 1990 V R EFERENCES   1  T. Hastie,  R Tibshirani, and J. Friedman The Elements of Statistical Learning New York:  Springer-Verlag, 2001 2  St. Pierre, E.R.; Tuv, E.; Borisov, A., "Spatial Patterns in Sort Wafer Maps and Identifying Fab Tool Commonalities Advanced Semiconductor Manufacturing Conference, 2008. ASMC 2008 IEEE/SEMI vol., no., pp.268-272, 5-7 May 2008 3  Kong, G., "Tool commonality analysis for yield enhancement Advanced Semiconductor Manufacturing 2002 IEEE/SEMI Conference and Workshop vol., no., pp. 202-205, 2002 4  Garling, L.K.; Woods, G.P., "Determining equipment performance using analysis of variance Semiconductor Manufacturing Science Symposium, 1990. ISMSS 1990 327 ASMC 2010 


   for overlooking the overall software development process and playing a crucial role in making important decisions  5.1.3. The Criterion Set As stated earlier, criterion is the attribute for which favorability of an alternativ e is calculated. The criterion set defined for this case study comprises of Reusability  Meeting Operational Requirements and Meeting Project Deadline By reusability, we mean, the amount of reuse of different functionalities that ca n be achieved from the previously developed system on Mine detection training tool. Meeting operational requirements implies how effectively a desired operational capability can be satisfied by an alternative. For example, some alternative might lack a certain operational capability like database support whereas another may support it with enhanced features. Meeting project deadline stresses on the fact whether the project requirements can be satisfactorily achieved within the stipulated deadline which in our case was around one year  5.1.4. The Alternatives To resolve the concerned issu e, the stakeholders decided to choose one software platfor m for developing the mine detection training tool among the three stated alternatives Adobe Director  Adobe Flash  Open GL were chosen as the three possible alternatives along with some justifications. Adobe Flash was chosen as one of the alternatives because; the stakeholders already had a previous developed system for mine detection developed using Adobe Flash    Figure 6. Mine Detection System Along With Three Alternatives One of the considerations involved here was to enhance this system rather than develop a new system from scratch. Same reason applied to choosing Adobe Director as one of the other alternatives. Open GL was picked up as one of the three alternatives in the case when a new development had to be started Open GL is an advanced software development platform and it could have served as a good platform for the mine detection training system  Figure 6 summarizes the pro ject and its three alternatives positions available  5.2. Prioritizing The Criteria  For an effective decision making, we had to weigh the criteria according to their im portance in the decision making process. For this, we choose Analytic Hierarchy process because of its effectiveness in performing pair wise comparison of elements .Table 2 shows the ranking table used for comparing the two criteria  Table 2. Criteria Comp arison Table For AHP  Value a ij  Comparison Description 1 Criteria i and j are of equal importance 3 Criteria i is weakly more important than j 5 Criteria i is strongly more important than j 7 Criteria i is very strongly more important than j 9 Criteria i is absolutely more important than j   Table 3. Comparison Values For Prioritizing Different Criteria   Reusability Meeting Operational Requirements Meeting project Deadline Reusability 1 1/5 3 Meeting Operational Requirements 5  1  7 Meeting Project Deadline  1/3 1/7  1   149 


   Table 4. Normalized Criteria Comparison Table In AHP   Reusability Meeting Operational Requirements Meeting project Deadline Reusability 0.157 0.148 0.272 Meeting Operational Requirements 0.789 0.744 0.636 Meeting Project Deadline 0.052 0.106 0.090  Table 3 and Table 4 show the weight values of the three criterions as compared to each other using the AHP process. These weights have been decided by the stakeholders after discussions among themselves Average weights can be derived from Table 4 as follows Reusability- 0.193 Meeting Operational Requirements- 0.724 Meeting Project Deadline- 0.083 These weights represent the priority of each criterion on a scale of 0 to 1  5.3. Argumentation Tree  We develop argumentation tree for each and every alternative separately. The ar guments are stated by stake holders and assembled under the alternative but they target a specific cr iterion. These arguments can either be supporting or attacking each other or their respective alternative nodes. We present three figures, where each figure represents the argumentation hierarchy for one alternative. Rectangular boxes represent the alternatives with the name of the alternative under it. Ovals represent the criteria with their descr iption. The arguments are specified by labels èAê, èBê, èCê for alternative çAdobe flashé, çAdobe Directoré and çOpen GLé respectively Along with the labels, the arguments also have indexes associated with them. Beneath the labels are two boxes The box on left shows the weight of the argument whereas the box on right shows the priority of the stakeholder who specifies the argument  Once the argument has been sp ecified, the user enters its weight. We first reassess the weights of the arguments using priority reassessment discussed in h e n us ing the techniques specified in [11 w e red u ce t h e arg u m e n t s  to a single level. Finally, the weighted summation of the arguments with the criteria weights helps us evaluate the final weights for the decision matrix. It is important to note here that, the aggregation method used for calculating the favorability is a weighted summation  The three argumentation hierarchies for the three alternatives are presented in the Figures 7, 8, and 9. The diagrams contain arguments, their weights and the stakeholderês priorities     Figure 7. Argumentation Tree For Adobe Flash   Figure 8. Argumentation Tree For Adobe Director 150 


     Figure 9. Argumentation Tree For Open GL  A1 The current system in flash does not have the functionality of dynamic allocation of particles like mine or clutter. It places them randomly  A1.1 That is not of much importance because it still gives a new position to mine and clutter particles A2 Current system in flash has faster response time as compared to system in Adobe Director A3 The current system doesnêt satisfy many of the features required for the new system like database A4 Adobe Flash cannot communicate with database A4.1 Flash doesnêt support database but database support is very important and critical A4.1.1 The system should be able to generate evaluation reports for trainee based on pr evious records stored in the database A5 Flash doesnêt create sound clips  A5.1 We donêt need sound creating features as the sys tem has to generate sound. We can play externally recorded sound files using Adobe Flash A6 Flash can provide good visual effects as compared to Adobe Director A7 The developer has good knowledge in development using Flash so the system can be developed quickly B1 We could reuse the system already developed for sound generation, as it is developed using Adobe Audition for analysis which is somehow related to Adobe Director B1.1 The current system is better synthesized in terms of sound production and the sound produced is also instantaneous rather than discrete B1.2 That current system has certain performance issues like slow response time B1.3 The current system in Adobe Director has the feature of producing dynamic coloring scheme on approaching a mine. This kind of scheme is highly preferable and is not present in Adobe Flash system B2 Adobe Director can provide more functionality as compared to the current flash system. E.g. Multiple sounds while detecting mines   B2.1 Adobe Director can provide better visual effects as compared to flash e.g. in case of GUIês   B2.2 A modified version of the current system in flash can also provide the same functionality B2.2.1 We cannot integrate code developed in other platforms with Flash, but Flash can be integrated in Adobe Director B3 The interface provided by flash is not professional enough. It is too simple and straight forward for doing more things in future   B4 Easily available plug-ins can help integrate the tracking system developed in C# with Adobe Director  B4.1 Code developed in Open GL/AL can also be integrated using Adobe Director using suitable stubs   B5 A new sound recognition algorithm is being developed in Adobe Audition which can be integrated with Adobe Director but not with Open GL or Flash Evidence supported B6 If the current system is reused; the project deadline can be met easily B7 The developer has very little experience in development using Adobe Director   B7.1 The developer can take help from the already developed system in Adobe Director C1 The tracking software already developed is coded in C#/NX5. We could reuse that and develop our system in Open GL/AL C1.1 Open GL has C# libraries which can be used to develop the system C2 Because the platform used is for high end application development, it can provide good GUI and database support C2.1 Open GL/AL can help us generate dynamic surfaces for mine detection and training which the original system in flash does not have C4 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C3 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C4 The time taken for developing the project using open GL will be comparatively more as the whole system would have to be developed from scratch C4.1 If Open GL has support for C# libraries, and then the system could be develope d faster as developer is quite familiar with programming languages like C 151 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





