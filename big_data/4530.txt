A Fuzzy Associative Classi\002cation System with Genetic Rule Selection for High-Dimensional Problems J Alcal 264 a-Fd 264 ez R Alcal 264 a and F Herrera Department of Computer Science and Arti\002cial Intelligence CITIC-UGR University of Granada Granada Spain 18071 Email f jalcala alcala herrera g decsai.ugr.es Abstract 227The learning of Fuzzy Rule-Based Classi\002cation Systems for High-Dimensional problems suffers from exponential growth of the fuzzy rule search space when the number of patterns and/or variables becomes high In this work we propose a fuzzy association rule-based classi\002cation method with genetic rule selection for high-dimensional problems to obtain an accurate and compact fuzzy rule-based classi\002er with low computational cost The results obtained from the comparison with other two genetic fuzzy systems over nine real-world datasets with different characteristics show the effectiveness of the proposed approach I I NTRODUCTION There are many real applications in which Fuzzy RuleBased Classi\002cation Systems FRBCSs ha v e been employed However in most of them the available data consists of a high number of patterns and/or variables In this situation the learning of FRBCSs suffers from exponential growth of the fuzzy rule search This growth makes the learning process more dif\002cult and in most cases leads to problems of scalability and/or complexity Association discovery is one of the most common Data Mining techniques used to extract interesting knowledge from large datasets Man y ef forts ha v e been made to use its advantages for classi\002cation under the name of associative classi\002cation 5 6 A typical associative classi\002cation system is constructed in two stages 1 Discovering the association rules inherent in the database 2 Selecting a small set of relevant association rules to construct a classi\002er In order to enhance the interpretability of the obtained classi\002cation rules and to avoid unnatural boundaries in the partitioning of the attributes different studies have been presented to obtain classi\002cation systems based on fuzzy association rules 8 Supported by the Spanish Ministry of Education and Science under grant no TIN2008-06681-C06-01 In this paper we present a Fuzzy Association Rule-based Classi\002cation method for High-Dimensional problems FARCHD with genetic rule selection to obtain an accurate and compact fuzzy rule-based classi\002er with a low computational cost This method is based on three stages 1 Fuzzy association rule extraction for classi\002cation A search tree is employed to list all possible frequent fuzzy itemsets and generate fuzzy association rules for classi\002cation 2 Candidate rule prescreening We consider the use of subgroup discovery to select the most interesting rules by means of a pattern weighting scheme 3 Genetic fuzzy rule selection We consider the use of Genetic Algorithms GAs 11 to select a compact set of fuzzy association rules with high classi\002cation accuracy In order to assess the performance of the proposed approach we have used nine real-world datasets with a number of variables ranging from 19 to 60 and a number of patterns ranging from 208 to 7400 We have shown the results obtained from the comparison with other two genetic fuzzy systems GFSs and we have made use of some non-parametric statistical tests for multiple comparison 13 14 of the performance of these classi\002ers Furthermore we have analyzed the scalability of the proposed approach This paper is arranged as follows The next section brie\003y introduces the fuzzy association rules for classi\002cation Section III describes in detail each stage of the proposed approach Section IV shows and discusses the results obtained on the nine real-world datasets Finally in Section V some concluding remarks are made II P RELIMINARIES  F UZZY ASSOCIATION RULES FOR CLASSIFICATION Fuzzy set theory has been used more and more frequently in Data Mining to represent and to identify dependencies between attributes in a database The use of fuzzy sets to describe associations between data extends the types of relationships 


that may be represented F or t his reason in recent years different studies have proposed methods for mining fuzzy association rules from quantitative data 17 Let us consider a simple database N with two attributes age and weight and three linguistic terms with their associated MFs see Fig 1 Based on this de\002nition a simple example of a fuzzy association rule could be Age is Low  W eight is M iddle Fig 1 Attributes and linguistic terms in a simple problem Support and con\002dence are the most common measures of interest of an association rule These measures can be de\002ned for fuzzy association rules as follows Support  A  B   P x p 2 N 026 AB  x p   j N j 1 Conf idence  A  B   P x p 2 N 026 AB  x p   P x p 2 N 026 A  x p  2 where j N j is the total number of transactions patterns in the database 026 A  x p  is the matching degree of the transaction x p with the antecedent part of the rule and 026 AB  x p  is the matching degree of the transaction x p with the antecedent and consequent of the rule In the last few years different studies have proposed methods to obtain fuzzy association rule-based classi\002ers 8 The task of classi\002cation is to 002nd a set of rules in order to identify the classes of undetermined patterns A fuzzy association rule can be considered a classi\002cation rule if the antecedent contains fuzzy item sets and the consequent part contains only one class label A fuzzy associative classi\002cation rule A  Clas j could be measured directly in terms of support and con\002dence as follows Support  A  Clas j   P x p 2 Clas j 026 A  x p   j N j 3 Conf idence  A  Clas j   P x p 2 Clas j 026 A  x p   P x p 2 N 026 A  x p  4 III FARC-HD F UZZY A SSOCIATION R ULE BASED C LASSIFIER FOR H IGH D IMENSIONAL P ROBLEMS In this section we will describe our proposal to obtain a fuzzy association rule-based classi\002er for high dimensional problems This method is based on three stages 1 Fuzzy association rule extraction for classi\002cation 2 Candidate rule prescreening 3 Genetic fuzzy rule selection A scheme of this algorithm is shown in Fig 2 Fig 2 Scheme of FARC-HD method In the following subsections we will introduce the three mentioned stages explaining in detail all their characteristics A Fuzzy association rule extraction for classi\002cation To generate the initial rule base RB we employ a search tree to list all the possible fuzzy itemsets of each class The root or level 0 of a search tree is an empty set All attributes are assumed to have an order 002rst setting the fuzzy items of the variable 1 then the fuzzy items of the variable 2 so on The one-itemsets corresponding to the attributes are listed in the 002rst level of the search tree according to their order The children of a one-item node for an attribute A are the two-item sets that include the one-item set of attribute A and a one-item set for another attribute after attribute A in the order and so on An example with two attributes  V 1 and V 2  with two linguistic terms  L and H  is detailed in Figure 3 Notice that we can not joint the items set of a node with the items set of another previous node in the level because we will repeat nodes generated for this level For instance if we joint V 2  L with V 1  L we will obtained the node V 2  LV 1  L  which is the same as the node V 1  LV 2  L  An itemset with a support higher than the minimum support is a frequent itemset If the support of an n-item set in a node A is less than the minimum support it does not need to be extended more because the support of any item set in a node in the subtree led by node A will also be less than the minimum support Likewise if a candidate item set generates a classi\002cation rule with con\002dence higher than the minimum con\002dence it is again unnecessary to extend it further Once all frequent fuzzy itemsets have been obtained the candidates fuzzy association rules for classi\002cation can be generated setting the frequent fuzzy itemsets in the antecedent of the rules and the corresponding class in the consequent This process is repeated for each class 


Fig 3 The search tree for two quantitative attributes V 1 and V 2 with two linguistic terms L and H The number of frequent fuzzy itemsets extracted depends directly on the minimum support The minimum support is usually calculated considering the total number of patterns in the dataset however the number of patterns for each class in a dataset can be different For this reason our algorithm recalculates the minimum support for each class using the distributions of the classes over the dataset Thus the minimum support of the class Clas j is de\002ned as M inimumSupport Clas j  minSup 003 f Clas j 5 where minSup is the minimum support determined by the expert and f Clas j is the pattern ratio of the class Clas j  In this stage we can generate a large number of rules It is however very dif\002cult for human users to handle such a large number of generated fuzzy rules It is also very dif\002cult for human users to intuitively understand long fuzzy rules with many antecedent conditions For this reason the depth of the trees is limited to a 002xed value  Depth max  determined by an expert B Candidate rule prescreening In order to decrease the computational costs we consider the use of subgroup discovery to select the most interesting rules from the RB obtained in the previous stage by means of a pattern weighting scheme Thus in each iteration the rules are ordered according to a rule evaluation criteria from best to worst The best rule is selected covered patterns are reweighted and the procedure repeats these steps until all patterns have been covered more than k t times or there are no more rules in the RB This process is to be repeated for each class This scheme treats the patterns in such a way that covered positive patterns are not deleted when the currently best rule is selected Instead the algorithm stores for each pattern a count i of how many times the pattern has been covered by a selected rule and decreases its weight according to the formula w  e j  i   1  i 1 in the 002rst iteration all target class patterns are assigned the same weight w  e j  0  1  Covered patterns are completely eliminated when these have been covered more than k t times for more information see In this paper we have modi\002ed the measure used in wWRAcc to enable handling fuzzy rules Thus this measure for a rule A  Clas j is then de\002ned as n 00  A 001 Clas j   n 0  Clas j  001  n 00  A 001 Clas j   n 00  A  000 n  Clas j   N  6 where n 00  A  is the sum of the product of the weight of each covered pattern by its matching degree with the antecedent part of the rule n 00  A 001 Clas j  is the sum of the product of the weight of each covered positive pattern by its matching degree with the rules and n 0  Clas j  is the sum of the weights of patterns of class Clas j  Moreover the 002rst term in the de\002nition of wWRAcc has been replaced by n 00  A 001 Clas j   n  Clas j  to reward rules which cover uneliminated patterns of the class Clas j  C Genetic Fuzzy Rule Selection We consider the use of GAs to select a compact set of fuzzy association rules with high classi\002cation accuracy from RB obtained in the previous stage We consider the CHC genetic model which ha s achie v ed good results for binary selection problems In the follo wing the main characteristics of this genetic approach are presented 017 Genetic model 017 Codi\002cation and initial gene pool 017 Chromosome evaluation 017 Crossover operator 017 Restarting approach 1 CHC Genetic Model The genetic model of CHC makes use of a 223Population-based Selection\224 approach P parents and their corresponding offspring compete to select the best P individuals to take part in the next population The CHC approach makes use of an incest prevention mechanism and a restarting process to provoke diversity in the population instead of the well known mutation operator This incest prevention mechanism will be considered in order to apply the crossover operator i.e two parents are crossed if their hamming distance divided by 2 is over a predetermined threshold L  This threshold value is initialized following the original CHC scheme as L   Genes   4  0 7 where  Genes is the number of genes in the chromosome for more information see In order to mak e this procedure independent of  Genes  in our case L will be decremented by a   of its initial value being  determined by the user usually 10 The algorithm restarts when L is below zero A scheme of this algorithm is shown in Fig 4 


Fig 4 Scheme of CHC 2 Codi\002cation and Initial Gene Pool Each chromosome is a binary vector that determines when a rule is selected or not alleles 1 and 0 respectively A chromosome C  f c 1   c M g represents a subset of rules so that If c i  1 then  R i 2 RB  else  R i 62 RB   with R i being the corresponding i th rule in the initial RB  To make use of the available information all the candidates rules are included in the population as an initial solution To do so the initial pool is obtained with an individual having all genes with value 1 and the remaining individuals generated at random in f 0 1 g  3 Evaluation To evaluate a determined chromosome we compute the classi\002cation rate clas  rat and maximize the following function de\002ned for maximization F itness  C   clas  rat 000 w 1 001 N R 8 where N R is the number of selected rules to penalize a large number of rules and w 1 is computed from the clas  rat considering all the candidate rules w 1  013 001  clas  rat initial N R initial  9 with 013 being a weighting percentage given by the system expert that determines the tradeoff between accuracy and complexity We have empirically tested that a good neutral choice is for example 1.0 good accuracy and not too many rules The 002tness value of a chromosome will be 000 w 1 if there is at least one class without selected rules 4 Crossover Operator The half uniform crossover scheme HUX is employed This operator e xactly interchanges the mid of the alleles that are different in the parents ensuring the maximum distance of the offspring to their parents exploration Figure 5 depicts the behavior of this operator 5 Restarting Approach To get away from local optima this algorithm uses a restart approach In this case the best chromosome is maintained and the remaining are generated at random in f 0,1 g  The algorithm restarts when L is below zero which means that all the individuals coexisting in the population are very similar Fig 5 Scheme of the behavior of the HUX operator IV E XPERIMENTAL STUDY In order to analyze the performance of the proposed approach we have considered 9 real-world datasets Table I summarizes the main characteristics of the 9 datasets where Attributes is the number of attributes P atterns is the number of patterns and Classes is the number of classes Furthermore this table shows the link to the KEEL project webpage from which the y can be do wnloaded Notice that we have removed the instances with any missing value in the dataset Dermatology TABLE I D ATA SETS CONSIDERED FOR THE EXPERIMENTAL STUDY  Name Attributes Patterns Classes  Segment 19 2310 7 Ringnorm 20 7400 2 Thyroid 21 7200 3 Wdbc 30 569 2 Ionosphere 34 351 2 Dermatology 34 358 6 SatImage 36 6435 6 Spambase 57 4597 2 Sonar 60 208 2  Available at http://sci2s.ugr.es/keel/datasets.php This section is organized as follows 017 First we describe the experimental set-up in subsection IV-A 017 Second we show a statistical analysis obtained from the comparison with another two GFSs in subsection IV-B 017 Finally we analyze the scalability of the proposed approach in subsection IV-C A Experimental Set up In this study we have compared the performance of our approach with another two GFSs including FH-GBML and SGERD algorithms 017 FH-GBML This method follows a Genetic CooperativeCompetitive Learning GCCL approach and consists of two processes The 002rst process is used for generating good fuzzy rules while the second one is used for 002nding good combinations of generated fuzzy rules This method simultaneously use multiple fuzzy partitions with different granularities for fuzzy rule extraction using four homogeneous fuzzy partitions with triangular fuzzy sets and a don't care condition 017 SGERD It is a steady-state GA to generate a prespeci\002ed number of Q rules per class following a GCCL approach In each iteration parents and their corresponding offspring compete to select the best Q rules for each class Simultaneously this method also uses multiple 


fuzzy partitions with different granularities and don't care conditions for fuzzy rule extraction To develop the different experiments we consider a 10-fold cross-validation model  i.e we randomly split the data set into 10 folds each containing 10 of the patterns of the data set and used nine folds for training and one for testing 1  For each of the ten partitions we executed three trials of the algorithms For each data set we therefore consider the average results of 30 runs The initial linguistic partitions are comprised by 002ve linguistic terms with uniformly distributed triangular membership functions giving meaning to them The values considered for the input parameters of each method are 017 FARC-HD M inSup  0  05  M inConf  0  8  Depth max  3  k t  2  P ob  50  Eval  5  000  013  0  02 017 FH-GBML N rules  20  F sets  200  Gen  1000  P c  0  9  P dcare  f 0  5  0  8  0  95 g  P michigan  0  5 017 SGERD Q  heuristics These values were selected according to the recommendation of the corresponding authors within each proposal which are the default parameters settings included in the KEEL software tool Notice that in the FH-GBML algorithm the authors used three different probabilities of don't care 0.5 0.8 and 0.95 depending on the size of the dataset to obtain fuzzy rules with a few antecedent fuzzy sets In these experiments we have used these three probabilities of don't care in each dataset and have shown in the tables the best average result obtained in each one B Results and Statistical Analysis The results obtained by the analyzed methods are shown in Table II where  R stands for the average number of rules  L stands for the average number of linguistic terms in the antecedent of the rules and T ra and T st respectively for the average accuracy obtained over the training and test data The results presented show that our proposal obtains the best global results for test Furthermore the average number of rules is low 13.5 rules on average and the rules obtained only involve three or less attributes in their antecedents giving the advantage of easier understanding from an user's perspective In order to compare the results we have used nonparametric statistical tests for multiple comparison 13  to 002nd the best approach considering the results obtained in the test partitions  T st  First of all we have used Friedman and ImanDavenport tests 25 in order to 002nd out whether signi\002cant differences exist among all the mean values Tables III shows the Friedman and ImanDavenport statistics and it relates them to the corresponding critical values for each distribution by using a level of signi\002cance 013  0  05  The p value obtained is also reported for each test Given that the statistics of Friedman and ImanDavenport are clearly greater than their associated critical values there are 1 The corresponding data partitions 10-fold for these data sets are available at the KEEL project webpage http://sci2s.ugr.es/keel/datasets.php signi\002cant differences among the observed results with a level of signi\002cance 013 024 0  05  TABLE III R ESULTS OF THE F RIEDMAN AND I MAN D AVENPORT TESTS  013  0  05   Friedman test  ImanDavenport test Statistic  X 2 F  Critical Value p value  Statistic  F F  Critical Value p value  14.001 5.991  0.001  28.001 3.634  0.001  Table IV shows the rankings computed using a Friedman test of the different methods considered in this study TABLE IV A VERAGE R ANKINGS OF THE METHODS  Method  Ranking  SGERD  2.666 FH-GBML  2.333 FARC-HD  1.000  We now apply Holm's test to compare the best ranking method FARC-HD with the remaining methods In Table V the methods are ordered with respect to the z-value obtained Holm's test rejects the hypothesis of equality with the rest of the methods  p  013=i  Therefore analyzing the statistical study shown in Tables IV and V we conclude that our model has shown itself to be the best performing method when compared with the remaining fuzzy GFSs applied in this study C Analysis of Scalability Table VI shows the average runtime of the analyzed methods on the 9 real-world problems The methods were implemented using Java and all of the experiments were performed using a Pentium Corel 2 Quad 2.5GHz CPU with 4Gb of memory and running Linux Analyzing the results presented we can see that 017 The SGERD algorithm presents a very low average runtime in all datasets obtaining a good scalability when we increase the size of the problem This method however obtains the worst ranking in the Friedman's test when we compare the results obtained in the test partitions see table IV 017 The FH-GBML algorithm expends a large amount of time when the number of attributes and patterns in the dataset is high 017 The FARC-HD approach presents a low computational cost in all datasets obtaining a good scalability and the best performance in accuracy V C ONCLUSION In this paper we have proposed a fuzzy associative classi\002cation method with genetic rule selection for high-dimensional datasets to obtain accurate and compact fuzzy associative classi\002ers with a low computational cost Analyzing the results obtained we can conclude that our model has shown itself to be the best performing method in the experimental study and presents a low computational cost in all datasets obtaining a good scalability Furthermore our proposal obtains models with a reduced number of rules with few attributes in the antecedent giving the advantage of high interpretability from an user's perspective 


TABLE II R ESULTS OBTAINED BY THE ANALYZED METHODS  FH-GBML SGERD FARC-HD Dataset   R  L Tra Tst   R  L Tra Tst   R  L Tra Tst  Segment  10.3 10.6 72.8 73.6  8.50 1.98 76.49 76.78  22.2 2.4 91.1 89.9 Ringnorm  6.9 11.3 87.3 86.9  6.83 2.00 73.21 72.63  9.8 1.5 88.2 87.9 Thyroid  3.0 6.7 93.5 93.2  2.13 2.00 92.89 92.84  3.7 2.5 93.6 93.4 Wdbc  7.2 4.9 95.1 92.3  3.70 2.00 91.79 90.68  6.3 1.2 95.4 94.4 Ionosphere  12.2 3.7 74.4 73.2  3.97 1.61 80.40 79.22  15.2 1.9 95.4 90.0 Dermatology  10.3 3.4 85.2 83.5  8.20 1.80 81.70 78.88  23.5 2.4 99.3 90.2 SatImage  5.8 19.6 75.0 74.4  21.80 2.00 64.08 64.05  19.0 2.7 75.6 74.9 Spambase  3.9 18.5 77.9 77.2  3.70 2.00 72.90 72.98  9.8 1.7 84.8 84.8 Sonar  10.3 4.7 80.6 68.2  3.17 2.00 74.22 71.90  11.6 2.4 93.8 78.9  Mean  7.8 9.3 82.4 80.3  6.9 1.9 78.6 77.8  13.5 2.1 90.8 87.1  TABLE V H OLM T ABLE FOR THE SELECTION METHODS WITH 013  0  05  i Method z p 013=i Hypothesis  2 SGERD 3.53 4.069E-4 0.025 Rejected 1 FH-GBML 2.82 0.0046 0.05 Rejected  TABLE VI R UNTIME OF THE ANALYZED METHODS  HH  MM  SS   Dataset  FH-GBML SGERD FARC-HD  Segment  02:03:51 00:00:03 00:00:26 Ringnorm  10:05:13 00:00:09 00:01:06 Thyroid  07:48:19 00:00:08 00:00:13 Wdbc  00:50:01 00:00:01 00:00:09 Ionosphere  00:41:28 00:00:01 00:00:04 Dematology  00:26:14 00:00:01 00:00:04 SatImage  12:04:02 00:00:19 00:04:51 Spambase  13:01:47 00:00:16 00:03:29 Sonar  00:21:02 00:00:01 00:00:36  Mean  05:15:46 00:00:06 00:01:13  R EFERENCES  H Ishib uchi T  Nakashima and M Nii Classi\002cation and Modeling with Linguistic Information Granules Advanced Approaches to Linguistic Data Mining  Berlin Springer-Verlag 2004  W  Combs and J Andre ws 223Combinatorial rule e xplosion eliminated by a fuzzy rule con\002guration,\224 IEEE Transactions on Fuzzy Systems  vol 6 no 1 pp 1\22611 1998  J Han and M Kamber  Data Mining Concepts and Techniques Second Edition  San Fransisco Morgan Kaufmann 2006  B Liu W  Hsu and Y  Ma 223Inte grating classi 002cation and association rule mining,\224 in Proceedings of the International Conference on Knowledge Discovery and Data Mining SIGKDD  New York USA 1998 pp 80\22686  G Dong X Zhang L W ong and J Li 223Caep Classi\002cation by aggregating emerging patterns,\224 in Proceedings of the Second International Conference on Discovery Science DS  ser Lecture Notes in Arti\002cial Intelligence S Arikawa and K Furukawa Eds Tokyo Japan Springer Berlin-Heidelberg 1999 vol 1721 pp 30\22642  B Liu Y  Ma and C W ong 223Classi\002cation using association rules weaknesses and enhancements,\224 in Data Mining for Scienti\002c and Engineering Applications  ser Massive Computing R Grossman C Kamath P Kegelmeyer V Kumar and R Namburu Eds Springer BerlinHeidelberg 2001 vol 2 pp 591\226602  Y C Hua and G.-H Tzeng 223Elicitation of classi\002cation rules by fuzzy data mining,\224 Engineering Applications of Arti\002cial Intelligence  vol 16 no 7-8 pp 709\226716 2003  Z Chen and G Chen 223Building an associati v e classi 002er based on fuzzy association rules,\224 International Journal of Computational Intelligence Systems  vol 1 no 3 pp 262\226273 2008  B Ka vsek and N La vrac 223 Apriori-sd Adapting association rule learning to subgroup discovery,\224 Applied Arti\002cial Intelligence  vol 20 no 7 pp 543\226583 2006  D Goldber g Genetic algorithms in search optimization and machine learning  New York Addison-Wesley 1989  J Holland Adaptation in natural and arti\002cial systems  London The University of Michigan Press 1975  J Dem 020 sar 223Statistical comparisons of classi\002ers over multiple data sets,\224 Journal of Machine Learning Research  vol 7 pp 1\22630 2006  S Garc 264 021a and F Herrera 223An extension on statistical comparisons of classi\002ers over multiple data sets for all pairwise comparisons,\224 Journal of Machine Learning Research  vol 9 pp 2579\2262596 2008  S Garc 264 021a A Fern 264 andez J Luengo and F Herrera 223A study of statistical techniques and performance measures for genetics-based machine learning Accuracy and interpretability,\224 Soft Computing  vol 13 no 10 pp 959\226977 2009  T  Sudkamp 223Examples countere xamples and measuring fuzzy associations,\224 Fuzzy Sets and Systems  vol 149 no 1 pp 57\22671 2005  J Alcal 264 a-Fdez R Alcal 264 a M Gacto and F Herrera 223Learning the membership function contexts for mining fuzzy association rules by using genetic algorithms,\224 Fuzzy Sets and Systems  vol 160 no 7 pp 905\226921 2009  T  Hong and Y  Lee 223 An o v ervie w of mining fuzzy association rules 224 in Studies in Fuzziness and Soft Computing  H Bustince F Herrera and J Montero Eds Springer Berlin/Heidelberg 2008 vol 220 pp 397\226410  L Eshelman 223The chc adapti v e search algorithm Ho w to ha v e safe serach when engaging in nontraditional genetic recombination,\224 in Foundations of Genetic Algorithms  G Rawlin Ed Morgan Kaufmann 1991 vol 1 pp 265\226283  J Cano F  Herrera and M Lozano 223Using e v olutionary algorithms as instance selection for data reduction in kdd an experimental study,\224 IEEE Transactions on Evolutionary Computation  vol 7 no 6 pp 561\226 575 2003  L Eshelman and J Schaf fer  223Real-coded genetic algorithms and interv al schemata,\224 in Foundations of Genetic Algorithms  D Whitley Ed Morgan Kaufmann 1993 vol 2 pp 187\226202  J Alcal 264 a-Fdez L S 264 anchez S Garc 264 021a M del Jesus S Ventura J Garrell J Otero C Romero J Bacardit V Rivas J Fern 264 andez and F Herrera 223KEEL A software tool to assess evolutionary algorithms to data mining problems,\224 Soft Computing  vol 13 no 3 pp 307\226318 2009  H Ishib uchi T  Y amamoto and T  Nakashima 223Hybridization of fuzzy gbml approaches for pattern classi\002cation problems,\224 IEEE Transactions on Systems and Man and Cybernetics Part B Cybernetics  vol 35 no 2 pp 359\226365 2005  E Mansoori M Zolghadri and S Katebi 223Sgerd A steady-state genetic algorithm for extracting fuzzy classi\002cation rules from data,\224 IEEE Transactions on Fuzzy Systems  vol 16 no 4 pp 1061\2261071 2008  M Friedman 223The use of ranks to a v oid the assumption of normality implicit in the analysis of variance,\224 Journal of the American Statistical Association  vol 32 no 200 1937  R Iman and J Da v enport 223 Approximations of the critical re gion of the friedman statistic,\224 Communications in Statistics Part A Theory and Methods  vol 9 pp 571\226595 1980  S Holm 223 A simple sequentially rejecti v e multiple test procedure 224 Scandinavian Journal of Statistics  vol 6 pp 65\22670 1979 


preMinsup Fig 3 Experimental results runtimes All experiments were run in a time-sharing environment in an 800 MHz machine The reported 336gures are based on the average of multiple runs Runtime includes CPU and I/Os it includes the time for both tree construction and frequent itemset mining steps We evaluated different aspects of the proposed algorithms which were implemented in C First we compared the performance of the three proposed algorithms using four different constraints one from each type of the above constraints Experimental results showed that the runtimes for both UF-streaming 100   150   200   250   10   20   30   40   50   60   70   80   90  Selectivity \(i.e., percentage of items selected CUF-streaming \(w=5 batches, each with 1M transactions Type IV constraint C4  Type II constraint C2                      Type III constraint C3                      Type I constraint C1                                 100   150   200   250   300   350   400   450   10   20   30   40   50   60   70   80   90  Selectivity \(i.e., percentage of items selected CUF-streaming \(w=50 batches, each with 1M transactions Type IV constraint C4  Type II constraint C2                      Type III constraint C3                      Type I constraint C1                                 50   55   60   65   70   75   80   85   90   0.002   0.003   0.004   0.005  preMinsup \(in percentage Runtime vs. existential probability & preMinsup Items take on an average number of existential probability values                      005 005    t t 327 327 005 items All 322extensions\323 of valid items were valid Due to the item ordering the algorithm stopped checking constraints whenever it detected the 336rst invalid items However for on the mining results For example using 0.8 C C C C C C C C C C w w C C w 0.9 preMinsup  90 of the mined constrained 322frequent\323 itemsets were truly frequent When and UF-streaming Asitexplored properties of these four constraints and pushed the constraints inside the mining process CUF-streaming required shorter runtimes than the other two algorithms As shown in Fig 3\(a the runtimes for handling all four types of constraints increased when the selectivity increased Among them a Runtime vs selectivity  a Type I constraint incurred the lowest runtime among the four types of constraints because CUF-streaming formed fewer 322extensions\323 as they consisted of only valid items Again due to the item ordering the algorithm stopped checking constraints whenever it detected the 336rst valid items Next we repeated the above experiment with a different the window size was low say 10 only a few small UF-trees were constructed and mined as the algorithm only 322extended\323 valid items and a shorter runtime 50 c Runtime vs  and the convertible monotonicity of  the monotonicity of  the convertible anti-monotonicity of 110 sec cf 160 sec in Fig 3\(a was required As another example for 5 batches when  the algorithm applied constraint checking on projected DBs for valid items as well as their 322extensions\323 because not all 322extensions\323 of valid items were valid  the algorithm 322extended\323  many bigger UF-trees were constructed and mined as the algorithm formed projected DBs for both valid as well as invalid domain items which took or having more batches in the sliding window had the bene\336ts of increasing the chance of not pruning relevant expected support information for truly frequent itemsets Moreover as shown in Fig 3\(c when increased fewer itemsets had expected support performed constraint checking as an intermediate step prior to storing the 322frequent\323 itemsets into the UF-stream structure In contrast CUF-streaming was more interesting as it runtimes depended on the type of constraints as well as the constraint selectivity Speci\336cally the algorithm explored the anti-monotonicity of a Type II constraint and a Type III constraint incurred the next two highest runtimes For  and thus shorter runtimes were required The 336gure also showed the effect of the distribution of item existential probability When items took on a few unique existential probability values the UF-tree b ecame smaller Thus times for both UF-tree construction and mining became shorter In addition we also measured the number of nodes in each UF-tree The experimental results showed that the total number of nodes in a UF-tree was no more than the total number of items with their existential probability in all transactions in the current batch of uncertain data stream Furthermore we measured the number of nodes in the UF-stream structure as well As UF-streaming   126 400 sec cf 230 sec in Fig 3\(a As all three algorithms are approximate algorithms we evaluated the effect of  95 of the mined constrained 322frequent\323 itemsets were truly frequent However lowering 5 b Runtime vs selectivity  were constant regardless of the constraint selectivity because these two algorithms did not explore property nor did they push the constraints inside the mining process Speci\336cally UF-streaming only valid preMinsup minsup preMinsup minsup preMinsup preMinsup preMinsup performed constraint checking as a postprocessing step whereas UF-streaming a Type IV constraint incurred the highest runtime because CUF-streaming 322extended\323 i.e formed projected DBs for both valid and invalid items performed constraint checking at a post-processing step the size of UF-stream was observed to be independent of the constraint selectivity In contrast as Items take on many different existential probability values  50 336xed-sized batches with each batch containing 0.1M transactions instead of using 5 336xed-sized batches with each batch containing 1M transactions With this setting each batch was smaller 0.1M vs 1M transactions Thus each batch required lower runtime e.g for constructing and mining UF-trees However the number of batches was higher 50 vs 5 batches than the previous setting This explains why the runtimes see Fig 3\(b took on a broader range than the previous experimental results For example when the selectivity of 1 2 3 4 4 2 3 2 3 1 2 4 w w 0   Runtime \(in seconds 0   Runtime \(in seconds 0   Runtime \(in seconds Items take on a few unique existential probability values                     50   50   0.001   


 ch 6 AAAI/MIT Press 2004  G  G r ahne L  V  S  L aks h m a nan and X  W ang 322E f 336 ci ent m i n i n g o f constrained correlated sets,\323 in ACM TKDD  Proc KDD 2009 Proc VLDB 1994 Proc KDD 2009 Proc IEEE ICDE 2008 Proc PAKDD 2007 Proc VLDB 2008 Proc IEEE ICDE 2000 Proc VLDB 2008 Proc IEEE ICDE 2009 Proc IEEE ICDM 2006 Proc IEEE ICDE 2002 Proc IEEE ICDE 2001 Proc IEEE ICDE 2008 preMinsup minsup Proc U  09 Proc PAKDD 2008 Proc ACM SIGMOD 2008 Data Mining and Knowledge Discovery Proc ACM SIGMOD 1993 Proc SSTD 2005 Proc ACM SIGMOD 2000 Proc ACM SIGMOD 2009 Proc ACM SIGMOD 1998 Proc ACM SIGMOD 2008 2 pp 18\32026 June 2005 12 C G ia n n e lla e t a l 322 M in in g f r e q u e n t p a tte r n s in d a ta s tr e a m s a t m u ltip le time granularities,\323 in 4 pp 337\320389 Dec 2003  C  K  S  L eung 322F r e quent i t e m s et m i ni ng w i t h cons t r ai nt s  323 i n 127 1 batch containing the entire dataset Then we compared our algorithms with UF-growth 22 b y as s i g n i n g t o each i t em i n e v er y t r an s act i o n in a dataset an existential probability of 1 i.e all items are de\336nitely present in the dataset and 005 005 Encyclopedia of Database Systems queries on uncertain streams,\323 in 34 28 34  pp 29\32037 2 R  A gr aw al et al   322M i n i n g a s s o ci at i o n r ul es bet w een s e t s of i t e ms i n large databases,\323 in  pp 207\320216 3 R  A gr aw al and R  S r i kant  322 F a s t al gor i t h ms f o r m i n i n g a s s o ci at i o n rules,\323 in  pp 487\320499 4 R  J  B ayar do J r   R  A g r a w a l  and D  G unopul os  322 C ons t r ai nt b as ed rule mining in large dense databases,\323 2\3203 pp 217\320240 July 2000 5 T  B e r n e c k e r e t a l 322 P r o b a b ilis tic f r e q u e n t ite m s e t m in in g in u n c e r ta in databases,\323 in  pp 119\320127 6 R  C h e n g e t a l 322 P r o b a b ilis tic v e r i\336 e r s  e v a lu a tin g c o n s tr a in e d n e a r e s tneighbor queries over uncertain data,\323 in  pp 47\32058 8 G  C or m ode and M  H adj i e l e f t her i ou 322F i ndi ng f r e quent i t e m s i n dat a streams,\323 in  pp 1530\3201541 9 G  C o r m o d e e t a l 322 F in d in g h ie r a r c h ic a l h e a v y h itte r s in s tr e a m in g d a ta  323  pp 400\320417  M M G a ber  A  B  Z a s l a v s k y  and S  K r i s hnas w am y  322Mi n i n g d at a streams a review,\323  pp 512\320521  J  H a n J  P e i  and Y  Y i n  322 Mi ni ng f r e quent pat t e r n s w i t hout candi dat e generation,\323 in  pp 1\32012 15 J  H u a n g e t a l 322 M a y B M S  a p r o b a b ilis tic d a ta b a s e m a n a g e m e n t s y s tem,\323 in  pp 1071\3201074  C  J i n e t a l   322 S l i di ngw i ndo w t op pp 301\320312  L  V  S  L a ks hm anan C  K  S  L e ung and R  T  N g 322E f 336 ci ent dynam i c mining of constrained frequent sets,\323  pp 9\320 18  C  K  S  L eung and B  H ao 322 Mi ni ng of f r e quent i t e m s et s f r o m s t r eam s of uncertain data,\323 in  pp 1663\3201670  C  K  S  L eung and Q  I  K han 322D S T r ee a t r e e s t r uct u r e f o r t he m i ni ng of frequent sets from data streams,\323 in  pp 928\320 933  C  K  S  L eung M A  F  Mat e o and D  A  B r a j czuk 322 A t r eebas e d approach for frequent pattern mining from uncertain data,\323 in  pp 13\320 24  J  P e i  J  H a n and L  V  S  L aks h m a nan 322Mi n i n g f r e quent i t e m s et s w i t h convertible constraints,\323 in  pp 433\320442  C  R 253 e et al 322Event queries on correlated probabilistic streams,\323 in  pp 715\320728 27 A  D  S a r m a  M  Th e o b a ld  a n d J  W id o m  322 Ex p lo itin g lin e a g e f o r con\336dence computation in uncertain and probabilistic databases,\323 in  pp 1023\3201032  K  Y i et al   322S m a l l s ynops es f o r g r oupby quer y ver i 336 cat i o n o n outsourced data streams,\323  pp 819\320832 322frequent\323 itemsets from uncertain data streams In terms of ef\336ciency the experimental results showed that UF-streaming was slightly faster because it did not perform any constraint checking whereas our three proposed algorithms performed the extra constraint checking step Among them CUF-streaming only performed constraint checking on some 322frequent\323 itemsets and the other two performed constraint checking on all 322frequent\323 itemsets However in terms of the mining results we observed that all four algorithms returned the same collection of 322frequent\323 itemsets This illustrated that our proposed algorithms could be used for mining unconstrained frequent itemsets from uncertain data streams Moreover it is important to note that while the UF-streaming is con\336ned to 336nding 322frequent\323 itemsets satisfying constraints with 100 selectivity our algorithms are capable of 336nding 322frequent\323 itemsets that satisfy constraints having lower selectivity Along this direction we set and CUF-streaming both pushed the constraint early the corresponding size of UF-stream was proportional to the selectivity of constraints Finally we evaluated the functionality and applicability of our proposed algorithms We again used four different constraints and we also set the constraint selectivity be 100 i.e all items are selected Then we compared our three proposed algorithms with UF-streaming  w hi c h w a s designed to mine 1 UF-streaming 4 article 2 Jan 2008 10 X  D a i e t a l 322 P r o b a b ilis tic s p a tia l q u e r ie s o n e x is te n tia lly u n c e r ta in data,\323 in 3 article 15 Aug 2009 29 Q  Zh a n g  F  Li a n d K  Y i 322 F in d in g f r e q u e n t ite m s in p r o b a b ilis tic data,\323 in  pp 1179\3201183 Springer 2009  C  K  S  L eung and D  A  B r a j czuk 322E f 336 ci ent a l gor i t h m s f o r m i n i n g constrained frequent patterns from uncertain data,\323 in Data Mining Next Generation Challenges and Future Directions w  UF-streaming unconstrained and CUF-streaming\321 which integrate i mining of uncertain data ii constrained mining and iii mining of data streams These algorithms effectively mine constrained frequent itemsets from uncertain data streams A CKNOWLEDGMENT This project is partially sponsored by Natural Sciences and Engineering Research Council of Canada NSERC and the University of Manitoba in the form of research grants R EFERENCES 1 C  C  A ggar w al et al   322F r e quent pat t e r n m i ni ng w i t h uncer t a i n dat a  323 i n  pp 653\320661  C  K  S  L eung R  T  N g  a nd H  Manni l a  322 O S S M  a s e gm ent a t i o n approach to optimize frequency counting,\323 in SIGMOD Record ACM TODS ACM TODS k 4   Again we observed that all four algorithms returned the same collection of frequent itemsets This illustrated that our proposed algorithms could also be used for mining unconstrained frequent itemsets from static uncertain datasets VII C ONCLUSIONS Frequent itemsets generally serve as building blocks for various patterns in many real-life applications Most of the existing algorithms 336nd unconstrained frequent itemsets from traditional static transaction databases consisting of precise data However there are situations in which ones are uncertain about the contents of transactions There are also situations in which users are only interested in some subsets of all the mined frequent itemsets Furthermore a 337ood of data can be easily produced in many situations To deal with all these situations we proposed three tree-based algorithms\321 namely UF-streaming   pp 973\320982 7 C  K  C hui  B  K ao a nd E  H ung 322Mi n i n g f r e quent i t e m s et s f r o m uncertain data,\323 in     pp 583\320592  R  T  N g et al   322E xpl or at or y m i n i n g a nd pr uni ng opt i m i zat i ons of constrained associations rules,\323 in  


              


   


                        





