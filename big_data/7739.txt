Big Data Gathering and Mining Pipelines for CRM using Open-source Kang Li   Vinay Deolalikar   and Neeraj Pradhan  Search and Data Mining Groupon Palo Alto CA 94306 E-mail  kli@groupon.com  vdeolalikar@groupon.com  neepradhan@groupon.com Abstract Customer Relationship Management CRM is currently the fastest growing sector of enterprise software estimated to increase to  36.5B worldwide by 2017 CRM technologies increasingly use data mining primitives across multiple applications At the same time the growth of big data has led to the evolution of an open source big data software stack primarily powered by Apache software that rivals traditional enterprise database RDBMS stacks New technologies such as Kafka Storm HBase have signiìcantly enriched this open source stack alongside more established technologies such as Hadoop MapReduce and Mahout Today enterprises have a choice to make regarding which stack they will choose to power their big data applications However there are no published studies in literature on enterprise big data pipelines built using open source components supporting CRM Speciìc questions that enterprises have include how is the data processed and analyzed in such pipelines What are the building blocks of such pipelines How long does each step of this processing take In this work we answer these questions for a large scale serving over a 100M customers industrial CRM pipeline that incorporates data mining and serves several applications Our pipeline has broadly two parts The rst is a data gathering part that uses Kafka Storm and HBase The second is a data mining part that uses Mahout and Hadoop MapReduce We also provide timings for common tasks in the second part such as data preprocessing for machine learning clustering reservoir sampling and frequent itemset extraction 1 Introduction Today perhaps more than ever before enterprises realize that understanding their users is critical to their success Accordingly Customer Relationship Management CRM is currently the fastest growing sector of enterprise software estimated to increase to 36.5B worldwide by 2017  4  CRM for an enterprise with a large customer base has signiìcant big data challenges typically involving both a streaming aspect capturing customer behavior and a batch component analyzing and mining captured behavior Also CRM technologies increasingly use data mining techniques across multiple applications At the same time the eld of big data has been evolving rapidly A highly signiìcant evolution is the formation of a formidable open source stack of technologies starting with Apache Hadoop and incorporating recent entrants such as Apache Kafka and Storm Typically today both RDBMS and open source stacks co-exist in different parts of the enterprise big data ecosystem Although RDBMS is still used mostly for structured data unstructured data is typically stored on HDFS including NoSQL databases like HBase which use HDFS under the hood Hadoop is used for large scale batch computations such as clustering which would be difìcult to replicate using traditional RDBMS Hadoop is also highly scalable and we can add more nodes to the cluster to reduce computation time something that is not the case with traditional RDMBS The preceding two threads bring us to the problem statement of this poster which we pose as two questions 1 What are the components of a big data processing and mining pipeline using the open source stack for powering CRM applications 2 What are reasonable performance baselines for standard tasks using such open source components In this poster we parse out concrete questions that we would like to answer 1a What are the open source components of such a big data CRM pipeline How do these components address the streaming and the batch aspects of the big data challenge for CRM 1b Where are fundamental primitives such as clustering frequent itemset extraction and sampling located in the pipeline 2 How long do typical data preprocessing steps and fundamental data mining primitives take without excessive tuning namely using default settings                  Fig 1 The big data gathering and processing Pipeline We present details of an industrial big data pipeline built using open source components that is currently serving CRM applications for upwards of 100M customers We provide details on how the various technologies play their respective roles in this data pipeline Finally we provide performance benchmarks for standard tasks encountered in CRM especially using data mining on open source components using their default settings 2015 IEEE International Conference on Big Data \(Big Data 978-1-4799-9926-2/15/$31.00 ©2015 IEEE 2936 


2 Related Work In this abstract we provide a brief overview of major streams of research that relate to our work CRM As noted CRM has become the fastest growing sector of enterprise software  4  Accordingly  there are se v eral white papers that describe CRM solutions from leading vendors such as Oracle and IBM  5  11  Ho we v e r  there is little in published research literature on end-to-end CRM solutions using open source frameworks and including benchmarks Our work attempts to ll that gap Big Data and Open Source Open source has become a disruptive trend in big data and the literature in it has become correspondingly large  10  lists 100 papers co v ering various aspects of big data that are addressed by open source solutions Most of the technical description for each open source component occurs online and corresponding URLs are provided throughout this paper Big Data Benchmarks and MapReduce Optimizations  6  d o a n in-depth performance analysis of MapReduce  7  provide a host of big data benchmarks on various platforms focusing on Hive and Hive-like SQL interfaces The PUMA project 1 provides MapReduce benchmarks on various algorithmic primitives such as sort and join In addition there is a body of work on optimizing MapReduce for certain tasks For example  15  study parallel K-means and  3  optimize K-means on MapReduce Query Platforms on MapReduce Certain platforms place a data warehousing solution on top of MapReduce that stores the data in the form of tables so that a user can query it using a restricted query language while taking advantage of the parallelization of the MapReduce framework A leading example is Hive  13  and its query language Hi v eQL which are built on top of Hadoop A more recent effort in this direction is Spark 2  Despite the growth of such platforms in the recent past a signiìcant proportion of open source big data still happens on Hadoop MapReduce Algorithmic Research on CRM Our work does not focus on algorithms for CRM In this area perhaps the most research effort in CRM has come from eCommerce in modeling users and recommending items to them Notable works include Amazon.comês item-item collaborative ltering  9  se gmentation of customers  14  and clustering of click-streams  12  Our work signiìcantly differs from these works since these works represent algorithmic research but do not discuss the big data pipelines required in order to scale these algorithms to over a hundred million users This is the gap in literature that our work tries to address 3 Two Parts of Big Data Pipeline We will assume background on Apache Kafka 3  Apache Storm 4  Apache HBase\(an open-source distributed versioned 1 https://engineering.purdue.edu puma/datasets.htm 2 https://spark.apache.org Although Spark does provide a query language it can be used to replace both streaming data like Storm as well as MapReduce and is strictly faster than MapReduce across multiple benchmarks It is fast gaining in adoption and there is speculation that it may replace Hadoop MapReduce as the de facto batch computation engine in the future 3 http://kafka.apache.org 4 https://storm.apache.org non-relational database modeled after Googleês Bigtable  2   and Hadoop MapReduce The study in this posted is conducted at a multi-billion dollar e-commerce companyÑGroupon It comprised tens of billions of customer activities per month aggregated over 100M customers of Groupon over thousands of products An activity refers to a gamut of customer interactions product views clicks purchases etc collected across multiple touch points website mobile application curated product catalogs sent over email etc Given the massive scale of the data above we built two big data pipelines The rst is for collecting and pre-processing such large amount of activities and the other one is for processing and mining the resulting data Broadly speaking the rst pipeline tackles the problem of big streaming data while the second addresses big batch data 3.1 Data Gathering and Joining Pipeline The rst part of our big data pipeline as shown in Fig 1 left of dashed vertical line is our data collection pipeline Here two different types of data with very different properties are gathered together The components of this part of our data pipeline are HDFS Kafka Apache Storm and HBase 1 First product attributes are collected from hadoop distributed le system HDFS In product attributes information about products is stored including product prices product locations product taxonomies 5 etc Some products such as services have location constraints associated with them These are also stored in HDFS 2 Second customer activities are collected from log les parsed into a canonicalized format in Kafka and are written into HBase via Storm For example when customers are browsing and using the product catalogs on Grouponês website they view click and/or make purchases In each case the customer ID product ID activity type view click or purchase and activity time are recorded After collecting data of deal attributes and customer activities a mapreduce job is then used to combine each customer activity and the related deal attributes by product IDs 3.2 Data Processing and Mining Pipeline The second part of our big data pipeline is the data processing analysis and mining pipeline shown in Fig 1 right of dashed vertical line This comprises several Hadoop MapReduce tasks outlined below These also use the Mahout libraries  First an aggregation job is used to integrate customer activities by customer ID for a uniìed view of customer activities over time In this step to reduce sparsity caused by the large amount of products we aggregate counts of activities on product attributes independently Besides time decays are added to the activity count to account for the impact of time Moreover these counts are normalized by product positions on the website to remove the biases caused by advertisement or product positions 5 Product taxonomy is a human deìned and tree structured category dictionary 2937 


 Task Library Mappers Reducers Time Taken Reformat data to vectors and normalize Custom using Hadoop MapReduce 1146 0 5mins 31sec K-Means with K  100 for 20M 150-dimensional vectors Mahout MapReduce K-Means 1146 1 Initialization 1mins 45sec Iteration 1 1mins 29sec Iteration 2 1mins 27sec Iteration 3 1mins 27sec Iteration 4 1mins 27sec Iteration 5 1mins 26sec Iteration 6 1mins 29sec Final Clustering 1mins 27sec Extract frequent itemsets from each cluster MapReduce Apriori 1146 100 24mins 47sec Estimate per-strata sample size for stratiìed sampling Custom using Hadoop MapReduce 160 1 3mins 31sec Reservoir sampling from each strata Custom using Hadoop MapReduce 160 20 2mins 34sec TABLE 1 Big data tasks performed in batch mode using Hadoop MapReduce and Mahout with default settings on a one-time dump of activities of roughly 20M users taken over a period of 6 months in 2014 Number of mappers and reducers are shown Both numbers are chosen by default based on DFS block sizes Time required for each task is shown  Second a MapReduce K-Means  15  i s applied to the aggregated data to cluster customers into K groups In the clustering aggregated activities on deal attributes are viewed as features of customers The cosine distance is used as the distance metric of the MapReduce K-Means  Third we run a MapReduce job on each group to extract frequent itemsets In this task we implement MapReduce Apriori 1  8  In each group the learned frequent itemset forms a signature that is used by multiple CRM applications  Finally across all the groups we perform a stratiìed reservoir sampling that yields 1M customer IDs corresponding to highly active customers that are also typical for their group This set is used for further data science using small-scale analysis tools such as R and Python This smaller set of users allows us to rapidly protoype research ideas To summarize Section 3  raw customer events are consumed from Kafka and processed into a canonicalized format The uniìed stream is then written into HBase via storm In this framework Kafka acts as a platform to handle large volume of real time logs a.k.a raw customer events and Storm acts as a reliable data transmission pipeline The Storm/Kafka pipeline is used for real-time data stream processing for analytics For instance within 15 mins of a customer activity occurring it is available for processing to our data mining pipeline because Storm continuously processes the data stream from Kafka and puts it into HBase In contrast Hadoop MapReduce is purely an ofîine engine that we use to process analyse and mine large quantities of static data 4 Performance Benchmarks The performance benchmarks using default settings are provided in Table 1 for activities from 20M customers Each customer is represented in 150-dimensional space The Hadoop computing cluster has 18 nodes and it can execute up to 126 mapper tasks and 54 reducer tasks at any given time Its heap memory size is 19.39 GB At the time of obtaining the benchmarks of Table 1  other tasks besides those in Table 1  running on the cluster amounted to less than 15 of its capacity so that we may assume that almost the entire cluster was available for the tasks reported in this paper 5 Conclusion In this paper we have attempted to ll a gap in literature by providing details of a big data pipeline using open source components that serves CRM at a large corporation with a user base exceeding 100M We provide benchmarks for processing activities from 20M customers using default settings in Hadoop MapReduce for various common tasks in big data analysis References  Rak esh Agra w a l and Ramakrishnan Srikant F ast algorithms for mining association rules in large databases Proc VLDBê94   F a y Chang Jef fre y Dean Sanjay Ghema w at W ilson C Hsieh Deborah A Wallach Mike Burrows Tushar Chandra Andrew Fikes and Robert E Gruber Bigtable A distributed storage system for structured data In Proc OSDI 06   Xiaoli Cui Pingfei Zhu Xin Y ang K eqiu Li and Changqing Ji Optimized big data k-means clustering usingmapreduce The Journal of Supercomputing  70\(3 2014  Gartner  Mark et share analysis Customer relationship management software worldwide Gartner White Paper  Feb 2014  IBM From social media to social crm What customers w ant IBM White Paper  Feb 2011  Da wei Jiang Beng Chin Ooi Lei Shi and Sai W u  The performance of mapreduce An in-depth study Proc VLDB Endow  3\(1-2 September 2010  Berk ele y AMP Lab  Big data benchmarks available online at https://amplab.cs.berkeley.edu/benchmark  last retrieved July 2015  Ning Li Li Zeng Qing He and Zhongzhi Shi P arallel implementation of apriori algorithm based on mapreduce Proc SNPDê12   Gre g Linden Brent Smith and Jeremy Y ork Amazon.com recommendations Item-to-item collaborative ltering IEEE Internet Computing  2003  Link edIn 100 open source big data architecture papers for data professionals available online at http://tinyurl.com/pkkcuc7  last retrieved July 2015  Oracle Kno wledge-infused customer relationship management A g amechanging investment for customer support Oracle White Paper Nov 2011  Qiang Su and Lu Chen A m ethod for disco v ering clusters of ecommerce interest patterns using click-stream data Electronic Commerce Research and Applications  2015  Ashish Thusoo Jo ydeep Sen Sarma Namit Jain Zheng Shao Prasad Chakka Suresh Anthony Hao Liu Pete Wyckoff and Raghotham Murthy Hive A warehousing solution over a map-reduce framework Proc VLDB Endow  2\(2 August 2009  Roung-Shiunn W u and Po-Hsuan Chou Customer se gmentation of multiple category data in e-commerce using a soft-clustering approach Electronic Commerce Research and Applications  2011  W eizhong Zhao Huif ang Ma and Qing He P arallel k-means clustering based on mapreduce Proc CloudComê09  2938 


     AF Rx hs S Rx ht T Rx N Rx  9 Where hs nd ht e threshold limitation functions, they are shown in Eq. 10 and Eq. 11  min min    0 SRx SRx S hs S Rx SRx S      10  min min    0 TRx TRx T ht T Rx TRx T      11 The antibody affinity is a measure of similarity degree of two rules. Two antibodies Rx v and Rx w represented by two binary code strings has an antibody affinity AF  Rx v  Rx w  shown in Eq. 12     1 1  M vw vj wj j AF Rx Rx Rx Rx M     12 Where Rx v,j and Rx w,j are the values on the j coding  position of Rx v and Rx w respectively;  is an XOR operator C  Clone probability calculation In biological immune systems, an antibody have a higher antigen affinity can be cloned to next generation with a bigger probability. At the same time, the effect of competitive exclusion restrains the antibodies with high density, so that the diversity is not easy to be lost in the antibody population. The density of an antibody Rx v can be calculated by antibody affinities, as shown in Eq. 13  1 11  1  A AA S vk kkv v SS vk vkkv AF Rx Rx D\(Rx  AF Rx Rx 012 012     13 Where S A is the scale of antibody population Both the antigen affinity and density of an antibody Rx are considered, the clone probability can be calculated in Eq. 14      C P Rx AF Rx D Rx  14 The clone operation is executed by this probability, so that the antibodies with high antigen affinities are stimulated in the population, as well as the diversity is maintained D  Algorithm implementation process The algorithm implementation process of AIARM is described as follows   Step1. Initialization: initialize the antibody population with a scale of S A and randomly sample S M  antibodies to build the antibody memory   Step2. Antibody evaluation and rule extraction calculate the antigen affinity AF  Rx and density D  Rx  of each Rx so that the P C  Rx n be gotten. After removing duplicate rules, all rules satisfied AF  Rx  0 would be stored in rule memory   Step3. Antibody memory updating: arrange all the antibodies in descending order of clone probabilities use the top S M antibodies to rebuild antibody memory   Step4. Iterative termination determination: if the termination condition is satisfied, quit the iteration process. Otherwise, go to Step 5   Step5. Antibody population updating: implement the clone selection of antibodies by P C  Rx If an antibody is not selected to be clone, it mutates with a predefined probability. Go to Step 2 AIARM transfers the information of better antibodies directly to next generation by clone operation, and realize the global random search by mutation operation. The antibody memory eliminates the degeneration of antibody population and the rule memory save all interesting rules that were found in the iterative search E  Non-frequent item hyper set detection When AIARM evaluates an antibody, the database needs to be scaned once. Since new rules are generated by random search in AIARM, there will be a lot of rules combined by nonfrequent items and AF  Rx 0. To decrease the database scan number, the non-frequent hyper set detection is designed as a modified method of AIARM According to the basic logic law in Apriori that a high order frequent item is combined by some lower order frequent items it can be known that the support value of a rule Rx which includes a non-frequent item in item-set must be lower than S min then the antigen affinity AF  Rx Because an antibody is coded in the form of binary string which represents a combination of concepts, an antibody is exactly an item-set and the relationship of two antibodies can be described by subset or hyper-set For an antibody Rx whose consist item codes are not all 0 if any concept item whose value is 1  in Rx is also values as 1 in another antibody Ry then Rx is defined as a hyper set of Ry  while Ry is a sub set of Rx The relationship can be denoted in Eq. 15  R yRx  15 If there is a relationship Ry Rx of two antibodies, in support calculating they must fulfill the inequality     SRy S Rx  16 Furthermore, if Rx is  a non-frequent item set S  Rx  S min  itís sub set Ry must be a non-frequent item set, and the antigen affinity of Ry must be zero A non-frequent item hyper set memory can be built to save the common hyper sets of antibodies that are non-frequent item 2015 IEEE 11th International Colloquium on Si g nal Processin g its Applications \(CSPA2015 y sia 18 


sets. Thus for a new antibody Rx need to be evaluated, the nonfrequent item hyper set detection can be implemented before antigen affinity calculation. If Rx was a sub set of a record in the memory, then AF  Rx 0, so that the database scan in antigen affinity calculation could be canceled. Only the antibodies that are confirmed as not sub sets need to calculate the antigen affinities. If a new non-frequent item set was found during the antigen affinity calculation, a common hyper set extraction method could be used to update the memory The non-frequent item hyper set detection is realized in binary coding space. With small computational cost, this method could decrease the database scan number significantly and improve the efficiency of AIARM V  C ALCULATION E XAMPLE A NALYSIS  The abalone database in UCI machine learning repository 13 is s e lec te d as a c a l c u l ati on  ex am ple to tes t t h e  performance of AIARM. In this database, there are 4177 records. Every record has 8 quantitative attributes and one classification information of 3 classes. By the means of cloud transformation, 8 quantitative attributes can be converted to 21 cloud concepts. Moreover, when both the entropy and hyperentropy are equal to zero, a cloud can represent a qualitative concept of hard partition. Hence the classification information can be expressed by 3 cloud concepts. Thus every record can be transformed to certainty degrees of 24 cloud concepts, and every association rule consists of a combination of 24 cloud concepts The sex classification attributes of abalone database are selected as task attributes. Within constrains of different S min  and T min Apriori and AIARM are used to mine the rules in abalone database. The numbers of rules mined by two algorithms are compared in TABLE I. The numbers of rules in the column of AIARM are average results of 20 times operation TABLE I  C OMPARISON OF M INED R ULES   N UMBER  Threshold Number of rules support confidence Apriori AIARM 10% 50 58 57.4 10% 60 34 33.8 15% 50 20 20 15% 60 9 9 20% 50 8 8 20% 60 2 2 Apriori uses the stepwise traversal search method to generate frequent item-set and it can get all association rules without any missing. In TABLE I, it can be seen that AIARM can get most of the rules by a random search The database scan number of Apriori and AIARM are shown respectively in TABLE II. In the low support threshold conditions, the number of AIARM is significantly less than the number of Apriori. Conversely, in the high support threshold conditions, the number of AIARM is bigger than the number of Apriori. The database scan number of AIARM is determined by the feature of random search, a certain amount of database scanning for antibody population evaluating can not be avoided TABLE II  C OMPARISON OF D ATABASE S CAN N UMBER  Threshold Database scan number support confidence Apriori AIARM 10% 50 18348 7064 10% 60 12514 6352 15% 50 10152 5493 15% 60 6257 5628 20% 50 5264 6092 20% 60 2788 6533 However, it should be noticed that these numbers of AIARM keep relatively stable in various constrains, and all iterative times are less than 100. AIARM has strong robustness of computational cost, so it can be used in an interactive association rule mining whose threshold constrains vary frequently. AIARM will neither waste a lot of time in frequent item-set generating as Apriori in low support threshold condition, nor construct decision trees repeatedly as FP-growth VI  C ONCLUSIONS  In this paper different types of attributes are converted into an uncertain concept space by cloud transformation, thus any quantitative or qualitative attribute of a record can be represented by a certainty degree set of cloud concepts that describe the attribute. The correlations among these cloud concepts are association rules. An optimization object function for mining these rules is constructed and an artificial immune algorithm AIARM  is proposed. In this algorithm, frequent item-sets are generated by random searches and the rules can be extracted. To improve the efficiency, a non-frequent item hyper set detection method is designed to reduce the times of database scanning. The result of numeric experiments shows that AIARM could accomplish the task of association rule mining in different threshold constrains with a relatively stable computational cost A CKNOWLEDGMENT  This research was supported by National Natural Science Foundation of China, under Grant NSFC51375368 R EFERENCES  1  R. Agrawal, T. Imielinske, A. Swami, ìMining association rules between sets of items in large databases,î in Proc of the ACM SIGMOD Int 'l Conf on the Management of Data, Washington USA, 1993, pp. 207-216 2  R. Agrawal, R. Srikant, ìFast algorithms for mining association rules in large databases,î Proceedings of the 20 th international conference on very large data bases, Santiago, Chile, 1994 pp.487-499 3  J. Dixit, A. Choubey, ìA survey of various association rule mining approaches,î International Journal of Advanced Research in Computer Science and Software Engineering, vol 4, no. 3, 2014, pp. 651-655 2015 IEEE 11th International Colloquium on Si g nal Processin g its Applications \(CSPA2015 y sia 19 


4  S. Kotsiantis, D. Kanellopoulos, "Association rules mining: A recent overview", International Transactions on Computer Science and Engineering, vol.32, no.1, 2006, pp.71-82 5  A. Bargiela, W. Peddrycz, ìToward a theory of granular computing for human-centered information processing,î IEEE Trans on Fuzzy Systems, Vol.16, no. 2, 2008, pp.320-330 6  Y.Y. Yao, ìGranular computing: past, present and future,î IEEE International Conference on Granular Computing, Hangzhou China, 2008, pp.80-85 7  D.Y. Li, Y. Du, ìArtificial Intelligence with Uncertainty Beijing: National Defend Industry Press, 2005 8  D.Y. Li, C.Y. Liu, W.Y. Gan, ìA new cognitive model: Cloud model,î International Journal of Intelligent Systems, vol. 24, no 3, 2009, pp. 357ñ375 9  L. Wang, J. Pan, L.C. Jiao, ìThe Immune Algorithm,î Acta Electronica Sinca, vol. 28, no. 7, 2000, pp. 74-78 10  T. Zeng, C. Tang, Y. Xiang, et al., ìA model of immune gene expression programming for rule mining,î Journal of Universal Computer Science, vol. 13, no. 10, 2007, pp.1484-1497 11  Mining fuzzy rules using an artificial immune system with fuzzy partition learning,î Applied Soft Computing, vol. 11, no 2, 2011, pp. 1965-1974 12  D.Y. Li, C.Y. Liu, ìStudy on the universality of the normal cloud model,î Chinese Engineering Science, vol. 6, 2004, pp 28-34 13  C.L. Blake, C.J. Merz, ìUCI repository of machine learning databases,î http://www.ics.uci.edu.html  2015 IEEE 11th International Colloquium on Si g nal Processin g its Applications \(CSPA2015 y sia 20 


E Impacted Coef\002cient of the Additional Itemset De\002nition 13 The impacted coef\002cient of an additional itemset is to describe how effective this itemset is to manufacture the derivative itemset from underlying itemset denoted as AU G 001 X j X 0   de\002ned as AU G 001 X j X 0   r C 2 001 X j X 0   W 2 001 X j X 0  2 14 This equation averages the value of C 001 X  in Equation 9 and W 001 X  in Equation 11 Here we use the Quadratic Mean QM also known as Root-Mean Square to measure the signi\002cance of the itemset 001 X in terms of both utility and relationship perspectives because it represents the sample standard deviation of the difference between W and C  thus the result cannot be affected heavily by the smaller value It is easy to prove QM 2  X    X  2  033 2  X  15 Here X and 033  X  stand for the arithmetic mean and the standard deviation of W and C  We also tried another measurement by Harmonic Mean HM as a baseline which is proven to be less effective in our experiments For a speci\002c X 0  for each itemset 001 X to be considered the higher AUG means this itemset is likely to impel the underlying itemset into higher utility itemset On the contrary the lower the AUG is the lower utility that derivative itemset might be As all the AUG would be calculated only the largest AUG value itemset will be chosen F The CUARM Algorithm In this section an algorithm named Combined UtilityAssociation Rule Mining CUARM is proposed to discover all the actionable combined utility-association rules At the beginning of the algorithm it picks all UIs as candidates For each UI all the combined patterns are discovered with their AUGs which form a combined pattern cluster as in Equation 2 and only the most effective pattern would be selected In addition if two patterns are coupled with utility increment and decrement a combined pattern pair forms The input is the transaction database including all transactions with the utility of each item and the output is the combined pattern pairs their underlying itemset and the corresponding utilities In line 1 we prepare all the itemsets with their utilities in the alphabetical order and the length of longest itemset In lines 2-5 we start with each of the UIs named itemset 0 with its utility U 0  In lines 6-11 the DIs are ready and we calculate their AUGs In line 12-13 we select the pattern with max AUG values as CUAR V E XPERIMENTS In this section we conduct intensive experiments to evaluate the proposed methods Our experiments were run on a PC with a 2.30 GHz Intel Core 16 gigabyte memory CUARM is implemented in Java Two real datasets and two synthetic datasets are used for the experiments The real Algorithm 1 CUARM Input  Transaction database D  including the utility U  X  of each item in D Output  All actionable combined utility-association rules 1 Get all itemsets utilities via UG-Tree  2 Get the length of longest itemset lmax  3 for len  1 len  lmax len do 4 for Itemset whose length is equal to len do 5 Get itemset 0 with U 0 itemset-utility 6 for itemset.length  len do 7 Check inclusive and utility changes 8 Get itemset 1 with U 1  9 Calculate C 10 Scan the database get W 11 Calculate AUG 12 Selected max one 13 Present this utility-association rule TABLE VIII C HARACTERISTICS OF DATASETS Dataset Number of Transactions Number of Items Average Length Retail 2 88162 16470 10.3 Chainstore 3 1112949 46086 7.3 t20i6d100k 100000 658 13.7 c20d10k 10000 187 13 datasets are Retail 2 and Chainstore 3  and the synthetic datasets are t20i6d100k and c20d10k  The parameters of the datasets are listed in Table VIII A Comparison of Two Functions for Calculating Impacted Coef\002cient Here we propose two functions for calculating the impacted coef\002cient One is the quadratic mean QM which is adopted in this paper the other function is the harmonic mean HM which is proved to be less accurate in the experiments Those itemsets with a good coef\002cient measurement should be associated with both high frequency and high utility growth we thus can separate the database randomly If the output itemsets discovered in each sub-database are stable we can assume that this measurement is suitable The experiments were conducted on the Retail dataset for the sake of simply examining the QM function The top 100 experimental results are selected and shown in Fig 4 The 002gure on the left shows the comparison between UP-Growth and QM while the 002gure on the right shows the result of QM and HM on C 001 X  and W 001 X   The database is split into 10 parts randomly The 002rst part contains 10 transactions in the database and each later part contains 10 more transactions than the former part such that the second part contains 20 and the last part is 100 The X axis is the k th  1 024 k 024 10  part of the database and the Y axis is the match ratio which means the ratio of the exact patterns found in the k th part 2 http://www.philippe-fournier-viger.com/spmf/index.php?link=datasets.php 3 http://cucis.ece.northwestern.edu/projects/DMS/MineBench.html 


Fig 4 The Comparison of HM QM and UP-Growth matching with the  k  1 th part As seen from the 002gures the QM method outperforms both HM and UP-Growth B Experimental Evaluation of CUARM Next we present the experimental results of comparing derivative itemsets with the traditional HUIs FIs and UIs Underlying Itemsets respectively The statistic values of each dataset are shown in Table VIII The experiment is conducted as follows Firstly we collect all the utility itemsets with their utilities and frequencies in each dataset respectively Secondly we also collect all the frequent itemsets with their frequency and utility Then we calculate the utilities of the frequent itemsets frequencies of the HUIs and both utilities and frequencies of the derivative itemsets At last we plot the frequency of itemsets discovered via UP-Growth and CUARM the utility of itemsets discovered by FP-Growth and CUARM and the utility changes from each underlying itemset to derivative itemset as shown in Fig 5 Such exhibition is made for the comparison of our algorithm with FIM and HUI to demonstrate the UtilityAssociation Rules we discovered have both high utility and high frequency Here all the frequent and utility itemsets we compare with contain at least two items because the derivative itemsets our algorithm discover contain no less than two items Experiments on Real Datasets We 002rst present the outputs of dataset Retail in Fig 5\(a and Fig 5\(b Top 50 patterns of each algorithm are selected for experiments By analyzing the frequencies and utilities of patterns many of them are without much difference in both two experiments which means the association rules we found via traditional AR algorithms are also high utility-association rules via our method In addition such rules are also with high utilities This explains why some parts of the curves overlap In addition customers prefer to buy a few products at one time that is most of FIs and HUIs contain only one or two items which also explains the observations In datasets Chainstore  the differences are much clearer because customers usually prefer a variety of products in each of transactions and high utility itemsets are always low in frequency while highly frequent itemsets are with low utility For example in Fig 5\(d the CUARM performs much better than that of FP-Growth while in Fig 5\(e even at some points the performance is not so good the global performance is much better To sum up we can assert that the performance of our algorithm CUARM is much better than the others Experiments on Synthetic Datasets Experimental results on synthetic datasets t20i6d100k and c20d10k are shown in Fig 5\(g Fig 5\(h Fig 5\(j and Fig 5\(k The results are much clearer than those from real datasets because the items included are much neat and with orderliness For most of the patterns discovered via CUARM the frequencies are much higher than those traditional high utility itemsets At the same time most Utility-associated rules are also with much higher i.e twice the utility than traditional association rules especially in Fig 5\(h from dataset t20i6d100k  C Evaluation of the Utility Increment We demonstrate the utility increment in a graphic way to show how the utility increases from underlying itemset to derivative itemset The utility increment is valued based on the same datasets as above points are ordered by the utility of derivative itemsets The performance of our algorithm varies from one dataset to another The performance in chainstore is much better than that in retail because the transaction time in chainstore is 12 times more than that in retail while the item types are only twice more However in the synthetic datasets the performance is better In conclusion for each dataset the performance is different but the utility actually increases D Utility Variation Experiments Conclusion Based on the above datasets and experimental results a table is used to demonstrate the conclusion that comes from our experiments and shown as Table IX This table describes the number of itemsets whose utilities are increase or decrease with given threshold Also two kinds of utility incremental forms are listed One is the utility of derivative itemset is higher than both the utilities of underlying itemset and additional itemset which is denoted as FA the other is that the utility of derivative itemset is higher than only the utility of underlying itemset which is denoted as FB As for each underlying itemset only one derivative itemset would be discovered some FA and FB might be ignored For the utility decrement itemsets whose utilities are only lower than the underlying itemsets would not be considered in this table because these itemsets can also be regarded as FBs when the underlying itemsets and additional itemsets exchange TABLE IX U TILITY V ARIATION C ONCLUSION Dataset Min Sup N.DI R.U N.FA N.FB N.DecI Retail 0.01 89 20.3 50.4 28 22 39 0.008 135 18.6 50.4 37 46 52 0.002 1667 8.4 50.4 473 769 425 Chainstore 0.002 79 4.6 207.2 7 19 53 t20i6d100k 0.017 33 25.7 78.5 8 11 14 0.015 79 22.8 78.5 24 19 36 0.012 383 1.8 78.5 112 137 134 c20d10k 0.05 120 19.7 150.9 28 48 44 In Table IX M in Sup in the minimum support for mining itemsets N:DI is the number of derivative itemsets discovered 


figures1//retail_f-eps-converted-to.pdf a retail figures1//retail_u-eps-converted-to.pdf b retail figures1//retail_i-eps-converted-to.pdf c retail figures1//ds7_f-eps-converted-to.pdf d chainstore figures1//ds7_u-eps-converted-to.pdf e chainstore figures1//ds7_i-eps-converted-to.pdf f chainstore figures1//t20i6d100k_f-eps-converted-to.pdf g t20i6d100k figures1//t20i6d100k_u-eps-converted-to.pdf h t20i6d100k figures1//t20i6d100k_i-eps-converted-to.pdf i t20i6d100k figures1//c20d10k_f-eps-converted-to.pdf j c20d10k figures1//c20d10k_u-eps-converted-to.pdf k c20d10k figures1//c20d10k_i-eps-converted-to.pdf l c20d10k Fig 5 Experiments for FP UP and CUARM with the threshold M in Sup  R:U in the utility incremental rate from underlying itemset to derivative itemset N:F A is the number of FA itemsets N:F B is the number of FB itemsets and N:DecI is the number of decremental itemsets VI C ONCLUSIONS AND F UTURE W ORK Traditional high utility itemset mining methods have the weak point that if the minimum utility threshold is set too high the itemsets discovered might contain unrepresentative items while if the threshold is set too low too many redundant itemsets will be found On the other hand traditional association rule mining ignores the utility hidden among the items This work proposes a novel pattern select method from two aspects One is the co-occurrence of two underlying and additional itemsets another is the utility increment from underlying itemset to derivative itemset It is an effective approach for identifying actionable combined utility itemsets in which for different items only one itemset will be selected with the highest association-utility growth which caters for both high association and high utility Thus only the most effectively impacted itemsets will be presented The results demonstrate that our method can discover patterns that are composed of different item combinations of both utility increment and high representativeness For the future work we may 002nd some more interesting pattern selection method For example there exists a dependent relationship between two itemsets A and B which means A might appear frequently alone or with other items but for most time B appears together with A VII A CKNOWLEDGMENTS This work is sponsored in part by Australian Research Council Discovery Grant P130102691 R EFERENCES  R Agra w al R Srikant 1994 F ast Algorithms for Mining Association Rules in Proc of the 20th Int'l Conf on Very Large Data Bases pp.487-499 Santiago Chile  C.F  Ahmed S.K T anbeer  B.-S Jeong and Y K Lee 2009 Ef 002cient Tree Structures for High utility Pattern Mining in Incremental Databases in Proc of IEEE Transactions on Knowledge and Data Engineering Vol 21 Issue 12 pp 1708-1721  L Cao Y  Zhao C Zhang 2008 Mining Impact-T ar geted Acti vity Patterns in Imbalanced Data IEEE Trans on Knowledge and Data Engineering 20\(8 1053-1066  L Cao P  S Y u C Zhang and Y  Zhao 2010 Domain Dri v en Data Mining Springer  L Cao 2013 Combined mining Analyzing object and pattern relations for discovering and constructing complex yet actionable patterns Wiley Interdisc Rew Data Mining and Knowledge Discovery 3\(2 140-155  J Han J Pei and Y  Y in 2000 Mining Frequent P atterns without Candidate Generation in Proc of the ACM-SIGMOD Int'l Conf on Management of Data pp 1-12 Dallas TX USA  J Han H Cheng D Xin and X Y an 2007 Frequent P attern Mining Current Status and Future Directions DMKD 15 55-86  M S Khan M Muyeba and F  Coenen 2008  A W eighted Utility Framework for Mining Association Rules in Proc of the Second UKSIM European Symposium on Computer Modeling and Simulation pp 87-92  X Lin Q Zhu F  Li Z Geng and S Shi 2010 S Share Strate gy for Utility Frequent Patterns Mining in Proc of the Seventh International Conference on Fuzzy Systems and Knowledge Discovery pp 14281432 Yantai China  J Liu K W ang and B C M Fung 2012 Direct Disco v ery of High Utility Itemsets without Candidate Generation in Proc of the IEEE Int'l Conf on Data Mining ICDM  M Liu and J Qu 2012 Mining High Utility Itemsets without Candidate Generation in Proc Of the ACM Int'l Conf on Information and Knowledge Management CIKM pp 55-64 


 Y  Liu W  Liao and A C houdhary  2005  A T w o-Phase Algorithm for Fast Discovery of High Utility Itemsets in Proc of PAKDD pp 689-695  S Shankar  T  Purusothaman S Kannimuthu and P  K V ishnu 2010 A Novel Utility and Frequency Based Itemset Mining Approach for Improving CRM in Retain Business International Journal of Computer Applications Volume 1 No 18 pp 87-94  V  S Tseng C.-W  W u B.-E Shie and P  S Y u 2010 UP-Gro wth An Ef\002cient Algorithm for High Utility Itemset Mining in Proc of Int'l Conf on ACM-SIGMOD pp.253-262  B V o B Le and J Jung 2012  A T ree-Based Approach for Mining Frequent Weighted Utility Itemsets in Proc of ICCCI 2012 Part I LNAI 7653 pp 114-123  C W u Y  Lin P  S Y u and V  S Tseng 2013 Mining High Utility Episodes in Complex Event Sequences in Proc of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD pp 536-544  C W u P  Philippe P  S Y u and V  S Tseng 2011 Ef 002cient Mining of a Concise and Lossless Representation of High Utility Itemsets in Proc of IEEE Int'l Conf on Data Mining ICDM pp.824-833  C W u B Shie V  S Tseng and P  S Y u 2012 Mining top-K high utility itemsets in Proc of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD pp 78-86  H Y ao H J Hamilt on and C J Butz 2004  A foundati onal approach to mining itemset utilities from databases in Proc of the 4th SIAM Int'l Conf on Data Mining Florida USA  J S Y eh Y  Li and C Cheng 2007 T w o-Phase Algorithms for a Novel Utility-Frequent Mining Model in Proc of PAKDD Workshop LNAI 4819 pp 433-444  J Y in Z Zheng and L Cao 2012 USpan An Ef 002cient Algorithm for Mining High Utility Sequential Patterns in Proc of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD pp 660-668  H Zhang et al 2008 Combined Association Rules Mining in Proc of PAKDD08 pp.1069-1074  Q Zhao and S Bho wmick 2003  Association Rules Mining a Surv e y Journal of Nanyang Technological University 2003116  Y  Zhao et al 2007 Mining for Combined Association Rules on Multiple Datasets in Proc of the KDD 2007 Workshop on Domain Driven Data Mining San Jose CA USA pp 18-23 


