Data-îow Testing in the Large Roberto Paulo Andrioli de Araujo and Marcos Lordello Chaim Software Analysis and Experimentation Group  SAEG School of Arts Sciences and Humanities University of Sao Paulo 
  
roberto.araujo,chaim usp.br 
Abstract 
Data-îow DF testing was introduced more than thirty years ago aiming at extensively evaluating a program structure It requires tests that traverse a path in which the deìnition of a variable and its subsequent use i.e a deìnitionuse association dua is exercised While control-îow testing tools have being able to tackle big systemsÑlarge and long running programs DF testing tools have failed to do so This situation 
is in part due to the costs associated with tracking duas at run-time Recently an algorithm called Bitwise Algorithm BA which uses bit vectors and bitwise operations for tracking intraprocedural duas at run-time was proposed This paper presents the implementation of BA for programs compiled into bytecodes Previous approaches were able to deal with small to medium size programs with high penalties in terms of execution and memory Our experimental results show that by using BA we are able to tackle large systems with more than 200 KLOCs and 300K required duas Furthermore for several programs the execution penalty was comparable with that imposed by a popular controlîow testing tool Keywords Structural testing Data-îow testing coverage Pro 
 
gram instrumentation Run-time environments Bytecode testing 
I I NTRODUCTION An important part of current software industry known as internet-based companies works in perpetual development mode in which new features are made available to users on a daily basis As a result the systems are continuously growing and being deployed Feitelson et al describe F acebook development and deployment process in which new code is released at high rate several times a working day To develop quality software at such a fast rate these companies rely in part on extensive automated testing Bro wser based automated testing and xUnit frameworks are used to execute thousands of tests In addition tools automatically collect code coverage data to assess the code produced by 
engineers In this context of continuous deploying coverage tools are only useful if able to produce coverage data at the same fast rate Code coverage consists in determining the structural testing entities covered by test cases Structural testing entities can be divided into two groups  
and testing In the rst one the testing entities are derived from the ow graph obtained from a program Examples of control-îow testing entities are  and  being a node every set of statements executed in sequence an edge the possible transfer 
controldata-îow node edge path 
of control between nodes and a path corresponds to a sequence of nodes Data-îow testing in turn involves the development of tests which exercise every value assigned to a variable and its subsequent references uses occurring either in a computation or in a predicate These entities are called 
dua To determine the testing entities covered by a test case the program object or source code should be instrumented Program instrumentation inserts additional code in a program to collect coverage information during its execution Instrumentation makes programs run slower and causes greater memory consumption Furthermore inefìcient program instrumentation might turn effective software testing techniques into infeasible 
deìnition-use associations 
techniques for industrial use Control-îow CF testing supporting tools have a widespread use in industry Although the tools performance may vary depending on the characteristics of the systems commercial e.g Clover 1  and open-source e.g Cobertura 2  JaCoCo 3  tools have been utilized to assess control coverage of large systems Moir reports that JaCoCo was run against 37,000 Eclipse JDT core tests with an execution overhead of 2 On the other hand despite of having been shown to be an effective testing technique 6 adequate for security assessment and useful to support f ault localization 8 one hardly nds Data-îow DF testing in use at industrial 
settings In part this situation can be explained by the fact that DF supporting tools are not scalable for large systems due to the costs associated with tracking duas at run-time Some DF testing instrumentation techniques 10 were proposed to address the costs associated with monitoring duas However these approaches rely on complex computations and expensive data structures to collect dua coverage Recently a novel algorithm called Bitwise Algorithm BA was proposed to tackle this issue The new algorithm utilizes efìcient bitwise operations and inexpensive data structures to track intra-procedural duas i.e duas occurring within a procedure Simulations show that BA is at least as good as the most efìcient DF instrumentation techniques and that it 
can be up to 100 more ef cient 11 The purpose of this paper is to present the BA implementation for programs compiled into bytecodes Such an implementation is realized in the BA-DUA Bitwise Algorithmpowered Deìnition-Use Association coverage tool BA-DUA follows BA original description with optimizations only related to bitwise operations For example if an operand in a union operation is empty we removed this operation in a subtraction 1 http://www.atlassian.com/software/clover 2 http://cobertura.sourceforge.net 3 http://www.eclemma.org/jacoco 
2014 IEEE International Conference on Software Testing, Verification, and Validation 978-0-7695-5185-2/14 $31.00 © 2014 IEEE DOI 10.1109/ICST.2014.19 81 


 
def = {i päuse={i,length def = {max array cäuse = {max cäuse = {i 3 2 1 6 5 4 def={i,array,length,max päuse={i,length cäuse={i array,max päuse={i array,max päuse={i 
002 002 002 
i k k j k k 
Fig 1 Example program  In control-îow testing the entities are derived from the ow graph of the program The most well known control-îow testing criteria are Fig 2 Annotated ow graph of the example program 
1 1 
   002 003 004      
P G N E s e N s e E n n n n n n n  n n  n i k<j n n E   
operation the logical negation of the subtrahend is carried out at instrumentation time Part of these optimizations were utilized in the simulations presented in The goal w as to assess BA original formulation without further optimizations Our evaluation comprehends two types of penalties imposed by instrumenting programs with BA execution overhead given by the extra time needed to execute the instrumentation code with respect to an uninstrumented program and memory overhead represented by the growth of the object code due to the insertion of BA code The subjects of our experiment consisted of ten programs varying from 672 to 209,482 LOC with a total number of duas from 1220 to 314,369 The test suites of these programs were run without any instrumentation and with BA-DUA and JaCoCo instrumentation Our results indicate that the BA-DUA execution overhead varied from negligible to 172 being the average 38 Interestingly the BA-DUA overhead was over 50 for only two subjects in ten programs Furthermore for several programs the BA-DUA execution penalty was comparable with that imposed by JaCoCo In terms of program growth the BA memory overhead ranges from 33 to 118 being the average 61 Previous approaches for dua monitoring were able to deal only with small to medium size programs with sizeable penalties By using BA-DUA we are able to tackle large systems with more than 200 KLOCs and 300K required duas with fairly moderate overheads Our experimental data suggests that BA allows intra-procedural data-îow testing to be used in a much bigger class of systems The contributions of this paper are summarized below the description of a scalable intra-procedural data-îow testing tool which utilizes the Bitwise Algorithm BA for dua monitoring an evaluation of the penalties caused by BA in a wide class of programs including small medium and large programs a comparison between the penalties imposed by BA and by a control-îow testing tool JaCoCo largely used in industry The remaining of the paper is organized as follows The next section contains the deìnition of the most used controland data-îow testing criteria The related work is discussed in Section III The Bitwise Algorithm BA for dua monitoring is presented in Section IV Section V describes BA-DUA coverage tool The experiment carried out and the results obtained are presented in Section VI In Section VII the results are analyzed Finally Section VIII contains our conclusions and future work II B ACKGROUND In this section we make a concise presentation of structural testing criteria concepts Let be a program mapped into a ow graph where is the set of blocks of statements nodes such that once the rst statement is executed all statements are executed in sequence is the start node is the exit node and is the set of edges    such that  which represents a possible transfer of control between node and node A is a sequence of nodes        where  such that    Consider the program presented in Figure 1 obtained from  which determines the maximum element in an array of integers The lines of code and nodes associated with the statements of the program are indicated in Figure 1 Figure 2 contains the ow graph obtained from the example program Line Node Statement int max\(int array int length 1 1 1 inti=0 2 1 int max  3 2 while\(i length 3 4 3  max 5 4 max  6 5 i=i+1 5 7 6 return max 6 and  A test set satisìes the all nodes edges criterion if it includes test cases that traverse at least once every node edge of the program The nodes edges traversed by a test set are said to be by it Table I presents the entities required by all nodes and all edges criteria for the max program Data-îow testing requires that selected test cases exercise paths in a program between every point a value is assigned to a variable and its subsequent references When a variable receives a new value it is said that a has occurred a of a variable happens when its value is referred to A distinction is made between a variable referred to compute a value and to compute a predicate When referred to in a predicate computation it is called a and is associated 
max 
   
path all nodes all edges covered deìnition use p-use 
82 


with edges otherwise it is called a when associated with nodes A path with respect to wrt a variable is a path where is not redeìned in any node in the path except possibly in the rst and last ones Data-îow testing criteria in general require that duas be covered The triple     called c-use dua represents a data-îow testing requirement involving a deìnition in node and a c-use in node of variable such that there is a deìnition-clear path wrt from to  Likewise the triple      called p-use dua represents the association between a deìnition and a p-use of a variable  In this case a deìnition-clear path      wrt should exist The all uses data-îow testing criterion requires the set of paths executed by the test cases of a test set to include a deìnition-clear path for each dua    r     of a program  The duas exercised by a test set are said to be by it Table I contains the duas required for program max TABLE I E NTITIES REQUIRED BY THE ALL NODES  ALL EDGES AND ALL USES CRITERIA FOR PROGRAM MAX  All nodes All edges All uses 1 1,2 1 6 max 4 6 max 1 3,4 max 2 2,3 1 3,5 max 4 3,4 max 4 3,5 max 3 2,6 1 4 i 1 5 i 1 2,3 i 4 3,4 1 2,6 i 1 3,4 i 1 3,5 i 5 3,5 5 4 i 5 5 i 5 2,3 i 6 4,5 5 2,6 i 5 3,4 i 5 3,5 i 5,2 1 4 array 1 3,4 array 1,\(3,5 array 1,\(2,3 length 1 2,6 length III R ELATED W ORK Our discussion regarding the related work is two-fold Firstly the techniques to track duas are described The tools developed to support DF testing are compared next The rst approach to track duas was based on FSA In this strate gy  each dua     or      is mapped into an automaton When a node is visited every automaton associated with a dua is checked whether it has reached the nal state In this case the dua is set as covered and the automaton is not checked anymore Ostrand and Weyuker in turn propose to positions to determine precisely which duas were covered Horgan and London resort to a strate gy kno wn as  Tables contain information about which variables are deìned and used in a given block Their strategy keeps track for each variable of the block at which it was deìned When a block that uses a deìned variable is reached the last deìnition of that variable is veriìed and the corresponding dua is recorded in a trace le Misurda et al utilize a strategy to instrument programs to track down duas The idea is to instrument the object code at initial points called seeds The seeds are probes inserted at nodes in which there are deìnitions of variable they are called def probes and one def probe is inserted at if there exists at least one dua      or      When a def probe is reached is recorded as the of and a respective use probe for every is inserted at  A use probe when reached checks whether the last deìnition is  if true it records the coverage of the respective dua After its execution the use probe is removed To track p-use duas e.g      probes are inserted to record the last node visited they are only covered if is the last deìnition of and is the previous node visited Santelices and Harrold proposed an approximate solution to track duas Their idea is that there are edges whose exercise implies the exercise of duas They analyze the code to determine the sets of duas whose exercise is inferable from the exercise of an edge and the set of duas conditionally inferable from the exercise of the edge Conditionally inferred duas mean that they are likely to have been exercised The set of duas not inferable by the exercise of any edge is also determined At run-time this approach tracks edges not duas which requires a less costly instrumentation After the program execution the set of exercised edges is processed to infer the duas actually and conditionally exercised We call this strategy  The same authors proposed a strategy This mechanism is able to precisely track duas at run-time They create a coverage matrix initialized with zeroes where each column corresponds to a use and each cell in a column represents a deìnition for that use Although not all cells correspond to a dua this approach allows quick access of the matrix by using two indexes  a use ID and a deìnition ID At run-time probes track the ID of the last executed deìnition for that variable At each use the instrumenter inserts code that reads the last deìnition ID and uses it as the row index when accessing the corresponding cell in the matrix Since the DF testing inception it was clear that its application was impossible without tool support As a result many toolsÑASSET T A CTIC 13 A T A C 14 POKETOOL JaB UT i 17 Jazz 18 DU A-FORENSICS 10 and DFC de v eloped at academic and research settings to support DF testing To the best of our knowledge the only two tools independently developed to support DF testing are Coverlipse 4 and JMockit 5  Table II summarizes our comparison of the DF testing tools It lists whether a tool tracks interor intra-procedural duas or both the technique it relies on to monitor duas the language it supports and its availability Three tools supports inter-procedural duas DUAFORENSICS JaBUTi and JMockit DUA-FORENSICS requires a single entry point while JaBUTi restricts the level of methods invocations to determine inter-procedural requirements JMockit implements a simpliìed version of the all deìnitions criterion for instance and static non-ìnal elds In general the tools implements the techniques devised by its authors to monitor duas The exceptions are Coverlipse DFC and JMockit for which we could not determine the dua tracking technique utilized though Coverlipse records the traversed path which is then processed to determine the 4 http://coverlipse.sourceforge.net 5 http://jmockit.googlecode.com 
c-use deìnition-clear deìnitionuse associations covered A Dua Tracking Techniques nite state automata track memory last deìnition demand-driven last deìnition dua inference matrix-based B DF Testing Tools 
X X D d u X d u X X d u D d u u X X d  u u X T d u X d u u X P D d u X d u u X D d d D d u X D d u u X d X D u d D d u u X d X u m 
002 002 002 002 002 002 002 
83 


002 003 002 
004 002 002 
002 002 002 002 002 
002 002 002 
1 2 3 4 repeat 5 6 7 8 9 until 10 return 
TABLE II DF TESTING TOOLS Tool name Coverage Technique Language Availability ASSET Intra Automata Pascal TACTIC Intra Memory tracking C ATAC Intra Last deìnition C/C 
POKE-TOOL Intra Automata C JaBUTi Inter/Intra Last deìnition Bytecode JMockit Inter  Java Coverlipse Intra Path recording Java Jazz Intra Demand-driven Java DUA-FORENSICS Inter/Intra Dua inference Java DFC Intra  Java covered duas The more recent tools support Java while the older ones focus on procedural languages especially C Finally half of the tools are available for download As already noticed by Yang et al and corroborated by our own survey there is not a commercial tool supporting DF testing Hassan and Andrews report that the y were able to nd only two working DF testing tools ATAC and DUA-FORENSICS Even these two tools had restrictions in analyzing particular programs These authors suggest that the high cost associated with tracking duas has discouraged tool vendors from implementing it These are evidences that the current strategies to track duas are not able to tackle large and/or long running programs Next we will present the Bitwise Algorithm BA for dua monitoring and the BA-DUA coverage tool BA-DUA relies on BA to support scalable intra-procedural DF testing IV B ITWISE D UA C OVERAGE A LGORITHM The Bitwise Algorithm BA for dua monitoring is based on a simple idea to encode a solution for the reaching deìnitions data-îow problem into the object code In what follows we present a brief presentation of the algorithm since its full description can be found in BA is based on sets associated with each node of the ow graph Below we formally deìne the sets of born  
Born  Disabled  PotCovered  Sleepy  Born  Disabled  PotCovered  Sleepy  Alive CurSleepy Covered Alive Alive Alive Input Output Algorithm 1 Alive Covered PotCovered CurSleepy CurSleepy CurSleepy Sleepy  Sleepy  Sleepy\(4 CurSleepy Sleepy\(4 PotCovered  Disabled  Born  Sleepy  
 disabled   potentially covered   and sleepy   duas for a node of a ow graph   set of duas    r     such that   set of duas    r     such that is deìned in and   set of duas    r    o that   set of duas      such that  To determine the covered duas BA keeps track of three working setsÑthe alive duas   the current sleepy duas   and the covered duas   How these extra sets are determined is described in Algorithm 1 Before the execution of the program the three working sets are empty lines 1 to 3 At line 5 variable is assigned with the node traversed in the program execution When the program execution starts the node traversed is the start node  The algorithm proceeds by processing the nodes traversed until the program execution nishes line 9 The set contains the duas that are alive in the path traversed so far in the program execution The duas belonging to are those that were born in the path and were not disabled in it line 7 Hence the contains the duas enabled for coverage in the path provided the respective cuse or p-use is met  nodes traversed during program execution sets Disabled  Sleepy  PotCovered  and Born   Covered set Alive   CurSleepy   Covered    node traversed in program execution Covered  Covered Alive CurSleep PotCovered  Alive  Alive Disabled  Born  CurSleepy  Sleepy   Bitwise dua coverage algorithm At line 6 this meeting is veriìed is utilized to determine the covered   duas by making the intersection with the potentially covered duas   of the current node The sleepy duas   are used to avoid temporally disabled p-use duas being covered The working set is updated at line 8 with the sleepy duas associated with the just visited node Algorithm 1 correctly determines the covered duas until the program termination see proof of correctness in To highlight the role of and sets in BA consider that path 1 2 3 4 5 has been traversed in Figure 2 At node 5 the set of potentially covered duas includes the following p-use duas 1 3,5 i 5 3,5 i 1 3,5 max 4 3,5 max and 1,\(3,5 array Yet these puse duas cannot be covered because node 4 was visited before node 5 in the path According to the deìnition of  encompasses all p-use duas After the execution of node 4 is updated with  As a result no p-use dua is allowed to be covered immediately after the visit of node 4 Thus at node 5 only c-use duas will be considered covered Typically BA can be implemented by inserting lines 6 to 8 at the beginning of the code associated with each node As a result it is not necessary to keep a list of traversed nodes All sets utilized can be implemented as bit vectors In addition sets    and are constant values computed statically at instrumentation time One can also see BA as a more efìcient implementation of the FSA-based algorithms being each bit a nite state automaton In the next section we present how BA is materialized in instrumentated code inserted into a bytecode program 
program execution nishes Covered 
002 002 002 002 002 
n n n n n N G N E s e n d u X d u u X d n n d u X d u u X X n d n n d u X d u u X u n n d u u X u n n s n n n n n n n n n n n n n n n 
      
84 


    6 64 
0 1 63 0 64 65 127 1 
covered alive cursleepy int long POT_COVERED_N DISABLED_N BORN_N SLEEPY_N int long OR AND AND POT_COVERED_N 0 037DISABLED_N DISABLED_N BORN_N  0 DISABLED_N 0 alive  alive  BORN_N DISABLED_N  0 
  Update covered 1 covered  covered  alive  037sleepy  POT_COVERED_N  Update alive 2 alive  alive  037DISABLED_N  BORN_N  Update sleepy 3 cursleepy  SLEEPY_N  
Fig 3 BA probe for a node  The union and intersection of two bit vectors are implemented by the bitwise inclusive 
instrumenter analyzer on-the-îy off-line window 
  005\006  007 
n 
V BA-DUA  BAPOWERED DUA C OVERAGE The Bitwise Algorithm BA for dua monitoring is realized in the BA-DUA tool which is described next BA-DUA is implemented as two Java programs The and the  The former is responsible to instrument Java classes in order to track duas at run-time The analyzer is responsible to report which duas were or not covered and the output of the instrumented program BA-DUA can instrument classes in two different ways instrumentation uses the Java agent and instruments classes as they are loaded by the Java Virtual Machine JVM All that needs to be done is to include the BA-DUA instrumenter agent by specifying an option in the command-line instrumentation changes speciìed classes before the program starts The instrumented classes should be in the classpath so that at run-time they are loaded instead of the original classes The instrumenter works as follows For each method a ow graph is built based in the sequence of the method bytecodes Initially every instruction is a node and the edges represent the possible ow of control between the instructions This graph is then transformed into a graph where each node is a sequence of instructions All instructions in these nodes are always executed in sequence except in the occurrence of exceptions Finally for each instruction we compute the variables local variables or elds that are deìned/used This information will generate the sets of deìnitions/uses of each node Duas are then computed using known data-îow analysis techniques Given the annotated ow graph of a method and the set of duas the instrumenter inserts BA code into the bytecode program If the method does not require any dua e.g straight line methods no instrumentation code is inserted Otherwise the instrumenter decides how to insert BA code into bytecode Our design rationale was to avoid any unnecessary method call since we assume that method calls are more costly than inline instrumentation code If a method has up to 32 64 duas we use integers longs to track them Each bit in the primitive type corresponds to a dua We do not need to worry about the size of primitive type since JVM speciìes that integers longs are always 32-bits 64-bits wide  and are translated into integer or long local variables  each of them occupies one or two slots on the local variable array of the method  depending on the number of duas However if a method has more than 64 duas no primitive type can hold them In this case our solution is to use more local variables Our experimental data suggests that these methods constitute a small part of a program see Table IV BA-DUA always utilizes long variables when the number of duas of the method exceeds 64 Each one of these local variables belongs to a For instance if a method has 150 duas then the instrumentation will allocate three windows belongs to  belongs to  and so on In that sense to hold  and  methods up to 32 duas need three slots in the local variable array methods up to 64 duas needs 6 slots and methods with more than 64 duas needs slots in the local variable array where is the set of duas of a method To translate Algorithm 1 into bytecodes we insert the operations that update  and at the beginning of each node of the ow graph The code is inserted just before the rst instruction of the node The inserted code only collects coverage information without changing the method behavior For each node  we compute   and at instrumentation time These sets are encoded as constants of type integer or long depending on the number of duas Again if the number of duas of the method is greater than 64 a constant is encoded for each window Suppose we are instrumenting node of a method with up to 64 duas The code in Figure 3 describes how a BA probe looks like  and are local variables of type or and represent  and of Algorithm 1   and are constants of type or and represent sets   and of Algorithm 1 and operators respectively The subtraction operation is implemented by negating the subtrahend and making a bitwise  If a method has more than 64 duas then the code in Figure 3 is duplicated to update/use the local variables and constants of each extra window Since all sets of node are static the BA code can be optimized at instrumentation time When  statement 1 can be removed In statement 2 the value of is determined at instrumentation time to save two instructions needed to negate  When  statement 2 is removed because too Finally statement 2 can be rewritten as when  To convert the statements from BA probe to bytecode instructions is straightforward BA-DUA optimizes the sequence of instructions of a BA probe to reduce the operand stack size needed to execute the probe BA probes requires a stack of size of 2 4 for integers longs 
Alive CurSleepy Covered Alive CurSleepy Covered Covered Alive CurSleepy Born  Disabled  PotCovered  Sleepy  Covered Alive CurSleepy PotCovered  Disabled  Born  Sleepy  
dua  dua   dua window dua  dua   dua window D  D n n n n n n n n n n n 
85 


002 002 002 002 002 
010 
002 002 002 
covered alive sleepy covered  return throw covered 
A Subject Programs 
 and 3.2 Library of mathematics and statistics components HSQLDB   Relational database engine with inmemory and disk-based tables JFreeChart Library to display professional quality charts in applications JTopas    Suite for parsing arbitrary text data Scimark2    Benchmark for scientiìc and numerical computing Weka r5178   and r1004 Collection of machine learning algorithms for data mining tasks XML-Security   Library to implement XML signature and encryption standards with a small memory footprint Thus these three programs perform data manipulation tasks Finally JFreeChart is a graphics library and Commons-Lang is a library that perform tasks related to string manipulation concurrency creation and serialization of objects among others All programs are either open-source or publicly available The third criterion is related to the size of the programs To assess the scalability of BA we deìned three ranges of systems small medium and large Small systems are those ranging from 150 to 2,000 LOCs medium size systems vary from 2 KLOCs up to 30 KLOCs and large systems were those with more than 30KLOCs These ranges were deìned by estimating the number of engineers working in the system A small program can be easily coded by a single engineer a medium size program can be coded by a single engineer in the bottom of the range 2 KLOCs but more help will be needed when its size reaches the top of the range 30 KLOCs Large systems will hardly preclude the need of a group of engineers Table IV describes the characteristics of the selected programs It contains the number of lines of code LOC the number of methods and classes the number of methods with more than 64 duas with more than 32 up to 64 duas with up to 32 duas and with zero duas it also contains the total number of duas and of edges The percentage numbers in braces mean the coverage achieved by running the program test suite For Commons-lang 99.30 of classes and 93.03 methods were covered as well as 88.05 of duas and 92 of edges The percentage of methods with a particular property e.g requiring more than 64 duas with respect to the total number of methods is represented by the percentage number without braces For example Commons-lang has 1.58 of its methods with more than 64 duas required Control-îow coverages were obtained with JaCoCo and data-îow coverage with BA-DUA The lines of code were measure with JavaNCSS 6  Two programsÑJTopas and Scimark2Ñare classiìed as small systems Commons-Lang Commons-Math-2.1 and XML-Security are medium size programs and ve programs Commons-Math 3.2 HSQLDB JFreeChart Weka r5172 and r10042Ñare large systems With the exception of the programs 6 http://www.kclee.de/clemens/java/javancss 
At the beginning of the method code instructions are added to initialize the new local variables   and  with the empty value Because is a local variable it is not sensitive to recursive calls nor threads However its scope is limited to the method call To keep the coverage between method calls each class has a reference to a global array of covered duas BA-DUA implements the global array as one array of longs   per class A method can be associated with more than an entry for instance if a method have more than 64 duas Just before a or a command instructions are added to update the correspondent entries in the global array of covered duas As mention before the local variable is not thread sensitive however the global array entries are Therefore race conditions may occur when entries of the global array are updated As a result the coverage obtained can be slightly lower than the real coverage for programs with multiple threads Strategies for dealing with race conditions are beyond the scope of this paper VI BA E VALUATION An experiment to evaluate the penalties imposed by BA was carried out The experiment was designed to answer the following research questions RQ1 How do the penalties imposed by BA impact on an instrumented program RQ2 How much more expensive is BA monitoring in comparison with edge monitoring Two kinds of penalties were assessed execution overhead given by the extra time needed to execute the instrumentation code added and memory overhead represented by the growth of the object code due to the insertion of instrumentation code BA-DUA was used to assess BA performance whereas JaCoCo was utilized to assess edge monitoring This tool was chosen because it has been used to obtain control-îow instruction and edge coverage of large systems Both BADUA and JaCoCo require limited extra memory at run-time thus the program growth provides an estimate of the extra memory needed to run both instrumentation approaches Next the evaluation carried out and its results are described Table III contains the description of the programs utilized in our evaluation We utilized several criteria to select them The rst criterion was to select programs that were previously utilized in data-îow testing instrumentation papers The idea was to allow a rough comparison between instrumentation techniques These programs are indicated in Table III by a star   Within this same criterion we added those programs utilized in the simulations contained in The y are referred to by a diamond   The second selection criterion was the characteristics of the systems The goal was to have a diverse selection of programs Five programsÑCommons-Math 2.1 and 3.2 Scimark2 and Weka r5178 and r10042Ñperform mathematical calculations JTopas parses arbitrary texts and XML-Security parse and manipulate XML les and HSQLDB is a relational database TABLE III D ESCRIPTION OF THE PROGRAMS SELECTED FOR EVALUATION  Program Description Commons-Lang Library with utilities classes for Java Commons-Math 2.1  
002 
86 


Baseline JaCoCo BA-DUA 
TABLE IV C HARACTERISTICS OF THE SELECTED PROGRAMS  Program LOC Classes Methods Methods w Methods w Methods w Methods w Duas Edges duas 
003 003 
real time 
64 64 duas 32 32 dua 0 duas=0 Commons-Lang 13,743 143 2,281 36 129 907 1,209 20,594 7,395 99.30 93.03 1.58 5.66 39.76 53 88.05 92 Commons-Math 2.1 26,347 428 3,995 125 207 1,099 2,564 42,901 8,745 99.53 86.43 3.13 5.18 27.51 64.18 84.78 86 Commons-Math 3.2 52,731 845 6,886 276 370 1,745 4,495 89,678 18,576 95.03 85.42 4.01 5.37 25.34 65.28 77.08 85 HSQLDB 116,618 439 8,365 414 587 3,090 4,274 122,987 34,722 66.29 48.44 4.95 7.02 36.94 51.09 35.40 35 JFreeChart 66,895 520 8,313 274 353 2,001 5,685 77,957 21,165 82.88 60.60 3.30 4.25 24.07 68.39 44.76 45 JTopas 6184 41 475 11 17 152 295 3,773 1,233 78.05 75.37 2.32 3.58 32.00 62.11 65.52 62 Scimark2 672 10 61 5 8 23 25 1220 218 90.00 62.30 8.20 13.11 37.70 40.98 58.11 59 Weka r5178 209,482 2,068 20,727 1,142 1,297 5,477 12,811 314,369 69,420 37.86 35.95 5.51 6.26 26.42 61.81 36.15 36 Weka r10042 184,454 2,257 19,694 982 1,151 5,196 12,365 271,443 64,760 28.93 28.49 4.99 5.84 26.38 62.79 25.96 26 XML-Security 17,929 317 2,353 38 82 745 1,488 16,821 6,273 74.13 61.24 1.61 3.48 31.66 63.24 56.10 53 for which we selected two versions the others are the latest version as of September 2013 
The treatments were deìned taking into account the coverage tool utilized if any Below they are deìned  This treatment regards an uninstrumented version of the program  This treatment regards a JaCoCo instrumentated program tracking instructions and edges at run-time  This treatment regards a BA-DUA instrumentated program tracking duas at run-time Our data collection utilized two procedures One to collect execution and other to obtain memory overhead The execution overhead was determined by executing the test suites accompanying each program Excepting HSQLDB and JTopas for which failing test cases were removed the test suites executed were those contained in the repository of the program The memory overhead was obtained by the ratio of the total size of all instrumented classes by the size of all compiled classes of each program For each treatment the execution time of the subject program was obtained by averaging the time of ten executions It was collected using the option of the Unix command since we were interested in obtaining the wall clock time Exceptionally for programs whose random input data generated outliers in the time measures we executed each program one hundred times Only one program needed this procedure which was XML-Security For two methods in Commons-Math 3.2 and two in HSLQDB the size of the instrumented method was larger than 64Kbytes which hampered their execution in the JVM Our procedure was not to instrument the classes of these methods Thus the coverage of these classes was not obtained for both tools JaCoCo and BA-DUA were run in off-line mode that is the instrumentation was performed before the execution of the programs All data was collected using an Intel\(R Core\(TM Duo CPU E4500@2.20GHz 2,063,668 kBytes of RAM running Debian GNU Linux 6.0.2 and Java HotSpot\(TM 32-Bit Client VM 1.7.0 40 The execution overhead results are presented in Table V The table contains the absolute averaged execution times for each treatment In addition it comprises the overhead imposed by JaCoCo and BA-DUA in percentage terms In the second row the data regarding Commons-lang is presented The uninstrumented version of the program Baseline executed the test suite in 20.02 seconds in average the JaCoCo instrumented version executed in 20.25 seconds and BA-DUA version in 20.57 seconds The overhead imposed by JaCoCo was 1.15 and BA-DUA overhead was 2.75 In Figure 4 the ratios Baseline to JaCoCo and Baseline to BA-DUA are presented The value 1 in the chart means that there is no overhead between the uninstrumented program and the one instrumented by the testing tool On the other hand a ratio equals to 1.5 means that there is 50 overhead when the instrumented version is executed Figure 5 presents JaCoCo and BA-DUA memory overhead with respect to the Baseline it describes the ratio between the size of all compiled classes of the uninstrumented program to the size of all JaCoCo instrumented classes and to all BA-DUA instrumented classes Analogously a value 1 means that there is no memory overhead a value 2 means 100 of overhead 
B Treatments C Procedure D Results 
   
87 


000 000\002\003 004 004\002\003 005 005\002\003 006\007\010\011\010\011 012\013\014\015\016\013 
TABLE V E XECUTION OVERHEAD OF THE SELECTED PROGRAMS  Program Baseline s JaCoCo s BA-DUA s Commons-lang 20.02 20.25 20.57 1.15 2.75 Commons-Math 2.1 22.79 25.03 31.48 9.83 38.13 Commons-Math 3.2 179.57 214.27 487.8 19.46 171.65 HSQLDB 24.39 26.99 28.64 10.66 17.43 JFreeChart 5.24 5.29 5.36 0.95 2.29 JTopas 50.24 148.7 68.59 195.98 36.52 Scimark2 27.31 30.47 27.08 11.57 0.84 Weka r5178 497.39 554.87 890.78 11.56 79.09 Weka r10042 106.85 130.27 139.13 21.92 30.21 XML-Security 51.41 51.47 50.96 0.12 0.88 Fig 4 JaCoCo:Baseline and BA-DUA:Baseline execution overhead ratios VII D ISCUSSION Our discussion of the results is guided by the research questions The section is concluded with the threats to validity of the experiment Fig 5 JaCoCo:Baseline and BA-DUA:Baseline memory overhead ratios over 50 Thus our data suggests that by using BA DF testing can be applied to larger class of programs with affordable execution overhead Misurda et al.ês demand-driven approach obtained an average of 127 overhead the values varying from 4 to 279 Hassan and Andrew report that DUA-FORENSICS imposed an overhead of 2078 160.4 and 86.9 for JTopas Scimark2 and XML-Security respectively Although a comparison with previous approaches should be carried out with caution due to differences in the DF testing implementation e.g interor intra-procedural testing and JVM and hardware utilized the results indicate that BA overhead is signiìcantly smaller This difference is more signiìcant when comparing the size of the program utilized in experiments Previously only small to medium size programs were utilized This is so because both approaches have to keep data structures that encompass all duas of the programs On other hand BA performance is governed by the number of duas in each method and by how long the execution remains in particular methods BA-DUA bit vectors are implemented as a 64-bit long thus there is no difference in tracking one or 64 duas However a method with 65 duas will require another long and more code to update the extra long As a result the execution overhead increases Table IV shows that the number of methods with more than 64 duas is small at most 8.2 and as low as 1.58 of all methods Even though a small number of methods with high concentration of duas can hurt performance For example Commons-Math 3.2 has 276 methods with more that 64 duas 24 of which with more than 320 duas 5 longs and 4 with more than 1000 duas 16 longs Thus depending on how long the execution remains in these methods the BA overhead will increase Particularly Commons-Math 3.2 has a high method coverage 85.42 which means that they are very likely to be executed by the test suite BA memory overhead is essentially the bit vectors created  
A BA Penalties 
covered live cursleepy 
The execution overhead imposed by BA varied from 0.88 for XML-Security to 171.65 for Commons-Math 3.2 being the average overhead 37.64 More interestingly though is to analyze the data presented in Table V and in Figure 4 For four of the subject programs the overhead was less than 3 which is negligible taking into account the number of duas tracked The overhead was between 10 and 50 for four subjects and only two subjects had overhead   and  and the extra code 
000 000\002\003 004 004\002\003 005 005\002\003 006 006\002\003 007\010\011\012\011\012 013\014\015\016\017\014 
88 


inserted 7  Figure 5 presents the program growth caused in BADUA instrumented programs in comparison to uninstrumented programs With the exception of Scimark2 the memory overhead is around 50 Although that can be considered a sizeable memory footprint a BA-DUA instrumented program can run in the majority of hardware platforms with the possible exception of embedded systems with very tight memory We purposely decided to implement BA in BA-DUA following its description in The idea w as to assess the original proposition of the algorithm However the program growth can be reduced by instrumenting not nodes as in BA-DUA but edges as in JaCoCo This simple strategy will reduce the size of the instrumented program and additionally the execution overhead A comparison with edge monitoring implemented in tools like JaCoCo is valid since dua coverage provided by BA-DUA can be approximately inferred from edge coverage In terms of execution overhead both tools have a similar performance for 7 out of 10 programs The exceptions are Commons-Math 3.2 and Weka r5178 for which JaCoCo is signiìcantly better For three programs BA-DUA performed better Scimark2 XMLSecurity and JTopas BA-DUA instrumented Scimark2 outperforms even the uninstrumented program this performance is possibly due to how the JVM Just-In-Time compiler optimizes register allocations and cache effectiveness for the BA-DUA instrumented code For XML-Security the performance are very similar and BA-DUA advantage is possibly due to the random input data JaCoCo JTopas instrumentated version is hampered by the fact that a method without duas is called a huge amount of times This method is not instrumented by BA-DUA thus it does incur in such an overhead The decision of not to instrument zero-duas methods is because dua coverage does not subsume edge coverage in the presence of infeasible paths  Thus one will not prescind a control-îo w tool if method edge and node coverage are needed The results suggest though that for several programs the execution overhead is similar In applications in which DF testing is most indicated namely security and critical applications approximate intraprocedural dua coverage will hardly be useful The approximate dua coverage can overshoot the precise coverage in up to 30 In these scenarios one is interested in kno wing which paths were not tested thus a precise dua coverage is needed It can be collected by using BA at affordable overhead BA-DUA utilizes a bit to represent a dua whereas JaCoCo uses a boolean to represent an edge node coverage is inferred from edge coverage JaCoCo memory overhead varies from 12 HSLQDB to 19 Scimark2 which can be observed by the almost constant ratio in Figure 5 BA-DUA incurs in higher memory overhead but its growth should be analyzed by the number of entities tracked at run-time Scimark2 which is the worst case with memory overhead of 118 for BA-DUA requires around 6 times more duas to be tracked than edges Thus the memory overhead grows 6 times 7 This is an estimate since we are not taking into account the size of the global array of covered duas created at run-time for each loaded class The external validity of the experiment is arguable on the grounds that our large programs are not large enough One may argue that a 30 KLOC program is not large or even a 200 KLOC program Although systems with millions of lines of code are rather common they are not monolithically coded On the contrary they are divided into manageable components of smaller size for which test suites are developed Our goal was to identify programs that could be part of a system of more than 1 MLOC In this sense the medium size programs are also candidates to be a component of such systems Our data suggests that BA-DUA can be applied to collect dua coverage of these components However it does not allow us to infer that BA-DUA can support integration testing of these huge systems The internal validity of our experiment concerns the results provided by JaCoCo and BA-DUA JaCoCo is widely used in industry which is an indication that its results are trustworthy BA-DUA has been validated with small programs and its results were compared with those provided by JaBUTi BA-DUA performs a more accurate data-îow analysis of bytecodes but the results were similar Regarding conclusion and construction validities the comparison among treatments was straightforward and a modest hardware was utilized to conduct the experiment Regarding this latter point the idea was to better observe the relationship between treatments Had a top hardware conìguration been utilized the results could be more compelling especially for the subjects with execution overhead above 50 VIII C ONCLUSION The goal of this paper was to show that Data-îow DF testing supporting tools can be implemented to tackle large programs DF testing requires tests that traverse a path in which the deìnition of a variable and its subsequent use i.e a deìnition-use association dua is exercised To achieve such a goal a tool called BA-DUA Bitwise Algorithm-powered Deìnition-use Association coverage was implemented BA-DUA utilizes a strategy to track duas at runtime called Bitwise Algorithm BA for intra-procedural dua monitoring BA was recently proposed and was only assessed by means of simulations It is based on a simple idea which consists of encoding a solution for the reaching deìnitions data-îow problem into an object code In this paper  we ha v e described BA and how it is mapped into bytecodes BA-DUA implements such a mapping An experiment was conducted to assess the penalties imposed by BA to track intra-procedural duas at run-time Two kinds of penalties were assessed execution overhead given by the extra time needed to execute the instrumentation code added and memory overhead represented by the growth of the object code due to the insertion of instrumentation code Ten programs with size varying from 600 LOC and 200 KLOC were utilized in the experiment Two programs were classiìed as small less than 2 KLOC three as medium size programs more than 2 KLOC and less than 30 KLOC 
B BA-DUA Costs versus JaCoCo Costs C Threats to Validity 
89 


and ve were categorized as large more than 30 KLOC The subject programs perform mathematical functions data manipulation tasks graphical functions and utilities functions for Java BA-DUA execution overhead varied from negligible to 172 being the average 38 However only two subjects in ten programs had their performance impacted more than 50 for four programs the overhead was less than 3 In terms of program growth the BA memory overhead ranges from 33 to 118 being the average 61 BA-DUA performance was also compared with that of the JaCoCo toolÑa widely used control-îow testing supporting tool In terms of execution overhead both tools have a similar performance for 7 out of 10 programs JaCoCo has a smaller memory footprint but tracks fewer entities at run-time We purposely implemented BA in BA-DUA following its original proposition The idea was to assess BA as it is The results suggest that by using BA to track duas the overheads especially the execution overhead are signiìcantly reduced As a result a larger class of program are able to be assessed by intra-procedural DF testing BA-DUA is available for download and use at https://github.com/saeg/ba-dua Nevertheless there is room for improvement The program growth and slowdown can be reduced by instrumenting edges instead of nodes However the BA main drawback are methods with a high concentration of duas since they require more code to be inserted causing higher execution and memory overheads We intend to investigate how the subsumption relationship among duas can be utilized to determine a minimal set of duas to be tracked at run-time We conjecture that in methods with many duas this minimal set is relatively small Other avenues of research encompass the extension of the BA algorithm to track inter-procedural duas and to tackle programs with multiple threads As a concluding remark we hope that the results presented in this paper encourage vendors to consider including DF testing in their set of testing tools A CKNOWLEDGMENT The authors acknowledge the support granted by the CAPES FAPESP to the INCT-SEC National Institute of Science and Technology  Critical Embedded Systems Brazil processes 573963/2008-9 and 08/57870-9 and by the Research Support Center on Open Source Software NAPSoL of the University of Sao Paulo R EFERENCES  D G Feitelson E Frachtenber g and K L Beck De v elopment and deployment at facebook 
 vol 17 no 4 pp 8Ö17 August 2013  S Grimm F acebook engineering What kind of automated testing does facebook do On A v ailable http://www quora.com/F acebookEngineering What-kind-of-automated-testing-does-Facebook-do  S Rapps and E J W e yuk er  Selecting softw are test data using data ow information  vol 11 no 4 pp 367Ö375 Apr 1985  K Moir  Releng of the nerds Open source release engineering sDK code coverage with JaCoCo  A v ailable http://relengofthenerds.blogspot.com.br/2011/03 sdk-code-coverage-with-jacoco.html  M Hutchins H F oster  T  Goradia and T  Ostrand Experiments of the effectiveness of dataîowand controlîow-based test adequacy criteria in  ser ICSE 94 1994 pp 191Ö200  P  G Frankl and O Iak ounenk o Further empirical studies of test efìctiveness in  ser FSE 98 1998 pp 153Ö162  T B Dao and E Shibayama Security sensiti v e data o w co v erage criterion for automatic security testing of web applications in  ser ESSoS 2011 pp 101Ö113  R Santelices J A Jones Y  Y u and M J Harrold Lightweight fault-localization using multiple coverage types in  ser ICSE 09 2009 pp 56Ö66  J Misurda J A Clause J L Reed B R Childers and M L Sof f a Demand-driven structural testing with dynamic instrumentation in  ser ICSE 05 2005 pp 156Ö165  R Santelices and M J Harrold Ef ciently monitoring data-îo w test coverage in  ser ASE 07 2007 pp 343Ö352  M L Chaim and R P  A de Araujo  An ef cient bitwise algorithm for intra-procedural data-îow testing coverage  vol 113 no 8 pp 293Ö300 2013  P  G Frankl The use of data o w information for the selection and evaluation of software test data Ph.D dissertation New York University New York October 1987  T  J Ostrand and E J W e yuk er  Data o w-based test adequac y analysis for languages with pointers in  ser TAV4 1991 pp 74Ö86  J R Hor gan and S London  A data o w co v erage testing tool for C  in  1992 pp 2Ö10  P  G Frankl and E J W e yuk er   An applicable f amily of data o w testing criteria  vol 14 no 10 pp 1483Ö1498 Oct 1988  M L Chaim POKE-T OOL  A tool to support structural program testing based on data ow analysis Masterês thesis School of Electrical and Computer Engineering State University of Campinas Campinas SP Brazil 1991 in Portuguese  A M R V incenzi J C Maldonado W  E W ong and M E Delamaro Coverage testing of java programs and components  vol 56 no 1-2 pp 211Ö230 Apr 2005  J Misurda J Clause J Reed B R Childers and M L Sof f a Jazz a tool for demand-driven structural testing in  ser CCê05 2005 pp 242Ö245  I Bluemk e and A Arthur Rembisze wski Dataîo w approach to testing java programs in  ser DEPCOSRELCOMEX 09 2009 pp 69Ö76  Q Y ang J J Li and D M W eiss  A surv e y of co v erage-based testing tools  vol 52 no 5 pp 589Ö597 2009  M M Hassan and J H Andre ws Comparing multi-point stride co v er age and dataîow coverage in  ser ICSE 13 2013 pp 172Ö181  A V  Aho M S Lam R Sethi and J D Ullman  2nd ed Boston Pearson AddisonWesley 2007  M L Chaim and R P  A de Araujo Proof of correctness of the bitwise algorithm for intra-procedural data-îow testing coverage PPgSI-001/2013 School of Arts Sciences and Humanities University of Sao Paulo Tech Rep 2013 available at http://ppgsi.each.usp.br relatorios-tecnicos-2013  M Marr  e and A Bertolino Using spanning sets for coverage testing  vol 29 no 11 pp 974Ö984 2003 
IEEE Internet Computing IEEE Trans on Software Eng Proc of the 16th International Conference on Software Engineering Proc of the ACM SIGSOFT Foundations of Software Engineering Conference Engineering Secure Software and Systems Proc of the 31st International Conference on Software Engineering Proc of the 27th International Conference on Software Engineering Proc of the 22nd IEEE/ACM International Conference on Automated Software Engineering Inf Process Lett Proc of the Symposium on Testing Analysis and Veriìcation Proc of Symposium on Assessment of Quality Software Development Tools IEEE Trans on Software Eng Science of Computer Programming Proc of the 14th International Conference on Compiler Construction Proc of the 2009 IEEE Fourth International Conference on Dependability of Computer Systems Comput J Proc of 35th International Conference on Software Engineering Compiler s principles techniques and tools IEEE Trans Software Eng 
90 


method increases slightly which is due to two reasons 1 the data before In this experiment we investigate the performance of OLTP queries when OLAP queries are running The workload is update-only and the keys being updated are uniformly distributed We launch ten clients to concurrently submit the updates when the system is deployed on 100 nodes Each client starts ten threads each of which submits one million updates 100 updates in batch Another client is launched to submit the data cube slice query That is one OLAP query and approximately 50,000 updates are concurrently processed in R-Store The system reaches its maximum usage in this setting based on our observation When the system is deployed on other number of nodes the number of clients submitting updates is adjusted accordingly Figure 11\(a shows the throughput of the system The throughput increases as the number of nodes increases which demonstrates the scalability of the system However when OLAP queries are running the update performance is lower than running only OLTP queries This result is expected because the OLAP queries compete for resources with the OLTP queries We also evaluate the latency of updates when the system is approximately fully used As shown in Figure 11\(b the aggregated response time for 1000 updates are similar with respect to varying scales VII C ONCLUSION MapReduce is a parallel execution framework which has been widely adopted due to its scalability and suitability in 0    500    1000    1500    2000  0  10  20  30  40  50  60  70  80  90  100  IncreQueryScan             IncreQueryExe              DC DC DC  Q i i i i T part  Q a Data Cube Slice Query                                                                                                b TPC-H Q1 Fig 7 Performance of Querying    Fig 8 Accuracy of Cost Model    Fig 9 Performance vs Freshness On each HBase-R node the key/values are stored in format Though only one or two versions of the same key are returned to MapReduce HBase-R has to scan all the of the table Since the is materialized to HDFS when it is full these 223les are sorted by time Thus instead of scanning all the and between  only the between   are scanned The value of decides the freshness of the result There is a trade-off between the performance of the query and the freshness of the result the smaller is the fewer real-time data are scanned Figure 9 shows the query processing time with different freshness ratios which is de\223ned as the percentage of the real-time data we have to scan for the query In this experiment  1600 million and 800 million updates on 1 distinct keys are submitted to HBase-R When the freshness ratio is 0 the input of the query is only the data cube Thus the cost of scanning the real-time data is 0 When the freshness ratio increases to 10 the cost of scanning the real-time data is around 1500 seconds because the cost of scanning the real-time table dominates the OLAP query As the freshness ratio increases the running time of and  and when it is not  and  We submit 800 million updates to the server each day and the percentage of keys updated is 223xed to 1 The data cube is refreshed at the beginning of each day and the OLAP query is submitted to the server at the end of the day Since the data are compacted after the data cube refresh the amount of data stored in the real-time table are almost the same at the same time of each day The processing time of and are thus almost constant In contrast when the compaction scheme is turned off HBase-R stores much more data and the cost of locally scanning these data becomes larger than the cost of shuf\224ing the data to MapReduce As a result the processing time of and increases over time and and a user speci\223ed timestamp still need to be scanned and 2 the amount of data shuf\224ed to mappers are roughly the same with different ratios Figure 10 depicts the effectiveness of our compaction scheme In this experiment we measure the processing time of the data cube slice query when the compaction scheme is applied  0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan Processing Time \(s I/Os \(X10 11  Percentage of Keys Updated CubeScan        Processing Time \(s Freshness Ratio CubeScan                                                                                                            50 0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan store\223le store\223les part memstore store\223les memstore store\223les IncreQuerying Baseline IncreQuerying Baseline-NC IncreQuerying-NC Baseline IncreQuerying Baseline-NC IncreQuerying-NC C Performance of OLTP 0    1200    2400    3600    4800    6000  1  5  10  15  20  25  0  0.8  1.6  2.4  3.2  4  IncreQueryScan        IncreQueryExe        I/Os estimated for IncreQuery                               I/Os estimated for  Baseline                 T T T T T T T T 


3000    6000    9000    12000  1  2  3  4  5  6  7  IncreQuerying                                   Baseline-NC                   IncreQuerying-NC                       51 002 Fig 10 Effectiveness of Compaction    a Throughput    b Latency Fig 11 Performance of OLTP Queries a large scale distributed environment However most existing works only focus on optimizing the OLAP queries and assume that the data scanned by MapReduce are unchanged during the execution of a MapReduce job In reality the real-time results from the most recently updated data are more meaningful for decision making In this paper we propose R-Store for supporting real-time OLAP on MapReduce R-Store leverages stable technology HBase and HStreaming and extends them to achieve high performance and scalability The storage system of R-Store adopts multi-version concurrency control to support real-time OLAP To reduce the storage requirement it periodically materializes the real-time data into a data cube and compacts the historical versions into one version During query processing the proposed adaptive incremental scan operation shuf\224es the real-time data to MapReduce ef\223ciently The data cube and the newly updated data are combined in MapReduce to return the real-time results In addition based on our proposed cost model the more ef\223cient query processing method is selected To evaluate the performance of R-Store we have conducted extensive experimental study using the TPCH data The experimental results show that our system can support real-time OLAP queries much more ef\223ciently than the baseline methods Though the performance of OLTP degrades slightly due to the competition for resources with OLAP the response time and throughput remain good and acceptable A CKNOWLEDGMENT The work described in this paper was in part supported by the Singapore Ministry of Education Grant No R-252000-454-112 under the epiC project and M.T 250 Ozsu\220s work was partially supported by Natural Sciences and Engineering Research Council NSERC of Canada We would also like to thank the anonymous reviewers for their insightful comments R EFERENCES  http://hbase.apache.or g  http://hstreaming.com  http://www comp.nus.edu.sg epic  M Athanassoulis S Chen A Ailamaki P  B Gibbons and R Stoica Masm ef\223cient online updates in data warehouses In  pages 865\205876 2011  Y  Cao C Chen F  Guo D Jiang Y  Lin B C Ooi H T  V o S W u and Q Xu Es2 A cloud data storage system for supporting both oltp and olap ICDE pages 291\205302 2011  S Ceri and J W idom Deri ving production rules for incremental vie w maintenance In  pages 577\205589 1991  T  Condie N Conw ay  P  Alv aro J M Hellerstein K Elmelee gy  and R Sears Mapreduce online In  pages 313\205328 2010  J Dean S Ghema w at and G Inc Mapreduce simpli\223ed data processing on large clusters In  pages 137\205150 2004  L Golab T  Johnson and V  Shkapen yuk Scheduling updates in a real-time stream warehouse ICDE pages 1207\2051210 2009  M Grund J Kr 250 uger H Plattner A Zeier P Cudre-Mauroux and S Madden Hyrise a main memory hybrid storage engine  4\(2 Nov 2010  A Gupta I S Mumick and V  S Subrahmanian Maintaining vie ws incrementally extended abstract In  pages 157\205166 1993  S H 264 eman M Zukowski N J Nes L Sidirourgos and P Boncz Positional update handling in column stores In  pages 543\205 554 2010  D Jiang G Chen B C Ooi and K.-L T an epic an e xtensible and scalable system for processing big data 2014  D Jiang B C Ooi L Shi and S W u The performance of mapreduce an in-depth study  3\(1-2 Sept 2010  D M Kane J Nelson and D P  W oodruf f An optimal algorithm for the distinct elements problem PODS 22010 pages 41\20552  A K emper  T  Neumann F  F  Informatik T  U Mnchen and DGarching Hyper A hybrid oltp&olap main memory database system based on virtual memory snapshots In  2011  T W  K uo Y T  Kao and C.-F  K uo T w o-v ersion based concurrenc y control and recovery in real-time client/server databases  52\(4 Apr 2003  K Y  Lee and M H Kim Ef 223cient incremental maintenance of data cubes In  pages 823\205833 2006  F  Li B C Ooi M T  250 Ozsu and S Wu Distributed data management using mapreduce In  2014  I S Mumick D Quass and B S Mumick Maintenance of data cubes and summary tables in a warehouse In  pages 100\205111 1997  A Nandi C Y u P  Bohannon and R Ramakrishnan Distrib uted cube materialization on holistic measures In  pages 183\205194 2011  L Neume yer  B Robbins A Nair  and A K esari S4 Distrib uted stream computing platform In  pages 170\205177 2010  C Olston B Reed U Sri v asta v a R K umar  and A T omkins Pig latin a not-so-foreign language for data processing In  pages 1099\2051110 2008  K Ser ge y and K Y ury  Applying map-reduce paradigm for parallel closed cube computation In  pages 62\20567 2009  M Stonebrak er  D J Abadi A Batkin X Chen M Cherniack M Ferreira E Lau A Lin S Madden E O\220Neil P O\220Neil A Rasin N Tran and S Zdonik C-store a column-oriented dbms In  pages 553\205564 2005  A Thusoo J S Sarma N Jain Z Shao P  Chakka S Anthon y  H Liu P Wyckoff and R Murthy Hive a warehousing solution over a mapreduce framework  2\(2 2009  P  V assiliadis and A Simitsis Near real time ETL In  volume 3 pages 1\20531 2009  C White Intelligent b usiness strate gies Real-time data w arehousing heats up  2012 SIGMOD VLDB NSDI OSDI SIGMOD SIGMOD Proc VLDB Endow In ICDE IEEE Trans Comput VLDB ACM Computing Survey SIGMOD ICDE ICDMW SIGMOD DBKDA VLDB PVLDB Annals of Information Systems DM Review 0    Processing Time \(s Time since the Creation of Data Cube \(day Baseline                  Updates Per Second \(K Number of Nodes Updates only                  Response Time for 1000 Updates\(s Number of Nodes Updates only                  0    20    40    60    80    100  10  20  30  40  50  60  70  Updates + OLAP                                    0    2    4    6    8    10  10  20  30  40  50  60  70  Updates + OLAP                                    Proc VLDB Endow 


  13    1  2   3   4   5   6   7   8  9  10  11   


and aeronautical engineering with degrees from Universitat Politecnica de Catalunya in Barcelona Spain and Supaero in Toulouse France He is a 2007 la Caixa fellow and received the Nortel Networks prize for academic excellence in 2002 Dr Bruce Cameron is a Lecturer in Engineering Systems at MIT and a consultant on platform strategies At MIT Dr Cameron ran the MIT Commonality study a 16 002rm investigation of platforming returns Dr Cameron's current clients include Fortune 500 002rms in high tech aerospace transportation and consumer goods Prior to MIT Bruce worked as an engagement manager at a management consultancy and as a system engineer at MDA Space Systems and has built hardware currently in orbit Dr Cameron received his undergraduate degree from the University of Toronto and graduate degrees from MIT Dr Edward F Crawley received an Sc.D in Aerospace Structures from MIT in 1981 His early research interests centered on structural dynamics aeroelasticity and the development of actively controlled and intelligent structures Recently Dr Crawleys research has focused on the domain of the architecture and design of complex systems From 1996 to 2003 he served as the Department Head of Aeronautics and Astronautics at MIT leading the strategic realignment of the department Dr Crawley is a Fellow of the AIAA and the Royal Aeronautical Society 050UK\051 and is a member of three national academies of engineering He is the author of numerous journal publications in the AIAA Journal the ASME Journal the Journal of Composite Materials and Acta Astronautica He received the NASA Public Service Medal Recently Prof Crawley was one of the ten members of the presidential committee led by Norman Augustine to study the future of human space\003ight in the US Bernard D Seery is the Assistant Director for Advanced Concepts in the Of\002ce of the Director at NASA's Goddard Space Flight Center 050GSFC\051 Responsibilities include assisting the Deputy Director for Science and Technology with development of new mission and measurement concepts strategic analysis strategy development and investment resources prioritization Prior assignments at NASA Headquarters included Deputy for Advanced Planning and Director of the Advanced Planning and Integration Of\002ce 050APIO\051 Division Director for Studies and Analysis in the Program Analysis and Evaluation 050PA&E\051 of\002ce and Deputy Associate Administrator 050DAA\051 in NASA's Code U Of\002ce of Biological and Physical Research 050OBPR\051 Previously Bernie was the Deputy Director of the Sciences and Exploration Directorate Code 600 at 050GSFC\051 Bernie graduated from Fair\002eld University in Connecticut in 1975 with a bachelors of science in physics with emphasis in nuclear physics He then attended the University of Arizona's School of Optical Sciences and obtained a masters degree in Optical Sciences specializing in nonlinear optical approaches to automated alignment and wavefront control of a large electrically-pumped CO2 laser fusion driver He completed all the course work for a PhD in Optical Sciences in 1979 with emphasis in laser physics and spectroscopy He has been a staff member in the Laser Fusion Division 050L1\051 at the Los Alamos National Laboratories 050LANL\051 managed by the University of California for the Department of Energy working on innovative infrared laser auto-alignment systems and infrared interferometry for target alignment for the HELIOS 10 kilojoule eight-beam carbon dioxide laser fusion system In 1979 he joined TRW's Space and Defense organization in Redondo Beach CA and designed and developed several high-power space lasers and sophisticated spacecraft electro-optics payloads He received the TRW Principal Investigators award for 8 consecutive years Dr Antonios A Seas is a Study Manager at the Advanced Concept and Formulation Of\002ce 050ACFO\051 of the NASA's Goddard Space Flight Center Prior to this assignment he was a member of the Lasers and Electro-Optics branch where he focused on optical communications and the development of laser systems for space applications Prior to joining NASA in 2005 he spent several years in the telecommunication industry developing long haul submarine 002ber optics systems and as an Assistant Professor at the Bronx Community College Antonios received his undergraduate and graduate degrees from the City College of New York and his doctoral degree from the Graduate Center of the City University of New York He is also a certi\002ed Project Management Professional 14 


 





 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


