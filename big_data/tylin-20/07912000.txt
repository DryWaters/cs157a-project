Efficient Incremental Itemset Tree for Approximate Frequent Itemset Mining On Data Stream Pavitra Bai S Assistant Professor, Dept., of Information Science S.J.B Institute of Technology Bangalore, India s.pavitra@gmail.com Dr. Ravi Kumar G K Affiliated to Visvesvaraya Technological University \(VTU Visvesvaraya Technological University Belgaum, India ravikumargk@yahoo.com   Abstract Mining frequent itemsets and association rules on data stream is an important and challenging task. Tree based approaches have been extensively studied and widely used for their parallel processing capability. Itemset Tree is an efficient data structure to represent the transactions for performing selective mining of frequent itemsets and association rules. The transactions are inserted incrementally and provide on-demand ad-hoc querying on the tree for fining frequent itemsets and association rules for different support and confidence values However the size of tree grows larger for unbounded data streams limiting the scalability.  In this paper, we propose an Approximation based Incremental Memory Efficient Itemset Tree \(AIMEIT\which is an extension to Memory Efficient Itemset Tree \(MEIT\nstruct the itemset tree from data stream. The user defined minimum support of interest has been used along with Lossy counting algorithm to prune transactions before inserting them into the tree Experimental results show that the proposed algorithm is more memory efficient and takes lesser processing time for constructing the tree KeywordsÑdata mining, itemset tree; frequent itemset assocition rule; lossy counting; data stream I   I NTRODUCTION  Frequent Itemset mining is a process to effectively and efficiently mine frequently occurring set of items and discover correlation among them in a transaction database. Several algorithms have been proposed to find frequent itemsets in a static database. Mining frequent itemsets on large volume of high velocity streaming data is being focused in the recent years. Mining frequent itemsets in streaming data is more challenging as frequent items may become infrequent and infrequent items may become frequent as time elapses. Also due to the rapid flow of the data, the algorithms will have only one chance to parse the data. so of the batch processing algorithms are found to be not suitable for mining streaming data due to its high velocity, volume and drift in the concept Many incremental algorithms have been proposed which store all or approximate frequent items satisfying a given minimum support. These algorithms generate frequent itemsets which satisfy the given minimum support and donêt construct a data structure for storing these items for future reference. Users cannot perform selective mining by querying the data for finding frequent itemsets or association rules for different support and confidence thresholds. This is due to the fact that the volume of the streaming is so huge that it cannot be stored and processed at real-time. It requires of huge amount of memory to hold th e data for quick processing. For todayês fast evolving business needs, constantly changing requirements, on-demand, ad-hoc querying is very essential In this paper, we propose a memory efficient data structure which is constructed incrementally and provides the flexibility to dynamically query for approximate frequent itemsets and association rules with no false positives This paper is organized into following sections: Section II provides the review of related work, Section III details the problem statement, Section IV describes the proposed AIMEIT algorithm, Section V provides the experimental results and characteristics of the datasets used in those experiments. The conclusion and future research is provided in Section VI II  R EVIEW OF R ELATED W ORK  There are many algorithms that have been proposed to build tree for frequent itemset mining from data streams. The Pre-FP Tree and Item set-TidSet Tree IT IT ree  proposed to incrementally generate the itemset tree from streaming data. The Pre-FP Tree is a based on FUFP-Tree FUFP-Tree is a regular FP-Tree with bidirectional links between the child and parent nodes. Both Pre-FP Tree and ITTree algorithm categorizes the itemsets as frequent, pre-large and infrequent in the original database and the stream window and handles the insertion of the itemsets based on their category in original database and window stream. It maintains additional tables to maintain the pre-large itemsets for future insertion into the original tree. Maintaining additional tables is a memory overhead. It is also processing intensive as it has to read the original tree to get the different categories of itemsets while processing each stream window The canonical ordered tree \(CAN-Tree ds the  transactions and arranges the tree nodes according to some canonical order. The user shall mention the canonical order before the tree construction or during the mining process. The algorithm finds the frequent itemsets for pre-Minimum Support value which is usually less than the actual minimum support and builds the tree in the canonical order defined earlier. It is efficient in terms of quality of tree as it stores only frequent itemsets but lacks on the processing time and memory usage. For large window size, more time is spent in 239 978-1-5090-2399-8/16/$31.00 c  2016 IEEE 


finding the frequent itemsets before inserting them into the tree. The data structure used to construct tree can also be improved for memory efficiency The Generate and Merge Tree \(GM-Tree d on the CAN-Tree. In GM-Tree, the frequent itemset tree is generated for every stream window and then merged with the original tree. The generation of itemset-tree for transactions in stream window works in the batch mode which requires parsing of the data twice. First parse to order the transactions and second to build the itemset-tree. Also the time complexity increases as the stream window size increases The Incremental Mining Binary Tree \(IMB-Tree requires the data to be stored in the database first and then the transactions are extracted one by one to build the lexicographic tree to store the generated frequent itemsets. The tree is a binary tree i.e. each node can represent maximum of two itemsets. The child node contains itemset of the parent node as well which makes the tree grow bigger for large sparse dataset Many other data structures have been proposed such as   d-Tree 8] etc whic h lack  the capability of incrementally mine the high volume data stream due to their batch processing nature Most of the trees proposed represent the format of Itemset  h provides a lossless represe ntation of transaction. It is an efficient representation of the itemsets and can be built incrementally. The tree can be dynamically queried. However the major limitation of this tree is that it consumes huge amount of memory. This limitation has been overcome by Memory Efficient Itemset Tree \(MEIT this algorithm the parent node information is not duplicated in the child node unless in the itemset tree. Also the leaf nodes contains the itemsets rather than single items if their support is same thus reducing the size of the tree The MEIT algorithm has been used as the base for the proposed faster incremental algorithm for its simplicity and memory efficiency III  P ROBLEM S TATEMENT  A  Preliminaries Consider a set of distinct items I = {i 1 i 2 i 3 i n  A subset of set I is called as an itemset. Let T be the set of transactions t 1 t 2 t 3 t m  where each transaction t contains subset of I  An itemset is said to be frequent if the count of occurrence of the itemset in the complete transaction is more than the given threshold called minimum support A data stream is an unbounded list Ds = {t 1 t 2 t 3  of transactions arriving at the processing node sequentially The items in the transactions are arranged in the lexicographical order before inserting them into the tree B  Problem Definition   The MEIT algorithm stores every transaction in the tree in form of itemset without pruning the infrequent itemsets. For sparse datasets, the size of the tree grows with large number of low interest itemsets which may never be mined consuming large memory   Large size of the tree also affects the query processing time as the algorithm needs to parse through different branches of the tree to serve the query   Pruning all infrequent itemsets from the stream window before inserting them into the tree becomes time consuming when the data is dense i.e. large number of frequent itemsets are generated or window size is large IV  AIMEIT  A LGORITHM  A  Determining Window Size The fixed sized window is adopted to buffer the streaming transactions. The size of the window WS is determined by the user minimum support of interest SI and acceptable support error as given below    The above equation sets the safer window size to pruning infrequent items safely The is considered with respect to SI provided by the user For example, if SI 0.2 and 0.1 then it means the error can be ±10% of the SI i.e. minimum support can vary between 0.22 and 0.18 B  Determining Delta The Delta value is the local minimum support that items should satisfy to be called as frequent in the given stream window. The Delta value is determined using the minimum support of interest SI and acceptable support error using the equation given below    Example: if SI 0.2 and 0.1, then WS 50 and Delta  9. Notice that the Delta value is just below the user specified minimum support interest which will avoid any border itemset getting pruned C  Algorithm Input SI Transactions from Stream Window Output Itemset Tree 1 Determine Window Size WS 2 Determine Delta value 3 Read Transaction Stream 4  if buffer size reached Window Size then  5  for each Transaction t in Stream do  6 Extract 1-itemsets and Update their Support Count 7  end for  8  for each 1-itemset do  9  if Support Count < Delta then  10 remove the 1-itemset from Transactions 11  end if  12  end for  13 MEIT.Insert \(Ordered Modified Transactions 14  else  15  Repeat Step 3 16 end if 240 2nd International Conference on Applied and Theoretical Computing and Communication Technology iCATccT 


The AIMEIT algorithm uses the MEIT algorithm for constructing the tree and adds a pre-processing stage to prune the infrequent items from the data stream before inserting the transactions into the tree. The algorithm is split into five major steps. First, the window size for buffering the data stream is determined \(line 1\Second, the delta value used by the Lossy counting algorithm for pruning infrequent itemsets is determined using the user specified minimum support of interest SI and acceptable error line 2\are read and stored in the buffer till the buffer size reaches the window size \(line 4 and 15\Th ird, the support of 1-itemsets in the given stream window is found \(line 5 to 7\ourth using Lossy counting algorithm, the infrequent 1-itemsets are removed from the transactions \(line 8 to 12 will then contain only those items which are frequent in the given stream window. Fifth and last step, the modified transactions are incrementally added into the MEIT \(line 13 V  E XPERIMENTS AND P ERFORMANCE A NALYSIS  In this section, the details of the dataset used for experimentation and results of the experiments are reported All experiments were perfomed on Dell Inspiron 3537 with Intel i5 CPU, 8 GB memory and Ubuntu Desktop 14.04 LTS operating system. The algorithm is implemented using Java JKD version 1.8. In the experiments, the proposed algorithm is compared with the MEIT implemention in SPMF open source data mining library using widely used synthetic and real datasets The T10I4D100K and T4 0I10D100K are synthetic datasets generated from the IBM Almaden Quest research  m i zed retail m a rket basket data from an anonymous Belgian ret The  Accident dataset contains the traffic accident data obtained from the National Institute of Statistics for the region of Flanders, Belgium for the period  The  characteristics of these datasets are given in Table I TABLE I  D ATASET C HARACTERISTICS   Transactions Count Distinct Items Count Minimum Transaction Length Maximum Transaction Length T10I4D100K 100000 870 1 29 T40I10D100K 100000 942 4 77 Retail 88162 16470 1 76 Accident 340183 468 18 51  The datasets were evaluated for different support threshold to find the distribution of density of frequent itemsets. The number of frequent itemsets for support threshold ranging from 0.01 to 1 is given in Table II. Due to memory and processing limitation on the hardware, the frequency count for accident dataset in Table II for support lesser than 0.1 is given as Very Large to denote that the frequent itemset counts are larger than the count at support 0.1 From Table I and Table II we can infer that the Accident dataset is found to be very dense. Next to it is the TK0I10D100K dataset. Retail and T10I4D100K datasets are sparse In the experiments, as shown in the Fig. 1 and 2, the following observations are derived: The memory consumed to generate the itemset tree using the proposed AIMEIT algorithm is reduced by average of 60% for sparse datasets and 3% for very dense dataset compared to MEIT algorithm The processing time taken to construct the itemset tree by AIMEIT algorithm is reduced by average of 43% for sparse data and increased by 52% for dense data TABLE II  F REQUENT I TEMSET D ISTRIBUTION IN D ATASETS  Minimum Support T10I4D100K T40I10D100K Retail Accident 0.01 385 65236 159 V ERY L ARGE  0.02 155 2293 55 V ERY L ARGE  0.03 60 793 32 V ERY L ARGE  0.04 26 440 18 V ERY L ARGE  0.05 10 316 16 V ERY L ARGE  0.06 4 239 15 V ERY L ARGE  0.07 2 183 13 V ERY L ARGE  0.08 0 137 13 V ERY L ARGE  0.09 0 110 12 V ERY L ARGE  0.1 0 82 9 10691549 0.2 0 5 3 889883 0.3 0 0 3 149545 0.4 0 0 2 32528 0.5 0 0 1 8057 0.6 0 0 0 2074 0.7 0 0 0 529 0.8 0 0 0 149 0.9 0 0 0 31 1.0 0 0 0 0  As only infrequent 1-Item sets are pruned before inserting into the tree, there may be k-itemsets in the tree which are not frequent. This overhead is compromised for the reduced processing time for constructing tree to enable the algorithm for real-time processing  Fig. 1  Memoary Utilized for Constructing Itemset Tree using AMEIT and MEIT Algorithms 2nd International Conference on Applied and Theoretical Computing and Communication Technology iCATccT 241 


 Fig. 2  Processing Time Taken for Constructing Itemset Tree using AMEIT and MEIT Algorithms VI  C ONCLUSION AND F UTURE W ORK  This paper described the Approximation based Incremental Memory Efficient Itemset Tree \(AIMEIT\which is a compact data structure for storing itemsets facilitating dynamic querying for frequent itemsets and association rules. The tree prunes only 1-itemsets from the transactions before inserting them into the tree to avoid pruning of itemsets which can be potential frequent items in future. The algorithm is very robust for sparse data and its performance reduces for highly dense data. Setting higher minimum support of interest SI shall make the algorithm efficient for dense data but a mechanism shall be employed to detect the density of the data and adjust the SI accordingly In the future work, the algorithm shall be made adaptive to both sparse and dense data, and improve the pruning strategy The algorithm can be redesigned for parallel processing on large cluster of computers scaling it for Big Data Stream References 1  L. Jian-ping, W. Ying, and Y. Fan-ding, çIncremental mining algorithm Pre-FP in association rules based on FP-tree,é in Proc. Of Int. Conf. on Networking and Distributed Computing \(ICNDC\gzhou, Oct 2010 pp. 199-203 2  T. P. Le, T. P. Hong, B. Vo and B. Le, çAn Efficient Incremental Mining Approach Based on IT-Tree,é in Proc. IEEE RIVF Int. Conf. on Computing and Communication Technologies, Research, Innovation and Vision for the Future, Ho Chi Minh City, Mar 2012, pp. 1-5 3  C. K. S. Leung, Q. I. Khan and T. Hoque, çCanTree: a tree structure for efficient incremental mining of frequent patterns,é in Proc. IEEE Int Conf. on Data Mining, Nov 2005 4  R. K. Roul and I. Bansal, çGM-Tree: An efficient frequent pattern mining technique for dynamic databaseé, in Proc. Of Int. Conf. on Industrial and Information System \(ICIIS\ Gwalior, Dec 2014, pp. 1-6 5  C. H. Yang and D. L. Yang, çIMBT--A Binary Tree for Efficient Support Counting of Incremental Data Miningé, in Proc. of Int. Conf. on Computational Science and Engineering, Aug 2009, pp. 324-329 6  E. Baralis, T. Cerquitelli, S. Chiusano and A. Grand, çArray-Tree: A persistent data structure to compactly store frequent itemsetsé, in Proc IEEE Int. Conf on Intelligent Systems, Jul 2010, pp. 108-113 7  Chen Zuyi and Zhao Taixiang, çA frequent itemset storing structureé, in Proc. of Int. Conf. on Image Analysis and Signal Processing \(IASP 2011, pp. 680-683 8  L. Yadav and P. S. Nair, çA new data structure for finding maximum frequent itemset in online data miningé, in Proc. of Int. Conf. on Computer, Communication and Control \(IC4 9  M. Kubat, A. Hafez, V. V. Raghavan, J. R. Lekkala and Wei Kian Chen Itemset trees for targeted association queryingé, IEEE Transactions on Knowledge and Data Engineering, vol. 15, issue. 6, Dec 2003, pp. 15221534 10  P Fournier-Viger, E Mwamikazi and T Gueniche, çMEIT: Memory Efficient Itemset Tree for Targeted Association Rule Miningé, in Proc Int. Conf. on Advanced Data Mining and Applications, Hangzhou, Dec 2013, pp. 95-106 11  Frequent Itemset Mining Implementations Repository http://fimi.ua.ac.b able: http://fi m i ua.ac.be  242 2nd International Conference on Applied and Theoretical Computing and Communication Technology iCATccT 


         


that case our proposed mining algorithm outperforms the conventional Apriori algorithm Fig. 3. TicTacToe data   Fig. 4. T8I5D100K   Fig. 5. T6I4D100K  V  CONCLUSIONS  AND  SUMMARY In this paper, we have developed a genetic based approach and compared the results with the results given by the Apriori algorithm for mining maximal frequent item sets. We have obtained the results through experimental analysis on real data sets using both of these algorithms. Several advantages have been demonstrated by the experimental analysis of this algorithm in comparison with Apriori algorithm, which are as follows 200  It gives better results than the Apriori algorithm by accessing large data sets for less numbers of nodes especially when the support value is set low by the users 200  For large data sets and low support value, both of these algorithms give the same solution by giving the same number of maximal frequent item sets. To get this solution Apriori considers a large number of candidate item sets with respect to a genetic based approach 200  For large data sets and high support values, Apriori performs better than a genetic based approach, since the genetic algorithm uses global search mechanism Apriori uses a level by level search procedure and it gets the solution by accessing less numbers of nodes because solution is near the root node. The nodes close to the root of lexicographic tree have higher support values 200  Low support value generates a long size frequent pattern which provides information like frequency of an exponential number of smaller sub patterns. In that case a genetic based approach performs better than other existing algorithms 200  The experimental results of a genetic based approach demonstrate the effect of generations of individuals, and prune all the subsets and supersets in a lexicographic tree, which is cost effective in the case of counting the support value and reducing the search space dramatically The other areas which should be focused for further research include developing genetic operators in such a way that it will help to reduce generating the same individuals and it can increase valid individuals for further generation Considering repetitive individuals sometimes takes a longer time to get the solution A CKNOWLEDGMENT  This research work was funded by School of Engineering and ICT, University of Tasmania, Australia, and website http://www.utas.edu.au/cricos, under CRICOS Provider Code 00586B  R EFERENCES  1  M M. J  K a bir  S  X u  B. H  K a ng a nd Z  Z h ao 223 A N o ve l Approach to Mining Maximal Frequent Itemsets Based on Genetic Algorithm,\224 in International Conference on Information Technology and Applications \(ICITA 2014 2  R C. A g ar w al C. C A g g ar w al and V V  V P r asa d  223 A Tr ee  Projection Algorithm For Generation of Frequent Itemsets,\224 Parallel Distrib. Comput. Spec. Issue High Perform. Data Min vol 61, no. 3, pp. 350\226371, 2001 3  A  S a l leb  Z M a a z ou zi  and C V r a in  223Mi n i n g M a xi m a l F r eq u e nt Itemsets by a Boolean Based Approach,\224 in European Conference on Artificial intelligence 2002, pp. 285\226289 4  J  H a n, J  P e i a n d Y  Y i n 223 M i n i n g F r e que nt P a tte r n s w i tho u t  Candidate Generation,\224 ACM SIGMOD vol. 29, no. 2, pp. 1\22612 2000 5  J  H i p p  U  G 374ntz e r a nd G  N a kh ae iz ade h 223 A l g or ithm s f o r  association rule mining\227a general survey and comparison,\224 ACM sigkdd Explor. \205 vol. 2, no. 1, pp. 58\22664, 2000 6  R. J  K u o an d C  W  S h i h 223 A s s o ciati o n r u l e m i ni ng t h r o ug h the a n t  colony system for National Health Insurance Research Database in 44 


Taiwan,\224 Comput. Math. with Appl vol. 54, no. 11\22612, pp. 1303\226 1318, Dec. 2007 7 J  H Holla n d   Adaptation in Natural and Artificial Systems Ann Arbor: University of Michigan Press, 1975 8  K  F M a n  K S  T a n g   a n d S  Kwo n g  223 G e n e t i c Al g o r i t h m s 037   Concepts and Applications,\224 IEEE Trans. Ind. Electron vol. 43 no. 5, 1996 9  D  Be as l e y  D  R. B u l l  an d R R Ma r tin 223 A n O v e r v iew of  G e ne tic  Algorithms\037: Part 1 , Fundamentals,\224 Univ. Comput vol. 15, no. 2 pp. 58\22669, 1993   M  S r i n i v a s an d L  M  P a tn ai k 223G e n et i c A lgo r i t h m s A S u r v ey  224  Computer \(Long. Beach. Calif vol. 27, no. 6, pp. 17\22626, 1994  D E  G o ld b e rg  Genetic Algorithms in Search, Optimization and Machine Learning Addison-Wesley Longman Publishing Co., Inc Boston, MA, USA, 1989  Z  M i ch a l ew i c z  Genetic algorithms + data structures = evolution programs Berlin: Springer, 1992  R  A gra w a l a n d R  Sri kan t  223F a s t A l gori t h m s for M i n i n g  Association Rules,\224 in 20th International Conference on Very Large Data Bases 1994, pp. 487\226499 14  D  I  L i n and Z  M. K e de m  223 P ince r S e a r c h A  N e w  A l go r ithm f o r  Discovering the Maximal Frequent Set,\224 in 6th International Conference on Extending Database Technology   R  J   B a y a rd o 223E ffic i e nt ly M i ni n g L on g Pa tt ern s from D a t a b a s e s  224 ACM SIGMOD pp. 85\22693, 1998 16  R  C  A g a r w a l  C  C  A g g a r w a l  a n d V  V  V  P r a s a d  223 D e p t h f i r s t  generation of long patterns,\224 Proc. sixth ACM SIGKDD Int. Conf Knowl. Discov. data Min. - KDD \22200 vol. 2, pp. 108\226118, 2000  D Bu rdi c k   M  Ca li m l i m   an d J   Geh r k e  223M AFI A a m a xi m a l  frequent itemset algorithm for transactional databases,\224 Proc. 17th Int. Conf. Data Eng no. X, pp. 443\226452 18  K   G o uda an d M. J  Z a ki, \223 G e n Max 037   A n E f f icie n t A l go r ithm f o r  Mining,\224 Data Min. Knowl. Discov vol. 11, no. 3, pp. 223\226242 2005    A l a t a and E. Akin, \223An efficient genetic algorithm for automated mining of both positive and negative quantitative association rules,\224 Soft Comput vol. 10, no. 3, pp. 230\226237, Apr 2005 20  W D o u  J  Hu  K Hi r a s a wa   a n d  G Wu  223 Q u i c k r e s p on s e d a t a  mining model using genetic algorithm,\224 2008 SICE Annu. Conf pp 1214\2261219, Aug. 2008  A  Sa ll e b a ou i ssi C Vra i n   C  Norte t  X K o n g  a n d D  C a ss a r d  223QuantMiner for Mining Quantitative Association Rules,\224 Mach Learn. Res vol. 14, no. 1, pp. 3153\2263157, 2013 22  A  S a l le b a o u is s i C  V r ai n an d C. N o r te t, \223 Q ua n tM ine r 037  A  Genetic Algorithm for Mining Quantitative Association Rules,\224 in 20th International Joint Conference on Artificial Intelligence 2007 pp. 1035\2261040  J  Hua n g Y  Ch e-t s un g  a nd  C  F u 223 A Gen e t i c A l g ori t h m  B a s e d Searching of Maximal Frequent Itemsets,\224 in International conference on artificial intelligence 2004    45 


0.80 0.76 0.72 0.68 0.64 0.60 0.56 0.52 0.48 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0   Apriori-AP counting speedup Apriori-AP overall speedup  Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time 
0.1 1 10 100 3000 2000 1000 0 1000 2000 3000 4000 5000 
0.6 0.5 0.4 0.3 0.2 0.1 0.0 10 100 
1 10 100 6000 4000 2000 0 2000 4000 6000 8000 10000 
0.20 0.19 0.18 0.17 0.16 0.15 0.14 0.13 0.12 0.11 0.10 0.09 0.08 0.07 0.06 0.05 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0 Apriori-AP counting speedup Apriori-AP overall speedup  Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time  0 10000 5000 15000 c Webdocs Figure 5 The performance results of Apriori-AP on three real-world benchmarks DP time SR time and CPU time represent the data process time on AP symbol replacement time on AP and CPU time respectively Webdocs switches to 16-bit encoding when relative minimum support is less then 0.1 8-bit encoding is applied in other cases 
E vs Eclat Eclat et al Eclat Eclat 
0.80 0.75 0.70 0.65 0.60 0.55 0 100 200 300 400 Computation Time \(s Relative minimum support 1.0X Symbol replacement time 0.5X Symbol replacement time 0.1X Symbol replacement time Figure 7 The impact of symbol replacement time on Apriori-AP performance for Pumsb how symbol replacement time affects the total AprioriAP computation time A reduction of 90 in the symbol replacement time leads to 2.3X-3.4X speedups of the total computation time The reduction of symbol replacement latency will not affect the performance behavior of AprioriAP for large datasets since data processing dominates the total computation time 
Apriori Eclat Equivalent Class Clustering   is another algorithm based on Downward-closure uses a vertical representation of transactions and depth-ìrst-search strategy to minimize memory usage Zhang  proposed a h ybrid depth-ìrst/breadth-ìrst search scheme to expose more parallelism for both multi-thread and GPU versions of  However the trade-off between parallelism and memory usage still exists For large datasets the nite memory main or GPU global memory will become a limiting factor for performance and for very large datasets the algorithm fails While there is a parameter which can tune the trade-off between parallelism and memory occupancy we simply use the default setting of this parameter for better performance Figure 8 shows the speedups that the sequential 
0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0 AP counting speedup Apriori-AP overall speedup    Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time  b Accidents 
T40D500K Counting T40D500K Overall T100D20M Counting T100D20M Overall Webdocs5X Counting Webdocs5X Overall Speedup Relative Minimum Support Figure 6 The speedup of Apriori-AP over Apriori-CPU on three synthetic benchmarks 
1 10 100 
a Pumsb 
696 


0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.1 1 10 Pumsb Accidents T40D500K Webdocs Webdocs5X T100D20M ENWiki Speedup Relative Minimum Support 
Figure 8 The performance comparison of CPU sequential and algorithm achieved with respect to sequential Apriori-CPU Though has 8X performance advantage in average cases the vertical bitset representation become less efìcient for sparse and large dataset high trans and freq item ratio This situation becomes worse as the support number decreases The Apriori-CPU implementation usually achieves worse performance than  though the performance boost of counting operation makes Apriori-AP a competitive solution to parallelized  Three factors make a poor t for the AP though it has better performance on CPU 1 requires bit-level operations but the AP works on byte-level symbols 2 generates new vertical representations of transactions for each new itemset candidate while dynamically changing the values in the input stream is not efìcient using the AP 3 Even the hybrid search strategy cannot expose enough parallelism to make full use of the AP chips Figure 9 and 10 show the performance comparison between Apriori-AP 45nm for current generation of AP and sequential multi-core and GPU versions of  Generally Apriori-AP shows better performance than sequential and multi-core versions of  The GPU version of shows better performance in Pumsb Accidents and Webdocs when the minimum support number is small However because of the constraint of GPU global memory Eclat-1G fails at small support numbers for three large datasets ENWiki T100D20M and Webdocs5X ENWiki as a typical sparse dataset causes inefìcient storage of bitset representation in Eclat and leads to early failure of EclatGPU and up to 49X speedup of Apriori-AP over Eclat-6C In other benchmarks Apriori-AP shows up to 7.5X speedup over Eclat-6C and 3.6X speedup over Eclat-1G This gure also indicates that the performance advantage of AprioriAP over GPU/multi-core increases as the size of the dataset grows The AP D480 chip is based on 45nm technology while the Intel CPU Xeon E5-1650 and Nvidia Kepler K20C on which we test are based on 32nm and 28nm technologies respectively To compare the different architectures in the same semiconductor technology mode we show the performance of technology projections on 32nm and 28nm technologies in Figure 9 and 10 assuming linear scaling for clock frequency and square scaling for capacity The technology normalized performance of Apriori-AP shows better performance than multi-core and GPU versions of Eclat in almost all of the ranges of support that we investigated for all datasets with the exception of small support for Pumsb and T100D20M Apriori-AP achieves up to 112X speedup over Eclat-6C and 6.3X speedup over Eclat-1G The above results indicate that the size of the dataset could be a limiting factor for the parallel algorithms By varying the number of transactions but keeping other parameters xed we studied the behavior of Apriori-AP and Eclat as the size of the dataset increases Figure 11 For T100 the datasets with different sizes are obtained by the IBM synthetic data generator For Webdocs the different data sizes are obtained by randomly sampling the transactions or by concatenating duplicates of the whole dataset In the tested cases the GPU version of fails in the range from 2GB to 4GB because of the nite GPU global memory Comparing the results using different support numbers on the same dataset it is apparent that the smaller support number causes Eclat-1G to fail at a smaller dataset This failure is caused by the fact that the ARM with a smaller support will keep more items and transactions in the data preprocessing stage While not shown in this gure it is reasonable to predict that the multicore implementation would fail when the available physical memory is exhausted However Apriori-AP will still work well on much larger datasets assuming the data is streamed in from the hard drive assuming the hard drive bandwidth is not a bottleneck VII C ONCLUSIONS AND THE F UTURE W ORK We present a hardware-accelerated ARM solution using Micronês new AP architecture Our proposed solution includes a novel automaton design for matching and counting frequent itemsets for ARM The multiple-entry NFA based design was proposed to handle variable-size itemsets MENFA-VSI and avoid routing reconìguration The whole design makes full usage of the massive parallelism of the AP and can match and count up to itemsets in parallel on an AP D480 48-core board When compared with the based single-core CPU implementation the proposed solution shows up to 129X speedup in our experimental 
Apriori Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat F Normalizing for technology Eclat G Data size Eclat Eclat Eclat Apriori 
18 432 
 
697 


0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 1 10 100 1000 10000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm a Webdocs\(1.4GB 0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 10 100 1000 10000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails d Webdocs5X 7.1GB Figure 10 Performance comparison among Apriori-AP Eclat-1C Eclat-6C and Eclat-1G with technology normalization on four large datasets 
0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(second Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm b Accidents 34MB 0.40 0.36 0.32 0.28 0.24 0.20 0.16 0.12 0.08 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm c T40D500K\(49MB Figure 9 Performance comparison among Apriori-AP Eclat1C Eclat-6C and Eclat-1G with technology normalization on three small datasets results on seven real-world and synthetic datasets This APaccelerated solution also outperforms the multicore-based and GPU-based implementations of 0.60 0.55 0.50 0.45 0.40 0.35 0.30 0.25 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails c T100D20M 6.3GB 
ARM a more efìcient algorithm with up to 49X speedups especially on large datasets When performing technology projections on future generations of the AP our results suggest even better speedups relative to the equivalent-generation of CPUs and GPUs Furthermore by varying the size of the datasets from small to very large our results demonstrate the memory constraint of parallel ARM particularly for GPU 
Eclat Eclat 
0.80 0.76 0.72 0.68 0.64 0.60 0.56 0.52 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm 
0.20 0.15 0.10 0.05 0.00 1 10 100 1000 10000 100000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(second Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails b ENWiki\(3.0GB 
a Pumsb 16MB 
698 


 Frequent pattern mining Current status and future directions  2007  R Agra w al and R Srikant F ast algorithms for mining association rules in  1994  M J Zaki Scalable algorithms for association mining  vol 12 no 3 pp 372Ö390 2000  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation in  2000  Y  Zhang F  Zhang and J Bak os Frequent itemset mining on large-scale shared memory machines in  2011  F  Zhang Y  Zhang and J D Bak os  Accelerating frequent itemset mining on graphics processing units  vol 66 no 1 pp 94Ö117 2013  Y  Zhang  An fpga-based accelerator for frequent itemset mining  vol 6 no 1 pp 2:1Ö2:17 May 2013  P  Dlugosch  An efìcient and scalable semiconductor architecture for parallel automata processing  vol 25 no 12 2014  I Ro y and S Aluru Finding motifs in biological sequences using the micron automata processor in  2014  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases in  1993  H No yes Micronês automata processor architecture Reconìgurable and massively parallel automata processing in  June 2014 keynote presentation  M J Zaki  Parallel data mining for association rules on shared-memory multi-processors in  1996  L Liu  Optimization of frequent itemset mining on multiple-core processor in  2007  I Pramudiono and M Kitsure ga w a P arallel fp-gro wth on pc cluster in  2003  E Ansari  Distributed frequent itemset mining using trie data structure  vol 35 no 3 p 377 2008  W  F ang  Frequent itemset mining on graphics processors in  2009  B Goethals Surv e y on frequent pattern mining  Univ of Helsinki Tech Rep 2003  C Bor gelt Ef cient implementations of apriori and eclat in  2003 p 90  Frequent itemset mining dataset repository   http mi.ua.ac.be/data  J Rabae y  A Chandrakasan and B Nik oli  c  2nd ed Pearson Education 2003 
100 1000 10000 5 50 500 5000 re_sup = 0.12 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails for re_sup = 0.08 re_sup = 0.08 
GPU fails re_sup = 0.42 re_sup = 0.42 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm  Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails re_sup = 0.3 re_sup = 0.3 b T100 Figure 11 Performance prediction with technology normalization as a function of input size implementation In contrast the capability of our AP ARM solution scales nicely with the data size since the AP was designed for processing streaming data With the challenge of the big data era a number of other complex pattern mining tasks such as frequent sequential pattern mining and frequent episode mining have attracted great interests in both academia and industry We plan to extend the proposed CPU-AP infrastructure and automaton designs to address more complex pattern-mining problems A CKNOWLEDGMENT This work was supported in part by the Virginia CIT CRCF program under grant no MF14S-021-IT by C-FAR one of the six SRC STARnet Centers sponsored by MARCO and DARPA NSF grant EF-1124931 and a grant from Micron Technology R EFERENCES  J Han 
et al Data Min Knowl Discov Proc of VLDB 94 IEEE Trans on Knowl and Data Eng Proc of SIGMOD 00 Proc of CLUSTER 11 J Supercomput et al ACM Trans Reconìgurable Technol Syst et al IEEE TPDS Proc of IPDPSê14 Proc of SIGMOD 93 Proc of Fifth International Symposium on Highly-Efìcient Accelerators and Reconìgurable Technologies et al Proc of Supercomputing 96 et al Proc of VLDB 07 Proc of PAKDD 03 et al IAENG Intl J Comp Sci et al Proc of DaMoN 09 Proc FIMI 03 Digital Integrated Circuits 
1 10 100 1000 10000 0.1 1 10 100 1000 
a Webdocs 
699 


