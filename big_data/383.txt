Mining Generalized Association Rules Using Pruning Techniques Yin-Fu Huang and Chiech-Ming Wu Institute ojElectronics and Information Engineering National Yunlin Universi ojScience and Technology Email huan~j~el.yirntech.edu iw Abstract The goal of the paper is to mine generalized association rules usingpruning techniques Given a large transaction database and a hierarchical fmonomy tree of the items we try tofind the association rules between the items at dxerent levels in the faronomy free under the assumption that 
original frequenf itemsets and association rules have already been generated beforehand In the proposed algorithm GMR we use join methods and pruning techniques to generate new generalized association rules Through several comprehensive experiments we find that the GMR algorithm is much better than BASIC and Cumulafe algorithms 1 Introduction In recent ten years with the developments of information and advances of Internet, vast data have Seen propagated and recorded in the databases of different applications Currently data mining is arising to 
understand analyze and use these data Data mining is designed for finding interesting samples in a large database and thus it is also the knowledge exploration center part 3 51 One of the major tasks of data mining is to find association rules for helping retail industries to understand the consumers\222 behaviors I 21 In data mining the most well-known method is the Apriori algorithm Z Since the Apriori algorithm is costly to find candidate itemsets many researches have been trying to improve it i.e reducing the size 
of candidate itemsets For example DHP algorithm is one using a hash-based skill to reduce the size of candidate itemsets 9 Besides someone used transaction reduction techniques including partitioning proposed by Savasere et al IO and sampling proposed hy Toivonen 1141 to generate frequent itemsets quickly. What\222s more a kind of data structure called FP-Tree \(Frequent Pattern Tree was proposed by Han et al to directly produce frequent itemsets not candidate itemsets 6 The motivation of our research is initiated with two situations One is 
that although mining association rules has completed, but it did not consider the taxonomy tree at that time. Another is that although the taxonomy tree was considered it may need to be adjusted as time goes by and thus the generalized association rules mined before are not useful any more Till now all proposed researches can only mine all association rules from scratch In the paper our goal is to find generalized association rules without re-scanning the original database Here the pre-knowledge including the taxonomy tree the original frequent itemsets and association rules both generated beforehand can be used to 
mine new generalized association rules The data type used in the paper is a transactional data form 7 At first we create maximal itemsets S 131 from the original frequent itamsers Next given a hierarchical taxonomy tree 4 E we must calculate the supports of the non-leaf items in the taxonomy tree from the maximal itemsets or VTV table S 1 I Then we use the original frequent itemsets L and L2 and non-leaf items with the supports 2 the minimum support 
to create an association graph In our paper different from those ones S IS we assume the minimum supports of all the items are the same Finally in our algorithm GMAR Generalized Mining Association Rules we use join methods and pruning techniques to generate new generalized association rules In the experiments we find that the GMAR algorithm is much better than BASIC and Cumulate algorithms 12 since it generates fewer candidate itemsets and furthermore prunes a large amount of irrelevant rules based on the minimum confidence The 
remainder of the paper is organized as follows. In Section 2 the mining problem for generalized association rules is defined We propose the GMAR algorithm using the original frequent itemsets and association rules to find generalized association rules in Section 3 In Section 4 several experiments are undertaken and the results show the superiority of the GMAR algorithm over BASIC and Cumulate algorithms Finally we make a conclusion in Section 5 2 Problem descriptions 227 0-7695-1754-4/02 17.00 Q 2002 IEEE 222   


The problem investigated here is to mine generalized association rules according to a large transaction database D and a hierarchical taxonomy tree H of the purchased items Transaction database D is a set of transactions where each transaction T consists of a set of items, called an itemset, such that T L 1 Here 1 i i2 i  im is a set of the items purchased by all the transactions. Besides hierarchical taxonomy tree H is a directed acyclic graph on the items where an edge in H represents an is-a relationship between items For example if there is an edge from p to c in H we call p a parent of c and c a child of p In other words we can say p represents a generalization of c Further if there is a path from d to e in the transitive-closure of H we call 3 an ancestor of e and e a descendant of 2 Since W is acyclic any item cannot be an ancestor of itself Besides all the leaf items in H must come from I Recent researches are getting interested in finding the association rules spanning several different levels in the taxonomy tree here called generalized association rules A generalized association rule is an implication of the form X  Y where X EI Y GI X n Y  0 and no item in Y is an ancestor of any item in X If c of the transactions in D that support X also support U we say that X Y holds in the transaction database D with confidence c The definition of Support\(X is the number of transactions purchasing X, divided by the number oftransactions in the database as shown in formula I The definition of Conf\(X 3 Y is the support of both antecedent and consequent divided by the support of antecedent in the rule as shown in formula 2 Support\(X  X.CO  1 conf\(x y  support\(X Y upport\(x 2 Definitely we can find generalized association rules from scratch given a large transaction database D and a hierarchical taxonomy tree H of the purchased items However it is not avoidable to scan the original database once more and this will be very inefficient Thus an efficient mining algorithm proposed here is to make use of the original frequent itemsets and association rules that have already been generated beforehand to produce new generalized association rules rather than re-scanning the original database. Besides we also propose some pruning techniques to speed up the mining process 3 Mining algorithms 3.1 Processing flow of mining algorithms The processing flow of our mining algorithm for finding generalized association rules is shown in Figure 1 Supposed that the components shown inside the dotted box such as vertical-TID-vector table, frequent itemsets and association rules were generated before our mining algorithm is executed Rather than scanning the original database we make use of the vertical-TID-vector table transformed from the original database to mine generalized association rules Since the bit storage is used in the vertical-TID-vector table the memory space and processing time to get the database information will be saved Besides the original frequent itemsets and association rules generated beforehand should not be thrown away since we need them to produce generalized association rules Figure 1 Processing flow of mining algorithms At first we create maximal itemsets from the original frequent itemsets and then store them in an array to reduce disk space and speed up the mining operation Although no subsets of a maximal itemset are stored we can use a hash function to calculate the position of each subset in a maximal itemset and find its support. Next given a hierarchical taxonomy tree we must calculate the supports of the non-leaf items in the taxonomy tree from the maximal itemsets or VTV table Basically for a non-leaf item I being the ancestor of i i2  i where items i i2  and in are leaf-items the support of item f can found in the array ofmaximal itemset I if i iZr  in is a subset of maximal itemset I Then, we use the original frequent itemsets L and L2 and non-leaf items with the supports 5 the minimum support to create an association graph It facilitates check whether k-itemsets k L 3 involving non-leaf and leaf items are frequent or not Finally in our algorithm GMAR, we use join methods and pruning techniques to generate new generalized association rules The join methods used in the GMAR algorithm can directly produce generalized association rules from the original association rules, and the pruning techniques are used to prune irrelevant rules thereby speeding up the production of generalized association rules 3.2 The vertical-TID-vector table The vertical-TID-vector table was generated before the 228 


mining algorithm is executed It uses bit vectors to record the transaction information. Each item is represented with a bit vector where the length of the bit vector is the total number of transactions in the database If an item appears in the j transaction the j bit of the corresponding bit vector is set to 1 otherwise the bit is set to 0 As shown in Figure 2 and 3 Since item i3 appears in transactions 1 8 9 and IO the bit vector for item i can be expressed as 1000000111 The bit vector facilitates the support computation of an itemset  i iz i3  ik Figure 3 Vertical-TID-vector table 3.3 Creating maximal itemsets A maximal itemset is defined as a frequent itemset not contained in any other frequent itemset For example given a set offieqnent itemsets ill iz\{i3 i4 is itemsets are  i2 is and il i3 is As shown in Figure 1 the procedure to find the maximal itemsets from a set of frequent itemsets is described as follows Gen-Max-ltemset Step 1 Initialize all frequent itemsets unmarked Step 2 Set k the maximal length of frequent itemsets Step 3 For each unmarked frequent k-itemsets Step 3.1 Generate all the proper subsets Step 3.2 If any frequent h-itemset p h  k is one ofthe subsets markp Step 4 k=k-l and then repeat from Step 3 until k=l Step 5 The maximal itemsets are those not marked so far An example as shown in Figure 4 illustrates how to find the maximal itemsets from a set of frequent itemsets it i3 il, is iz is i3 id it i3 is the maximal where the unmarked itemsets are the maximal ones i.e id iz id and il i3 id Figure 4 Finding maximal itemsets Here we only need to store the maximal itemsets as shown in Figure 5 to reduce disk space and speed up the mining operation In the figure the supports of all non-empty subsets of maximal itemset il i3 is are 0.5 for il 0.4 for i3 0.7 for is 0.3 for i i3 0.5 for i is 0.3 for i3 is and 0.3 for il i3 is respectively Instead of storing all non-empty subsets of a maximal itemset we can use the hash function to calculate the position of any subset in a maximal itemset and find its support For example through the hash function we can compute the value 5 for the position of i is in maximal itemset i i3, is and find its support value 0.5 The hash function used here can he expressed as follows T the length of a maximal itemset L the length of a subset itemset X\(i the position in a maximal itemset for the ith item of a HE the position of a subset item in a maximal itemset subset itemset where 1  i  L and X\(O 14 10.3 I i is 10.4 10.7 10.4 I i i3 is 10.5 10.4 10.7 10.3 10.5 10.3 10.3 I Figure 5 The maximal itemsets 3.4 Calculating the supports of non-leaf items Given the original frequent itemsets and association rules already generated beforehand and a hierarchical taxonomy tree we can produce new generalized association rules rather than re-scanning the original database However the items, except those at the leaf level in the taxonomy tree do not appear in the original frequent itemsets and association rules. Therefore we must calculate the supports of the non-leaf items in the taxonomy tree from the maximal itemsets or VTV table For a non-leaf item I being the ancestor of it i2 _ i where items i i2  and i are leaf-items the support of 229 


item f can be calculated as follows Support\(r in i,,i2  in isa subset of maximal itemset I or Support\(1  Ivector\(i OR _ OR vector\(i of trans Here we take an example to illustrate how the supports of non-leaf items are calculated. Given the VTV table and the taxonomy tree as shown in Figure 3 and 6 we can calculate the supports of non-leaf items 1001 1002 and 1003 as follows Support\( 1001  Ivector 1 OR vector\(2 of trans  Ol1001111  0.7 Support\(1002  Ivector\(1 OR vector\(2\OR vector\(3 no of trans  I 11 lOOlllll]~/lO  0.8 Support\(1003  jvector\(4 OR vector\(5 of trans  011111 I11 l]l/lO  0.9 AA 1 2 Figure 6 Taxonomy tree 3.5 Creating association graphs In our mining algorithm different from the Apriori algorithm we only use the original frequent itemsets L and L2 to generate k-itemsets k b 3 involving non-leaf and leaf items and check whether they are frequent or not Our method is based on a graph called association graph to search all these k-itemsets The association graph is defined as follows A graph AG\(V E called association graph, consists a set of vertices or items V and a set of edges E where 1 V  L U T where T  v I v is a non-leaf item and support\(v 2 the minimum support and 2 E  w I U v in Lzf U x-y I at least one item of x y is in T x and y are not in the ancestor-offspring relationship and support\(\(x y b the minimum support Given the VTV table and the taxonomy tree as shown in Figure 3 and 6 the minimum support 0.3 and the original frequent itemsets LI and Lz we can obtain the frequent itemsets T and then create the association graph as shown in Figure 7 LI  1,2,3,4,51 Lz I 31 I 51 2 51 3,511 T IOOl 1002,1003 Support I 1003  0.5 Support\(\(2, 1003  0.4 Support\(\(3 IOOI  0.3 Support\(\(3, 1003  0.3 Support\(\(5 lOOl Support\(\(S,lO02 0.7 Supp0rt\({1001,1003  0.7 Support\(\( 1002,1003 0.7 Support\(\(4 IOOI support\(\(4,1oo2 1002 003 5 3 Figure 7 Association graph 3.6 Pruning techniques In the GMAR algorithm to be discussed in the next section we use pruning techniques to generate new generalized association rules since pruning irrelevant rules can speed up the production of generalized association rules Here we have six pruning techniques described as follows PT 1 For a frequent itemset I where i is the item with the minimum support wunt within it if support\(\(i subset\(l is less than the minimum confidence then we can prune any rule subset\(1  I  subset\(1 Rationale For any rule subset\(1 3 I  subset\(l the confidence check is support\(l subset\(l Since support\(l must be less than or equal to support\(\(i support\(I subset\(I is definitely less than the minimum confidence as long as support\({i subseqI is less than the minimum confidence In other words we can use support\(\(i instead of support\(1 in the confidence check PT 2 For a subset P in a frequent itemset I if the rule P  I  P holds then any rule whose antecedent containing P such as superset\(P I  supenet\(P holds as well Rationale If the rule P a I  P holds it implies that 230 


support\(l P is more than or equal to the minimum confidence Nevertheless since support\(superset\(P is less than or equal to support\(P support\(l superset\(P is definitely more than or equal to the minimum confidence and thus the corresponding rule holds as well PT 3 For two rules with the same antecedent if 1 the rule P  QI holds and 2 the rule P  42 does not hold due to support\(P U Q2  the minimum support then all the rules corresponding to the itemset 1  P U QI U Q2 do not hold Rationale If one of the rules corresponding to the itemset I holds, then I must be a frequent itemset and all the subsets are frequent itemsets as well However we know P U Q2 is not a frequent itemset since support\(P U Q2 is less than the minimum support Thus all these rules corresponding to the itemset I do not hold For example if 1 the rule i i2 holds and 2 the rule i 3 i3 does not hold due to support\(\(i i3 c the minimum support, then all the rules such as i r i2h i3 i2 3 i A i3 i3 3 i A i2 i2 A i 3 i i A i3 3 i2 and i A i2 3 i3 do not hold PT 4 If the rule P 3 Q holds then the rule P 3 42 always holds where Q2 is derived from Q1 by replacing some items in QI with their ancestors Rationale If the rule P 3 Ql holds it implies that support\(P U Q P is more than or equal to the minimum confidence Since support\(Q2 is more than or equal to definitely more than or equal to the minimum confidence and thus the rule P a Q2 always holds support\(QJ support\(P U Qz  is PT 5 For two rules with the same antecedent if 1 the rule P 3 Q holds and 2 the rule P  Q2 does not hold due to not satisfying the minimum confidence although it satisfies the minimum support then the rule Pa Q U Q2 does not hold Rationale For the confidence check support\(P U Q2 P  the minimum confidence support\(P U Q U P\is definitely less than the minimum confidence since support\(P U QI U Q2 is less than or equal to support\(P U Q2 Thus the rule P QI U Q2 does not hold PT 6 If any item in a frequent itemset 1 does not appear in association graph AG or all the items in I do not form a complete connection in AG then for a subset P of I the rule P 3 I  P does not holds Rationale If the rule P 3 I  P holds P is not only more than or equal to the minimum confidence but also support\(1 should be more than or equal to the minimum support In other words, all the items in I would appear in AG and form a complete connection This contradicts the assumption and therefore the rule P 3 I  P does not holds 3.7 GMAR algorithm The Apriori algorithm proposed by Agrawal and Srikant is a two-step process which consists of join and pruning actions to find frequent itemsets and then uses the frequent itemsets to derive association rules In the section an algorithm GMAR Generalized Mining Association Rules is proposed which generates generalized association rules not directly based on the raw data from the database but based on the original frequent itemsets and association rules Here an association rule is called weak when it satisfies the minimum support threshold but not minimum confidence threshold. In the GMAR algorithm we need both strong and weak association rules in the current level k-l to generate the generalized association rules for from the next level k to the next 2k-3 level as shown in Figure 8 Therefore the generation of the generalized association rules is not based on a step-by-step manner The detailed algorithm is described as follows GMAR Algorithm Input VTV table, original association rules RJ taxonomy tree min-sup min-conf Output new generalized association rules Method Step 1 Generate new frequent I-itemsets L using VTV Step 2 Create association graph AG using original table and taxonomy tree frequent itemsets Lo and Lo2, and new frequent I-itemsets LnI x y is not in Lo2 Step 3 For each edge x--y in association graph AG where Add the rules 223x 3 y 224 and 223y 3 x\224 to k If support\(\(x y x 2 min-conf Set the rule \223x 3 y\224 strong If support\(\(x y y 2 min-conf Set the rule 223y 3 x\224 strong Step 4 For\(k  3 Rn\(k.,l 0 k Generate weak k rules using the original frequent k-1 LNk.1 and add them to cl GMAR-Genkk.1 1 k.i 3 23 1 


GMAR-Gen\(R,,w.t R,,\(x-i Step 5 R  uk r 1 r E  and r is a strong Procedure GMAR-Gen\(R R2 1 For each rule rl E RI For each rule r2 B R2  I\222 Using PT 5  If r,.antecedent  r,.antecedent and r,.consequent f r2.consequent\and conf\(rl t min-con0 and conf\(r2 5 min-con0 I Using PT 3 and PT 6 I Ifall the items in r,.antecedent U rl.consequent U r,.consequent can form a complete connection in association graph AG I Using PT 2 and PT 4  r  223r,.antecedent 2 r,.consequent i  length\(r A r2.consequent 224  Using PT 1  Add the ruler to ki If con t min-conf Set the ruler strong If r,.antecedent  r,.antecedent and r,.consequent  r,.consequent I Using PT 3 and PT 6  If all the items in r,.antecedent U r2.antecedent U r,.consequent can form a complete connection in association graph AG I Using PT 2 and PT 4  r  223rl.antecedent A rz antecedent i  length\(r Add the rule r to Kj If conf\(r t min-conf 1 r consequent\224 I Using PT 1  Set the rule r strong 1 Figure 8 GMAR algorithm to generate new generalized association rules Given the VTV table a5 shown in Figure 3 the original frequent itemsets Lo and Lo2 as shown in Figure 4 the original association rules  including the strong rules in and h3 the taxonomy tree as shown in Figure 6 the minimum support 0.3 and the minimum confidence 0.5 new generalized association rules R including the strong rules in Rd and R\223 can be generated using the GMAR algorithm as follows. Among the rules, a rule is marked if it is a strong one Ro2 Rules Confidence Rule Confidence 1-3 0.6 3=1@3\222  OL7s s~lool 1 1003a2 0.4444444 lool5s 1\222 1001=3 0.4285714   1003-3 0.3333333 Ro3 Rules Confidence Rules Confidence 1,3-5 1r 5=1,3 0.4285714  z join Rd U R join KO Rules l=3,1003 0.6 3a1,1003 0175 1,321003  1 J;IOO   0.6 100 1 23,5 0.42857 I4 1003sI  I 5,100123 0.4285714 3=S5,1001 0.75 523,1001 0.4285714 5*1001  1 t 00 1,1003=.3 0.42857 14 3,1001~S1  I 1001 3,1003 0.4285714 3=>l001,1003 0.75  1003~3,1001 0.3333333     Confidence Rules    Confidence 4=0 K4  r I r E 4 join  U join Rn3 and length\(r 4 0 4 Performance evaluations 232 


4.1 Simulation model In the section we evaluate the performances of the three algorithms including BASIC 1121 Cumulate 12 and GMAR on a DELL PowerEdge 4400 Server with Intel" Xeon Processor and 756MB main memory running Windows 2000 server All the experimental data are generated randomly and stored on a local 30GB SCSI Disk Ultra 160 with a RAID controller The relative simulation parameters are shown in Table 1 To make our data representative, we generate two types of databases in the experiments i.e DENSE databases and SPARSE databases. Each item in the DENSE database is randomly generated from a pool P called potentially frequent itemsets with size 300 while each item in the SPARSE database is randomly generated from a pool N i.e the set of all the items\with size 1000 Since the items in the DENSE database are more clustered than those in the SPARSE database, larger frequent itemsets will probably be produced in the DENSE database for the same minimum support Besides we use the notations T for average number of items per transaction I for average number of items in a frequent itemset, and D for number of transactions. For example the experiment labeled with 7lOB.DIK represents the simulation environment with IO items on the average per transaction 3 items on the average in a frequent itemset, and 1000 transactions in total Table 1 Simulation parameters with default values ID INumber of transactions 1000-500,000 7 INumber ofthe items per transaction 15-15 P INumber of potentially frequent 1300  litemsets I I Number of the items in a frequent 12-5 4.2 Experimental results Experiment 1 In the experiment we explore the execution time of BASIC Cumulate and GMAR algorithms for the environment 7lO.L3,DIK under different minimum support and minimum confidence pairs as shown in Figure 9 In the figure, we find that our algorithm GMAR is almost faster 2-16 times than BASIC especially for larger minimum support and minimum confidence pairs whereas Cumulate is only faster 1.3-1.5 times than BASIC although R Srikant and R Agrawal claimed that Cumulate runs faster 2-5 times than BASIC 1121 In general the larger the minimum support and minimum confidence pair is the faster the execution time of the three algorithms becomes To be fair to all algorithms, we have added the extra time of generating original frequent itemsets and association rules for GMAR However the time is helow 1 of total execution time thus we do not show it in the figure h n  B a 3 i.m.26 i.ma 1.~1.3 i.mm 1.740.3 I.w Figure 9 Execution time for different pairs minimum support  Confidence Experiment 2 In the experiment we extend Experiment 1 by fixing the minimum support IS and observe their variations For the minimum support 1.5 all the algorithms except GMAR are not sensitive to the changes of the minimum confidences as shown in Figure IO The reason is that larger minimum confidences will make GMAR prune more irrelevant rules. Nevertheless, GMAR is still in the first rank D  2 4M cm 22 CUlIIUlStt  gm BASIC 8 Irn 0 0 0.26 0.28 0.3 0.32 0.34 0.3 minimum confidence Figure 10 Execution time for different minimum confidences Experiment 3 In the experiment, we explore the execution time of the three algorithms for the environment ZlO.I3.DxK i.e different numbers of transactions generated in the SPARSE database and in the DENSE database as shown in Figure Il.\(a and b respectively. Both cases have the same minimum confidence 0.3 However to get comparable number of frequent itemsets, we set a smaller minimum support 1 in the SPARSE case and a larger 233 


minimum support 2 in the DENSE case As expected GMAR is still the hest one among them in the SPARSE and DENSE case especially when there are a huge amount of transactions From the both cases we find that much more frequent itemsets are generated in the DENSE database than in SPARSE database so that BASIC and Cumulate are not practicable candidates there B Ixa 8 Imo 222ixa 80 0 Imo 3033 m m loo30 number of transactions Figure 11 a Execution time for different numbers of transactions in the SPARSE database cumh?t  6 loo30  rma 2230 lorn m 5m 7cw IwD3 number of transactions Figure 1 l.\(b Execution time for different numbers of transactions in the DENSE database 5 Conclusions In the paper we try to find the association rules between the items at different levels in the taxonomy tree under the assumption that original frequent itemsets and association rules have already been generated beforehand The primary challenge is how to make use of the original frequent itemsets and association rules to directly generate new generalized association rules rather than rescanning the database In the proposed algorithm GMAR we use join methods and pruning techniques to generate new generalized association rules Through several comprehensive experiments we find that the GMAR algorithm is much better than BASIC and Cumulate algorithms since it generates fewer candidate itemsets and furthermore prunes a large amount of irrelevant rules based on the minimum confidence 6 Acknowledgments This research was supported in part by the National Science Council Taiwan under contract NSC-90-22 13-E-224-026 7 References I R Agrawal T Imielinski and A Swami 223Mining Association Rules between Sets of Items in Large Databases,\224 Pmc ACM International Conference on Mananement of Data  1993 pp 207-216 121 R Anrawal and R Srikant 223Fast Alaorithms for Mmine Adsociation Rules,\224 Pmc 2Vh Internationaiconference on Ve Large Data Bases 1994 pp 487-499 131 Yong-Jian Fu 223Data Mining,\224 IEEE Potentials Yol 16 No 4 1997 pp 18-20 141 Jia-Wei Han and Yong-Jian Fu 223Mining Multiplelevel Association Rules in Large Databases,\224 IEEE Transactions on Knowledge and Data Engineering Yo 11 No 5 1999 pp 798-805 5 Iia-Wei Han and Micheline Kamber Data Mining Concepts and Techniques Morgan Kaufmann Publishers 2001 6 Iia-Wei Han lian Pei and Yi-Wen Yin 223Mining Frequent Patterns without Candidate Generation,\224 Pmc ACM International Conference on Management o Data 2000 pp 1-12 7 Mon-Fong Jim Shian-Shyong Tseng and Shan-Yi Lia 223Data Types Generalization for Data Mining Algorithms,\224 Pmc IEEE International Conference on stems Man and Cybernetics 1999 pp 928-933 8 Bing Liu Wynne Hsu and Yi-Ming Ma 223Minin Association Rules with Multiple Minimum Supports,\224 Pmc 5 ACM International Conference on Knowledne Discovery and B DataMining 1999 pp 337-341 191 J S Park M S Cben and P S Yu 223An Effective  H&h-based Algorithm for Mining Association Rules,\224 Pmc ACM Internotional Conjerence on Mamgement o Data 1995 pp 175-186 IO A Savasere E Omiecinski, and S Navathe 223An Efficient Algorithm for Mining Association Rules in Large Databases,\224 P 21\224 lnternationk Conference on Very La Data Bases 1995 pp 432-443 Ill Pradeep Shenoy layant Haritsa S Sudarshan Gaurav Bhalotia, Mayank Bawa and Devavrat Shah 223Turbo-charging Vertical Mining of Large Databases,\224 Pmc ACM International Conference on Management of Data 2000 pp 22-33 I21 R Srikant and R Agrawal 223Mining Generalized Association Rules,\224 Pme 21\224 International Conference on Very Large DataBases 1995 pp 407-419 I31 S Y Sung K Wag and L. Chua 223Data Mining in a Large Database Environment,\224 Pmc IEEE International Conference on Systems Man and Cybernetics 1996 pp 988-993 I41 H Toivonen 223Sampling Large Databases for Association Rules,\224 Pmc 2T\221 International Conference on Very Large Data Bases 1996 pp 134-145 I51 Ming-Cheng Tseng Wen-Yang Lin and Been-Chian Chien 223Maintenance of Generalized  Association Rules with Multiple Minimum Supports,\224 Pmc 9th IFSA World Congress and 20th NAFIPSInternational Conference 2001 pp 1294-1299 234 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


