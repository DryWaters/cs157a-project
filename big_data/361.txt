Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 STOCK MARKET TIME SERIES DATA MINING BASED ON REGULARIZED NEURAL NETWORKAND ROUGH SET XIAO-YE WANG, ZHENG-OU WANG Institute of Systems Engineering, Tianjin University, Tianjin 300072,China E-MAIL Xiaoye@F\222ublic.Tpt.Tj.Cn Abstraet This paper presents a new method of stock market time series data mining which combines regularized neural network with rough set The process includes preprocessing of time series database and data mining The preprocessing clean and filter time series Then 
we partition the time series into a series 01 static pattern which is based on the trend i.e increasing or decreasing 01 closing price An information table is formed by the most important predicting attributes and target attributes identited from each pattern The regularized neural network RNN is used to study and predict the data Rough set can extract rule knowledge in the trained neural network that can he used to predict the time series behavior in the future The method camhines the high generalization faculty of regularized neural network and the rule reduction capability 
01 rough set The experiment demonstrates the effectiveness 01 the algorithm Keywords Time series Regularized neural network Data mining Rough set 1 Introduction Time series data arises in a variety of domains such as stock market analysis environmental data telecommunications data medical and financial data Recently there has been an explosion of interest in time series data mining with researchers attempting to index cluster, classify and mine association rules from increasing massive sources of data For example, Das et al attempt to show how association rules can be leamed from time series 223I It uses a sliding window to discretize the time 
series and then cluster these subsequences using a suitable measure of patterns From these patterns they can find rules relating pattern in a time series to other patterns in that series,or pattern in one series to patterns in another series Keogh et al introduce a new scalable time series classification algorithm 222I All these algorithms that operate on time series data need to compute the simliarity between patterns The above methods are very sensitive to the computational algorithm of the window\222s width and the distance Han et al investigate time series databases for periodic segments 22131 and partial periodic patterns 14 using data mining methods 
Their techniques are aimed at discovering temporal patterns rather than temporal rules 0-7803-7508-4/02/$17.00 02002 IEEE In most data mining problems a regular static database contains a set of records each is constructed from a set of attributes The order of record has no significance In contrast a time series database contains a set of records in which some of the attributes are associated with a timestamp 222I For example a stock market database in which each record includes the static attributes such as stock name some dynamic attributes such as closing price and opening price In a stock market database we 
are more interested in the behavior of the stock over time rather than in its value on a single day In this paper we are interested in finding rules ahout long behaviors of the stock price An example would be a rule such as 223 If the current fluctuant is high then the duration of this quotation is short\224 We convert time series to the static patterns which are appropriate for general data mining method and use the attributes of static patterns as the basis for exploratory rule induction Our data mining approach is based on regularized neural network and rough set 
The approach highly improves the generalization of the algorithm due to using the regularized neural network The gained rules can better predict the points at which the stock will change the direction of its trend compared with the method using general neural network A time series can be converted into a series of subsequences by using the methodology of extremum The subsequence is a static pattern From the stock investor perspective the mere change in the general trend i.e from increasing to decreasing is very important since it may trigger a buying or a selling action Then we identify the most important predicting attributes and target attribute from each pattern to 
form an information database The general data-mining method such as neural network can then be used directly on the information database to uncover the rules for predicting the length of pattern i.e the turning point of a stock market quotation Neural network are perhaps the most widely used and successful method for leaming data Although it often provides a high level of predictive accuracy rarely do they facilitate human inspection or understanding This situation is due to the fact that neural network represents their learned solution in the architecture In contrast to neural network the solution formed by \223symbolic\224 system such as rough set are usually 315 


much more amenable to human comprehension. Therefore we develop an algorithm which uses regularized neural network it has a high generalization faculty and is appropriate for the stock data mining of have high noisy to learn the information table Then rough set is used to extract rule from the knowledge of neural network The remainder of the paper is organized as follows In section 2 we describe the general methodology for preprocessing of stock time series data In section 3 the algorithm of combining regularized neural network with rough set for data mining is presented In section 4 the performance of the proposed algorithms is reported by experiment results We conclude our study in section 5 2 preprocessing the stock market time series Definitionl a time series is a collection of records which were made sequentially in time Stock market time series is a collection of closing price of every transaction day sequentially in time The preprocessing of the time series includes the following steps Step 1 Denoise clean the raw data by signal processing techniques to remove the additive noise Step 2 Divide the time series into patterns based on the trend i.e increasing or decreasing of closing price Step 3 Extract the features from pairs of adjacent patterns that describe the database Then create an information databases about the patterns Based on the databases we can predict the length of a pattern 2.1 Data cleaning In stock time series database the closing price of each day is composed of daily random fluctuations and long term trends. Therefore we should clean the raw data We can assume that the raw data a n are composed of a long trends signal a\(n and noise e\(n  that is a,,\(n dn 1 The cleaning operation should produce 2\(n to estimate the long-term signal n\(n The noise signal is of random natilre and influenced daily by various sources In contrast the long-term signal is stable and deterministic To clean the data we use a low-pass filter operator that eliminates the mostly noise part One of the simplest is the finite impulse response FlR form The filter is defined as Where amw n is original data 2\(n is cleaned data c\(i is vector with N coefficients The size and value of the coefficients c\(i are subject to design; one can use the signal toolbox from Matlab to find Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 316 the coefficients The result is an approximated version of the original signal 2.2 Pattern dividing The purpose of the pattern dividing is to find the turning point of the data trend which is the end of the forward pattern and is the start of the second pattern For the investor, the length of the quotation is the most important The most simplest method is to find the extremum that is the point I t O  the t is ordered in a set T ko tN,},then we eliminate the short intervals\(1ess than the user defined threshold d Then T divides the time series into Ne patterns 23 Feature extraction Because the data trend in every pattern is consistent we can approximate the pattern with linear function. The length interval is ti+l ti here 0  i  Ne 1 it means the length of the quotation which is the target attribute of the information database b Slope of the pattern For each interval we take 2\(n value in its start and end We can now derive the slope as 3 Signal to noise ratio SNR Another important feature in stock time series is the Signal to noise mtio SNR This feature expresses the fluctuations of the series data A high SNR value indicates that the series is unstable and influenced by different factors We can calculate the SNR in the following way over an interval pattern ti I a  3fi  tic1 ti c  Where t la  a\(t is original data 2.4 Feature discretization In order to express the knowledge in the rules the data in the information database must be represented in the discrete form integer, character string and enumeration type\The features are split according to naive scaler algorithm 61 We can create an information database using the above features in which slope and SNR are input predicting attributes and length is the target attribute classification Neural network can be used to predict unknown value of target attributes in a manner similar to classification 


Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 There are many noises in the stock time series database Therefore the general neural network will be over fitting while it learns the stock data An effective method of avoiding the over fitting is regularization 223I We use a three layer regularized feed forward network to unknown knowledge in the information database learn the information database where the predicting attributes are regarded as input and target attribute is regarded as the output The local regularized least square algorithm based on single weight is introduced by us in 8 as follow 3 Datamining The steps of data mining in time series are as follows  Use the regularized neural network to discovery the  Use Rough set to extract rules from the neural network 3.1 Regularized neural network where wji is weight j=1,2;-4V i=1,2,-,nji nj is the number of weights connected with the jth node N is the number of the network npdes \(exclude the input nodes M is the number of output nodes Fqji\(k is the gradients of the qth output node with respect to wji  Ck 1 I is forgetting factor and v 30 the regularization parameter 3.2 Roughset The knowledge learned from the information database S is distributed in the architecture of neural network It is not facilitate human inspection or\222 understanding In contrast Rough set is usually much more amenable to human comprehension Therefore we use the neural network to learn the data Through learning we can remove the data that cannot be learned from the information database S We consider it as noise data Then use the Rough set to extract a tidy rule set from the new information database Sn This rule set is an approximate representation of the knowledge in the network Rule reduction is one of the major concerns in Rough set The rule reduction is a process of core value reduction of the information table During this process only internal information provided by the data under consideration is used We quote the algorithm of Literature 9 4 Experiment result We demonstrate our approach on the stock market database. The stock time series includes stock name, stock opening price and stock closing price Because investors pay more attention to the change of the closing price we only analyze the closing price Stock prices are noisy and influenced daily by many 223buttertly effect\224 data cleaning must be done prior to data mining For investor the mere change in the general trend i.e from increasing to decreasing is very important since it may trigger a buying or a selling action Therefore our main interest is to predict the point at which the stock will change the direction of its slope In this section we demonstrate the process of data mining on the stock database fmm Shanghai Stock Exchange We investigated the stocks\222 performance over a three-years period from 1998.1 to 2000.12 4.1 Data preprocess First we cleaned the original database of the 100 companies from Shanghai Stock Exchange using a low pass filter described in section 2.1 We took bandwidth of O.l/day and size of 30 coefficients The pattern dividing and feature extraction in section 2.2 and 2.3 have resulted in a set of records where each record is related to one pattern of a specific stock The total number of 820 patterns forms the information table S The record attributes include interval slope the Signal to Noise Ratio SNR and pattern interval length The attributes of the pattern are represented in Table 1 Then we split the continuous attributes into discrete value represented by character string The related discrete attributes of the pattern are given in Table 2 4.2 Training the neural network For enhance the generalization of neural network we divided the information table into a training set SI include 600 patterns and validation set S2 220 patterns The thermometer-coding scheme was used for representations of the attributes Because the SNR2 Slope2 and Slope1 are split into three four and four discrete value respectively the number of input codes is 11 The length2 is split into three discrete values therefore the output number is 3 The architecture of the R\223 is 11-20-3 The initial weights were randomly selected from 0 to 1 The parameters of the algorithm were chosen to be 1=0.999  v=O.1 and pii\(0  the error 317 


Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 Length2 performance function is MSE Mean Square Error that is 0.001 In the process of training if the MSE is little than 0.001 we think it is learning success Table 3 shows a comparison result of the RNN and BP We can see from Table 3 that the R has the higher generalization performance validation accuracy despite they have the same training accuracy Lengthof Target interval of classification Table 1 The related attributes I Amibute I Amibute I Role in data I name I Description I Mining sNR2 I SNR of second I Input attem attem Slope2 Slope of second Input  Slope1 Slope of first Input I second pattern I attribute Table 2 Attributes discretization Table 3 Training result Training Validation 97.79 90.58 97.70 94.77 43 Rules extraction The records that cannot be learned by R are deleted from the training set S1 We acquired 14 rules by applying the rules extraction algorithm on the S1 The rules has the following form Rulel If SNR2 is Low and slope2 is Positive-large then Length2 is medium Rule2 If SNR2 is High then Length2 is Long However we acquired 18 rules by using BP network tn learn the training set The rules prediction accuracy using the RNN for the validation set S2 is 85 it is higher than the rule prediction accuracy of the BP algorithm 70.5 5 Conclusions In this paper we have combined two data mining technologies the RNN and the Rough set for the stock market time series data mining It sufficiently applies the high generalization performance of RNN and the rule generation capability of the Rough set The discovered knowledge is much more amenable to human comprehension The prediction accuracy on the stock market data is high This algorithm will be useful for investment decision supports References l Das,G.,Lin K Mannila H Renganathan G Smyth P 1998 Rule discovery from time series Roc of the 4 International Conference of howledge Discovery and Data Mining New York,NY Aug 27 31.~~16-22 2 Keogh E  Pazzani M 1998 An enhanced representation of time series which allows fast and accurate classification clustering and relevance feedback. Proc of the 4h International Conference of Knowledge Discovery and Data Mining New York,NY Aug 27-31. pp239-241 Han,J.,Gong W.,&Yin Y.\(1998 Mining segment wise periodic patterns in time-related databases Proc of International Conference of Knowledge Discovery and Data Mining New York,NY Aug 27-31 pp 214 218 Han,J.,Dnng G,&Yin Y.\(1999 Efficient mining of partial periodic pattem in time series databases Proc of International Conference of Data Engineering ICDE99 Sydney, Australia Mar 5 Last M Klein Y Knowledge Discovery in Time series Databases. IEEE Trans on System Man and Cybernetics-part b VOL31 I 2001 6 G.Wang Rough set theory and knowledge acquire Xi-an jiaotong University Publish Company in Chinese\2001.5 pp 99-104 7 Gir0si.F J0nes.M Regularization Theory and Neural Networks Architectures Neural Computation 7,219 269 1995 8 Dongmei Li Zhengou Wang A New method for improving the Real-time ability and Generalization performance of Feedforward Neural Network Accepted by Journal of Motor and Control in Chinese 9 Li-yun Chang Guo-yin Wang An Approach for Attribute Reduction and Rule Generation Based on Rough Set Theory. Joumal of Software VOLl0 1 l 1999 3 4 31 8 


Inputs Reference Figure 3 Input and reference patterns repeated periodically unless a pattern is presented that restarts the learning process Figure 4 presents the results obtained for R1-15-1 when using the following equations d dt yi\(t,s  yi\(t,s  fi  1 wii 4s  yj\(t J dA aE ai t,s  Ai\(t,s ZAj\(t,s fj\(t,s W t,s t,s  Ai\(T,d=0  J i dt Output  Reference Output  Reference Weights 81 Error 401 0 Figure4 Resultsfor R1-15-1 incaseofthe Equations7a-c a  lO,stepsize=O.l Upper figures Output of the net after 4000 updates Lower figures Accumulated weight functions of the output neuron after 4000 updates horizontal units   of samples Accumulated total  partial errors horizontal unit   of updates III-501 


Equations 7a-c are the gradient version of the characteristic equations of the Hamiltonian h\(t Ai t Fj YO W\(t   E\(t,y\(t WO  7d i which treats weights as time-varying parameters see relation 3b or 11,51 In the case that weights are restricted to constant parameter functions the condition 3b as well as Equation 7c have to be integrated over time l 11 The combined result reads 2 1 0 1 2  T dWli ah a s    t dt   Io  Ai\(t,s 6 0,s yj\(t,s  T 0 aW T  dt  8 aw I v ds 50 100 150 200 A relation similar to 10 was obtained by Pearlmutter for a special cost function 41  Figure 5 depicts corresponding results It is to be noted here that the variable 5 has no interpretation as a dynamical process run by the network This is in contrast to the fully dynamical case which has a single time t for recall as well as learning Hence we may differentiate between learning rules fulfilling 3b and learning laws similar to 6d Output  Reference Output  Reference  out 2 Error 30   Ob 1000 2000 3000 4000 5000 6000 Figure 5 Resultsfor R1-15-1 and the Equations7a-b,8 apT  O.l,stepsize=O.l Upper figures output of the net after 6000 updates Lower figures Accumulated weight functions of the output neuron after 6000 updates horizontal units   of samples Accumulated total  partial errors horizontal unit   of updates COMPARING THE FIGURES 2,4,5 It is clearly seen that Equations 6a-c give the fastest decline of the error functions in terms of updates as well as computation time Further fitting with constant weights lasts considerably longer in terms of equal error In passing we note that the recurrent network R1-15-1 was found always to work better than the feedforward version of R1-15-1 111-502 


CONCLUSIONS We have described a neural network as being a dynamical system Its observables are introduced as solutions of a partial differential equation of Hamilton-Jacobi type The observables are functions of time the neural states and the weight states The dynamics of an observable is interpreted as a surface over the state space of the neural net which is generated by all its admissible trajectories If the weights are treated as parameters the only dynamical process that actually will be run by the network is the recall process It is shown that the known types of learning dynamics can be reproduced in this framework and that weights are generally time-varying during recall This feature is shown to account for much faster learning as compared to weights being constant for each learning epoch If on the other hand weights are conceived as variables a fully dynamical new picture of recall and learning results. Neurons as well as weights obey differential equations ie recall and learning are dynamical processes of the neural net It is important to note that the differential equations to be obeyed by neurons and weights respectively constitute half of the characteristic equations associated with the Hamiltomlacobi equation The remaining equations are obeyed by the conjugate variables of the neurons and weights respectively These latter are mediating the interaction between neurons and weights Comparing the fully dynamical concept with the optimization approach that exploits techniques from Optimization Theory Control Theory or Dynamic Programming 12  the dynamical concept was found to learn much faster than the optimization concept ACKNOWLEDGEMENT We are particular grateful to B Schurmann for discussing with US the Hamiltonian concept and its interpretation with respect to neural networks REFERENCES D Rumelhart G Hinton and R Williams Learning Internal Representations by Error Propagation In PARALLEL DISTRIBUTED PROCESSING EXPLORATIONS IN THE MICRO STRUCTURE OF COGNITION Vol I MIT Press 1986 L.B Almeida: "Backpropagation In Non-Feedforward Networks IEEE INT CONF ON NEURAL F.J PINEDA Generalization Of Backpropagation To Recurrent Neural Networks PHYS REV LETTERS 59 pp 2229,1987 B.A Pearlmutter Learning State Space Trajectories In Recurrent Neural Networks NEURAL COMPUTATION 1 pp 263,1989 0 Farotimi A Dembo and T Kailath A General Weight Matrix Formulation Using Optimal Control IEEE TRANSACTION ON NEURAL NETWORKS May 1991 Vol2 No 3 pp 378 U Ramacher B.Schurmann Unified Description of Neural Algorithms For Time-Independent Pattern Recognition in VLSl DESIGN OF NEURAL NETWORKS Kluwer Acad. Publishers 1991 L.O Chua L Yang Cellular Neural Networks Theory and Applications IEEE E Kamke PARTIAL DIFFERENTIAL EQUATIONS B.G Teubner Stuttgart 1977 F Verhulst NONLINEAR DIFFERENTIAL EQUATIONS AND DYNAMICAL SYSTEMS Springer 1985 NETWORKS SAN DIEGO PP 11-609,1987 TRANSACTIONS ON CIRCUITSANDSYSTEMS Vol 35 NO 10 pp 1257-1290 Oct 1988 lo A.P Sage and C.C White OPTIMUM SYSTEMS CONTROL Prentice-Hall 1977  1 11 U Ramacher M Wesseling Hamiltonian Approach to Neural Network Dynamics Proc IJCNN-91 Singapore, Vol 3 pp.1930-1936 Nov 1991 12 L.S Pontrijagin et al THE MATHEMATICAL THEORY OF OPTIMAL PROCESSES Wiky 1962 III-503 


Typically the number of complex relationships found was greater than but correlated with the number of other relationship types found As Figure 9 shows the number of complex relationships at a given con\223dence threshold was sensitive to the variance in the number of the other relationship types Self-exclusion and self-colocation were modeled together in Figure 9 emphasize the complementary relationship between the two as described in section 5.2 This is revealed in the corresponding steepness f gradient for self-exclusion/colocation at con\223dence 000 000 001 001 and con\223dence 002 000 001 002  7.5 Limitations/Strengths of the representation While there are representational issues with any type of data appropiate representation is particularly important in the spatial domain 9 Limitations In one-to-many relationships this model doesn\220t capture interesting ranges or distributions in the 217many\220 which is a task better suited for mixture modelling or the techniques described in As pointed out in 10 t h e cos t o f ful l y t r ans cri b i n g s pat i a l data into a transactional representation can in some cases be more expensive than the mining of the colocations but as a full representation is necessary to accurately add the features representing absent and multiple items a solution to this in the current representation may be problematic Strengths The most obvious strength of this representation is that currently it is the only model that allows the mining of complex relationships in spatial data A major strength of a transactional representation of spatial data not explored here is that it may be combined with non-spatial data and so the addition of nonspatial data to the representation described here would be uncomplicated 8 Conclusions  Future Work We have de\223ned the concept of complex relationships in spatial data We have described how even in transactional representations spatial data is undamentally erent from other forms of data making the need to mine complex relationships of inherent interest We have demonstrated that even when simple relationships are the goal of mining spatial data the mining of complex relationships is necessary for determining the signi\223cance of those relationships We have implemented and demonstrated a transactional representation of spatial data that allows the ef\223cient mining of complex relationships and discussed its limitations and strengths 8.1 Future Work Apart from investigating improvements to the representation to address the limitations mentioned in 7.5 there are several future directions evident such as the application to other types of data with a spatial component such as spatiotemporal data and to a lesser extent natural language and biological systems One important step would be the combination of spatial coordinate features with spa tial volume features this is especially important in Geographic Information Systems where a volume may represent the area of a lake valley etc As we have demonstrated that with a purely coordinate system 003 in 004 000 003 must be treated as a volume the inclusion of features that explicitly represent volumes should prove interesting References  R  A gra w al and R  S r i kant  F ast al gori t h ms for m i n i n g a ssociation rules In J B Bocca M Jarke and C Zaniolo editors Proc 20th Int Conf Very Large Data Bases VLDB  pages 487\205499 Morgan Kaufmann 12\20515 1994 2 T  C  B aile y a n d A T  Gatrell Interactive spatial data analysis  Longman Scienti\223c  Technical 1995 3 S  B rin  R  R asto g i  a n d K Sh im M i n i n g o p timized g a in rules for numeric attributes IEEE transactions on knowledge and data engineering  15 2003 4 G  P iatetsk y Sh ap iro  Discovery analysis and presentation of strong rules AAAI/MIT Press 1991 5 J  H an J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In W Chen J Naughton and P A Bernstein editors 2000 M SIGMOD Intl Conference on Management of Data  pages 1\20512 ACM Press 05 2000 6 Y  H uang H Xi ong and S  S hekhar  M i n i n g con\223 dent colocation rules without a support threshold In Proc 18th M Symposium on Applied Computing ACM SAC  2003  K K operski and J Han Di sco v e ry of spat i a l a ssoci at i o n rules in geographic information databases In M J Egenhofer and J R Herring editors Proc 4th Int Symp Advances in Spatial Databases SSD  volume 951 pages 47\205 66 Springer-Verlag 6\2059 1995 8 R  M unr o S  C h a w l a  a nd P  S un C o mpl e x spat i a l r el at i onships University of Sydney School of Information chnologies chnical Report 539  2003 9 D  J  P euquet  Representations of space and time  Guilford Press 2002  S  S h ekhar and S  C ha wl a Spatial Databases A Tour  2002  S  S h ekhar and Y  H uang Di sco v e r i ng spat i a l c ol o cat i o n patterns A summary of results Lecture Notes in Computer Science  2121 2001  X  W u  C  Z hang and S  Z hang Mi ni ng bot h posi t i v e and negative association rules In 19th International Conference on Machine Learning ICML-2002  2002 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


 A A A A A A A A B B B B B B B A B A B A B A B AB A B A A A A B B B A B A B A A B B B B A B A B A B A B A B A B A disjoint B A inside B A contains B A equals B A meets B A covered by B A covers B A overlaps B A B A B A B A B A B AB Figure 4 Topology and resolution increase with minimum bounding circles 64Mb of main memory Since the Apriori algorithm uses the number of transactions as support and we wanted to compare our algorithm with Apriori we have implemented MaxOccur and the na\250 021ve with transaction based support MaxOccur1 The second version of MaxOccur MaxOccur2 used the object-based support as presented in Algorithm 3.1 Table 9 shows the average execution times for the four algorithms with different image set sizes and 033 0 0  05 for Apriori 223Na\250 021ve\224 and MaxOccur1 and 0  0035 for MaxOccur2 The results are graphically illustrated in Figure 5 Clearly MaxOccur scales well with both versions treating one thousand images in 1.3 seconds on average regardless of the size of the data set The running time for 002ltering the frequent item-sets with 033 0  the maximum support threshold line 16 of Algorithm 3.1 is negligible since it is done in main memory once the frequent item-sets are determined Moreover the calculation of the total number of items line 4 of Algorithm 3.1 is done during the 002rst scan of the data set and has limited repercussion on the algorithms execution time The major difference between Apriori and MaxOccur is in ascertaining the candidate item-sets and counting their repeated occurrences in the images Obviously MaxOccur discovers more frequent item-sets The na\250 021ve algorithm also 002nds the same frequent item-sets but is visibly capable of less performance in execution time The left graphic in Figure 6 shows the average number of frequent item-sets discovered with the three algorithms Apriori found on average 109 different frequent k-item-sets while MaxOccur1 and Na\250 021ve found 148 on the same data sets and MaxOccur2 found 145 on average The discrepancy between MaxOccur1 and MaxOccur2 is basically due to the different de\002nition of support The price we pay in performance loss with MaxOccur is gained by more frequent item-sets and thus more potentially useful association rules with recurrent items discovered ofimages Apriori Na\250 021ve MaxOccur1 MaxOccur2 10K 6.43 70.91 13.62 13.68 25K 15.66 176.69 32.35 34.11 50K 30.54 359.38 66.07 67.44 75K 44.93 514.33 97.27 101.23 100K 60.75 716.01 130.12 137.81 Table 9 Average execution times in seconds with different number of images 0 100 200 300 400 500 600 700 800 10K 25K 50K 75K 100K Apriori MaxOccur1 MaxOccur2 Na\357ve time images Figure 5 Scale up of the algorithms 6 Discussion and conclusion We have introduced in this paper multimedia association rules based on image content and spatial relationships between visual features in images using coarse to 002ne resolution approach and we have demonstrated the preservation and changes in topological features during resolution re\002nement We have put forth a Progressive Resolution Re\002nement approach for mining visual media at different resolution levels and have presented two algorithms for the discovery of content-based multimedia association rules These rules would be meaningful only in a homogeneous image collection a collection of semantically similar images or received from the same source channel Many improvements could still be added to the multimedia mining process to speed up the discovery or to re\002ne or generalize the discovered results 017 One major enhancement in the performance of the multimedia association rule discovery algorithms is the addition of some restrictions on the rules to be discovered Such restrictions could be given in a metarule form Meta-rule guided mining consists of dis#ofimages 033 0 0  25 0  20 0  15 0  10 0  05 10K 1.43 2.20 2.70 5.06 13.51 25K 2.80 4.78 6.31 11.20 32.35 50K 6.27 9.28 11.59 22.74 66.07 75K 8.24 13.57 17.69 33.94 97.27 100K 11.32 17.63 23.13 46.74 130.12 Table 10 Average execution time in seconds of MaxOccur with different thresholds 


 0 20 40 60 80 100 120 140 160 MaxOccur2 MaxOccur1 Na\357ve Apriori Apriori MaxOccur1 MaxOccur2 Na\357ve F k  Figure 6 Frequent item\255sets found by the dif\255 ferent algorithms covering rules that not only are frequent and con\002dent but also comply with the meta-rule template For example with a meta-rule such as 223 H-Next-to X Y   Colour x red  Overlap Y Z   P  Y Z  224 one need only to 002nd frequent 3-item-sets of the form f HNext-to\(red Y  Overlap Y 003  P  Y 003  g where Y is an attribute value and P a visual descriptor or spatial relationship predicate Obviously such a 002lter would greatly reduce the complexity of the search problem A method for exploiting meta-rules for mining multilevel association rules is given in  017 We have approximated an object in an image to a locale which is an area with a consistent visual feature such as colour Objects in images and videos are obviously more complex In a recent paper 9 re gions and their signatures are used as objects in a similarity retrieval system A computationally ef\002cient way to identify distinct objects in images is however still to be proposed Automatically identifying real objects and using spatial relationships between real objects would reduce the number of rules discovered and make them more signi\002cant for some multimedia applications 017 Object recognition or identi\002cation in image processing and computer vision is a very active research 002eld Accurately identifying an object in a video for example as being an object in itself is a very dif\002cult task We believe that data mining techniques can help in this perspective Multimedia association rules with spatial relationships using the motion vector of locales as a conditional 002lter can be used to discover whether locales moving together in a video sequence are part of the same object with a high con\002dence 017 There are many application domains where multimedia association rules could be applied and should be tested such as global weather analysis and weather forecast medical imaging solar surface activity understanding etc We are investigating the application with Magnetic Resonance Imaging MRI to discover associations between lesioned structures in the brain or between lesions and pathological characteristics Further development and experiments with mining multimedia data will be reported in the future References 1 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules In Proc VLDB  pages 487\226499 1994 2 M  J  E genhof er  Spatial Query Languages  PhD thesis University of Maine 1989 3 M  J  E genhof er and J  S har ma T opol ogi cal r e l a t i ons between regions in r 2 and z 2 In Advances in Spatial Databases SSD'93  Singapore 1993 4 U  M  F ayyad S  G  D j or go vski  a nd N  W e i r  A ut omat i n g the analysis and cataloging of sky surveys In U Fayyad G Piatetsky-Shapiro P Smyth and R Uthurusamy editors Advances in Knowledge Discovery and Data Mining  pages 471\226493 AAAI/MIT Press 1996 5 Y  F u a n d J Han  M e ta-ru le-g u i d e d m in in g o f a sso ciatio n rules in relational databases In Proc 1st Int Workshop Integration of Knowledge Discovery with Deductive and ObjectOriented Databases  pages 39\22646 Singapore Dec 1995 6 J  H an an d Y  F u  Disco v e ry o f mu ltip le-le v el asso ciatio n r u l es from large databases In Proc VLDB  pages 420\226431 1995 7 Z  N  L i  O R Z a 250 021ane and Z Tauber Illumination invariance and object model in content-based image and video retrieval Journal of Visual Communication and Image Representation  10\(3\:219\226244 September 1999 8 R  M iller a n d Y  Y a n g  Asso ciatio n r u l es o v e r i n t erv a l d ata In Proc ACM-SIGMOD  pages 452\226461 Tucson 1997 9 A  N atse v  R Rasto g i  a n d K Sh im W ALR U S A s imilar ity retrieval algorithm for image databases In Proc ACMSIGMOD  pages 395\226406 Philadelphia 1999  R Ng L  V  S  L akshmanan J  H an a nd A Pang E x ploratory mining and pruning optimizations of constrained associations rules In Proc ACM-SIGMOD  Seattle 1998 11 R Srik an t a n d R Ag ra w a l M i n i n g q u a n titati v e asso ciatio n rules in large relational tables In Proc ACM-SIGMOD  pages 1\22612 Montreal 1996  P  S t ol or z H  N a kamur a  E  M esr obi an R  M unt z E  S h ek J Santos J Yi K Ng S Chien C Mechoso and J Farrara Fast spatio-temporal data mining of large geophysical datasets In Proc Int Conf on KDD  pages 300\226305 1995  O  R  Z a 250 021ane Resource and Knowledge Discovery from the Internet and Multimedia Repositories  PhD thesis School of Computing Science Simon Fraser University March 1999  O  R  Z a 250 021ane,J.Han,Z.-N.Li,J.Y.Chiang,andS.Chee MultiMediaMiner A system prototype for multimedia data mining In Proc ACM-SIGMOD  Seattle 1998  O  R  Z a 250 021ane J Han Z.-N Li and J Hou Mining multimedia data In CASCON'98 Meeting of Minds  Toronto 1998 


18001  balancing mechanism which requires further investi gation 4.5 Speedup Figure 12 shows the speedup ratio for pass 2 vary ing the number of processors used, 16 32 48 and 64 where the curve is normalized with the 16 processor execution time The minimum support value was set to 0.4 4.5 0.5 1 1 0 I 10 20 30 40 50 60 70 number of mxessors Figure 12 Speedup curve NPA HPA and HPA-ELD attain much higher lin earity than SPA HPA-ELD an extension of HPA for extremely large itemset decomposition further in creases the linearity HPA-ELD attains satisfactory speed up ratio This algorithm just focuses on the item distribution of the transaction file and picks up the extremely frequently occurring items Transferring such items could result in network hot spots HPA-ELD tries not to send such items but to process them locally. Such a small mod ification to the original HPA algorithm could improve the linearity substantially 4.6 Effect of increasing transaction Figure 13 shows the effect of increasing transac tion database sue as the number of transactions is increased from 256,000 to 2 million transactions We used the data set t15.14 The behavior of the results does not change with increased database size The minimum support value was set to 0.4 The num ber of processors is kept at 16 As shown each of the parallel algorithms attains linearity 5 Summary and related work In this paper we proposed four parallel algorithms for mining association rules A summary of the four database size Sizeup 0 I 0 500 loo0 1500 uxw amount of transaction thousands Figure 13 Sizeup curve algorithms is shown in Table 5 In NPA the candi date itemsets are just copied amongst all the proces sors Each processor works on the entire candidate itemsets NPA requires no data transfer when the supports are counted However in the case where the entire candidate itemsets do not fit within the mem ory of a single processor the candidate itemsets are divided and the supports are counted by scanning the transaction database repeatedly Thus Disk 1/0 cost of NPA is high PDM, proposed in 6 is the same as NPA which copies the candidate itemsets among all the processors Disk 1/0 for PDM should be also high The remaining three algorithms SPA HPA and HPA-ELD partition the candidate itemsets over the memory space of all the processors Because it better exploits the total system's memory, disk 1/0 cost is low SPA arbitrarily partitions the candidate itemsets equally among the processors Since each processor broadcasts its local transaction data to all other pro cessors the communication cost is high HPA and HPA-ELD partition the candidate itemsets using a hash function which eliminates the need for transac tion data broadcasting and can reduce the comparison workload significantly HPA-ELD detects frequently occurring itemsets and handles them separately which can reduce the influence of the workload skew 6 Conclusions Since mining association rules requires several scans of the transaction file its computational requirements are too large for a single processor to have a reasonable response time This motivates our research In this paper we proposed four different parallel algorithms for mining association rules on a shared nothing parallel machine and examined their viabil 29 


Table 5 characteristics of algorithms ity through implementation on a 64 node parallel ma chine the Fujitsu AP1000DDV If a single processor can hold all the candidate item sets parallelization is straightforward It is just suf ficient to partition the transaction over the proces sors and for each processor to process the allocated transaction data in parallel We named this algo rithm NPA However when we try to do large scale data mining against a very large transaction file the candidate itemsets become too large to fit within the main memory of a single processor In addition to the size of a transaction file a small minimum support also increases the size of the candidate itemsets As we decrease the minimum support computation time grows rapidly but in many cases we can discover more interesting association rules SPA HPA and HPA-ELD not only partition the transaction file but partition the candidate itemsets among all the processors We implemented these al gorithms on a shard-nothing parallel machine Per formance evaluations show that the best algorithm HPA-ELD attains good linearity on speedup by fully utilizing all the available memory space which is also effective for skew handling At present we are doing the parallelization of mining generalized association rules described in 9 which includes the taxonomy is-a hierarchy Each item belongs to its own class hierarchy In such mining associations between the higher class and the lower class are also examined Thus the candidate itemset space becomes much larger and its computation time also takes even longer than the naive single level association mining Parallel pro cessing is essential for such heavy mining processing Acknowledgments This research is partially supported as a priority research program by ministry of education We would like to thank the F\221ujitsu Parallel Computing Research Center for allowing us to use their APlOOODDV sys tems References l R.Agrawal T.Imielinski and ASwami 223Min ing Association Rules between Sets of Items in Large Databases\224 In Proc of the 1993 ACM SIGMOD International Conference on Manage ment of Data pp207-216 May 1993 2 R.Agrawal and RSrikant 223Fast Algorithms for Mining Association Rules\224 In Proc of the 20th International Conference on Very Large Data Bases pp.487-499 September 1994 3 J.S.Park M.-S.Chen and P.S.Yu 223An Effec tive Hash-Based Algorithm for Mining Associ ation Rules\224 In Proc of the 1995 ACM SIG MOD International Conference on the Manage ment of Data SIGMOD Record Vo1.24 pp.175 186 June 1995 4 H.Mannila H.Toivonen and A.I.Verkamo 223Ef ficient Algorithms for Discovering Association Rules\224 In KDD-94:AAAI Workshop on Knowl edge Discovery in Databases pp.181-192 July 1994 5 A.Savasere, E.Omiecinski and S.Navathe 223An Effective Algorithm for Mining Association Rules in Large Databases\224 In Proc of the 21th International Conference on Very Large Data Bases pp.432-444 September 1995 6 J.S.Park M.-S.Chen and P.S.Yu 223Efficient Parallel Data Mining for Association Rules\224 In Proc of the 4th International Conference on In formation and Knowledge Management pp.31 36 November 1995 7 T.Shintani and M.Kitsuregawa 223Considera tion on Parallelization of Database Mining\224 In Institute of Electronics Information and Com munication Engineering Japan SIG CPS Y95 88 Technical Report Vo1.95 No.47 pp.57-62 December 1995 8 T.Shimizu T.Horie and H.Ishihata 223Perfor mance Evaluation of the APlOOO Effects of message handling broadcast and barrier syn chronization on benchmark performance-\224  In S WO PP 22292 9.2 ARC 95 Information Processing Society of Japan Vo1.92 No.64 1992 9 R.Srikant and R.Agrawal 223Mining Generalized Association Rules\224 In Proc of the 21th Inter national Conference on Very Large Data Bases pp.407-419 September 1995 30 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


