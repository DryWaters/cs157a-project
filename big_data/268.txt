Using Information-theoretic Measures to Assess Association Rule Interestingness Julien Blanchard Fabrice Guillet R  egis Gras Henri Briand LINA FRE 2729 CNRS  Polytechnic School of Nantes University La Chantrerie BP 50609  44306 Nantes cede x3ÖFrance  julien.blanchard,fabrice.guillet,henri.briand,regis.gras  polytech.univ-nantes.fr Abstract Assessing rules with interestingness measures is the cornerstone of successful applications of association rule discovery However there exists no information-theoretic measure which is adapted to the semantics of association rules In this article we present the 
Directed Information Ratio  DIR   a new rule interestingness measure which is based on information theory DIR is specially designed for association rules and in particular it differentiates two opposite rules a  b and a   b  Moreover to our knowledge DIR is the only rule interestingness measure which rejects both independence and what we call equilibrium i.e it discards both the rules whose antecedent and consequent are negatively correlated and the rules which 
have more counter-examples than examples Experimental studies show that DIR is a very ltering measure which is useful for association rule post-processing 1 Introduction Many data mining techniques produce results in the form of rules These are expressions of the type if antecedent then consequent  where the boolean propositions antecedent and consequent are conjunctions of assignment expressions variable=value  Rules have the advantage of being very intelligible for users since they model information explicitly They are also a major element of most 
theories of knowledge representations in cognitive sciences  and in particular the y underlie man y w orks in artiìcial intelligence such as the expert systems In knowledge discovery in databases the main rule-based paradigms are the classiìcation rules used in supervised learning to predict a unique class variable as consequent and the association rules which can ha v e an y combination of v ariables as antecedent and consequent Classiìcation rules can be generated by induction algorithms such as CN2 or decision tree algorithms such as C4.5 while association rules are mined by combinatorial algorithms such has Apriori 1 Due to their unsupervised nature association rule mining algorithms commonly generate large amounts of rules 
with much redundancy  T o help the user to nd relevant knowledge in this mass of information one of the main solutions consists in evaluating and sorting the rules with interestingness measures There are two kinds of measures the subjective user-oriented ones and the objective data-oriented ones Subjective measures take into account the userês goals and domain knowledge 16 whereas only the data cardinalities appear in the calculation of objective measures surveys can be found in 11 24  In this article we are interested in the objecti v e measures We have shown in that there are tw o dif ferent b u t complementary aspects of the rule interestingness the de 
viation from independence and the deviation from what we call equilibrium maximum uncertainty of the consequent given that the antecedent is true Thus the objective measures of interestingness can be classiìed into two classes  the measures of deviation from independence which have a xed value when the antecedent and consequent are independent  p  ab  p  a  p  b   such as lift  con viction 8 rule-interest 17 Loe vinger inde x  implication intensity 6  the measures of deviation from equilibrium which 
have a xed value when examples and counterexamples are equal in numbers  p  ab  p  a b  1 2 p  a   such as conìdence Sebag and Schoenauer index  IPEE 4 Among the objective measures of rule interestingness the information-theoretic measures are particularly intelligible and useful since they can be interpreted in terms of information More precisely as pointed out by Smyth and Goodman there is an interesting parallel to dra w bet 
ween the use of information theory in communication systems and the use of information theory to evaluate rules In communication systems a channel has a high capacity if it can carry a great deal of information from the source to the receiver As for a rule the relation is interesting when the antecedent provides a great deal of information about Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


 conditional entropy  H c   p b/a  log 2 p b/a  p b/a  log 2 p b/a mutual information  MI  p ab  log 2 p ab p a p b  p a b  log 2 p a b p a p b  p a b  log 2 p a b p a p b  p a b  log 2 p a b p a p b Theil uncertainty coefìcient  u  MI  p b  log 2 p b  p b  log 2 p b J-measure  J  p ab  log 2 p ab p a p b  p a b  log 2 p a b p a p b Gini index  G  p a  p b/a 2  p b/a 2  p a  p b a 2  p b a 2   p b 2  p b 2 Table 1 Information-theoretic measures of interestingness for a rule a  b the consequent Smyth and Goodman speak of the information content of a rule The information-theoretic measures commonly used to evaluate rule interestingness are the Shannon conditional entropy the a v erage mutual information often simply called mutual information the Theil uncertainty coefìcient 22 the J-measure  and the Gini inde x  2 12 cf the formulas in table 1 The Shannon conditional entropy measures the average amount of information of the consequent given that the antecedent is true it is used in the CN2 algorithm The average mutual information Shannon entropy decrease measures the average information shared by the antecedent and the consequent The Theil uncertainty coefìcient measures the entropy decrease rate of the consequent due to the antecedent The J-measure is the part of the average mutual information relative to the truth of the antecedent Finally the Gini index is the quadratic entropy decrease Even if these measures are commonly used to evaluate association rules see 22 2 the y are all better suited to evaluate classiìcation rules As pointed out by Jaroszewicz and Simovici an association rule should be assessed only on the variable values which are comprised in the rule 1  whereas the information-theoretic measures consider the full joint distribution of the antecedent and consequent this is relevant for classiìcation rules since in supervised learning the user is interested in all the values of the consequent because it is the class variable Consequently the information-theoretic measures do not vary with the permutation of the values of a variable 2 This invariance is undesirable for association rules since the permutation of values deìnitely transforms an association rule 1 Indeed association rule mining algorithms transform each multivalued variable into several binary variables called items 2 More precisely the Shannon conditional entropy and the J-measure vary with the permutation of the values of a variable in the antecedent but not in the consequent We say that association rules are not only variable-based relations but also value-based relations If all the same such measures are applied on association rules then this must be done carefully since it is not possible to distinguish between positive and negative correlations To be appropriate to association rules an interestingness measure must respect their value-based semantics by not systematically giving the same value to a rule a  b and to its opposite a  b  Intuitively if a  b is strong then a  b should be weak In this article we propose an interestingness measure based on information theory which respects the value-based semantics of association rules This new measure named DIR for Directed Information Ratio  allows to reject both the independence and equilibrium situations i.e with only one xed threshold it allows to discard both the rules whose antecedent and consequent are negatively correlated and the rules which have more counter-examples than examples To our knowledge this is a unique feature for a rule interestingness measure In the next section we introduce the new measure DIR from earlier works on the assessment of rules using information theory Section 3 will then review the properties of DIR  Finally in section 4 we compare DIR to other rule interestingness measures within the framework of formal and experimental studies 2 Measuring the information content of rules 2.1 Notations We consider a set of objects described by boolean variables In the association rule terminology the objects are transactions stored in a database the variables are called items and the conjunctions of variables are called itemsets An association rule is a couple  a b  noted a  b where a and b are two itemsets which have no items in common The examples of the rule are the objects which verify the antecedent a and the consequent b  while the counter-examples are the objects which verify a but not b  A rule is all the better since it has lots of examples and few counter-examples In the following we study two itemsets a and b that we simply call the variables The Shannon entropy of the variable a is H  a   p  a 1  log 2 p  a 1  p  a 0  log 2 p  a 0 The Shannon conditional entropy of the variable b given an event a 1 is deìned by H  b/a 1   p  b 1 a 1  log 2 p  b 1 a 1  p  b 0 a 1  log 2 p  b 0 a 1 As can be seen the entropic functions combine variables and realizations of variables In order to distinguish them Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


Figure 1 Plot of the measure j w.r.t p  a 1 b 1 Figure 2 Plot of the measure i w.r.t p  a 1 b 1 Figure 3 Plot of the reduced entropy  H  a  I refers to the statistical independence of a and b  the realizations of a variable b must be noted b 1 and b 0 in this article and not b and  b as commonly done in the association rule literature With these explicit notations an association rule should be written  a 1   b 1  but we retain the classical notation a  b  2.2 The amount of information that a   gives about b Let us consider the amount of information that an event a   gives about a variable b    0 1    We note M  a   b  the measures of this amount of information Blachman studied the M  a   b  whose expectation when averaged over all   is the average mutual information between the variables a and b  MI  a b    M  a   b   1 The two most frequently used measures are the following see gures 1 and 2 j  a   b  p  b 1 a     log 2 p  b 1 a    p  b 1  p  b 0 a     log 2 p  b 0 a    p  b 0 i  a   b  H  b   H  b/a    Blachman shows that j is the only non-negative information-theoretic measure satisfying 1 while i is the only antisymmetric 3 information-theoretic measure satisfying 1 The measure j is the cross-entropy between the apriori and a posteriori distributions of b  It is traditionally accepted as the measure of the amount of information that 3 i is antisymmetric with regard to the apriori and a posteriori distributions P   p  b   and Q   p  b/a     of the variable b  i  P Q   i  Q P  a   gives about b  In particular the J-measure the most commonly used information-theoretic measure within the context of association rules directly comes from j  J  j  P a     Although the meaning of the measure i is much more obvious it is the entropy decrease of b due to the event a    one prefers j to i because j vanishes only if the variables a and b are independent while i can vanish outside the independence see gures 1 and 2 This behavior is due to the symmetrical nature of the entropy H it does not vary with the permutation of the variable values 2.3 Reduced entropy In order to remove the symmetry introduced by the entropy in the measure i  we propose to use a directed entropic function  H named reduced entropy  see gure 3 Deìnition 1 The reduced entropy  H  a  of a variable a is deìned by  if p  a 1  1 2 then  H  a    if p  a 1  1 2 then  H  a  H  a   One similarly deìnes the conditional reduced entropy of the variable b given the realization of a   if p  b 1 a 1  1 2 then  H  b/a 1   if p  b 1 a 1  1 2 then  H  b/a 1 H  b/a 1  The entropy H  a  of a variable a can be written as the sum of two reduced entropies H  a   H  a   H  a   1  with a being the negation of a Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


Figure 4 Plot of  i w.r.t p  a 1 b 0 I refers to the statistical independence of a and b  Contrary to H   H is an asymmetric measure which differently evaluates an imbalance in favor of a 1 and an imbalance in favor of a 0   H  a     H   a   More precisely if a 1 is more frequent than a 0  then  the reduced entropy  H  a  measures the entropy of a   H  a  H  a    the reduced entropy  H  a  is 1 If a 1 is less frequent than a 0  then the roles are reversed In other words  H measures a directed uncertainty in favor of one of the values in the sense that if this value is not the more likely then the uncertainty is considered as maximal 2.4 Directed Information Ratio By introducing the reduced entropy  H in the measure i  we have i  a 1 b   H  b   H  b    H  b/a 1   H  b/a 1 Hence i  a 1 b   i  a 1 b   i  a 1  b  with  i  a 1 b   H  b    H  b/a 1 So the index i which measures the decrease of the entropy H is the sum of two decreases of reduced entropy  H    i  a 1 b  which is the decrease of reduced entropy of b due to a 1    i  a 1  b  which is the decrease of reduced entropy of b due to a 1  Contrary to the measures i and j  the new index  i  a  1 b  is absolutely appropriate to evaluate the interestingness of an association rule a  b   i  a 1 b   i  a  b  Indeed  i  a 1 b  increases with the number of examples probability p  a 1 b 1  decreases with the number of counter-examples probability p  a 1 b 0  see gure 4 and respects the value-based semantics of association rules by differentiating opposite rules a  b and a  b  The higher  i  a 1 b   the more the event a 1 brings information in favor of b 1  and the more the interestingness of the rule a  b is guaranteed If  i  a 1 b  is negative this means that a 1 brings no information in favor of b 1  and even that it removes some information the uncertainty is lesser by predicting b 1 randomly rather than by predicting b 1 using the rule a  b  In our opinion  i is a measure of what Smyth and Goodman call the information content of rules Like the directed contribution to  2 13  i allows to distribute the average mutual information of two variables over the rules between them MI  a b  p  a 1   i  a  b  p  a 1   i  a  b   p  a 0   i  a  b  p  a 0   i  a  b  p  a 1   i  a  b  is the directed contribution of the rule a  b to the average mutual information Each rule takes part in the average mutual information by giving or removing its share of infomation Like the  2  the average mutual information can also be written with the contributions of the four opposite rules For all these characteristics we propose to retain the index  i to evaluate the interestingness of association rules However a drawback of  i is that its maximal value is not xed but depends on p  b 1  making the comparison of rules with different consequents difìcult This maximal value is obtained for logical rules i.e rules with no counter-examples  p  a 1 b 0  In order to facilitate the ltering of the most informative rules we normalize  i by assigning the maximal value 1 to the logical rules This amounts to calculating the decrease rate of reduced entropy Deìnition 2 The Directed Information Ratio  DIR  of arule a  b is deìned by DIR  a  b   H  b    H  b/a 1  H  b  if p  b 1  1 If p  b 1  then  H  b  and DIR is not deìned However such rules are obviously to be discarded since they are completely expected   i is indeed 0 for these rules A rule is said to be informative if its DIR is strictly positive Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


A p  b 1  1  2 B p  b 1  1 2 Figure 5 Plot of DIR w.r.t p  a 1 b 0 I refers to independence and E to equilibrium 3 DIR properties The main properties of DIR are given in table 2 It must be noticed that DIR satisìes the three properties that deìne a good interestingness measure according to PiatetskyShapiro it is 0 a t independence it increases with the examples and it decreases with the sizes of the antecedent and consequent variations with all other parameters xed Furthermore DIR has no symmetry  it does not assign the same value to a  b and to its opposite a  b  since it respects the value-based semantics of association rules  it does not either assign the same value to a  b and to its converse b  a  which is better when the user interprets association rules as quasi-implications As shown in gure 5 DIR is a convex decreasing function of the number of counter-examples Among the rule interestingness measures it belongs to the demanding indexes i.e the indexes which decrease quickly with the rst counter-examples and thus allow to better discriminate the good rules larger dispersion of values Range   1 Value for logical rules 1 Value for independence 0 Value for equilibrium 1   H  b   1  0 Variation w.r.t p  a 1 b 1  Variation w.r.t p  a 1  Variation w.r.t p  b 1  Table 2 DIR properties Let us consider a rule  a  b  described by the probabilities p  a 1  p  b 1  and p  a 1 b 0 4  The independence is deìned by p  a 1 b 0 p  a 1 p  b 0  while the equilibrium is deìned by p  a 1 b 0 1 2 p  a 1  By varying p  a 1 b 0 with xed p  a 1 and p  b 1  one can distinguish two different cases for DIR   If p  b 1  1 2  then p  a 1 p  b 0  1 2 p  a 1 so the rule goes through the independence before going through the equilibrium when p  a 1 b 0 increases The measure DIR vanishes at independence and then admits negative values gure 5.\(A  If p  b 1  1 2  then p  a 1 p  b 0  1 2 p  a 1 so the rule goes through the equilibrium before going through the independence when p  a 1 b 0 increases The measure DIR vanishes but does not admit negative values gure 5.\(B DIR allows to reject both the independence and equilibrium situations Indeed in these situations DIR is either negative or worth zero see table 2 By retaining only strictly positive values of DIR informative rules the user discards all the rules whose deviation from indepedence is bad rules between negatively correlated variables and also all the rules whose deviation from equilibrium is bad rules with more counter-examples than examples So the measure must be used with a strictly positive threshold to lter the rules To our knowledge DIR is the only rule interestingness measure which can reject both independence and equilibrium with a xed threshold This approach is completely original for rule interestingness assessment 4 As often in the association rule literature we choose the probability of counter-examples as a parameter but the results are the same with the probability of examples since p  a 1 b 1 p  a 1  p  a  1 b 0  Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


Figure 6 Information-theoretic measures of deviation from independence 4 Comparison to other measures 4.1 Formal comparison In this section we compare DIR to the informationtheoretic measures traditionally used to evaluate rule interestingness see table 1 for formulas  the Shannon conditional entropy which measures the deviation from equilibrium  the mutual information the Theil uncertainty 23  the J-measure 21 and the Gini inde x  2 12 which measure the deviation from independence As the last four measures have similar behaviors see gure 6 we only plot one of them in the comparisons below We choose the J-measure since it is used a lot within the context of association rules in particular it does not assign the same value to a rule a  b and to its converse b  a  As for the conditional entropy it is not the function H c of the table 1 which is plotted in the comparisons below but the complementary function 1  H c  Indeed H c assigns its smallest values to the best rules 5  One generally prefers the opposite behavior for a rule interestingness measure The gures 7.\(A and 7.\(B compare DIR to the conditional entropy and to the J-measure when the probability of counter-examples p  a 1 b 0 increases The gures clearly illustrate that the conditional entropy and the J-measure do not respect the value-based semantics of association rules since they can increase even though the probability of counter-examples increases Moreover one can see that DIR and the conditional entropy have the advantage of systematically assigning the value 1 to the logical rules which are the best rules from an objective point of  5 To generate relevant rules the CN2 algorithm tries to minimize H c  and not to maximize it Items Objects Outputted rules T10.I4.D5k 12 5000 97688 T10.I4.D100k 1000 100000 478894 BREAKDOWNS 92 2883 43930 PROFILES 30 2299 28938 Table 3 Data characteristics view This makes the comparisons among the rules easier and facilitates the choice of a threshold to lter the rules On the contrary for the J-measure and the three other measures of deviation from independence a value can be assigned to a good rule lots of examples few counter-examples even though on other data the same value would be assigned to a bad rule In fact except for the value 0 which always correponds to independence the values taken by these measures cannot be interpreted in an absolute way i.e independently of the data The gures 7.\(A and 7.\(B show that the conditional entropy detects the equilibrium but not the independence it could even take high values at independence On the other hand the J-measure detects the independence but not the equilibrium In all cases ltering the rules on DIR with a strictly positive threshold is enough to reject both independence and equilibrium As illustrated in gure 7.\(B DIR is similar to the conditional entropy when p  b 1  1 2 the functions are partly identical This is what enables DIR to detect the equilibrium when p  b 1  1 2  4.2 Experimental comparison We compare the distributions of DIR to the distributions of other interestingness measures on the association rules mined from four datasets described in table 3 The two rst datasets were generated using the IBM synthetic data generator 6 described in which simulates purchases in a supermarket The two other datasets are a database of lift breakdowns provided by a lift maintenance company and a database of workers psychological proìles used in human resource management The rules were mined with the Apriori algorithm with a l o w support threshold to a v o id the premature elimination of potentially interesting rules As we want here to compare the distributions of measures we choose measures which as DIR  take the value 1 for the logical rules Among the information-theoretic measures only the conditional entropy satisìes this condition So we add to our comparisons two reference measures of rule interestingness which satisfy the condition the conìdence  p  b 1 a 1 and the Loevinger index  1  p  a 1 b 0 p  a 1 p  a 0   They respectively measure the deviation from equilibrium and from independence As gures 6 http://www.almaden.ibm.com/software/quest/Resources/index.shtml Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


A p  b 1  1  2 B p  b 1  1 2 Figure 7 Plot of DIR  J-measure and conditional entropy w.r.t p  a 1 b 0 8.\(A-D show DIR is the most ltering index for the four datasets whichever the threshold chosen between 0 and 1 DIR prunes more rules than the others This is especially useful within the context of association rules where the mining algorithms often generate huge amounts of rules Let us explain why DIR is very ltering In gure 8.\(E in parallel coordinates each line represents a rule The gure shows representative rules from T10.I4.D5k that are judged good by conìdence but not by the Loevinger index they have a good deviation from equilibrium but not from independence On the other hand gure 8.\(F shows representative rules from BREAKDOWNS that are judged good by the Loevinger index but not by conìdence they have a good deviation from independence but not from equilibrium DIR gives bad values to all these rules since it takes into account both independence and equilibrium 5 Conclusion In this article we have presented the Directed Information Ratio  DIR   a new rule interestingness measure which is based on information theory DIR is specially designed for association rules and in particular it respects their value-based semantics by differentiating the opposite rules a  b and a  b  Moreover to our knowledge DIR is the only rule interestingness measure which rejects both independence and equilibrium i.e it discards both the rules whose antecedent and consequent are negatively correlated and the rules which have more counter-examples than examples Experimental studies have also shown that DIR is a very ltering measure which is useful for association rule post-processing To continue this research work we will integrate DIR into a data mining platform in order to experiment with this new measure in real applications Like all the information-theoretic measures DIR is a frequential index This means that it takes into account the size of the data only in an relative way and not in an absolute way see More generally  i n o rder to ha v e a complete assessment of the rules one has to measure not only the deviations from equilibrium and independence but also the statistical signiìcance of these two deviations For example  2  or implication intensity 6 allo w t o m easure the statistical signiìcance of the deviation from independence while IPEE allo ws to measure the statistical signiìcance of the deviation from equilibrium These approaches are complementary to DIR  References  R Agra w al H Mannila R Srikant H T o i v onen and A I Verkamo Fast discovery of association rules pages 307 328 AAAI 1996  R J Bayardo and R Agra w al Mining the most interesting rules In Proceedings of ACM KDDê1999  pages 145Ö154 ACM Press 1999  N M Blachman The amount of information that y gives about x  IEEE Transcations on Information Theory IT14\(1 1968  J Blanchard F  Guillet H Briand and R Gras Assessing rule interestingness with a probabilistic measure of deviation from equilibrium In Proceedings of the 11th international symposium on Applied Stochastic Models and Data Analysis ASMDA-2005  pages 191Ö200 2005  J Blanchard F  Guillet R Gras and H Briand Mesurer la qualit  edesr  egles et de leurs contrapos  ees avec le taux informationnel TIC Revue des Nouvelles Technologies de lêInformation  E-2:287Ö298 2004 Actes EGC2004  J Blanchard P  K untz F  Guillet and R Gras Implication intensity from the basic statistical deìnition to the entropic version In Statistical Data Mining and Knowledge Discovery  pages 473Ö485 Chapman  Hall 2003 Chapter 28 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


A T10.I4.D5k B T10.I4.D100k C BREAKDOWNS D PROFILES E rule sample from T10.I4.D5k F rule sample from BREAKDOWNS Figure 8 Measure distributions on the whole sets of rules A B C D and on two samples of rules E and F in parallel coordinates  S Brin R Motw ani and C Silv erstein Be yond mark et baskets generalizing association rules to correlations SIGMOD Record  26\(2 1997  S Brin R Motw ani J D Ullman and S Tsur  D ynamic itemset counting and implication rules for market basket data SIGMOD Record  26\(2 1997  P  Clark and T  Niblett The CN2 induction algorithm Machine Learning  3\(4 1989  J Holland K Holyoak R Nisbett and P  Thagard Induction  Processes of inference learning and discovery MIT Press 1986  X.-H Huynh F  Guillet and H Briand ARQA T  An e xploratory analysis tool for interestingness measures In Proceedings of the 11th international symposium on Applied Stochastic Models and Data Analysis ASMDA-2005  pages 334Ö344 2005  S Jarosze wicz and D A Simo vici A general measure of rule interestingness In Proceedings of PKDDê2001  pages 253Ö265 Springer-Verlag 2001  I Lerman F oundations in the lik elihood linkage analysis classiìcation method Applied Stochastic Models and Data Analysis  7:69Ö76 1991  B Liu W  Hsu S Chen and Y  Ma Analyzing the subjective interestingness of association rules IEEE Intelligent Systems  15\(5 2000  J Loe vinger  A systematic approach to the construction and evaluation of tests of ability Psychological Monographs  61\(4 1947  B P a dmanabhan and A T uzhilin Une xpectedness as a measure of interestingness in knowledge discovery Decision Support Systems  27\(3 1999  G Piatetsk y-Shapiro Disco v e ry  a nalysis and presentation of strong rules In Knowledge Discovery in Databases  pages 229Ö248 AAAI/MIT Press 1991  J Quinlan editor  C4.5 Programs for Machine Learning  Morgan Kaufmann 1993  M Sebag and M Schoenauer  Generation of rules with certainty and conìdence factors from incomplete and incoherent learning bases In Proceedings of EKAW88  pages 28.1Ö28.20 1988  C Shannon and W  W e a v er  The mathematical theory of communication  University of Illinois Press 1949  P  Smyth and R M Goodman An information theoretic approach to rule induction from databases IEEE Transactions on Knowledge and Data Engineering  4\(4 1992  P N T an V  K umar  and J Sri v asta v a  Selecting the right objective measure for association analysis Information Systems  29\(4 2004  H Theil On the estimation of relationships in v olving qualitative variables American Journal of Sociology  76:103 154 1970  B V a illant P  Lenca and S Lallich A c lustering of interestingness measures In Proceedings of the 7th International Conference on Discovery Science  pages 290Ö297 2004  M J Zaki Mining non-redundant association rules Data Mining and Knowledge Discovery  9\(3 2004 Proceedings of the Fifth IEEE International Conference on Data Mining \(ICDMê05 1550-4786/05 $20.00 © 2005 IEEE 


21  and denote wjk as the set of weights for Yj where    k j k jw 1 1.  A classifier H is defined as YD ? such that it assigns a weight of the correct class label to an instance as  iWdH where deD, and k j i WW ? . For a set of single-class instances I = &lt; \(x1 y1 x2, y2  xn, yn Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE Table 4. Classification accuracy of PART RIPPER, CBA and MMAC Dataset PART RIPPER CBA MMAC Tic-Tac 92.58  97.54  98.60  99.29 Contactlenses 83.33  75.00  66.67  79.69 Led7 73.56 69.34  72.39  73.20 Breastcancer 71.32  70.97  68.18  72.10 Weather 57.14  64.28  85.00  71.66 Heart-c 81.18  79.53  78.54  81.51 Heart-s 78.57  78.23  71.20  82.45 Lymph 76.35  77.70  74.43  82.20 Mushroom 99.81 99.90  98.92  99.78 primarytumor 39.52  36.28  36.49  43.92 Vote 87.81  87.35  87.39  89.21 CRX 84.92  84.92  86.75  86.47 Sick 93.90  93.84  93.88  93.78 Balancescale 77.28  71.68  74.58  86.10 Autos 61.64  56.09  35.79  67.47 Breast-w 93.84  95.42  94.68  97.26 Hypothyroid 92.28 92.28  92.29  92.23 zoo 91.08  85.14  83.18  96.15 kr-vs-kp 71.93 70.24  42.95  68.75 is  m k ii i ydHw m 1  1 ?  , where          yxif yxif yx 0 1  For example, if an item \(A ,a labels  c1    c2  and  c3  7, 5  and 3 times 


labels  c1    c2  and  c3  7, 5  and 3 times respectively, in the training data. Each class label will be assigned a weight, i.e. 7/15, 5/15, and 3/15, respectively for labels  c1    c2  and  c3  This technique assigns the predicted class label weight to the case if the predicted class label matches the case class label. For instance if label  c2  of item \(A, a test data that has  c2  as its class, then the case will be considered a hit, and 5/15 will be assigned to the case 5. Experimental Results We investigated our approach against 19 different datasets from [20] as well as a different datasets for forecasting the behaviour of an optimisation heuristic within a hyperheuristic framework [5, 16]. Stratified tenfold cross-validation was used to derive the classifiers and error rates in the experiments. Cross-validation is a standard evaluation measure for calculating error rate on data in machine learning. Three popular classification techniques a decision tree rule \(PART CBA have been compared to MMAC in terms of classification accuracy, in order to evaluate the predictive power of the proposed method The choice of such learning methods is based on the different strategies they use to generate the rules. Since the chosen techniques are only suitable for traditional classification problems where there is only one class assigned to each training instance, we therefore used classification accuracy derived by only the top-label evaluation measure for fair comparison All experiments were conducted on a Pentium IV 1.6 GH PC.  The experiments of PART and RIPPER were conducted using the Weka software system [20]. Weka stands for Waikato Environment for Knowledge Analysis. It is an open java source code for the machine teaching community that includes implementations of different methods for several different data mining tasks such as classification, clustering, association rule and regression. CBA experiments were conducted using a VC++ implementation version provided by [19]. Finally MMAC was implemented using Java We have evaluated 19 selected datasets from Weka data collection [20], in which, a few of them \(6 reduced by ignoring their integer and/or real attributes Several tests using ten-fold cross-validation have been performed to ensure that the removal of any real/integer attributes from some of the datasets does not significantly affect the classification accuracy. To do so we only considered datasets where the error rate was not more than 6% worse than the error rate obtained on the same dataset before the removal of any real/integer attributes.  Thus, the ignored attributes do not impact on the error rate too significantly Many studies have shown that the support threshold plays a major role in the overall classification accuracy of the set of rules produced by existing associative classification techniques [9, 12]. Moreover, the support value has a larger impact on the number of rules produced in the classifier and the processing time and storage needed during the algorithm rules discovery and generation. From our experiments, we noticed that the support rates that ranged between 2% to 5% usually achieve the best balance between accuracy rates and the size of the resulted classifiers. Moreover, the classifiers derived when the support was set to 2% and 3 achieved high accuracy, and most often better than that of decision trees rule \(PART the MinSupp was set to 3% in the experiments. The confidence threshold, on the other hand, is less complex and does not have a large effect on the behaviour of any associative classification method as support value, and thus it has been set to 30 


Table 4 represents the classification rate of the classifiers generated by PART, RIPPER, CBA and MMAC against 19 benchmark problems from Weka data Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 3 5.00 2 5.00 15.00 5.00 5.00 15.00 2 5.00 3 5.00 4 5.00 55.00 6 5.00 75.00 8 5.00 9 5.00 1 2 3 4 5 6 7 8 9 Nine  Scheduling Runs D iff er en ce  in  A cc u ra cy  CB A To p-label A ll-label A ny-label Figure 3a. Difference of accuracy between MMAC evaluation measures and CBA algorithm 35.00 25.00 15.00 5.00 5.00 15.00 25.00 35.00 45.00 55.00 65.00 75.00 85.00 95.00 1 2 3 4 5 6 7 8 9 Nine  Scheduling Runs D iff er en ce in  A cc u ra cy  P A RT To p-label A ll-label A ny-label Figure 3b. Difference of accuracy between MMAC   evaluation measures and PART 


MMAC   evaluation measures and PART algorithm 0 2 4 6 8 10 12 14 16 18 20 22 24 26 Run1 Run2 Run3 Run4 Run5 Run6 Run7 Run8 Run9 Ten Runs  Scheduling Data N um be r o f R ul es To p Label P A RT CB A Figure 4. Classifier sizes of MMAC \(toplabel the scheduling   data collection. The accuracy of MMAC has been derived using the top-label evaluation measure. Our algorithm outperforms the rule learning methods in terms of accuracy rate, and the won-loss-tied records of MMAC against PART, RIPPER and CBA 13-6-0, 15-4-0 and 154-0, respectively The evaluation measures of MMAC have been compared on 9 solution runs produced by the Peckish hyperheuristic [5] with regard to accuracy, and number of rules produced. Figures 3a and 3b represent the relative prediction accuracy that indicates the difference of the classification accuracy of MMAC evaluation measures with respect to those derived by CBA and PART, respectively. In other words, how much better or worse MMAC measures perform with respect to CBA and PART learning methods. The relative prediction accuracy numbers shown in Figures 3a and 3b are conducted using the formula PART PARTMMAC Accuracy AccuracyAccuracy  and CBA CBAMMAC Accuracy AccuracyAccuracy  respectively. After analysing the charts, we found out that there is consistency between the top-label and label-weight measures, since both of them consider only one class in the prediction. The top-label takes into account the topranked class, and the label-weight considers only the weight for the predicted class that matches the test case Thus, both of these evaluation measures are applicable to traditional single-class classification problems. On the other hand, the any-label measure considers any class in the set of the predicted classes as a hit whenever it matches the predicted class regardless of its weight or rank. Is should be noted that, the relative accuracy of MMAC evaluation methods against dataset number 8 in Figure 3a and 3b, is negative since CBA and PART 


Figure 3a and 3b, is negative since CBA and PART achieved a higher classification rate against this particular dataset A comparison of the knowledge representation produced by our method, PART and CBA has been conducted to evaluate the effectiveness of the set of rules derived. Figure 4 represents the classifiers generated form the hyperheuristic datasets. Analysis of the rules sets indicated that MMAC derives a few more rules than PART and CBA for the majority of the datasets. In particular, the proposed method produced more rules than PART and CBA on 8 and 7 datasets, respectively. A possible reason for extracting more rules is based on the recursive learning phase that MMAC employs to discover more hidden information that most of the associative classification techniques discard, since they only extract the highest confidence rule for each frequent item that survives MinConf Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 6. Conclusions A new approach for multi-class, and multi-label classification has been proposed that has many distinguishing features over traditional and associative classification methods in that it \(1 that contain rules with multiple labels, \(2 evaluation measures for evaluating accuracy rate, \(3 employs a new method of discovering the rules that require only one scan over the training data, \(4 introduces a ranking technique which prunes redundant rules, and ensures only high effective ones are used for classification, and \(5 discovery and rules generation in one phase to conserve less storage and runtime. Performance studies on 19 datasets from Weka data collection and 9 hyperheuristic scheduling runs indicated that our proposed approach is effective, consistent and has a higher classification rate than the-state-of-the-art decision tree rule \(PART and RIPPER algorithms. In further work, we anticipate extending the method to treat continuous data and creating a hyperheuristic approach to learn  on the fly   which low-level heuristic method is the most effective References 1] R. Agrawal, T. Amielinski and A. Swami. Mining association rule between sets of items in large databases In Proceeding of the 1993 ACM SIGMOD International Conference on Management of Data, Washington, DC May 26-28 1993, pp. 207-216 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rule. In Proceeding of the 20th International Conference on Very Large Data Bases, 1994, pp. 487   499 3] M. Boutell, X. Shen, J. Luo and C. Brown. Multi-label semantic scene classification. Technical report 813 Department of Computer Science, University of Rochester Rochester , NY 14627 &amp; Electronic Imaging Products R &amp D, Eastern Kodak Company, September 2003 4] A. Clare and R.D. King. Knowledge discovery in multilabel phenotype data. In L. De Raedt and A. Siebes editors, PKDD01, volume 2168 of Lecture Notes in Artificial Intelligence, Springer - Verlag, 2001,  pp. 42-53 5] P. Cowling and K. Chakhlevitch. Hyperheuristics for Managing a Large Collection of Low Level Heuristics to Schedule Personnel. In Proceeding of 2003 IEEE conference on Evolutionary Computation, Canberra Australia, 8-12 Dec 2003 6] R. Duda, P. Hart, and D. Strok. Pattern classification Wiley, 2001 7] E. Frank and I. Witten. Generating accurate rule sets without global optimisation. In Shavlik, J., ed., Machine Learning: In Proceedings of the Fifteenth International 


Learning: In Proceedings of the Fifteenth International Conference, Madison, Wisconsin. Morgan Kaufmann Publishers, San Francisco, CA, pp. 144-151 8] J. Furnkranz. Separate-and-conquer rule learning Technical Report TR-96-25, Austrian Research Institute for Artificial Intelligence, Vienna, 1996 9] W. Li, J. Han and J. Pei. CMAR: Accurate and efficient classification based on multiple class association rule. In ICDM  01, San Jose, CA, Nov. 2001, pp. 369-376 10 ] T. Joachims. Text categorisation with Support Vector Machines: Learning with many relevant features. In Proceeding Tenth European Conference on Machine Learning, 1998,  pp. 137-142 11] T. S. Lim, W. Y. Loh and Y. S. Shih. A comparison of prediction accuracy, complexity and training time of thirtythree old and new classification algorithms. Machine Learning, 39, 2000 12] B. Liu, W. Hsu and Y. Ma. Integrating Classification and association rule mining. In KDD  98,  New York, NY, Aug 1998 13] J.R. Quinlan. C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann, San Francisco, 1993 14] J.R. Quinlan. Generating production rules from decision trees. In Proceeding of the 10th International Joint Conferences on Artificial Intelligence,  Morgan Kaufmann San Francisco, 1987, pp. 304-307 15] R. Schapire and Y. Singer, "BoosTexter: A boosting-based system for text categorization," Machine Learning, vol. 39 no. 2/3, 2000, pp. 135-168 16] F. Thabtah, P. Cowling and Y. Peng. Comparison of Classification techniques for a personnel scheduling problem. In Proceeding of the 2004 International Business Information Management Conference, Amman, July 2004 17]Y. Yang. An evaluation of statistical approaches to text categorisation. Technical Report CMU-CS-97-127 Carnegie Mellon University, April 1997 18] X. Yin and J. Han. CPAR: Classification based on predictive association rule. In  SDM  2003, San Francisco CA, May 2003 19]CBA:http://www.comp.nus.edu.sg/~dm2/ p_download.html 20] Weka: Data Mining Software in Java http://www.cs.waikato.ac.nz/ml/weka 21] M. J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. New algorithms for fast discovery of association rules. In Proceedings of the 3rd KDD Conference, Aug. 1997 pp.283-286 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 R=1000 1e-4 0.2 High 1.153e3 1.154e3 3.180e3 2.698e3 45 45.0 597 0.214 0.9900 0.0071 1.0000 0.0162 2e-4 2e-4 Low 0.4017 0.4024 109.291 71.994 4 4.0 39.7 0.996 0.9960 0.0075 0.9995 0.0012 2e-3 1.0 1e-3 Medium Medium 0.4128 0.4129 17.488 19.216 6 6.0 29.6 0.992 0.9938 0.0018 0.9987 0.0034 R=0.1 0.01 4e-3 High 0.4612 0.4722 4.1029 5.410e3 12 12.0 28.0 0.992 0.9902 0.0045 1.0000 0.0132 0.04 1e-8 Low 0.0250 0.0272 253.917 155.263 3 3.0 38.0 1.002 0.9991 0.0001 0.9994 0.0008 1e-3 1e4 1e-7 High Medium 0.0338 0.0290 17.303 12.837 3 3.0 14.5 0.332 0.9900 0.0058 0.9951 0.0045 R=1e-5 1e-2 1e-6 High 0.0918 0.0557 1.6071 1.225e5 8 8.0 9.9 0.992 0.9906 0.0034 0.9994 0.0142 1e-3 TABLE II SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=100, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 2e-4 Low 0.0408 0.0407 2.9255 2.0060 5 5.0 30.3 0.996 0.9933 0.0016 0.9998 0.0020 0.02 1.0 1e-3 Medium Medium 0.0446 0.0446 0.5143 333.138 10 10.0 27.8 0.996 0.9900 0.0042 0.9995 0.0093 R=0.01 0.1 4e-3 High 0.0763 0.1062 0.1580 3.402e3 84 84.0 90.9 0.793 0.9900 0.0099 1.0000 0.0334 0.4 TABLE III SIMULATION RESULTS FOR: TRACKING REGIME PD=0.9, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.166e3 1.167e3 1.073e4 3.357e4 11 12.2 257 0.542 0.9900 0.0089 1.000 0.0073 5e-5 1e-4 0.1 Low Medium 1.167e3 1.167e3 5.509e3 1.481e4 21 23.2 309 0.858 0.9900 0.0067 1.0000 0.0052 R=1000 1e-4 0.2 High 1.168e3 1.170e3 3.180e3 6.979e3 46 50.6 475 0.798 0.9900 0.0089 1.000 0.0022 2e-4 3] T. Fortmann, Y. Bar-Shalom, Y. Scheffe, and S. B. Gelfand  Detection Thresholds for Tracking in Clutter- A Connection Between Estimation and Signal Processing  IEEE Trans Auto. Ctrl., Mar 1985 4] S. B. Gelfand, T. Fortmann, and Y. Bar-Shalom  Adaptive Threshold Detection Optimization for Tracking in Clutter  IEEE Trans. Aero. &amp; Elec. Sys., April 1996 5] Ch. M. Gadzhiev  Testing the Covariance Matrix of a Renovating Sequence Under Operating Control of the Kalman Filter  IEEE Auto. &amp; Remote Ctrl., July 1996 6] L. C. Ludeman, Random Processes: Filtering, Estimation and Detection, Wiley, 2003 7] L. Y. Pao and W. Khawsuk  Determining Track Loss Without Truth Information for Distributed Target Tracking Applications  Proc. Amer. Ctrl. Conf., June 2000 8] L. Y. Pao and R. M. Powers  A Comparison of Several Different Approaches for Target Tracking in Clutter  Proc Amer. Ctrl. Conf., June 2003 9] X. R. Li and Y. Bar-Shalom  Stability Evaluation and Track Life of the PDAF Tracking in Clutter  IEEE Trans. Auto Ctrl., May 1991 10] X. R. Li and Y. Bar-Shalom  Performance Prediction of 


10] X. R. Li and Y. Bar-Shalom  Performance Prediction of Tracking in Clutter with Nearest Neighbor Filters  SPIE Signal and Data Processing of Small Targets, July 1994 11] X. R. Li and Y. Bar-Shalom  Detection Threshold Selection for Tracking Performance Optimization  IEEE Trans. on Aero. &amp; Elect. Sys., July 1994 12] D. Salmond  Mixture Reduction Algorithms for Target Tracking in Clutter  SPIE Signal and Data Processing of Small Targets, Oct. 1990 13] L. Trailovic and L. Y. Pao  Position Error Modeling Using Gaussian Mixture Distributions with Application to Comparison of Tracking Algorithms  Proc. Amer. Ctrl. Conf., June 2003 4323 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207ñ216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intíl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intíl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





