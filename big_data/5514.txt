AbstractIn this paper, we propose a two-phase fuzzy mining approach based on a tree structure to discover fuzzy frequent itemsets from a quantitative database. A simple tree structure called the upper-bound fuzzy frequent-pattern tree abbreviated as UBFFP tree purpose. The two-phase fuzzy mining approach can easily derive the upper-bound fuzzy supports of itemsets through the tree and prune unpromising itemsets in the first phase, and then finds the actual frequent fuzzy itemsets in the second phase. Experimental results also show the good performance of the proposed approach I. INTRODUCTION In the past, many algorithms for mining association rules from transactions were proposed [1-3]. Most of the approaches were based on the Apriori algorithm [1-2], which generated and tested candidate itemsets level by level. This processing way might, however, cause iterative database scans and high computational costs. Han et al. thus proposed the Frequent-Pattern-tree \(FP-tree mining association rules without generation of candidate itemsets [9] to get the better performance Papadimitriou et al. proposed a tree structure called fuzzy frequent pattern tree \(abbreviated as FFPT association rules [15]. Lin et al. also proposed a fuzzy FP tree [13] and a compressed fuzzy frequent pattern tree abbreviated as CFFP tree  itemsets. Different sorting strategies were used in the two approaches to construct tree structures In this paper, we propose a two-phase mining approach to avoid the overhead of the attached arrays in the CFFP tree. A simple tree structure called the upper-bound fuzzy frequent-pattern tree \(abbreviated as UBFFP tree designed to help mine fuzzy frequent itemsets from a quantitative database. Based on the concept, the UBFFP-growth mining approach is proposed, which adopts a two-phase approach to drive fuzzy frequent itemsets from an UBFFP tree. Experimental results also show that the proposed UBFFP tree has a better performance than the C. W. Lin is with the Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, 701, Taiwan, R.O.C e-mail: p7895122@mail.ncku.edu.tw Tzung-Pei Hong is with the Department of Computer Science and 


Information Engineering, National University of Kaohsiung, Kaohsiung 811, Taiwan. He is also with the Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, 80424, Taiwan R.O.C. \(corresponding author; phone: +886+7+5919191; fax 886+7+5919374; e-mail: tphong@nuk.edu.tw W. H. Lu is with the Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, 701, Taiwan, R.O.C e-mail: whlu@mail.ncku.edu.tw CFFP tree both in the execution time and the number of tree nodes II. REVIEW OF RELATED WORKS A. Mining Algorithms for Fuzzy Association Rules In the past, Srikant et al. proposed a quantitative association rule mining method to find association rules by partitioning the quantitative database and transforming the problem into binary one [17]. The fuzzy set theory is concerned with quantifying and reasoning using natural language. Chan et al. thus proposed an F-APACS algorithm to mine fuzzy association rules [4]. Kuok et al. proposed a fuzzy mining approach to handle numerical data in databases and to derive fuzzy association rules [12]. Hong et al. then proposed a fuzzy mining algorithm to mine fuzzy rules from quantitative databases [10 As to using a tree structure to fuzzy data mining Papadimitriou et al. proposed a tree structure called fuzzy frequent pattern tree \(abbreviated as FFPT association rules [15]. Lin et al. also proposed a fuzzy FP-tree structure to mine fuzzy association rule [13]. In that approach, the fuzzy regions in each transaction were sorted in a descending order of their fuzzy values in that transaction Two transactions with the same fuzzy regions but different orders were put into two different paths of the tree. They then proposed a novel tree structure called the compressed fuzzy frequent pattern tree \(CFFP tree  branches with the same fuzzy regions but different orders Although the node number in the CFFP tree could be significantly reduced, each node in the tree had to keep the membership value of the fuzzy frequent 1-itemset within it as well as the membership values of its super-itemsets in the path. Some additional memory overloads were thus needed Some related researches about fuzzy mining are still in progress [5-6, 16 


B. The FP-Growth Algorithm Han et al. proposed the Frequent-Pattern-tree structure FP-tree frequent itemsets without generation of candidate itemsets 9]. The mining algorithm consisted of two phases. The first phase focused on constructing the FP-tree from a database and the second phase focused on deriving frequent patterns from the FP-tree. The tree construction process was executed tuple by tuple, from the first transaction to the last one. After the FP tree was constructed from a database, the FP-growth mining algorithm was then executed to find all frequent itemsets [9]. A conditional FP tree was generated for each frequent item, and the frequent itemsets with the processed item could be recursively derived from the conditional A Two-Phase Fuzzy Mining Approach Chun-Wei Lin, Tzung-Pei Hong and Wen-Hsiang Lu 978-1-4244-8126-2/10/$26.00 2010 IEEE FP-tree structure. Several other algorithms based on the FP-tree structure have also been proposed [8, 11 III. THE PROPOSED UBFFP CONSTRUCTION ALGORITHM A. The UBFFP Tree Construction Algorithm The proposed mining approach integrates the fuzzy-set concepts, the FP-tree-like algorithm and the two-phase processing way to efficiently derive fuzzy frequent itemsets from quantitative transactions. It first constructs an UBFFP tree from the given transactions and then uses the two-phase UBFFP tree to mine frequent fuzzy itemsets from the tree The UBFFP tree construction algorithm is stated as follows The UBFFP tree construction algorithm INPUT: A quantitative database consisting of n transactions and m items, a set of membership functions, and a predefined minimum support threshold s OUTPUT: A constructed UBFFP tree STEP 1: Transform the quantitative value vij of each item Ij in the i-th transaction into a fuzzy set fij represented as \(fij1/Rj1 + fij2/Rj2 + + fijhj/Rjhj membership functions, where hj is the number of fuzzy regions for Ij, Rjl is the l-th fuzzy region of Ij jhl 1 , and fijl is vijs fuzzy membership value in region Rjl STEP 2: Calculate the scalar cardinality countjl of each 


fuzzy region Rjl in all the transactions as    n i ijljl fcount 1  STEP 3: Find max-countj 1 jl h l countMAX j for j = 1 to m where hj is the number of fuzzy regions for item Ij and m is the number of items. Let max-Rj be the region with max-countj for item Ij. It will then be used to represent the fuzzy characteristic of item Ij in the later mining process STEP 4: Check whether the value max-countj of a fuzzy region max-Rj, j = 1 to m, is larger than or equal to the predefined minimum count n*s. If a fuzzy region max-Rj satisfies the above condition, put the fuzzy region with its count in L1. That is L1 = {max-Rj | max-countj n*s, 1 jm STEP 5: While executing the above steps, also find the occurrence number o\(max-Rj max-Rj in the quantitative database. It is used to decide the item order later STEP 6: Build the Header_Table by keeping the fuzzy regions in L1 in the descending order of their occurrence numbers STEP 7: Remove the fuzzy regions of the items not existing in L1 from the transactions of the transformed database. Sort the remaining fuzzy regions in each transaction according to the order of the fuzzy regions in the Header_Table STEP 8: Initially set the root node of the UBFFP tree as root STEP 9: Insert the transactions in the transformed database into the UBFFP tree tuple by tuple by the following substeps Substep 9-1: If a fuzzy region max-Rj in the currently processed i-th transaction 


has appeared at the corresponding path of the UBFFP tree, add the membership value of the region max-Rj in the transaction to the node with max-Rj Otherwise, add a new node with max-Rj to the end of the corresponding path and set the membership value of the region max-Rj in the currently processed i-th transaction as the value in the node Substep 9-2: Insert a link from the node of max-Rj in the last branch to the current node If there is no such a branch, insert a link from the entry of max-Rj in the Header_Table to the current node After STEP 9, the final UBFFP tree is built. In STEP 9, a corresponding path is a path in the tree which corresponds to the fuzzy regions to be processed in a transaction according to the order of fuzzy regions appearing in the Header_Table B. An Example Assume the quantitative database shown in Table I is used as the example. It consists of 8 transactions and 5 items denoted A to E. The number behind an item represents the amount of the item purchased TABLE I THE QUANTITATIVE DATABASE IN THE EXAMPLE TID Items 1 A:5, C:10, D:2, E:9 2 A:8, B:2, C:3 3 B:3, C:9 4 A:5, B:3, C:10, E:3 5 A:7, C:9, D:3 6 B:2, C:8, D:3 7 A:5, B:2, C:5 8 A:3, C:10, D:2, E:2 Also assume the predefined minimum support threshold is set at s = 30% and the fuzzy membership functions shown in Figure 1 are used for all the items In this example, amounts are represented by three linguistic terms as Low, Middle and High. Thus, three fuzzy membership values are produced for each item according to 


the predefined membership functions. Note that the proposed approach also works when the membership functions of the items are not the same and the function numbers are arbitrarily given. The UBFFP tree is constructed from the given transactions as follows 0      1   6                   11 Amount Low Middle High 1 Membership Value Fig. 1.  The membership functions used in the example The quantitative values of all the items in the transactions are represented as fuzzy sets, which are shown in Table II TABLE II THE FUZZY SETS TRANSFORMED FROM THE DATA IN TABLE I TID Items 1 0.2 0.8 0.2 0.8 0.8 0.2 0.4 0.6  A Low A Middle C Middle C High D Low D Middle E Middle E High b 2 0.6 0.4 0.8 0.2 0.6 0.4 A Middle A High B Low B Middle C Low C Middle  3 0.6 0.4 0.4 0.6 B Low B Middle C Middle C High  4 LowEMiddleEHighCMiddleCMiddleBLowBMiddleALowA 6.0  4.0  8.0  2.0  4.0  6.0  


8.0  2.0  5 0.8 0.2 0.4 0.6 0.6 0.4  A Middle A High C Middle C High D Low D Middle  6 LowDMiddleDHighCMiddleCMiddleBLowB 6.0  4.0  4.0  6.0  2.0  8.0  7 0.2 0.8 0.8 0.2 0.2 0.8 ALow A Middle B Low BMiddle C Low C Middle  8 0.6 0.4 0.2 0.8 0.8 0.2 0.8 0.2 A Low AMiddle C Middle C High D Low D Middle E Low E Middle   Since the counts of \(A.Middle B.Low C.High D.Low in the set of L1, which will be used to construct the UBFFP tree later. The fuzzy regions in L1 are then sorted in the descending order of their occurrence numbers and are put in the Header_Table. The fuzzy regions not existing in L1 are removed from the transactions from Table II. The remaining fuzzy regions in each transaction are then sorted according to the order in STEP 6. After this step, the updated transactions are shown in Table III The root of the UBFFP tree is initially set as root. The 


updated transactions in Table III are used to construct the UBFFP tree tuple by tuple from the first transaction to the last one. The resulting Header_Table and the constructed UBFFP tree are shown in Figure 2 Header_Table Items Count A.Middle 4.2 C.High 4.0 B.Low 3.6 D.Low 2.8 root A.Middle 4.2 C.High 3.0 D.Low 2.2 B.Low 1.6 C.High 1.0 B.Low 1.4 D.Low 0.6 B.Low 0.6 Fig. 2.  The finally constructed UBFFP tree IV. THE PROPOSED UBFFP-GROWTH MINING ALGORITHM After the UBFFP tree is constructed, the desired fuzzy frequent itemsets can then be derived by the proposed UBFFP-growth mining algorithm through the tree. It is stated as follows TABLE III THE UPDATED TRANSACTIONS AFTER STEP 7 TID Frequent fuzzy regions 1 0.8 0.8 0.8 A Middle C High D Low 2 0.6 0.8  


A Middle B Low 3 0.6 0.6 C High B Low 4 LowBHighCMiddleA 6.0  8.0  8.0 5 0.8 0.6 0.6 A Middle C High D Low 6 LowDLowBHighC 6.0  8.0  4.0 7 0.8 0.8 A Middle B Low 8 0.4 0.8 0.8  A Middle C High D Low A. The Mining Algorithm The proposed UBFFP-growth mining algorithm processes the fuzzy frequent 1-itemsets in the Header_Table one by one and bottom-up. It is divided into two phases. In the first phase, the approach first finds the upper-bound fuzzy supports of itemsets through the tree using the minimum operator for fuzzy union. In the second phase, the transferred database is then re-scanned to find the actual values of the remaining candidate fuzzy itemsets, and output the desired ones to users. The UBFFP-growth mining algorithm is described below The UBFFP-growth mining algorithm INPUT: The constructed UBFFP tree, its corresponding Header_Table, the transferred database, and the predefined minimum support threshold s OUTPUT: The fuzzy frequent itemsets 


STEP 1: Process the fuzzy regions in the Header_Table one by one and bottom-up by the following steps. Let the currently processed region is max-Rj STEP 2: Find all the nodes with the fuzzy region max-Rj in the UBFFP tree through the links initially from the Header_Table STEP 3: The super-itemsets in the path of the currently processed node are thus traced to generate the fuzzy k-itemsets recursively \(k  2 upper-bound value is thus set as the minimum operator of the generated fuzzy regions. The generated fuzzy itemsets are then against the predefined minimum count to find the satisfied fuzzy k-itemsets. The satisfied fuzzy k-itemsets are used to generate fuzzy \(k+1 This iteration is then processed until no more fuzzy itemsets are generated STEP 4: Sum the values of the same fuzzy itemsets together STEP 5: If the summed value of a fuzzy itemset in STEP 4 is larger than or equal to the predefined minimum count \(n*s STEP 6: Repeat STEPs 2 to 5 for another fuzzy region until all the fuzzy regions in the Header_Table are processed STEP 7: The transferred database in STEP 7 in the construction part of the UBFFP tree is then re-scanned to get its actual value of each candidate fuzzy itemset in C against the predefined minimum count. Output the satisfied candidate fuzzy itemsets as the fuzzy frequent itemsets B. An Example For the constructed UBFFP tree in Figure 2, the proposed UBFFP-growth mining algorithm finds the fuzzy frequent itemsets as follows The fuzzy regions in the Header_Table are processed one by one and bottom-up. In this example, the fuzzy regions are processed in the order of D.Low, B.Low, C.High and A.Middle. The fuzzy region D.Low is first processed The nodes with the currently processed fuzzy region D.Low in the UBFFP tree are found. The super-itemsets in the paths of the processed nodes are then used to form the 


candidate fuzzy itemsets recursively. The upper-bound values of the same fuzzy itemsets are summed together. In this case, there are two same fuzzy 2-itemsets of \(C.High D.Low as 2.8. It is thus considered as the candidate fuzzy itemsets The above steps are repeated for another fuzzy region until all the fuzzy regions in the Header_Table are processed The transferred database in Table III is then re-scanned to find the actual fuzzy supports of the derived itemsets after STEP 6. After that, the finally derived fuzzy frequent itemsets are shown in Table IV TABLE IV THE FINALLY DERIVED FUZZY FREQUENT ITEMSETS 1-itemsets Itemset Fuzzy support A.Middle 4.2 B.Low 3.6 C.High 4.0 D.Low 2.8 2-itemsets Itemset Fuzzy support A.Middle, C.High C.High, D.Low V. EXPERIMENTAL RESULTS The experiments were performed in Java on an Intel Core 2 Duo PC with a 1.8G Hz processor and 2G main memory running the Microsoft Windows XP operating system. A real dataset called mushroom was used in the experiments [7 Random quantitative values from the range [1, 11] were assigned to the items in the transactions in a uniform distribution. The quantitative database was then transferred into fuzzy regions by the predefined membership functions Two and three membership functions for each item were tested, respectively. Figure 3 shows a comparison of the execution time by the fuzzy FP-tree approach [13], the compressed fuzzy frequent-pattern \(CFFP 14] and the proposed two-phase approach on the UBFFP tree when there were two membership functions for an item The execution time included the tree construction and the mining. The minimum support thresholds were set from 43 to 47%, with 1% increment each time Fig. 3.  The execution time of the three approaches for two fuzzy regions 


It is obvious from Figure 3 that the proposed two-phase algorithm ran faster than the other two in different minimum support thresholds. The numbers of tree nodes generated from the three algorithms for two fuzzy regions are shown in Figure 4, where the two lines for the CFFP tree and the UBFFP tree are the same Fig. 4.  The numbers of tree nodes generated from the three approaches for two fuzzy regions It could be seen from Figure 4 that the numbers of nodes in the CFFP tree and in the UBFFP tree were the same and less than those in the Fuzzy FP tree. This was because the building of both the CFFP tree and the UBFFP tree followed the descending occurrence frequencies of fuzzy regions, but the construction of the fuzzy FP-tree followed the descending membership values of fuzzy regions in each transaction. In the fuzzy FP-tree approach, two transactions with the same fuzzy regions but different orders were put into two different paths of the tree. The fuzzy FP tree was thus loose and huge. Besides, the UBFFP tree didnt need to store the additional array in each tree node, but the CFFP needed VI. CONCLUSION In this paper, we have simplified the CFFP tree structure to avoid storing the additional arrays when constructing the tree. We have also proposed a two-phase fuzzy mining algorithm to find fuzzy frequent itemsets based on the tree The approach can easily derive the upper-bound fuzzy supports of itemsets through the tree and prune unpromising itemsets in the first phase, and then finds the actual frequent fuzzy itemsets in the second phase. Experimental results have also shown that the proposed approach generates the same number of tree nodes as the CFFP-tree algorithm, but less than the fuzzy FP-tree algorithm. Besides, the proposed approach also ran faster than the other two REFERENCES 1] R. Agrawal and R. Srikant, "Fast algorithms for mining association rules in large databases," The 20th International Conference on Very Large Data Bases, pp. 487-499, 1994 2] R. Agrawal, T. Imielinski, and A. Swami, "Mining association rules between sets of items in large databases," International Conference on Management of Data, pp. 207-216, 1993 3] R. Agrawal, T. Imielinski, and A. Swami, "Database mining: A 


performance perspective," IEEE Transactions on Knowledge and Data Engineering, vol. 5, pp. 914-925, 1993 4] K. C. C. Chan and W. H. Au, "Mining fuzzy association rules," The 6th International Conference on Information and Knowledge Management, pp. 209-215, 1997 5] M. Delgado, N. Marin, D. Sanchez, and M. A. Vila, "Fuzzy association rules: General model and applications," IEEE Transactions on Fuzzy Systems, vol. 11, pp. 214-225, 2003 6] D. Dubois, E. Hullermeier, and H. Prade, "A systematic approach to the assessment of fuzzy association rules," Data Mining and Knowledge Discovery, vol. 13, pp. 167-192, 2006 7] B. Goethals, Frequent itemset mining dataset repository. Available http://fimi.cs.helsinki.fi/data 8] G. Grahne and J. Zhu, "Fast algorithms for frequent itemset mining using fp-trees," IEEE Transactions on Knowledge and Data Engineering, vol. 17, pp. 1347-1362, 2005 9] J. Han, J. Pei, Y. Yin, and R. Mao, "Mining frequent patterns without candidate generation: A frequent-pattern tree approach," Data Mining and Knowledge Discovery, vol. 8, pp. 53-87, 2004 10] T. P. Hong, C. S. Kuo, and S. C. Chi, "A data mining algorithm for transaction data with quantitative values," The 7th National Conference on Fuzzy Theory and Its Applications, pp. 874-878, 1999 11] T. P. Hong, C. W. Lin, and Y. L. Wu, "Incrementally fast updated frequent pattern trees," Expert Systems with Applications, vol. 34, pp 2424-2435, 2008 12] C. M. Kuok, A. Fu, and M. H. Wong, "Mining fuzzy association rules in databases," SIGMOD Record, vol. 27, pp. 41-46, 1998 13] C. W. Lin, T. P. Hong, and W. H. Lu, "Mining fuzzy association rules based on fuzzy fp-trees," The 16th National Conference on Fuzzy Theory and Its Applications, pp. 11-16, 2008 14] C. W. Lin, T. P. Hong, and W. H. Lu, "Fuzzy data mining based on the compressed fuzzy fp-trees," IEEE International Conference on Fuzzy Systems, pp. 1068-1072, 2009 15] S. Papadimitriou and S. Mavroudi, "The fuzzy frequent pattern tree The 9th WSEAS International Conference on Computers, pp. 1-7 2005 16] R. Srikant and R. Agrawal, "Mining sequential patterns Generalizations and performance improvements," The 5th International Conference on Extending Database Technology Advances in Database Technology, pp. 3-17, 1996 17] R. Srikant and R. Agrawal, "Mining quantitative association rules in large relational tables," SIGMOD Record, vol. 25, pp. 1-12, 1996 


18] Y. Sucahyo and R. Gopalan, "Building a more accurate classifier based on strong frequent patterns," Lecture Notes in Computer Science vol. 3339, pp. 1036-1042, 2005 


time of these two algorithms gets increasing, but the PHP is much speeder than the other, especially when count in 6. so wen know the more frequent itemsets and their items, the more efficient of the algorithm V. THE APPLICATION After we got the frequent itemset by the improved PHP algorithm, we can generate the strong associate rule that satisfied minimum support and minimum confidence. The confidence can be computed by Confidence\(A ? B A | B A  B support_count\(A Support_count\(A  B support_count\(A And then we can use confidence threshold strengthen the association, we use improved PHP algorithm dig X company 2002 year sale data, minimum support is 5, 3-itemset, they are 83,549,915}, {485,558,1290}, {454,1097,1546 83,549,982}, {631,980,1490}, {454,1103,1546 810,1026,1469}, {360,830,1036}, and their none null subset as for frequent 3-item {83,549,915}, we can get 83 ? 549? 915,      confidence = 5/9 = 56 83 ? 915? 549,      confidence = 5/5 = 100 915 ? 549? 83,      confidence = 5/5 = 100 83? 915 ? 549,      confidence = 5/50 = 10 549? 83 ? 915,      confidence = 5/78 = 6 915? 83 ? 549,      confidence = 5/73= 7 If the minimum confidence threshold is 80%, only if 83 ? 915 ? 549and 915 ? 549 ? 83 can generate strong associate rules, after these rules, we can not only arrange the shelf of related goods in pairs or groups, but also combine those goods that have most consumers, promote the consumer the pretermission goods. For example, the stronger associate rule 83 ? 915? 549, from the database field product, we know product_id corresponding product_name, this stronger rule is pillow ? pillow clothe ? bedsheet , if there is someone bought pillow and pillow clothe, then promote good bedsheet is an efficient way of sale VI. CONCLUSION We put Hash candidates k-itemset into affair of Dk, that is itemset include in affair, so CPU running time decreased improved PHP algorithm is more efficiency, we can use this improved PHP algorithm in commercial database digging and 


other wide usages REFERENCES 1] YANG Xuejun. The application of CRM[J].computer application,2002\(5 2] CHEN Shuangqiu, LIU Dinghong, LI Hongxing, the costumer analysis and design based on data warehouse[J].computer engineering and application,2001\(4 3] Jawei Han and Micheline Kamber,Data Mining:Comcepts and Techniques \(2001 4] J.D.Holt,and S.M.Chung, Efficient Mining of Association Rules in Text Databases CIKM99,  Kansas City, USA, pp.234-242, \(Nov.1999 5] J.S.Park, M.S.Chen and P.S.Yu, Using a Hash-Based Method with Transaction Trimming for Mining Association Rules, IEEE Transactions on Knowledge and Data Engineering, Vol.9, No.5 Sept/Oct, 1997 6] S. A. zel and H. A. Gvenir, An Algorithm for Mining Association Rules Using Perfect Hashing and Database Pruning, in: Proceedings of  the Tenth Turkish Symposium on Artificial Intelligence and Neural Networks \(TAINN'2001 Eds Gazimagusa, T.R.N.C. \(June 2001 7] SUN Zhengxi, The theory and technology of Intelligent control M].Beijing Qinghua press, 1997 8] Mitani, Koji, CRM pursues economies of depth [J]. Japanese Journal of Diamond Harvard Business, 1999, \(617  the Tenth Turkish Symposium on Artificial Intelligence and Neural Networks \(TAINN'2001 Eds Gazimagusa, T.R.N.C. \(June 2001 7] SUN Zhengxi, The theory and technology of Intelligent control M].Beijing Qinghua press, 1997 8] Mitani, Koji, CRM pursues economies of depth [J]. Japanese Journal of Diamond Harvard Business, 1999, \(617  


algorithm could not extract the correct hierarchy with 30 assigning five labels incorrectly to the root label. None of the HE algorithms could extract the correct hierarchy in the absence of 40% multi-labels. With 40% and Voting, the number of labels falsely assigned to the root was 13, while with GT it was only three. For BoosTexter, Voting assigned two labels wrongly to the root label in the experiment with TABLE I 20 NEWSGROUPS ALL, -20%, -30% AND -40% RESULTS Measure all 20% 30% 40%ARAM FAM kNN BoosT. ARAM FAM kNN BoosT. ARAM FAM kNN BoosT. ARAM FAM kNN BoosT A 0.635 0.638 0.429 0.549 0.613 0.633 0.383 0.456 0.596 0.619 0.322 0.412 0.563 0.591 0.255 0.387 F1 0.694 0.696 0.565 0.677 0.675 0.688 0.528 0.604 0.662 0.677 0.469 0.566 0.640 0.657 0.392 0.542 F 0.691 0.692 0.480 0.605 0.671 0.688 0.429 0.507 0.658 0.676 0.364 0.465 0.630 0.652 0.296 0.441 OE 0.221 0.220 0.336 0.222 0.259 0.236 0.387 0.275 0.273 0.259 0.415 0.301 0.301 0.291 0.434 0.316 RL 0.100 0.098 0.124 0.073 0.108 0.110 0.128 0.077 0.098 0.103 0.132 0.079 0.101 0.106 0.135 0.082 C 4.188 4.168 6.080 4.164 4.397 4.446 6.184 4.286 4.246 4.340 6.326 4.328 4.334 4.463 6.397 4.379 AP 0.789 0.790 0.677 0.778 0.774 0.782 0.657 0.758 0.769 0.774 0.645 0.747 0.759 0.764 0.638 0.740 AUPRC 0.775 0.772 0.618 0.749 0.743 0.727 0.581 0.691 0.733 0.722 0.555 0.671 0.715 0.708 0.535 0.660 H-loss 0.103 0.123 0.121 0.094 0.102 0.098 0.124 0.108 0.106 0.103 0.132 0.117 0.115 0.111 0.145 0.122 Wins 1 5 0 3 1 6 0 2 2 6 0 1 2 6 0 1 LCAPD 0 0 0 0 0 0 0 0 0 0 0.12 0 0.05 0 0 0 0.51 0.26 0.17 0 CTED 0 0 0 0 0 0 0 0 0 0 0.14 0 0.07 0 0 0 0.50 0.21 0.21 0 TO* 0 0 0 0 0 0 0 0 0 0 0.11 0 0.05 0 0 0 0.39 0.18 0.15 0 30% removed labels and and six labels in the experiment with 40% removed labels. GT resulted in zero distances in the both cases. Assigning more labels to the root creates more shallow and wider hierarchies \(trivial case as stated before The good hierarchy extraction with ART networks demonstrates the system robustness  even with strongly damaged data the system can rebuild the original hierarchy C. RCV1-v2 Dataset The next experiment was based on the tokenized version of 


the RCV1-v2 dataset introduced in [21]. Only the topics label set consisting of 103 labels arranged in a hierarchy of depth four is examined here. Documents of the original training set of 23,149 were converted to TF-IDF weights and normalized Afterwards the set was splitted in 15,000 randomly selected documents as training and the remaining as test samples In this case, the Voting variant of HE applied to the TTML resulted in the LCAPD, CTED and TO* values 0.12, 0.15 and 0.13, respectively. The corresponding values of the GT variant are 0.05, 0.07 and 0.05. The poor performance of the Voting method is due to the fact that for the TTML only very high threshold values succeed in removing enough noise The Voting results are thus dominated by bad hierarchies extracted for all but the highest thresholds The classification and HE results for this dataset are shown in Table II. ML-ARAM has better performance results on this data set in all points than ML-FAM except for RL being the best of all classifiers in terms of the multi-label performance measures. BoosTexter is the best in terms of all ranking measures For both HE algorithms the distances of BoosTexter are the best, those of ML-FAM second, followed ML-ARAM and ML-kNN. All three distance measures correlate. Interesting is also that for ML-kNN the distance values obtained by both HE methods are almost the same The hierarchy extracted by GT from the TTML has much lower distances values as compared with the hierarchies extracted by both methods from predicted multi-labels. This reflects a specific problem of HE, since only a small fraction of the incorrectly classified multi-labels can prevent building of a proper hierarchy. For example, 16.5% of misassigned labels in the extracted hierarchy are responsible for about 80% of LCAPD calculated from the predictions of MLARAM. This large part of the HE error is caused by only 4% of the test data. Under these circumstances the other distances behave analogically. Most labels were not assigned making them trivial edges, but six labels were assigned to a false branch. This can happen when labels have a strong correlation and in the step Hierarchy Construction of the basic algorithm the parent is not unique in the confidence matrix. BoosTexters results suffer less from this problem because it generally sets more labels for each test sample 


Both HE algorithms behaved similarly on the predictions of the ART networks. They constructed a deeper hierarchy than the original one and wrongly assigned the same 11 labels to the root node. The higher distances come from Voting assigning more labels to the wrong branch. For MLkNN both algorithms again create very similar hierarchy trees, both misassigned 28 labels to the root label. For BoosTexter it was seven with Voting and eight with GT Voting produced a deeper hierarchy here D. WIPO-alpha Dataset The WIPO-alpha dataset1 comprises patent documents collected by the World Intellectual Property Organization WIPO ments. Preprocessing was performed as follows: From each document, the title, abstract and claims texts were extracted stop words were removed using the list from [20] and words were stemmed using the Snowball stemmer [22]. All but the 1%-most-frequent stems were removed, the remaining stems were converted to TF-IDF weights and these were normalized to the range of [0, 1]. Again, TF-IDF conversion and normalization were done independently for the training and the test set. The original hierarchy consists, from top to bottom, of 8 sections, 120 classes, 630 subclasses and about 69,000 groups. In our experiment, only records from the sections A \(5802 training and 5169 test samples H \(5703 training and 5926 test samples 1http://www.wipo.int/classifications/ipc/en/ITsupport/Categorization dataset/wipo-alpha-readme.html August 2009 TABLE II RCV1-V2 RESULTS Measure ARAM FAM kNN BoosT A 0.748 0.731 0.651 0.695 F1 0.795 0.777 0.735 0.769 F 0.805 0.787 0.719 0.771 OE 0.077 0.089 0.104 0.063 RL 0.087 0.086 0.026 0.015 C 11.598 11.692 8.563 5.977 AP 0.868 0.860 0.839 0.873 AUPRC 0.830 0.794 0.807 0.838 H-loss 0.068 0.077 0.097 0.081 Wins 4 0 0 5 LCAPD 0.29 0.22 0.25 0.20 0.34 0.34 0.21 0.18 


CTED 0.32 0.23 0.28 0.22 0.38 0.37 0.24 0.20 TO* 0.27 0.18 0.22 0.17 0.31 0.30 0.21 0.17 document in the collection has one so-called main code and any number of secondary codes, where each code describes a group the document belongs to. Both main and secondary codes were used in the experiment, although codes pointing to groups outside of sections A and H were ignored. We also removed groups that did not contain at least 30 training and 30 test records \(and any documents that only belonged to such small groups 7,364 test records with 924 attributes each and a label set of size 131 In this case, the Voting variant of the HE algorithm applied to the TTML resulted in the LCAPD, CTED and TO* values of 0.13, 0.12 and 0, respectively. GT showed the same values. Remarkable are the TO* distances, which are equal to 0. This is due to the fact that the WIPO-alpha hierarchy contains 16 single-child labels that are not partitioned by the true multi-labels: whenever a single-child label j is contained in a multi-label, so is its child, and vice versa. It is therefore theoretically impossible to deduce from the multilabels which of them is the parent of the other. As a result the HE algorithms often choose the wrong parent, resulting in higher LCAPD and CTED values. TO*, as described above is invariant under such choices The results obtained on the WIPO-alpha dataset are shown in Table III. The classification performance of the ART-based networks on this dataset is slightly worse than that of BoosTexter. Mostly in the terms of OE, RL, C, AP, AUPRC, and H-loss measures BoosTexter is better because its rankings are better and it assigned more labels to each sample. But the ART networks have better HE results because their predicted labels are more consistent with the original hierarchy. MLkNN has the worst classification results and distance values again. The reason for the high relative difference between LCAPD as well as CTED and TO* obtained for the ART networks or BoosTexter as compared to the results of the other datasets is because most of the labels were assigned in the right branch but not exactly where they belong Both HE algorithms extracted the same hierarchy from the predictions of ML-ARAM and a very similar hierarchy for ML-FAM. About 5% labels were assigned wrongly to the 


root label in the hierarchies of the ART networks. For MLTABLE III WIPO-ALPHA\(AH Measure ARAM FAM kNN BoosT A 0.588 0.590 0.478 0.564 F1 0.694 0.691 0.614 0.693 F 0.682 0.682 0.593 0.679 OE 0.052 0.057 0.110 0.042 RL 0.135 0.136 0.056 0.025 C 25.135 25.269 22.380 11.742 AP 0.790 0.785 0.724 0.802 AUPRC 0.720 0.684 0.688 0.762 H-loss 0.090 0.093 0.149 0.079 Wins 1 2 0 6 LCAPD 0.16 0.16 0.17 0.17 0.32 0.38 0.21 0.21 CTED 0.18 0.18 0.19 0.19 0.38 0.53 0.27 0.27 TO* 0.05 0.05 0.07 0.07 0.24 0.32 0.08 0.08 kNN both HE methods wrongly assigned about the half of the labels and about 20% of total labels were assgined to the root label. Here, GT extracted a much worse hierarchy as shown by CTED being 0.15 higher for GT than for Voting For BoosTexter both HE methods built the same hierarchy and no label was wrongly assigned to the root. All extracted hierarchies were one level deeper than the original one Although Voting produced worse hierarchies than GT on two previous datasets, this time its distance values were comparable or even better. In comparison to Voting, GT has higher values for all distances on the multi-labels of MLkNN. Voting has the advantage of being a much simpler method and of being more dataset independent. Still the tree distances have the same ranking order for all classifiers for both HE methods VI. CONCLUSION In this paper we studied Hierarchical Multi-label Classification \(HMC tive was to derive hierarchical relationships between output classes from predicted multi-labels automatically. We have developed a data-mining-system based on two recently proposed multi-label extensions of the FAM and ARAM neural networks: ML-FAM and ML-ARAM as well as on a Hierarchy Extraction \(HE algorithm builds association rules from label co-occurrences 


and has two modifications. The presented approach is general enough to be used with any other multi-label classifier or HE algorithm. We have also developed a new tree distance measure for quantitative comparison of hierarchies In extensive experiments made on three text-mining realworld datasets, ML-FAM and ML-ARAM were compared against two state-of-the-art multi-label classifiers: ML-kNN and BoosTexter. The experimental results confirm that the proposed approach is suitable for extracting middle and large-scale class hierarchies from predicted multi-labels. In future work we intend to examine approaches for measuring the quality of hierarchical multi-label classifications REFERENCES 1] M. Ruiz and P. Srinivasan, Hierarchical text categorization using neural networks, Information Retrieval, vol. 5, no. 1, pp. 87118 2002 2] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni, Incremental algorithms for hierarchical classification, The Journal of Machine Learning Research, vol. 7, pp. 3154, 2006 3] , Hierarchical classification: combining Bayes with SVM, in Proceedings of the 23rd international conference on Machine learning ACM New York, NY, USA, 2006, pp. 177184 4] F. Wu, J. Zhang, and V. Honavar, Learning classifiers using hierarchically structured class taxonomies, in Proceedings of the 6th International Symposium on Abstraction, Reformulation And Approximation Springer, 2005, p. 313 5] L. Cai and T. Hofmann, Hierarchical document categorization with support vector machines, in Proceedings of the thirteenth ACM international conference on Information and knowledge management ACM New York, NY, USA, 2004, pp. 7887 6] C. Vens, J. Struyf, L. Schietgat, S. Dz?eroski, and H. Blockeel Decision trees for hierarchical multi-label classification, Machine Learning, vol. 73, no. 2, pp. 185214, 2008 7] E. P. Sapozhnikova, Art-based neural networks for multi-label classification, in IDA, ser. Lecture Notes in Computer Science, N. M Adams, C. Robardet, A. Siebes, and J.-F. Boulicaut, Eds., vol. 5772 Springer, 2009, pp. 167177 8] M. Zhang and Z. Zhou, ML-kNN: A lazy learning approach to multilabel learning, Pattern Recognition, vol. 40, no. 7, pp. 20382048 2007 9] R. Schapire and Y. Singer, BoosTexter: A boosting-based system for text categorization, Machine learning, vol. 39, no. 2, pp. 135168 


2000 10] K. Zhang, A constrained edit distance between unordered labeled trees, Algorithmica, vol. 15, no. 3, pp. 205222, 1996 11] A. Maedche and S. Staab, Measuring similarity between ontologies Lecture notes in computer science, pp. 251263, 2002 12] G. Carpenter, S. Martens, and O. Ogas, Self-organizing information fusion and hierarchical knowledge discovery: a new framework using ARTMAP neural networks, Neural Networks, vol. 18, no. 3, pp. 287 295, 2005 13] A.-H. Tan and H. Pan, Predictive neural networks for gene expression data analysis, Neural Networks, vol. 18, pp. 297306, April 2005 14] G. Carpenter, S. Grossberg, N. Markuzon, J. Reynolds, and D. Rosen Fuzzy ARTMAP: A neural network architecture for incremental supervised learning of analog multidimensional maps, IEEE Transactions on Neural Networks, vol. 3, no. 5, pp. 698713, 1992 15] Y. Freund and R. Schapire, A decision-theoretic generalization of online learning and an application to boosting, Journal of computer and system sciences, vol. 55, no. 1, pp. 119139, 1997 16] K. Zhang and T. Jiang, Some MAX SNP-hard results concerning unordered labeled trees, Information Processing Letters, vol. 49 no. 5, pp. 249254, 1994 17] G. Tsoumakas and I. Vlahavas, Random k-labelsets: An ensemble method for multilabel classification, Lecture Notes in Computer Science, vol. 4701, p. 406, 2007 18] K. Punera, S. Rajan, and J. Ghosh, Automatic construction of nary tree based taxonomies, in Proceedings of IEEE International Conference on Data Mining-Workshops. IEEE Computer Society 2006, pp. 7579 19] T. Joachims, A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization, in Proceedings of the Fourteenth International Conference on Machine Learning. Morgan Kaufmann Publishers Inc. San Francisco, CA, USA, 1997, pp. 143151 20] A. McCallum, Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering, 1996 http://www.cs.cmu.edu/ mccallum/bow 21] D. Lewis, Y. Yang, T. Rose, and F. Li, RCV1: A new benchmark collection for text categorization research, The Journal of Machine Learning Research, vol. 5, pp. 361397, 2004 22] M. Porter, Snowball: A language for stemming algorithms, 2001 http://snowball.tartarus.org/texts/introduction.html 


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


