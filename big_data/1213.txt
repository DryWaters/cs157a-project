A Parallel Genetic Algorithm for Rule Mining Nordine Melab Laboratoire d’Informatique du Littoral Universit e du Littoral C ote d’Opale 50 rue Ferdinand Buisson BP 719 62228 Calais cedex France E-mail melab@lil.univ-littoral.fr El-Ghazali Talbi Laboratoire d’Informatique Fondamentale de Lille Universit e des Sciences et Technologies de Lille1 59655 Villeneuve d’Ascq cedex France E-mail talbi@li.fr Abstract Rule mining consists of discovering valid and useful rules in large databases As other data mining tasks it is known to be time-consuming and I/O intensive Evolutionary algorithms and parallelism are two important ways 


to deal with that performance problem In this paper we propose a parallel genetic algorithm for rule discovery namely PGA  RM  We evaluated it on the Nursery School public domain data set available from the UCI Repository of Machine Learning databases The results show that PGA  RM is efÞcient and allows to discover high quality rules 1 Introduction Rule mining in large databases is one of the most studied tasks in data mining 1 3 10 I t c ons i s t s of di s c o v e ri ng rules having the form IF C THEN P where C 


and P represent respectively the condition and the prediction of the rule Basically C and P are conjunctions of terms The terms can be simply attributes For instance let us consider a commercial database where the attributes represent products The rule IF  X and Y  THEN Z means that a customer who buys the products X and Y are likely to buy the product Z  In 7  s u c h r u l es ar e r ef er r e d t o a s boolean association rules 


 Indeed the semantics of the rule remains the same if one replaces X  Y and Z with respectively the terms X 1  Y 1 and Z 1  and assigns the value 0 to all the other products Generally in practice the attributes have quantitative values e.g age or categorical values e.g matrimonial status Rules with such attributes are referred to in 7 a s quantitative association rules  In this paper we focus on the the problem of nding in a very large database 


asetof small interesting quantitative rules with categorical attributes Moreover the prediction part of each rule must have only one term The attribute of this term is called goal attribute  and it belongs to a user-specied list of attributes Such a problem is referred to in 2 a s t h e dependence modeling problem  As the number of rules that are candidates for extraction is known to be exponential  w e p ropos e a pa ra l l e l g e netic algorithm GA to perform the rule discovery Each chromosome in the population encodes a candidate rule Initially they are generated randomly and partitioned into 


sub-populations All the individuals of each subpopulation have the same goal attribute in their prediction part Consequently the genetic operators are only applied on individuals belonging to the same sub-population Three kinds of operators are used by the GA the classical crossover mutation operators and a removal operator Th last one can be performed on one individual with a probability proportional to its length Its role is to reduce the length of the rules as the objective is to discover small interesting rules The operators are similar to those proposed in b ut t h e i r combination is different 


On the other hand the tness function is derived from the information theory the entropy function The function includes the two basic known metrics the support and the condence factor A m a j or e xpe ri m e nt a l s t udy i s done on the tness function that allows its improvement A parallel version of the GA is also proposed The parallel GA is SPMD-based where the master assigns the subpopulations to the processors in a Round-Robin scheme Each worker performs the GA on its sub-populations and returns the found rules to the master The database is replicated on the processors The parallel GA has been evalu 


ated on an Ethernet network of SGI workstations using the Nursery school public domain database This paper is organized as follows Section 2 presents a formulation of the rule mining problem and the GA In Section 3 we propose a parallel version PGA-RM of the GA Section 4 describes and discusses an experimental evaluation of the PGA-RM Finally some concluding remarks and futur focus points are given 1 

















Definition 4.1 Projected database Given a transaction database 7 an itemset a and an order R 1 Itemset p is called the max-prefix projection of trans action tid It E 7 w.r.t R if and only if 1 a C It and p C It 2 a is a prefix of j3 w.r.t R and 3 there exists no proper superset y of p such that y 5 It and y also has a as a prefix w.r.t R 2 The a-projected database is the collection of max prefix projections of transactions containing a w.r.t R Remark4.2 Given a transaction database 7 a support threshold  and a convertible anti-monotone constraint C Let a be a frequent itemset satisfying C The complete set of frequent itemsets satisfying C and having a as a prefix can be mined from the a-projected database The mining process can be further improved by the fol lowing lemma Definition 4.2 \(Ascending and descending orders An order R over a set of items I is called an ascending order for function h  2  R if and only if 1\for items a and b h\(a  h\(b implies a R 6 and 2 for itemsets a U a and a U b such that both of them have a as a prefix and a R b f\(a U a 5 f\(a U b R-l is called a descending order for function h For example, it can be verified that the value ascending order is an ascending order for function aug\(S and a de scending order for function maz\(S S Lemma 4.2 Given a convertible anti-monotone constraint C E f\(S 6 v 6 E I w.bt ascending/descending order R over a set of items I where f is a prefix function Let a be a frequent itemset satisfying C and al a2    a be the set of frequent items in a-projected database listed in the order of R 1 Ifitemset a U ai 1 5 i  m violates C forj such that i  j 5 m itemset Q U  aj also violates C 2 If itemset a U aj 1 5 j  m satisfies C but a U  aj  aj+l violates C no frequent itemset having a U  aj as a properprefi satisfies C Based on the above reasoning we have the algo rithm FICA as follows for mining Frequent ltemsets with  Convertible Anti-monotone constraints Algorithm 1 FICA Given a transaction database 7 a support threshold  and a convertible anti-monotone con straint C w.r.t an order R over a set of items I the algo rithm computes the complete set of frequent itemsets satis fying the constraint C Method Call fiea 0,T function fieQ a a is the itemset as prefix and 71 is the projected database 1 2 3 4 5 Scan 71 once find frequent items in 71 Let I be the set of frequent items within 71 such that Vu E I C\(a U a  true If I  0 return, else Vu E I output a U a as a frequent itemset satisfying the constraint If C is in form off S 8 where f is a prefix function and 0 E 5  using Lemma 4.2 to optimize the mining by removing items b from I such that there exists no frequent itemset satisfying C and having a U  b as a proper prefix Scan 71 once more Vu E I generate a U a projected database 71,u For each item a in II call fiea\(a U a 71au{a Rationale The correctness and completeness of the algo rithm has been reasoned step-by-step in this section The efficiency of the algorithm is that it pushes the constraint deep into the mining process so that we do not need to gen erate the complete set of frequent itemsets in most of cases Only related frequent itemsets are identified and tested As shown in Example 6 and in the experimental results the search space is decreased dramatically when the constraint is sharp 4.3 3X Mining frequent itemsets with mono tone constraints In the last two subsections an efficient algorithm for mining frequent itemsets with convertible anti-monotone constraints is developed. Under similar spirit an algorithm for mining frequent itemsets with convertible monotone constraints can also be developed Due to lack of space instead of giving details of formal reasoning, we illustrate the ideas using an example and then present the algorithm Example 7 Let us mine frequent itemsets in transaction database 7 in Table 1 with constraint C G avg\(S 5 20 Suppose the support threshold   2 In this example, we use the value descending order R exactly as is used in Ex ample 6 Constraint C is convertible monotone w.r.t order R After one scan of transaction database 7 the set of fre quent 1-itemsets is found Among the 7 frequent 1-itemsets g d 6 c and e satisfy the constraint C According to the definition of convertible monotone constraints, frequent itemset having one of these 5 itemsets as a prefix must also satisfy the constraint That is the g d b e and e projected database can be mined without testing constraint C because adding smaller items will only decrease the value of avg But a and f-projected databases should be mined with constraint C testing However as soon as its fre quent k-itemsets for any k satisfy the constraint, constraint checking will not be needed for further mining of their pro jected databases We present the algorithm TZCM for mining frequent itemsets with convertible monotone constraint as follows 439 


Algorithm 2 FIC Given a transaction database 7 a support threshold and a convertible monotone constraint C w.r.t an order R over a set of items I the algorithm com putes the complete set of frequent itemsets satisfying the constraint C Method Call ficm 0,7,1 function ticm TI check-flag 1 Scan 71 once find frequent items in TIa If check-flag is 1 let f be the set of frequent items within 71 such that Vu E I C\(a U a  true and 1 be the set of frequent items within 71 such that Vb E I C\(a U b  false If check-flag is 0 let I be the set of frequent items within 71 and I be 0 2 Vu E I output Y U a as a frequent itemset satisfy ing the constraint 3 Scan 71 once more Vu E fliUIl generate au{a projected database 71,u 4 Foreach itema inI~~,callfic,\(aU{a},7~au~a 0 Foreach itemainfI;,call fic,\(aU{a},71 1 Rationale The correctness and completeness of the algo rithm can be shown based on the similar reasoning in Sec tion 4.2 Here, we analyze the difference between 31CM with an Apriori-like algorithm using constraint-checking as post-processing Both F1CM and Apriori-like algorithms have to gener ate the complete set of frequent itemsets no matter whether the frequent itemsets satisfy the convertible monotone con straint The frequent itemsets not satisfying the constraint cannot be pruned. That is the inherent difficulty of convert ible monotone constraint The advantage of TICM a ainst Apriorix-like algo rithms lies in the fact that FIG only tests some of fre quent itemsets against the constraint. Once a frequent item set satisfies the constraint, it guarantees all of frequent item sets having it as a prefix also satisfy the constraint. There fore, all that testing can be saved An Apriori-like algorithm has to check every frequent itemset against the constraint In the situation such that constraint testing is costly such as spatial constraints, the saving over constraint testing could be non-trivial. Exploration of spatial constraints is beyond the scope of this paper 4.4 Mining frequent itemsets with strongly convert ible constraints The main value of strong convertibility is that the con straint can be treated either as convertible anti-monotone or monotone by choosing an appropriate order The main point to note in practice is when the constraint has a high selec tivity fewer itemsets satisfy it converting it into an anti monotone constraint will yield maximum benefits by search a is the itemset as prefix 71 is the a-projected database and check-flag is the flag for constraint checking space pruning When the constraint selectivity is low \(and checking it is reasonably expensive\then converting it into a monotone constraint will save considerable effort in con straint checking The constraint awg\(S  w is a classic example 5 Experimental Results To evaluate the effectiveness and efficiency of the algo rithms, we performed an extensive experimental evaluation In this section we report the results on a synthetic trans action database with IOOK transactions and 10K items The dataset is generated by the standard procedure described in l In this dataset the average transaction size and aver age maximal potentially frequent itemset size are set to 25 and 20 respectively The dataset contains a lot of frequent itemsets with various length This dataset is chosen since it is typical in data mining performance study The algorithms are implemented in C All the exper iments are performed on a 233MHz Pentium PC with 128MB main memory running Microsoft WindowsNT To evaluate the effect of a constraint on mining frequent itemsets we make use of constraint selectivity where the selectiviy S of a constraint C on mining frequent itemsets over transaction database 7 with support threshold is de fined as  of frequent itemsets NOT satisfying C  of frequent itemsets 6 Therefore a constraint with 0 selectivity means every fre quent itemset satisfies the constraint, while a constraint with 100 selectivity is the one cannot be satisfied by any fre quent itemset The selectivity measure defined here is con sistent with those used in 7,61 To facilitate the mining using projected databases we employ a data structure called FP-tree in the implementa tions of FICA and FIC FP-tree is first proposed in SI and also be adopted by 8,9 It is a prefix tree structure to record complete and compact information for frequent item set mining A transaction database/projected database can be compressed into an FP-tree while all the consequent projected databases can be derived from it efficiently We refer readers to 5 for details about FP-tree and methods for FP-tree-based frequent itemset mining Since FP-growth 5 is the FP-tree-based algorithm mining frequent itemsets and is much faster than Apriori we include it in our experiment. Comparison among FICA 3ZCM and FP-growth makes more sense than using pure Apriori as the only reference method 5.1 Evaluation of FZCA To test the efficiency of FZCd w.r.t constraint selec tivity in mining frequent itemsets with convertible anti monotone constraints, we run a test over the dataset with 440 


160 g 140 f 120 s 100 1 Figure 3 Scalability with constraint selectivity            A     FP-gmwth h FIC\(A 20 D FIC\(A 80  04 i 00 02 04 06 08 10 Support threshold Figure 4 Scalability with support threshold support threshold   0.1 The result is shown in Fig ure 3 Various settings are used in the constraint for various selectivities As can be seen from the figure 31CA achieves an al most linear scalability with the constraint selectivity As the selectivity goes up i.e fewer itemsets satisfy the con straint 31CA cuts more search space since one frequent itemset not satisfying the constraint means all frequent itemsets having it as a prefix can be pruned We compare the runtime of both Apriori and FP-growth in the same figure All these two methods first compute the complete set of frequent itemsets and then use the constraint as a filter So their runtime is constant w.r.t constraint selectivity However only when the constraint selectivity is 0 i.e every frequent itemset satisfies the constraint does FICA need the same runtime as FP-growth In all other situations FICA always requires less time We also tested the scalability of FZC\224 with support threshold and the number of transactions respectively The corresponding results are shown in Figure 4 and Figure 5 From these figures we can see that 3ZCA is scalable in both cases Furthermore the higher the constraint selectiv ity the more scalable FZCA is That can be explained by the fact that 3ZCd always cuts more search space using constraints with higher selectivity 5.2 Evaluation of FZCM As analyzed before convertible monotone constraint can be used to save the cost of constraint checking but it cannot cut the search space of frequent itemsets In our experi ments since we use relatively simple constraints, such as those involving avg and sum the cost of constraint check ing is CPU-bound However the cost of the whole frequent itemset mining process is I/O-bound This makes the effect of pushing convertible monotone constraint into the mining process hard to be observed from runtime reduction In our experiments 31CM achieves less than 3 runtime benefit 0 200 400 600 800 1000 Number of transactions K Figure 5 Scalability with number of transactions in most cases However if we look at the number of constraint tests performed, the advantage of FICM can be evaluated objec tively FZC can save a lot of effort on constraint testing Therefore in the experiments about 31C\222 the number of constraint tests is used as the performance measure We test the scalability of 3ZCM with constraint selec tivity in mining frequent itemsets with convertible mono tone constraint The result is shown in Figure 6 The fig ure shows that FZCM has a linear scalability When the constraint selectivity is low, i.e most frequent itemsets can pass the constraint checking most of constraint tests can be saved This is because once a frequent itemset satisfies a convertible monotone constraint every subsequent frequent itemset derived from corresponding projected database has that frequent itemset as a prefix and thus satisfies the con straint, too We also tested the scalability of 31CM with support threshold The result is shown in Figure 7 The figure shows that FZCM is scalable Furthermore the lower the con straint selectivity the better the scalability FZCM is In summary our experimental results show that the method proposed in this paper is scalable for mining fre quent itemsets with convertible constraints in large transac tion databases The experimental results strongly support our theoretical analysis 6 Discussions: Mining Frequent Itemsets with Multiple Convertible Constraints We have studied the push of single convertible con straints into frequent itemset mining 223Can we push mul tiple constraints deep into the frequent pattern mining pro cess?\222 Multiple constraints in a mining query may belong to the same category e.g all are anti-monotone or to different categories Moreover different constraints may be on dif ferent properties of items e.g some could be on item price 441 


1  0 20 40 60 80 100 Selectivity Figure 6 Scalability with constraint selectivity others on sales profits the number of items etc As shown in our previous analysis unlike anti monotone, monotone and succinct constraints convertible constraints can be mined only by ordering items properly However different constraints may require different and even conflicting item ordering Our general philosophy is to conduct a cost analysis to determine how to combine mul tiple order-consistent convertible constraints and how to se lect a sharper constraint among order-conflicting ones The details will not be presented here for lack of space 7. Conclusions Constraints involving holistic functions such as median algebraic functions such as avg or even those involving dis tributive functions like sum over sets with positive and neg ative item values are difficult to incorporate in an optimiza tion process in frequent itemset mining The reason is such constraints do not exhibit nice properties like monotonicity etc. A main contribution of this paper is showing that by im posing an appropriate order on items, such tough constraints can be converted into ones that possess monotone behavior To this end we made.a detailed analysis and classification of the so-called convertible constraints We characterized them using prefix monotone functions and established their arithmetical closure properties As a byproduct we shed light on the overall picture of various classes of constraints that can be optimized in frequent set mining While con vertible constraints cannot be literally incorporated into an Apriori-style algorithm they can be readily incorporated into the FP-growth algorithm Our experiments show the effectiveness of the algorithms developed We have been working on a systematic implementation of constraint-based frequent pattern mining in a data min ing system More experiments are needed to understand how best to handle multiple constraints An open issue is given an arbitrary constraint, how can we quickly check if it is strongly convertible We are also exploring the use of constraints in clustering 160000   t 221FP-growth tFIC\(M 20 tFIC\(M 80 0.0 0.2 0.4 0.6 0.8 1.0 Support threshold Figure 7 Scalability with support threshold References I R Agrawal and R Srikant. Fast algorithms for mining asso ciation rules In Proc 1994 Int Con Very Large Data Bases VLDB\22294 pages 487-499 Santiago, Chile, Sept 1994 2 R J Bayardo R Agrawal and D Gunopulos Constraint based rule mining on large dense data sets In Proc 1999 Int Conj Data Engineering ICDE\22299 Sydney, Australia Apr 1999 3 S Brin R Motwani and C. Silverstein Beyond market bas ket Generalizing association rules to correlations In Proc 1997 ACM-SIGMOD Int Con Management of Data SIG MOD\22297 pages 265-276 Tucson Arizona May 1997 4 G Grahne L Lakshmanan and X Wang Efficient min ing of constrained correlated sets In Proc 2000 Int Con Data Engineering \(ICDE\222OO pages 5 12-521 San Diego CA Feb 2000 5 J Han J Pei and Y Yin Mining frequent patterns with out candidate generation In Proc 2000 ACM-SIGMOD Int Con Managementof Data \(SIGMOD\222OO pages 1-12 Dal las, TX May 2000 6 L V S Lakshmanan R Ng J Han and A Pang Opti mization of constrained frequent set queries with 2-variable constraints In Proc 1999 ACM-SIGMOD Int Con Man agement of Data SIGMOD\22299 pages 157-168 Philadel phia PA June 1999 7 R Ng L V S Lakshmanan J Han and A Pang Ex ploratory mining and pruning optimizations of constrained associations rules In Proc 1998 ACM-SIGMOD Int Con Management of Data SlGMOD\22298 pages 13-24 Seattle WA June 1998 8 1 Pei and J Han Can we push more constraints into fre quent pattem mining In Proc 2000 Int Con Knowl edge Discovery and Data Mining KDD\222OO pages 350 354, Boston MA Aug. 2000 9 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets In Proc 2000 ACM SIGMOD Int Workshop Data Mining and Knowledge Dis covery DMKD\222OO pages 1 1-20 Dallas TX May 2000 lo R Srikant Q Vu and R Agrawal. Mining association rules with item constraints In Proc 1997 Int Con Knowledge Discovery and Data Mining KDD\22297 pages 67-73 New port Beach CA Aug 1997 442 


expect this optimization to be of greatest bene\336t when the transaction sizes are large r example if our transaction is T 000 f A\000 B 000 C\000 D\000 E g  k 000 3 fan-out 000 2 then all the 3-subsets of T are f ABC,ABD,ABE,ACD,ACE,ADE,BCD,BCE,BDE,CDE g  Figure 2 shows the candidate hash tree C 3  We ave to increment the support of every subset of T contained in C 3  We egin with the subset AB C  and hash to node 11 and process all the itemsets In this downward path from the root we mark nodes 1 4 and 11 as visited We then process subset AD B  and mark node 10 Now consider the subset CDE  We see in this case that node 1 has already been marked and we can preempt the processing at this very stage This approach can r consume a lot of memory r a n fan-out F  for iteration k  e need additional memory of size F k to store the 337ags In the parallel implementation we have to keep a VISITED 336eld for each processor bringing the memory requirement to P\000F k  This can still get very large especially with increasing number of processors In we sho w a mechanism by which further reduces the memory requirement to only k 000F  The approach in the parallel setting yields a total requirement of k 000F 000P  5 Experimental Evaluation Database T I D Total Size T5.I2.D100K 5 2 100,000 2.6MB T10.I4.D100K 10 4 100,000 4.3MB T15.I4.D100K 15 4 100,000 6.2MB T20.I6.D100K 20 6 100,000 7.9MB T10.I6.D400K 10 6 400,000 17.1MB T10.I6.D800K 10 6 800,000 34.6MB T10.I6.D1600K 10 6 1,600,000 69.8MB Table 2 Database properties 5.1 Experimental Setup All the experiments were performed on a 12-node SGI Power Challenge shared-memory multiprocessor Each node is a MIPS processor running at 100MHz There\325s a total of 256MB of main memory The primary cache size is 16 KB 64 bytes cache line size with different instruction and data caches while the secondary cache is 1 B 128 bytes cache line size The databases are stored on an attached 2GB disk All processors run IRIX 5.3 and data is obtained from the disk via an NFS 336le server We used different synthetic databases with size ranging form 3MB to 70MB 2  and are generated using the procedure described in These databases mimic the transactions in a retailing en vironment Each transaction has a unique ID followed by a list of items bought in that transaction The 2 While results in this section are only shown for memory resident databases the concepts and optimization are equally applicable for non memory resident databases In non memory resident programs I/O becomes an important problem Solutions to the I/O problem can be applied in combination with the schemes presented in this paper These solutions are part of future research 11 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


  0 500 1000 1500 2000 2500 0 2 4 6 8 10 12 Number of Large Itemsets Iterations Large Itemset at Support = 0.5 222T5.I2.D100K\222  222T10.I4.D100K\222   222T15.I4.D100K\222   222T20.I6.D100K\222   222T10.I6.D400K\222   222T10.I6.D800K\222   222T10.I6.D1600K\222  Figure 3 Large Itemsets per Iteration data-mining provides information about the set of items generally bought together Table 2 shows the databases used and their properties The number of transactions is denoted as jD j  average transaction size as j T j  and the average maximal potentially large itemset size as j I j  The number of maximal potentially large itemsets j L j 000 2000 and the number of items N 000 1000 We refer the reader to for more detail on the database generation All the e xperiments were performed with a minimum support value of 0.5 and a leaf threshold of 2 i.e max of 2 itemsets per leaf We note that the  improvements shown in all the experiments except where indicated do not take into account initial database reading time since we speci\336cally wanted to measure the effects of the optimizations on the computation Figure 3 shows the number of iterations and the number of large itemsets found for different databases In the following sections all the results are reported for the CCPD parallelization We do not present any results for the PCCD approach since it performs very poorly and results in a speed-down on more than one processor 3  5.2 Aggregate Parallel Performance Table 3 s actual running times for the unoptimized sequential and a naive parallelization of the base algorithm Apriori for 2,4 and 8 processors without any f the techniques descibed in sections 3 and 4 In this section all the graphs showing  improvements are with respect to the data for one processor in table 3 Figure 4 presents the speedups obtained on different databases and different processors for the CCPD parallelization The results presented on CCPD use all the optimization discussed 3 Recall that in the PCCD approach every processor has to read the entire database during each iteration The resulting I/O costs on our system were too prohibitive for this method to be  12 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


