An Algorithm for Mining Frequent Closed Itemsets Zhang Tiejun Department of Computer Science &Technology Xiêan University of Science &Technology Zhangtiejun_1983@163.com  Yang Junrui Department of Computer Science &Technology Xiêan University of Science &Technology yangjr@xust.edu.cn Wang Xiuqin Wangxiuqin8000@163.com Abstract The problem of mining frequent itemsets plays an essential role in mining association rules, but it is not necessary to mine all frequent itemsets, instead it is sufficient to mine the set of  frequent closed itemsets, which is much smaller than the set of all frequ ent itemsets. In this paper we present an efficient algorithm, FCI-Miner for mining all frequent closed itemsets. It based on the improved FP-Tree \(Frequent Pattern Tree\, and used depth-first search strategy without generating conditional FP-Trees and candida- te itemsets. The experimental evaluation on a number of real and synthetic databases shows that our algorithm outperforms previous method in most cases  Key words Data mining; Association rule Frequent closed itemsets 1. Introduction 1 It has been well recognized that association rules mining is a very important problem in data mi The iss u e of m i ning frequent  itemsets plays a cruc ial role in transaction databases, sequential databases, etc. However mining frequent itemsets often generates a large number of frequent itemsets and rules  This work was supported by Natural Science Foundation of Shaanxi Province \( 2005F13 \d The Research Fund by Education Department of Shaanxi Province \( 06JK248 Subsequently, mining maximal frequent item sets [2 is studi e d popularly in data m i ning field  because the set of maximal frequent itemsets MFI\ orders of magnitude smaller than the set of frequent itemsets \(FI  However, only mining MFI has the following deficiency: from MFI and its support s we just know that all its subsets are frequent and the corresponding support of any of its subsets is not less than s but we do not know the exact value of the support. To solv e this problem, another type of a frequent itemsets, called frequent closed itemsets \(FCI\was proposed in  frequent itemsets X is closed if none of its proper supersets have the same support. Any frequent itemset has the support of its smallest closed superset. and the following relationship hol  MF I FCI FI Pasquier et al introduced the problem of mining FCI, and proposed an Apriori-like algorithm A-close in a ki and Hsiao [5  proposed another mining FCI algorithm CHARM. HAN et al in 6] extended the FP-growth method to a method called CLOSET for mining all FCI. Burdick et al, proposed an efficient algorithm de pth-first search strategy to mining all FCI  In this paper, we present a new algorithm for ___________________________________ 978-1-4244-2197-8/08/$25.00 ©2008 IEEE Proceedings of 2008 3rd International Conference on Intelligent System and Knowledge Engineering 


mining FCI. Our performance study shows that FCI-Miner is an effective method 2  Problem Description  2.1 Frequent Itemsets and Frequent Closed Itemsets Let I = {i 1 i 2 i m  be a set of m different items, and D be a database of transactions where each transaction T is a set of items such that T I The number of the transactions in D is denoted by D associated with each transaction is a unique identifier, called its Tid. We call X I an itemset. The count of X is the number of transactions in D that contain X which is labeled as count\(X and let support\(X be the percentage of transactions in D that contain X  The relationship of count\(X and support\(X is count\(X\ = support\(X\ ◊ |D Definition 1 Given a transaction database D and a minimum support threshold s or the minimum support count if support\(X s or count\(X we say that X is a frequent itemset, and we denote the set of all frequent itemsets by FI. The number of item contained in X is called its dimension or length, denoted by X The frequent 1-itemset is called frequent itemset for short Definition 2 A frequent itemset X is closed if there does not exist an itemset X such that X X and support\(X'\ = support\(X We say that X is a frequent closed itemset, and we denoted the set of all frequent closed itemsets by FCI Property 1 Let L 1 be a list of all frequent items sorted in their frequency descending order, and each of the frequent closed itemsets is made up of the item in L 1  Property 2 All subsets of a frequent itemset are frequent Property 3 All supersets of an infrequent itemset are infrequent 2.2 Data Format & Its Operation In this paper, we define bit object to represent itemsets, which is a set of binary bits, where the number of bits it contained equals to the number of frequent items in L 1 Each bitês position value in bit object correspond to one frequent item Letês take the databa se shown in table 1 for example, when  2 , we can get all frequent item which are \(f:5 d:4 b:3 e:3 a:2\nd their corresponding position value in bit object are shown in table 2, where the rightmost bitês position value is 0 which stands for the item a with the lowest minimum support and the next bitês to the left is 1 stands for the item e,  and so forth. Fro m the description above, Each transaction can be represented as one bit object where the corresponding value of each bit \(0 or 1\pends on whether the frequent item this current bit sta nds for exists in this transaction \(yes, set to 1, otherwise set to 0 Therefore, each transactionês frequent items is represented by one bit object where the length of bit object is L 1  6. \(Each transactionês corresponding bit object is shown in table 3, and ordered frequent items in each transaction are listed in the second co lumn of table 3 Table1. Business database D Table2. Position value Table3. The bit objects of D  Tid Items  100 a, c, d, f, n 200 b, c, d, e, f, i, k  300 d, e, f, g, m  400 b, f, p, s 500 c, d, f, j 600 a, b, c, e, h, o L 1 Position value f5 c4 d3 b2 e1 a0 Proceedings of 2008 3rd International Conference on Intelligent System and Knowledge Engineering 


Let T\(x\ a frequent itemês bit object which is also a set of binary bits \(0 or 1\ The length of T\(x\qual to the number of the bit objects considered. Letês show an example in the third column in table 3 where the length of each bit object is 6, its position value range from 0 to 5 from right to left, the position value 4 stands for the frequent item c, so each bit value of this position in all bit objects composed frequent item cês bit object which is T\(c\110011  Theorem 1  The number of ones in T\(X\s the support of item X, denoted by T\(X\ount Rationale If there exists only one item x in X we have this theorem according to the definition of the bit object  If X = x 1 x 2 we first perform And-Operation on these two bit objects T\(x 1 T\(x 2 en the number of ones \(denoting the times they co-occur\T\(X\support of item X If X = x 1 x 2 x n the same rationale as above Letês examine the bit objects of D shown in table 3, where T\(fc\ = T\(f\ & T\(c\ = 111110 110011 = 110010. So we have T\(fc\.count = 3 which indicates that the support of fc is 3 According to theorem 1, we observe that the bit objectês And-Operation is an effective method to generate candidate itemset and count their support 2.3 The Improved FP-Tree Frequent pattern tree, or FP-Tree for short proposed by Han et al, each node consists four fields: node-name node-count node-link and node-parent. In addition Htable is structured to facilitate the traverse of FP-Tree. However many FP-Tree-based methods still suffer from the following costly costs: They may need to generate a large number of conditional FP-Trees iteratively. Moreover, cand idate set generation and test is still expensi ve, especially when there exists quite low minimum support threshold Therefore, we improved FP-Tree from the following aspects: The node-name field and the field Item-name in Htable are replaced by items corresponding position value in order to use bit object and its operations in the process of mining There are some differences in constructing improved FP-Tree with the former FP-Tree, and the constructing algo rithm is described as follows 1\n the database D once, collect the set of frequent items and their supports, sort frequent items in support descending order as L 1 and form the corresponding position value 2\ all bit objects of each transaction in D  insert all the bits whose value is 1 into the improved FP-Tree, the remaining construction steps is the same as Han et al. The improved FP-Tree of D shown in table 1 is shown in figure 1, where each node contains its position value and its support, such as node \(5:5 indicate the position value is 5, the number after indicate the support       Figure 1. FP-Tree of D with 2  From the construction process of FP-Tree, we can observe the following property 4 Property 4 Any frequent item x, all the possible frequent itemsets contain x can be obtained by following xês Item-head Theorem 2 If there are some nodes whose node-count is no less than the predefined minimum support threshold, the itemsets formed by its prefix nodes and it must be frequent Tid Ordered FI Bit objects 100 f, c, d, a 1 1 1 0 0 1 200 f, c, d, b, e 1 1 1 1 1 0 300 f, d, e 1 0 1 0 1 0 400 f, b 1 0 0 1 0 0 500 f, c, d 1 1 1 0 0 0 600 c, b, e, a 0 1 0 1 1 1 Proceedings of 2008 3rd International Conference on Intelligent System and Knowledge Engineering 


Rationale Suppose that node N is the suffix of path P and N.node-count then any node N in Nês prefix path, we have N node-count N.node-count so N.node-count is the least in path P. Therefore, itemset formed by path P is frequent Base on the property 4, we can extract all items carrying the same Item-name via Item-head and their corresponding prefix subpaths. Each item and its prefix subpath can be transformed to one bit object where the length of the bit object is L 1  6, the current node and its prefix nodesê corresponding position value are set to 1, others are set to 0. Let the array A be used to register the number of each bit object which carry the same count with node-count. The so formed bit objects are called the current nodeês bit object group Letês examine the positi on value which is 3\(corresponding to frequent item d\ in figure 1 Its bit object group is shown in figure 2, which contain two bit objects The first is ç111000 where the leftmost two bits stand for the position value 5 and 4\(corresponding to f and c contained in the prefix subpath of item d. the  Figure 2. The bit object group of node d third bit to th e right stands for th e current node d so the three positions \(5 4 and 3\ to 1 Due to the postfix nodes neednêt to be considered, so the remaining three bits \(2 1 and 0\ are set to 0, and the A carry the same count as the node dês node count which is 3. The second is ç101000é, where the second bit to the right is set to 0 due to the position value 4 correspon- ding to c.\does not exist in its prefix subpath, and A is registered by the node-count which is 1. By doing so, we can get the item dês bit object group shown in figure 2 Theorem 3 All the frequent closed itemsets of node x can be generated from its bit object group and the array A  Rationale Based on node xês bit object group  we have just consider the nodes carrying the same Item-name and their corresponding prefix subpaths. The array A register the bit objectês number, that is, the nodeês prefix subpath has the same count as the current node. Letês show the example in figure 2, we have T\(dc\ T\(d\ T\(c 11 & 10 = 10, so T\(dc\3 + 0 = 3; T\(df T\(d\ & T\(f\= 11 & 11 = 11, so T\(df\unt = 3 1 = 4. So we have the above-mentioned theorem 3 Description of Algorithm 3.1 Problem Analysis The main process of mining frequent closed itemsets has the following two main steps  1\the constructing algorithm to represent and compress but complete frequent itemset information of th e entire database into the improved FP-Tree  2\d on the FP-Tree, we perform a divide-and-conquer framew ork to mine all the frequent closed itemsets, which means that we discover each frequent itemês \(in Htable frequent closed itemsets by constructing its bit object group When performing FCI-Miner on frequent itemês bit object group we adopt a depth-first search strategy described below 3.2 Mining Strategy   When mining each frequent itemês all frequent closed itemsets based on its bit object group the searching space can be organized as an enumeration tree 7 rooted by the current frequent item during mining. We adopt a depth-first search strategy to visit all nodes by a top-down way. Figure 3 shows the search space tree of node e, which contains all the frequent closed itemsets with the suffix of e. Based on the definition of frequent closed itemsets and property 3, the problem of mining frequent closed itemsets can be viewed as finding a cut through such search tree, such that all elements above the cut are frequent, and all below are infrequent. All frequent cl osed itemsets with the suffix of e exist in the frequent itemsets above Proceedings of 2008 3rd International Conference on Intelligent System and Knowledge Engineering 


the cut, and this cu t exists in any cases Therefore, when there are some elements infrequent, all its ch ildren nodes are also infrequent; they are all below this cut and can be trimmed out of the search space. By doing so we can reduce the search space dramatically thus improve the mining efficiency. Letês consider node eês search tree in figure 3, the node e will be counted firstly by the depth-first way, then the node eb is counted, based on the cut in figure 3 the two nodes are frequent, thus they are stored into LFCI \(storing the local FCI then count ebdês support, because T\(ebd 1 is less than the minimum support threshold which is 2  it is infrequ ent, and all its children nodes rooted by ebd need to be pruned away. In the rest of the search tree, ebc will be counted then, using the same way until all frequent closed itemsets are mined. Finally, we can get LFCI = {e: 3, ebc: 2, edf: 2   Figure 3. The search space tree of node e 3.3 Algorithm of FCI-Miner Algorithm Mining frequent closed itemsets algorithm FCI-Miner  Input The minimum support threshold and the improved FP-Tree  Output FCI 1. FCI = NULL 2. For each x Htable //x is element in Htable 3. {Construct xês bit object group 4 x_Miner Bitsets Begin LFCI CurItem mine all frequent close d itemsets with suffix x 5. GetFCI \(LFCI, FCI  6. Return FCI Procedure x_Miner \(Bitsets, Begin, LFCI CurItem\ //Begin is the cu rrent nodeês position in bit object, End is the position having the top position value, CurItem is the current items position value 1.   Tempfci = CurItem; // Tempfci store the position value of the current item 2   for \( i = Begin; i End; ++ i   3.      {if \(\(T\(i\T\(i+1\ount  4.          {GetFCI \(Tempfci, LFCI 5.           x_Miner \(Bitsets, i+1, LFCI Tempfci +item \(i+1\ês position value Procedure GetFCI \(TFCI, FCI 1. if \(each M TFCI\ isnêt the subset of some elements in FCI with the same support 2.      {FCI = FCI M 3.        Delete elements which are the subset of M in FCI with the same support 3.4 Experimental Evaluation  We conduct all experiments on a notebook of TM\Duo 1.6 GHz CPU and 1024MB of ma in memory, running Microsoft windows XP. All the codes are written in Microsoft Visual C ++ 6.0. We compare FCI-Miner with Mafia \(Mafia can download from the website http://fimi.cs.helsinki.fi e chose several real and synthetic databases for testing the performance of FCI-Miner. The real databases are all taken from the website of UCI Machine Learning Database Repository  http://www.ics.uci.edu/~ml earn/MLRepository html real databases are very dense. We also chose a few synthetic databases, which can be generated by the generator from IBM Almaden Centre website http://www.almaden.ib m.com/cs/quest/demos.ht ml Database T25I20D10K has 50 items, T25 shows that the average transaction length is 25 I20 shows that the average maximal potentially frequent itemset size is 20; D10K shows that there are 10K transactions. These databases are sparse Table4. Database characteristics Table 4 above shows the characteristics of all databases used in our evaluation. It shows the number of items, the average transaction length Proceedings of 2008 3rd International Conference on Intelligent System and Knowledge Engineering 


and the number of transaction in each database The following figures show the testing results on these databases   0 0.2 0.4 0.6 0.8 1 1.2 0.1 0.5 1 5 10 25 40 60 65 90 Min Support Total time \(s Mafia FCI-Mine r Figure 4. FCI-Miner versus Mafia on Mushroom 0 0.3 0.6 0.9 1.2 75 80 85 90 95 Min Support Total time \(s Mafia FCI-Miner Figure 5. FCI-Miner versus Mafia on Chess  0 5 10 15 20 75 80 85 90 95 Min Support T otal tim e \(s Mafia FCI-Mine r Figure 6. FCI-Miner versus Mafia on Connect-4 0 0.2 0.4 0.6 0.8 1 1.2 1.4 40 50 60 70 80 90 Min Support Total time \(s Mafia FCI-Miner Figure 7. FCI-Miner versus Mafia on T25I20D10K 0 5 10 15 20 70 75 80 85 90 95 Min Support T otal tim e \(s Mafia FCI-Miner Figure 8. FCI-Miner versus Mafia on T50I20D100K  As shown in figure 4 to figure 8 above, we tested the total execution time of Mafia and FCI-Miner on five databases described in table 4 In figure 4, FCI-Miner outperforms Mafia range from 0.001 to 1. When te sted on dense databases such as Chess and Connect-4, FCI-Miner performs well when minimum support threshold is greater than 0 75. On T25I20D10K and T50I20D100K, FCI-Min er also outperforms Mafia in most cases 4 Conclusions In this paper, we presented and evaluated an efficient algorithm for mining frequent closed itemsets in transaction database. FCI-Miner adopts a novel data structure for compressing crucial information abou t frequent itemsets which can reduce the database greatly, and also uses an effective method to generate candidate itemsets and count their support without generating conditional FP-Trees. The experimental results show that it is an effective and efficient algorithm  5. References  Agrawal R. Srikant Fast algorithm for mining  In: Proc of the 20th In têl Conf. on VLDBê94. Santiago: Morgan Kaufmann 1994: 487-499  R. Bay ardo Ef ficiently mining long patterns from datab Proc. of 1998  AC M SIGMOD Intêl Conf. on Management of Data. New York: ACM Press, 1998: 85 93  N. Pasquier  Y  Bastide, R   T a ouil and L. Lakhal  Discovering frequent closed itemsets for association rules. Proceeding of the 7th Intêl Conference on Database Theory. 1999: 398-416  D. Burd ick M. Calimlim J. Gehrk e. Maf ia: A  maximal frequent i temset algorithm for transactional da Proceeding of the 17th  Intêl Conference on Data Engineering. Heidelberg IEEE Press, 2001: 443-452   Hsiao CJ. CH ARM: An ef ficient algorithm for closed itemset mining. Proceeding of the 2nd SIAM Intêl Conference on Data Mining Arlington: SIAM, 2002: 12-28  R. CLOSET: An efficient algorithm for mining frequent closed itemsets Proceeding of the 2000 ACM SIGMOD Intêl Workshop on Data Mining and Knowledge Discovery. Dallas: ACM Press, 2000: 21-30  R y mon R. Search thro ugh s y stematic set enumeration. Proceeding of the 3rd Intêl Conference on Principles of Knowledge Representation and Reasoning. 1992: 539-550  Proceedings of 2008 3rd International Conference on Intelligent System and Knowledge Engineering 


                                                                                                                 
456 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efﬁcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efﬁciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efﬁciency When the clustering model is available it is a signiﬁcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques Sufﬁcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efﬁcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efﬁcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conﬁdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conﬁdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conﬁdence The sufﬁcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efﬁcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efﬁciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufﬁciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70–81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207–216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487–499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145–154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146–153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9–15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597–600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern Classiﬁcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155–162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512–521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1–12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265–278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283–304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476–482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559–563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220–231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13–24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10–17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549–550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188–201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259–283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909–921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305–345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432–444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conﬁdence Intelligent Data Analysis  9\(4\:381–395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49–73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407–419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1–12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki Efﬁciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642–658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483–490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194–203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344–353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efﬁcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103–114 1996 
618 
618 


