Linguistic Modeling for Function Approximation Using Grid Partitions Hisao Ishibuchi Takashi Yamamoto and Tomoharu Nakashima Department of Industrial Engineering Osaka Prefecture University 1-1 Gakuen-cho, Sakai Osaka 599-8531 JAPAN hisaoi, yam nakashi}@ie.osakafu-u.ac.jp Absbrrcr modeling of nonllnear fundons with many Laput variables Our task la to extract a small number of comprehensible lingoisae roles from numerical data for describing nonlinear hndons in a human understandable manner First we bow the necessity of genera NI La the handUng of aonllnear functions with many input 
vnriables Next we compare B standard interpolation-bared huzy reasoning method with our uon-stmdnrd sped8elty-bared method When a rule base is a mlxinre of general and specific rules different results are obtained ftom these two methods Then we atend two perforounce measures Le confidence and support of sasociatiou rules in data mlnlng to the care of Ungulatle rules. These two measures are wed for evalualing each linguirtie rule The vllidlty of our foly reM0dg method is dlsewed wing these measures Finally we show two genetlc algorithm-bared approaches to linguistic modeling Oae is 
a role seleetlon method and the other is B genetics-based machine learning GBML algorithm This paper discnues various issues related to unguistie I INTRODUCTION For linguistically describing an unknown nonlinear function y  f x with n input variables we use linguistic rules of the following form At  If x1 is AH and  and x is A then y is Et 1 where xi is the i-th input variable of an n-dimensional input vector x  xl _._ xn   y is an output variable k 
is a rule index Ak is an antecedent linguistic value for xi and Bk is a consequent linguistic value In this paper, we do not discuss the specification of antecedent and consequent linguistic values We assume that a set of linguistic values has already been given for each input and output\variable. Examples of typical linguistic values sre shown io Fig 1 M l.0  0.0 0.0 1.0 Fig 1 Triangular membership functions of linguistic values S small MS medium small M medium ML medium lorge and L large Linguistic rules for a two-input nonlinear function 
are usually written in a tabular form as in many applications of fuzzy rule-based systems to control problems While such a tahular form is easily understood by human users it cannot scale up to highdimensional problems because the numher of rules in a tahular form expnentislly increases with the dimensionality of the input space It is a troublesome task for human users to manually examine a large number of linguistic rules Many antecedent conditions also deteriorate the interpretability of linguistic rules i.e it is difficult for human users to intuitively understand long rules with many antecedent conditions\Recently several approaches have been proposed for genersting interpretable fuzzy rule-based systems from numerical data 
1-6 The main issue in those studies is to find a good tradeoff i.e compromise between accuracy and interpretability. Many studies use clustering techniques for finding a small number of fuzzy if-then rules with multi dimensional antecedent fuzzy sets When the linguistic interpretstion of each fuzzy if-then rule is required single dimensional antecedent fuzzy sets are generated hm multi dimensional ones by the projection to each axis of the input space When similar singledimensional antecedent fuzzy sets are generated they are merged for improving the interpretability of fuzzy if-then rules In such a clustering based approach antecedent fuzzy 
sets are generated from numerical data without human howledge On the contrary, we assume that a set of linguistic values has already been given for each input and output variable hy human users This assumption means that a fuzzy partition i.e information granularity of the input and output space has already been given by human users In Such a situation with human howledge it may he a natural my to use the given linguistic values in antecedent and consequent parts of our linguistic rules Our fuzzy reasoning and rule extraction methods in this paper are also applicable to more general cases where fuzzy partitions are to be determined from numerical data without human 
knowledge 11 NECESSlTY OFGENERALRULES We use linguistic rules of the form in I for describing an unknown nonlinear function y  f x with n input variables For simplicity of explanation let us assume that the input space and-the output space am an n-dimensional unit cube 0,1 and a unit interval 0,1 respectively When we use the five linguistic values in Fig 1 in the antecedent part each linguistic value covers the following fraction of the domain mWal 0,1 of each input variable 0-7803-7293-X/Ol/$I7.00 B 2001 IEEE 47 2001 IEEE International Fuzy Systems Conference 


small 114 medium small 112 medium 112 medium large 112 large 114 Thus we can see that each linguistic value covers 215 of the domain interval on the average where Since each rule has n antecedent linguistic values it covers 215 of the ndimensional input space O,l on the average when no don't care conditions are included The number of linguistic rules required for covering the whole input space is roughly estimated as 512 The above discussion clearly shows that the number of linguistic rules exponentially increases with the number of input variables Our trick for avoiding the exponential increase is to use don't care as an antecedent linguistic value in linguistic rules in 1 We represent don't care by a special memhersbip function whose membership value is always unity in the whole domain interval of each input variable i.e pdon~tcarr\(x for Vx  The number of antecedent conditions excluding don't care conditions is referred to as the rule length Short rules have only a few antecedent conditions while long rules have many conditions Short and long rules are refemd to as general and specific rules in this paper, respectively When we use general rules of the length m for the n-input nonlinear function m  n  each rule covers 215 of the whole input space  0 I on the average Thus the number of linguistic rules required for covering the whole input space is roughly estimated as 5/2 which does not depend on the dimensionality n of the input space ut depends on the length m of each genera rule When m n  5/2 is much smaller than 512 This means that the whole input space can he covered by a small number of general rules Of course general rules do not always represent nonlinear functions very well Thus some specific rules together with general rules may he required for linguistically describing nonlinear functions General rules are also preferred to specific rules fmm the viewpoint of the interpretability of linguistic rules It is much easier for human users to intuitively understand general rules with only a few antecedent conditions than specific rules with many conditions 2/5=\(2x114+3x1/2 2 111 SPECIFICITY-EASED FUZZY REASONING Let us consider a rule base consisting of the following three linguistic rules RA  y is smal R  If XI is small theny is medium 3 4 RC  If XI is small and x2 is small then y is large 5 where small medium and large are linguistic values shown in Fig 1 Our fuzzy reasoning task in this example is to estimate the output value y for any input vector x  x I  x2  using the three rules RA RB and R  For an input vector with small x and smoll x2 e.g x=\(O 0 all the three rules are applicable Since many fuzzy reasoning methods are based on interpolation techniques of applicable rules the estimated output value may be a real number around 0.5 that corresponds to the interpolation of the consequent linguistic values of the three rules In Fig 2 a we show the nonlinear function obtained hm the above three rules using the following simplified version of Takagi-Sugeno method where qk x is the compatihility grade of the linguistic rule Rk with the input vector x and bk is a representative real number of the consequent linguistic value Bk  The compatihility grade pk x is usually calculated by the product operation as where phi is the membership function of the antecedent linguistic value Ah  The estimated output value for an input vector with small x and small x2 e.g x 0 0 by our specificity-based fuzzy reasoning method 7,8 is a real number around 1.0 because the most specific rule i.e RC is mainly used for the estimation Our fuzzy reasoning method is witteu as Pk\(X xl Xn 7 N x6\(Rk x Pk X N x6\(Rk X X k=l j\(x  k=l 8 where m\(Rk x is a weight determined by the relative specificity level of the linguistic rule Rk  The value of 6\(Rk x becomes small when Rk includes more specific rules compatible with the input vector x In this case the weight of Rk is discounted in our fuzzy reasoning method The weight 4 Rk x is defined as 6\(Rk x n l-Pq X 9 Rq CRi q*k When no linguistic rule is included in Rk  Rk x is specified as  Rk x hecause the weight of Rk should not he discounted in this case In Fig 2  we show the nonlinear function obtained 60m the three linguistic rules in 3 5 using our specificity-based fuzzy reasoning method a standard reasoniog  Our method Fig 2 Comparison oftiny reasonkg results 48 


IV FUZZlFlCATlON OF ASSOCIATION RULES The concept of association rules 9 WBS used in many studies on data mining Two measures i.e support and confidence were used for evaluating each association rule X Y using a transaction set D The transaction set corresponds to the given training data because a bxnsaction corresponds to an input-output pair The association rule X 3 Y is said to hold in the transaction set D with a confidence c if c xl00  of transactions in D that contain X also contain Y The association rule X s Y is said to have a supports in D ifs xl00  of transactions in D contain X and Y For details of association rules see 9 These two measures can be defined for our linguistic rule Rk in I using the given training data xp y  p 1,2  m where xp xPl  xpn The confidence of the linguistic rule Rk can be defined as follows IO m c\(Rk  b'k Xp  Yp z b'k xp  lo p=l LI where p  xp  is the compatibility grade of the input vector xp witb the antecedent part of Rk  and pk\(yp is the compatibility grade of the output value y witb the consequent part of Rk  The denominator of IO corresponds to the number of input-output pairs that are compatible with the antecedent part of Rk  The numerator corresponds to the number of input-output pairs that are compatible with both the antecedent and consequent parts of Rk  The support of Rk can be defined as follows IO k 1 1 m s\(Rk  z pk Xp  Yp p=1 When both the antecedent and consequent parts of the linguistic rule Rk are specified by non-fuzzy concepts these two definitions in IO and 11 are exactly the same as those used for association rules in data mining Let us illustrate these two measures using the nonlinear function in Fig 2 a in the previous section From this nonlinear function we first generated 441 input-output pairs xpi xp2 y p  1 2 _ 441 where xp  0.00 0.05  1 OO for i  I,2  That is these training data were generated using a 21x21 grid of the two-dimensional input space O,l]x[O I Then we calculated the confidence c and the support s of each of the possible 62 x 5 linguistic rules using the training data obtained from Fig 2 a\For example If xl is small then y is medium smull c 0.75 s 0.1 I 12 If XI is small and x2 is small then y is medium c 0.54,s 0.01 13 We can see that the confidence of these intuitively acceptable rules is high The nonlinear function in Fig 2 a was actually obtained fmm the three linguistic rules in Section 3 using the standard interpolation-based fuzzy reasoning. The confidence c and the support s of those rules are calculated as y is mal1 c 0.82 s 0.82 14 If x1 is smll then y is medium c 0.1 I s 0.02 IS If XI is smull and xz is small then y is large c 0.00 s O.OO 16 The confidence of the last two rules is very small This suggests that the standard interpolation-based fuzzy reasoning may lead to counterintuitive results when a rule base consists of both general and specific rules When we use our specificity-based fuzzy reasoning method the nonlinear function in Fig 2 b is obtained fmm the above three linguistic rules These three rules have high confidence for the nonlinear function in Fig 2 b This suggests that OUT fuzzy reasoning leads to intuitively acceptable results V LINGUISTIC RULE SELECTION The main issue in linguistic modeling is to exhnct a small number of comprehensible linguistic rules from numerical data For classification problems we have already formulated linguistic rule extraction as a three-objective combinatorial optimization problem ll The objectives were to maximize the classification accuracy to minimize the number of linguistic rules and to minimize the rule length We can formulate a similar three-objective problem for OUT function approximation problem As in tbe previous section, it is assumed that m input-output pairs x yp p=l 2 m are given for linguistic rule exhnction Let Ki and Kg be the number of linguistic values given for the i-tb input variable i  l,2  n and the output variable respectively Linguistic rules are generated by combining n antecedent and single consequent linguistic values as shown in I in Section I Since don't cure is used in addition to the K linguistic values for each input variable xi the total number of rules is N  KI  I  x\(K  1 KO Thus the total numher of rule sets is ZN Our task is to find a compact rule set S from these rules for linguistically describing the unknown nonlinear function y  f\(x  The performance of the rule set S is measured by its approximation accuracy the numher of rules and the total rule length. Thus a three-objective problem is formulated as where fl S is the total squared emr by S f2 S is the number of linguistic des in S and f3 S is the total rule length of linguistic rules in S More specifically fi\(S is written as Minimize f1 S fz S f3 S 17 m fl SI cmx  t2 12 18 where i  x   is the estimated output by the rule set S for the input vector xp  When the estimated output j xp  cannot be calculated i.e when there are no compatible linguistic rules With the input vector xp  a pre-specified penalty value is used as the difference between the estimated output j xp  and the target output yp  In our computer simulations the penalty value was specified as lj\(xp I when j\(x  could not be calculated. This penalty value is equal to p4 49 


the width oftheoutput space 0,1 When a rule selection method is applied to our three objective problem in 17 candidate linguistic rules have to be generated before rule selection Let Scan be a set of candidate rules 6om which a small number of linguistic rules are to be selected When the number of input variables is not large candidate rules can be generated hm all combinations of antecedent and consequent linguistic values On the contrary all combinations cannot be examined in the case of nonlinear functions with many input variables In this case only a tractable number of candidate rules have to he generated using some tricks For example candidate rules can be specified as only general rules whose length is less than or equal to a pre specified threshold value. The confidence and the support of linguistic rules can be used for prescreening candidate rules together with the constraint condition on the rule length When a set of candidate rules i.e Scandidate is generated any subset S of Scan can be represented by a binary string of the length Ncan&date as where sk I means that the k-th candidate rule Rk is included in the rule set S and s  0 means that R is not included in S  k  1,2  Nc,,*date  Since each rule set is represented by a binary string any multi-objective genetic algorithms 11,12 can be used for finding non-dominate rule sets with respect to the three objectives in 1 7 19 VI GENETICS-BASED MACHWE LEARNING me performance of the rule selection method strongly depends on the choice of candidate rules. The prescreening of candidate rules can reduce the size of the search space On the contrary the search space of our genetics-based machine leaning GBML algorithm consists of all combinations of antecedent and consequent linguistic values In our GBML algorithm each linguistic rule Rk in I is coded by its n antecedent and single consequent linguistic values as Rk  AklAkZ   AknBk A rule set S is denoted by a concatenated string where each substring corresponds to a single linguistic rule Initial rules are generated by randomly specifying their antecedent and consequent linguistic values Our GBML algorithm is implemented in the framework of multi-objective genetic algorithms 11,12 The number of linguistic rules is adjusted by a crossover operation In the current version of our GBML algorithm we use a kind of one point crossover in Fig 3 for adjusting the numher of linguistic rules and mixing up the order of linguistic rules in each string Antecedent and consequent linguistic values are replaced with another ones by a mutation operation VII CONCLUDING REMARKS We discussed linguistic modeling of nonlinear functions using linguistic rules Due to the page limitation we only showed the outline of OUT approach. Simulation results will be reported in the presentation at the conference Parent 1 Parent 2 Fig 3 A kind of one-point crossover with different cutoff points REFERENCES I H Isbibuchi T Murata and I B Turksen 224Single-objective and two-objective genetic algorithms for selecting linguistic des for pattern classification problems,\224 Fuay Seu and Systems vol 89 pp 135.149 July 1997 2 I V de Oliveira 223Semantic constraints for membership function aptitnhtion.\224 IEEE Tmm on System Man and Cybernnics  Pari A Systems and Hmm vol 29 no I pp 128-138 January 1999 3 I Yen and L Wang 223Silifying fuzzy de-based models using orthogonal bunsformation methods.\224 IEEE Tram on System Man and Cybemeth  Pan B Cybernetics vol 29 no 1 pp 13-24 February 1999 4 Y Jin W von Seelen and B Sendho 223On generating FC3 timy rule systems from data using evolution s!xategies,\224 IEEE Tmnrnctiom on Systems Man and Cybemtiics  Part B Cybmetics vol 29 no 6 pp 829-845 December 1999 5 Y Jin 223Fuzzy modeling of high-dimensional systems Complexity reduction and interpretability improvement,\224 IEEE Tram on Fwzy Systems vol 8 no 2 pp 212-221 April 2000 6 M Setnes and H Roubos 223GA-fuzzy modeling and classification Complexity and performance,\224 IEEE Tmm on Fwzy Systems vol 8 no 5 pp 509-522 October 2000 7 H Ishibuchi 223A tbzy reasoning method for handling limy rules witb diht specificity levels,\224 Proc of 18th Intemfional Confwence of the North American Fuay Information Processing Society\(NewYork, USA 110-114 June 1999 8 H Isbibwhi 223Fuzzy reasoning method in hray rule-based systems with general and specific rules for function approximation.\224 hc of Fm-LEEE Smul Korea pp.198-203 August 1999 9 R Agmwal and R Srikant 223Fast algorithms for mining association des,\224 Proe of 20th Intemationol Conference on Vq LnrgeDotn Bases Santiago chile pp 487499 September 1994 Expanded version is available as IBM Research Report RT9839 June 1994 10]H Ishibuchi T Yamamoto and T Nakashi 222\221Fuzzy data mining Effect of tkzy discretization,\224 Proc of 2001 IEEE ltItenUrtiOM/ Conference on Data Mining San Jose November 29  December2 to appear Ill H Ishibuchi T Nakashima and T Murata 2237hree-objective genetics-based machine learning for linguistic de exeaction.\224 Informzition Sciences vol 136 no 1.4 pp 109-l33,20OL I21 E Zikler and L de 223Multiobjective evolutionary algorithms A comparative case study and the strength Pareto Approach,\224 IEEE Tram on Evolutionary Computotion vol 3 pp 257-271 1999 50 


step 6 terminates and outputs all m-patterns found Otherwise Step 7 scans the data to count the occurrences of each candidate in Ck+l Steps 4 through 8 are repeated until no more qualified candidates are found Implementing this algorithm requires keeping a counter for each candidate pattern in Ck As each market basket in D is examined the counter is increased by one if the candidate is a subset of the transaction We refer to 2 for further implemen tation details This algorithm needs k-1 data scans The complexity of this algorithm is linearly dependent on the length of data but is ex ponentially dependent on the size of the longest m-pattern In practice the algorithm converges quickly, especially when pat terns are not very long C Pruning Candidate M-Patterns The most time-consuming step in the above algorithm is counting the occurrences of candidates Clearly the smaller Ck the faster the algorithm Therefore we should prune candidates as much as possible before scanning the data and counting Be low we show how this can be done by making use of a necessary condition for the presence of an m-pattern Property 3 Upper bound property Let E\222 be a non-empty subset of E Let an item a E E\222 Then 2 PD\(E    5 PD\(E  E\222 E\222 Proof Recall that PD\(E  a}l{a  PD\(E  Equation3\Since PD\(E 5 PD\(E-{u the first conclusion holds. Further since a E E\222 PD\(E  U 5 PD\(E  E\222 and PD\({u 2 PD\(E\222 the second inequality holds as well 0 The above property provides an upper-bound for the empirical conditional probability PD\(E  a}l{a Moreover it also proves that PD\(E  a a is the tightest upper bound among possible upper bounds Using Property.3 we easily obtain a necessary condition for qualifying an m-pattern Property 4 Neeessary condition If E is an m-pattern with minp then 1 PD\(E  a}l{a 5 pD\(E  a a supportu\(E  a a 2 minp for any item a 8 Proof This follows by combining Equation 5 4 and Property 3 0 We note that the above necessary condition only depends on the support of patterns found at level 1 and level k  1 this is different from Property 1 which is dependent on support E computed by an additional data scan Thus this condition can be used to prune candidates in Step 5 thereby reducing the num ber of candidates for which counting is done in Step 7 We sum marize the pruning algorithm as follows Input a set of candidate patterns Ck Output a set of pruned candidate patterns 1 For each pattern E in Ck 2 For each item a in E 3 Algorithm Pruning If supportD\(E  a a  minp 4 5 Goto\(,v Ck  Ck  E 6 Endfor 7 End for 8 Return CI D Algorithm for Mining M-Patterns Putting the above described algorithms together we obtain Algorithm DiscoverMPatternsWithPruning Input minp and data D Output all qualified m-patterns Lk 1 Li a}l~E I};C2={{a,b}la,bEI 2 Scan D to count the occurrences of each pattern in L 1 and c2 3 k  2 4 Compute the qualified candidate set Lk  v E CklisMPattern\(v  true 5a Construct the new candidate set Ck+l based on Lk by the downward closure property 56 Prune Ck+l based on Property 4 using the Pruning algo rithm 6 if Ck+1 is empty output Lk and terminate 7 Scan D and count the occurrence of each pattern v E Ck+l 8 k  k  1 go back to 4 We note that the level-wise algorithm for m-patterns and that for the frequent association have two common steps construct ing the next level candidate patterns based on the previously qualified patterns \(Step 5a and counting occurrences of candi dates \(Step 7 However these algorithms differ in several ways In particular our algorithm for m-pattern discovery a requires a different treatment for the first and the second levels Steps 1 to 3 b takes advantage of an extra pruning step \(Step 5b and c\employs a different algorithm to qualify a candidate pattern Step 4 E Extensions Here we consider a couple of extensions to our algorithm for discovering m-patterns First, note that the definitions and results thus far presented make no assumption about how transactions in D are obtained Thus if items have a timestamp e.g temporal event data See 15 141 for the detailed definition transactions can be con structed using windowing schemes as in[15 Doing so allows us to discover temporal m-patterns I Second we show how further performance gain can be ob tained by partitioning items We note that Property 3 can be used to partition the search space To illustrate this consider E  a b By the property 3 E may be an m-pattern if both Pu\({a b 2 minp and Pu\({b a L minp By Equation 2 we obtain supportu b 5 support a 5 supportD b 9 Extending this to other items we can partition items so that the above equation will not hold for items in two different partitions In this way a potential m-pattern can only be a subset of items in one and only one partition Consequently, the original problem is divided into several sub-problems each of which relates to 221Some cares are needed to deal with the overlapped windows We refer to  151 for more implementation details 413 


one partition of items This reduces the search space as we only need to consider candidates within a partition Further, we can solve these sub-problems in parallel IV FREQUENT M-PATTERNS This section shows that the concepts of frequent itemsets and m-patterns can be combined to develop an algorithm that dis covers frequent m-patterns A frequent m-pattern is defined in terms of both minp and minsup That is, a frequent m-pattern is significantly mutually dependent with the dependence thresh old minp and has support threshold minsup How do frequent m-patterns compare with frequent associa tion rules A frequent association rule with the form E1  Ez requires two conditions I suppmt~\(E1  Ez 2 minsup and 2 PD\(EzlE1 2 mincon f We note that the second con dition does not have the closure property and thus it is com putationally intractable to consider 2 alone In contrast, m patterns can be discovered efficiently since downward closure holds However, mutual dependency, which is required by m patterns is a stronger condition than association as required by association rules We now formalize the notion of frequent m-patterns DeJinition4 E is said to be a frequent m-pattern with minsup and minp iff E is an m-pattern with minp and the number of occurrences of E is no less than minsup We note that the frequent m-pattern is a more general pattern than the frequent itemset and the m-pattern In that when minsup  0 the frequent m-pattern reduces to the m-pattern When minp  0 the frequent m-pattern becomes the frequent itemsets We can mine all frequent m-patterns efficiently The key insight is that frequent m-patterns are downward closed We demonstrate this by showing a much stronger result 1171 dis cusses this in a slightly different form Property 5 Conjunction and disjunction of downward closure properties Let boolean functions fl and f2  be two qualification functions of an item set such that fl and fa are both downward closed Then, the qualification func tion f,\(E A f2\(E is downward closed where 223A\224 represents the 223and\224 operation And fl\(E V fz\(E is also downward closed V is the 223or\224 operation Proof Let E be a nonempty itemset E\221 C E Assume that fl\(E A fz\(E holds Since fl\(E and fz\(E both hold we know that both fl\(E\222 and fz\(E\222 are true since fl and fa are downward closed Therefore we obtain fl E\222 A fa E\222 is true Similarly assume that fl E V f E holds. Then, at least one of fl\(E and fi\(E is true and so fl\(E\222 V fz\(E\222 is true 0 The foregoing allows us to construct a level-wise algorithms for frequent m-patterns by modifying Step 7 of the m-pattern mining algorithms For a frequent m-pattern the qualified pat terns at level k is Lk  U E CklisMPattern\(v  true and supportu\(v  minsup The remaining steps of the al gorithm are unchanged v EXPERIMENTAL RESULTS This section assesses our algorithms for discovering m patterns. Two kinds of assessments are presented The first eval uates the performance of our algorithm using synthetic trans 221 I I a 100 35 2 2 a0 350 1 1  ss0 rnfNmb..d*.\223.LDn.,\224 223..\223d Fig 2 Average nm time in second vs the number of transactions in 1,ooO action data The second studies our algorithm using real data collected from a production computer network A Synthetic data We begin by using synthetic data to study the scalability and efficiency of our algorithm for discovering m-patterns The syn thetic data are constructed by first generating items randomly and uniformly, and then adding instances of patterns into ran domly selected transactions Thus the synthetic data are spec ified by the following parameters the number of transactions the number of distinct items the average number of random items per transaction the number of patterns and their length and the noise to single ratio NSR\Here, the NSR for an item in a pattern is defined by the ratio between the number of ran dom instances to the number of the item instances in the pattern Throughout, the number of distinct items is 1000 the number of patterns is IO with length 5 the average number of random items in a transaction is 20 and the NSR is 5 We assess scalability by varying the number of transactions We compare the level-wise algorithm for mining frequent pat tern itemsets with our DiscoverMPatternsWithPruning algo rithm for mining m-patterns The values we choose for minsup for frequent patterns and minp for m-pattern are set2 so that there is no false positive above level 2 An experiment con sists of 5 runs done with different random seeds. Figure 2 plots the average CPU time against the total number of transactions in ten thousands The results for frequent itemsets are des ignated by the 222*\222 markers and those for m-patterns by the 222+\222 marker We see that the two curves are almost indistinguishable although the curve for frequent patterns is just below that for m-patterns It is somewhat surprising that m-pattern discovery is so efficient since qualifying an m-pattern requires k compar isons where as frequent itemset only requires one comparison This suggests that a linear algorithm for qualifying m-patterns is sufficiently fast Indeed we see that both algorithms scale linearly as the number of transactions increases Now we study the effect of minp and the benefits provided by the Pruning algorithm Here the number of transaction is fixed at 50,000 The results are plotted in Figure 3 The x-axis is minp and the y-axis is the CPU seconds required to discover m-patterns The line with the 222+\222 markers are the results for DiscoverMPatterns and the line with the 222*\222 markers is for DiscoverMPatternsWithPruning Note that for larger values 20ur results are not sensitive to the specific values used for rninsup and minp 414 


 t  Fig 3 Average run time vs minp of minp e.g minp  7 there is little impact on CPU con sumption This is because minp is sufficiently large compared to the fraction of \223noise\224 transactions However, when minp is small pruning provides significant benefits In fact, when minp is 6.5 a typical run generates about 3730 candidates at the third level With pruning the number of candidates reduces to 2550 B Production Data This section applies our algorithms for discovering m patterns in data from a production computer network. Here our evaluation criteria are more subjective than the last section in that we must rely on the operations staff to detect whether we have false positives or false negatives Two temporal data sets are considered The first was collected from an insurance company that has events from over two thou sand network elements e.g routers hubs and servers The second was obtained from an outsourcing center that supports multiple application servers across a large geographical region Events in the second data set are mostly server-oriented \(e.g the CPU utilization of a server is above a threshold and those in the first relate largely to network events e.g 223link down\224 events Each data set consists of a series of records describing events received by a network console An event has three attributes of interest here host name which is the source of the event alarm type which specifies what happened e.g a connection was lost port up and the time when the event message was received at the network console We preprocess these data to convert events into items, where an item is a distinct pair of host and alarm type. The first data set contains approximately 70,000 events for which there are over 2,000 distinct items during a two week period The second data set contains over 100,000 events for which there are over 3,000 distinct items across three weeks We apply our algorithm for m-pattern discovery to both data sets, and compare the results to those for mining frequent item sets We fix minsup to be 3 so as to eliminate a pattern with only one or two instances, and we vary minp Our results are reported in Figures 4 and 5 for data sets 1 and 2 respectively These figures plot the total number of m-patterns the solid line and the number of border m-patterns the dashed line against minp Here a border pattern refers to a pattern that is not a sub set of any other pattern The x-axis is minp and the y-axis is the number of m-patterns discovered on a log scale Clearly minp provides a very effective way to select the strongest patterns in that the number of m-patters discovered drops dramatically as 14 0.1 0 0 0 0 0 J Fig 4 M-patterns of the first data set 223-\224 the number of m-patterns in the log scale 223..\224 the number of border m-patterns in the log scale x-axis is minp Fig 5 M-patterns of the second data set 222-\224 the number of m-patterns in the log scale 223..\224 the number of border m-patterns in the log scale x-axis is minp minp increases Many of these patterns have very low support levels For example we found 59 border m-patterns with length from 2 to 5 in the first data set when minp  0.7 Half of these patterns have support levels below 10 To compare with frequent patterns it suffices to set minp  0 since the algorithm reduces to mining frequent patterns Figure 6 reports frequent patterns found in the first data Here the x-axis is minsup and the y-axis is the log of the number of patterns found Note that the number of frequent patterns is huge-in ex cess of 1 veri when when minsup is 20 Examining the frequent patterns closely we find that most are related to items that occur frequently, not necessarily items that are causally re lated This is not surprise since the marginal distribution of items in our data is highly skewed Indeed a small set of items account for over 50 of total events and consequently these items tend to appear in many frequent patterns Beyond the quality of the results produced by mining for fre t no U a man Fig 6 Frequent patterns of the first data set 223-\224 the number of frequent patterns in the log scale 223..\224 the number of border frequent patterns in the log scale; x-axis is minp 415 


quent itemsets, there is an issue with scalability as well In Fig ures 4 and 5 minp 2 0.1 and minsup  3 Suppose we have minp  0 and minsup  3 so that we are mining for frequent itemsets but with a very low support threshold When we at tempt to run this case more than 30k candidates are generated at the third level. Not only does this result in very large compu tation time, we ultimately run out of memory and so are unable to process the data We reviewed the m-pattern found with the operations staff Many patterns are related to installation errors \(e.g a wrong parameter setting of a monitoring agent and redundant events e.g 11 events are generated to signal a single problem In addition a couple of correlations were discovered that are being studied for incorporating into event correlation rules for the real time monitoring We emphasize that over half of the m-patterns discovered have very low support levels Why are m-patterns common in these data One reason is a result of physical dependence that manifests itself as a set of events when a problem arises For example when a local area network LAN fails ail hosts connected to the LAN gen erate 223lost connection\224 events. Further, the same hosts generate these events if the same failure occurs This results in the mu tual dependence of these events This observation suggests that m-patterns can be used to construct signatures for problematic situations A second cause of m-patterns is redundant information For example a device may generate an event to report a problem it detects However, there may also be several management agents that monitor the same device and report the same problem This results in an m-pattern consisting of redundant events Identify ing such m-patterns can aid in constructing filtering rules that re move redundant events More details and insights can be found in 13][10 VI CONCLUSION Motivated by the need to discover infrequent but strongly correlated patterns we propose a new pattern a mutual depen dence pattern or a m-pattern M-patterns are defined in terms of minp the minimum probability of mutual occurrence of items in the pattern In contrast to one-way dependence as in asso ciation rules an m-pattern is characterized by a strong mutual dependency between any two of its subsets. That is if any part of an itemset occurs, the other part is very likely to occur as well Our results suggest that such strong mutual dependencies are common in computer networks such as due to interrelated components that are impacted by the same failure We develop an efficient algorithm for discovering m-patterns This is accomplished in three steps First we develop a linear algorithm to qualify an m-pattern based on an equivalence we prove Second we show that a level-wise search can be used for m-pattern discovery a technique that is possible since we prove that m-patterns are downward closed Last, we develop an effec tive technique for candidate pruning by establishing a necessary condition for the presence of an m-pattern A significant impact of the resulting algorithm is that it discovers strongly correlated itemsets that may occur with low support levels something that is difficult to do with existing mining algorithms Using synthetic data we demonstrate that our algorithm scales well as the data set increases in size We also show that the pruning algorithm provides considerable benefit, especially for small values of minp We apply our algorithm to data collected from two produc tion computer networks The results show that there are many m-patterns, many of which of have very low support levels \(e.g fewer than 10 occurrences\Attempting to discover these pat terns using A-priori requires a very small value for support lev els, which results in an explosion of candidates that overruns the memory of the computer we used We further develop frequent m-patterns that are defined in terms of both minsup and minp We show that this is a more general pattern That is, when minp  0 this pattern is equiv alent to frequent itemsets and when minsup  0 frequent m patterns become m-patterns ACKNOWLEDGMENT The authors would like to thank Chang-shing Pemg for help ful discussions REFERENCES C Agganval C Agganval and V.V.V Parsad Depth first generation of long patterns In lnt\222l Conf on Knowledge Discover rind Drm Mining 2000 R Agrawal T Imielinski and A Swami Mining association rules be tween sets of items in large databases In Proc fj\222VLDB pages 207-216 1993 R Agrawal and R. Srikant Fast algorithms for mining association rules In Proc of VLDB 1994 R. Agrawal and R Srikant Mining sequential patterns In Proc of the I Ith Int 221I Conference on Datu Engineering Taipei Taiwan 1995 R Bayardo, R. Agrawal and D Gunopulos Constraint-based rule mining in large dense database In ICDE 1999 R.J Bayardo. Efficiently mining long patterns from database In SIGMOD pages 85-93 1998 S Brin, R. Motiwani and C Silverstein Beyond market baskets Gen eralizing association rules to correlations Datu Mining and Knowledge Discovery pages 39-68 1998 Edith Cohen Mayur Datar Shinji Fujiwara Aristides Gionis Piotr Indyk Rajeev Motwani, Jeffrey D Ullman and Cheng Yang Finding interesting associations without support pruning In ICDE pages 489-499 2000 J Han J Pei and Y Yin Mining frequent patterns without candidate generation \(pdf In Proc 2000 ACM-SIGMOD Int Cunf on Munugement of Data SIGMOD\222OO Dallas TX 2000 J.L Hellerstein and S Ma Mining event data for actionable patterns In lnternutional Conference for the resource manugement  perfiormance evaluation of enterprive computing systems 2000 B Liu and W Hsu Post-analysis of learned rules In AAA/-96 pages 828-834 1996 Bing Liu Wynne Hsu and Yiming Ma Pruning and summarizing the dis covered associations In Proceedings of the ACM SICKDD International Conjerence on Knowledge Discovery  Datu Mining pages 15 18 1999 S Ma and J.L Hellerstein Eventbrowser A flexible tool for scalable analysis of event data In DSOM\22299 1999 S Ma and J.L Hellerstein Mining partially periodic event patterns In ICDE pages 205-214,2001 H Mannila H Toivonen and A Verkamo. Discovery of frequent episodes in event sequences Data Mining mid Knowledge Discover 1\(3 1997 B Padmanabhan and A Tuzhilin A belief-driven method for discovering unexpected patterns In KDD-98 1998 J Pei and J Han Can we push more constraints into frequent pattern mining In CorS on Knowledge Discover rind Datu Mining KDD\222OO Boston MA 2000 H Toivonen Discovery of frequent patterns in large data collections 1996 Technical Report A-1996-5 Department of Computer Science Uni versity of Helsinki 416 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


