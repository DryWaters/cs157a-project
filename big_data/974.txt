html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">On the  Influence of Thread Allocation for Irregular Codes in NUMA Systems Juan A. Lorenzo, Francisco F. Rivera Computer Architecture Group Electronics and Computer Science Dept University of Santiago de Compostela Spain juanangel.lorenzo,ff.rivera}@usc.es Petr Tuma Distributed Systems Research Group Dept. of Software Engineering Charles University, Prague Czech Republic petr.tuma@dsrg.mff.cuni.cz Juan C. Pichel Galicia Supercomputing Centre Santiago de Compostela Spain jcpichel@cesga.es Abstract  This work presents a study undertaken to characterise the FINISTERRAE supercomputer, one of the biggest NUMA systems in Europe. The main objective was to determine the performance effect of bus contention and cache coherency as well as the suitability of porting strategies regarding irregular codes in such a complex architecture Results show that: \(1 considered as independent processors in this context; \(2 big data sizes, the effect of sharing a bus degrades the final performance but masks the cache coherency effects; \(3 the NUMA factor \(remote to local memory latency ratio an important factor on irregular codes and \(4 kernel allocation policy is not optimal in this system. These results allow us to understand the behaviour of thread-to-core mappings and memory allocation policies Keywords-Itanium2, Hardware Counters, Irregular Codes FinisTerrae I. INTRODUCTION Irregular codes [1] are the core of many important scientific applications, therefore several widespread techniques to parallelise them exist [1], [2], [3]. Many of these techniques were designed to work in SMP systems, where the memory access latency is the same for all processors. However, stateof-the-art architectures involve many cache levels in complex several-node NUMA configurations containing different number of multi-core processors. A good example is the new supercomputer FINISTERRAE installed at the Galicia Supercomputing Centre \(CESGA  an SMP-NUMA system with more than 2500 processors 19 TB of RAM memory, 390 TB of disk storage and a 20-Gbps Infiniband network, orchestrated by a SuSE Linux distribution. Designed to undertake great technological and scientific computational challenges, it is one of the biggest shared-memory supercomputers in Europe In such a complex infrastructure, the observations we can make are not straightforward, because the interplay of cache contention, bus contention, cache coherency and other mechanisms is far from transparent and therefore requires experimental assessment. In this context, the memory allocation and the thread-to-core distribution may become very important in the performance of a generic code and, more noticeably, in strategies to parallelise irregular codes which reorder the data to optimise the use of the cache hierarchy as in iterative kernels such as the sparse matrix-vector multiplication \(SpMV latencies, depending on the processor the data are assigned to, could significantly affect the performance In a recent work, a framework for automatic detection and application of the best mapping among threads and cores in parallel applications on multi-core systems was presented [5]. In addition, Williams et al. [6] propose several optimisation techniques for the sparse matrix-vector 


several optimisation techniques for the sparse matrix-vector multiplication which are evaluated on different multi-core platforms. Authors examine a wide variety of techniques including the influence of the process and memory affinity Regarding memory allocation, Norden et al. [7] study the co  location of threads and data motivated by the nonuniformity of memory in NUMA multi-processors, although they do not analyse the behaviour of interleaved memory regions. To our knowledge, nobody has studied the behaviour of several cores sharing a socket and/or a bus on an Itanium2 platform and its influence on irregular codes before The main objective of this work is to characterise the behaviour of the FINISTERRAE system and to study the suitability and impact of applying the mentioned strategies considering the influence of thread and memory allocations The article is organised as follows: Section II introduces the NUMA system under study. Section III explains the methodology followed to characterise its architecture, discussing the influence of the thread allocation. In Section IV the results of running some benchmarks in our architecture are presented and analysed. Afterwards, an actual parallelisation technique for irregular codes was ported to test its performance, which is explained in Section V. Finally, some concluding remarks and future work are given in Section VI II. THE FINISTERRAE SUPERCOMPUTER FINISTERRAE is an SMP-NUMA machine which comprises 142 HP Integrity rx7640 computation nodes. Each node consists of eight 1.6-Ghz-DualCore Intel Itanium2 Montvale \(9140N 2009 International Conference on Parallel and Distributed Computing, Applications and  Technologies 978-0-7695-3914-0/09 $26.00  2009 IEEE DOI 10.1109/PDCAT.2009.42 146 Figure 1. Block diagram of an HP Integrity rx7640 node. 4 dual-core Itanium2 Montvale are connected to their local memory in every cell through a Cell Controller, which uses a crossbar to communicate with an identical second cell NUMA configuration1. Figure 1 shows the block diagram of a node as well as its core disposition. As seen, a cell is composed of two buses at 6.8 GB/s, each connecting two sockets \(four cores chipset \(Cell Controller a cache-coherent memory system using a directory-based memory controller and connects both cells through a 27.3 GB/s crossbar. It yields a theoretical processor bandwidth of 13.6 GB/s and a memory bandwidth of 16 GB/s \(four buses at 4 GB/s The main memory address range handled by the Cell Controller is split in two modes: three fourths of the address range map to addresses in the local memory, the remaining one fourth maps in an interleaved manner to addresses in both local and remote memory Focusing on the processors, Figure 2 shows a block diagram of an Itanium2 Montvale processor. Each processor comprises two 64-bit cores and three cache levels per core This architecture has 128 General Purpose Registers and 128 FP registers. L1I and L1D \(write-through way set-associative, 64-byte line-sized caches. L2I and L2D write-back caches. Note that FP operations bypass L1 III. METHODOLOGY The previous section presented a several-memory-layer platform where each NUMA node  s cell behaves as an SMP. Since a good performance of many shared-memory scientific applications depends on the correctness of this assumption, our study focuses on quantifying the behaviour of a FINISTERRAE node depending on how the data allocation, the memory latency and the thread-to-core mapping can influence the code  s final performance 1There exists a 143th node composed of 128 Montvale cores and 1 TB 


1There exists a 143th node composed of 128 Montvale cores and 1 TB memory, which is not considered in this work Figure 2. 1.6Ghz Dual-Core Intel Itanium 2 Montvale \(9140N ture. Note the separated L3 cache per core In order to characterise our test platform, an analysis in two stages was carried out 1 bus contention as well as the effect of the cache coherency overhead were executed 2 gular codes, designed to be executed on SMP systems was ported to FINISTERRAE to evaluate its behaviour and, if possible, to apply the knowledge acquired in the benchmarking stage to improve its performance Orthogonally to this, two policies were adopted 1 cated in local cell memory and only cores in the same cell, following several thread-to-core assignments, are involved 2 allocated in any cell Outcomes were collected with PAPI [8] \(which accesses the Perfmon2 interface [9] underneath  The benchmarks used in the benchmarking stage perform some warm-up iterations before starting to measure and subsequently, they process the outcomes statistically to show the results in median values, which guarantees that the outliers are not taken into account. On the other hand, in the actual scenario stage, only values inside the outcomes  standard deviation were considered The next sections will develop the characterisation described above. Section IV will undertake the benchmarking stage to study issues such as the bus contention \(Section IV-A Section IV-B allocation \(Section IV-C study similar aspects \(sections V-B and V-C parallelisation technique for irregular codes IV. BENCHMARKING FINISTERRAE A. Influence of Thread Allocation in Bus Contention The first issue under study was the influence of the thread allocation upon the node buses. Considering that every four cores share a bus, it was reasonably foreseeable that any 147 Table I MEDIAN MEMORY ACCESS LATENCY \(IN TICKS BENCHMARK FOR DIFFERENT CONFIGURATIONS. DEPENDING ON THE SECOND CORE INVOLVED, CORE 8 WILL SHARE WITH IT THE BUS, THE CELL OR NONE OF THEM. A MEASUREMENT WITH CORE 8 ALONE FOR EACH MEMORY SIZE IS ALSO SHOWN AS A REFERENCE Cell Processors Memory allocated10KB 64MB 1GB 10GB Cell 1 8 - 0 4,0 338,6 349,8 532,7 8 - 2 3,9 338,8 349,6 534,5 8 - 4 4,0 338,4 349,6 532,6 8 - 6 3,9 338,6 349,6 532,8 8 3,5 329,0 340,6 525,7 Cell 0 8 - 9 3,5 352,9 366,4 546,2 8 - 10 3,5 354,3 366,0 550,2 8 - 12 3,5 342,3 353,7 539,3 8 - 14 3,5 342,2 353,6 537,4 allocation which spreads out the threads as much as possible through the different buses would get a better performance than another one which maps several threads in cores within the same bus, due to a possible bandwidth competition To quantify this effect, a benchmark called memtest was used. Memtest focuses on how multiple cores share the bandwidth to memory. It allocates a given-sized, private block of memory per core filled with a randomly linked pointer trail and goes through it reading and writing the data, which creates traffic associated to the data read and 


data, which creates traffic associated to the data read and subsequently, written-back to memory. To quantify the effect of sharing a bus, several configurations comprising different thread allocations were used. To avoid the system allocating the data in different memory regions \(local, remote or interleaved Cell 0. One thread was always mapped to Core 8 \(see Figure 1 same cell, either to Core 9 \(same processor, same bus Core 10 \(different processor, same bus different processor, different bus different processor different bus cores in Cell 1 \(cores 0, 2, 4 and 6 quantify the effect of using two cores which do not share any resources inside a cell. To have a comparison value a test mapping just one thread to Core 8 was also carried out. Table I quantifies the outcomes of those configurations for memory blocks of 10KB, 64MB, 1GB and 10GB in clock ticks per memory access \(note that, since the RDTSC instruction was used to measure the memory access time the final average calculation will include the access time but also some overhead in each test and the size of the memory allocated, a decrease in the performance was expected as long as the traffic in the bus increases \(because of bigger memory sizes cores share the same bus. That is, a memory size of 10KB is not expected to consume too much bandwidth since the data fits in the L1 and, once loaded in cache, the bus will not be used until write-back. For block sizes bigger than 9MB \(the L3 size Figure 3. Average L1 DTLB misses per access for the memspeed benchmark. The y-axis represents the number of events. The x-axis, the memory size in bytes for the considered cases only because of write-back, but also because of the cache replacing. Therefore, a poorer performance is expected when two threads mapped to two cores in the same bus and a big amount of memory allocated are considered The outcomes in Table I for 10KB show that, regardless of the pair of cores involved in Cell 0, the number of required clocks to access the data is the same \(3,5 ticks for a data block small enough to fit into a L1 there is almost no traffic in the bus and, therefore, no performance differences are observed. When the second core belongs to Cell 1 the time to access the data is also almost constant \(3,94,0 because all data are allocated in Cell 0 When the size of the block is increased over the L3 size 64MB and 1GB Focusing on Cell 0, the lowest average latency access occurs when Core 8 is alone in the bus. A second case with the highest latency access appears when Core 8 shares the bus with a core in the same processor \(Core 9 but in the same bus \(Core 10 enough not to fit into cache can generate enough traffic in the bus to decrease the performance when both cores compete for it. The third case appears when two cores access memory from different buses \(Cores 12 and 14 performance decrease is not as important as in the case when the same bus is shared. Taking into account that the latency is not as low as the one-core case and that the throughput in the Cell Controller-to-memory bus is the same regardless of the pair of cores used, we should conclude that the Cell Controller introduces, somehow, a small bottleneck when dealing with traffic from both buses. Besides, since there are no significant differences between allocating a thread in the same socket or in other socket sharing the bus, we can also conclude that, regarding bus sharing, each core can be considered as an independent processor in this context Focusing on Cell 1, the latency is approximately constant regardless of the core and bus involved \(?338 cycles for 64MB case and ?349 for 1GB 148 


148 cores in the same cell. Although this upholds our conclusion about the Cell Controller introducing a bottleneck, it must be taken with caution given that the difference is quite small and some measurement errors are assumed to be introduced The last case studied \(10GB regions. However, the latency increases noticeably whereas the 64MB and 1GB scenarios showed little difference between them. It seemed reasonable to think that the randomly linked pointer in such a large block size was increasing the page eviction. To confirm it, we studied the behaviour of the TLB. Figure 3 shows the L1 DTLB misses. Indeed, we can see that the number of cache misses is similar for both the 64MB and 1GB cases. However, the 10GB case yields a higher number of TLB misses. Even if some page requests can be satisfied by the L2 DTLB, the rest will produce page faults which will increase the access latency noticeably We can therefore conclude that it seems advisable that the data, regardless of the size \(as long as it is larger than the cache size B. Influence of Thread Allocation in Cache Coherency The second issue studied was the influence of the thread allocation upon the memory coherency protocols. The rx7640 memory coherency is implemented in two levels. A standard snooping bus coherence \(MESI for the two sockets sharing a bus, having on top of it an in-memory directory \(MSI Therefore, higher latencies are expected when the coherence has to be kept up between two cores in different buses than for two cores in the same bus, since in the former case, the directory must be read To quantify the effect of sharing a variable between two cores, a producer-consumer benchmark was used. The producer allocates and accesses a whole data block filled with a randomly linked pointer trail, subsequently modifying the data after fetching it into cache. Once the producer has finished, the consumer just reads the whole data. We defined a configuration where Core 14 is always the producer and different cores play the role of the consumer. Table II shows the ticks per access to transfer the data from the producer to different consumers. 10KB, 128KB, 6MB and 1GB data sizes were used to make them fit in the L1, L2, L3 or in memory, respectively At the sight of the results we can observe that if the consumer is in the same bus as the producer the time to fetch a cache line is shorter than if both are in different buses which is the case of Cores 12 and 15 for 10KB, 128KB and 6MB. This is due to the behaviour of the MESI protocol implemented at bus level, which is faster than reading the directory. It is also noticeable that the time is the same regardless of whether the consumer shares the socket with the producer or not. Remembering also that cores in an Itanium 2 Montvale processor do not share any cache level we can conclude that, regarding cache coherency, each core Table II DURATION \(TICKS/ACCESS TRANSFER THE ALLOCATED DATA BETWEEN TWO CORES Prod-Cons Memory allocated10 KB 128 KB 6 MB 1 GB 14 - 0 317,6 353,2 353,2 296,8 14 - 8 220,0 258,4 261,2 194,4 14 - 10 225,2 258,4 261,2 194,4 14 - 12 79,2 79,2 87,2 192,0 14 - 15 79,2 79,2 87,2 192,0 behaves as an independent processor in this context. When the consumer is in a different bus than the producer, which is the case of Cores 0, 8 and 10, the directory must be read to check in which bus the requested data is, with the subsequent rise in the access time. Cores 8 and 10 are in the same cell, so their latencies are similar and lower than the latency from Core 0, which is in another cell. In this latter case, since the data must be brought through another Cell Controller, it exhibits the highest latency 


Despite all data fitting in any cache in the previous cases \(10KB, 128KB and 6MB time increases slightly with the data size. This fact can be explained arguing that we are observing the effect of cache collisions due to the limited associativity of the caches An exception to the observed outcomes occurs for 1GB In that case, the time to fetch a cache line is practically identical regardless of the cores involved, as long as they belong to the same cell. The cause for this behaviour lies in the size of the allocated memory. For 10KB, 128KB and 6MB the data can reside in the L1, L2 or L3. However for 1GB the producer must flush the data back to memory after modifying it, so the consumer must fetch the data from main memory in most cases instead of doing it from another cache. Note also that the time to retrieve a data from a core in a remote cell is higher than from the local memory, as it is seen in the 14-0 case for 1GB, compared to the 6MB case We can conclude that, to minimise the effect of cache coherency, any parallel application working with a reduced amount of shared data  not much bigger than the cache size  should map its threads to the available cores in the same bus regardless of the socket in which they are. When this is not possible, the best choice is the adjacent bus in the same cell and, as a last option, a core in a different cell. On the contrary, a parallel application which allocates a large amount of memory might saturate the bus \(as was shown in Section IV-A performance. In this case, mapping the threads to cores in different buses of the same cell might be the best option since, as shown in Table II, for a big amount of memory the latencies due to cache coherency become the same for all buses. Therefore, the application should be first characterised to find out whether the restricting factor is the traffic in the bus due to the amount of allocated memory or due to the cache coherency, in order to take a proper decision 149 C. Influence of Memory Allocation As explained in Section II, about one fourth of a cell  s memory in FINISTERRAE is configured in interleaving mode, which means that all data allocated in that part are distributed between local memory addresses and remote ones This behaviour allows to decrease the average access time when accessing data simultaneously from cores belonging to different cells In this section, an experiment was carried out to compare the theoretical memory latency given by the manufacturer 11] with our observations. We measured the memory access latency of a small Fortran program, which creates an array and allocates data in it. The measurements were carried out using specific Itanium2  s hardware counters, the Event Address Registers \(EAR  s underlying interface, Perfmon2, samples the application at run-time using EARs, getting the memory position and access latency of a given sample accurately Figure 4 depicts the results when allocating the data in the same cell as the used core \(a b the interleaving zone \(c happen within 50 cycles, corresponding with the accessed data which fit in cache memory. There is a gap and, then different values can be observed depending on the figure Figure 4\(a when accessing the cell local memory. The frequency of our processors is 1.6 Ghz, which yields a latency from 180,9 to 239,7 ns. Its average value is 210,3 ns, slightly higher than the 185 ns given by the manufacturer When accessing data in a cell remote memory we measured occurrences between 487 and 575 cycles, that is, from 304,8 to 359,8 ns, with an average value of 332,3 ns. The manufacturer does not provide any values in this case In the case of accessing data in the interleaving zone, the manufacturer value is 249 ns. Our measurements give two 


manufacturer value is 249 ns. Our measurements give two zones, depending on whether the local or remote memory are accessed. Indeed, the average access time in the interleaving zone is the average of combining accesses to the local or remote memory. Our outcomes gave an average value of 278,3 ns We can conclude that, when working with codes mapped to cores in a same cell \(especially for those who demand a high level of cache replacement allocated in the same cell  s memory. The access to remote memory becomes very costly, so if cores in both cells must be used, the allocation of the data in the interleaving memory makes sense V. INFLUENCE OF THREAD ALLOCATION ON IRREGULAR CODES A. Ported Technique The next step consisted of porting a parallelisation technique for irregular codes to our target architecture, in order a b c Figure 4. Latency of memory accesses when the data is allocated in memory local to the core \(a b interleaving zone \(c access. The x-axis shows the latency in cycles per memory access. Regions of interest have been zoomed in 150 Table III SPARSE MATRIX-VECTOR PRODUCT \(LEFT REDUCTION \(RIGHT for j = 1 to N do for k = col\(j j +1 i = row\(k Z\(i i k j end for end for for j = 1 to NNZ do i = row\(k Z\(i i end for to check out the effect of thread allocation in an actual application. The chosen technique was IARD [12] \(Irregular Access Region Descriptor characterise irregular codes at run-time which exploits the properties found in the access patterns, expressing them with a structure that allows a strong reduction in storage requirements without loss of relevant information. The lowcost, compact characterisation of the subscript arrays can be used to perform an efficient parallelisation of a wide set of irregular codes In order to test the efficiency of this technique, two well known benchmarks \(Sparse matrix-Dense vector Product and Irregular Reduction parallelised using OpenMP. The pseudocodes are shown in Table III. In both cases, some selected sparse matrices stored in CSC format from the Harwell-Boeing Sparse Matrix Collection [13], which had previously been reordered using this technique, were used as an input. The key features of these matrices are shown in Table IV The compiler used for the experiments was Intel ifort 9.1.052. The baseline compile configuration used in all our tests involved the -O3 option and the interprocedural optimisation \(-ipo B. Influence of Bus Contention To study the influence of the bus contention several scenarios were tested. On the one hand, an intra-cell scenario where the data are allocated in a single cell  s memory and the cores involved belong to the same cell. On the other hand, an inter-cell scenario where the data are fetched from the other cell  s memory and there are cores involved from both cells. Before presenting the results, it is necessary to 


both cells. Before presenting the results, it is necessary to clarify the meaning of a so-called thread distribution. For example, a distribution 15-11-13-9 means that for a onethread execution, the thread will be allocated in Core 15. For Table IV SQUARED-MATRIX INPUT SET USED IN OUR TESTS. Na=MATRIX SIZE Nz =NUMBER OF NON-ZEROS Name Na Nz Description s3dkq4m2 90451 4820892 FEM, cylindrical shell 3dtube 45330 3213618 3-D pressure tube nasasrb 54872 2677324 Shuttle rocket booster struct3 53570 1173694 Finite element bcsstk29 13992 619488 Model of a 767 rear bulkhead bcsstk17 10973 428650 Elevated pressure vessel Table V PERFORMANCE IMPROVEMENT \(% MFLOPS DISTRIBUTION 15-11-13-9 OVER 15-14-13-12 FOR Nc = 2 AND Nc = 4 CORES. DATA ARE ALLOCATED IN CELL 0 MEMORY Matrix Nc Benchmark IrregRed SpMV s3dkq4m2 2 0,0 6,34 1,6 23,9 3dtube 2 0,8 6,64 0,7 13,5 nasasrb 2 2,7 1,44 -1,1 7,7 struct3 2 -1,1 0,74 0,5 1,6 bcsstk29 2 0,9 1,74 1,3 1,8 bcsstk17 2 1,0 0,24 1,5 0,7 two threads, they will be allocated in Cores 15 and 11 and for four threads, they will be in Cores 15, 11, 13 and 9. In the cases where more cores are used and the distribution is not pointed out, it means that the order in which the remaining available cores are assigned is not relevant 1 library was used to allocate all data in the Cell 0 memory and only cores in this cell were used. Several scenarios were set up to study the effect of running the Irregular Reduction and SpMV benchmarks with different distributions, in order to quantify the effect of the bus sharing 15-11-13-9 vs 15-14-13-12: This scenario is focused on 2 and 4 cores. In both benchmarks the sparse matrix must be completely read, which is expected to cause some number of capacity and compulsory cache misses. Therefore, the bigger the matrix, the higher the traffic in the bus, so a performance decrease is expected for the biggest matrices when sharing the bus. At the sight of the results \(Table V a better behaviour of the distribution 15-11-13-9 can be appreciated \(differences over 6% are highlighted biggest matrices \(3dtube, nasasrb and s3dkq4m2 four-core case. Indeed, it was confirmed that the bigger the matrix, the higher the traffic in the bus and the higher the improvement when there are only two threads per bus \(15-11 and 13-9 15-14-1312 can be observed, the traffic is not high enough to be relevant It is noticeable that the most important improvements occur only in the SpMV benchmark, especially for four processors This behaviour happens because SpMV executes a loop which goes through five different arrays, whereas Irregular Reduction goes only through two. So in the former case the cache reuse is smaller than in the latter one. At any given moment, therefore, the SpMV benchmark generates more traffic in the bus than the Irregular Reduction one 15-11-13-9 vs 15-14-11-10: This scenario is focused on 4 cores. The outcomes in Table VI show that there are no noticeable differences between the case when the threads are 151 Table VI PERFORMANCE IMPROVEMENT \(% MFLOPS DISTRIBUTION 15-14-11-10 OVER 15-11-13-9 FOR Nc = 4 CORES DATA ARE ALLOCATED IN CELL 0 MEMORY Matrix Nc 


Matrix Nc Benchmark IrregRed SpMV s3dkq4m2 4 4,1 2,4 3dtube 4 -0,5 1,6 nasasrb 4 0 0,8 struct3 4 0,6 0,3 bcsstk29 4 0,6 -0,2 bcsstk17 4 1,4 0,1 allocated in different sockets and the case when two threads in the same bus share the socket. We know that IARD has no dependencies among the cores involved. Since the outcomes show almost no differences between threads mapped to different cores in different sockets and threads sharing the same socket, we can confirm, then, the conclusion of Section IV-A, which stated that each core behaves as an independent processor regardless of the sockets they are placed in 2 fetching data from a memory module in other cell was studied. Moreover, a comparison was carried out between the automatic thread assignment the system does and our manual assignments. To do that, libnuma was used through its command line tool numactl, in order to allocate all the data in the cell we were working in \(memory 0 memory \(memory 1 memory 2 We intended to infer from these experiments whether the default system behaviour is reliable or, on the contrary another memory allocation policy should be used 15-11-13-9 vs system-assigned: We used the Perfmon2 library to check the system  s default policy to assign threads to cores. This scenario compares a manual allocation with the default one done by the system. We found out that the system assigns the threads without a defined policy changing it in every execution, although it strives to place the threads in separate cells as far as it is possible. Therefore after discarding outliers, we present a range of results inside the standard deviation instead of a single average value. The outcomes in Table VII show a dramatic improvement with our distribution, especially for the biggest matrices \(3dtube nasasrb and s3dkq4m2 cell scenario, there was no distribution that showed such a big difference in the performance, which indeed suggests that the system does not maximise locality Threads spread over both cells: Our second test intended to find out whether the system assignment spreads the threads over two cells, even if there are available cores in the same cell. So the automatic assignment was compared to a manual distribution where the number of involved threads is kept balanced between both cells. The threads were distributed to cores 15-7 for two threads, 15-7-11-3 for four and 15-7-11-3-13-5-9-1 for eight. Focusing on the Table VII PERFORMANCE IMPROVEMENT \(% MFLOPS DISTRIBUTION 15-11-13-9 \(DATA ALLOCATED IN CELL 0 MEMORY OVER AUTOMATIC ASSIGNMENT FOR Nc = 2 AND Nc = 4 CORES AVERAGE, MAXIMUM AND MINIMUM IMPROVEMENTS ARE SHOWN Matrix Nc Benchmark IrregRed SpMV Avg. Max Min Avg. Max Min s3dkq4m2 2 23,8 25,5 23,1 32,5 32,9 32,34 9,8 14,3 7,5 15,0 17,5 13,9 3dtube 2 17,8 20,0 16,3 34,0 34,4 33,84 4,0 7,7 1,5 10,9 13,5 9,1 nasasrb 2 9,5 13,5 7,9 31,6 33,0 31,24 -0,8 4,3 -1,9 14,6 18,0 11,7 struct3 2 -0,3 2,5 -1,1 13,9 17,0 11,54 -1,1 3,0 -1,7 3,6 8,4 1,6 bcsstk29 2 0,1 2,0 -0,3 1,3 5,2 0,64 -1,5 3,8 -2,3 0 2,4 -0,7 bcsstk17 2 0,3 3,3 0 1,6 4,8 1,14 -1,5 10,1 -3,3 0,2 3,2 -0,4 Table VIII PERFORMANCE IMPROVEMENT \(% MFLOPS DISTRIBUTION 15-7-11-3 WITH DATA ALLOCATED IN THE INTERLEAVING ZONE VS. AUTOMATIC DISTRIBUTION FOR Nc = 2 Nc = 4 AND Nc = 8 CORES. AVERAGE, MAXIMUM AND MINIMUM IMPROVEMENTS ARE SHOWN 


IMPROVEMENTS ARE SHOWN Matrix Nc Benchmark IrregRed SpMV Avg. Max Min Avg. Max Min s3dkq4m2 2 -6,9 -5,6 -7,4 4,2 4,4 4,04 -4,5 -0,6 -6,5 0,8 3,0 -0,2 8 -2,0 3,0 -3,0 2,0 6,3 -1,9 3dtube 2 -2,5 -0,8 -3,9 5,3 5,7 5,14 4,4 8,1 1,9 -3,2 -0,9 -4,7 8 2,2 18,3 -2,7 2,4 8,0 -1,7 nasasrb 2 -3,7 -0,2 -5,1 3,0 4,1 2,64 -2,3 2,7 -3,3 -2,5 0,4 -5,0 8 -0,1 12,1 -1,9 6,8 12,6 2,5 struct3 2 1,1 3,9 0,3 -3,6 -1,0 -5,64 0,1 4,3 -0,4 -1,2 3,4 -3,1 8 0,2 6,5 -1,5 3,0 7,3 0,6 bcsstk29 2 0,1 2,1 -0,3 -2,2 1,0 -2,94 -0,1 5,2 -1,0 0,6 3,0 -0,1 8 0,7 11,1 -2,7 -1,4 3,5 -2,6 bcsstk17 2 -0,1 2,9 -0,4 0,9 4,1 0,54 0,5 12,6 -1,4 0,6 3,6 0 8 1,4 20,4 -3,6 -0,6 5,0 -1,8 IARD, where there are no dependencies between threads, the outcomes in Table VIII show that all differences are below 6%, except for some cases in the biggest matrices: nasasrb SpMV, 8 threads IrregRed, 2 threads In general, especially for small matrices, our distribution performs better than the automatic one C. Influence of Cache Coherency Taking into account that this kind of irregular codes strives to maximise the thread locality, we do not expect to notice important performance decreases due to the cache coherency protocols when the input matrices are big enough, since they will be masked by the traffic in the bus. If the matrices fit in each core  s cache, all misses, except the capacity ones, will be solved by the snooping protocol inside the bus, faster than 152 asking the directory. However, this will not usually be the case in real applications, because this type of codes usually operates with matrices far bigger than the cache size VI. CONCLUSIONS This paper presented the results of several tests carried out to characterise FINISTERRAE, an SMP-NUMA machine, in order to study the suitability of applying strategies to parallelise irregular codes initially developed for SMP systems The main factors considered were the thread-to-core distribution and the memory allocation. Firstly, a benchmark was executed to test the performance influence of several cores when sharing a bus and allocating the data in local or remote memory. Secondly, another benchmark was used to evaluate the influence of the cache coherency between two cores when sharing data. Furthermore, another test evaluated the memory access latency depending on the memory module allocated \(local, remote or interleaving At the sight of the results, we can claim that, especially for applications which use the bus intensively, the effect of sharing a bus between two or more cores degrades the performance and should be avoided by spreading the threads among cores in different buses when possible. We must take into account, though, that the test to evaluate the local and remote memory access latencies yielded important differences between them. Therefore, in cases where the threads must be spread in two cells the best policy will be to analyse and split the data in both memory zones, maximising the locality or, when not possible, allocating the data in the interleaving zone Regarding the cache coherency, the effects in the performance are noticeable when dealing with small sizes of data which fit into caches. The more the memory size increases the less significant the effect is. Therefore, for small data sizes, it would be advisable to map the threads on cores in the same bus. For bigger data sizes, the effect of sharing a bus will be more important and it will mask the effect of cache coherency. A noteworthy fact is that every core in the same processor behaves as an independent processor, so we can consider two processors in a bus as four independent 


can consider two processors in a bus as four independent cores After the benchmarking stage, an actual strategy to parallelise irregular codes, successfully tested on SMP architectures, was ported to FINISTERRAE. A set of matrices was chosen and reordered with this strategy, being applied subsequently to the sparse matrix-vector product and the Irregular Reduction benchmarks. Since this code works with sparse data, the effects regarding bus sharing were not as noticeable as in the previous stage. However, it was possible to confirm the importance of spreading the threads among cores in different buses when dealing with big data sizes the behaviour of each core as an independent processor, and the fact that the coherency effects are masked by the bus sharing when increasing the data size As a future work, we intend to develop strategies to guide applications at run-time in the frame of our project, so the conclusions presented here will help to define a thread-tocore mapping and memory allocation policies ACKNOWLEDGEMENTS This work was supported by the MCyT of Spain through the TIN2007-67537-C03-01 project and by the 2008/CE377 contract among HP, CESGA, UDC and USC REFERENCES 1] L. Rauchwerger, N. M. Amato, and D. A. Padua  Run-time methods for parallelizing partially parallel loops  in Proc. of the Int. Conf. on Supercomputing. ACM press, 1995, pp 137  146 2] R. Eigenmann, J. Hoeflinger, and D. Padua  On the automatic parallelization of the perfect benchmarks  IEEE Trans Parallel Distrib. Syst., vol. 9, no. 1, pp. 5  23, 1998 3] E. Gutie  rrez, O. Plata, and E. L. Zapata  A compiler method for the parallel execution of irregular reductions in scalable shared memory multiprocessors  in Proc. of the Int. Conf on Supercomputing, ACM SIGARCH. Springer-Verlag, May 2000, pp. 78  87 4] Galicia Supercomputing Centre, http://www.cesga.es 5] J. W. Tobias Klug, Michael Ott and C. Trinitis  Autopin - automated optimization of thread-to-core pinning on multicore systems  in Transactions on HiPEAC, vol. 3, no. 4, 2008 6] S. Williams, L. Oliker, R. Vuduc, J. Shalf, K. Yelick, and J. Demmel  Optimization of sparse matrix-vector multiplication on emerging multicore platforms  in SC  07: Proc. of the 2007 ACM/IEEE Conf. on Supercomputing. New York NY, USA: ACM, 2007, pp. 1  12 7] M. Norde  n, H. Lo  f, J. Rantakokko, and S. Holmgren  Dynamic data migration for structured amr solvers  International Journal of Parallel Programming, vol. 35, no. 5, pp 477  491, 2007 8] Performance Application Programming Interface http://icl.cs.utk.edu/papi 9] S. Eranian, The perfmon2 Interface Specification. Technical Report HPL-2004-200R1. HP Labs, February 2005 10] Perfmon2 monitoring interface and Pfmon monitoring tool http://perfmon2.sourceforge.net 11] HP Integrity rx7640 Server Quick Specs, http://h18000 www1.hp.com/products/quickspecs/12470 div/12470 div.pdf 12] D.E.Singh, M.J.Martin, and F.F.Rivera  A run-time framework for parallelizing loops with irregular accesses  in Proc Seventh Workshop Languages, Compilers, and Run-Time Systems for Scalable Computers, Washington DC, USA, 2002 13] The Harwell-Boeing Sparse Matrix Collection http://math.nist.gov/MatrixMarket/collections/hb.html 153 pre></body></html 


also be used to define constraints for service selection At run-time, the workflow engine for XML nets interprets and evaluates Filter Schemas and transition inscriptions to filter and create Web service descriptions as XML documents. With process monitoring and administration tools which are usual components of workflow management systems, new criterions and constraints for WS discovery and selection can be incorporated at run-time by reconfiguring transition inscription statements and variable instantiations in Filter Schemas  Instantiating abstract WS transition Figure 4 shows a fragment of the abstract WSC process in Section 5.1 to demonstrate the ability of XML nets in WS discovery. To obtain transport information, Web services whose descriptions are stored in an unknown WSDL repository should be discovered The token count of the place representing the repository is supposed to be unlimited. The abstract transitions find transport info service? and ?get transport information? are used to bind and call Web services at runtime. The incoming arc of the transition ?find transport info service? is a reading connection \(represented by a dashed arc nipulative Filter Schema FS2 used to create Web service descriptions is assigned to the outgoing arc of this transition. FS1 and FS2 are both generated according to WSDL Schema  Adapting abstract WS transition at run-time Suppose that the transition ?get transport information? is used to acquire flight information. Variables in FS1 should be \(partly  3 http://www.w3.org/TR/wsdl#A4.1 4 http://uddi.org/schema/uddi_v3.xsd es supplying flight information. As shown in Figure 5a the variable of the attribute filter ?name? of the element filter ?service? is instantiated with a regular expression for filtering Web services whose name contains the string ?flight?. This regular expression, together with the Filter Schema, is parsed to XQuery/XPath statements in process simulation or execution. If we use a reconfiguration tool to modify the regular expression at run-time as shown for example in Figure 5b, the transition ?get transport information? can then be adapted to acquire other transport information, e.g., train information. After a suitable WSDL document has been found, FS2 is used to create a new WSDL document for the place ?WS description by cloning the discovered document. Figure 5c shows the diagram of FS2 containing the WSDL root element definitions? and an element placeholder, which is used because the internal structure and details of the document to be created are not of interest in the case of document duplication  QoS-aware WS selection XML nets can also be used to select desired Web services if service constraints like QoS or customized preferences are taken into consideration. As WSDL and OWL-S provide no or limited ways for describing Web service?s QoS, we use in this paper OWL-QoS 43], which complements OWL-S with an ontology to specify QoS metrics for Web services, to demonstrate the ability of XML nets for \(QoS-aware Naturally, other Web service ontologies \(e.g., MOQ 11 e.g., WSLA [18 can also be used for this purpose Figure 6 shows another XML net fragment for the acquisition of transport information by selecting OWL 


acquisition of transport information by selecting OWLQoS documents which describe quality of Web services from a repository that contains 3 OWL-QoS documents discovered previously. Similar to the example in Figure 2, the Filter Schema FS1 is used to read and select appropriate OWL-QoS documents, and the Filter Schema FS2 to create new OWL-QoS documents for the place ?OWL-QoS document?. As Filter Schema provides no means for formulating inequality operators like ?&gt;? and ?&lt;=?, which are usually used to express constraints with parameters whose quantitative values are limited to a certain interval, transition inscriptions Figure 4. XML net for Web service discovery Figure 5. Filter Schemas for Web service discovery definitions 1 service name: \\s*flight\\s*/i c definitions service name: \\s*train\\s*/i b a definitions 1 1 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 can be used in combination with Filter Schemas to formulate QoS constrains Suppose that the traveler wants to use only the Web services that cost no more than 100 US Cent. The Filter Schema FS1 should then be created as shown in Figure 7. The variable of the attribute filter ?rdf: resource? of the element filter ?owl: onProperty? is instantiated with a regular expression to filter OWL-QoS documents specifying the property ?costUSCent?. To ensure that the maximal cost of the Web service doesn?t exceed 100 US cent, the inscription of the transition ?select transport info service? should be formulated as owl:maxCardinality &lt;= 100?. In process simulation or execution, the inscription is parsed and integrated into XQuery statements using XPath inequality operators  6. Conclusions  In this paper we presented a WSC method based on XML nets, which inherit advantages of Petri nets such as formal semantics and graphical expression. XML nets have additional strengths in the description of process and data objects, and the exchange of XMLbased structured data. The advantage of using XML nets for WSC is that control flow modeling, data and data flow modeling, and WS discovery and selection can be realized using a uniform powerful modeling language. Some uncomplicated tasks, as illustrated before, can be fulfilled without developing or using additional software components or agents. Naturally XML nets can also be combined with other Web service techniques to fulfill more complicated and demanding tasks  7. Acknowledgement  The authors would like to thank the anonymous referees for many valuable comments on an earlier version of this paper  8. References  1] P. Alvarez, J. Banares, and J. Ezpeleta, ?Approaching 


1] P. Alvarez, J. Banares, and J. Ezpeleta, ?Approaching Web Service Coordination and Composition by Means of Petri Nets: the Case of the Nets-within-Nets Paradigm?, ICSOC 2005, LNCS 3826, pp.185-197, 2005 2] M. ter Beek, A. Bucchiarone, and S. Gnesi, ?Web Service Composition Approaches: From Industrial Standards to Formal Methods?, Second International Conference on Internet and Web Applications and Services \(ICIW?07 Computer Society, 2007 3] X.N. Feng, Q. Liu, and Z. Wang, ?A Web Service Composition Modeling and Evaluation Method Used Petri Net?, LNCS Volume 3842/2006, pp. 905-911, SpringerVerlag, 2006 4] H. Foster, S. Uchitel, J. Magee, and J. Kramer, ?Modelbased verification of Web Service Compositions?, 18th IEEE International Conference on Automated Software Engineering, pp. 152- 161, 2003 5] X. Fu, T. Bultan, and J.W. Su, ?Analysis of Interacting BPEL Web Services?, WWW2004, pp. 17-22, New York USA, May 2004 6] J.D. Ge, H.Y. Hu, P. Lu, H. Hu, and J. L  Translation of Nets Within Nets in Cross-Organizational Software Process Modeling?, SPW 2005, LNCS 3840, pp. 60-375 Springer-Verlag, 2005 7] Group for program system, faculty of Information Technique University Dortmund, ?PDDL?, http://ls5www.cs.uni-dortmund.de/~edelkamp/ipc-4/pddl.html 8] R. Hamadi and B. Benatallah, ?A Petri Net-based Model for Web Service Composition?, Fourteenth Australasian Database Conference \(ADC2003 CRPIT, Vol. 17, pp. 191-200, 2003 9] S. Hinz, K. Schmidt, and C. Stahl, ?Transforming BPEL to Petri Nets?, BPM 2005, LNCS 3649, pp. 220?235, Springer-Verlag, 2005 10] H. Kang, X.L. Yang, and S.M. Yuan, ?Modeling and verification of Web Services Composition based on CPN 2007 IFIP International Conference on Network and Parallel Computing-Workshops, pp. 613-617, 2007 11] H.M. Kim, A. Sengupta, and J. Evermann, "MOQ: Web Services Ontologies for QOS and General Quality EvaluaFigure 6. XML net for Web service selection Figure 7. Filter Schema for QoS-aware WS selection owl:Ontology owl:Class rdfs:subClassOf owl:Restriction rdf :resource : "\\s+#costUSCent owl:onProperty owl:maxCardinality   rdf:RDF Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 tions", European Conference on Information Systems \(ECIS 2005 12] S. Klink, Y. Li, and A. Oberweis, "INCOME2010 - a Toolset for Developing Process-Oriented Information Systems Based on Petri Nets", International Workshop on Petri Nets Tools and APplications \(PNTAP 2008, associated to SIMUTools 2008 March 2008 13] O. Kluge, ?Petri nets as a Semantic Model for message Sequence Chart Specifications?, proceedings of INT 2002 pp. 138-147, 2002 14] K. Lenz and A. Oberweis, ?Inter-Organizational Business Process Management with XML Nets?, H. Ehrig, W Reisig, G. Rozenberg, H. Weber \(Eds gy for Communication Based Systems, LNCS 2472, pp. 243263, Springer-Verlag, 2003 15] K. Lenz and A. Oberweis, "Workflow Services: A Petri Net-Based Approach to Web Services", Int. Symposium on Leveraging Applications of Formal Methods, pp. 35-42, Pa 


Leveraging Applications of Formal Methods, pp. 35-42, Paphos/Cyprus, November 2004 16] L. Lin and I.B. Arpinar, ?Discovery of Semantic Relations between Web Services?, IEEE International Conference on Web Services \(ICWS?06 17] Q. Lin, J.D. Ge, H. Hu, and J. Lu, ?An Approach to Model Cross-Organizational Processes using Object Petri net?, 2007 IEEE Congress on Services \(SERVICES 2007 pp. 146-152, July 2007 18] H. Ludwig, A. Keller, A. Dan, R. P. King, and R Franck, ?Web Service level Agreement \(WSLA Specification version 1.0?, IBM, 2003 19] N. Lohmann, P. Massuthe, C. Stahl, and D. Weinberg Analyzing Interacting BPEL Process?, S. Dustdar, J.L. Fiadeiro, and A. Sheth \(Eds 32, Springer-Verlag, 2006 20] S.A. Mcllraith and T.C. Son, ?Adapting Golog for Composition of Semantic Web Services?, 8th International Conference on Knowledge Representation and Reasoning KR2002 21] S.A. Mcllraith, T.C. Son, and H.L. Zeng, ?Semantic Web Services?, IEEE Intelligent Systems, March/April 2001 16\(2 22] N. Milanovic and M. Malek, ?Current Solution for Web Service Composition?, IEEE Internet Computing, NovemberDecember 2004 23] S. Narayanan and S.A. Mcllraith, ?Simulation, Verification and Automated Composition of Web Service?, 11th International World Wide Web Conference, Honolulu, Hawaii, USA, May 2002 24] The Organization for the Advancement of Structured Information Standards \(OASIS Process Execution Language Version 2.0?, 11 April, 2007 http://docs.oasis-open.org/wsbpel/2.0/wsbpel-v2.0.pdf 25] S.R. Ponnekanti and A. Fox, ?SWORD: a Developer Toolkit for Web Services Composition?, 11th International World Wide Web Conference, Honolulu, Hawaii, USA, May 2002 26] Z.Z. Qian, S.L. Lu, and L. Xie, ?Colored Petri Nets Based Automatic Service Composition?, 2007 IEEE AsiaPacific Services Computing Conference, pp. 431-438, 2007 27] C. Ouyang, E. Verbeek, W.M.P. van der Aalst, S. Breutel1, M. Dumas1, and A.H.M. ter Hofstede1, ?Formal semantics and analysis of Control flow in WS-BPEL?, BPM center Technical Report, BPM-05-15, 2005 28] J.H. Rao and X.M. Su, ?A Survey of Automated Web Service Composition Methods?, SWSWPC 2004, LNCS 3387, pp. 43-54, Springer-Verlag, 2005 29] J.H. Rao, P. Kuegas, and M. Matskin, ?Application of Linear Logic to Web Service Composition?, 1st International Conference on Web Services, Las Vegas, USA, June 2003 30] J.H. Rao, P. Kuegas, and M. Matskin, ?Logic-based Web Services Composition: From Service Description to Process Model?, 2004 International Conference on Web Services, pp.446-453, San Diego, USA, July 2004 31] M. Sgroi, A. Kondratyev, Y. Watanabe, L. Lavagno and A. Sangiovanni-Vincentelli, ?Synthesis of Petri Nets from Message Sequence Charts Specifications for Protocol Design?, Conference on Design, Analysis, and Simulation of Distributed Systems, pp. 193-199, 2004 32] W.M.P. Van der Aalst, "The Application of Petri Nets to Workflow Management", The Journal of Circuits, Systems and Computers, 8\(1 33] The World Wide Web Consortium \(W3C Semantic Markup for Web Services?, 22 November, 2004 http://www.w3.org/Submission/OWL-S 34] The World Wide Web Consortium \(W3C vices Choreography Description Language Version 1.0?, 17 December, 2004, http://www.w3.org/TR/2004/WD-ws-cdl10-20041217 35] The World Wide Web Consortium \(W3C vice Choreography Interface \(WSCI 


http://www.w3.org/TR/wsci 36] The World Wide Web Consortium \(W3C vice Modeling Language \(WSML http://www.w3.org/Submission/WSML 37] The World Wide Web Consortium \(W3C vice Modeling Ontology \(WSMO http://www.w3.org/Submission/WSMO 38] J. Yang and M.P. Papazoglou, ?Web Component: A Substrate for Web Service Reuse and Composition?, Proc 14th Conf. Advanced Information Systems Eng. \(CAiSE 02 LNCS 2348, pp. 21?36, Springer-Verlag, 2002 39] Y.P. Yang, Q.P. Tan, and Y. Xiao, ?Verifying Web Services Composition Based on Hierarchical Colored Petri Nets?, IHIS?05, pp. 47-53, Bremen, Germany, 2005 40] Y.P. Yang, Q.P. Tan, Y. Xiao, J.S. Yu, and F. Liu, ?Exploiting Hierarchical CP-Nets to Increase the Reliability of Web Services Workflow?, Symposium on Applications and the Internet \(SAINT?06 41] X.C. Yi and K.J. Kochut, ?Process Composition of Web Services with Complex Conversation Protocols: a Colored Petri Nets Based Approach?, Conference on Design, Analysis, and Simulation of Distributed Systems, pp.141-148, 2004 42] D. Zhovtobryukh, ?A Petri Net-based Approach for Automated Goal-Driven Web Service Composition?, SIMULATION, Vol. 83, Issue 1, pp.33-63, January 2007 43] C. Zhou, L.T. Chia, and B.S. Lee, "Web Services Discovery with DAML-QoS Ontology", International Journal of Web Services Research, vol. 2: no. 2: pp. 43-66, 2005   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


  17 Mission Phase Relevant Archit ecture Informat ion Purpose Funct ion Mat urit y Pr oduct s DODAF M odel Re f e r e nce N ot e s Preliminary System Design Integrated Risk List Cross  functional list of risks compiled across integrated product team PDR Delivery Consolidated Ri sk Li st FFP1A Operational Environment M atrix documenting how test requirements that have been levied are satisfied with test \(at both the component and system levels Identification of the method that w ill be used to verify the requirement Identification of any potential non-conformances  Initial Delivery Environmental Te st  Verification Matrix FFP7 This will show the capability of the SV to withstand various environments \(i.e. launch vehicles System Interface Control Documentation This will be a suite of documents, the mission plan s hould identify critical system boundaries that reuqire a formal interface control document M inimum Criteria:  SV - Ground and Payload to SV ICD initial drafts must be compl e t e Other pertinent ICDs:  LV - Spacecraft, Component interface ICD Initial Delivery Interface Control Documentation SV 1  SV 6 Schedule Program Driving Schedule Requirements Key schedule driven technical decisions Giver receiver relationships that span different program elements Li v i n g Document Intgrated Milestone Schedule PV2 System Sub-system Design Specifications Partial Preliminary understanding of system subsystem design Allocation of required system functions to configuration items Demonstration of how system requirements are satisfied by design Initial Delivery PDR De si gn  Presentation SV 5 Open System Trades Desription organized by susbsystem of open design trades and decsions that need to be completed Each trade should have an owner and assocaited due dates that are aligned with program constriants Li v i n g Document Trade  Decision tracking matrix FFP5 Technical Performance Measures Demonstrate design peformance to critical program requirements outlined within the requirements document Initial Delivery Technical performance budget SV 7 Values in the budget should be compared to industry standards for a given maturity in the devleopment De t ai l e d De si gn System  Design Specifications Detailed description of "to be"  system subsystem design Allocation of required system functions to configuration items Demonstration of how system requirements are satisfied by design Final Delivery CDR De si gn  Presentation SV 4  SV 5 Note: Reference Lesson 11 - Need to look at some views and diagrams that would be useful for every subsystem Integrated Risk List Cross  functional list of risks compiled across integrated product team CDR Delivery Consolidated Ri sk Li st FFP1A Operational Environment Matrix documenting how test requirements that have been levied are satisfied with test \(at both the component and system levels Identification of the method that w ill be used to verify the requirement Identification of any potential non-conformances  Final Delivery Environmental Te st  Verification Matrix FFP7 Note: previous delivery s houl d have defined how requirements would be satisfied for long lead components.  This delivery would address all remaiing compents and system levels System Interface Control Documentation This will be a suite of documents, the mission plan s hould identify critical system boundaries that reuqire a formal interface control document M inimum Criteria:  SV - Ground and Payload to SV ICD initial drafts must be compl e t e Other pertinent ICDs:  LV - Spacecraft, Component Interface ICD Final Delivery Interface Control Documentation SV 1  SV 6 Schedule Program Driving Schedule Requirements Key schedule driven technical decisions Giver receiver relationships that span different program elements CDR De l i v e r y Intgrated Milestone Schedule PV2 Integration Prodcution Plan List of all components under procurement and their expected and need dates List should include all piece parts, miscellaneous mat ls, connectors and required ground support equipment Initial Delivery Sy st e m Pa r t s  Li st  FFP6 Technical Performance Measures Demonstrate design peformance to critical program requirements outlined within the requirements document Final Delivery Technical performance budget SV 7 Values in the budget should be compared to industry standards for a given maturity in the devleopment Open System Trades Desription organized by susbsystem of open design trades and decsions that need to be completed Each trade should have an owner and assocaited due dates that are aligned with program constriants Li v i n g Document Trade  Decision tracking matrix FFP5   


 LNCRITIC 0.884 \(.005 0.362 352 0.593 053  CRPRO -0.007 \(.306 0.012 183 0.002 798  CRCON 0.010 \(.291 0.013 230 0.019 095  Model fit F p value 24.900 lt;.0001 11.110 lt;.0001 5.940 lt;.0001 Adjusted R2 0.559 0.553 0.320 p &lt; .10 p &lt; .05 Notes: p values are in parentheses  4.5. South Korean versus American market  In terms of the effect of WOM, we find no discernable difference in the motion picture markets of South Korea and the United States. Volume of WOM is positively correlated to the following week?s revenue in both markets, and valence of WOM is not significant The effect of critical reviews, however, did not concur While the literature on the American market data reports that positive critical reviews are positively related to box office revenue[21, 34], the results on the Korean market was different. There could be several reasons for this. First, South Korea and the United States have different sources for critical reviews, and the sources may have different impacts on moviegoers Second, the characteristics of critics might be different i.e., Korean critics may prefer movies that are considered less commercial or artistic than American critics  5. Conclusion  WOM and critical reviews both are important attributes that influence box office revenue in the motion picture industry. In this study, six hypotheses related to this issue were set up and tested. Data was collected on the motion picture industry of South Korea by using several websites that provide content and statistical data about movies. Finally, data on 118 movies was collected and the movies were categorized into two groups based on the distributors of the movies If the distributor of a movie was one of the major distributors in South Korea, that movie was categorized into mainstream movies, and if not then the movie was categorized into non-mainstream movies. As expected mainstream movies had much higher box office revenue and volume of WOM than non-mainstream Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 movies. In the case of the volume of critical reviews 


movies. In the case of the volume of critical reviews however, there was no big difference between mainstream and non-mainstream movies. WOM and critical reviews were usually positive H1 and H2 tested the relationship between WOM and weekly box office revenue, and the results supported the hypotheses. The volume of WOM was positively related to weekly box office revenue, while the valence of WOM had no significant effect. H3 H4a, and H4b tested the impact of critical reviews, and the results also supported the hypotheses except H4b The volume and valence of critical reviews had no consistent significances to weekly box office revenue H3 H4b. Table 7 showed that the number of critical reviews was statistically significant to aggregate box office revenue \(H4a for the attitude of critical reviews \(H4b the result more detail, an additional test was performed using only those factors related to critical reviews as independent variables. The result of the additional test supported H4, but the signs were reversed, i.e. positive critical reviews had minus signs, and negative critical reviews had plus signs. This reversed signs imply that the preference of critical reviewers is very similar to that of normal moviegoers. H5s and H6s tested the different effects of WOM and critical reviews on mainstream and non-mainstream movies. The result failed to determine that WOM give different impact on mainstream and non-mainstream movies, so H5a and H5b were rejected. H6, however, was supported, i.e the effects of critical reviews were different for mainstream and non-mainstream movies. There were no significant relationships between critical reviews and aggregate box office revenue in mainstream movies. For non-mainstream movies, however, the volume of critical reviews and the percentage of negative critical reviews were significant. Nonmainstream movies have fewer sources from which consumers can get information, and this might explain the results The above findings lead to several managerial implications. First, producers and distributors of movies could forecast weekly box office revenue by looking at previous weeks? volume of WOM. It does not matter what attitude people have when they spread WOM, the important factor is its volume. Therefore producers and distributors need to develop an appropriate strategy to manage WOM for their movies For example, the terms related to WOM marketing such as buzz and viral marketing are easily found Second, for the distributors who usually distribute less commercial and more artistic movies, and consequently have a smaller market compared to the major distributors, critical reviews can impact their movies box office revenues in a significant way. There are usually fewer sources for information for nonmainstream movies than mainstream movies, and so small efforts could leverage the outcomes. Finally, for those who are dealing with mainstream movies, the finding that the valence of WOM and critical reviews do not have significant relationship with box office revenue can have certain implications. Particularly, the attitude of critical reviews showed reversed effects Therefore, they may need to concentrate on other features rather than attitude of moviegoers or critical reviews, such as encouraging moviegoers to spread WOM This study contributes to the understanding of the motion picture industry, especially the relationship between box office revenue and WOM including critical reviews. There are existing studies that already 


critical reviews. There are existing studies that already dealt with similar issues, but this study has some differentiated features compare to prior studies. First the data used in this study was collected from South Korea, while most of the relevant studies usually focus on the North American market. This helps to provide the opportunity to understand the international market especially the Asian market, even though South Korea is a small part of it in terms of the motion picture industry. Second, movies were categorized to two groups, i.e. mainstream and non-mainstream and this study attempted to determine how WOM impacts these categories differently by testing several hypotheses In this study, there are also several limitations that could be dealt with in future research. First, using box office revenue as a dependent variable is more meaningful for distributers rather than producers. Due to there is close correlation between box office revenue and number of screens, one of producers? main concerns is how many screens their movies can be played on. Moreover, DVD sales are also important measurement for success of movies these days, and so it also could be a dependent variable. Therefore, it could be possible to give more fruitful managerial implications to various players in the motion picture industry by taking some other dependent variables Second, in this study, movies were categorized simply as mainstream and non-mainstream movies, but there could be further studies with diverse techniques of movie categorizations. For example, it would be possible to study the varying influence of WOM or critical reviews on different genres or movie budgets Third, an interesting finding of this study is that positive critical reviews could have negative relationship with box office revenue while negative critical reviews could have positive relationship. This study tried to provide a reasonable discussion on the issue, but more studies could be elaborate on it  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 References  1] Dellarocas, C., The Digitization of Word of Mouth Promise and Challenges of Online Feedback Mechanisms Management Science, 2003. 49\(10 2] Bone, P.F., Word-of-mouth effects on short-term and long-term product judgments. Journal of Business Research 1995. 32\(3 3] Swanson, S.R. and S.W. Kelley, Service recovery attributions and word-of-mouth intentions. European Journal of Marketing, 2001. 35\(1 4] Hennig-Thurau, F., et al., Electronic word-of-mouth via consumer-opinion platforms: What motivates consumers to articulate themselves on the internet? Journal of Interactive Marketing, 2004. 18\(1 5] Fong, J. and S. Burton, Electronic Word-of-Mouth: A Comparison of Stated and Revealed Behavior on Electronic Discussion Boards. Journal of Interactive Advertising, 2006 6\(2 6] Gruen, T.W., T. Osmonbekov, and A.J. Czaplewski eWOM: The impact of customer-to-customer online knowhow exchange on customer value and loyalty. Journal of Business Research, 2006. 59\(4 7] Garbarino, E. and M. Strahilevitz, Gender differences in the perceived risk of buying online and the effects of receiving a site recommendation. Journal of Business Research, 2004. 57\(7 8] Ward, J.C. and A.L. Ostrom, The Internet as information minefield: An analysis of the source and content of brand information yielded by net searches. Journal of Business Research, 2003. 56\(11 


9] Goldsmith, R.E. and D. Horowitz, Measuring Motivations for Online Opinion Seeking. Journal of Interactive Advertising, 2006. 6\(2 10] Eliashberg, J., A. Elberse, and M. Leenders, The motion picture industry: critical issues in practice, current research amp; new research directions. HBS Working Paper, 2005 11] S&amp;P, Industry surveys: Movies and home entertainment 2004 12] KNSO, Revenue of Motion Picture Industry 2004 Ministry of Culture, Sports, and Tourism, 2004 13] Duan, W., B. Gu, and A.B. Whinston, Do Online Reviews Matter? - An Empirical Investigation of Panel Data 2005, UT Austin 14] Zhang, X., C. Dellarocas, and N.F. Awad, Estimating word-of-mouth for movies: The impact of online movie reviews on box office performance, in Workshop on Information Systems and Economics \(WISE Park, MD 15] Mahajan, V., E. Muller, and R.A. Kerin, Introduction Strategy For New Products With Positive And Negative Word-Of-Mouth. Management Science, 1984. 30\(12 1389-1404 16] Moul, C.C., Measuring Word of Mouth's Impact on Theatrical Movie Admissions. Journal of Economics &amp Management Strategy, 2007. 16\(4 17] Liu, Y., Word of Mouth for Movies: Its Dynamics and Impact on Box Office Revenue. Journal of Marketing, 2006 70\(3 18] Austin, B.A., Immediate Seating: A Look at Movie Audiences. 1989, Wadsworth Publishing Company 19] Bayus, B.L., Word of Mouth: The Indirect Effects of Marketing Efforts. Journal of Advertising Research, 1985 25\(3 20] Faber, R.J., Effect of Media Advertising and Other Sources on Movie Selection. Journalism Quarterly, 1984 61\(2 21] Eliashberg, J. and S.M. Shugan, Film critics: Influencers or predictors? Journal of Marketing, 1997. 61\(2 22] Reinstein, D.A. and C.M. Snyder, The Influence Of Expert Reviews On Consumer Demand For Experience Goods: A Case Study Of Movie Critics. Journal of Industrial Economics, 2005. 53\(1 23] Gemser, G., M. Van Oostrum, and M. Leenders, The impact of film reviews on the box office performance of art house versus mainstream motion pictures. Journal of Cultural Economics, 2007. 31\(1 24] Wijnberg, N.M. and G. Gemser, Adding Value to Innovation: Impressionism and the Transformation of the Selection System in Visual Arts. Organization Science, 2000 11\(3 25] De Vany, A. and W.D. Walls, Bose-Einstein Dynamics and Adaptive Contracting in the Motion Picture Industry Economic Journal, 1996. 106\(439 26] Bagella, M. and L. Becchetti, The Determinants of Motion Picture Box Office Performance: Evidence from Movies Produced in Italy. Journal of Cultural Economics 1999. 23\(4 27] Basuroy, S., K.K. Desai, and D. Talukdar, An Empirical Investigation of Signaling in the Motion Picture Industry Journal of Marketing Research \(JMR 2 295 28] Neelamegham, R. and D. Jain, Consumer Choice Process for Experience Goods: An Econometric Model and Analysis. Journal of Marketing Research \(JMR 3 p. 373-386 29] Lovell, G., Movies and manipulation: How studios punish critics. Columbia Journalism Review, 1997. 35\(5 30] Thompson, K., Film Art: An Introduction. 2001 McGraw Hill, New York 31] Zuckerman, E.W. and T.Y. Kim, The critical trade-off identity assignment and box-office success in the feature film industry. Industrial and Corporate Change, 2003. 12\(1 


industry. Industrial and Corporate Change, 2003. 12\(1 27-67 32] KOFIC, Annual Report of Film Industry in Korea 2006 Korean Film Council, 2006 33] Sutton, S., Predicting and Explaining Intentions and Behavior: How Well Are We Doing? Journal of Applied Social Psychology, 1998. 28\(15 34] Basuroy, S., S. Chatterjee, and S.A. Ravid, How Critical Are Critical Reviews? The Box Office Effects of Film Critics Star Power, and Budgets. Journal of Marketing, 2003. 67\(4 p. 103-117  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 





