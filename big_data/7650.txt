Mining for Strong Negativ e Asso ciations in a Large Database of Customer T ransactions Ashok Sa v asere 003 Edw ard Omiecinski Shamk an tNa v athe College of Computing Georgia Institute of T ec hnology A tlan ta GA 30332 f ashok,e dwar do,sh am g c c.ga te c h.e du Abstract Mining for asso ciation rules is c onsider e d an imp ortant data mining pr oblem Many di\013er ent variations of this pr oblem have b e en describ e d in the liter atur e In this p ap er we intr o duc e the pr oblem of mining for ne gative asso ciations A naive appr o ach to 014nding ne gative asso ciations le ads to a very lar ge numb er of rules with low inter est me asur es We addr ess this pr oblem by c ombining pr eviously disc over e dp ositive asso ciations with domain know le dge to c onstr ain the se ar ch sp ac e such that fewer but mor e inter esting ne gative rules ar e mine d We describ e an algorithm that e\016ciently 014nds al l such ne gative asso ciations and pr esent the exp erimental r esults 1 In tro duction Wide spread use of computers in business op erations and the a v ailabili t yof c heap storage devices ha v e led to an explosiv e gro wth in the amoun t of data gathered and stored b y most business organizations to da y  There has b een a trend in recen ty ears to searc h for in teresting patterns in the data and use them for impro v ed decision making e.g see 3  Data mining is de\014ned as the pro cess of 014nding hidden non trivial and previously unkno wn information from a large collection of data 10  It has b een recognized as one of the promising areas of researc h encompassing databases statistics and mac hine learning 6 12 15  Recen tly  there has b een considerable in terest in 014nding asso ciations b et w een items in a database of customer transactions suc h as the sales data collected at sup er mark et c hec k out coun ters 1 2,5,7,11,8 14 Asso ciation rules iden tify items that are most often b ough t along with certain other items b y a signi\014can t fraction of the customers F or example w e ma y 014nd that 95  of the customers who b ough t bread also b ough t milk Ev ery rule m ust satisfy t w o user sp eci\014ed constrain ts one is a measure of statistical signi\014cance called supp ort and the other a measure of go o dness of the rule called c on\014denc e  The supp ort constrain t ensures that the rule o ccurs relativ ely often to b e considered useful The con\014dence measures ho ww ell the rule predicts the asso ciation b et w een the items The supp ort of a rule X   Y is de\014ned as the fraction of transactions that con tain X  Y  where X and Y are sets of items The con\014dence is de\014ned as 003 Curren t address Data Mining Group T andem computers Inc 14231 T andem Blvd Austin TX 78728 the ratio supp ort  X  Y  supp ort  X  The goal is to 014nd all rules that satisfy minim um supp ort and minim um con\014dence These t yp es of rules sp ecify the lik eliho o d of 014nding Y in a customer bask et giv en that it con tains X  Applications of suc h rules include crossmark eting attac hed mailing catalog design add-on sales store la y out etc 3  In this pap er w e consider the complemen tary problem i.e what items a customer is not lik ely to buy giv en that he buys a certain set of items W e call these t yp es of rules as the ne gative asso ciation rules  An example of suc h a rule is 60  of the customers who buy p otato c hips do not buy b ottled w ater Suc h negativ e asso ciation rules can pro vide v aluable information ab out customer buying patterns and help managers in devising b etter mark eting strategies In this pap er w e consider negativ e asso ciations in the con text of retail sales data Ho w ev er the solutions dev elop ed here can b e applied to other domains T o the b est of our kno wledge w e are una w are of an yw ork that addresses the issue of mining negativ e rules from data In this pap er w e discuss the di\016culties of mining negativ e rules and describ e an algorithm whic h e\016cien tly mines negativ e rules in large datasets Finding negativ e asso ciations is not straigh t forw ard due to the follo wing reason in a large retail store there ma y b e tens of thousands of items Ev en if billions of customer transactions are considered man y of the item com binations ma y not app ear ev en once F or example if there are 50,000 items the p ossible com binations of items is 2 50  000 a ma jorit y of whic h will not app ear ev en once in the en tire database If the absence of a certain item com bination is tak en to mean negativ e asso ciation then w e can generate millions of negativ e asso ciation rules Ho w ev er most of these rules are lik ely to b e extremely unin teresting The problem is therefore one of 014nding only inter esting negativ e rules W e call suc h rules str ong negativ e rules In the follo wing section w e de\014ne the problem more precisely and then discuss the notion of in terestingness and the t yp e of negativ e rules that fall within this de\014nition 1.1 Measure of In terestingness The ob jectiv e measure of in terestingness of a rule is de\014ned in terms of the unexp ectedness  of the rule 1 1 This is only one measure of in terestingne ss There ma ybe other factors whic h mak e a particular rule in tersting 


Simply stated a rule is in teresting if it con tradicts or deviates signi\014can tly from our exp ectation based on previous b elief The previous b elief is usually stated in terms of the a priori probabilities based on our kno wledge of the problem domain F or example in the retail mark eting con text supp ose there are 50,000 distinct items and there are 10 million transactions eac h con taining 5 items on an a v erage Without an y prior kno wledge w ew ould exp ect all items are equally lik ely to b e b ough t Then the n um b er of transactions con taining a sp eci\014c item is 1000 No who w ev er after scanning the customer transactions if w e 014nd that 500,000 transactions con tain that particular item w e sa y that w eha v e disco v ered an in teresting fact b ecause it signi\014can tly deviates from our earlier exp ectation In information theoretic terms the a priori probabilities represen t our state of ignor anc e and the deviation of the a p osteriori probabilities represen t the degree of information gained In the case of negativ e rules w e are in terested in 014nding itemsets that ha v eav ery lo w probabilit yof b eing b ough t with certain other items That is w e are in terested in cases where t w o sp eci\014c sets of items app ear v ery rarely in the same transaction Ho w ev er this p oses t w o problems as follo ws 1 In the absence of an y kno wledge ab out customer buying preferences w e exp ect the items to b e b ough t indep enden tly of eac h other In the ab o v e example the exp ected supp ort for a sp eci\014c pair of items is 1000/10 millio n 002 1000/10 millio n 1/100,000,000 Ev en if the actual supp ort turns out to b e zero the deviation from exp ectation is extremely small Consequen tly  a rule whic h states that these t w o items are negativ ely asso ciated do es not carry m uc h information and hence unin teresting 2 If w e are lo oking for item com binations whic h ha v ev ery lo w supp ort there will b e a v ery large n um berofcom binations ha ving v ery lo worev en zero supp ort In the ab o v e example ev en if w e tak e only pairs of items there are appro ximately 2.5 billion com binations Since w eha v e only 10 millio n transactions most of the com binations will not app ear ev en once Therefore w ema y generate billions of negativ e rules most of whic h will b e useless The problem gets ev en w orse if w e include com binations of larger sizes T o summarize mining for negativ e rules is imp ossible with a naiv e approac h due to the follo wing reasons a W ema y 014nd billions of negativ e asso ciations in a large dataset and b Almost all of these negativ e asso ciations will b e extremely unin teresting Ev en though the preceding argumen t sho ws that negativ e asso ciations ma y b e inheren tly unin teresting there are man y situations where this is not true W e motiv ate this with the follo wing examples Example 1 Supp ose w e consider the sales of a particular brand of c hips sa y Ru\017es and t w o brands of soft drinks sa y Cok e and P epsi 2  After scanning through a large n um b er of transactions supp ose w e 014nd that when customers buy Ru\017es they also usually buy Cok e but not P epsi W e can then conclude that Ru\017es has an in teresting negativ e asso ciation with P epsi The reason this is in teresting is that b y considering only the asso ciations b et w een Ru\017es and Cok e w e exp e cte d the asso ciation b et w een Ru\017es and P epsi also to b e high In other w ords w e reform ulated our b elief from one of indep endence to a p ositiv e asso ciation based on a our kno wledge that Cok e and P epsi fall under the same category and therefore they can b e exp ected to ha v e similar t yp es of asso ciations with other items and b the p ositiv e asso ciation b et w een Ru\017es and Cok e Example 2 After scanning the data supp ose w e 014nd a strong p ositiv e asso ciation b et w een the frozen y ogurt category and the b ottled w ater category  Supp ose eac h of these categories con tains t w o individual brands sa y brands Bry ers and Health y Choice of frozen y ogurt and brands Evian and P errier of b ottled w ater F or simplicit y assume that the sales of eac h individual brand accoun ts for 50 of the sales of its category  Based on the asso ciation b et w een the t w o categories w e can reasonably exp ect eac h of the four com binations of frozen y ogurt and b ottled w ater brands to accoun t for 25 of the supp ort for f frozen y ogurt b ottled w ater g Ho w ev er if w e actually 014nd the supp ort for f Bry ers P errier g accoun ts for only 2 w e can conclude that Bry ers and P errier are negativ ely asso ciated Example 3 In the ab o v e example w e can also exp ect the supp ort for f F rozen y ogurt P errier g to b e 50 of the supp ort for f frozen y ogurt b ottled w ater g Ho wev er supp ose w e actually 014nd this supp ort to b e 10 w e can deduce that most customers m ust b e buying Evian when they buy either brands of frozen y ogurt whic h indicates an in teresting negativ e asso ciation b et w een the frozen y ogurt category and P errier The preceding examples sho ws that w e can indeed 014nd certain in teresting negativ e asso ciations In eac h of these examples w e based our conclusion on the information already presen t in the data and additional domain kno wledge whic h groups similar items together W e use these ideas to mine for a class of negativ e rules whic h are in teresting and lik ely to b e useful The basic idea b ehind our approac h is to lo ok at only those cases where w e exp ect a high degree of p ositiv e asso ciation If the actual supp ort is found to b e signi\014can tly smaller then w e can conclude that they ha v e a negativ e asso ciation W e mak e the follo wing observ ations with resp ect to this approac h 1 Domain kno wledge The t yp e of domain kno wledge w e dep end on to mine the negativ e rules is a grouping of similar items This information is most naturally con v ey ed b y a taxonom yon the items In this pap er w e assume suc h a tax2 The actual brand names used in this pap er are for the purp ose of illustration only  


onom yis a v ailable In most retail organizations items are already classi\014ed under departmen ts categories sub-categories etc 2 Uniformit y assumption One of the fundamen tal assumptions in our approac h is that the items that b elong to the same paren t in a taxonom y are exp ected to ha v e similar t yp es of associations with other items This is similar to the principle of uniformit y of nature found in inductiv e reasoning 3 Class of negativ e rules Since w e consider only those cases where an exp ected supp ort can b e computed based on the taxonom y  the class of negativ e rules w e can generate is not all p ossible negativ e rules but a subset of those rules If additional domain kno wledge is incorp orated it ma y b e p ossible to mine additional t yp es of negativ e rules It should b e noted that our approac h solv es the t w o problems describ ed earlier That is w e generate few er negativ e rules and all those rules are lik ely to b e in teresting 1.2 Previous W ork The problem of mining for asso ciation rules w as intro duced in 1  Since then it has b een recognized as one of the imp ortan tt yp es of data mining problems and man y algorithms ha v e b een prop osed to impro v e its p erformance for e.g 2 7 8 Related w ork also includes 9  In 14  the problem w as extended to include taxonomies on the items and mining rules bet w een di\013eren t lev els in the taxonom y  This enables more expressiv et yp es of rules to b e mined b y allo wing a limited form of disjunction in the asso ciation rules e.g o v ercoats OR jac k ets AND sho es   p erfume Ho w ev er w e are una w are of an yw ork that tries to 014nd negativ e asso ciations in data The closest w ork is the tec hnique to prune unin teresting rules prop osed in 14  2 Problem Statemen t F ormally  the problem can b e stated as follo ws Let I  f i 1 i 2 i m g b e a set of m distinct literals called items 3  Let T b e a taxonom yon I  Let L\022 I b e the set of leaf items in T and let C\032I b e the set of in ternal no des called categories D is a set of v ariable length transactions o v er L  Eac h transaction T c ontains a set of items i i i j i k 032L  A transaction also has an asso ciated unique iden ti\014er called TID In general a set of items is called itemsets  The n um ber of items in an itemset is called the length of an itemset Itemsets of some length k are referred to as k itemsets F or an itemset X 001 Y if Y is an m itemset then Y is called an m extension of X  An itemset X 032I  has supp ort X   s  if the fraction of transactions in D con taining X equals s A ne gative asso ciation rule is an implicatio n of the form X 6   Y  where X Y 032I  and X  Y    X is called the an teceden t and Y is 3 This description and the terminology used are largely based on 1  and 2  called the consequen t of the rule Ev ery rule also has a rule in terest measure W e de\014ne the in terest measure RI of a negativ e asso ciation rule X 6   Y  as follo ws RI  E  suppor t  X  Y  000 suppor t  X  Y  suppor t  X  Where E supp ort X  is the exp e cte d supp ort of an itemset X W e describ e ho w exp ected supp ort is computed in the follo wing section Note that the rule interest RI is negativ ely related to the actual supp ort of the itemset X  Y  It is highest if the actual supp ort is zero and zero if the actual supp ort is same as the exp ected supp ort re\015ecting our earlier de\014nition of in terestingness The problem of 014nding negativ e rules can b e no w b e stated as follo ws giv en a database of customer transactions D and a taxonom y T on the set of items 014nd all rules X 6   Y suc h that a supp ort X  and supp ort Y  are greater than minim um supp ort MinSup  and b the rule in terest measure is greater than MinRI where MinSup and MinRI are sp eci\014ed b y the user Condition a is necessary to ensure that the generated rule is statistically signi\014can t F or example ev en if the rule p erfectly predicts 10 out of 10 millio n cases it is not v ery useful b ecause it is not general enough The problem can b e decomp osed in to t w o subtasks a 014nding itemsets whose actual supp ort deviates at least MinSup 002 MinRI from their exp ected supp ort W e call suc h itemsets as ne gative itemsets and b generating the negativ e rules after the negativ e itemsets are found The 014rst condition eliminates testing for rules whic h will ha v e an RI less than MinRI W e consider eac h of these tasks in the follo wing sections 2.1 Finding Negativ e Itemsets Finding negativ e itemsets in v olv es follo wing steps 1 W e 014rst 014nd all the generalized large itemsets in the data i.e itemsets at all lev els in the taxonom y whose supp ort is greater than the user sp eci\014ed minim um supp ort 2 W e next iden tify the candidate negativ e itemsets based on the large itemsets and the taxonom y and assign them exp ected supp ort 3 In the last step w e coun t the actual supp ort for the candidate itemsets and retain only the negativ e itemsets The 014rst step of disco v ering generalized large itemsets has b een discussed in 14 4  The second step is the most imp ortan t step and is discussed in the next section The last step is relativ ely straigh t forw ard as an y data structures to coun t supp ort can b e used 2.1.1 Generating Candidate Negativ e Itemsets The candidate negativ e itemsets are generated based on the previously determined large itemsets F or eac h 


  A D                  E F G                       H J                     K B                     C Figure 1 T axonom y large itemset l k w e generate candidate itemsets comp osed of the immediate descenden ts and siblings of the items in l k  The candidate itemsets will also b e of size k  In general an y itemset whose exp ected supp ort can b e computed based on the supp ort of the large itemsets and whose computed exp ected supp ort is greater than MinSup 002 MinRI is a candidate itemset W e can iden tify the follo wing cases for generating candidate itemsets All the examples refer to the taxonom y sho wn in Figure 1 where f CG g has b een found to b e large W e sho w only pairs of items in these examples Ho w ev er they can b e extended to an yn um ber of items It should b e noted that all the 1-itemsets in a candidate m ust ha v e minim um supp ort Otherwise no rule will b e pro duced for this itemset b ecause b oth an teceden t and the consequen t of a rule m ust ha v e minim um supp ort Therefore if all 1-itemsets of a candidate are not large that candidate is imm ediately rejected Case 1 Here the candidates are formed from the immediate c hildren of large itemsets F or example the candidates in this case will b e f DJ g  f DK g    f EJ g  f EK g    etc The exp ected supp orts are computed from the supp ort of their paren ts F or example the exp ected supp ort of f DJ g is computed as E  sup  DJ   sup  CG  002 sup  D  002 sup  J  sup  C  002 sup  G  In general if f  p  q  t g is a large itemset E  sup  p q t   sup  p   q  001\001\001   t  002 sup  p  002 sup  q  002\001\001\001\002 sup  t  sup  p  002 sup  q  002 001\001\001 002 sup   t  where  p  q  t are paren ts of p;q;:::;t  resp ectiv ely  The candidate itemsets are formed b y taking ev ery p ossible com bination of the c hildren of the items in the large itemset Case 2 The candidate itemsets are f CJ g  f CK g    f GD g  f GE g    etc The exp ected supp ort for sa y  f CJ g is computed as E  sup  CJ   sup  CG  002 sup  J  sup  G  In general E  sup  p q  r  t   sup  p  q   r 001\001\001  t  002 sup  r  002 001\001\001 002 sup  t  sup  r  002 001\001\001 002 sup   t  where  p  q  t are paren ts of p;q;:::;t  resp ectiv ely  The candidate itemsets are generated as all com binations of the c hildren of the items in the large itemset Case 3 In this case the candidates are formed from the siblings of the items in the large itemset F or the ab o v e example the candidates are f CH g  f CI g  and f BG g  The exp ected supp ort for f CH g is giv en b y E  sup  CH   sup  CG  002 sup  H  sup  G  In general E  sup  p q  r  t   sup  p  q  r 0 001\001\001 t 0  002 sup  r  002 001\001\001 002 sup  t  sup  r 0  002 001\001\001 002 sup  t 0  where p 0 q 0 r 0 are siblings of p;q;:::;t  resp ectiv ely  It is p ossible that same candidates generated from di\013eren t large itemsets F or example w ema y generate the candidate f CH g from the large itemset f CG g according to case 3 No w if the itemset f AF g is also large then the candidate f CH g will b e generated according to case 1 This ma y result in di\013eren tv alues of exp ected supp ort for the same candidate dep ending on ho w it is generated In suc h situations the largest v alue of the exp ected supp ort is c hosen The main idea b ehind generating candidate negativ e itemsets is that the items close together in the taxonom y will ha v e similar asso ciations with other items Ho w ev er the candidates are generated only if w e can assign an exp ected v alue of supp ort based on the supp orts of other large itemsets Therefore ev en though w e can iden tify man y more cases where itemsets can b e formed from the neigh b ors of large itemsets they are not considered as candidates F or example the follo wing itemset com binations are not considered as candidates 1 itemsets con taining only the siblings of the items in the large itemset 2 itemsets con taining the immediate ancestors and immedia te c hildren of the items in the large itemset 3 itemsets con taining the immediate ancestors and siblings of the items in the large itemset 4 itemsets con taining imm ediate descenden ts and siblings of the items in the large itemset 


Items Supp ort Bry ers 20,000 Health y Choice 10,000 Evian 10,000 P errier 5,000 F rozen y ogurt 30,000 Bottled w ater 20,000 F rozen Y ogurt and Bottled w ater 15,000 T able 1 Supp orts for the example Items Exp e cte d A ctual Supp ort Supp ort Bry ers and Evian 6,000 7,500 Bry ers and P errier 4,000 800 Health y Choice and Evian 3,000 4,200 Health y Choice and P errier 2,000 2,500 T able 2 Exp ected and actual supp orts Example W e illustrate the candidate and negativ e itemset generation using the an example Figure 2 sho ws the taxonom y and T able 1 sho ws the supp orts for the individual brands w eha v e sho wn absolute v alues of supp ort for simplicit y The minim um supp ort is sp eci\014ed as 4,000   Beverages Corbonated NonCorbonated Bottled juices Desserts Frozen yogurt Avian Bryers Healthy Choice Bottled water Ice creams Perrier Figure 2 T axonom y for the example The negativ e candidates in this case are f Bry ers Evian g  f Bry ers P errier g  f Health y Choice Evian g and f Health y Choice P errier g  Their exp ected supp orts are sho wn in T able 2 Among these f Bry ers Evian g and f Health y Choice Evian g will already b e found to b e large Hence they are not considered negativ e candidates If the actual supp orts are as sho wn in T able 2 and the minim um RI is sp eci\014ed as 0.5 then the only negativ e asso ciation rule will b e P errier 6   Bry ers 2.1.2 Num b er of Candidates The actual n um b er of candidates generated dep ends on the c haracteristics of the data and the taxonom y b eing used Ho w ev er w e can estimate the n um ber of candidates generated as k X 1=1 022 k i 023 f i  k  f 000 1 where f is the a v erage fan-out in the taxonom y and k is the itemset size As can b e seen the n um ber of candidates is exp onen tial in the size of the candidates b eing considered Ho w ev er as the size is increased the n um b er of large itemsets rapidly decreases A large n um b er of candidates generated can also b e pruned b y applying optimizations F or example since all 1-itemsets con tained in a candidate m ust b e large the candidates are generated b y discarding all small 1-itemsets in the taxonom y  This has the e\013ect of reducing the fanout and hence the candidates generated Other optimizations will b e discussed in Section 2.2 2.1.3 E\013ect of the T axonom y The taxonom y is considered as the domain kno wledge whic h is used to induce negativ e rules Therefore the rules generated will b e a\013ected b y the qualit yofthe taxonom y  One of the implicit assumptions in our approac h is that the items b elonging to the same category are substitute items i.e customers ma y substitute one item in place of the other within that category  T axonomies on a giv en set of items can b e generated based on an y arbitrary criterion Ho w ev er our approac hw orks b est if the taxonom y is generated based on the similarit y of usage of the items This also ensures that the items group ed under the same category are substitutes Most real life taxonomies are in general of this nature Therefore the approac hw orks satisfactorily in most situations Another issue that needs to b e considered is the lev el of detail in the taxonom y  A taxonom y with 014ner gran ularit y  i.e one in whic h categories are classi\014ed in to smaller and smaller sub-categories leads to the generation of b etter rules compared to a shallo w taxonom y where h undreds or thousands of items are group ed under the same category  This is to b e in tuitiv ely exp ected b ecause the information con ten tofa 014ne gran ular taxonom y is more than a taxonom yof coarser gran ularit y  leading to b etter domain kno wledge and hence b etter qualit y of rules This can b e explained in more practical terms as follo ws w e generate the negativ e candidate itemsets and assign them exp ected supp orts based on the supp orts of either their paren ts or siblings In a taxonom y with 014ner gran ularit y  eac h category will ha v e few er c hildren and also few er siblings compared to a taxonom y with coarser gran ularit y  As the n um ber of c hildren or siblings in a category increases the relativ e supp ort of an individual c hild or sibling decreases F or example in a category with t w oc hildren the relativ e supp ort of eac hc hild ma y b e 50 on a v erage Ho w ev er if there are 100 c hildren the relativ e supp ort will drop to 1 Therefore the exp ected supp orts whic h are computed from the relativ e supp orts of the items in a candidate will ha v e relativ ely larger error terms in a taxonom y with coarser gran ularit y leading to less accurate rules 


The second reason wh y 014ne gran ularit y taxonomies are preferred is that the n um b er of candidates generated increases rapidly with the increase in the a v erage fanout Fine gran ularit y taxonomies alleviate this problem 2.2 Algorithm W e assume that the transactions are in the form h TID i j i k i n i and the complete taxonom yis a v ailable Generating negativ e asso ciation rules inv olv es 014nding all the negativ e large itemsets and generating the negativ e rules W e 014rst consider the problem of 014nding negativ e large itemsets As explained in section 2.1 generating negativ e large itemsets inv olv es 014nding generalized large itemsets generating negativ e candidates coun ting supp ort for the candidates and generating the negativ e large itemsets T o 014nd all large itemsets w e can use one the algorithms Basic  Cumulate or EstMer ge  prop osed in 14  T o generate the negativ e large itemsets w e describ e t w o algorithms b elo w Both the algorithms use similar approac hes The 014rst algorithm is a straigh t forw ard implemen tation and the second algorithm incorp orates some optimizations to impro v e the p erformance Both the algorithms require m ultiple iterations 2.2.1 Naiv e Algorithm Eac h iteration of this algorithm consists of t w o phases In the 014rst phase of iteration k w e compute the generalized large itemsets of size k using one of Basic  Cumulate or EstMer ge  In the second phase 014rst the negativ e candidate itemsets of size k are generated as describ e in section 2.1.1 Next supp ort for the candidates is coun ted b y making a pass o v er the data Therefore this algorithm requires t w o passes o v er the data during eac h iteration for a total of 2 002 n passes where n is the total n um b er of iterations The impro v ed algorithm reduces the n um b er of passes o v er the data as describ ed b elo w 2.2.2 An Impro v ed Algorithm This algorithm incorp orates t w o optimizations o v er the naiv e algorithm 014rst all small 1-itemsets are deleted from the taxonom y second the instead of generating the negativ e itemsets during eac h iteration they are generated in a single step after generating the large itemsets of all sizes The 014rst optimization reduces the n um b er of negativ e candidates generated The second optimization reduces the n um b er of passes o v er the data from 2 002 n to n  1 The algorithm is sho wn in Figure 3 2.3 Generating Rules Once the negativ e itemsets are generated 014nding all negativ e rules is straigh t forw ard F or a negativ e itemset n w e output the rule a 6    n 000 a  where a  and  n 000 a  are nonempt y subsets of n ha ving minim um supp ort if its rule in terest measure is greater than sp eci\014ed minim um  MinRI Our algorithm for generating negativ e rules is an extension of the apgenrules algorithm describ ed in 2  The extensions handle the requiremen t that the an teceden t and the L 1  f large 1{itemsets g  k 2  k represen ts the pass n um ber  First generate all large itemsets while  L k 000 1 6    b egin  Generate new candidates of size k using Basic  Cum ulate or EstMerge C k  GenCands  L k 000 1  forall transactions t 2D b egin C t  subset C k  t  forall candidates c 2 C t c coun t end L k  f c 2 C k j c coun t 025 MinSup g k  k 1 end  No w generate negativ e itemsets Delete all small 1-itemsets from the taxonom y k 2 while  L k 6    b egin  Generate negativ e candidates of size k as  describ ed in section 2.1.1 NC k  GenNegCands  L k  NC  NC  NC k  k  k 1 end forall transactions t 2D b egin NC t  subset NC k  t  forall candidates c 2 NC t c coun t end N k  f c 2 NC k j c coun t  MinSup 002 MinR I g Figure 3 An impro v ed algorithm consequen ts of the rule m ust b e large Note that if a turns out to b e small none of the extensions need to b e generated b ecause they will not ha v e the minim um supp ort Similarl y if a rule a 6    n 000 a  do es not ha v e minim um RI then none of the subsets of a need to b e considered b ecause none of those rules will ha v e minim um RI either The rule generation algorithm is sho wn in Figure 4 The apriori-gen function is the join follo w ed b y the prune step describ ed in 2  2.4 Data Structures The algorithm to generate negativ e asso ciation rules is v ery similar to generating generalized association rules with additional steps to generate negativ e itemsets Since ev ery 1-itemset of candidate m ust ha v e minim um supp ort instead of testing ev ery itemset for minim um supp ort while generating candidates it is considerably faster to compress the taxonom yb y 014ltering out all items whic h do not ha v e minim um supp ort Then no candidate in whic h one of the 1-item 


forall negativ e itemsets n k of size k  k 025 2 do H 1  f consequen ts of rules generated from n k with one item in the consequen t g  call genrules  n k  H 1  L 2  L k 000 2  end pro cedure genrules  n k  H m  L m 1  L k 000 m 000 1   n k  negativ e k itemset H m  set of m item consequen ts if  k>m 1 then b egin H m 1  apriori-gen  H m  forall h m 1 2 H m 1 if  h m 1 2 L m 1 then if  n k 000 h m 1  2 L k 000 m 000 1  then RI  E  sup  n k  000 sup  n k  sup  n k 000 h m 1  if  RI 025 MinRI  then output rule  n k 000 h m 1  6   h m 1  else delete h m 1 from H m 1  else delete h m 1 from H m 1  end call genrules  n k  H m 1  end Figure 4 Rule Generation subsets do es not ha v e minim um supp ort will b e generated As men tioned earlier same candidate ma ybe generated in more than one w a y  Therefore all candidates are put in a hash table for fast lo okup Whenev er a candidate is generated 014rst the hash table is c hec k ed If the candidate is not found then the candidate is inserted in the hash table Otherwise the exp ected supp ort for the candidate is set to larger of the t w o During the rule generation pro cess the coun ts of the subsets of the negativ e itemset are required All large itemsets are also placed in a hash table for fast lo okup 2.5 Memory Managemen t Since negativ e candidates of all sizes are generated at the same time and their supp ort is tested in a single pass it is p ossible that the n um b er of candidates generated is to o large to accommo date in a v ailable memory Ho w ev er if suc h a situation arises the negativ e candidate generation pro cess is stopp ed and the supp ort for the candidates already generated is coun ted The generated negativ e itemsets are either written bac k to the disk or if they are su\016cien tly small are k ept in the main memory  The candidate generation pro cess is no w con tin ued This will necessitate more than one pass o v er the database 3 Exp erime n tal Results In this section w e describ e the exp erimen tal results of our tec hnique for generating negativ e asso ciations W e p erformed the exp erimen ts using syn thetic data on Sun SP AR Cstation 5 with 32 MB of main memory  3.1 Syn thetic Data The syn thetic data is generated suc h that it sim ulates customer buying pattern in a retail mark et environmen t W eha v e used the same basic metho d as describ ed in 14  Ho w ev er to sim ulate the buying pattern more accurately w e adapted the hierarc hical mo del of consumer c hoice called the nested logit mo del to generate the data In this mo del consumers 014rst decide on whic h category to buy and then decide whic h particular brand to buy within that category  W e 014rst generate a taxonom yo v er the items F or an yin ternal no de the n um berofc hildren are pic k ed from a P oisson distribution with mean set to F This pro cess is generated starting from the ro ot lev el Then at lev el 2 and so on un til there are no more items W e next generate a set of p oten tially maxim al large itemsets from whic h itemsets are assigned to a transaction T o generate the set of p oten tially maxim al large itemsets w e 014rst generate p oten tially maxim al clusters of categories comprising of items one lev el ab o v e the leaf lev el The clusters sizes are pic k ed from a P oisson distribution with mean equal to a v erage cluster size Next for eac h cluster w e generate a set of p oten tially maxim al itemsets from the c hildren of the items in the cluster The n um b er of suc h itemsets is pic k ed from a P oisson distribution with mean set to a v erage n um b er of itemsets The size of eac h itemset is also pic k ed from a P oisson distribution with mean set to a v erage large itemset size Eac h cluster has an asso ciated w eigh t that determines the probabilit y that this cluster will b e pic k ed The w eigh tispic k ed according to an exp onen tial distribution with mean set to 1 The w eigh ts are normalized suc h that the sum of all w eigh ts equals 1 The itemsets asso ciated with eac h cluster are also giv en w eigh ts whic h determine the probabilit y this itemset will b e pic k ed once that particular cluster is pic k ed The w eigh ts are pic k ed from an exp onen tial distribution with mean set to 1 The w eigh ts are then normalized suc h that the sum of all w eigh ts of all itemsets for a cluster equal 1 The length of a transaction is determined b yP oisson distribution with mean 026 equal to j T j Un til the transaction size is less than the generated length a cluster is pic k ed according to its w eigh t Once the cluster is determined an itemset from that cluster is pic k ed and assigned to the transaction Not all items from the itemset pic k ed are assigned to the transaction Items from the itemset are dropp ed as long as an uniformly generated random n um ber bet w een 0 and 1 is less than a corruption lev el c  The corruption lev el for itemset is determined b y a normal distribution with mean 0.5 and v ariance 0.1 The transactions con tain only leaf items from the taxonom y  The parameters used in the generation of the synthetic data are sho wn in T able 3 3.2 P erformance W e generated t w o sets of data Short and T all based on di\013eren ta v erage fanouts in the taxonom y on the items Both datasets con tain the same n um ber of items leaf items in the taxonom y and the same n um b er of transactions The parameter v alues used to generate the data are sho wn in T able 4 


j D j Num b er of transactions j T j Av erage size of transactions j C j Av erage size of maxim al p oten tially large clusters j I j Av erage size of maxim al p oten tially large itemsets j S j Av erage n um b er of itemsets for eac h cluster j L j Num b er of maxim al p oten tially large clusters N Num b er of items R Num ber of roots F F anout T able 3 P arameters P arameter Short T all j D j 50,000 50,000 j T j 10 10 j C j 5 5 j I j 5 5 j S j 3 3 j L j 2,000 2,000 N 8,000 8,000 R 10 10 F 9 3 T able 4 Data parameters W e ran b oth algorithms on the t w o data sets for v arious v alues of minim um supp ort The minim um RI w as set to 0.5 in all cases The results are sho wn in Figures 5 and 6 The times sho wn include b oth generation of negativ e itemsets and negativ e rules Since our goal w as to study the p erformance of generating negativ e asso ciations w eha v e not included the time tak en to generate the generalized large itemsets The T all dataset whic h has a taxonom y with smaller fanout to ok longer to complete than the Short dataset The reason w as the far larger n um b er of generalized large itemsets that w ere generated for the T all dataset F or example at a supp ort lev el of 1.5  15,476 large itemsets w ere generated for the T all dataset as opp osed to 1,499 for Short If normalized for the n umb er of generalized large itemsets the times for the T all dataset are m uc h smaller than those of Short as p er our exp ectations Next w e compared the e\013ect of the di\013eren t fanouts in the taxonom y  The n um b er of negativ e candidates generated and the n um b er of negativ e rules for eac h data set are sho wn in the Figure 7 T ok eep the results comparable w eha v e normalized these n um b ers with resp ect to the n um b er of large itemsets As can b e seen the exp erimen t con\014rmed that the n um ber of candidates increases with the increase in fanout 4 Conclusion W ein tro duced the problem of mining negativ e asso ciation rules in a large database of retail customer   0 50 100 150 200 250 300 350 400 0.01 0.015 0.025 0.04 0.06 0.1 0.2 Time \(sec Minimum Support Naive  Better  Figure 5 Execution times Short data set   0 200 400 600 800 1000 1200 0.01 0.015 0.025 0.04 0.06 0.1 0.2 Time \(sec Minimum Support Naive  Better  Figure 6 Execution times T all data set transactions Mining negativ e information is nontrivial and in the case of retail transactions data the problem b ecomes imp ossible to solv e Our approac his to use the grouping information suc h as a taxonom y o v er the items and the existing p ositiv e asso ciations in the data to induce negativ e rules b et w een items close to the items in the p ositiv e asso ciations This approac h solv es the problem pruning the com binatorial searc h space to a small subset of cases whic hha v e a high p oten tial of b eing in teresting W e presen tan algorithm for mining negativ e rules and impro v emen ts and study their p erformance on syn thetic data 4.1 F uture W ork Man y problems remain unsolv ed in the problem of mining negativ e rules Tw o of the biggest problems are as follo ws 017 W e assume only the taxonom yo v er the items as the domain kno wledge a v ailable to mine for negativ e rules Ho w ev er there ma y b e man y other 


 10 100 1000 10000 100000 1e+06 2 3 4 5 Number of candidates \(Normalized Size of the itemset Fanout = 9  Fanout = 3  Figure 7 Num b er of negativ e candidates t yp es of information a v ailable F or instance a kno wledge of substitute items Ho w to incorp orate other t yp es of information to impro v e the qualit y of rules needs to b e explored further 017 The n um b er of candidates generated is exp onential o v er the length of the large itemsets b eing considered More e\016cien t candidate generation tec hniques need to b e dev elop ed Ac kno wledgmen t The 014rst author wishes to thank Dr Rak esh Agra w al at IBM Almaden Researc h Cen ter for suggesting the problem and for man y insigh tful discussions References  R Agra w al T Imielinski  and A Sw ami Mining asso ciation rules b et w een sets of items in large databases In Pr o c e e dings of the 1993 A CM SIGMOD International Confer enc e on Management of Data  pages 207{216 W ashington DC Ma y 26-28 1993  R Agra w al and R Srik an t F ast algorithms for mining asso ciation rules in large databases In Pr o c e e dings of the 20th Internationa l Confer enc eonV ery L ar ge Data Bases  San tiago Chile August 29-Septem b er 1 1994  T J Blisc hok Ev ery transaction tells a story Creating customer kno wledge through mark et-bask et analysis Chain Stor eA ge Exe cutive  V71:50  57 Marc h 1995  J Han and Y F u Disco v ery of m ultiple-lev el asso ciation rules from large databases In Pr o c e e dings of the VLDB Confer enc e  pages 420  431 Septem b er 1995  M Houtsma and A Sw ami Set-orien ted mining of asso ciation rules In Pr o c e e dings of the International Confer enc e on Data Engine ering T aip ei T aiw an Marc h 1995  R Krishnam urth y and T Imielinski  Practitioner problems in need of database researc h A CM SIGMOD R e c or d  20\(3 Septem b er 1991  H Mannila H T oiv onen and A I V erk amo E\016cien t algorithms for disco v ering asso ciation rules In KDD-94 AAAI Workshop on Know le dge Disc overy in Datab ases  pages 181  192 Seattle W ashington July 1994  J S P ark M-S Chen and P S Y u An e\013ectiv e hash based algorithm for mining asso ciation rules In Pr oc e e dings of the A CM-SIGMOD Confer enc e on Management of Data  pages 229  248 San Jose California Ma y 1995  G Piatetsky-Shapiro Disc overy A nalysis and Pr esentation of Str ong R ules  pages 229  248 AAAI Press/The MIT Press Menlo P ark California 1991  G Piatetsky-Shapiro and W J F ra wley  editors Know le dge Disc overy in Datab ases  MIT Press 1991  A Sa v asere E Omiecinski and S Na v athe An e\016cien t algorithm for mining asso ciation rules In Pr oc e e dings of the VLDB Confer enc e  pages 432  444 Zuric h Switzerland Septem b er 1995  A Silb ersc hatz M Stonebrak er and J Ullman Database systems ac hiev emen ts and opp ortunities Communic ations of the A CM  34\(10 Octob er 1991  P Sm yth and R M Go o dman R ule Induction Using Information The ory  pages 159  177 AAAI Press/The MIT Press Menlo P ark California 1991  R Srik an t and R Agra w al Mining generalized association rules In Pr o c e e dings of the VLDB Confer enc e  pages 407  419 Septem b er 1995  M Stonebrak er R Agra w al U Da y al E Nuehold and A Reuter Database researc h at a crossroads The vienna up date In Pr o c e e dings of the 19th International Confer enc eon V ery L ar ge Data Bases  pages 688{192 Dublin Ireland August 1993 


with the same parameters but di\013eren t in size ranging from 25K to 100K The v alues of w minsup are set as the ab o v e scale-up exp erimen t In this 014gure the time is giv en in ln  sec  F rom the 014gure the execution time increases with the n um b er of transactions linearly with ln scale implying that the complexit yof the algorithms is exp onen tial in the n um b er of transactions 1  5.2.3 Exp erimen t for sp ecial case In this section w e are in terested in the p erformance in the sp ecial case whic h is the item w eigh ts equal to 0 or 1 only  In this case w e mak e the 014rst 900 w eigh ts b e 0 and the remaining w eigh ts b e 1 Other things including database and threshold equal as ab o v e section W e carried out the exp erimen t for the normalized w eigh ted case to compare the t w o algorithms There are t w o ma jor 014ndings 1 The p erformance of the sp ecial case is m uc h b etter than the general case where item w eigh ts follo w a distribution b et w een 0 and 1 2 Con trary to the previous cases MINW AL\(W p erforms b etter than MINW AL\(O F rom Figure 8 w e notice that the time needed in MINW AL\(W is m uc h less than the MINW AL\(O for all the thresholds This is b ecause in the joining step the n um b er of starting seed candidate itemsets in C 1 to generate itemsets in C 2  is less than MINW AL\(O case In this situation the 0/1 w eigh ts giv e the adv an tage to MINW AL\(W During the 014rst step the algorithm MINW AL\(W will easily prune all the small itemsets with 0 w eigh ts while MINW AL\(O will k eep those small itemsets with 0 w eigh ts As the starting seed is smaller in size MINW AL\(W w ould p erform w ell in this case 6 Conclusion W eha v e prop osed to study a new problem of mining w eigh ted asso ciation rule This is a generalization of the asso ciation rule mining problem In this generalization the items are assigned w eigh ts to re\015ect their imp ortance to the user The main di\013erence b et w een mining w eigh ted asso ciation rules and the mining un w eigh ted asso ciation rules is the do wn w ard closure prop ert y  W e prop osed t w o di\013eren t de\014nition of w eigh ted supp ort without normalization and with normalization W e prop osed new algorithms based on the supp ort b ounds  the algorithms MINW AL\(O and MINW AL\(W MINW AL\(O is applicable to b oth normalized and unnormalized cases and MINW AL\(W is applicable to the normalized case only  The p erformance ev aluation has b een done on these t w o algorithms W e found that MINW AL\(O outp erforms MINW AL\(W in most cases but MINW AL\(W p erforms b etter for the sp ecial case with only 0/1 item w eigh ts So far w eha v e only considered the mining of binary w eigh ted asso ciation rules Some of the researc hers did the researc h for the problem of the quan titativ e assoication rules suc has[4  3 W ema yin v estigate the problem of quan titativ e asso ciation rules with w eigh ted items whic his anin teresting topic in the future References  R Agra w al and R Srik an t F ast algorithms for mining asso ciation rules In Pr o c e e dings of the 20th VLDB Confer enc e  pages 487{499 1994  D Cheung V.T Ng A F u and Y F u Ef\014cien t mining of asso ciation rules in distributed databases In IEEE T r ansactions on Know le dge and Data Engine ering  pages 1{23 1996  T ak eshi F ukuda Y asuhik o Morimoto Shinic hi Morishita and T ak eshi T okuy ama Data mining using t w o-dimensional optimized asso ciation rules Sc heme algorithms an visualization In Pr o c e e dings of A CM SIGMOD  pages 13{23 1996  T ak eshi F ukuda Y asuhik o Morimoto Shinic hi Morishita and T ak eshi T okuy ama Mining optimized asso ciation rules for n umeric attributes T ec hnical Rep ort 1623-14 IBM T oky o Researc h Lab oratory  1996  J Han M Kam b er and J Chiang Mining m ultidimensional asso ciation rules using data cub es T ec hnical rep ort Database Systems Researc h Laboratory Sc ho ol of Science Simon F raser Univ ersit y  1997  J.S P ark M-S Chen and P S Y u An e\013ectiv e hash-based algorithm for mining asso ciation rules In Pr o c e e dings of A CM SIGMOD  pages 175{186 1995  A Sa v asere E Omiecinski and S Na v athe An e\016cien t algorithm for mining asso ciation rules in large databases In Pr o c e e dings of the 21th International Confer enc eon V ery L ar ge Data Bases  pages 432{444 1995 


expect this optimization to be of greatest bene\336t when the transaction sizes are large r example if our transaction is T 000 f A\000 B 000 C\000 D\000 E g  k 000 3 fan-out 000 2 then all the 3-subsets of T are f ABC,ABD,ABE,ACD,ACE,ADE,BCD,BCE,BDE,CDE g  Figure 2 shows the candidate hash tree C 3  We ave to increment the support of every subset of T contained in C 3  We egin with the subset AB C  and hash to node 11 and process all the itemsets In this downward path from the root we mark nodes 1 4 and 11 as visited We then process subset AD B  and mark node 10 Now consider the subset CDE  We see in this case that node 1 has already been marked and we can preempt the processing at this very stage This approach can r consume a lot of memory r a n fan-out F  for iteration k  e need additional memory of size F k to store the 337ags In the parallel implementation we have to keep a VISITED 336eld for each processor bringing the memory requirement to P\000F k  This can still get very large especially with increasing number of processors In we sho w a mechanism by which further reduces the memory requirement to only k 000F  The approach in the parallel setting yields a total requirement of k 000F 000P  5 Experimental Evaluation Database T I D Total Size T5.I2.D100K 5 2 100,000 2.6MB T10.I4.D100K 10 4 100,000 4.3MB T15.I4.D100K 15 4 100,000 6.2MB T20.I6.D100K 20 6 100,000 7.9MB T10.I6.D400K 10 6 400,000 17.1MB T10.I6.D800K 10 6 800,000 34.6MB T10.I6.D1600K 10 6 1,600,000 69.8MB Table 2 Database properties 5.1 Experimental Setup All the experiments were performed on a 12-node SGI Power Challenge shared-memory multiprocessor Each node is a MIPS processor running at 100MHz There\325s a total of 256MB of main memory The primary cache size is 16 KB 64 bytes cache line size with different instruction and data caches while the secondary cache is 1 B 128 bytes cache line size The databases are stored on an attached 2GB disk All processors run IRIX 5.3 and data is obtained from the disk via an NFS 336le server We used different synthetic databases with size ranging form 3MB to 70MB 2  and are generated using the procedure described in These databases mimic the transactions in a retailing en vironment Each transaction has a unique ID followed by a list of items bought in that transaction The 2 While results in this section are only shown for memory resident databases the concepts and optimization are equally applicable for non memory resident databases In non memory resident programs I/O becomes an important problem Solutions to the I/O problem can be applied in combination with the schemes presented in this paper These solutions are part of future research 11 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


  0 500 1000 1500 2000 2500 0 2 4 6 8 10 12 Number of Large Itemsets Iterations Large Itemset at Support = 0.5 222T5.I2.D100K\222  222T10.I4.D100K\222   222T15.I4.D100K\222   222T20.I6.D100K\222   222T10.I6.D400K\222   222T10.I6.D800K\222   222T10.I6.D1600K\222  Figure 3 Large Itemsets per Iteration data-mining provides information about the set of items generally bought together Table 2 shows the databases used and their properties The number of transactions is denoted as jD j  average transaction size as j T j  and the average maximal potentially large itemset size as j I j  The number of maximal potentially large itemsets j L j 000 2000 and the number of items N 000 1000 We refer the reader to for more detail on the database generation All the e xperiments were performed with a minimum support value of 0.5 and a leaf threshold of 2 i.e max of 2 itemsets per leaf We note that the  improvements shown in all the experiments except where indicated do not take into account initial database reading time since we speci\336cally wanted to measure the effects of the optimizations on the computation Figure 3 shows the number of iterations and the number of large itemsets found for different databases In the following sections all the results are reported for the CCPD parallelization We do not present any results for the PCCD approach since it performs very poorly and results in a speed-down on more than one processor 3  5.2 Aggregate Parallel Performance Table 3 s actual running times for the unoptimized sequential and a naive parallelization of the base algorithm Apriori for 2,4 and 8 processors without any f the techniques descibed in sections 3 and 4 In this section all the graphs showing  improvements are with respect to the data for one processor in table 3 Figure 4 presents the speedups obtained on different databases and different processors for the CCPD parallelization The results presented on CCPD use all the optimization discussed 3 Recall that in the PCCD approach every processor has to read the entire database during each iteration The resulting I/O costs on our system were too prohibitive for this method to be  12 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


