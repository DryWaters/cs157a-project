html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Proceedings  of the 5  WorId Congress on Intelligent Control %hlft  tkBkE@US flakit&amp and Automation, June 15-19, 2004, Hangzhou, P.R. China 2004 4 6  R 15 - 19 8. KPEMM gMPIS  Maximal-Profit Item Selection based on Generalized Cross-Selling Considerations Bihong Liu, Fansheng Kong, Xiaobing Yang Institute of Artificial Intelligence, Zhejiang University, Hangzhou 3 10027 E-mail: bibongli3002@yahoo.com Abst?acf-An algorithm was proposed to rank items with respect to profits and to select the most profitable ones for business and other applications by analyzing historical transaction data see The decision of maximal-profit items subset considers both the cross-selling effects among items and the customers  changing buying behaviors. Generalized loss rules are proposed to model the effects of the unselected items to the customers  purchase aetinns. We show that the approach models customers  buying behaviors well and is very suitable and practicable for real business applications. We proposed a fast heuristic algorithm to the problem. Experiments show that the algorithm is highly effective and efficient and is scalable for large data sets Keywords- item selection, cross-selling, association rule, data mining I. INTTRODUCTION Item selection is a hasic problem in business and other applications. In retailing, limited by shelf spaces and financial resources, a retailer usually wants to select only a limited amount of items for sale. Certainly, the retailer wants to get a maximal profit from the selection. So, the problem becomes the following: bow to select a subset of items so that the estimated profit will be maximized. Simply selecting items based on their individual profits is not a good solution because it doesn  t consider the mutual effects between different items: a missing item not only makes a customer  s desirable item unavailable, but also reduces the probability of the customer  s purchasing of other items The easy availability of vast data sets has provided some foundations for the problem. Data mining has showed its great capabilities on the discovery of patterns from large data Association rule mining origins from the analysis of basket data and has been extended to discover the correlations between different things[2,3,6,9]. Although many algorithms have been proposed, there are relatively few researches on how to use data mining techniques directly for the business decision makingp In recent years, several algorithms for maximal-profit item selection problem have been proposed, such as PROFSET[4,5 HAP[II] and MPIS[IO]. PROFSET decomposes each transaction into several disjoint subsets and use 0-1 programming on these subsets to select items. HAP models the mutual reinforcement relation between items as the hublauthority weights of web pages and ranks items basing on authority weights. MPIS uses the lose rules to estimate.the profits lost because of the unavailability of some items and heuristically remove items until the expected number of items meet PROFSET, HAP and MPIS solve the item selection problem to some extent, but they still have some disadvantages. The main disadvantage is that they don  t well model the purchasing behaviors of customers. PROFSET  s maximal fiequent ifemsets doesn  t reflect a customer  s purchase intention well and HAP doesn  t well consider the effects of an unavailable item to the purchasing behavior of a customer MPIS considers the effects, but it makes a questionable hide assumption that a customer will reduce an item  s purchase if the item is unavailable. Another disadvantage is that these algorithms can  t be used to add new items for sale The disadvantages of these algorithms urge us to develop an efficient and effective algorithm-gMPIS: generalized Maximal-Profit Item Selection with the consideration of cross-selling effects. gMPIS uses generalized loss rules to 


cross-selling effects. gMPIS uses generalized loss rules to model the effects of the unavailable items to the purchase actions of customers. Unlike MPIS, gMPIS assumes a customer will choose some similar items if his  intended  item is unavailable. We propose a heuristic algorithm to speed the selection. Experiments show that the algorithm is highly effective and efficient and is scalable for large data sets 0-7803-8273-0/04/$20.00 02004 IEEE 4255 11. PROBLEM DEFMITION Given a set of items and a transaction database about the selling of the items, maximal-profit item selection problem intends to select a subset of the items, so that the selected items will produce a maximal profit based on the consideration of cross-selling effect. We define the problem in a formal form similar to [io Definition 1 \(Taxonomy tree 12, . . ., I"} ,  a taxonomy tree H on I is a tree H with each item in I as vertex. An edge in H represents an is-a relationship. If there is an edge in H from p to e, we call p a parent of c and c a child ofp We use vertex and item interchangeable in the paper Definition 2 \(Cut items of a taxonomy tree taxonomy tree H on itemset Z={Z,% I&gt;, ..., I " ] ,  the cut items I ' C I  of H is a set of items in H such that each path from the root to each leaf of H contains exactly an item in 1 Definition 3 \(gMPIS: generalized Maximal-Profit Item Selection problem data set D={t;l 15i5m,m,tjc I }  with m transactions. Let ~ \( I J the profit of item I, in transaction t j \( lY5m The profit of an item may differ since the amount of purchase may be different and its price may be changing along time taxonomy tree on I and I ' C I  be the cut items of H. gMPIS problem is the problem of selecting a subset S C I '  of J elements, such that the estimated profit produced by S is maximal among the profits produced by all subsets of size J in I', where J is a parameter specified by user Definition 4 \(Generalized item on itemset I={[,, 12, ..,,I,,}, for an item I, in H, the generalized item I,' of I ,  is the ancestor of I, in the cut items of H Definition 5 \(Generalized transaction ransaction t ;CI ,  the generalized transaction T; of r, is defined as: T,= {Ia' II, E t Definition 6 \(Selected generalized items transaction tiGI, a selection S C I ' ,  we define the set of selected generalized items as t;'T.nS, the set of generalized items not selected in ti as di'=Tt-tz Definition 7 \(Profit of a generalized item and a generalized item Ior in T;,p\(I,,', generalized transaction T P U a ' , T 1 We use historical transaction data sets to estimate the profit P\(S is S P\(S 2 Here, csfdj'Jo if the generalized itemset d;' is unavailable, at what probability is the generalized item I,' won't be bought in transaction 1 p\(Ia',Tj transaction T Definition 8 \(Loss rule d;: which means a customer who purchases generalized item I,' will also purchase at least one generalized item in d Definition 9 \(Confidence of loss rule l,'=&gt;Vd the number of transactions containing both I,' a id  at least one element in d;' divided by the number of transactions containing Io Modeling csf\(d;',I l,,=&gt;Vd S Po =~==,c,,,,.d~:,~x\(1-... \(1:=&gt;4 3 111. RELATED WORK Association rule mining has received many attentions in 


Association rule mining has received many attentions in retent yean[2,3,6,9]. The formal definition of association rule is the following. Given a set of items I={I,J2, . . .Jn] and a data set B { t $ ; L I ,  l s i s m } ,  an association rule is the implication of the form X=&gt;Y, where XL I, Y c  I, XnY=@. The rule X=&gt Y holds in the data set D with confidence e% if c% of transactions in D that contain X also contain Y. The rule X=&gt;Y has support s% in the transaction set D i fs% of transactions in D containXU y[2,3 There are some algorithms used for item selection PROFSET [4,5] first identifies a customer's purchase intention in a transaction by decomposing the transaction into several disjoint "maximal frequent itemsets", which are the itemsets with support above some user-defined minimal support and have no proper supersets. Then PROSET applies 0-1 programming to select some purchase intentions, not single items, kom the whole purchase intentinns Just as pointed in [ 111, PROFSET has several drawbacks such as, it considers none of the strength of relation between items and marimal frequent itemsets doesn't reflect purchase intentions well Another item selection algorithm is HAP[Il]. It uses the hub-authority" method as in [7] to rank items. Items are 4256 represented by vertices in a graph and each link from I, to 4 represents the cross-selling effects from 1, to 4. Each item is assigned a hub weight and an authority weight. An item pointed by many items with high huh weights is a good authority item. Correspondingly, an item pointed by many items with high authority weights is a good hub item. HAP selects items based on the ranking of authority weights In [IO], drawbacks of HAP are pointed, such as, the problems of dead ends or spider traps, item with small profit will gain high autharity weight if many items point to it. H A P can't solve the problem of generalized items selection The most recent algorithm is MPIS. Our gMPIS algorithm flows the same concept of loss rule from it. MPIS estimates the probability of not purchasing each available item I, in a transaction t, under a selection S using the confidence of I,=&gt Vd;, where d; is the unavailable items in 1 MPIS is efficient and effective for item selection. However it also has some drawbacks. First, MPIS makes a questionable hide assumption that a customer won't choose to buy another item when his intended item is unavailable, even if there are some items very similar to it. Also, a transaction isn't the just description of a customer's purchase intention, because the customer may have no a concrete purchase intention and he is buying under the selection of controlled available items and affected by the situations at that time. MPIS decreases the cross-selling factor, since it computes cross-selling factor using the confidence between item I ,  and 4, hut a more accurate estimation of cross-selling factor should he between generalized items. MPIS can't he used to select new items IV. gMPIS ALGONTHM A. gW1S Algorithm In this section, we propose our gMPIS algorithm. At first Definition 10 \(Individual generalized transaction generalized transaction T; contains a generalized item I,' only then T, is an individual generalized transaction of lo'. The individual count c.' is the total count of individual generalized transaction ofl we give some definitions Definition 11 \(Average profit of generalized item be the set of transactions that contain generalized item Io', the average profit of I.' is defined as 4 amp;ez,# PU, ' &gt;  T The following are our gMPIS algorithm and it has used some methods in [IO gMPIS algorithm 1. Read each transaction from disk, translate it into generalized items, compute the individual count of each generalized item, the count of each generalized item, the count 


generalized item, the count of each generalized item, the count of each pair of generalized items, average profit of each generalized item 2. Compute the estimated set for each generalized item. For each generalized item I,', compute the estimated value of generalized item 4' from 4': e;;=pj'X C~'+@~'+P;~ X sup\(l where sup\(1,'A Choose J-I generalized items with the highest estimated values and put them into the estimated set S,' for 1,: put the rest into U&amp 3. Compute the benefit of each generalized item. For each generalized item I;: compute the estimated benefit of 1 bi'=P\(Sj'U {I and let MaxF'rofif=b 4. Item pruning. Compute the estimated lowest profit L,'and estimated highest profit H,' of each generalized item.1,: where L,'=f\({l Sl'U { l Sj value L from all L;'. For each generalized item I\(', remove I;' if Hj'&lt;L. Let I' be the set of all generalized items that have not been pruned value from I'. For each generalized item I,' in I 5. Prune a generalized item I,' with the smallest benefit 5.1 Iflx' is in S,: remove 1,'from S;', add a generalized item 4' from UnS;' with highest eij' value into S,: then recalculate b;kP\(S,'U \(4 5.2 If  bi5MaxS, let MaxS be &amp;'U {c'}, let MaxProfit be b 6. Select a generalized itemset M with J generalized items from I' with highest benefit values. Calculate P\(M P\(M M I. If I' contains exactly J elements, return MaxS as a selection and MaxProfit as the estimated profit, else goto step 5 B. Some Notes to gMfIS It's easy to add user's constraints into gMPIS, such as which generalized items are selected and which ones are not selected When estimating the profit of a selection, the calculation of 4257 con  gt;Vd   scan the whole transactions and the total scans of the transactions data sets are up to the sum of the amount of the generalized items bought by each customer. We use the method similar to MPIS to approximate the calculation. That is, con  gt;Vdi? is approximated by con    gt;vd   C d . e d i , ~ u p    d   2,1 5 From [IO], we can know that con    gt;Vd approximation of con  gt;Vd    s easy to see that the estimated profits using con  Z  gt;Vd;? will be not greater than the profits estimated by using con\(l  gt;Vd    Io  gt Vd;? can be easy calculated in constant time the items that are sure to be non-profitable items In the step 4, we use the same method as in [IO] to prune gMPIS has great flexibilities on the selection of cut items Note that the definitions of taxonomy tree and cut items have guaranteed that there are no two cut items such that one is the generalized items of the other. Although there is no limitation on tbe selection of cut items, we recommend users not to select leaf items. That will reduce the merits of our generalized items selection model. But, a user can climb up the taxonomy tree to select more generalized items. A user can first decide which generalized items should be selected, and continue to select more specialized items among selected generalized items. A general method is to select the parents of leaf items as cut items. After selecting generalized items, a user can select specialized items based  on his other methods for example, select the most profitable items, or all items, or add new items C. Our Contribulions Our contributions are the following First, gMF  lS is based on the framework of generalized items and the  generalized cross-selling effect models 


items and the  generalized cross-selling effect models customers  purchase behaviors well. It also gives users the flexibilities of selecting items on multiple granularities Second, gMPIS uses a fast algorithm to compute the confidence of a generalized loss rule and speeds the algorithm a lot Third, gMPIS always records the optimal solution has gotten up to now, and after removing a generalized item, it selects a local optimal solution. That only takes a little time but it makes gMPIS converge to optimal solution fast. A user can get an optimal solution up to now at any time and can control the running V. PERFORMANCE EVALUATION We use AMD XP 2500+ PC to conduct our experiments. All programs are written in Visual C-H. We use theIBM synthetic data generator[l] to generate the test data with item taxonomy[9]. The features of the generated data sets are: 5000 leaf items, 10,000 transactions, IO items per transaction on average, 4 items per frequent itemset on average, 5 fanouts, IO roots. We add a new root to be the parent of all the roots generated by [I]. We use the same method as in [ I l l  to model the profit distribution. 10% of items have low profit range between $0.1 and $1, 80% of items have the medium profit range between $ I  and $5, and 10% of items have  the high profit range between $5 and $10 Figure 1 shows the runtime \(in seconds different numbers of generalized items are selected. Figure 2 shows the profitability of gMPIS when different numbers of generalized items are selected. Here profitability is the ratio of estimated profit of a selection to the total profit of all transactions. Figure 3 shows the run time of gMPIS when only the number of.leaf items changes and keep the selected generalized items number half of the total generalized items number. Figure 4 describes the,run time of gMPIS when only the transactions number changes and keep the selected generalized items number as 500 _. .. . 1 .m Fig.1 3 0 7 Fig.4 4258 In Fig. I ,  when selected generalized items number is 200 the run time is only 28 seconds and attains maximal value. In Fig. 2, the profitability of the selection of 100 generalized items is 70.9% and when 300 generalized items are selected the profitability will be 93.6%. In Fig. 3, when the number of leaf items increases up to 11000, the run time is only 30 seconds. In Fig. 4, the run time increases linearly as the transaction number increases. When the transaction number is 26000, the run time is only 56 seconds. Experiments show that gMPIS is highly efficient and effective. It is fast and can find items with high profitability and also is scalable to data sets with large number of items and transactions VI. CONCLUSION In this paper, we propose an algorithm to select most profitable items based on historical transaction data sets. Our algorithm uses generalized items and models cross-selling effects as generalized loss rules. The generalized loss rules model customers' purchase behaviors well. We apply a heuristic algorithm to speed the calculation of the confidence of generalized loss rules and record the optimal selection at each step, thus make the selection fast and easy controlled by users. Experiments show that gMPIS is highly efficient and effective. It is fast and can find items with high profitability and also is scalable to large data sets Z] M e s h  Agrawal, Tomasz Imilienski, and A N ~  Swami, "Mining association N I ~ S  between sets of i t em in large databaSes,l' in Pmc. of ACM-SIC Int? Con/ on Manogemen1 of Dolo, 1993 m e s h  Agrawal, and Ramakrishnan Srikant, "Fast algorithms for mining assmiation rules," in Pmc. of :he 20th Inl'l Con/ on Very Lowe Dotaboses, 1994 


Tom Bnjs, Ban Goethals, Gilben Swinnen, Koen Vanhoof, and Geen Wets, "A data mining framework for optimal product selection in retail supermarket data: The generalized profset model," in Pmc. ofACM-SIC in1 ' I  con/ on knowledge discovery and doto mining, 2000 Tom Brijs, Gilben Swimen, Koen Vanhoof, and Geetl Wets, "Using associalion rules for pmduct assonment decisions: A case shldy," in Pmc ofACM-SIG in1 ' I  con/ on knowledge discovery and dam mining, 1999 6] Jiawei Hm, Jim Pei, and Yiwen Yin, "Mining frequent panems without candidate generation," in Pmc. of ACA4-SIG IM'I Con/ on Monogemen1 of Dolo, 2000 7] Jon M. Kleinberg, "Authoritative sources in a hyperlinked environment In Pme ACM-SlAMSy". on Discme Algorithms, 1998 3 141 SI 8] Jon Kleinberg, Christos Papadimitnou, and Prabhakar Raghavan, "A microeconomic view of data mining," Dolo Mining and Knowledge Dlreovery, vol. 2, 1998 9] Ramakrishnan Srikant, and Rakesh Agrawal, "Mining Generalized Association Rules", in Pmc. of the ZlsI Inl? Confir" on Very L q e Dotobases, 1995 I O ]  Raymond Chi-Wing Wong, Ada Wai-Chee Fu, and Ke Wang, "MPIS Maximal-profit item selection with cross-sellinp considerations". in Pmc ofIEEE Inl? Con/ on Dora Mifling, 2003 Ill Ke Wang, and Ming-Yen Thomas Su, "Item selection by hub-authority" profit ranking," in Pmc. of ACM-SIC in17 con/ on howledge discovery and dolo mining, 2002 4259 pre></body></html 


transactions to hundreds of seconds for 500,000 h-ansactions As FP-split algorithm only scans the database once but not twice as the FP-tree construction algorithm does, the FP-split algorithm saves more time. In the event of large number of transaction data, the I/O cost remains much less compared with that of FP-tree construction algorithm. On the other hand FP-tree construction algorithm is required to filter the non-frequent items by checking the transaction record and it reorder those frequent items in each transaction record. On the contrary, FP-split algorithm doesn  t need to do so Ev. CONCLUSIONS In this paper, we have proposed an adaptive FP-tree construction algorithm - the FP-split algorithm. The FP-split algorithm is superior to FP-tree construction algorithm. There are three reasons such that the proposed method outperformed FP-tree construction algorithm in terms of tree construction The first one is that our proposed method scans the database only once. The second one is that filtering out and sorting the items in each transaction record will no longer be employed in our method. The thud one is that the header table and links will not be repeatedly searched, while adding a new node in the FP-split tree. Our future work is to develop one modified FP-split method for native XML databases ACKNOWLEDGMENT This work is supported by National Science Council of Taiwan, NSC- 93-2213-E-324-006 REFERENCES R. Agrawal, T. Imielinski, and A. Swami  Mining Association Rules between Sets of Items in Large Databases  Proceedings ofthe ACM SIGMOD conference OR Management ofData, 1993, pp. 207-2 16 R. Agrawal and R. Srikant  Fast AIgorithms for Mining Association Rules  Proceedings of the 20th Intemaiional Conference on Vety Large Databnses, 1994 R. Agrawal, J. Gehrke, D. Gunopulos, and P. Raghavan  Automatic Subspace Clustering of High Dimensional Data for Data Mining Applications  Proceedings of ihe ACM SIGMOD Conference on Monugement ofDota, 1998, pp. 94-105 K. Wan&amp; S. Zhoy and S. C. Liew  Building Hierarchical Classifiers Using Class Proximity  Proceedings of ihe 25th International Conference on Very Large Daiu Bmes, 1999, pp. 363-374 J. Han and M. Kamber, Data Mining: Concepts and Techruques, CA Morgan Kauham Publishers, 2001 J. Han, J, Pei, and Y, Yin  Mining Frequent Patterns Without Candidate Generation  Proceedings of ?he ACM SIGMOD Intemaiional Conference on Management ofData, 2000, pp. 1-12 J. Han, J. Pei, Y .  Yin, and R. Mao  Mining FreCperit Patterns without Candidate Generation: A Frequent-Pattern Tree Approach  In DataMining andKnowledge Discovery, 2004, Vol. 8, pp, 53-87 K. Wang, L, Tang, J .  Him, and J. Liu  Top Down FP-Growth for Association Rule Mining  Proceedings of che 6ih Pac$c-Asia Conference on Advances in Knowledge Dircove?y ondData Mining, 2002 J, S. Park, M. S. Chen, and P. S. YU  An Effective Hash Based Algorithm for Mining Association Rules  Proceedings of the ACM pp. 334-340 S  GMOD, 1995, PP. 175-186 IO] S.  Y. Wur and Y: Leu  An Effective Boolean Algorithm for Mining Association Rules in Large Databases  The 62h Internutional Confirenee on Database Sysfems for Advanced Applications, 1999, pp. 179-1 86 1 1 1 1  R. Agarwal, C. Agganval, and V. V. V Prasad  A Tree Projection Algorithm for Generation of Frequent Itemsets  Journal on PurafIel and Disrributed Computing, 2000, Vol. 61, pp, 350-371 U]  I. Pei, J. Han, H .  Lu, S. Nishio, S. Tang, and D. Yang  H-Mine: Hyper Structure Mining of Frequent Patterns in Large Database  Proceedings of the 1st IEEE Intemotionul Conference on Dafa Mining, 2001, pp 441-448 I131 IBM Almaden Research Center, Quest synthetic data generation http://www.almaden .ibm.comlsoRware/quest/Resources/dataset html, 2005 463 pre></body></html 


pre></body></html 


 The required delivery date is a Range constraint any date within the next 30 days Attribute Required-Delivery-Date  today today+30 days Attribute S&H query\(UPS Product   say it is 59.95 Attribute Value  query\(Catalog Product  Attribute Price  Attribute Total  Inter-attribute constraints in PO14 Price  Quantity  Value  1-x Total  Price  S&H  1.088 Total  Total1  Total4 In this case the Quantity attribute value has changed to 2 by adding both requests together Furthermore the Delivery-Date attribute value is a result of finding a common range of the two The obvious saving in this case is 2*$39.95 59.95\1.088  21.71 Whether a bunch of POs should be aggregated in a particular way depend on whether costing savings can be achieved while satisfying all the constraints 6.2 Intelligent Aggregation of Purchase Orders in e-Procurement with Negotiations Aggregation under dynamic negotiation is harder because supplier side could be revising its own strategies and parameters on the fly While human intervention in the aggregation process is possible we focus on automated aspects of the aggregation in this paper Suppose we have a simple supplier side rule buy one and get second one half price from LT a supplier of mice keyboard and trackball Suppose we have requests to buy Mice as follows PO5 Attribute Buyer  Organization 223B\224 User 223Joe\224 Location 223PS\224 Attribute Supplier  mpany 223LT\224 Catalog  http://\205/LT Attribute Product  223Optical Mouse\224 Attribute Quantity 1 Attribute Required-Delivery-Date  01/21/05 02/21/05   a r an g e of dat e s  order dat e  deadline d Attribute S&H9  query\(UPS Product   say it\222s 4.95 Attribute Value  query\(Catalog Product  Attribute Price10  Value  say it\222s 29.95 Attribute Total10  Total10  Price10  S&H10  1 tax rate results in a value 29.95  4.95\1.088  39.97 PO6 Attribute Buyer  Organization 223C\224 User 223Al\224 Location 223PS\224 Attribute Supplier  mpany 223LT\224 Catalog  http://\205/LT Attribute Product  223Optical Mouse\224 Attribute Quantity 1 Attribute Required-Delivery-Date  01/25/05 02/28/05   a r an g e of dat e s  order dat e  deadline d Attribute S&H10  query\(UPS Product   say it\222s 4.95 Attribute Value  query\(Catalog Product sayit\222s 29.95 per mouse Attribute Price10 Value Attribute Total10  Total10  Price10  S&H10  1 tax rate results in a value 29.95  4.95\1.088  39.97 6.2.1 PO Aggregation Under Negotiation The rule-based aggregation engine uses the Negotiation service to understand supplier\222s offers and tries to take advantage of the terms in the offers For example by aggregating PO5 and PO6 can be aggregated as follows PO56 Attribute Buyer  Organization B C User 223Joe\224 223Al\224 Location 223PS\224 Attribute Supplier  mpany 223LT\224 Catalog  http://\205/LT Attribute Product  223Optical Mouse\224 Attribute Quantity 2 Attribute Required-Delivery-Date  01/25/05 02/21/01 Attribute S&H10  query\(UPS Product   say it\222s 4.95 Attribute Value  query\(Catalog Product sayit\222s 29.95 per mouse Attribute Price10 1.5*Value Attribute Total10  Total10  Price10  S&H10  tax  1.5 29.95  4.95\1.088  54.26 A saving of 39.97  2 54.26  25.68 or over 32 of savings Note the changes of the 223Quantity\224 and 223RequiredDelivery-Date\224 attributes after aggregation The quantities are added up and the required delivered dates are merged for a common range Due to the constraints on object attributes aggregation may require complex constraint solving Proceedings of the 2005 Ninth IEEE International ED OC Enterprise Computing Conference \(EDOC\22205 0-7695-2441-9/05 $20.00 \251 2005  IEEE 


7 Conclusions and Future Work This paper describes an Intelligent Aggregation facility in enterprise e-Procurement process This facility introduces an information model a rule-based aggregation engine corporate agreement policies and negotiation in aggregating large volume of POs in enterprise eprocurement to reduce cost and maximize efficiency This information model includes extensive use of constraints for and among attributes in a PO These constraints guard the integrity of POs as they are aggregated The intelligent aggregation facility can be inserted as a value-added service in the enterprise e-Procurement workflow An enterprise generates millions of POs every year but the number of distinct products and services the enterprise purchases is actually much smaller in the hundreds rather than in the millions This presents cost saving opportunities by aggregating POs that makes best use of terms and conditions in corporate agreements or supplier offers Some concrete examples are used to show the idea of automated aggregation and the opportunities in reducing procurement cost As millions of POs are generated even a small percentage of savings would mean substantial savings for large enterprises The ideas described in this paper have not been fully implemented in our prototype One area needs more work is the formal representation of policies in corporate agreements which would allow the aggregation engine to automatically explore aggregation opportunities before POs are made to suppliers Another is the semantic model of products which would enable more semantics-based aggregation of POs 8 References  e bX M L  h t t p   w w w ebxml  org  2 e n g  J  S u  S  Y  W  L a m H   a n dH e l a l S   223Achieving Dynamic Inter-Organizational Workflow Management by Integrating Business Processes Events and Rules,\224 Proceedings of the 35th Hawaii International Conference on System Sciences HICSS35 Hawaii USA January 2002 3 u  S  Y  W  L a m H   L e e  M  B a i S   a n dS h e n  Z   An Information Infrastructure and E-services for Supporting Internet-based Scalable E-business Enterprises Proceedings of the 5th International Enterprise Distributed Object Computing Conference Seattle Washington USA September 2001 4 S u S Y  W   H ua ng C  H a mme r J   H u a ng Y   L i  H   Wang,L.,LiuY.,Pluempitiwiriyawej,C.,Lee,M and Lam H 223An Internet-based Negotiation Server for E-commerce,\224 VLDB Journal Vol 10 No 1 2001 pp.72-90 5 M o r r i s S l o m a n  223 P o l i c y D ri v e n M an ag em e n t f o r Distributed Systems\224 Journal of Network and Systems Management Plenum Press Vol 2 No 4 1994  M aarten S teen  J oh n D errick  223 For m ali s ing ODP Enterprise Policies\224 Proceedings of the 3 rd International nterprise Distributed Object Computing Conference Mannheim Germany IEEE CS Press September 1999  J am e s H a ns on  Z oran M i l o s e v i c 223 C o n v e r s at i onOriented Protocols for Contract negotiations\224 Proceedings of the 7th International Enterprise Distributed Object Computing Conference Brisbane Australia IEEE CS Press September 2003  S  N eal J  C ole P.F L i n i ng ton  Z  Milose v i c S Gibson S Kulkarni 223Identifying Requirements for Business Contract Language a Monitoring Perspective\224 Proceedings of the 7th International Enterprise Distributed Object Computing Conference Brisbane Australia IEEE CS Press September 2003 9 T  D im itrak o s  I  D j o rd j e v i c Z  Milo sev i c A  J o san g  C Phillips 223Contract Performance Assessment for Secure and Dynamic Virtual Collaborations\224 Proceedings of the 7th International Enterprise Distributed Object Computing Conference Brisbane Australia IEEE CS Press September 2003 Proceedings of the 2005 Ninth IEEE International ED OC Enterprise Computing Conference \(EDOC\22205 0-7695-2441-9/05 $20.00 \251 2005  IEEE 


absolute values. The results can vary on other computers. But it can be guaranteed that performance ratio of the algorithms will remain the same After making the comparisons with sample data, we came to the conclusion that PD algorithm performs significantly better than the other two especially with larger datasets. PD outperforms DCP and PIP regarding running time. On the other hand, since PD reduces the dataset, mining time does not necessary increase as the number of transactions increases and experiments reveals that PD has better scalability than DCP and PIP. So, PD has the ability to handle the large data mine in practical field like market basket analysis and medical report documents mining 5. References 1] R. Agrawal and R. Srikant, "Fast algoritlnns for mining association rules", VLDB'94, pp. 487-499 2] R. J. Bayardo, "Efficiently mining long patterns from databases", SIGMOD'98, pp.85-93 3] J. Pei, J. Han, and R. Mao, "CLOSET: An Efficient Algorithm for Mining Frequent Closed Itemsets \(PDF Proc. 2000 ACM-SIGMOD International Workshop on Data Mining and Knowledge Discovery, Dallas, TX, May 2000 4] Qinghua Zou, Henry Chiu, Wesley Chu, David Johnson, "Using Pattern Decomposition\( PD Finding All Frequent Patterns in Large Datasets", Computer Science Department University of California - Los Angeles 5] J. Han, J. Pei, and Y. Yin, "Mining Frequent Patterns without Candidate Generation \(PDF  SIGMOD International Con! on Management of Data SIGMOD'OOj, Dallas, TX, May 2000 6] S. Orlando, P. Palmerini, and R. Perego, "The DCP algoritlnn for Frequent Set Counting", Technical Report CS2001-7, Dip. di Informatica, Universita di Venezia 2001.Available at http://www.dsi.unive.itl?orlando/TR017.pdf 7] MD. Mamun-Or-Rashid, MD.Rezaul Karim, "Predictive item pruning FP-tree algoritlnn", The Dhaka University  Journal of Science, VOL. 52, NO. 1, October,2003, pp. 3946 8] Park, J. S., Chen, M.-S., and Yu, P. S, "An Effective Hash Based Algoritlnn for Mining Association Rules", Proc ofthe 1995 ACM-SIGMOD Con! on Management of Data 175-186 9] Brin, S., Motwani, R., Ullman, J., and Tsur, S, "Dynamic Itemset Counting and Implication Rules for Market Basket Data", In Proc. of the 1997 ACM-SIGMOD Conf On Management of Data, 255-264 10] Zaki, M. J., Parthasarathy, S., Ogihara, M., and Li, W New Algoritlnns for Fast Discovery of Association Rules In Proc. of the Third Int'l Con! on Knowledge Discovery in Databases and Data Mining, 283-286 11] Lin, D.-I and Kedem, Z. M., "Pincer-Search: A New Algoritlnn for Discovering the Maximum Frequent Set", In Proc. of the Sixth European Conf on Extending DatabaseTechnology, 1998 12] R. Ramakrishnan, Database Management Systems University of Wisconsin, Madison, WI, USA; International Edition 1998 pre></body></html 


tors such as union, di?erence and intersection are de?ned for pairs of classes of the same pattern type Renaming. Similarly to the relational context, we consider a renaming operator ? that takes a class and a renaming function and changes the names of the pattern attributes according to the speci?ed function Projection. The projection operator allows one to reduce the structure and the measures of the input patterns by projecting out some components. The new expression is obtained by projecting the formula de?ning the expression over the remaining attributes [12 Note that no projection is de?ned over the data source since in this case the structure and the measures would have to be recomputed Let c be a class of pattern type pt. Let ls be a non empty list of attributes appearing in pt.Structure and lm a list of attributes appearing in pt.Measure. Then the projection operator is de?ned as follows ls,lm c id s m f p ? c, p = \(pid, s, d,m, f In the previous de?nition, id ing new pids for patterns, ?mlm\(m projection of the measure component and ?sls\(s ned as follows: \(i s usual relational projection; \(ii sls\(s and removing the rest from set elements. The last component ?ls?lm\(f computed in certain cases, when the theory over which the formula is constructed admits projection. This happens for example for the polynomial constraint theory 12 Selection. The selection operator allows one to select the patterns belonging to one class that satisfy a certain predicate, involving any possible pattern component, chosen among the ones presented in Section 5.1.1 Let c be a class of pattern type pt. Let pr be a predicate. Then, the selection operator is de?ned as follows pr\(c p Join. The join operation provides a way to combine patterns belonging to two di?erent classes according to a join predicate and a composition function speci?ed by the user Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE Let c1 and c2 be two classes over two pattern types pt1 and pt2. A join predicate F is any predicate de?ned over a component of patterns in c1 and a component of patterns in c2. A composition function c pattern types pt1 and pt2 is a 4-tuple of functions c cStructureSchema, cDataSchema, cMeasureSchema, cFormula one for each pattern component. For example, function cStructureSchema takes as input two structure values of the right type and returns a new structure value, for a possible new pattern type, generated by the join. Functions for the other pattern components are similarly de?ned. Given two patterns p1 = \(pid1, s1, d1,m1, f1 p2 = \(pid2, s2, d2,m2, f2 p1, p2 ned as the pattern p with the following components Structure : cStructureSchema\(s1, s2 Data : cDataSchema\(d1, d2 Measure : cMeasureSchema\(m1,m2 Formula : cformula\(f1, f2 The join of c1 and c2 with respect to the join predicate F and the composition function c, denoted by c 1   F  c  c 2   i s  n o w  d e  n e d  a s  f o l l o w s    F  c  c 2     c  p 1   p 2   p 1    c 1  p 2    c 2  F   p 1   p 2     t r u e   5.1.3. Cross-over database operators OCD Drill-Through. The drill-through operator allows one to 


Drill-Through. The drill-through operator allows one to navigate from the pattern layer to the raw data layer Thus it takes as input a pattern class and it returns a raw data set. More formally, let c be a class of pattern type pt and let d be an instance of the data schema ds of pt. Then, the drill-through operator is denoted by c c Data-covering. Given a pattern p and a dataset D sometimes it is important to determine whether the pattern represents it or not. In other words, we wish to determine the subset S of D represented by p \(p can also be selected by some query the formula as a query on the dataset. Let p be a pattern, possibly selected by using query language operators, and D a dataset with schema \(a1, ..., an ible with the source schema of p. The data-covering operator, denoted by ?d\(p,D responding to all tuples in D represented by p. More formally d\(p,D t.a1, ..., t.an In the previous expression, t.ai denotes a speci?c component of tuple t belonging to D and p.formula\(t.a1, ..., t.an instantiated by replacing each variable corresponding to a pattern data component with values of the considered tuple t Note that, since the drill-though operator uses the intermediate mapping and the data covering operator uses the formula, the covering ?\(p,D D = ?\(p not be equal to D. This is due to the approximating nature of the pattern formula 5.1.4. Cross-over pattern base operators OCP Pattern-covering. Sometimes it can be useful to have an operator that, given a class of patterns and a dataset, returns all patterns in the class representing that dataset \(a sort of inverse data-covering operation Let c be a pattern class and D a dataset with schema a1, ..., an pattern type. The pattern-covering operator, denoted as ?p\(c,D all patterns in c representing D. More formally p\(c,D t.a1, ..., t.an true Note that: ?p\(c,D p,D 6. Related Work Although signi?cant e?ort has been invested in extending database models to deal with patterns, no coherent approach has been proposed and convincingly implemented for a generic model There exist several standardization e?orts for modeling patterns, like the Predictive Model Markup Language \(PMML  eling approach, the ISO SQL/MM standard [2], which is SQL-based, and the Common Warehouse Model CWM  ing e?ort. Also, the Java Data Mining API \(JDMAPI 3] addresses the need for a language-based management of patterns. Although these approaches try to represent a wide range of data mining result, the theoretical background of these frameworks is not clear. Most importantly, though, they do not provide a generic model capable of handling arbitrary cases of pattern types; on the contrary only a given list of prede?ned pattern types is supported To our knowledge, research has not dealt with the issue of pattern management per se, but, at best, with peripheral proximate problems. For example, the paper by Ganti et. al. [9] deals with the measurement 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


