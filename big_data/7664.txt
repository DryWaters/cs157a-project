 000\003  Abstract In this article we are presenting a solution for the problem of combining various i ndexation algorithms in order to acquire a semantic multimedia indexation and to provide responses to the user complex queries. The challenge of this problem concerns the big heterogeneity of the multimedia indexation algorithms and the weak semantic aspect they address. Our solution considers a generic interface for the indexation algorithms, an implem entation as Web services, as well as a semantic description in terms of WSMO \(Web Service Modeling Ontology tionality and orchestration Original contribution of the article concerns the idea of organizing the various multimedia metadata types into a generic structure, used to express the user queries, the algorithms generic interface, as well as the algorithms’ WSMO metadata This approach facilitates the defi nition of algorithm combination rules, and enables the reduction of the multimedia retrieval task to a metadata matching process  Index Terms Multimedia indexing, Semantic Web services algorithm generic interface  I  I NTRODUCTION  ARIOUS domains such as news gathering, TV, banks of resources for commercial or consumer applications collaborative work, video surveilla nce were flooded in the last years by a huge amount of video and multimedia sources now, these domains are in a growing demand of solutions for their management. In this context, LINDO \(Large scale distributed INDexation of multimedia Objects http://www.lindo-itea.eu\ITEA 2-06011 project aims to develop an distributed generic architecture where storage is distributed and where, rather than moving content massively to central processing facilities, the relevant indexation routines are sent to the remote sites to be run locally in each storage environment. Then, only the minimum required content extracted from its clip or coll ection, can be transferred as required The most sensitive aspect of multimedia distributed objects management concerns the multimedia content semantics: the indexation algorithms for images, audio and video content are mainly in charge with low-level multimedia features analysis  Mihaela Brut, Alexandru Ioan Cuza University, 11 Carol I Blv., Iasi Romania, \(phone 4-0746-054774; fax 4-0232-201490; e-mail mihaela@info.uaic.ro Florence Sedes and Ana Maria Mân zat, Institut de Recherche en Informatique de Toulouse, Paul Saba tier University, 108 Route de Narbonne Toulouse, France \(phone: +33-\(0\561 557443; fax: +33-\(0\561-556258; email: florence.sedes@irit.fr ana-maria.manzat@irit.fr However, the combination of multiple indexation algorithms could lead to semantic information about the multimedia content This paper presents a solution, developed inside the Lindo project team, for the problem of combining various indexation algorithms in order to acquire a semantic multimedia indexation and to provide responses to the user complex queries. Because the multimedia indexation algorithms are quite heterogeneous, our soluti on defines a generic interface for them. In order to enhance th e weak semantic they address being in charge especially with multimedia physical features our solution accompanies th e algorithms by a WSMO semantic description of their functionality and orchestration The solution is based on a generic metadata structure defined and used to express the algorithms’ interface and metadata, as well as the user queries. This approach enables the reduction of the multimedia retrieval task to a metadata matching process The paper will present first the generic metadata structure we mentioned above. Then, th e definition of a generic interface for the indexation al gorithms will be presented where the input and output data are expressed in terms of the defined metadata structure. The algorithms’ semantic description with the support of WSMO will be presented further, emphasizing the facility of specifying algorithm combination rules. An arch itectural overview of the multimedia indexation and retrieval process will be exposed further. In the end, conclusions and further work directions will be emphasized  II  MULTIMEDIA  METADATA  ORGANIZATION The metadata provided by the indexation algorithms are expressed into a wide variety of formats, standards, or vocabularies. Many times, a similar metadata is expressed in different formats if it is retu rned by different algorithms Moreover, some algorithms return results in form of numerical, Boolean or string valu es; in these cases, the name and the value of the metadata expressed through such a result is not transparent In order to gain a uniform representation of the metadata produced by the indexation algorithms, we conceived a generic metadata structure, where multimedia features are organized in two levels, and original metadata namespaces are managed into a metadata di ctionary. Because different metadata vocabularies for a certain multimedia type include some common metadata \(under the same or different names A Web Services Orchestration Solution for Semantic Multimedia Indexing and Retrieval Mihaela Brut, Florence Sedes, Ana Maria Mânzat V 
International Conference on Complex, Intelligent and Software Intensive Systems 978-0-7695-3575-3/09 $25.00 © 2009 IEEE DOI 10.1109/CISIS.2009.180 1187 
International Conference on Complex, Intelligent and Software Intensive Systems 978-0-7695-3575-3/09 $25.00 © 2009 IEEE DOI 10.1109/CISIS.2009.180 1187 


 which specify the same multimedia feature, we selected certain vocabularies for expre ssing each particular multimedia feature. However, when a new feature is captured by an algorithm, the namespace of the corresponding vocabulary will accompany the metadata The mentioned two levels are 1  General metadata, valid for all multimedia types filename, location, title, subject, author, producer copyright, keywords, creati onDate, GPSPosition, size 2  Media-specific metadata 000x  The text metadata structure contains all the metadata from the DublinCore \(http://dublincore.org/\ent set, some information that can be extracted from the PDF OpenOffice and MicrosoftOffice documents. Among these elements we could mention language, charCount wordCount, pages, document etc 000x  The image metadata structur e includes the Dublin Core Element Set, the elements of Exif \(Exchangeable Image File Format - http://www.exif.org/\IPTC International Press Telecommunication Council http://www.iptc.org s. For the medical images the DICOM \(Digital Imaging and Communication in Medicine - http://medical.nema.org/\rmat is also added in the structure. Such particular elements are histogramColor, texture, resolution, positionX, positionY etc 000x  The video metadata are expressed through MXF \(SMPTE Material eXchange Format http://ww.smpte-mxf.org standard and include support for video segmentation and object recognition. We could mention segmentVideo GPS, PTZ, dimension1, dimension2 elements 000x  The audio metadata provide support for expressing audio segmentation \(through the MXF/Exif standard SegmentAudio, topic, speaker, episode, scribe elapsedTime, and unit are some audio specific metadata elements  Figure 1: The structure of the metada ta describing the multimedia content  In order to express the semantic information about the multimedia indexed content, we included the <object element inside the metadata st ructure of each media type. For us an object is any particular se mantic information that can be extracted from a media. For this object the structure proposed is presented in Figure 1 III  A  GENERIC  INTERFACE  FOR  THE  MULTIMEDIA  INDEXATION  ALGORITHMS The available algorithms for multimedia documents indexing are characterized by a great heterogeneity concerning their input and output data, as well as implementation details, hosting platforms or architectures. In order to uniformly handling these algorithms such as to produce concrete metadata orga nized according to the above mentioned structure, the algorithms should be structured into a generic interface, where the input and output data are expressed accordingly We discuss below some existing approaches in defining generic interfaces and combina tion rules for the indexation algorithms. In the end of this section, we will present our solution for the generic interface Two approaches oriented towards defining such a generic interface are the following 1  The COMM project \(http://comm.semanticweb.org proposes a Core Ontology for Multimedia enabling to describe the multimedia types and their particular features. In addition, some patterns for indexing operations are considered, including the semantic annotation pattern and the indexation algorithm pattern which propose such generic interface The multimedia indexation algorithms are considered as Web services, and their results are represented in MPEG-7 form e lack of a formal semantics of MPEG-7 language, some semantically equivalent results could be represented through some syntactically different MPEG-7 descriptors. The COMM project developed the Core Ontology for Multimedia in order to provide a formal semantics to MPEG-7. Their framework relies on domainspecific ontologies for representing the real world entities depicted in multimedia content Our approach differs from the COMM project goal in two aspects: we do not limit the indexation algorithms to a result in MPEG-7 format \(our generic metadata structure integrate all kinds of multimedia formats, managed through a metadata dictionary\ut we try to limit their scope to detection of perceivable content of the multimedia objects \(instead of a more accurate semantic desc ription through domain-specific ontology concepts\The reason of this limitation concerns our desire to exploit the semantics detected by the algorithms themselves, without considering a manual ontology-based annotation 2  The Web Consortium’s Algorithm representation use case   whi c h proposes al gori t h m ont ol ogy t o record and uniformly describe available algorithms for image analysis The support for describing the conditions for the algorithm combination is integrated into the generic interface itself: the algorithm ontology includes relations for describing the algorithms input and output, but also relations for describing the preconditions to be fulfilled for algorithm execution and the effects of their execution. Among the possible applications of this algorithm representation, the composition of web 
1188 
1188 


 services to automatically anal yze media based on user goals and preferences is considered As could be noticed, the both approaches are focused on extending the black box of the indexation algorithms with a well defined structure and semantic of the algorithms’ input and output data, as well as of their functionality In order to separate the description of the input/output data of the indexation algorithm from the semantic description of its functionality and orchestration rules, we include the former inside the generic interface, while the second will be expressed in terms of WSMO The indexation algorithm generic interface is characterized by 000x  input concerns a particular multimedia object. Because the general metadata structure presented in the section II includes all the necessary information to locate and define a multimedia object, we represent the algorithm input through this structure 000x  output could be a multimedia object, but also a numeric Boolean or string value. For representing this output, we adopt the metadata elements describing the indexation result, as media-specific metadata, according the format also presented in the section II. Concrete examples will be provided in the next section This algorithm interface is generi c in the sense it is able to support any kind of indexation algorithm because the input and output data of such algorithm could be mapped into the described format 000x  for any multimedia input object, the incorporated metadata could be extracted and organized into the generic structure, it is just a format conversion matter 000x  the conversion of output data into the proposed metadata structure is a more complicat ed issue, addressed for the moment through human intervention: the semantics of a numeric, Boolean or string result is not transparent to the machine, but such result illustrates a value for a certain multimedia metadata. As consequence, when a new algorithm is re-written accordi ng the generic interface, the coder is in charge with expressing the possible results through corresponding values for a certain media-specific metadata Concerning the implementation of the algorithms with such generic interface, the presented best practices recommends the Web services approach to be  good solution due to the Web services main characteristic s: autonomy, composability choreography, discoverability [2 r so lu tio n ex ten d s th is approach with a semantic description of the algorithms expressed through WSMO. We benefit by WSMO Studio  http://www.wsmostudio.org/\which provides support for the services implementation, as well as compatibility with the framework for algorithms’ functionality semantic description and orchestration rules  IV  A  SEMANTIC  ORIENTED  SOLUTION  FOR  INDEXING  ALGORITHMS  COMBINATION A  Combining Multiple Indexation Algorithms Usually, a user complex query involves more than a single algorithm to be combined and sequentially run over the multimedia content. In the multimedia-indexing domain, the indexation process for responding to such complex queries is produced by applying a sequence of indexation algorithms in a given order. In order to obtain proper algorithms sequential combination, a set of rules should be established, defining conditions for each algorithm We consider as concrete example the following user complex query: “locate the vide o files where Tim Berners Lee speaks about the semantic Web”. In order to build query results, the combination of mu ltiple algorithms is necessary such the recognition of Tim Berners Lee, the English Language detection, specific wo rds recognition, etc. Some rules applicable for this example include: words recognition could be applied if the E nglish Language detection was previously performed; also, the English Language detection could be performed only after the human presence detection e.g. through the recognition of Tim Berners Lee. These rules could be specified as preconditi ons specific to each algorithm In order to establish a algorithm sequential combination for a certain user query, a compatibility checking should be performed based on the corresponding algorithm preconditions For expressing such precond itions, we will accompany the indexation algorithms by their semantic description provided with the support of WSMO As example, we will consider the necessity of combining some algorithms exactly in the required sequence For the above user query example, we provide below a short description of the output data for each algorithm to be called \(the input data has always the same structure, as already presented a  Tim Berners Lee presence detection: a successful detection will return an <object> element including the temporal information object name>man</name keywords>Tim Berners Lee</keywords timeStamp begin 10:33:00” end 11:33:00 object b  The English Language Detection will be applied for the video sequences successfully detected by the previous algorithm and will locate the sub-sequences where English language is present in the audio part. The structure  of the returned metadata follows audio_metadata lang="en Speaker id="sp1" name="Tim Berners Lee"  type="male timeStamp begin=”10:40 00” end=”10:50:00 audio_metadata c  Recognition of the “semantic Web” phrases will be also applied over the sub-sequences previously located, and 
1189 
1189 


 the returned metadata will further specify the relevant sub-sub-sequences object name>word</name keywords>semantic web</keywords timeStamp begin 10:44:00” end 10:46:00 object In a further section, we de scribe the WSMO solution for locating and combining in such sequence the corresponding algorithms For selecting the suitable algorithms to be invoked in order to respond to a user query, we assume the query is preprocessed \(e.g. by a Querying module, based on natural language processing techniques\ded into a metadata structured manner. The above que ry should be received in the following form object name>man</name keywords>Tim Berners Lee</keywords  object audio_metadata lang="en Speaker id="sp1" name="Tim Berners Lee" type="male audio_metadata object name>word</name keywords>semantic web</keywords  object  B  Existing Approaches for Web Services Semantic Description  Considering multimedia indexation algorithms implemented as Web services \(as the best practices recommend\and modeled with the generic interface described in the previous section, we discuss and present further a solution to combine various algorithms by assigning them a semantic description Usually, the Web services are located, invoked and combined inside a certain app lication according the statements established by the application’s human coder. He/she assumes the task of Web services orchestration specifying how each service achieves its capability by making use of other Web services In the context of the Seman tic Web age, some approaches aim to make more machine-processable the Web content accessible through Web services usage. They are focused on assigning semantic annotations to Web services and on exploiting these in order to automate the tasks of Web services discovery, composition and invocation. Thus, the Web services could be integrated inside multiple, distributed applications The OWL Services Coalition s the vocabulary defined by OWL-S \(OWL-based Web Service Ontology\in order to provide semantic annotations of services. An insufficiency of OWL-S from our problem point of view concerns the lack of support fo r describing the conditions for combining a set of Web services METEOR-S http://lsdis.cs.uga.edu/Projects/METEOR-S is a technology centered approach, aiming to integrate the multiple Web services technologi es, but it does not provide a conceptual model for Web services description IRS-II \(Internet Reasoning Service is a framework for semantically describing and for executing Web services http://kmi.open.ac.uk/technologies/irs  With common roots as IRS-II and more focused on the semantic description of Web services WSMO provides the conceptual underpinning and a formal language for seman tically describing all relevant aspects of Web services accessible through a Web service interface in order to facilita te the automatization of discovering, combining and invoking services over the Web We adopt WSMO because it provides support for all issues related to our problem, especially regarding the usage of Web service semantic description in order to check the possibility of multimedia indexing algorithm combination  C  Using WSMO for Web Services Semantic Description and Orchestration WSMO Web Service Modeling Ontology  four main elem 000x  ontologies providing the terminology used by other WSMO elements 000x  Web services providing access to services that, in turn provide some value in some domain 000x  goals   representing user desires 000x  mediators dealing with interoperability problems between different WSMO elements WSMO also provides a logical language for defining formal statements in WSMO We are focusing on WSMO support for describing Web services, which enables to depict the functionality of the Web services \(through class Capability as the orchestration of the Web service \(through one ore more Interfaces  The capability of a Web service is expressed by the state of the world before the Web servi ce is executed and the state of the world after successful Web service provision, and is defined by the following class Class capability hasNonFunctionalPrope rty type nonFunctionalProperty importsOntology type ontology usesMediator type oMediator hasSharedVariables type sharedVariables hasPrecondition type axiom hasAssumption type axiom hasPostcondition type axiom hasEffect type axiom The set of non-functional properties are mainly used to describe non-functional aspects of the Web service such as creator, creation date, natural language descriptions, etc 
1190 
1190 


 Imported Ontologies allow a modular approach for ontology design when a more formal semantic description is intended for a particular problem domain. Some specific mediators are used when an alignment of the imported ontologies is necessary Shared Variables  represent the variables used to define the preconditions, postconditons, assumptions and effects: the accomplishment of the preconditions and assumptions implies the postconditions and effects production. For our problem each multimedia metadata consider ed in the two levels generic metadata structure constitutes a shared variable. As example for the above presented three output examples, the following shared variables could be considered: ?object.name object.keywords, ?obj ect.timestamp object.timestamp  a udi o_m et adat a[l a ng  audio_m audio_metadata.Speaker[nam audio_m audio_metadata.timestamp audio_metadata.timestamp Preconditions specify what information a Web service requires in order to provide its value. As preconditions we will specify the particular metadata a multimedia object requires in order to be processed by the current indexation algorithm. If this particular metadata is not available, then the particular algorithm\(s\to produce it will be located and will be executed first We provide below the preconditions for expressing the rules for our particular example. The first precondition concern the English Language detection algorithm, which could be performed only after the human presence detection e.g. through the recognition of Tim Berners Lee  precondition axiom preconditionEnglishDetection definedBy exists ?name, ?keywords object[name hasValue ?name keywords hasValue ?keywords memberOf object and object.name=”man” and object.keywords=”Tim Berners Lee  The second precondition concerns the words recognition algorithm, which could be applied if the English Language detection was previously performed  precondition axiom preconditionWordRecognition definedBy exists ?lang, ?name audio_metadata lang hasValue ?lang Speaker[nam n am e memberOf audio_metadata and audi o_m audio_metadata. Sp eaker[nam Berners Lee  Preconditions’ specification is enough for our particular problem \(suitably combining indexation algorithms However, the WSMO specification provides also support for expressing 000x  Assumptions describe the state of the world which is assumed before the execution of the Web service, but is not necessarily checked by the Web service. For our example, we could assume a video file with Tim Berners Lee speaking about the Semantic Web exists is correct located and could be properly processed 000x  Postconditions describe the state of the information space that is guaranteed to be reached after the successful execution of the Web service The postcondition should not reproduce the algorithm’s output format and content, but just its essential information eventually related to the input information. For the first example, the postcondition could \(facultatively\ emphasis the name of the speaker, which should be the same as those mentioned in the precondition postcondition axiom postconditionEnglishDetection definedBy exists ?name name memberOf a udio_metadata. Speaker[nam name = ?object.keywords 000x  Effects should be mentioned, for example, when the multimedia object suffers an alteration, such as the image color segmentation, or video color enhancement  D  The Proposed Service Oriented Architecture for Multimedia Indexing and Retrieval As could be noticed from the previous discussion, in order to accomplish a complex multimedia indexation task three main collections should be handled 000x  multimedia collection when a new object is added to this, the general metadata are extracted \(by applying a set of implicit indexation algorithms, denoted as implicit extractors” in Figure 2\ncluded inside the metadata collection 000x  metadata collection metadata associated with each multimedia object is structured on the two levels general and media-specific The first level is obtained as described above, while the second is enriched with every new object indexation process 000x  indexation algorithms collection when a new algorithm is added, it is processed in two ways o  it is re-written as a Web service where the input and output data are converted to the generic interface o  its combination rules are specified through WSMO Capability class 
1191 
1191 


 Because the user queries could be reduced to the retrieval of multimedia objects having some particular features, ideally this queries should be expressed through the metadata illustrating these features, as in the example provided in Section IV.A. We suppose this ideal case, where the query conversion task belongs to a querying module separated by the indexation module focused by our discussion   Figure 2: The service-oriented arch itecture for multimedia indexation and retrieval  For responding to the user queries, a matching process between the content representation \(the metadata format\and the query representation \(available in the same metadata format\plished The querying module should perform the results ranking and should transmit if the results are or not acceptable \(pertinen t pertinent, the indexation module should accomplish a new multimedia objects indexation, by using additional algorithms This process consists in some steps 000x  Analyze the query representation in order to establish the list of additional indexation algorithms \(denoted as explicit extractors” in Fi gure 2\ to be applied. The output format of these algorithms should include metadata specified by query 000x  Consult the WSMO metadata associated with these algorithms in order to establish the cascade for the previous extractors list 000x  Analyze the query representation in order to establish the multimedia objects sub-collection possibly considered by the user query 000x  Index this collection based on the previous established extractors cascade In Figure 2, the multimedia indexation process is illustrated as well as the matching process performed in order to respond to a user query. All the tasks represented through circle forms are conceived as Web services in this distributed architecture V  CONCLUSIONS  AND  FURTHER  WORK This paper presents a solution for the problem of combining various indexation algorithms in order to acquire a semantic multimedia indexation and to pr ovide responses to the user complex queries. Because the multimedia indexation algorithms are quite heterogeneous, our solution defines a generic interface for them. Becau se the indexation algorithms are in charge especially with multimedia physical features detection and analysis, our solution try to enhance the weak semantic they address, acco mpanying the algorithms by a WSMO semantic description of their functionality and orchestration. As original contribution, the solution is based on a generic metadata structure defined and used to express the algorithms’ interface and WSMO metadata, as well as the user queries. This approach enables the reduction of the multimedia retrieval task to a metadata matching process Service oriented architecture is provided in order to illustrate the proposed solution As further work, we intent to adopt and integrate the proposed solution into the dist ributed multimedia environment developed by the Lindo project where multimedia indexation and retrieval is coordinated from a central server, but is effectively performed at the level of local nodes, where various types of multimedia collections are located R EFERENCES    Arndt, R., Troncy, R., Staab, S., Hardman, L. Vacura, M., “COMM Designing a Well-Founded Multimedia Ontology for the Web”, in K Aberer et al. \(Eds Proceedings of ISWC/ASWC 2007 LNCS 4825, pp 30–43, Springer, 2007   Erl, T Service-Oriented Architecture: Concepts, Technology, and Design Prentice Hall PTR, 2005   Haidar, B., Joly, P., Haidar S., “A Graph-based Approach to Automatically Chain Distributed Mu ltimedia Indexing Services”, in Proceedings of 9th Int. Conf. on Internet and Multimedia Systems and Applications ACTA Press, Switzerland, 2005   Roman  D., Keller, U., Lausen, H., de Bruijna, J., Lara, R., Stollberg M., Polleres, A., Feier, C., Bussler, C., Fensel, D., “Web Service Modeling Ontology Applied Ontology 1 \(2005\ pp. 77–106, IOS Press 2005   Tzouvaras, V., Troncy, R., Pan, J., “Multimedia Annotation Interoperability Framework W3C Technical Reports Boston, 2007 http://www.w3.org/2005/Incubato r/mmsem/XGR-interoperability  
1192 
1192 


Expression yields true, and a False Stencil which is selected when the Conditional Expression yields false A sample Stencil is shown in Figure 6 and described in section 4.3 The AAP phase uses the token list of the document to resolve the Stencil, by resolv ing the elements in the Stencil in order. String elements are directly emitted to the output. Future Expressions are evaluated, the result of which then converted to String and sent to the output. For each Branch element, depending on the evaluation result of its Conditional Expression, either the True Stencil or the False Stencil is selected and recursively resolved Note that Future Expressions can only take document token list as input and doesn’t have access to XSLT variables. As a result, if an expression depends on an XSL variable whose value can’t be determined at PreP phase \(because its value is conditionally updated by a conditional element\no transformation optimization for this combination of stylesheet and structure 3.3.5 Compression for Efficient Transmission Efficient transmission of XML documents is particularly important in mobile environments Structure recurrence can be exploited for compression and efficient transmission by avoiding the redundant transmission of structural information and by compressing recurring test nodes and attribute values  Assuming sender S first sends document A to receiver R and later sends to R document B which has the same structure as document A, if S knows that R has locally kept the structure information for A, then S only needs to send the structure identification of B, along with the text node and attribute values of B. Efficiency is achieved by replacing the structure of document B with the \(much shorter\identification of the structure of the document  Further, the text node and attribute values in document A has a 1-to-1 mapping with those in document B. If sender S is aware that receiver R also keeps text node and attribute values of document A, then when S detects that a value in B is the same as the corresponding value in A, S only needs to signal R to reuse the value in A, instead of retransmitting it The combination of the two offers potentially high compression ratio with very low computation overhead Note that it is not necessary for the sender and the receiver to be both aware of the existence of the compression. For example, in wireless mobile environments, the gateway of a mobile device can serve as the proxy between the outside world and the mobile device: XML documents are transmitted in compressed form between a mobile device and its gateway, while the gate way and the outside world communicate in usual text form 4 IMPLEMENTATION We have implemented in Java a XML tokenizer, a DOM-style parser, and a XSLT processor, based on Structure Encoding. The DOM-style parser is implemented as an exten sion to kXML, while the XSLT processor is implemented as an extension to XT 4.1 Tokenizer The tokenizer operates in three different modes  The Swift mode, which is activated when the incoming document contains sender assigned structure ID, and that cached token list for the structure is found  The Hash mode, which is activated when the incoming document does not contain sender assigned structure ID The hash function we use is an implementation of MD5 which generates 128-bit digests  The Nature mode, which is normal XML tokenization and is activated under all other conditions Fi g ure 6. An Exam p le of Transformation Stencil 
317 


Under Swift mode, the tokenizer checks each token in the token list, and takes different action depending on the types of the token  If it is a Text token, it reads a text string \(delimited by and may contain entity references\ the document character stream, and uses the text string to replace the value of the token  If it is a Start Element token, the tokenizer first skips a number of characters from the document character stream. The exact number of characters is determined by the length of the name of the element. It then enters a loop that reads attributes of the element. Within the body of the loop, it first checks if the next token is Attribute Name. If it is not, then the processing for the Start Element token completes. Otherwise, it skips the name of the attributes, reads the value of the attribute and updates the value of the next token \(which must be of type Attribute Value  In all other cases, the tokenizer simply skips a number of characters. The exact number of characters is determined by the type of the token and the length of the value of the token In the Nature mode, the tokenizer reads in XML constructs, makes sure that they are syntactically correct, and then converts them into tokens. Operations in Hash mode are mostly the same, except that the tokenizer also needs to send structure characters to the hash function for ID generation 4.2 Structure Encoding Based Parser Figure 7 shows a structure encoding based XML parser. It has three components: The Tokenizer as described above  The Structure Manager, which contains a cache manager that manages the reuse of document trees, and an optional Structure Optimizer, which does tree optimizations described in section 3.3.3  The Controller, which includes an interface with the kXML DOM parser. It also includes a Value Loader which does text node and attribute value reassignments The Controller falls back to kXML DOM parser, by passing to it a SAX parser implemented on top of the tokenizer, when it is unable to get a reusable document tree from the Structure Manager. Each document structure has a mapping table that associates a text node or an attribute value object with a token in the token list. The Value Loader in the Controller uses this mapping table to quickly grab text or attribute values from incoming document’s token list, and reassigns them to corresponding text node or attribute value objects. Figure 8 illustrates using Structure Encoding based parser to parse the book record document shown in Figure 1 4.3 Structure Encoding Based Transformer Figure 9 shows the components of a Structure Encoding based transformer hosted in the XT XSLT processor. Of its four components, the tokenizer and the Structure Manager are the same as in the parser The two different components are the Pre-processor and the Transformation controller Pre-processor carries out the PreP phase of structure encoding based transformation using stylesheet and document structure to generate transformation Stencil Within the document structu re, the text and attribute nodes are changed to objects of a special type, which upon access, throws a special exception which uses an integer index to indicate which text node or attribute is accessed. The structure is then treated as a regular document and fed to XT along with the stylesheet. We have also extended XT, so that any expression that may catch the above-mentioned special exception is extended into a Future Expression, which can be evaluated by taking only t oken list as input. A Future Expression in turn throws itself as a special exception which is then either caught by another Future Expression or, eventually, by an action. An action generates a result segment, which can be one of the following  A string, in the case when there is no special exception caught  A Future Expression, when one or more special exceptions are caught, and  A Branch, when the action is an IfAction generated from an xsl:if or xsl:choose XSL element.The Stencil Resolver in the Transformation Controller is called in the AAP phase after the source document arrives and a transformation is requested. It retrieves the Stencil using the stylesheet and the document structure as key, then uses the token list for the source document as input to Figure 7. A Structure Encod ing Based XML Parser 
318 


solve the Stencil, using methods described in section 3.3.4 Figure 10 shows a simple XSL stylesheet used to convert the book record document shown in Figure 1 into HTML format. Note th at it contains an xsl:if element, which tests if the “special” price is greater than zero, and if the condition is true, it outputs an onsale price. Figure 11 shows the result of the transformation Figure 6 shows the Stencil generated by the PreP phase, where i  i th entry in the mapping ta ble \(see section 4.2 Stencil” contains a Branch, which has a “GreaterThan Number Relation” that can’t be pre-evaluated, as it depends on the number value of a Text or Attribute Value token. The True Stencil of the Branch is a simple Stencil, while its False Stencil is empty 4.4 Structure Encoding Based Compression The implementation of Structure Encoding based compression for transmission is straight forward. The sender and receiver both maintain an encoding table Each entry of the table is a mapping from a structure ID to a cached document structure as well as a list of template values for attr ibutes and text nodes associated Figure 8  Example of Structure Encoding Based Figure 9. Components of a Structure Encoding Based XSLT Processor xml version="1.0" encoding="UTF-8 xsl:stylesheet version="1.0 xmlns:xsl="http://www.w3.org/1999/XSL/Transform xsl:template match="book html head title><xsl:value-of select="title"/></title head body b><xsl:text>Author: </xsl:text></b><xsl:value-of select="author"/><br b><xsl:text>Price: </xsl:text></b><xsl:value-of select="price xsl:if test="special > 0 b><xsl:text> On Sale For: </xsl:text><xsl:value-of select="special"/></b xsl:if><br b><xsl:text>Cover: </xsl:text></b><xsl:value-of select="size/@cover"/><br b><xsl:text>Publisher: </xsl:text></b><xsl:value-of select="publisher"/><br b><xsl:text>ISBN: </xsl:text></b><xsl:value-of select="isbn"/><br br body html xsl:template xsl:stylesheet Figure 10. Transforming the Book Record into Figure 11. Result HTML File from Sample Transformation html head title>Book on XML</title head body b>Author: </b>Foo<br b>Price: </b>49.99<b> On Sale For: 34.99</b br b>Cover: </b>Paperback<br b>Publisher: </b>ABC Inc; 1st edition \(Jan. 2002\<br b>ISBN: </b>1111111111<br br body html 
319 


Table 2. Measurement Results \(all numbers are in milliseconds Tokenization Parsing Transformation 100% Cache Hit 0% Cache Hit 100% Cache Hit 0% Cache Hit Document Nature Swif t Hash kDOM Sender Assign Receiver Assign Sender Assign Receiver Assign XT Sender Assign Receiver Assign Sender Assign Receiver Assign axis 7.1 4.8 9.5 11.4 5 9.6 10.7 13 36.3 6.9 11.4 36.3 39.3 Backwards 33.3 7.4 43.4 60.7 7.6 43.7 56.1 66.6 232.2 12.9 49.5 232.7 252.4 chart 18.5 7.7 24 32.1 7.8 24.9 30.1 36.3 117.8 21.9 39.7 118 129.3 book 15.1 8.6 18.2 24.4 8.8 18.5 21.1 25.5 48.3 11.5 20.9 48.9 55.2 game 33.1 7.2 43.2 60.7 7.6 43.5 56.4 67.3 92.2 8.9 45.1 92 105.5 midsummer 1990 913.4 2311 5 4686.9 997 2332.5 4012.8 4360.8 7398.5 1475 3068 7268.1 7555.2 nitfstylized 67.3 25.1 76 105.3 27 77.3 93.8 107.7 178.3 34.4 85 188.5 204.3 recipes 164.3 99 180.1 249.2 107.6 184.8 209.9 234.3 574.4 144.3 224.1 566 585.1 sort 129.8 32.2 159.8 248.2 33 163.6 242.3 281.5 1094.9 55.6 188.1 1121.9 1174.4 sp 36.2 21.5 40 53.7 23.2 40.6 45.5 51.3 122.8 28.2 45.2 126.6 138.9 total 18.6 6.8 24.5 32.4 7.4 25 30.6 36.2 53.9 7.6 25.5 54 61.5 trend 36.5 10.8 41 62.1 10.4 41.1 66.3 70.9 2249.3 56.9 89.1 2249.3 2279.5 wai 9.1 4.5 12.5 14.2 4.6 12.6 13.4 17.1 96.4 7.7 15.8 99.9 105 with the structure \(Note that document structures and template values can be flushed out of the cache and reconstructed from files stored in persistent storage The first document of a given structure will be sent uncompressed. But an entry will be added to the encoding tables on both sender and receiver sides, with the attribute and text valu es of this first document used as template values. A following document of the given structure will be encoded as a two-byte integer indicating the index of th e structure in the encoding table, followed by the list of attribute and text string values. A special 1-byte symbol replaces any value that is an exact match of its corresponding template value Such compression is used between a mobile handset and its service gateway in cellular environments. Note that compression happens on a document send from the handset, through the gateway, to a server on the Internet, as well as on a document received by the handset through the gateway Table 1. XML Documents Used in the Experiments, and Th eir Corresponding Stylesheets Document Size in KB Stylesheet axis 0.38 Tests XPath selection along the different axes backwards 2.62 Reverses order of elements using the document used in game chart 1.29 Generates an HTML chart of some sales data book 1.21 Convertin book record to HTML game 2.62 Produces a HTML table of the data midsummer 146 Converting the play to HTML nitf-stylized 5.79 NITFML to HTML recipes 16.7 Converting Recipes in XML to HTML sort 10.3 Sorting input tree according to element name sp 3.53 Web site construction kit total 1.31 Reports on sales data trend 1.9 Computes trends in the input data wai 0.58 Schematron validator for WAI docs 
320 


5 EVALUATION Evaluation experiments described in this section are conducted on a TI OMAP 1511 Innovator device which runs in frequencies up to 200MHz. It has 32MB ROM as well as 32MB RAM. The OS used is a version of embedded Linux, and the JVM used is Intent from TAO Group The source documents and stylesheets used for evaluation are randomly selected from Sarvega’s XSLTBench and DataPower’s XSLTMark [17  after initially excluding some unrealistic transformations. We select these two benchmarks as they are influential indust ry benchmark and that they both support XSLT benchmarking. Note that some of the large source documents are less likely to have recurrent structures in real-world applications. We included them regardless as they might help us understand the implication of document size on system performance. Table 1 lists sizes of sample documents and the transformation stylesheets used for them. Table 2 lists the raw numbers used in following discussions Documents are fully read into memory before any measurement starts to minimize external disturbance Warm-up runs are always conducted prior to real measurement runs 5.1 Tokenization Figure 12 compares the speed of Swift mode and Hash mode against the speed of Nature mode tokenization The Swift mode is by far the fasted, having a median speedup of 2.40 over Nature mode. The median speed of Hash mode is about 81% of that of Nature mode which means a median hashing overhead of about 23.5%. In the worst case, this overhead is about 37.4 for wai  5.2 DOM-style Parsing Figure 13 and Figure 14 compare speed of Structure Encoding based parsing against that of kXML parsing for both Receiver ID Assigning and Sender ID Assigning. In Receiver ID Assigning, the underlying tokenizer always uses Hash mode, while in Sender ID Assigning, the tokenizer uses Swift mode when cache hit ratio is 100% \(i.e., structure of incoming document is always in cache\ode when cache hit ratio is 0 Figure 13 shows the results for the ideal case when cache hit ratio is 100%: the median relative speed is 4.34 when Sender ID Assigning is used, and is 1.35 when Receiver ID Assigning is used. The best case is in backwords and game which use the same source document\to 8 under Sender ID Assigning Figure 14 shows the results for the worst case when cache hit ratio is 0%: the median relative speed is 1.07 when Sender ID Assigning is used, and is 0.90 \(or an overhead of %11.1\ID Assigning is used. The worst sample is wai where the overhead is about 20.5% under Receiver ID Assigning. The reason Figure 12. Comparing Performance of Tokenization Modes Figure 14. Comparing Structure Encoding Based XML Parsing and kXML Parsing \(Cache Hit Ratio = 0 Figure 13. Comparing Structure Encoding Based XML Parsin g and kXML Parsin g Cache Hit Ratio 100  
321 


that Structure Encoding is a little faster than kXML even when cache hit ratio is 0 is likely because of the fact that, while we use kXML for tree building, we modified it slightly to interface it with our tokenizer which resulted in differences in execution traces and slight differences in runtime performance when execute in JVM Overall, these two figures demonstrate that Structure Encoding based parsing provides substantial speedup when structure recurrence is frequent, and incurs low overhead when such recurrence is rare 5.3 Transformation Similarly, Figure 15 and Figure 16 compare transformation speed of Structure Encoding based XSL processor against that of XT for both Sender ID Assigning and Receiver ID Assigning. Again, under Receiver ID Assigning, the underlying tokenizer always uses Hash mode, while under Sender ID Assigning, the tokenizer uses either Swift mode or Nature mode Figure 15 shows the results for the ideal case when cache hit ratio is 100%: the median relative speed of Structure Encoding based implem entation is 5.38 when Sender ID Assigning is used, and is 2.72 when Receiver ID Assigning is used. The best case is trend  where the relative speed is over 39 for Sender ID Assigning, and over 25 for Receiver ID assigning Figure 16 shows the results for the worst case when cache hit ratio is 0%: the median relative speed is 0.996 when Sender ID Assigning is used, and is 0.918 or an overhead of 8.9 is used. The worst sample is nitf-stylized where the overhead is about 14.6% under Receiver ID Assigning These two figures show that Structure Encoding based XSL processing offers even higher potential speedup than parsing, yet incurs lower overhead in worst cases This is understandable as, typi cally, a large portion of operations in transformation is for costly structurerelated operations, which we have pre-processed offline. Note that, as long as structure recurrence is frequent, the potential improvement is very high \(up to a median relative speed of 2.72\even if the receiver has to calculate structure ID through hashing 5.4 Compression We conducted an experiment to examine the effectiveness of Structure Encoding based compression. In the experiment, for each sample document, we randomly changed the values of 3 rd 8 th  13 th i.e., one in every 5, starting from the 3 rd  attribute and text node values. Thus our experiment Figure 16. Comparing Structu re Encoding Based Transformation and XT  Cache Hit Ratio 0  Figure 15. Comparing Structu re Encoding Based Transformation and XT  Cache Hit Ratio = 100  Table 3. Compression and its effect on tokenization speed \(Assuming changes in 20% of the text and attribute values Document Size in KB Comp Size Byte Natur e ms Swift ms Comp  ms axis 0.38 48 7.1 4.8 0.8 backwards 2.62 81 33.3 7.4 1.8 chart 1.29 91 18.5 7.7 1.3 book 1.21 129 15.1 8.6 1.1 game 2.62 81 33.1 7.2 1.8 midsummer 146 20760 1990 913.4 111.7 nitf-stylized 5.79 564 67.3 25.1 4.0 recipes 16.7 2355 164.3 99 11.4 sort 10.3 752 129.8 32.2 8.8 sp 3.53 554 36.2 21.5 2.6 total 1.31 91 18.6 6.8 1.3 trend 1.9 169 36.5 10.8 2.7 wai 0.58 15 9.1 4.5 0.6 
322 


assumes that around 20% of the attribute and text values are different from the template values Table 3 lists results from this experiment. The third column of the table shows the number of bytes transmitted for each sample document, while the 6 th column shows the time used to construct tokens from the compressed document. The table shows that, under above-described setup, the amount of data transmitted are reduced to 2.6% to 15.7% of the original size second column, listed in KB time is reduced to 5.4% to 11.3% of the Nature mode tokenization time, or 11.5% to 25% of the Swift mode tokenization time 5.5 Discussion Since in our implementations, we make changes to base systems \(KXML and XT changes are required to implement Structure Encoding performance differences demonstrated in this section are caused by differences in techniques rather than differences in implementations Our experiments clearly show that Structure Encoding can, potentially, greatly improve the efficiency of XML processing with relatively low penalty for worst cases. The worst case scenario happens when a receiver uses hash function to identify the structure of a document, only to find out that the document structure is not in cache. The penalty it pays for such worst case scenario, 11.1% for DOM-style parsing and 8.9% for transformation, can easily be compensated by future structure recurrence. Such cache-miss penalty is negligible when sender-assigning scheme is used Systems that are conscious of such client-side penalty can let the sender or anyone in the middle of the transmission path to assign ID without altering the semantics of the message or document Although we didn’t measure the impact of compression on parsing and transformation, it can be inferred from tables 2 and 3. For example, in the book case, with tokenization time reduced from 8.6ms Swift mode\s \(compressed\e will likely be further reduced from 8.8ms to around 1.3ms compared with 24.4ms for KDOM\Similarly transformation time may be further reduced from 11.5ms to around 4.0ms \(compared with 48.3ms for XT\Encoding based XML compression offers additi onal, significant performance improvements for XML parsing and transformation in mobile environments, where closelycoupled proxies commonly exists. With compression turned off, Structure Enc oding is fully compatible with Web specifications and can be used between any two Internet hosts, and it still offer very significant performance improvements when there is structure recurrence Without compression, for documents with recurrent structures, tokenization and structure hashing cost dominates the overall cost for parsing, and is the major part of the cost for transformation. Tokenization and structure hashing however are relatively simple operations that may be implemented in hardware with low cost. \(In fact, some mobile chipsets already have hardware implementation of hash functions such as MD5 for security purposes implementation can reduce tokenization and structure hashing cost to a fifth of the current cost, then there will be additional substantial improvement for the parsing and transformation of most of the documents used in our experiments In our system, a document havi ng a structure slightly different \(e.g., added a new element\from a previous structure will not be able to reuse the structure processing result of the previous structure. However our system pays off as long as this new, slightly different structure recurs in future documents 6 CONCLUSION, LIMITATIONS, AND FUTURE WORK In this paper we motivated exploiting structure recurrence to speedup XML processing in mobile environments, by reduci ng structure related transmission and processing costs. We presented the concept of Structure Encoding, and described approaches to quickly identifying recurring structures including one using collision-resistant hash function We explained in detail how to use structure encoding to speedup XML transmission, tokenization, treebuilding, and transformation. We described our implementation of st ructure encoding based tokenizer DOM parser \(based on kXML\processor \(based on XT\pression scheme. Our experiments conducted on a mobile test-bed demonstrated dramatic performance improvement in the presence of structure recurrence and low overhead otherwise. In ideal cases structure encoding offers speedups of up to 7 for parsing and over 38 for XSL transformation, and up to 97.4% in size reduction when 20% of the text and attribute values change Structure encoding, however, is not applicable to all XML applications. Rather, it is more applicable to data-centric XML processing than to document-centric XML processing. A user randomly browsing Web pages is not likely to have high structure recurrence probabilities 
323 


Our current implementa tion does not support documents with “variable-le ngth arrays” – lists of identically structured elements with non-fixed lengths Otherwise identically structured documents with different array lengths are currently considered as having different structure We are currently working on supporting “variablelength arrays” to extend the applicability of Structure Encoding. We are also looking at provide similar, but less aggressive, optimization support for schemaconforming documents REFERENCES  Nokia Web Services – Helping Operators Mobilize the Internet Http://www.projectliberty.org/resources/whitepapers/W S_Operators_A4_0408.pdf  The SAX Project. http://www.saxproject.org  XML Pull Parsing. http://www.xmlpull.org  W3C Document Object Model http://www.w3.org/DOM  WAP Binary XML Content Format http://www.w3.org/TR/wbxml  Efficiency Structured XML. http://www.esxml.org  VTD-XML. http://vtd-xml.sourceforge.net  XSLTC Documentation. http://xml.apache.org/xalanj/xsltc  kXML. http://www.kxml.org  Liefke, H. and D. Suciu. XMill: An Efficient Compressor for XML Data. In Proc. of the ACM SIGMOD Conference on Management of Data. May 2000  Liu, L., C. Pu, and W. Tang. WebCQ: Detecting and Delivering Information Changes on the Web" In the Proceedings of International Conference on Information and Knowledge Management \(CIKM  The XT XSLT processor http://www.blnz.com/xt/index.html  Sarvega,Inc. http://www.sarvega.com  DataPower Technology, Inc http://www.datapower.com  Rax Content Processor http://www.tarari.com/rax/index.html  The Sarvega XSLT Benchmark Study, Sarvega Inc http://www.sarvega.com/xslt-benchmark.php  XSLTMark http://www.datapower.com/xmldev/xsltmark.html  Eisenhauer, G. and L. K. Daley. Fast Heterogenous Binary Data Interchange. In Proceedings of the 9th Heterogeneous Computing Workshop \(HCW 2000 90-101  Bustamente, F., G. Eisenhauer, K.Schwan, and P Widener. Efficient Wire Formats for High Performance Computing. In Proceedings of High Performance Networking and Computing Conference, 2000 SC’2000  Toshiro Takase, Hisashi Miyashita, Toyotaro Suzumura, and Michiaki Tatsubori, An Adaptive, Fast and Safe XML Parser Based on Byte Sequence Memorization. In Proc. of WWW’2005  XML-RPC. http://www.xmlrpc.com  Open  Mobile Alliance http://www.openmobilealliance.org  RSS 2.0 Specification http://blogs.law.harvard.edu/tech/rss  Open Mobile Alliance http://www.openmobilealliance.org  M ogul, J., F. Douglis, A. Feldm an, and B Krishnamurthy. Potential benefits of deltaencoding and compression for HTTP In Proc SIGCOMM’97 1997  Spring, N. T., and D. W e therall. A protocolindependent technique for eliminating redundant network traffic In Proc. SIGCOMM’00 2000  Chiu, K., and W  Lu. A Com piler-Based Approach to Schema-Specific XML Parsing. In First Internati onal Workshop on High Performance XML Processing, May 2004  Matsa, M., E. Perkins, A. Heifets, M. G.aitatzes Kostoulas, D. Silva, N. Mendelsohn, M. Leger. A high-performance interpretive approach to schema-directed parsing. In Proceedings of the 16th International Conference on World Wide Web, 2007  Noga, M  L., Schott, S., and Löwe, W  2002. Lazy  XML processing. In Proceedings of the 2002 ACM Symposium on Document Engineering McLean, Virginia, USA, November 08 - 09 2002\02. ACM, New York, NY  Farfán, F., V. Hristidis and R. Rangaswam i Beyond Lazy XML Parsing. In Proceedings of the 18th International Conference \(DEXA 200 September 3-7, 2007  
324 


 15 7  B.-N. Vo and W.-K. Ma, \223The Gaussian Mixture Probability Hypothesis Density Filter,\224 IEEE Trans Signal Processing Vol. 54, pp. 4091-4104, November 2006 8  B. Ristic, S. Arulampalam, and N. Gordon Beyond the Kalman Filter Artech House, 2004 9  Y. Bar-Shalom, X. Rong Li, and T. Kirubarajan Estimation with Applications to Tracking and Navigation, New York: John Wiley & Sons, pg. 166 2001 10  X. R. Li, Z. Zhao, and V. P. Jilkov, \223Estimator\222s Credibility and Its Measures,\224 Proc. IFAC 15th World Congress Barcelona, Spain, July 2002 11  M. Mallick and S. Arulampalam, \223Comparison of Nonlinear Filtering Algorithms in Ground Moving Target Indicator \(GMTI Proc Signal and Data Processing of Small Targets San Diego, CA, August 4-7, 2003 12  M. Skolnik, Radar Handbook, New York: McGrawHill, 1990 13  A. Gelb, Editor Applied Optimal Estimation The MIT Press, 1974 14  B. D. O. Anderson and J. B. Moore Optimal Filtering  Prentice Hall, 1979 15  A. B. Poore, \223Multidimensional assignment formulation of data ass ociation problems arising from multitarget and multisensor tracking,\224 Computational Optimization and Applications Vol. 3, pp. 27\22657 1994 16  A. B. Poore and R. Robertson, \223A New multidimensional data association algorithm for multisensor-multitarget tracking,\224 Proc. SPIE, Signal and Data Processing of Small Targets Vol. 2561,  p 448-459, Oliver E. Drummond; Ed., Sep. 1995 17  K. R. Pattipati, T. Kirubarajan, and R. L. Popp, \223Survey of assignment techniques for multitarget tracking,\224 Proc  on Workshop on Estimation  Tracking, and Fusion: A Tribute to Yaakov Bar-Shalom Monterey CA, May 17, 2001 18  P. Burns, W.D. Blair, \223Multiple Hypothesis Tracker in the BMD Benchmark Simulation,\224 Proceedings of the 2004 Multitarget Tracking ONR Workshop, June 2004 19  H. Hotelling, \223The generalization of Student's ratio,\224 Ann. Math. Statist., Vol. 2, pp 360\226378, 1931 20  Blair, W. D., and Brandt-Pearce, M., \223Monopulse DOA Estimation for Two Unresolved Rayleigh Targets,\224 IEEE Transactions Aerospace Electronic Systems  Vol. AES-37, No. 2, April 2001, pp. 452-469 21  H. A. P.  Blom, and Y. Bar-Shalom, The Interacting Multiple Model algorithm for systems with Markovian switching coefficients IEEE Transactions on Au tomatic Control 33\(8  780-783, August, 1988 22  M. Kendall, A. Stuart, and J. K. Ord, The Advanced Theory of Statistics, Vol. 3, 4th Edition, New York Macmillan Publishing, pg. 290, 1983 23  T.M. Cover and P.E. Hart, Nearest Neighbor Pattern Classification, IEEE Trans. on Inf. Theory, Volume IT-13\(1 24  C.D. Papanicolopoulos, W.D. Blair, D.L. Sherman, M Brandt-Pearce, Use of a Rician Distribution for Modeling Aspect-Dependent RCS Amplitude and Scintillation Proc. IEEE Radar Conf 2007 25  W.D. Blair and M. Brandt-Pearce, Detection of multiple unresolved Rayleigh targets using quadrature monopulse measurements, Proc. 28th IEEE SSST March 1996, pp. 285-289 26  W.D. Blair and M. Brandt-Pearce, Monopulse Processing For Tracking Unresolved Targets NSWCDD/TR-97/167, Sept., 1997 27  W.D. Blair and M. Brandt-Pearce, Statistical Description of Monopulse Parameters for Tracking Rayleigh Targets  IEEE AES Transactions, Vol. 34 Issue 2,  April 1998, pp. 597-611 28  Jonker and Volgenant, A Shortest Augmenting Path Algorithm for Dense and Sparse Linear Assignment Problems, Computing, Vol. 38, 1987, pp. 325-340 29  V. Jain, L.M. Ehrman, and W.D. Blair, Estimating the DOA mean and variance of o ff-boresight targets using monopulse radar, IEEE Thirty-Eighth SSST Proceedings, 5-7 March 2006, pp. 85-88 30  Y. Bar-Shalom, T. Kirubarajan, and C. Gokberk 223Tracking with Classification-Aided Multiframe Data Association,\224 IEEE Trans. on Aerospace and Electronics Systems Vol. 41, pp. 868-878, July, 2005   


 16 B IOGRAPHY  Andy Register earned BS, MS, and Ph  D. degrees in Electrical Engineering from the Georgia Institute of Technology.  His doctoral research emphasized the simulation and realtime control of nonminimum phase mechanical systems.  Dr. Register has approximately 20 years of experience in R&D with his current employer, Georgia Tech, and product development at two early-phase startups. Dr. Register\222s work has been published in journals and conf erence proceedings relative to mechanical vibration, robotics, computer architecture programming techniques, and radar tracking.  More recently Dr. Register has b een developing advanced radar tracking algorithms and a software architecture for the MATLAB target-tracking benchmark.  This work led to the 2007 publication of his first book, \223A Guide to MATLAB Object Oriented Programming.\224  Mahendra Mallick is a Principal Research Scientist at the Georgia Tech Research Institute \(GTRI\. He has over 27 years of professional experience with employments at GTRI \(2008present\, Science Applications International Corporation \(SAIC Chief Scientist \(2007-2008\, Toyon Research Corporation, Chief Scientist 2005-2007\, Lockheed Martin ORINCON, Chief Scientist 2003-2005\, ALPHATECH Inc., Senior Research Scientist 1996-2002\, TASC, Principal MTS \(1985-96\, and Computer Sciences Corporation, MTS \(1981-85 Currently, he is working on multi-sensor and multi-target tracking and classification bas ed on multiple-hypothesis tracking, track-to-track association and fusion, distributed filtering and tracking, advanced nonlinear filtering algorithms, and track-before-detect \(TBD\ algorithms He received a Ph.D. degree in  Quantum Solid State Theory from the State University of New York at Albany in 1981 His graduate research was also based on Quantum Chemistry and Quantum Biophysics of large biological molecules. In 1987, he received an MS degree in Computer Science from the John Hopkins University He is a senior member of the IEEE and Associate Editor-inchief  of the Journal of Advances in Information Fusion of the International Society of Information Fusion \(ISIF\. He has organized and chaired special and regular sessions on target tracking and classific ation at the 2002, 2003, 2004 2006, 2007, and 2008 ISIF conferences. He was the chair of the International Program Committee and an invited speaker at the International Colloquium on Information Fusion \(ICIF '2007\, Xi\222an, China. He is a reviewer for the IEEE Transactions on Aerospa ce and Electronics Systems IEEE Transactions on Signal Pr ocessing, International Society of Information Fusion, IEEE Conference on Decision and Control, IEEE Radar Conference, IEEE Transactions on Systems, Man and Cybernetics, American Control Conference, European Signal Processing Journal and International Colloquium on Information Fusion ICIF '2007   William Dale Blair is a Principal Research Engineer at the Georgia Tech Research Institute in Atlanta, GA. He received the BS and MS degrees in electrical engineering from Tennessee Technological University in 1985 and 1987, and the Ph.D. degree in electrical engineering from the University of Virginia in 1998. From 1987 to 1990, he was with the Naval System Division of FMC Corporation in Dahlgren, Virginia. From 1990 to 1997, Dr Blair was with the Naval Surface Warfare Center, Dahlgren Division NSWCDD\ in Dahlgren, Virg inia. At NSWCDD, Dr Blair directed a real-time experiment that demonstrated that modern tracking algorithms can be used to improve the efficiency of phased array radars. Dr Blair is internationally recognized for conceptualizing and developing benchmarks for co mparison and evaluation of target tracking algorithms Dr Blair developed NSWC Tracking Benchmarks I and II and originated ONR/NSWC Tracking Benchmarks III and IV NSWC Tracking Benchmark II has been used in the United Kingdom France, Italy, and throughout the United States, and the results of the benchmark have been presented in numerous conference and journal articles. He joined the Georgia Institute of Technology as a Se nior Research Engineer in 1997 and was promoted to Principal Research Engineer in 2000. Dr Blair is co-editor of the Multitarg et-Multisensor Tracking: Applications and Advances III. He has coauthored 22 refereed journal articles, 16 refereed conference papers, 67 papers and reports, and two book chapters. Dr Blair's research interest include radar signal processing and control, resource allocation for multifunction radars, multisen sor resource allocation tracking maneuvering targets and multisensor integration and data fusion. His research at the University of Virginia involved monopulse tracking of unresolved targets. Dr Blair is the developer and coordinator of the short course Target Tracking in Sensor Systems for the Distance Learning and Professional Education Departmen t at the Georgia Institute of Technology. Recognition of Dr Blair as a technical expert has lead to his election to Fellow of the IEEE, his selection as the 2001 IEEE Y oung Radar Engineer of the Year, appointments of Editor for Radar Systems, Editor-InChief of the IEEE Transactions on Aerospace and Electronic Systems \(AES\, and Editor-in- Chief of the Journal for Advances in Information Fusion, and election to the Board of Governors of the IEEE AES Society,19982003, 2005-2007, and Board of Directors of the International Society of Information Fusion   


 17 Chris Burton received an Associate degree in electronic systems technology from the Community College of the Air force in 1984 and a BS in Electrical Engineering Technology from Northeastern University in 1983.  Prior to coming to the Georgia Institute of Technology \(GTRI\ in 2003, Chris was a BMEWS Radar hardware manager for the US Air Force and at MITRE and Xontech he was responsible for radar performance analysis of PAVE PAWS, BMEWS and PARCS UHF radar systems Chris is an accomplished radar-systems analyst familiar with all hardware and software aspects of missile-tracking radar systems with special expertise related to radar cueing/acquisition/tracking for ballistic missile defense ionospheric effects on UHF radar calibration and track accuracy, radar-to-radar handover, and the effects of enhanced PRF on radar tracking accuracy.  At GTRI, Chris is responsible for detailed analysis of ground-test and flight-test data and can be credited with improving radar calibration, energy management, track management, and atmospheric-effects compensation of Ballistic Missile Defense System radars   Paul D. Burns received his Bachelor of Science and Masters of Science in Electrical Engineering at Auburn University in 1992 and 1995 respectively. His Master\222s thesis research explored the utilization of cyclostationary statistics for performing phased array blind adaptive beamforming From 1995 to 2000 he was employed at Dynetics, Inc where he performed research and analysis in a wide variety of military radar applications, from air-to-air and air-toground pulse Doppler radar to large-scale, high power aperture ground based phased array radar, including in electronic attack and protection measures. Subsequently, he spent 3 years at MagnaCom, Inc, where he engaged in ballistic missile defense system simulation development and system-level studies for the Ground-based Midcourse defense \(GMD\ system. He joined GTRI in 2003, where he has performed target tracking algorithm research for BMD radar and supplied expertise in radar signal and data processing for the Missile Defense Agency and the Navy Integrated Warfare Systems 2.0 office.  Mr. Burns has written a number of papers in spatio-temporal signal processing, sensor registration and target tracking, and is currently pursuing a Ph.D. at the Georgia Institute of Technology  


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


