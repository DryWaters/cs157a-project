Dache A Data Aware Caching for Big-Data Applications Using The MapReduce Framework Yaxiong Zhao  and Jie Wu   Amazon.com Inc  Department of Computer and Information Sciences Temple University zhaoyaxi@amazon.com jiewu@temple.edu Abstract The buzz-word big-data application refers to the large-scale distributed applic ations that work on unprecedentedly large data sets Googleês MapReduce framework and Apacheês Hadoop its open-source implementation are the defacto software system for big-data applications An observation regarding these applications is that they generate a large amount of intermediate data and these abundant information is thrown away after the processing nish Motivated by this observation we propose a data-aware cache framework for big-data applications which is called Dache In Dache tasks submit their intermediate results to the cache manager A task before initiating its execution queries the cache manager for potential matched processing results which could accelerate its execution or even completely saves the execution A novel cache description scheme and a cache request and reply protocol are designed We implement Dache by extending the relevant components of the Hadoop project Testbed experiment results demonstrate that Dache signiìcantly improves the completion time of MapReduce jobs and saves a signiìcant chunk of CPU execution time Index Terms Big-data MapReduce Hadoop distributed le system cache management I I NTRODUCTION MapReduce 5  a nd i t s open-s ource i m pl ement a t i o n Hadoop 4 are a s o ft w a re frame w o rk for l ar ge-s cal e d i s tributed computing on large amounts of data Applications specify the computation in terms of a map and a reduce function working on partitioned data items The MapReduce framework schedules computation across a cluster of machines MapReduce provides a standardized framework for implementing large-scale distributed computation on unprecedentedly large-scale data set However there is a limitation of the system i.e the inefìciency in incremental processing Incremental processing refers to the applications that incrementally grow the input data and continuously apply computations on the input in order to generate output There are potential duplicate computations being performed in this process However MapReduce does not have the mechanism to identify such duplicate computations Motivated by this observation we propose Dache a data-aware cache system for Big-data applications using the MapReduce framework Dache aims at extending the MapReduce framework and provisioning a cache layer fo r efìciently identifying and accessing cache items in a MapReduce job The following technical challenges need to be addressed before implementing this proposal 1 Cache description scheme Data-aware  User program Master Worker Worker Worker split0 split1 split2 split3 Worker Worker Output1 Output2 Input files Map phase Intermediate files Reduce phase Output file Cache manager r e q u e s t r e q u e s t Cache submission Cache submission reply reply Fig 1 High level description of the architecture of Dache caching requires each data object to be indexed by its content In the context of Big-data applications this means that the cache description sch eme needs to describe the application framework and the data contents Although most big-data applications run on standardized platforms their individual tasks perform completely diff erent operations and generate different intermediate resu lts 2 Cache request and reply protocol The size of the aggregated intermediate data can be very large Usually the programs are moved to data nodes in order to avoid network communications The protocol should be able to collate cache items with the worker processes potentially that need the data so that the transmission delay and overhead are minimized In this paper we present a no vel cache description scheme This scheme identiìes the source input from which a cache item is obtained and the operations applied on the input In the reduce phase we devise a mechanism to take into consideration the partition operations applied on the output in the map phase We also present a method for reducers to utilize the cached results in the map phase to accelerate their execution We implement Dache in the Hadoop project by extending the relevant components Our implementation follows a non-intrusive approach so it only requires minimum changes to the application code II C ACHE D ESCRIPTION A Map Phase Cache Description Scheme Cache refers to the intermediate data that is produced by worker nodes/processes during the execution of a MapReduce 978-1-4673-5946-7/13/$31.00 ©2013 IEEE 2013 Proceedings IEEE INFOCOM 35 


 block file split offset size Fig 2 The high level illustration of a le in DFS task A piece of cached data is stored in a distributed le system DFS The content of a cache item is described by its original data and the operations that obtained it from the original data item Cache descr iptions can also be recursive to describe sequential processing on the same data set Formally a cache item is described by a 2-tuple  Origin Operation   Origin is the name of a le in the DFS Operation is a linear list of available operations performed on the Origin le For example in the word count application each mapper node/process emits a list of  word count  tuples that record the count of each word in the le that the mapper processes Dache stores this list to a le This le becomes a cache item Given an original input data le word list 08012012 txt  the cache item is described by  word list 08012012 txt  item count   Here item refers to white-space-separated character strings Note that the new line character is also considered as one of the white spaces so item precisely captures the word in a text le and item count directly corresponds to the word count operation performed on the data le The exact format of the cach e description of different applications varies according t o their speciìc semantics This could be designed and implemented by application developers who are responsible for implementing their MapReduce tasks In our prototype we present several supported operations  Item Count This operation collects the counts of all records in the input  Sort This operation sorts the records of the le  Selection This operation selects an item that meets a given criterion  Transform This operation transforms each item in the input le into a different item  Classiìcation This operation splits input items into multiple groups which is a deterministic processing that can be repeatedly applied on the same input to produce exactly same results B Reduce Phase Cache Description Scheme The input for the reduce phase is also a list of key-value pairs where the value could be a list of values Much like the scheme used for the map phase cache description the original input and the applied operations are required The original input is obtained by storing the intermediate results of the map phase in the DFS The applied operations are identiìed by unique IDs that are speciìed by the user The cached results unlike those generated in the Map phase cannot be directly used as the nal output  This is because in incremental processing intermediate results generated in the Map phase are likely mixed in the shufîing phase which causes a mismatch file split file split file split file split input stream to reducer sorting and shuffling Fig 3 The input stream to a reducer is obtained by sorting and then shufîing multiple output les of mappers between the original input of the cache items and the newly generated input A remedy is to apply a ner description of the original input of the cache items in the reduce phase The description should include the original data les generated in the Map phase For example two data les le1.data and le2.data are shufîed to produce two input les input1.data and input2.data for two reducers input1.data and input2.data should include le1.data and le2.data as its shufîing source As a result new intermediate data les of the Map phase are generated during incremental processing the shufîing input will be identiìed in a similar way The reducers can identify new inputs from the shufîing sources by shufîing the newly-generated intermediate result from the Map phase to form the nal results For example assume that input3.data is a newly generated results from Map phase the shufîing results le1.data and le2.data include a new shufîing source input3.data A reducer can identify the input le1.data as the result of shufîing input1.data input2.data and input3.data The nal results of shufîing the output of input1.data and input2.data are obtained by querying the cache manager The added shufîing output of input3.data is then added to get the new results The input given to the reducers is not cached exactly Only a part of the input is identical to the input of the cache items The rest is from the output of the incremental processing phase of the mappers If a reducer could combine the cached partial results with the results obtained from the new inputs and substantially reduce the overall computation time reducers should cache partial results Actually this property is determined by the operations executed by the reducers Fortunately almost all real-world applications have this property III P ROTOCOL A Relationship Between Job Types and Cache Organization The partial results generated in the map and reduce phases can be utilized in different scenarios There are two types of cache items the map cache and the reduce cache They have different complexities when it comes to sharing under different scenarios Cache items in the map phase are easy to share because the operations applied are generally wellformed When processing each le split the cache manager reports the previous le splitting scheme used in its cache item The new MapReduce job needs t o split the les according to the same splitting scheme in order to utilize the cache items However If the new MapReduce job uses a different 2013 Proceedings IEEE INFOCOM 36 


 Cache manager map reduce same reduce tasks map reduce Previous mapreduce job Same map tasks Saved computation Request cache Appended input shuffling Request cache Combining partial results from cache and new input Fig 4 The situation where two MapReduce jobs have the same map and reduce tasks le splitting scheme the map results cannot be used directly unless the operations applied in the map phase are context free  By context free we mean that the operation only generates results based on the input records which does not consider the le split scheme This is generally true When considering cache sharing in the reduce phase we identify two general situations The rst is when the reducers complete different jobs than the cached reduce cache items of the previous MapReduce jobs In this case after the mappers submit the results obtained from the cache items the MapReduce framework uses the partitioner provided by the new MapReduce job to feed input to the reducers The saved computation is obtained by removing the processing in the Map phase Usually new content is appended at the end of the input les which requires a dditional mappers to process However this does not require additional processes other than those introduced above The second situation is when the reducers can actually take advantage of the previously-cached reduce cache items as illustrated in Fig 4 Using the description scheme discussed in Section II the reducers determine how the output of the map phase is shufîed The cache man ager automati cally identiìes the best-matched cache item to feed each reducer which is the one with the maximum overlap in the original input le in the Map phase B Cache Item Submission Mapper and reducer nodes/processes record cache items into their local storage space When th ere operations are completed the cache items are forwarded to the cache manager which acts like a broker in the publish/subscribe paradigm  T he cache manager records the desc ription and the le name of the cache item in the DFS The cache item should be put on the same machine as the worker pr ocess that generates it This requirement improves data locality The cache manager maintains a copy of the mapping between the cache descriptions and the le names of the cache items in its main memory to accelerate queries It also ushe s the mapping le into the disk periodically to avoid permanently losing data A worker node/process contacts the cache manager each time before it begins processing an input data le The worker process sends the le name and the operations that it plans to apply to the le to the cache manager The cache manager receives this message and compares it with the stored mapping data If there is a exact match to a cache item i.e its origin is the same as the le name of the request and its operations are the same as the proposed operations that will be performed on the data le then the manager will send back a reply containing the tentative description of the cache item to the worker process The worker process receives the tentative description and fetches the cache item For further processing the worker needs to send the le to the next-stage worker processes The mapper needs to inform the cach e manager that it already processed the input le splits for this job The cache manager then reports these results to the next phase reducers If the reducers do not utilize the cache service the output in the map phase could be directly shufîed to form the input for the reducers Otherwise a more com plicated process is executed to obtain the required cache items  which will be explained in Section III-D If the proposed operations are different from the cache items in the managerês reco rds there are situations where the origin of the cache item is the same as the requested le and the operations of the cache item is a strict subset of the proposed operations The concept of a strict super set refers to the fact that the item is obtained by applying some additional operations on the subset item For example an item count operation is a strict subset operation of an item count followed by a selection operation This fact means that if we have a cache item for the rst operation we could just add the selection operation which guarantees the correctness of the operation C Lifetime Management of Cache Item The cache manager needs to determine how much time a cache item can be kept in the D FS Holding a cache item for an indeìnite amount of time will waste storage space when there is no other MapReduce task utilizing the intermediate results of the cache item There are two types of policies for determining the lifetime of a cache item as listed below The cache manager also can promot e a cache item to a permanent le and store it in the DFS which happens when the cache item is used as the nal result of a MapReduce task In this case the lifetime of the cache item is no longer managed by the cache manager The cache manager still maintains the mapping between cache descrip tions and the actual storage location 1 Fixed Storage Quota Dache allocates a xed amount of storage space for storing cach e items Old cache items need to be evicted when there is not enough storage space for storing new cache items The eviction policy of old cache items can be modeled as a classic cache replacement problem 2 In our preliminary implementation the least recent used LRU is employed The cost of allocating a xed storage quota could be determined by a pricing model that captures the monetary expense of using that amount of storage space Such pricing models are available in a public Cloud service We discuss more details about the model in Section III-C2 2013 Proceedings IEEE INFOCOM 37 


2 Optimal Utility Increasing the storage space of cache items will likely hit a plateau due to the diminishing return effect A utility-based measurement can be used to determine the optimal space used for storing cache items in order to trade off between the beneìts and the costs This scheme estimates the saved computation time t s  by caching a cache item for a given amount of time t a  These two variables are used to derive the monetary gain and cost The net proìt i.e the difference of subtracting cost from gain should be made positive To accomplish this an accurate pricing model of computational resources is required Although conventional computing infrastructures do not offer such a model Cloud computing does Monetary values of computational resources are well captured in existing C loud computing services i.e Amazon AWS a nd G oogl e C omput e E ngi ne 3 Expense t s  P storage  S cache  t s 1 Save t s  P computation  R duplicate  t s 2 The equations 1 and 2 show how to compute the expense of storing cache and the corresponding saved expense in computation The details of computing the variables introduced above are as follows The gain of storing a cache item for t s amount of time is calculated by accumulating the charged expenses of all the saved computation tasks in t s  The number of the same task that is submitted by the user in t s is approximated by an exponential distribution The mean of this exponential distribution is obtained by sampling in history A newly-generated cache item requires a bootstrap time to do the sampling The cost is directly computed from the charge expense of storing the item for t a amount of time The optimal lifetime of a cache item is the maximum t a  such that the proìt is positive The overall beneìts of this scheme are that the user will not be charged more and at the same time the computation time is reduced which in turn reduces the response time and increases the user satisfaction D Cache Request and Reply 1 Map Cache There are several complications that are caused by the actual designs of the Hadoop MapReduce framework The rst is when do mappers issue cache requests As described above map cache items are identiìed by cache descriptions which are not directly corresponding to the les in the HDFS le system Therefore cache requests must be sent out before the le splitting phase The jobtracker which is the central controller that manages a MapReduce job issues cache requests to the cache manager T he cache manager replies a list of cache descriptions The jobtracker then splits the input le on remaining le sections that have no corresponding results in the cache items That is the jobt racker needs to use the same le split scheme as the one used in the cache items in order to actually utilize them In this scenario the new appended input le should be split among the same number of mapper tasks so that it will not slow the entire MapReduce job down Their results are then combined together to form an aggregated Map cache item This could be done by a nested MapReduce job  File splits File splits overlap partitioner partitioner Reducer index Reducer index Must be the same Cache item Request Fig 5 In order to compare a cache description and a cache request the cache manager needs to examine the p artitioner and the reducer indexes Fig 6 The speedup of Dache over Hadoop and their completion time of word-count program 2 Reduce Cache The cache request process is more complicated The rst step is to compare the requested cache item with the cached items in the cache managerês database As described in Section II-B the cached results in the reduce phase may not be directly used due to the incremental changes As a result the cache manager n eeds to identify the overlaps of the original input les of the requested cache and stored cache In our preliminary implementation this is done by performing a linear scan of the stored cache items to nd the one with the maximum overlap with the request When comparing the request and cache item the cach e manager rst identify the partitioner The partitioner in the request and the cache item have to be identical i.e they should use the same partitioning algorithm and the same number of reducers This requirement is illustrated in Fig 5 The overlapped part means that a part of the processing in the reducer could be saved by obtaining the cached results for that part of the input The incremented part however will need to be processed by the reducer itself The nal results are generated by combining both parts The actual method of combining results is determined by the user Fig 7 The speedup of Dache over Hadoop and their completion time of tera-sort program 2013 Proceedings IEEE INFOCOM 38 


 Fig 8 CPU utlization ratio of Hadoop and Dache in the word-count program Fig 9 CPU utlization ratio of Hadoop and Dache in the tera-sort program IV P ERFORMANCE E VA L UAT I O N A Experiment Settings Hadoop is running in pseudo-distributed mode on a server that has an 8-core CPU each core running at 3GHz 16GB memory and a SATA disk The number of mappers is 16 in all experiments the reducers count varies We use two applications to benchmark the speedup of Dache over Hadoop the classic MapReduce model word-count and tera-sort  Word-count counts the number of unique words in large input text les tera-sort sorts key-value records based on the lexical order of the key More details are in Hadoop manual 4  W ordcount is an IO-intensive application that requires loading and storing a sizeable amount of data during the processing On the other hand tera-sort uses more mixed word loads It needs to load and store all input data and needs a computation-intensive sorting phase The input of two applications are generated randomly and all are 10GB in size B Results Figs 6 and 7 present the speedup and completion time of two programs The completion time and the speedup are put together Data is appended to the input le The size of the appended data varies and is represented as a percentage number to the original input le size which is 10GB Terasort is more CPU-bound compared to word-count as a result Dache can bypass computation tasks that take more time which achieves larger speedups The speedup decreases with the growing size of appended data but Dache is able to complete jobs faster than Hadoop in all situations The map phase of tera-sort does not perform much computation which also makes it easier for Dache to work Figs 8 and 9 show the CPU utilization ratio of the two programs It is measured by averaging the CPU utilization ratio of the processes of the MapReduce jobs over time Fig 10 Total cache size in GB of two programs Tera-sort consumes more CPU cycles than word-count does which is determined by the CPU-bound nature of the sorting procedure From the gures it is clear that Dache saves a signiìcant amount of CPU cycles which is demonstrated by the much lower CPU utilization ratio These results are consistent with Fig 6 and 7 With a larger incremental size the CPU utilization ratio of Dache grows signiìcantly too This is because Dache needs to process the new data and cannot utilize any cached results for bypassing computation tasks Figs 6 7 8 and 9 collectively prove that Dache indeed removes redundant tasks in inc remental MapReduce jobs and reduces job completion time Fig 10 presents the size of all the cache items produced by a fresh run of the two programs with different input data sizes In tera-sort cache items should have the same size as the original input data because sorting does not remove any data from the input The difference between the input data size and the cache size is caused by the data compression Note also that the cache item in terasort is really the nal output which means that the used space is free in the sense that no extra cost is incurred in storing cache items The word-count results are more related to the input record distribution V C ONCLUSION We present the design and evaluation of Dache a data-aware caching framework for MapReduce Dache requires only a slight modiìcation in the input format and task management of the MapReduce framework and applications needs only slight changes in order to utilize Dache We implement Dache in Hadoop Testbed experiments show that it can eliminate all the duplicate tasks in incremental MapReduce jobs and does not require substantial changes to the application code A CKNOWLEDGMENT This research was supported in part by NSF grants ECCS 1231461 ECCS 1128209 CNS 1065444 and CCF 1028167 R EFERENCES 1 A m a w o n w eb s e r v ices  h ttp://a w s  a m azon com   2 C ache a lgor ithm s  h ttp://en w i kipedia or g w i ki Ca c h e algorithms  G oogle c om pute e ngine http://cloud google com  prod uct s c om pute engine.html  H adoop http://hadoop apache or g  5 J ef f r e y D ean and S anjay G hem a w a t M a pr educe s i m p li ed data pr oces s ingonlargeclusters Commun of ACM  51\(1 January 2008 6 P atr i ck T h  E ugs ter  P a s cal A  F e lber  R achid G u er r a oui and A nneM a r i e Kermarrec The many faces of publish/subscribe ACM Comput Surv  35\(2 June 2003 2013 Proceedings IEEE INFOCOM 39 


 Figure 13. Best Neural Network DNI Error  Table 1 and 2 describe the performance of the models generated using these tools.  Note that these performance numbers should be compared qualitatively since the different input parameter configurations can yield different numbers of training data points Table 1. Performance Comparison of the Generated NonParametric Models \(GENFIS3  Table 2. Performance Comparison of the Generated NonParametric Models \(NN10   A sub-optimal predictor was constructed in order to show its performance relative to that of the non-parametric models.  This predictor was based on the average GHI DHI, and DNI values for each time bin in the data set Table 3 shows the improvement of the non-parametric models when compared to this sub-optimal predictor named \223Time Bin Mean\224 in the table below Table 3. Performance of Best Non-Parametric to Mean Time Bin Sub-Optimal Predictor   During this analysis, the aspect of the scalability of the GENFIS3 and NFTOOL tools was evaluated.  The model generation time for NFTOOL was always shorter than GENFIS3 for the same data sets.  The relationship of NFTOOL execution time to dataset length and dimension was generally linear for the test cases evaluated.  The relationship of GENFIS3 execution time to dataset length was also linear; however, its relationship between dataset dimension and execution time was a function of the dataset dimension squared.  This is shown in Figure 14 and Figure 15  Figure 14. Model Generation Execution Time Relationship with Dataset Dimension  182 


 Figure 15. Model Generation Execution Time Relationship with Dataset Length 5  Conclusion  This paper presents a high level look at some of the tools available in the Matlab toolset that enable the user to extract information from \223Big Data\224 sources in order to draw useful conclusions.  As described in Section II, the specific application discussed in this paper is the prediction of the amount of solar power generated by a micro-grid Section III then discusses the data that was gathered to support this exercise.  Section IV discusses the steps and techniques considered while trying to generate the best solar irradiance prediction model.  Techniques discussed included dataset sanitation, training input parameter selection, model generation via Fuzzy C-Means Clustering and Rule Inference GENFIS3 Neural Network training using back propagation NFTOOL Pre-Processing nonlinear variables to add to the training data set, and the use of PCA to reduce the dimension of the training data while maximizing the information retained in the data set  It was observed in the results presented in Section IV that the best model predicting solar irradiance was one utilizing the maximum number of original and preprocessed variables, which was then reduced to a manageable dimension using PCA prior to use in training the model.  The results in this section also showed that the non-parametric model generation methods discussed in this paper performed significantly better than a sub-optimal predictor.  Finally, the results describing the model generation times for the two techniques showed that NFTOOL provides significantly better training times especially when the dimension of the dataset is high  Future work on this topic is planned to address the benefits of using Genetic Programming to optimally reduce the dimension of the dataset, the use of cloud computing to generate models for larger data sets, and the design and evaluation of a controller to buy or sell money from the grid based on demand and predictions of received solar energy References 1  M. Jamshidi \(ed.\, Systems of Systems Engineering \226 Principles and Applications, CRC \226 Taylor & Francis Publishers, London, UK, 2008 2  M. Jamshidi \(ed.\, System of Systems Engineering \226 Innovations for the 21st Century, John Wiley & Sons Publishers, New York, NY, 2009 3  Y. S. Manjili, A. Rajaee, M. Jamshidi, B. Kelley 223Fuzzy Control of Electricity Storage Unit for Energy Management of Micro-Grids1\224, World Automation Congress \(WAC\co, 2012 4  Texas Sustainable Energy Research Institute Proposal to National Science Foundation on PV Forecasting and Energy Management, M. Jamshidi PI February 2013, San Antonio, Texas 5  National Renewable Energy Laboratory. \(2012 Current Irradiance and Meteorological Conditions Data Set Retrieved from http://www.nrel.gov/midc/srrl_bms 6  National Renewable Energy Laboratory. \(2012 SOLPOS Data Set. Retrieved from http://www.nrel.gov/midc/solpos/ solpos.html 7  Iowa Environmental Mesonet. \(2012\. Automated Surface Observing System Data Set. Retrieved from http://mesonet.agron.iastate.edu/ASOS 8  L. I. Smith. \(2002, Feb. 26\\223A Tutorial on Principal Components Analysis\224, [Onlin ailable   http://www.cs.otago.ac.nz/cosc453/student_tutorials/princi pal_components.pdf 9  J. Shlens. \(2009, Apr. 22\A Tutorial on Principal Component Analysis\224 Version 3.01, [Onlin A v ailab l e  http://www.snl.salk.edu/~shlens/pca.pdf 183 


  Z. Jia et al. ìThe Implications of Diverse Applications and Scalable Data Sets in Benchmarking Big Data Systemsî. Second workshop of big data benchmarking \(WBDB 2012 India\ & Lecture Note in Computer Science \(LNCS   Y. Chen et al, ìWe Donít Know Enough to make a Big Data Benchmark suiteî. Workshop on Big Data Benchmarking. 2012   J. Zhan et al, ìHigh volume computing: Identify and characterizing throught oriented workloads in data centersî. In Parallel and Distributed processing Symposium Workshops & PhD Forum IPDPSW\, 2012 IEEE 26 th International pages 1712-1721. IEEE 2012   http://en.wikipedia.org/wiki/Principal_component_analysis   http://en.wikipedia.org/wiki/K-means_clustering   125 


overhead of job initialization in Hadoop is much larger than cNeural VIII C ONCLUSION AND F UTURE W ORK The past several years have witnessed an ever-increasing growth speed of data To address large scale neural network training problems in this paper we proposed a customized parallel computing platform called cNeural Different from many previous studies cNeural is designed and built on perspective of the whole architecture from the distributed storage system at the bottom level to the parallel computing framework and algorithm on the top level Experimental results show that cNeural is able to train neural networks over millions of samples and around 50 times faster than Hadoop with dozens of machines In the future we plan to develop and add more neural network algorithms such as deep belief networks into cNeural in order to make further support training large scale neural networks for various problems Finally with more technical work such as GUI done we would like to make it as a toolbox and open source it A CKNOWLEDGMENT This work is funded in part by China NSF Grants No 61223003 the National High Technology Research and Development Program of China 863 No 2011AA01A202 and the USA Intel Labs University Research Program R EFERENCES  C Bishop Neural networks for pattern recognition  Clarendon press Oxford 1995  J Collins Sailing on an ocean of 0s and 1s  Science  vol 327 no 5972 pp 1455Ö1456 2010  S Haykin Neural networks and learning machines  Englewood Cliffs NJ Prentice Hall 2009  R Hecht-Nielsen Theory of the backpropagation neural network in Proc Int Joint Conf on Neural Networks,IJCNN IEEE 1989 pp 593Ö605  Y  Loukas  Artiìcial neural netw orks in liquid chromatography Efìcient and improved quantitative structure-retention relationship models Journal of Chromatography A  vol 904 pp 119Ö129 2000  N Serbedzija Simulating artiìcial neural netw orks on parallel architectures Computer  vol 29 no 3 pp 56Ö63 1996  M Pethick M Liddle P  W erstein and Z Huang P arallelization of a backpropagation neural network on a cluster computer in Proc Int Conf on parallel and distributed computing and systems PDCS  2003  K Ganeshamoorthy and D Ranasinghe On the performance of parallel neural network implementations on distributed memory architectures in Proc Int Symp on Cluster Computing and the Grid CCGRID  IEEE 2008 pp 90Ö97  S Suresh S Omkar  and V  Mani P arallel implementation of back-propagation algorithm in networks of workstations IEEE Trans Parallel and Distributed Systems  vol 16 no 1 pp 24Ö34 2005  Z Liu H Li and G Miao Mapreduce-based backpropagation neural network over large scale mobile data in Proc Int Conf on Natural Computation ICNC  vol 4 IEEE 2010 pp 1726Ö1730  M Glesner and W  P  ochm  uller Neurocomputers an overview of neural networks in VLSI  CRC Press 1994  Y  Bo and W  Xun Research on the performance of grid computing for distributed neural networks International Journal of Computer Science and Netwrok Security  vol 6 no 4 pp 179Ö187 2006  C Chu S Kim Y  Lin Y  Y u  G  Bradski A Ng and K Olukotun Map-reduce for machine learning on multicore Advances in neural information processing systems  vol 19 pp 281Ö288 2007  U Seif fert  Artiìcial neural netw orks on massi v ely parallel computer hardware Neurocomputing  vol 57 pp 135Ö150 2004  D Calv ert and J Guan Distrib uted artiìcial neural netw ork architectures in Proc Int Symp on High Performance Computing Systems and Applications  IEEE 2005 pp 2Ö10  H Kharbanda and R Campbell F ast neural netw ork training on general purpose computers in Proc Int Conf on High Performance Computing HiPC  IEEE 2011  U Lotri  c and e a Dobnikar A Parallel implementations of feed-forward neural network using mpi and c on  net platform in Proc Int Conf on Adaptive and Natural Computing Algorithms  Coimbra 2005 pp 534Ö537  Q V  Le R Monga and M e a De vin Building high-le v e l features using large scale unsupervised learning in Proc Int Conf on Machine Learning ICML  ACM 2012 pp 2Ö16  J Ekanayak e and H e a Li T wister a runtime for iterati v e mapreduce in Proc of the 19th ACM International Symposium on High Performance Distributed Computing  ACM 2010 pp 810Ö818  Y  Bu B Ho we M Balazinska and M D Ernst Haloop Efìcient iterative data processing on large clusters Proc of the VLDB Endowment  vol 3 no 1-2 pp 285Ö296 2010  M Zaharia M Cho wdhury  T  Das A Da v e  J  Ma M McCauley M Franklin S Shenker and I Stoica Resilient distributed datasets A fault-tolerant abstraction for in-memory cluster computing in Proc USENIX Conf on Networked Systems Design and Implementation  USENIX Association 2012 pp 2Ö16 384 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


