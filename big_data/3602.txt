Mining Relevant Text from Unlabelled Documents Daniel Barbar\264 a Carlotta Domeniconi Ning Kang Information and Software Engineering Department George Mason University Fairfax VA 22030 000 dbarbara,cdomenic,nkang 001 gmu.edu Abstract Automatic classi\336cation of documents is an important area of research with many applications in the 336elds of document searching forensics and others Methods to perform classi\336cation of text rely on the existence of a sample of documents whose class labels are known However in many situations obtaining this sample may not be an easy or 
even possible task In this paper we focus on the classi\336cation of unlabelled documents into two classes relevant and irrelevant given a topic of interest By dividing the set of documents into ckets for instance answers returned by different search engines and using association rule mining to 336nd common sets of words among the ckets we can ef\336ciently obtain a sample of documents that has a large percentage of relevant ones This sample can be used to train models to classify the entire set of documents We prove via experimentation that our method is capable of 336ltering relevant documents even in adverse conditions where the percentage of irrelevant documents in the ckets is relatively 
high 1 Introduction In information retrieval such as content based image retrieval or web page classi\223cation we face an asymmetry between positive and negative examples 10 2  S uppos e for example we submit a query to multiple search engines Each engine retrieves a collection of documents in response to our query Such collections in clude in general both relevant and irrelevant documents Suppose we want to discriminate the relevant documents from the irrelevant ones 
The set of all relevant documents in all retrieved collections represent a sample of the positive class drawn from an underlying unknown distribution On the other hand the irrelevant documents may come from an unknown number of different 215negative\216 classes In general we cannot approximate the distributions of the negative classes as we may have too few representatives for each of them Hence we are facing a problem with an unknown number of classes with the user interested in only one of them 
Modelling the above scenario as a two-class problem may impose misleading requirements that can yield poor results We are de\223nitely better off focusing on the class of interest as positive examples in this scenario have a more compact support that re\224ects t he correlations among their feature values Moreover more often than not the class labels of the data are unknown either because the data is too large for an expert to label it or because no such expert exists In this work we eliminate the assumption of having even partially 
labelled data We focus on document retrieval and develop a technique to mining relevant text from unlabelled documents Speci\223cally our objective is to identify a sample f positive documents tive of e underlying class distribution The scenario of a query submitted to multiple search engines will serve as running example throughout the paper lthough the technique can be applied to a variety of scenarios and data Our approach re\224ects the asymmetry between positive and negative data and does not make any 
particular and unnecessary assumption on the negative examples 2 Related Work In 4 t he aut hors di s cus s a hi erarchi cal document cl us tering approach using frequent set of words Their objective is to construct a ierarchy of documents for browsing at increasing levels of speci\223city of topics In 1 t he aut hors cons i d er t h e p robl em of enhanci ng t h e performance of a learning algorithm allowing a set of unlabelled data augment a small set of labelled examples The driving application is the classi\223cation of Web pages Al 
though similar to our scenario the technique depends on the existence of labelled data to begin with The authors in 6 e xpl oi t s emant i c s i mi l a ri t y bet ween terms and documents in an unsupervised fashion DocuProceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


ments that share terms that are different but semantically related will be considered as unrelated when text documents are represented as a bag f words  The purpose of the work in 6 i s t o o v e rco m e t h i s limitatio n b y l earn i n g a semantic proximity matrix from a given corpus of documents by taking into consideration high order correlations Two methods both yielding to the de\223nition of a kernel function are discussed In particular in one model documents with highly correlated words are considered as having similar content Similarly words contained in correlated documents are viewed as semantically related 3 The DocMine Algorithm Given a document it is possible to associate with it a bag of words 5 3 7 Speci\223cally  w e r epres ent a document as a binary vector 000 000 001 000  in which each entry records if a particular word stem occurs in the text The dimensionality 000 of d is determined by the number of different terms in the corpus of documents size of the dictionary  and each entry is indexed by a speci\223c term Going back to our example suppose we submit a query to 001 different search engines We obtain 001 collections or buckets  of documents 002 001 000 002 000 002 003 001 003 004 000\001 003\005\005\005\003\001  While many documents retrieved by a speci\223c search engine a bad one might be irrelevant the relevant ones are expected to be more frequent in the majority of buckets In addition since we can assume that positive documents are drawn from a single underlying distribution a compact support uni\223es them across all buckets On the other hand the negatives manifest a large variation We make use of these characteristics to develop a technique that discriminates relevant documents from the irrelevant ones In details we proceed as follows We mine each bucket 002 001 to 223nd the frequent itemsets that satisfy a given support level Each resulting itemset is a set of words The result of this process is a collection of sets of itemsets one set for each bucket 006 001 000 002 007 002 004 007 002 is a requent itemset in cket 004 003 for 004 000 001 003\005\005\005\003\001 whereit is possible that 006 001 000 005 forsome 004  Now we compute all itemsets that are frequent in b buckets t 003 000 002 007 002 004 007 002 000 006 001 000 006 006 001 001 006 005\005\005 006 006 001 000 003  for distinct 004 000 004 003  In general b 000 007 001\n 002 b 003\001  In our experiments we set b 000 001 since we consider a limited number of buckets  001 000 004  n by the number of available documents per topic We wish now to etrieve the documents that support the itemsets that are frequent in b buckets Then for each 007 002 000 t 003 we select in each of the b buckets that contain 007 002 as frequent itemset the documents that has 007 002 expressed within The resulting collection of documents 013 represent the presumed positive documents relevant to our query The algorithm which we call DocMine  Doc ument Min ing is summarized in the following The algorithm takes as input e 001 buckets of documents and the minimum support  f\r\016 003\002\000  for the computation of frequent itemsets 1 Input  000 buckets of documents 001 000 000 000 000 001 001 000  002 000\001 003 004\004\004\003\000  005\006\007 002\001\003 003\b  2 Compute frequent itemsets in each bucket 001 000  t 000 000 000 n 001 002 n 001 013\000 f r 016 017\020 006\017\021\022 013\022\017\b\000\017\022 013\021 023\006\024\025 017\022 002 001  002 000\001 004\004\004 003\000 3 Compute all itemsets that are frequent in b buckets 026 002 000 000 n 001 002 n 001 003 t 000 000 004 t 000 001 004 004\004\004 004 t 000 000 001 4 Set 027 000 005  5 for each n 001 003 026 002 006 for each 030 000\001 003 004\004\004\003 b such that n 001 003\004 002 004 000\001 t 000 001 226 for each 000 003 001 000 001 007 if d contains n 001 b 027 000 027 t\000 000 001 6 Output theset 027 presumed positive documents It is important to remark that the DocMine algorithm can be tuned to ignore itemsets of small size Some words in fact may be common to documents of different topics they would not discriminate Our experience tells us that for instance combinations of two frequent words are not suf\223cient to discriminate among different topics 4 Experimental Results To test the feasibility of our approach we use the Reuters21578 text categorization collection 8 omitting empty documents and those without labels Common and rare words are removed and the vocabulary is stemmed with the Porter Stemmer 9 After s t emming the v ocab ulary s ize is 12113 In our experiments we consider 223ve buckets of documents  001 000\004  and vary the percentage 017 of relevant documents i.e concerning the topic of interest in each bucket from 004\005\006 to 007\005\006  As topics of interest we select the topics with the largest number of documents available in the data set Once we have identi\223ed a topic the non relevant documents are randomly selected from the remaining topics We observe that some documents in the Reuters data have multiple topics associated e.g grain and crops  In our experiments a document is considered positive if it has the topic of interest among its associated topics For each topic examined we test three different values of the minimum support  001\005\006  004\006  b\006  We have also investigated different threshold values from 2 to 5 for the cardinality of the frequent itemsets  004 007 002 004  Only frequent itemsets of size above or equal to Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


the threshold are considered for the retrieval of relevant documents The rationale beyond this test is that if an item is too common across different documents then it would have little discriminating power The setting of a proper threshold for 000 000 000 000 allows to discard frequently used words not removed during preprocessing that are not discriminating Our experiments show that threshold values of 4 or 5 depending on the value of the minimum support ive good results In the following tables we report for each value of 001  the number of retrieved documents in 002  000 002 000  the number of positive relevant documents in 002  000 002 000 000  the percentage of positive documents in 002  000 000 002 000 000  205precision 205 and the percentage of positive documents retrieved by 002  003  recall\205 Each caption has in parenthesis the total number of positive documents versus the l number of documents in the 223ve buckets We have considered different topics in our experiments For lack of space we report only the results for the topic earn 3776 documents Similar results were obtained for the other topics We distribute all the available positives among the buckets and adjust the number of negatives accordingly to the 001 value considered Tables 1-4 show the results Figures 1-3 plot the precision values for e topic earn  for increasing threshold 004 on the itemset size 000 000 000 000  Each line corresponds to a value of 001 percentage of positive documents in each bucket The plots show that in each case the setting of 004 001 002 allows the achievement of a precision value very close to 1 For larger support lues 5 and 10 004 001\003 suf\223ces for the selection of an almost 215pure\216 sample of documents Even in the adverse condition of 50 of irrelevent documents in the buckets the DocMine algorithm is able to achieve a very high precision These results are very promising for the purpose of constructing a classi\223er that uses the selected collection of documents 002 as a positive sample 5 Conclusions We have introduced a new algorithm based on association rule mining to select a representative sample of positive examples from a given set of unlabelled documents Our experiments show that our method is capable of selecting sets of documents with precision above 90 in most cases when frequent itemsets of cardinality 4 or 5 are considered We emphasize that in all cases the precision tends to reach high ls as the card inality of the common itemsets grows regardless of the value of the support or the percentage of relevant documents in the original buckets Table 1 Topic earn  001 001\002\004\000  005\006\006\007 005 006\002\002\b  006\007\b 001\000\002 000 000 000 000 000 002 000 000 002 000 000 000 000 002 000 000 r 001 b 5323 2824 0.53 0.74 10 001 005 2538 2204 0.87 0.58 001 003 1848 1848 1.00 0.49 001 002 1103 1103 1.00 0.29 001 b 6441 3012 0.47 0.80 5 001 005 4653 2597 0.56 0.69 001 003 1972 1913 0.97 0.51 001 002 1284 1284 1.00 0.34 001 b 7246 3597 0.50 0.95 3 001 005 5789 2943 0.51 0.78 001 003 3671 2408 0.66 0.64 001 002 1642 1628 0.99 0.43 Table 2 Topic earn  001 001\007\004\000  005\006\006\007 005 007\b\t\003  006\007\b 001\000\002 000 000 000 000 000 002 000 000 002 000 000 000 000 002 000 000 r 001 b 4453 2932 0.66 0.78 10 001 005 2725 2250 0.83 0.60 001 003 1842 1841 0.99 0.49 001 002 1403 1403 1.00 0.37 001 b 5684 3507 0.62 0.93 5 001 005 3985 2668 0.67 0.71 001 003 2045 1999 0.98 0.53 001 002 1381 1376 0.99 0.36 001 b 5859 3561 0.61 0.94 3 001 005 4636 2928 0.63 0.78 001 003 3311 2490 0.75 0.66 001 002 1879 1875 0.99 0.50 Table 3 Topic earn  001 001\006\004\000  005\006\006\007 005 002\005\t\003  006\007\b 001\000\002 000 000 000 000 000 002 000 000 002 000 000 000 000 002 000 000 r 001 b 3592 2940 0.82 0.78 10 001 005 2515 2274 0.90 0.60 001 003 1849 1842 0.99 0.49 001 002 1674 1674 1.00 0.44 001 b 4784 3467 0.72 0.92 5 001 005 3253 2747 0.84 0.73 001 003 2027 1989 0.98 0.53 001 002 1644 1642 0.99 0.43 001 b 4982 3555 0.71 0.94 3 001 005 4422 3447 0.78 0.91 001 003 3550 3079 0.87 0.81 001 002 1807 1803 0.99 0.48 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


References 1 B l u m A  M i t c hel l T   1998  C ombi ni ng L a bel l e d a nd Unl a belled Data with Co-Training Proceedings of the 1998 Conference on Computational Learning Theory  2 C hen Y   Z hou X S   H uang T  S   2001  O necl ass S VM for learning in image retrieval Proceedings of the International Conference on Image Processing  3 D u m ais S T   L etsch e  T  A  Littman  M  L  Lan d a u e r  T K 1997 Automatic cross-language retrieval using latent semantic indexing AAAI Spring Symposium on CrossLanguage Text and Speech Retrieval  4 F ung B  C  M  W ang K    E s t e r M   2003  H i e r a r c hi cal Document Clustering Using Frequent Itemsets Proceedings of the SIAM International Conference on Data Mining  5 J oachi ms T   1998  T e x t cat e gor i zat i o n w i t h suppor t v ect or machines Proceedings of the European Conference on Machine Learning  6 K andol a J  S ha weT ayl or  J    C r i st i a ni ni  N   2002  Learning Semantic Similarity Neural Information Processing Systems NIPS  7 L eopol d E    Ki nder mann J  2002  T e x t cat e gor i zat i o n with support vector machines how to represent texts in input space Machine Learning  46 423-444 8 L e w i s  D   R e ut er s21578 T e xt C a t e gorization Test Collection Distribution 1.0 http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html 9 P or t e r  M  1980  A n a l gor i t h m f or suf 223x stripping Program 14 3\:130-137 http://www.tartarus.org 000 martin/PorterStemmer  Z hou X S    Huang T  S   2001  S mal l sampl e l ear ning during multimedia retrieval using BiasMap Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition   0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 1.05 1.1 1 2 3 4 5 6 Precision t 50  60   70   80  Figure 1 Precision values for topic earn and 000\001\002 000\001\002 000\001\002  The x-axis is the minimum cardinality of common itemsets  003  Table 4 Topic earn  004 000\003\004\002  001\005\005\006 005 007\005\b\004  000\001\002 000\001\002 000 006 001 000 000 007 000 000 007 000 000 002 000 007 000 000 r 001 b 3192 2810 0.88 0.74 10 001 001 2398 2279 0.95 0.60 001 007 1394 1393 0.99 0.37 001 t 1205 1205 1.00 0.32 001 b 4151 3483 0.84 0.92 5 001 001 3003 2763 0.92 0.73 001 007 2126 2111 0.99 0.56 001 t 1589 1587 0.99 0.42 001 b 4294 3493 0.81 0.93 3 001 001 3854 3275 0.85 0.87 001 007 3059 2780 0.91 0.74 001 t 2447 2377 0.97 0.63 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 1.05 1.1 1 2 3 4 5 6 Precision t 50  60   70   80  Figure 2 Precision values for topic earn and 000\001\002 000\001\002 000\t\002  The x-axis is the minimum cardinality of common itemsets  003  0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 1.05 1.1 1 2 3 4 5 6 Precision t 50  60   70   80  Figure 3 Precision values for topic earn and 000\001\002 000\001\002 000\n\004\002 The x-axis is the minimum cardinality of common itemsets  003  Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


Figure 7 Histograms of RM AGE and PTRATIO without losing the strength of the pattern the user modified the range of the query by using visualized relevance and distance values The user can find two yellow-green regions easily as shown in Fig.8\(a Figure 8\(b illustrates the way of widening the range of NOX as the range includes the yellow-green region After the refinement the query became 12.2~ZN~100 0.0~CRIM~0.006 0.46 1INDUS<6.88 0.39<NOX<0.49  1.87 ILSTAT19.62 As a result support and confi dence became 20 and 70 respectively As demon strated above the user can find a strong pattern in the database only with his/her perceptual capability 4 CONCLUSION Data mining in very large databases is one of the most important challenges in the research area of databases The task is to efficiently find interesting data sets i.e clusters of similar data or correlations between several parameters Our approach to support the data mining process enhances the capability of traditional database querying by visualizing database itself and giving users visual feedbacks of queries Since our method is inde pendent of any specific domain area and requires no knowledge of statistics such as cluster analysis users with perceptual capabilities and general knowledge are responsible for doing the analysis and interpretation As we demonstrated the proposed method in Section 3 users of the system can explore databases by incremen tally refining queries guided by the visualized database and visual feedbacks of previous queries We will im prove the method by extending the capability of the query language The visualization techniques for relevance CRIM ZN INDUS NOX RM AGE PTRATIO LSTAT b Modification of the range of NOX Figure 8 Modification of ranges based on visualized relevance and distance values displaying a numbers of attributes and data should be further investigated REFERENCES R Agrawal H Mannila R Srikant, and H Toivonen Fast discovery of association rules In Advances in Knowledge Discovery and Data Mining chapter 12 pp 307-328 AAAI/MIT Press 1996 T M Anwar H W beck and S B Navathe Knowl edge mining by imprecise querying A classification based approach In Proceedings 8th International Con ference on Data Engineering pp 622-630 1992 V 892 


B G. Becker Using mineset for knowledge discovery IEEE Computer Graphics and Applications pp 75 78, 1997 S Brin R Motwani J D Ullman and S Tsur Be yond market baskets Generalizing association rules to correlations In Proceedings of the 1997 ACM SIGMOD International Conference on Management of Data pp 265-276, 1997 P Cheeseman and J Stutz Bayesian classification autoclass  Theory and results In Advances in Knowledge Discovery and Data Mining chapter 6 pp 153-180 AAAI/MIT Press 1996 M Chen J Han and P S Yu Data mining An overview from a database perspective IEEE Trans actions on Knowledge and Data Engeneering Vol 8 No 6 pp 866-881 1996 C Faloutsos and K I Lin FastMap A fast alge rithm for indexing data-mining and visualization of traditional and multimedia datasets In Proceedings of the 1995 A CM SIGMOD International Conference on Management of Data pp 163-174, 1995 U M Fayyad G P-Shapiro and P Smith From data mining to knowledge discovery An overview In Advances in Knowledge Discovery and Data Mining chapter 1 pp 1-34 AAAI/MIT Press 1996 T Fukuda Y Morimoto and S Morishita. Construct ing efficient decision trees by using optimized numeric association rules In Proceeding of the 22nd VLDB Conference pp 146-155 1996 T Fukuda Y Morimoto S Morishita and T Tokuyama Data mining using twedimensional op timized association rules Scheme algorithms and vi sualization. In Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data pp Takeshi Fukuda Yasuhiko Morimoto Shinichi Mor ishita and Takeshi Tokuyama Mining optimized as sociation rules for numeric attributes In Proceedings of the 15th ACM SIGACT-SIGMOD-SIGART Sympo sium on Principles of Database Systems pp 182-191 1996 J Han and Y Fu Discovery of multiple-level associ ation rules from large database In Proceedings of the 21st VLDB Conference pp 420-431 1995 Y Iizuka H Shiohara T Iizukam and S Isobe Au tomatic visualization method for visual data mining In Research and Development in Knowledge Discovery and Data Mining pp 174-185 Springer 1998 D A Keim and H.-P Kriegel VisDB Database ex ploration using multidimensional visualization IEEE Computer Graphics and Applications pp 40-49 1994 D A Keim and H.-P Kriegel Visualization tech niques for mining large databases A comparison IEEE Transactions on Knowledge and Data Engeneer ing Vol 8 No 6 pp 923-938 1996 D A Keim H.-P Kriegel and M Ankerst Recur sive pattern A technique for visualizing very large amounts of data In proceedings of Visualization 95 pp 279-286 1995 13-23 1996 17 D A Keim H.-P Kriegel and T Seidl Support ing data mining of large databases by visual feedback queries In IEEE 10th International Conference on Data Engeneering pp 302-313, 1994 18 W Klosgen Explora A multipattern and multistrat egy discovery assistant In Advances in Knowledge Discovery and Data Mining chapter 10 pp 249-271 AAAI/MIT Press 1996 19 W Klosgen and J M Zytkow Knowledge discovery in databases terminology In Advances in Knowledge Discovery and Data Mining chapter A pp 573-592 AAAI/MIT Press 1996 20 H Y Lee and H L Ong Visualization support for data mining IEEE Ezpert Intelligent Systems and Their Application Vol 11 No 5 pp 69-75, 1996 21 B Lent A Swami and J Widom Clustering asso ciation rules In Proceedings of the 13rd International Conference on Data Engeneering pp 220-231, 1997 22 Ramakrishnan Srikant Rekesh Agrawal Mining se quential patterns In Proceedings of the list Inter national Conference on Data Engeneering pp 3-14 1995 23 S J Sangwine and R E N Horne The Colour Image Processing Handbook CHAPMAN&HALL 1998 241 R Srikant and R Agrawal Mining generalized asso ciation rules In Proceedings of the 21st VLDB Con ference pp 407419 1995 251 R Uthurusamy From data mining to knowledge dis covery Current challenges and future directions In Advances in Knowledge Discovery and Data Mining chapter 23 pp 561-569 AAAI/MIT Press 1996 26 P C Wong and R D Bergeron 30 years of multi dimensional multivariate visualisation In Scientific Visualization Overviews Methodologies and Tech niques IEEE Computer Society Press 1997 V 893 


 t  Fig 3 Average run time vs minp of minp e.g minp  7 there is little impact on CPU con sumption This is because minp is sufficiently large compared to the fraction of \223noise\224 transactions However, when minp is small pruning provides significant benefits In fact, when minp is 6.5 a typical run generates about 3730 candidates at the third level With pruning the number of candidates reduces to 2550 B Production Data This section applies our algorithms for discovering m patterns in data from a production computer network. Here our evaluation criteria are more subjective than the last section in that we must rely on the operations staff to detect whether we have false positives or false negatives Two temporal data sets are considered The first was collected from an insurance company that has events from over two thou sand network elements e.g routers hubs and servers The second was obtained from an outsourcing center that supports multiple application servers across a large geographical region Events in the second data set are mostly server-oriented \(e.g the CPU utilization of a server is above a threshold and those in the first relate largely to network events e.g 223link down\224 events Each data set consists of a series of records describing events received by a network console An event has three attributes of interest here host name which is the source of the event alarm type which specifies what happened e.g a connection was lost port up and the time when the event message was received at the network console We preprocess these data to convert events into items, where an item is a distinct pair of host and alarm type. The first data set contains approximately 70,000 events for which there are over 2,000 distinct items during a two week period The second data set contains over 100,000 events for which there are over 3,000 distinct items across three weeks We apply our algorithm for m-pattern discovery to both data sets, and compare the results to those for mining frequent item sets We fix minsup to be 3 so as to eliminate a pattern with only one or two instances, and we vary minp Our results are reported in Figures 4 and 5 for data sets 1 and 2 respectively These figures plot the total number of m-patterns the solid line and the number of border m-patterns the dashed line against minp Here a border pattern refers to a pattern that is not a sub set of any other pattern The x-axis is minp and the y-axis is the number of m-patterns discovered on a log scale Clearly minp provides a very effective way to select the strongest patterns in that the number of m-patters discovered drops dramatically as 14 0.1 0 0 0 0 0 J Fig 4 M-patterns of the first data set 223-\224 the number of m-patterns in the log scale 223..\224 the number of border m-patterns in the log scale x-axis is minp Fig 5 M-patterns of the second data set 222-\224 the number of m-patterns in the log scale 223..\224 the number of border m-patterns in the log scale x-axis is minp minp increases Many of these patterns have very low support levels For example we found 59 border m-patterns with length from 2 to 5 in the first data set when minp  0.7 Half of these patterns have support levels below 10 To compare with frequent patterns it suffices to set minp  0 since the algorithm reduces to mining frequent patterns Figure 6 reports frequent patterns found in the first data Here the x-axis is minsup and the y-axis is the log of the number of patterns found Note that the number of frequent patterns is huge-in ex cess of 1 veri when when minsup is 20 Examining the frequent patterns closely we find that most are related to items that occur frequently, not necessarily items that are causally re lated This is not surprise since the marginal distribution of items in our data is highly skewed Indeed a small set of items account for over 50 of total events and consequently these items tend to appear in many frequent patterns Beyond the quality of the results produced by mining for fre t no U a man Fig 6 Frequent patterns of the first data set 223-\224 the number of frequent patterns in the log scale 223..\224 the number of border frequent patterns in the log scale; x-axis is minp 415 


quent itemsets, there is an issue with scalability as well In Fig ures 4 and 5 minp 2 0.1 and minsup  3 Suppose we have minp  0 and minsup  3 so that we are mining for frequent itemsets but with a very low support threshold When we at tempt to run this case more than 30k candidates are generated at the third level. Not only does this result in very large compu tation time, we ultimately run out of memory and so are unable to process the data We reviewed the m-pattern found with the operations staff Many patterns are related to installation errors \(e.g a wrong parameter setting of a monitoring agent and redundant events e.g 11 events are generated to signal a single problem In addition a couple of correlations were discovered that are being studied for incorporating into event correlation rules for the real time monitoring We emphasize that over half of the m-patterns discovered have very low support levels Why are m-patterns common in these data One reason is a result of physical dependence that manifests itself as a set of events when a problem arises For example when a local area network LAN fails ail hosts connected to the LAN gen erate 223lost connection\224 events. Further, the same hosts generate these events if the same failure occurs This results in the mu tual dependence of these events This observation suggests that m-patterns can be used to construct signatures for problematic situations A second cause of m-patterns is redundant information For example a device may generate an event to report a problem it detects However, there may also be several management agents that monitor the same device and report the same problem This results in an m-pattern consisting of redundant events Identify ing such m-patterns can aid in constructing filtering rules that re move redundant events More details and insights can be found in 13][10 VI CONCLUSION Motivated by the need to discover infrequent but strongly correlated patterns we propose a new pattern a mutual depen dence pattern or a m-pattern M-patterns are defined in terms of minp the minimum probability of mutual occurrence of items in the pattern In contrast to one-way dependence as in asso ciation rules an m-pattern is characterized by a strong mutual dependency between any two of its subsets. That is if any part of an itemset occurs, the other part is very likely to occur as well Our results suggest that such strong mutual dependencies are common in computer networks such as due to interrelated components that are impacted by the same failure We develop an efficient algorithm for discovering m-patterns This is accomplished in three steps First we develop a linear algorithm to qualify an m-pattern based on an equivalence we prove Second we show that a level-wise search can be used for m-pattern discovery a technique that is possible since we prove that m-patterns are downward closed Last, we develop an effec tive technique for candidate pruning by establishing a necessary condition for the presence of an m-pattern A significant impact of the resulting algorithm is that it discovers strongly correlated itemsets that may occur with low support levels something that is difficult to do with existing mining algorithms Using synthetic data we demonstrate that our algorithm scales well as the data set increases in size We also show that the pruning algorithm provides considerable benefit, especially for small values of minp We apply our algorithm to data collected from two produc tion computer networks The results show that there are many m-patterns, many of which of have very low support levels \(e.g fewer than 10 occurrences\Attempting to discover these pat terns using A-priori requires a very small value for support lev els, which results in an explosion of candidates that overruns the memory of the computer we used We further develop frequent m-patterns that are defined in terms of both minsup and minp We show that this is a more general pattern That is, when minp  0 this pattern is equiv alent to frequent itemsets and when minsup  0 frequent m patterns become m-patterns ACKNOWLEDGMENT The authors would like to thank Chang-shing Pemg for help ful discussions REFERENCES C Agganval C Agganval and V.V.V Parsad Depth first generation of long patterns In lnt\222l Conf on Knowledge Discover rind Drm Mining 2000 R Agrawal T Imielinski and A Swami Mining association rules be tween sets of items in large databases In Proc fj\222VLDB pages 207-216 1993 R Agrawal and R. Srikant Fast algorithms for mining association rules In Proc of VLDB 1994 R. Agrawal and R Srikant Mining sequential patterns In Proc of the I Ith Int 221I Conference on Datu Engineering Taipei Taiwan 1995 R Bayardo, R. Agrawal and D Gunopulos Constraint-based rule mining in large dense database In ICDE 1999 R.J Bayardo. Efficiently mining long patterns from database In SIGMOD pages 85-93 1998 S Brin, R. Motiwani and C Silverstein Beyond market baskets Gen eralizing association rules to correlations Datu Mining and Knowledge Discovery pages 39-68 1998 Edith Cohen Mayur Datar Shinji Fujiwara Aristides Gionis Piotr Indyk Rajeev Motwani, Jeffrey D Ullman and Cheng Yang Finding interesting associations without support pruning In ICDE pages 489-499 2000 J Han J Pei and Y Yin Mining frequent patterns without candidate generation \(pdf In Proc 2000 ACM-SIGMOD Int Cunf on Munugement of Data SIGMOD\222OO Dallas TX 2000 J.L Hellerstein and S Ma Mining event data for actionable patterns In lnternutional Conference for the resource manugement  perfiormance evaluation of enterprive computing systems 2000 B Liu and W Hsu Post-analysis of learned rules In AAA/-96 pages 828-834 1996 Bing Liu Wynne Hsu and Yiming Ma Pruning and summarizing the dis covered associations In Proceedings of the ACM SICKDD International Conjerence on Knowledge Discovery  Datu Mining pages 15 18 1999 S Ma and J.L Hellerstein Eventbrowser A flexible tool for scalable analysis of event data In DSOM\22299 1999 S Ma and J.L Hellerstein Mining partially periodic event patterns In ICDE pages 205-214,2001 H Mannila H Toivonen and A Verkamo. Discovery of frequent episodes in event sequences Data Mining mid Knowledge Discover 1\(3 1997 B Padmanabhan and A Tuzhilin A belief-driven method for discovering unexpected patterns In KDD-98 1998 J Pei and J Han Can we push more constraints into frequent pattern mining In CorS on Knowledge Discover rind Datu Mining KDD\222OO Boston MA 2000 H Toivonen Discovery of frequent patterns in large data collections 1996 Technical Report A-1996-5 Department of Computer Science Uni versity of Helsinki 416 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


