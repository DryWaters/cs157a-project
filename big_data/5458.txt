A Novel Method of Mining Frequent Item Sets   Dong Liyan1, Liu Zhaojun1, Shi Mo2, Yan Pengfei1, Tian Zhuo1, Li Zhen1 1.College of Computer Science and Technology, Jilin University, Changchun, 130012, China 2.Changchun Taxation College, Changchun, 130117, China dongliyan@gmail.com Corresponding Authour:  Liu Zhaojun  Email:zhaojun@jlu.edu.cn  Abstract  The aim of mining association rules is to discover the association relationship among the item sets from mass data In some practical applications, its role is mainly to assist decisionmaker. The paper proposes a novel association rule algorithm of mining frequent item sets, which introduces a new data structure and adopts compressed storage tree to improve the run performance of this algorithm. At last, the experiment indicates that the algorithm proposed in this paper has much more advantages in load balance and run time compared with most existing algorithms  Index Terms  Data Mining, Association Rules, Frequent Item Sets  I.  INTRODUCTION Data mining[1], a multidisciplinary field, is a KDD process. And its aim is to extract some interesting patterns and rules from mass data. As one of the most well-known Data mining problems, association rules mining is more and more attractive to some scientists. At present, we know lots of association rules mining technology have been successfully applied in some financial enterprises, and have exerted much more effect on the customer requirement in bank The aim of mining association rules[2] is to discover the association relationship among item sets from mass data An association rule is an implication of the form A B where A  and B are the disjoint subsets of the item set 1 2 7{ , ,... }I i i i= . And each association rule holds support s and confidence c in the transaction set D , where s and c respectively is the percentage of transactions in D  that contain A B*  and A  that also contain B , namely[3     


s A B p A B c A B p B A     Considering the arguments s and c, rules could be divided into two classes: strong rule and weak rule. Rules are called strong, if and only if they satisfy both a minimum support threshold and a minimum confidence threshold In most cases, A transaction could be denoted by a binary string[4], and each numerical digit denotes the existing state of corresponding item. In this method the computation of the frequency  of  items is very easy, but in practice, the size of a transaction is much smaller than the size of the set I , and so the transaction set D  may be denoted by a sparse matrix which would not take well advantage of the limited memory resource. The paper adopts transaction tree to storage transaction. Before detailing the theory of transaction tree, we first need to acquaint ourselves with the concept of storage tree. To illustrate this method, lets study the following figure 1, where a complete storage tree between i1 and i3 has the formal as: brother nodes are sunwise arranged according to alphabet order, and every node is denoted by a set, which consists of two parts, namely node element and all nodes in the path from this node to the root  Fig. 1 A complete storage tree between i1 and i4 Above tree structure could be view as a set containing all paths from the root to each leaf node, and each element of this set corresponds to the all item sets occurring in some transaction Now, we introduce the concept of transaction tree[5 Transaction tree is the modified version of storage tree, and it gives a mark to every node for recording the number of the transactions containing corresponding item set. Figure 2 indicate a transaction database D  and corresponding transaction tree  Fig. 2 Transaction database D and the corresponding transaction tree Besides, we also want to present the data structure applied in our paper. We know, the data structures in different 


algorithms are different, and according to the data layout these data represents could be grouped into three classes horizontal method, vertical method and hybrid method[6 Most candidate sets generating detection algorithms like Apriori algorithm adopt horizontal data layout, while most pattern growth algorithms, which contain FP-growth 5 Root 1 1 1 13 7 5 4 1 1 3 3 41 3 4 4 5 1 7 1 Iid Items 1 3 4 5 6 7 9 2 1 3 4 5 13 3 1 2 4 5 7 11 4 1 3 4 8 1 3 5 1 3 4 10 Root 1 2 3 2 3 3 3 173978-1-4244-5704-5/10/$26.00 2010 IEEE Proceedings of the 2010 IEEE International Conference on Information and Automation June 20 - 23, Harbin, China algorithm, H-Ming algorithm and OP algorithm and so forth adopt the hybrid method[7] that combines horizontal method 


with vertical method[8]. In this paper, we use a data structure called IG, which also is hybrid method. And this structure possesses the following advantages[9 z One-to-one mapping relation between  item labels and natural number field z Ignoring transaction labels and linking corresponding item in different transactions z Item table IT storing all items occurring in the transaction database, which element has a support count. And the first occurrence of all items all link to the item table IT z Linking all transactions to the item table IT z IL represents mass transactions in transaction database D , and these transactions are the subsets of all items in the item table IT. The relative position of the elements in these item sets is in accordance with the position in the item set IT. Besides, each item in one transaction is linked to corresponding item in next transaction. In other words, we use a chain table to link the first occurrences of all items, and so we could easily get the support count of every item As shown in figure 3, all frequent item sets whose support greater than the smallest support count 2 in the transaction database D are considered and recorded in the item table IT. For example, to examine the occurrence of i7 we first localize the position of i7 in I1 from IL, and then find i7 in I3 other than to scan I2  Fig. 3 The data structure IG Due to holding all advantages of horizontal data layout and vertical data layout, similar to H-structure, IG guarantees the implement of the algorithm that applies these two data layouts. The difference between IG and H-structure is: in IG there is a link between every occurrence and next occurrence of each item, while in H-structure, the chain table always points to the first occurrence of each item, so to fetch some item, and we have to traverse from the head of some transaction. Because the chain table in IG directly points to the same item of different transactions, when we traverse all occurrences of each item, the time needed is drastically shortened to some extent II. MINING ALGORITHM 


At present, many relative algorithms have been proposed and could be group into three classes[10,11]: the first class is the improved algorithm based on Apriori algorithm, which is built upon the Boolean association rule problem proposed by Agrawal R. in 1994; the second class is partition-based algorithm, and which is high parallelable, because ultimate frequent item sets generating depends on the frequent item sets on many independent logical blocks; the last class is FP-tree frequent item sets algorithm whose proposal successfully overcomes the drawback that Apriori algorithm may brings redundant candidate item sets. The paper proposes an association rules mining algorithm based on the third class algorithm Firstly, we introduce the concept of compressed transaction tree. Because of having many same subtrees, a complete storage tree must waste lots of memory resource, so to diminish the node size of the transaction tree, we would adopt a compression method to group all transactions to share the same nodes. As shown in figure 4, the storage tree has three same subtrees t1, t2 and t3, and generating a complete storage tree needs lots of extra memory  Fig. 4 The storage tree containing three same subtrees So in the paper, we adopt a compressive technology to store the same subtrees to diminish the node size of the storage tree, as shown in figure 5, which node size is decreased to 8 nodes from 16 nodes in figure 4. For example for an n-item set, its corresponding complete storage tree has 2n nodes, while its compressed storage tree has 2n-1 nodes at the most, so the size of the storage tree is diminished to the half at the least. In fact, the number of all nodes in the transaction database D  is always seriously less than the maximum of the problems discovered in the frequent item sets Of course, each node in the compressed storage tree needs to add a new field, which includes two parts that are the layer and the access count of the head item of each different sub item set, to store some extra information about the nodes pruned from the complete storage tree t3t2 t1 Root 2 


3 4 4 4 4 3 1 2 3 4 4 4 3 4 Item 1 3 4 5 7 Count 4 4 5 3 2IT IL 3 4 5 7 1 4 5 7 1 3 4 5 1 3 4 1 3 4 I1 I2 I5 I4 I3 174 Fig. 5 The compressed transaction tree In figure 5, the node 3 has three access counts: \(0,1 1,1 2,1 0,1 that there is a transaction, which head item locates the zeroth layer, and a path, which includes three nodes i1,i2 and i3, from the root to the node 3. The next access count \(1,1 there is a transaction whose head node locates the first layer and a path that consists of the i2 and i3 from the root the node 3. In like manner, \(2, 1 whose head node locates the second layer and a path that is composed of the root and the node 3. So if we suppose the transaction count of each node of the storage tree in figure 4 is 1, then the item sets surrounded by dash rectangles could map to the item sets of the compressed transaction tree in figure 4 we note these dash rectangles are not the parts of the data structure IG, just for illustration expediently 


Because the paths from the root to those different nodes correspond to the different transactions, we could group all transactions by sharing the same items, which high improves the performance of the mining algorithm Next, we would describe the mining algorithm based on the pattern growth algorithm in detail, which adopts the above compressed transaction tree Algorithm IFP-growth includes four procedures: the first procedure is to extract the 1-item sets. Firstly, we should initialize the item table IT. Because the frequent item sets meet the prior principle, which means they hold the character of anti-monotone, for mining frequent item sets, we only need 1-item sets, which are attained by scanning the transaction database and finding the item sets whose support count greater than mini-sup. Then we store these frequent 1-item sets to the item table IT. To facilitate the carryout of the next procedure we sort all elements by ascent according to their support count and denote them by the integer; the second procedure is to generate the compressed transaction tree. When we generate the compressed transaction tree, the items is denoted by their label, and insert all different transactions to the compressed transaction tree. The nodes of the compressed transaction tree correspond to the all frequent 1-itemsets. The count set of each node denotes the number of the transactions, which contain the subsets of all items in the path from the root to this node The next procedure is to traverse the compressed transaction tree to constitute the item table IT and the transaction chain IL for generating IG; The last procedure is to mine frequent item sets. In this procedure, we use a recursive function to assist mining the frequent k-item sets, where k > 2 To illustrate the above procedures, we consider the following example. Suppose mini-sup is 2, we want to mine all frequent item sets in the transaction database. The first step is to scan all transaction for attaining frequent 1-item sets Examine every item in each transaction whether is the element of the item table IT or not, if the answer is positive, and then add the count of corresponding element, otherwise we should add this item to the IT and let its count be 1. When we finish scanning the database, we get all frequent 1-item sets {1 3}, {4}, {5}, {7}. After attaining all 1-item sets, we should sort all elements in IT by ascent according to their supports and use the integer to denote them. As show in table 1 


Table 1 Item table IT index item Count The second step is to generate a transaction tree. We utilize the information in table 1 to insert all transactions containing some 1-item set corresponding to some node into the transaction tree. For example, i7 is mapped to it1 in the transaction tree, i5 is mapped to i2, i3 is mapped to it3 and so on. As shown in figure 6  Fig. 6 The compressed storage tree of the sample database Each node all stores the count of the transactions that contain the corresponding item, so every node in the tree may be accessed many times. For example, the count \(0, 0 node 5 in the transaction tree denote the item set {it1, it2, it3 it4, it5} that is not occur all the time, where the first 0 means the item set starts at the zeroth layer and the second 0 means the count of the transactions that contain this item set To implement this compressed transaction tree, we need to put all access point into a array, and some layer of a access point is a array index other than storing it explicitly The third step is to generate IL in the item table IT, and traverse the transaction tree by applying depth-first algorithm Theoretically, all nodes whose transaction counts greater than 4 5 0 0 1 1 2 2 3 0 4 0 0 0 1 0 2 0 3 0 123 23 3 0 0 1 0 2 0 


0 0 1 0 0 0 1 0 1 2 3 4 0 0 1 12 2 124 24 1234 234 34 4 12345 2345 345 45 5 55 1235 235 35 0 1 1 0 2 0 0 1 1 0 1245 245 0 1 1 1 2 1 3 1 123 23 3 0 1 1 1 2 1  


0 1 1 1 0 1 0 1 1 1 1 2 3 4 3 4 4 4 0 1 1 12 2  13 0 1 0 1 134 124 24 1234 234 34 4 14 175 0 in the tree should be mapped to the access points of the transaction chain. In figure 6, there are 3 such nodes, and it means there are 3 corresponding access points of the transaction tree. What we to do is only to link the all non-zero accesses to the corresponding lines in the transaction chain. As shown in figure 7  Fig. 7 Item Group data structure In the improved IG data structure, we put the access count of every line to a array, so the array indexes are not stored directly The last step is to mine longer frequent item sets than 1item sets. The result is shown in table 2 table 2 The frequent item sets mined recursively Prefix T1\(count count 1 2\(2 1 1 2 2 2 2 12 3\(1 1 2 2 2 3\(2 2 3 3 2 2 3 23 4\(1 2 2 24 5\(2 2 


3 4\(3 4 4 3 4 34 5\(3 3 4 5\(4 4 4 5 None 5\(5  The algorithm IFP-growth is described as follow  Algorithm IFP-growth\(D, min-sup. All frequent item sets begin procedure Generate_IFIS \( DB transaction I, item i begin for I D for i I if i IT then count \( i else inset i to IT count \( i end procedure Gene_ITree \( D begin for I D for i I if i IT?  and count \( i then insert i to transaction ree end procedure Gene_IG \( ITree transaction IL begin for depth-first traverse path ITree let path map  to a access  in IL add the access count into the access link IL to the previous occurrence of  i end procedure IFP-growth \( IG temporary list T1 begin for i IT? ? and count \( i add i to the frequent item sets 


generate T1 for 1m T? ?  and count \( m add im to frequent item sets for 1n T? ?  and count \( n after m Mfis \( im, n end procedure Mfis \( im, n recursively begin      // im is prefix, n is the item after im patterm cim=the count list of im item cn=the count list of n item cimn cim cn if cimn>=min-sup new_im = union im and n add new_im to the frequent item sets for 1i T? ?  and count \( i after n Mfis \( new_im , 1 end end  After generating frequent 2-item sets using the prefix it1 we use the count list of the associative items to generate frequent 3-item sets. The rest can be done in the same manner  III. THE EXPERIMENTAL ESTIMATION The paper uses PC as the experimental environment, and the configuration of the PC is Pentium\(R 1.80GHZ, 896M memory, 120G IDE hard disk. The operating system we use is Windows XP, and the programming environment is Visual Studio 2008, and the programming language is C#, and the dataset we use is commercial transaction record In our dataset, there are 150, 000 records, and in all records there are 1000 items. Besides, in each record, there is about 15 items 1 IT Index 1 2 3 4 5 Item 7 5 3 1 4 Count 2 3 4 4 5 


IL 2 3 4 5 1 2 4 5 1 2 3 5 I1 I2 I3 1 11 1 2 1 2 176 The scalability is an index that measures the ability to either handle growing amounts of work in a graceful manner or to be readily enlarged. The following figure 8 indicates the scalability of our algorithm whose minimum supports respectively is 0.002, 0.004 and 0.005 0 50 100 150 200 250  The size of dataset/10,000 R es po ns e tim e S    a 0 50 100 150 200 


 The size of dataset/10,000 R es po ns e tim e S    b 0 50 100 150  The size of data/10, 000 R es po ns e tie m S    c Fig. 8 The change of the response time in our algorithm as the size of data growing We know FP-growth algorithm is a classical algorithm In our paper, we compare our algorithm with FP-growth upon the scalability when the size of data respectively are 0.002 0.004 and 0.005. As shown in figure 9 0 50 100 150 


200 250 300  The size of dataset/10,000 R es po ns e tim e S    a 0 50 100 150 200  The size of dataset/10,000 R es po ns e tim e S    b 0 10 20 30 40 50 


60  The size of dataset/30,000 R es po ns e tim e S    c Fig. 9 The scalability of our algorithm compared with FP-growth  Paper [12] proposed a way to reduce times of scanning transaction database to reduce the cost of I/O IV. CONCLUSIONS AND FUTURE WORK This paper first discusses the theory of foundations and association rules and presents an association rules mining algorithm, namely, FP-growth algorithm. And then we propose an improved algorithm IFP-growth based on many association rules mining algorithms. At last we implement the algorithm we propose and compare it with algorithm FPgrowth algorithm. The experimental evaluation demonstrates its scalability is much better than algorithm FP-growth 177 Now, lets forecast something we want to do someday Firstly, we would parallelize our algorithm, because data mining needs massive computation, and a parallelable environment could high improve the performance of the algorithm; Secondly, we would apply our algorithm on much more datasets and study the run performance; At last, we would study the performance when the algorithm deal with other kinds of association rules  REFERENCES 1] S. Sumathi and S. N. Sivanandam. Introduction to Data Mining and its Applications, Springer, 2006 2] V. J. Hodge, J. Austin, A survey of outlier detection 


methodologies, Artificial Intelligence Review, 2004, 22 85-126 3] Han, J. and M. Kamber. Data Mining: Concepts and Techniques. Morgan Kaufmann, San. Francisco, 2000 4] Jianchao Han, Mohsen Beheshti. Discovering Both Positive and Negative Fuzzy Association Rules in Large Transaction Databases, Journal of Advanced Computational Intelligence and Intelligent Informatics 2006, 10\(3 5] Jiuyong Li, Hong Shen, Rodney Topor. Mining Informative Rule Set for Prediction. Journal of Intelligent Information Systems, 2004, 22\(2 6] Jianchao Han, and Mohsen Beheshti. Discovering Both Positive and Negative Fuzzy Association Rules in Large Transaction Databases. Journal of Advanced Computational Intelligence, 2006, 10\(3 7] Doug Burdick, Manuel Calimlim, Jason Flannick Johannes Gehrke, Tomi Yiu. MAFIA: A Maximal Frequent Itemset Algorithm. IEEE Transactions on Knowledge and Data Engineering, 2005, 17\(11 1504 8] Assaf Schuster, Ran Wolff, Dan Trock. A highperformance distributed algorithm for mining association rules. Knowledge and Information Systems, 2005, 7\(4 458-475 9] Mohammed J. Zaki. Mining Non-Redundant Association Rules. 2004, 9\(3 10] J.Han, J.Pei, Y.Yin, Mining frequent patterns without candidate generation, Proceedings ACM SIGMOD 2000 Dallas, TX, May 2000: 1-12 11] P.Viola, M.Jones. Rapid Object Detection Using A Boosted Cascade of Simple Features. Proc. IEEE Conf. on Computer Vision and Pattern Recognition, 2001 12] Anthony K. H. Tung, Hongjun Lu, Jiawei Han, Ling FengJan. Efficient Mining of Intertransaction Association Rules. 2003, 154\(1 178 


For each vertex b in g form j forests body\(a, g, i s.t. bodyAnt\(a, g, i a, g, i with itemsets Ant\(b b and each subset of itemsets Ant\(b b in P\(a, g, j Assign to each leaf l of trees bodyAnt\(a, g, i bodyCons\(a, g, i a fresh variable Vm,M, m, M = size\(itemset\(l Assign to each leaf l of tree headAnt\(a, g, j the variable assigned to itemset l in some leaf of some tree bodyCons\(a, g, i TABLE II.  EXPERIMENTAL DATA Conf. #rules #pruned #dftrs PtC 0.5 6604 2985 1114 0.6 2697 2081 25 0.75 1867 1606 10 0.8 1266 1176 0 0.95 892 866 1 0.98 705 699 1 DSP 0.5 2473 1168 268 0.6 1696 869 64 0.75 1509 844 89 0.8 1290 1030 29 0.95 1032 889 15 0.98 759 723 1 Arry 0.5 770 492 82 0.6 520 353 60 0.75 472 327 39 0.8 408 287 22 0.95 361 255 25 0.98 314 243 30  Our induction algorithm has been launched for each combination of thresholds. Our scheme eliminates all redundant rules in the sense of [25, 31], i.e. those association rules that are not in the covers. All the meta-rule deductive schemes implicitly included in [25] and [31] are induced by our method. The percentage of pruning, thus, outperforms [25 


The results produced for k=3, support 0.25 and confidences between 0.7 and 0.99 are shown in Fig. 3, in terms of pruning percentage \(vertical axis when applied to low confidences \(from 0.7 to 0.9 The percentage of pruning achieved diminishes as the confidence is superior to 0.9. Nevertheless, the pruning is effective with confidence of 0.99 in the majority of cases Pruning at Support = 0.25 0,00 5,00 10,00 15,00 20,00 25,00 30,00 35,00 40,00 45,00 50,00 0,7 0,8 0,9 0,95 0,99 Confidence P ru n in g L e v e l Case 1 Case 2 Case 3  Figure 3.  Pruning experiences at support 0.25  V. DISCUSSION AND CHALLENGES It is important to discuss the technique presented here with focus on the purpose the technique pursues:  to produce semantic recommendation The reader should have noticed that the algorithm presented 


relies strongly on "choice". For instance, the algorithm chooses ears in the graph to form an order for elimination, and the choice is arbitrary. This strategy is essential to maintain low complexity \(polynomial practical. Nevertheless, a warned reader may conclude that this arbitrary choice implies that there are many compactions to produce and therefore the approach as a whole does not show to produce an optimal solution. And the reader is right in this conclusion. Since the goal is compaction, the search for an optimal solution can be bypassed provided a substantial level of pruning is achieved To complete the whole view, we describe how web service descriptions are complemented with the association rules as recommendations. In effect, under our scheme, the document describing the web service is augmented with a set of OWL/RDF/S triples that only incorporate the non-pruned rules with the format of Example 1, that is, the set ARmin of the compaction program obtained by our algorithm, together with the thresholds applied to the mining process and a registered URI of a registered description service. The assumptions and defeaters are not added to the web service description. If the associations encoded in the triples are not sufficient for the client \(a search engine, for instance widening of the response to the description service identified by the given URI, and then the assumptions and defeaters are produced. The reasoning task required for deriving all the implicitly published rules is client responsibility Notice that, under this scheme, the actual rules that appear as members of the set initial ARmin set are irrelevant; the only important issue is the size of the set The developed scheme also supports an extension of the algorithm that admits the assignment of priorities to rules and to itemsets, in order to allow the user to produce a more controlled program as output. Nonetheless, the importance of the extension has not been already tested, and therefore it is beyond the subject of the present paper It would be also interesting to design a scheme that supports queries where the client provides an itemset class and values for support and confidence and the engine produces a maximal class of inferred associated itemsets as a response. This scheme is also under development, so we have not discussed this aspect here 


VI. CONCLUSION In this paper, we have presented a defeasible logic framework for managing associations that helps in reducing the number of rules found in a set of discovered associations. We have presented an induction algorithm for inducing programs in our logic, made of assumption schemas, a reduced set of association rules and a set of counter-arguments to conclusions called defeaters, guaranteeing that every pruned rule can be effectively inferred from the output. Our approach outperform those of [17], because all reduction compactions presented there can be expressed and induced in our framework, and several other patterns, particular to the given datasets, can also be found. In addition, since a set of definite clauses can be obtained from the induced programs, the knowledge obtained can be modularly inserted in a richer inference engine Abduction can be also attempted, asking for justifications that explain the presence of certain association in the dataset The framework presented can be extended in several ways Admitting defeaters to appear in the head of assumption, to define user interest Admitting arithmetic expressions within assumptions for adjustment in pruning Admitting set formation patterns as itemset constants Extending the scope, to cover temporal association rules REFERENCES 1]  R. Agrawal, and R. Srikant: Fast algorithms for mining association rules In Proc. Intl Conf. Very Large Databases. \(1994 2]  A. V. Aho, J. E. Hopcroft, J. Ullman. The design and analysis of computer algorithms, Addison-Wesley, 1974 3]  G. Antoniou, D. Billington, G. Governatori, M. J. Maher, A. Rock: A Family of Defeasible Reasoning Logics and its Implementation. ECAI 2000: 459-463 4]  G. Antoniou, D. Billington, G. Governatori, M. J. Maher: Representation results for defeasible logic. ACM Trans. Comput. Log. 2\(2 2001 5]  A. Basel, A. Mahafzah, M. Al-Badarneh: A new sampling technique for association rule mining, Journal of Information Science, Vol. 35, No. 3 358-376 \(2009 6]  R. Bayardo and R. Agrawal: Mining the Most Interesting Rules. In Proc of the Fifth ACMSIGKDD Intl Conf. on Knowledge Discovery and Data Mining, 145-154, \(1999 


7]  R. Bayardo, R. Agrawal, and D. Gunopulos: Constraint-based Rule Mining in Large, Dense Databases. Data Mining and Knowledge Discovery Journal, Vol. 4, Num-bers 2/3, 217-240. \(2000 8]  A. Berrado, G. Runger: Using metarules to organize and group discovered association rules. Data Mining and Knowledge Discovery Vol 14, Issue 3. \(2007 9]  S. Brin, R. Motwani, J. Ullman, and S. Tsur: Dynamic itemset counting and implication rules for market basket analysis. In Proc. ACMSIGMOD Intl Conf. Management of Data. \(1997 10] L. Cristofor and D.Simovici: Generating an nformative Cover for Association Rules. In ICDM 2002, Maebashi City, Japan. \(2002 11] Y. Fu and J. Han: Meta-rule Guided Mining of association rules in relational databases. In Proc. Intl Workshop on Knowledge Discovery and Deductive and Object-Oriented Databases. \(1995 12] B. Goethals, E. Hoekx, J. Van den Bussche: Mining tree queries in a graph. KDD: 61-69. \(2005 13] G. Governatori, D. H. Pham, S. Raboczi, A. Newman and S. Takur: On Extending RuleML for Modal Defeasible Logic. RuleML, LNCS 5321 89-103. \(2008  14] G. Governatori and A. Stranieri. Towards the application of association rules for defeasible rules discovery In Legal Knowledge and Information Systems, JURIX, IOS Press, 63-75. \(2001 15] J. Han, J. Pei and Y. Yin: Mining frequent patterns without candidate generation. In Proc. ACM-SIGMOD Intl Conf. Management of Data 2000 16] C. Hbert, B. Crmilleux: Optimized Rule Mining Through a Unified Framework for Interestingness Measures. DaWaK: LNCS 4081, 238247. \(2006 17] E. Hoekx, J. Van den Bussche: Mining for Tree-Query Associations in a Graph. ICDM 2006: 254-264 18] R. Huebner: Diversity-Based Interestingness Measures For Association Rule Mining. Proceedings of ASBBS Volume 16 Number 1, \(2009 19] B. Johnston, Guido Governatori: An algorithm for the induction of defeasible logic theories from databases. Proceedings of the 14th Australasian Database Conference, 75-83. \(2003 20] P. Kazienko: Mining Indirect Association Rules For Web Recommendation. Int. J. Appl. Math. Comput. Sci., Vol. 19, No. 1, 165 186. \(2009 21] M. Klemettinen, H. Mannila, P. Ronkainen, H. Toivonen, and A Verkamo: Finding interesting rules from large sets of discovered association rules. In Proc. 3rd Intl Conf. on Information and Knowledge 


Management. \(1994 22] M. J. Maher, A. Rock, G. Antoniou, D. Billington, T. Miller: Efficient Defeasible Reasoning Systems. International Journal on Artificial Intelligence Tools 10\(4 2001 23] C. Marinica, F. Guillet, and H. Briand: Post-Processing of Discovered Association Rules Using Ontologies. The Second International Workshop on Domain Driven Data Mining, Pisa, Italy \(2008 24] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal: Closed sets based discovery of small covers for association rules. In Proc. BDA'99 Conference, 361-381 \(1999 25] N. Pasquier, R. Taouil, I. Bastide, G. Stume, and  L. Lakhal: Generating a Condensed Representation for Association Rules. In Journal of Intelligent Information Systems, 24:1, 29-60 \(2005 26] P. Pothipruk, G. Governatori: ALE Defeasible Description Logic Australian Conference on Artificial Intelligence.  110-119 \(2006 27] J. Sandvig, B. Mobasher Robustness of collaborative recommendation based on association rule mining, Proceedings of the ACM Conference on Recommender Systems \(2007 28] W. Shen, K. Ong, B. Mitbander, and C. Zaniolo: Metaqueries for data mining. In Fayaad, U. et al. Eds. Advances in Knowledge Discovery and Data Mining. \(1996 29] I. Song, G. Governatori: Nested Rules in Defeasible Logic. RuleML LNCS 3791, 204-208 \(2005 30] H. Toivonen, M. Klemettinen, P. Ronkainer, K. Hatonen, and H Mannila: Pruning and grouping discovered association rules. In ECML Workshop on Statistics, Machine Learning and KDD. \(1995 31] M. Zaki: Generating Non-Redundant Association Rules. In Proc. of the Sixth ACMSIGKDD Intl Conf. on Knowledge Discovery and Data Mining, 34-43, \(2000 32] w3c. OWL Ontology Web Language Reference. In http://www.w3.org/TR/2004/REC-owl-ref-20040210 33] w3c. RDF/XML Syntax Specification. In: http://www.w3.org/TR/rdfsyntax-grammar 34] w3c. RDF Schema. In: http://www.w3.org/TR/rdf-schema      


 8   2  3\f            8  D    F  \b 1 8 & #J      b 1  1  4    2  


4 1    9  E 1  2 4 1    9 1   4      8 2  8 1  D 1        1 1  b 


     b b b b b  K            8          2 D 9   F  \b 1 8 ,+J  9 


     b 1     1 2  9 1  12 L 1   9  8       1  2      2   


     b b b b b  K            2  0 \b f  b\f      9       


  8 2   E 1   1     M13 31L 1    b  8E 1   1 #3\b?### 1  1     E 1   1 \b?###3        


1   1   b 1  2 2 18 2     8              1    2 \b 1    2  


    2          2   1 L 2 1   1   L 2 2    2 1  2        


    8  2H D \b A             2  2H D \b A 2 \f 3%\f  f   4%\f f !  , \f\b  C    2    2 


 6    3 1      253 6   1 L 2    6   1         f\b3\f       


               1     1     8 2    E       2  1   


     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


