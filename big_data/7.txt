Moment Maintaining Closed Frequent Itemsets r a Stream Sliding Window Yun Chi 001  Haixun Wang 201  Philip S Yu 201  Richard R Muntz 001 001 Department of Computer Science University of California Los Angeles CA 90095 201 IBM Thomas J Watson Research Center awthorne NY 10532 ychi@cs.ucla.edu  haixun,psyu  us.ibm.com muntz@cs.ucla.edu Abstract This paper considers the problem of mining closed frequent itemsets over a sliding window using limited mem 
ory space We design a synopsis data structure o monitor transactions in the sliding window so that we can output the current closed frequent itemsets at any time Due to time and memory constraints the synopsis data structure cannot monitor all possible itemsets However monitoring only frequent itemsets will make it impossible to detect new itemsets when they become frequent In this paper e introduce a compact data structure the closed enumeration tree CET to maintain a dynamically selected set of itemsets over a sliding-window The selected itemsets consist of a boundary between closed frequent itemsets and the rest of the itemsets Concept drifts in a data stream are re\337ected by boundary movements in the CET n other words a status change f any itemset e.g from non-frequent to frequent 
must occur through the boundary Because the boundary is relatively stable the cost of mining closed frequent itemsets over a sliding window is dramatically reduced to that of mining transactions that can possibly cause boundary movements in the CET Our experiments show that our algorithm performs much better than previous approaches 1 Introduction Mining data streams for knowledge discovery is important to many applications such as fraud detection intrusion detection trend learning etc In this paper we consider the problem of mining closed frequent itemsets on data streams Mining frequent itemset on static datasets has been studied extensively owever data streams have posed new challenges First data streams are continuous high-speed 
and unbounded It is impossible to mine association rules from them using algorithms that require multiple scans Second the data distribution in streams are usually changing with time and very often people are interested in the most recent patterns It is thus of great interest to mine itemsets that are currently frequent One approach is to always focus on frequent itemsets in the most recent window A similar effect can be achieved by exponentially discounting old itemsets 001 The work of these two authors was partly supported by NSF under Grant Nos 0086116 0085773 and 9817773 For the window-based approach we can come up with two naive methods 
1 Regenerate frequent itemsets from the entire window whenever a new transaction comes into or an old transaction leaves the window 2 Store every itemset frequent or not in a traditional data structure such as the pre\223x tree and update its support whenever a new transaction comes into or an old transaction leaves the window Clearly method 1 is not ef\223cient In fact as long as the window size is reasonable and the concept drifts in the stream is not too dramatic most itemsets do not change their status from frequent to non-frequent or from nonfrequent to frequent often Thus instead of regenerating all frequent itemsets every time from the entire window we shall adopt an 
incremental approach Method 2 is incremental However its space requirement makes it infeasible in practice The pre\223x tree is often used for mining association rules on static data sets In a pre\223x tree each node n I represents an itemset I and each child node of n I represents an itemset obtained by adding a new item to I  The total number of nodes is exponential Due to memory constraints we cannot keep a pre\223x tree in memory and disk-based structures will make real time update costly In view of these challenges we focus on a 
dynamically selected set of itemsets that are i informative enough to answer at any time queries such as 215what are the closed frequent itemsets in the current window\216 and at the same time ii small enough so that they can be easily maintained in memory and updated in real time The problem is of course what itemsets shall we select for this purpose To reduce memory usage we are tempted to select for example nothing but frequent or even closed frequent itemsets However if the frequency of a non-frequent itemset is not monitored we will never know when it becomes frequent A naive approach is to monitor all itemsets whose support is above a reduced threshold minsup 212 001 
 so that we will not miss itemsets whose current support is within 001 of minsup when they become frequent This approach is apparently not general enough In this paper we design a synopsis data structure to keep track of the boundary between closed frequent itemsets and the rest of the itemsets Concept drifts in a data stream are re\224ected by boundary movements in the data structure In Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


other words a status change of any itemset e.g from nonfrequent to frequent must occur through the boundary The problem of mining an in\223nite amount of data is thus converted to mine data that can potentially change the boundary in the current model Because most of the itemsets do not often change status which means the boundary is stable and even if some does the boundary movement is local the cost of mining closed frequent itemsets is dramatically reduced Our Contribution This paper makes the following contributions 1 We introduce a novel algorithm Moment 1  to mine closed frequent itemsets over data stream sliding windows To the best of our knowledge our algorithm is the 223rst one for mining closed frequent itemsets in data streams 2 We present an in-memory data structure the closed enumeration tree CET which monitors closed frequent itemsets as well as itemsets that form the boundary between the closed frequent itemsets and the rest of the itemsets We show that i a status change of any itemset e.g from non-frequent to frequent must come through the boundary itemsets which means we do not have to monitor itemsets beyond the boundary and ii the boundary is relatively stable which means the update cost is minimum 3 We introduce a novel algorithm to maintain the CET in an ef\223cient way Experiments show Moment has signi\223cant performance advantage over state-of-the-art approaches for mining frequent itemsets in data streams Related Work Mining frequent itemsets from data streams has been investigated by many researchers Manku et al proposed an approximate algorithm that for a iven time t  mines frequent itemsets over the entire data streams up to t  Charikar et al presented a 1 pass algorithm that returns most frequent items whose frequencies satisfy a threshold with high probabilities Teng et al presented an algorithms FTP-DS that mines frequent temporal patterns from data streams of itemsets Chang et al presented an algorithm estDec  that mines recent frequent itemsets where the frequency is de\223ned by an aging function Giannella et al proposed an approximate algorithm for mining frequent itemsets in data streams during arbitrary time intervals An in-memory data structure FP-stream isusedto store and update historic information about frequent itemsets and their frequency over time and an aging function is used to update the entries so that more recent entries are weighted more Asai et al presented an online algorithm StreamT  for mining frequent rooted ordered trees To reduce the number of subtrees to be maintained an update policy that is similar to that in online association rule mining w a s used and therefore the results are ine xact In all these studies approximate algorithms were adopted In contrast our algorithm is an exact one because we assume that the approximation step has been implemented through the sampling scheme and our algorithm works on a sliding window containing the random samples which are a synopsis of the data stream In addition closely related to our work Cheung et al 7 and Lee et al 13 proposed algorithms to maintain discovered frequent itemsets through incremental updates Although these algorithms are exact they focused on min1 M aintaining Clo sed Frequent Item sets by Incremen tal Updat es ing all frequent itemsets as o the above approximate algorithms The e number of frequent itemsets makes it impractical to maintain information about all frequent itemsets using in-memory data structures In contrast our algorithm maintains only closed frequent itemsets As demonstrated by extensive experimental studies e.g there are usually much fewer closed frequent itemsets compared to the total number of frequent itemsets 2 Problem Statement Preliminaries Given a set of items 001  a database D wherein each transaction is a subset of 001  and a threshold s called the minimum support  minsup  0 s 001 1  the frequent itemset mining problem is to 223nd all itemsets that occur in at least s D transactions We assume that there is a lexicographical order among the items in 001 and we use X 002 Y to denote that item X is lexicographically smaller than item Y  Furthermore an itemset can be represented by a sequence wherein items are lexicographically ordered For instance  A B C  is represented by ABC given A 002 B 002 C  We also abuse notationbyusing 002 to denote the lexicographical order between two itemsets For instance AB 002 ABC 002 CD  As an example let 001  A B C D   D   CD B ABC ABC   and s  1 2  then the frequent itemsets are F    A 3   B 3   C 3   AB 3   AC 2   BC 2   ABC 2  In F  each frequent itemset is associated with its support in database D  Combinatorial Explosion According to the a priori property any subset of a frequent itemset is also frequent Thus algorithms that mine all frequent itemsets often suffer from the problem of combinatorial explosion Two solutions have been proposed to alleviate this problem In the 223rst solution e.g 11 only maximal frequent itemsets are discovered A frequent itemset is maximal if none of its proper supersets is frequent The total number of maximal frequent itemsets M is much smaller than that of frequent itemsets F  and we can derive each frequent itemset from M  However M does not contain information of the support of each frequent itemset unless it is a maximal frequent itemset Thus mining only maximal frequent itemsets loses information In the second solution e.g 17 only closed frequent itemsets are discovered An itemset is closed if none of its proper supersets has the same support as it has The total number of closed frequent itemsets C is still much smaller than that of frequent itemsets F  Furthermore we can derive F from C  because a frequent itemset I must be a subset of one or more closed frequent itemset and I 220s support is equal to the maximal support of those closed itemsets that contain I  In summary the relation among F  C  and M is M\003 C\003F  The closed and maximal frequent itemsets for the above examples are C    C 3   AB 3   ABC 2  M    ABC 2  2 Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


Since C is smaller than F  and C does not lose information about any frequent itemsets in this paper e focus on mining the closed frequent itemsets because they maintain suf\223cient information to determine all the frequent itemsets as well as their support Problem Statement The problem is to mine closed frequent itemsets in the most recent N transactions in a data stream Each transaction has a time stamp which is used as the tid transaction id of the transaction Figure 1 is n example with 001  A B C D  and window size N 4  We use this example throughout the paper with minimum support s  1 2  window #3  tid items 1 2 3 4 5 6 C,D A,B A,B,C A,B,C A,C,D B,C time line window #1 window #2 Figure  A Running Example To 223nd frequent itemsets on a data stream we maintain a data structure that models the current frequent itemsets We update the data structure incrementally The combinatorial explosion problem of mining frequent itemsets becomes even more serious in the streaming environment As a result on the one hand we cannot afford keeping track of all itemsets or even frequent itemsets because of time and space constraints On the other hand any omission for instance maintaining only M  C or F instead of all itemsets may prevent us from discovering future frequent itemsets Thus the challenge lies in designing a compact data structure which does not lose information of any frequent itemset over a sliding window 3 The Moment Algorithm We propose the Moment algorithm and an in-memory data structure the closed enumeration tree  to monitor a dynamically selected small set of itemsets that enable us to answer the query 215what are the current closed frequent itemsets?\216 at any time 3.1 The Closed Enumeration Tree Similar to a pre\223x tree each node n I in a closed enumeration tree CET represents an itemset I  A child node n J  is obtained by adding a new item to I such that I 002 J  However unlike a pre\223x tree which maintains all itemsets a CET only maintains a dynamically selected set of itemsets which include i closed frequent itemsets and ii itemsets that form a boundary between closed frequent itemsets and the rest of the itemsets As long as the window is reasonably large and the concept drifts in the stream are not too dramatic most itemsets do not change their status from frequent to non-frequent or from non-frequent to frequent In other words the effects of transactions moving in and out of a window offset each other and usually do not cause change of status of many involved nodes If an itemset does not change its status nothing needs to be done except for increasing or decreasing the counts of the involved itemsets If it does change its status then as we will show the change must come through the boundary nodes which means the changes to the entire tree structure is still limited 2 1 2 3 4 C,D A,B A,B,C A,B,C items window #1      ACAB ABCD ABC 3331 32 tid Figure  The Closed Enumeration Tree Corresponding to Window 1 each node is labeled with its support  We further divide itemsets on the boundary into two categories which correspond to the boundary between frequent and non-frequent itemsets and the boundary between closed and non-closed itemsets respectively Itemsets within the boundary also have two categories namely the closed nodes and other intermediary nodes that have closed nodes as descendants For each category e de\223ne speci\223c actions to be taken in order to maintain a shifting boundary when there are concept drifts in data streams Section 3.3 The four types of itemsets are listed below infrequent gateway nodes A node n I is an infrequent gateway node if i I is an infrequent itemset ii n I 220parent n J  is frequent and iii I is the result of joining I 220s parent J  with one of J 220s frequent siblings In Figure 2 D is an infrequent gateway node represented by dashed circle In contrast AD is not an infrequent gateway node hence it does not appear in the CET because D is infrequent unpromising gateway nodes A node n I is an unpromising gateway node if i I is a frequent itemset and ii there exists a closed frequent itemset J such that J 002 I  J 004 I  and J has the same support as I does In Figure 2 B is an unpromising gateway node because AB has the same support as it does So is AC because of ABC  In Figure 2 unpromising gateway nodes are represented by dashed rectangles For convenience of discussion when a node in the CET is neither an infrequent gateway node nor an unpromising gateway node we call it a promising node intermediate nodes A node n I is an intermediate node if i I is a frequent itemset ii n I has a child node n J such that J has the same support as I does and iii n I is not an unpromising gateway node In Figure 2 A is an intermediate node because its child AB has the same support as A does closed nodes These nodes represent closed frequent itemsets in the current sliding-window A closed node can be an internal node or a leaf node In Figure 2 C  AB  and ABC are closed nodes which are represented by solid rectangles 3 Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


3.2 Node Properties We prove the following properties for the nodes in the CET Properties 1 and 2 enable us to prune a large amount of itemsets from the CET while Property 3 makes sure certain itemsets are not pruned Together they enable us to mine closed frequent itemsets over a sliding window using an ef\223cient and compact synopsis data structure Property 1 If n I is an infrequent gateway node then any node n J where J 004 I represents an infrequent itemset Proof Property 1 is derived from the a priori property A CET achieves its compactness by pruning a large amount of the itemsets It prunes the descendants of n I and the descendants of n I 220s siblings nodes that subsume I  However it 217remembers\220 the boundary where such pruning occurs so that it knows where to start exploring when n I is no longer an infrequent gateway node An infrequent gateway node marks such a boundary In particular infrequent gateway nodes are leaf nodes in a CET For example in Figure 2 after knowing that D is infrequent we do not explore the subtree under D  Furthermore we do not join A with D to generate A 220s child nodes As a result a large amount of the itemsets are pruned Property 2 If n I is an unpromising gateway node then n I is not closed and none of n I 325s descendents is closed Proof Based on the de\223nition of unpromising gateway nodes there exists an itemset J such that i J 002 I  and ii J 004 I and support  J  support  I   From ii we know n I is not closed Let i max be the lexicographically largest item in I  Since J 002 I and J 004 I  there must exist an item j 005 J  I such that j 002 i max  Thus for any descendant n I 001 of n I wehave j 005 I 001  Furthermore because support  J  support  I   itemset J  I must appear in every transaction I appears which means support  n I 001  support  n  j 002 I 001  so I 001 is not closed Descendants of an unpromising gateway node are pruned because no closed nodes can be found there and it 217remembers\220 the boundary where such pruning occurs Property 3 If n I is an intermediate node then n I is not closed and n I has closed descendants Proof Based on the de\223nition of intermediate nodes n I is not closed Thus there must exists a closed node n J such that J 004 I and support  J  support  I  If I 002 J  then n J is n I 220s descendant since J 004 I If J 002 I  then n I is an unpromising y node which means n I is not an intermediate node Property 3 shows that we cannot prune intermediate nodes 3.3 Building the Closed Enumeration Tree In a CET we store the following information for each node n I  i the itemset I itself ii the node type of n I  iii support  the number of transactions in which I occurs and  tid sum the sum of the tids of the transactions in which I occurs The purpose of having tid sum is because we use a hash table to maintain closed itemsets The Hash Table We frequently check whether or not a certain node is an unpromising gateway node which means we need to know whether there is a closed frequent node that has the same support as the current node We use a hash table to store all the closed frequent itemsets To check if n I is an unpromising gateway node by de\223nition we check if there is a closed frequent itemset J such that J 002 I  J 004 I  and support  J  support  I   We can thus use support as the key to the hash table However it may create frequent hash collisions We know if support  I  support  J  and I 007 J  then I and J must occur in the same set of transactions Thus a better choice is the set of tid s However the set of tid s take too much space so we instead use  support tid sum  as the key Note that tid sum of an itemset can be incrementally updated To check if n I is an unpromising gateway node we hash on the  support tid sum  of n I  fetch the list of closed frequent itemsets in the corresponding entry of the hash table and check if there is a J in the list such that J 002 I  J 004 I  and support  J  support  I   Tree Construction To build a CET 223rst we create a root node n 003  Second we create  001  child nodes for n 003 i.e each i 005 001 corresponds to a child node n  i   and then we call Explore on each child node n  i   Pseudo code for the Explore algorithm is given in Figure 3 Explore  n I  D  minsup  1 if support  n I   minsup 267|D then 2 mark n I an infrequent gateway node 3 else if lef tcheck  n I  true then 4 mark n I an unpromising gateway node 5 else 6 reach frequent right sibling n K of n I do 7 create a new child n I 002 K for n I  8 compute support and tid sum for n I 002 K  9 reach child n I 001 of n I do 10 Explore n I 001  D  minsup  11 if b a child n I 001 of n I such that support  n I 001  support  n I  then 12 mark n I an intermediate node 13 else 14 mark n I a closed node 15 insert n I into the hash table Figure  The Explore Algorithm Explore is a depth-\223rst procedure that visits itemsets in lexicographical order In lines 1-2 of Figure 3 if a node is found to be infrequent then it is marked as an infrequent gateway node and we do not explore it further Property 1 However the support and tid sum of an infrequent gateway node have to be stored because they will provide important information during a CET update when an infrequent itemset can potentially become frequent In lines 3-4 when an itemset I is found to be non-closed because of another lexicographically smaller itemset then n I is an unpromising gateway node Based on Property 2 we do not explore n I 220s descendants which does not contain any closed frequent itemsets However n I 220s support and 4 Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


tid sum must be stored because during a CET update n I may become promising In Explore  leftcheck  n I  checks if n I is an unpromising gateway node It looks up the hash table to see if there exists a previously discovered closed itemset that has the same support as n I and which also subsumes I  and if so it returns true in this case n I is an unpromising gateway node otherwise it returns false in this case n I is a promising node If a node n I is found to be neither infrequent nor unpromising then we explore its descendants lines 6-10 After that we can determine if n I is an intermediate node or a closed node lines 11-15 according to Property 3 Complexity The time complexity of the Explore algorithm depends on the size of the sliding-window N  the minimum support and the number of nodes in the CET However because Explore only visits those nodes that are necessary for discovering closed frequent itemsets so Explore should have the same asymptotic time complexity as any closed frequent itemset mining algorithm that is based on traversing the enumeration tree 3.4 Updating the CET New transactions are inserted into the window as old transactions are deleted from the window We discuss the maintenance of the CET for the wo operations addition and deletion Adding a Transaction In Figure 4 a ew transaction T  tid 5 is added to the sliding-window We traverse the parts of the CET that are related to transaction T  For each related node n I  we update its support  tid sum  and possibly its node type 2 1 2 3 4 5 C,D A,B A,B,C A,B,C A,C,D items        CDADACAB ABCD ABC 4342 2133 tid Figure  Adding a Transaction Most likely n I 220s node type will not change in which case we simply update n I 220s support and tid sum  and the cost is minimum In the following we discuss cases where the new transaction T causes n I to change its node type n I was an infrequent gateway node If n I becomes frequent e.g from node D in Figure 2 to node D in Figure 4 two types of updates must be made First for each of n I 220s left siblings it must be checked if new children should be created Second the originally pruned branch under n I  must be re-explored by calling Explore  For example in Figure 4 after D changes from an infrequent gateway node to a frequent node node A and C must be updated by adding new children  AD and CD  respectively Some of these new children will become new infrequent gateway nodes e.g node AD  and others may become other types of nodes e.g node CD becomes a closed node In addition this update may propagate down more than one level n I was an unpromising gateway node Node n I may become promising e.g from node AC in Figure 2 to node AC in Figure 4 for the following reason Originally b  j 002 i max and j 005 I  s.t j occurs in each transaction that I occurs However if the new transaction T contains I but not any f such j 220s then the above condition does not hold anymore If this happens the originally pruned branch under n I  must be explored by calling Explore  n I was a closed node Based on the following property n I will remain a closed node Property 4 Adding a new transaction will not change a node from closed to non-closed and therefore it will not decrease the number of closed itemsets in the sliding-window Proof Originally t J 004 I support  J   support  I  after adding the new transaction T  t J 004 I if J 007 T then I 007 T  Therefore if J 220s support is increased by one because of T sois I 220s support As a result t J 004 I support  J   support  I  still holds after adding the new transaction T  However if a closed node n I is visited during an addition its entry in the hash table will be updated Its support is increased by 1 and its tid sum is increased by adding the tid of the new transaction n I was an intermediate node An intermediate node such as node A in Figure 2 can possibly become a closed node after adding a new transaction T  Originally n I was an intermediate node because one of n I 220s children has the same support as n I does if T contains I but none of n I 220s children who have the same support as n I had before the addition then n I becomes a closed node because its new support is higher than the support of any f its children However n I cannot change to an infrequent gateway node or an unpromising gateway node First n I 220s support will not decrease because of adding T  so it cannot become infrequent Second if before adding T  lef tcheck  n I  false  then 006 b  j 002 i max and j 005 I  s.t j occurs in each transaction that I occurs this statement will not change after we add T  Therefore lef tcheck  n I  false after the addition Figure 5 ives a high-level description of the addition operation Adding a new transaction to the sliding-window will trigger a call of Addition on n 003  the root of the CET Deleting a Transaction In Figure 6 an old transaction T  tid 1 is deleted from the sliding-window To delete a transaction we also traverse the parts of the CET that is related to the deleted transaction Most likely n I 220s node type will not change in which case we simply update n I 220s support and tid sum  and the cost is minimum In the following we discuss the impact of deletion in detail If n I was an infrequent gateway node obviously deletion does not change n I 220s node type If n I was an unpromising y node deletion may change n I to infrequent but will not change n I to promising for the following reason For an unpromising gateway node n I  if before deletion 5 Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


Addition  n I  I new  D  minsup  1 if n I is not relevant to the addition then return  2 reach child node n I 001 of n I do 3 update support and tid sum of n I 001  4 F\n n I 001  n I 001 is newly frequent   5 reach child node n I 001 of n I do 6 if n I 001 is infrequent then 7 re n I 001 an infrequent gateway node 8 else if lef tcheck  n I 001  true then 9 re n I 001 an unpromising gateway node 10 else if n I 001 is a newly frequent node or n I 001 is a newly promising node then 11 Explore n I 001  D  minsup  12 else 13 reach n K 005F s.t I 001 002 K do 14 add n I 001 002 K as a new child of n I 001  15 Addition n I 001  I new  D  minsup  16 if n I 001 was a closed node then 17 update n I 001 220s entry in the hash table 18 else if 006 b a child node n I 001\001 of n I 001 s.t support  n I 001\001  support  n I 001  then 19 mark n I 001 a closed node 20 insert n I 001 into the hash table 21 return  Figure  The Addition Algorithm 2 2 3 4 5 A,B,C A,B A,B,C A,C,D items  window #2    ACAB ABCD ABC 4331 33 tid Figure  Deleting a Transaction lef tcheck  n I  true  then b  j 002 i max and j 005 I  s.t j occurs in each transaction that I occurs this statement remains true when we delete a transaction If n I was a frequent node it may become infrequent because of a decrement of its support in which case all n I 220s descendants are pruned and n I becomes an infrequent gateway node In addition all of n I 220s left siblings are updated by removing children obtained from joining with n I For example in Figure 6 when transaction T  tid 1 is removed from the window node D becomes infrequent We prune all descendants of node D aswellas AD and CD  which were obtained by joining A and C with D  respectively If n I was a promising node it may become unpromising because of the deletion for the following reason If before the deletion b  j 002 i max and j 005 I  s.t j occurs in each transaction that I occurs except only for the transaction to be deleted then after deleting the transaction I becomes unpromising This happens to node C in Figure 6 Therefore if originally n I was neither infrequent nor unpromising then we have to do the leftcheck on n I  For a node n I to change to unpromising because of a deletion n I must be contained in the deleted transaction Therefore n I will be visited by the traversal and we will not miss it If n I was a closed node it may become non-closed To demonstrate this we delete another transaction T  tid 2 from the sliding-window Figure 7 shows this example where previously closed node n I e.g A and AB  become non-closed because of the deletion This can be determined by looking at the supports of the children of n I after visiting them If a previously closed node that is included in the deleted transaction remains closed after the deletion we still need to update its entry in the hash table its support is decreased by 1 and its tid sum is decreased by subtracting the tid of the deleted transaction 2 3 4 5 A,B,C A,B,C A,C,D items    ACAB ABCD ABC 3231 23 tid Figure  Another Deletion From the above discussion we derive the following property for the deletion operation on a CET Property 5 Deleting an old transaction will not change a node in the CET from non-closed to closed and therefore it will not increase the number of closed itemsets in the sliding-window Proof If an itemset I was originally non-closed then before the deletion b j 005 I s.t j occurs in each transaction that I occurs Obviously this fact will not be changed due to deleting a transaction So I will still be non-closed after the deletion Figure 8 ives a high-level description of the deletion operation Some details are skipped in the description For example when pruning a branch from the CET all the closed frequent itemsets in the branch should be removed from the hash table Discussion In the addition algorithm Explore is the most time consuming operation because it scans the transactions in the sliding-window However as will be demonstrated in the experiments the number of such invocations is very small as most insertions will not change node types In addition the new branches grown by calling Explore are usually very small subsets of the whole CET therefore such incremental growing takes much less time than regenerating the whole CET n the other hand deletion only involves related nodes in the CET and does not scan transactions in the sliding-window Therefore its time complexity is at most linear to the number of nodes Usually it is faster to perform a deletion than an addition It is easy to show that if a node n I changes node type frequent/infrequent and promising/unpromising then I is in the added or deleted transaction and therefore n I is guaranteed to be visited during the update Consequently our algorithm will correctly maintain the current close frequent itemsets after any of the two operations Furthermore if n I remains closed after an addition or a deletion and I is contained in the added/deleted transaction then its position in the hash table is changed because its support and tid sum are changed To make the update we delete the itemset 6 Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


Deletion  n I  I old  minsup  1 if n I is not relevant to the deletion then return  2 reach child node n I 001 of n I do 3 update support and tid sum of n I 001  4 F\n n I 001  n I 001 is newly infrequent   5 reach child node n I 001 of n I do 6 if n I 001 was infrequent or unpromising then 7 continue  8 else if n I 001 is newly infrequent then 9 prune n I 001 220s descendants from CET 10 mark n I 001 an infrequent gateway node 11 else if lef tcheck  n I 001  true then 12 prune n I 001 220s descendants from CET 13 mark n I 001 an unpromising gateway node 14 else 15 reach n K 005F s.t I 001 002 K do 16 prune n I 001 002 K from the children of n I 001  17 Deletion n I 001  I old  minsup  18 if n I 001 was closed and b a child n I 001\001 of n I 001 s.t support  n I 001\001  support  n I 001  then 19 mark n I 001 an intermediate node 20 remove n I 001 from the hash table 21 else if n I 001 was a closed node then 22 update n I 001 220s entry in the hash table 23 return  Figure  The Deletion Algorithm from the hash table and re-insert it back to the hash table based on the new key value However such an update has amortized constant time complexity In our discussion so far we used sliding-windows of 223xed size However the two operations\205 addition and deletion 205are independent of each other Therefore if needed the size for the sliding-window can grow or shrink without affecting the correctness of our algorithm In addition our algorithm does not restrict a deletion to happen at the end of the window at a given time any transaction in the slidingwindow can be removed For example if when removing a transaction the transaction to be removed is picked following a random scheme e.g the newer transactions have lower probability of being removed than the older ones then our algorithm can implement a sliding-window with soft boundary i.e the more recent the transaction the higher chance it will remain in the sliding-window 4 Experimental Results We performed extensive experiments to evaluate the performance of Moment and we present some of them in this section For more results we refer readers to the full version of this paper W e use Charm a s tate-of-the-art algorithm proposed by Zaki et al as the baseline algorithm to generate closed frequent itemsets without using incremental updates All experiments were done on a 2GHz Intel Pentium IV PC with 2GB main memory running RedHat Linux 7.3 operating system All algorithms are implemented in C and compiled using the g 2.96 compiler T20I4D100K The 223rst dataset is generated using the synthetic data generator described by Agrawal et al n  Data from this generator mimics transactions from retail stores We have adopted the commonly used parameters the number of transactions D is 100,100 the average size of transactions T is 20 the average size of the maximal potentially frequent itemsets I is 4 We call this dataset T 20 I 4 D 100 K  We report the average performance over 100 consecutive sliding windows each with size N  100  000     0 0.2 0.4 0.6 0.8 1   10 2122                   10 2121                   10 0                   10 1                   10 2 Th e Minim u m Suppo r t  Running Time \(sec Charm Moment a   0 0.2 0.4 0.6 0.8 1 1 2 3 4 5 6 7 8 Th e Minim u m Suppo r t  Memory Usage Per Pattern\(KB b Figure  Running Time and Memory Usage for T 20 I 4 D 100 K Figure 9 gives the result on T 20 I 4 D 100 K  Figure 9\(a shows the average running time for Moment and for Charm over the 100 sliding windows under different minimum supports As can be seen from the 223gure as minimum support decreases because the number of closed frequent itemsets increases the running time for both algorithms grows However the response time of Moment is faster than that of Charm by more than an order of magnitude under all the minimum supports minsup closed CET CET node  changed new in   itemset  node  per closed node  node  1.0 4097 148450 36.2 0.14 6.28 0.9 5341 168834 31.6 0.06 0.92 0.8 6581 187076 28.4 0.13 0.64 0.7 8220 212774 25.9 0.09 0.35 0.6 10270 249549 24.3 0.08 1.30 0.5 12655 309575 24.5 0.10 2.58 0.4 16683 433595 26.0 0.18 4.20 0.3 24907 722645 29.0 0.26 9.52 0.2 45353 1614726 35.6 0.67 27.23 0.1 172396 5955425 34.5 3.41 88.69 0.05 722261 19691999 27.3 14.75 286.34 0.03 1704558 45246906 26.5 41.07 646.84 Table 1 Data Characteristics for T 20 I 4 D 100 K Table 1 shows the characteristics of the data and the mining results All reported data are average values taken over the 100 sliding windows The 223rst three columns show the minimum support the number of closed itemsets and the number of nodes in the CET From the table we can see that as the minimum support decreases the number of closed itemsets grows rapidly o does the number of nodes in the CET However the ratio between the number of nodes in the CET and the number of closed itemsets which is reported in column 4 remains approximately the same This implies that the sizes of the CET is linear in the number of closed frequent itemsets Because an addition may trigger a call for Explore which is expensive e study how many nodes change their status from infrequent/unpromising to frequent/promising column 5 and how many new nodes are created due to the addition column 6 From the data we can see that during an addition the average number of nodes that change from infrequent to frequent or from unpromising to promising in the CET is very small relative to the total number of nodes in the CET Similarly the number of new nodes created due to an addition is also very small These results 7 Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


verify the postulation behind our algorithm that an update usually only affects the status of a very small portion of the CET and the new branches grown because of an update is usually a very small subset of the CET We also studied the memory usage for Moment As shown in Figure 9\(b the average memory usage per closed frequent itemset actually decreases as the minimum support decreases This suggests that as the CET becomes larger it becomes more memory-ef\223cient in terms of memory usage per closed frequent itemset BMS-WebView-1 The second dataset we used is BMSWebView-1 which is a real dataset that contains a few months of clickstream data from an e-commerce web sites This dataset was used in KDDCUP 2000 There are 59,602 transactions in the dataset We set the sliding window size N to be 50,000 and do experiment on 100 consecutive sliding windows Other parameters are the number of distinct items is 497 the maximal transaction size is 267 and the average transaction size is 2.5    0 0.05 0.1 0.15 0.2 10 2124 10 2122 10 0 10 2 Th e Minim u m Suppo r t  Running Time \(sec Charm Moment a   0 0.05 0.1 0.15 0.2 10 2 10 4 10 6 10 8 Th e Minim u m Suppo r t  Total Number of Nodes CET nodes Closed Itemset b Figure 10 Performance for BMS-WebView-1 Figure 10\(a shows the running time for Moment and Charm From the 223gure we can see that because the average transaction size of this data set 2.5 is smaller than that of the previous synthetic dataset 20 the relative performance of Moment is even better\205it outperforms Charm by 1 to 2 orders of magnitudes Figure 10\(b shows the total number of nodes in the CET and the total number of closed itemsets As can be seen although both grow exponentially as the minimum support decreases the relative ratio between the two remains approximately the same which suggests that for real data the CET size is also linear in the number of closed frequent itemsets 5 Conclusion In this paper we propose a novel algorithm Moment to discover and maintain all closed frequent itemsets in a sliding window that contains the most recent samples in a data stream In the Moment algorithm an ef\223cient in-memory data structure the closed enumeration tree CET is used to record all closed frequent itemsets in the current sliding window In addition CET also monitors the itemsets that form the boundary between closed frequent itemsets and the rest of the itemsets We have also developed ef\223cient algorithms to incrementally update the CET when newly-arrived transactions change the content of the sliding window Experimental studies show that the Moment algorithm outperforms a state-of-the-art algorithm that mines closed frequent itemsets without using incremental updates In addition the memory usage of the Moment algorithm is shown to be linear in the number of closed frequent itemsets in the sliding window Acknowledgement We thank Professor Mohammed J Zaki at the Rensselaer Polytechnic Institute for providing us the Charm source code References 1 R C A g a r w a l C C A g g a r w a l  a n dV V V P r a s a d  At r e e projection algorithm for generation of frequent item sets Journal of rallel and Distributed Computing  61\(3 371 2001  R Agra w a l and R Srikant F ast algorithms for mining association rules In Proc of the 20th Intl Conf n Very Large Databases VLDB\32594  1994  T  Asai H Arimura K Abe S Ka w a soe and S Arika w a  Online algorithms for mining semi-structured data stream In Proc 2002 Int Conf n Data Mining ICDM\32502  2002  R J Bayardo Jr  E f 223ciently mining long patterns from databases In Proceedings of the M SIGMOD  1998  J H Chang and W  S Lee Finding recent frequent itemsets adaptively over online data streams In Proc of the 2003 Int Conf Knowledge Discovery and Data Mining SIGKDD\32503  2003  M Charikar  K  Chen and M  F arach-Colton Finding frequent items in data streams In Proc of the 29th Int\325l Colloquium on Automata s and ramming  2002  D W  Cheung J Han V  Ng and C Y  W ong Maintenance of discovered association rules in large databases An incremental updating technique In Proceedings of the Twelfth International Conference on Data Engineering  1996  D W  Cheung S D Lee and B Kao A general incremental technique for maintaining discovered association rules In Proceedings of the Fifth International Conference on Database Systems for Advanced Applications DASFAA  1997  Y  Chi H W ang P  S Y u  a nd R R Muntz Catch the moment Maintaining closed frequent itemsets over a data stream sliding window Technical Report IBM 2004  C Giannella J Han E Robertson and C Liu Mining frequent itemsets over arbitrary time intervals in data streams Technical Report tr587 Indiana University 2003  K Gouda and M J Zaki Ef 223ciently mining maximal frequent itemsets In Proceedings of the 2001 IEEE Int\325l Conf on Data Mining  2001  C Hidber  Online association rule mining In Proc of the M SIGMOD int\325l conf n Management of data  1999  C Lee C Lin and M Chen Sliding-windo w 223ltering an ef\223cient algorithm for incremental mining In Proc of the int\325l conf n Info and knowledge management  2001  G Manku and R Motw ani Approximate frequenc y c ounts over data streams In Proceedings of the 28th International Conference on Very Large Data Bases  2002  W G T eng M.-S Chen and P  S Y u  A re gression-based temporal pattern mining scheme for data streams In Proceedings of 29th International Conference on Very Large Data Bases VLDB\32503  2003  J W a ng J Han and J  Pei Closet searching for the best strategies for mining frequent closed itemsets In Proc of the 2003 Int Conf Knowledge Discovery and Data Mining SIGKDD\32503  2003  M J Zaki and C Hsiao Charm An ef 223cient algorithm for closed itemset mining In 2nd SIAM Int\325l Conf n Data Mining  2002  Z Zheng R K oha vi and L  Mason Real w orld per formance of association rule algorithms In Proc of the 2001 Int Conf Knowledge Discovery and Data Mining SIGKDD\32501  2001 8 Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


 The required delivery date is a Range constraint any date within the next 30 days Attribute Required-Delivery-Date  today today+30 days Attribute S&H query\(UPS Product   say it is 59.95 Attribute Value  query\(Catalog Product  Attribute Price  Attribute Total  Inter-attribute constraints in PO14 Price  Quantity  Value  1-x Total  Price  S&H  1.088 Total  Total1  Total4 In this case the Quantity attribute value has changed to 2 by adding both requests together Furthermore the Delivery-Date attribute value is a result of finding a common range of the two The obvious saving in this case is 2*$39.95 59.95\1.088  21.71 Whether a bunch of POs should be aggregated in a particular way depend on whether costing savings can be achieved while satisfying all the constraints 6.2 Intelligent Aggregation of Purchase Orders in e-Procurement with Negotiations Aggregation under dynamic negotiation is harder because supplier side could be revising its own strategies and parameters on the fly While human intervention in the aggregation process is possible we focus on automated aspects of the aggregation in this paper Suppose we have a simple supplier side rule buy one and get second one half price from LT a supplier of mice keyboard and trackball Suppose we have requests to buy Mice as follows PO5 Attribute Buyer  Organization 223B\224 User 223Joe\224 Location 223PS\224 Attribute Supplier  mpany 223LT\224 Catalog  http://\205/LT Attribute Product  223Optical Mouse\224 Attribute Quantity 1 Attribute Required-Delivery-Date  01/21/05 02/21/05   a r an g e of dat e s  order dat e  deadline d Attribute S&H9  query\(UPS Product   say it\222s 4.95 Attribute Value  query\(Catalog Product  Attribute Price10  Value  say it\222s 29.95 Attribute Total10  Total10  Price10  S&H10  1 tax rate results in a value 29.95  4.95\1.088  39.97 PO6 Attribute Buyer  Organization 223C\224 User 223Al\224 Location 223PS\224 Attribute Supplier  mpany 223LT\224 Catalog  http://\205/LT Attribute Product  223Optical Mouse\224 Attribute Quantity 1 Attribute Required-Delivery-Date  01/25/05 02/28/05   a r an g e of dat e s  order dat e  deadline d Attribute S&H10  query\(UPS Product   say it\222s 4.95 Attribute Value  query\(Catalog Product sayit\222s 29.95 per mouse Attribute Price10 Value Attribute Total10  Total10  Price10  S&H10  1 tax rate results in a value 29.95  4.95\1.088  39.97 6.2.1 PO Aggregation Under Negotiation The rule-based aggregation engine uses the Negotiation service to understand supplier\222s offers and tries to take advantage of the terms in the offers For example by aggregating PO5 and PO6 can be aggregated as follows PO56 Attribute Buyer  Organization B C User 223Joe\224 223Al\224 Location 223PS\224 Attribute Supplier  mpany 223LT\224 Catalog  http://\205/LT Attribute Product  223Optical Mouse\224 Attribute Quantity 2 Attribute Required-Delivery-Date  01/25/05 02/21/01 Attribute S&H10  query\(UPS Product   say it\222s 4.95 Attribute Value  query\(Catalog Product sayit\222s 29.95 per mouse Attribute Price10 1.5*Value Attribute Total10  Total10  Price10  S&H10  tax  1.5 29.95  4.95\1.088  54.26 A saving of 39.97  2 54.26  25.68 or over 32 of savings Note the changes of the 223Quantity\224 and 223RequiredDelivery-Date\224 attributes after aggregation The quantities are added up and the required delivered dates are merged for a common range Due to the constraints on object attributes aggregation may require complex constraint solving Proceedings of the 2005 Ninth IEEE International ED OC Enterprise Computing Conference \(EDOC\22205 0-7695-2441-9/05 $20.00 \251 2005  IEEE 


7 Conclusions and Future Work This paper describes an Intelligent Aggregation facility in enterprise e-Procurement process This facility introduces an information model a rule-based aggregation engine corporate agreement policies and negotiation in aggregating large volume of POs in enterprise eprocurement to reduce cost and maximize efficiency This information model includes extensive use of constraints for and among attributes in a PO These constraints guard the integrity of POs as they are aggregated The intelligent aggregation facility can be inserted as a value-added service in the enterprise e-Procurement workflow An enterprise generates millions of POs every year but the number of distinct products and services the enterprise purchases is actually much smaller in the hundreds rather than in the millions This presents cost saving opportunities by aggregating POs that makes best use of terms and conditions in corporate agreements or supplier offers Some concrete examples are used to show the idea of automated aggregation and the opportunities in reducing procurement cost As millions of POs are generated even a small percentage of savings would mean substantial savings for large enterprises The ideas described in this paper have not been fully implemented in our prototype One area needs more work is the formal representation of policies in corporate agreements which would allow the aggregation engine to automatically explore aggregation opportunities before POs are made to suppliers Another is the semantic model of products which would enable more semantics-based aggregation of POs 8 References  e bX M L  h t t p   w w w ebxml  org  2 e n g  J  S u  S  Y  W  L a m H   a n dH e l a l S   223Achieving Dynamic Inter-Organizational Workflow Management by Integrating Business Processes Events and Rules,\224 Proceedings of the 35th Hawaii International Conference on System Sciences HICSS35 Hawaii USA January 2002 3 u  S  Y  W  L a m H   L e e  M  B a i S   a n dS h e n  Z   An Information Infrastructure and E-services for Supporting Internet-based Scalable E-business Enterprises Proceedings of the 5th International Enterprise Distributed Object Computing Conference Seattle Washington USA September 2001 4 S u S Y  W   H ua ng C  H a mme r J   H u a ng Y   L i  H   Wang,L.,LiuY.,Pluempitiwiriyawej,C.,Lee,M and Lam H 223An Internet-based Negotiation Server for E-commerce,\224 VLDB Journal Vol 10 No 1 2001 pp.72-90 5 M o r r i s S l o m a n  223 P o l i c y D ri v e n M an ag em e n t f o r Distributed Systems\224 Journal of Network and Systems Management Plenum Press Vol 2 No 4 1994  M aarten S teen  J oh n D errick  223 For m ali s ing ODP Enterprise Policies\224 Proceedings of the 3 rd International nterprise Distributed Object Computing Conference Mannheim Germany IEEE CS Press September 1999  J am e s H a ns on  Z oran M i l o s e v i c 223 C o n v e r s at i onOriented Protocols for Contract negotiations\224 Proceedings of the 7th International Enterprise Distributed Object Computing Conference Brisbane Australia IEEE CS Press September 2003  S  N eal J  C ole P.F L i n i ng ton  Z  Milose v i c S Gibson S Kulkarni 223Identifying Requirements for Business Contract Language a Monitoring Perspective\224 Proceedings of the 7th International Enterprise Distributed Object Computing Conference Brisbane Australia IEEE CS Press September 2003 9 T  D im itrak o s  I  D j o rd j e v i c Z  Milo sev i c A  J o san g  C Phillips 223Contract Performance Assessment for Secure and Dynamic Virtual Collaborations\224 Proceedings of the 7th International Enterprise Distributed Object Computing Conference Brisbane Australia IEEE CS Press September 2003 Proceedings of the 2005 Ninth IEEE International ED OC Enterprise Computing Conference \(EDOC\22205 0-7695-2441-9/05 $20.00 \251 2005  IEEE 


absolute values. The results can vary on other computers. But it can be guaranteed that performance ratio of the algorithms will remain the same After making the comparisons with sample data, we came to the conclusion that PD algorithm performs significantly better than the other two especially with larger datasets. PD outperforms DCP and PIP regarding running time. On the other hand, since PD reduces the dataset, mining time does not necessary increase as the number of transactions increases and experiments reveals that PD has better scalability than DCP and PIP. So, PD has the ability to handle the large data mine in practical field like market basket analysis and medical report documents mining 5. References 1] R. Agrawal and R. Srikant, "Fast algoritlnns for mining association rules", VLDB'94, pp. 487-499 2] R. J. Bayardo, "Efficiently mining long patterns from databases", SIGMOD'98, pp.85-93 3] J. Pei, J. Han, and R. Mao, "CLOSET: An Efficient Algorithm for Mining Frequent Closed Itemsets \(PDF Proc. 2000 ACM-SIGMOD International Workshop on Data Mining and Knowledge Discovery, Dallas, TX, May 2000 4] Qinghua Zou, Henry Chiu, Wesley Chu, David Johnson, "Using Pattern Decomposition\( PD Finding All Frequent Patterns in Large Datasets", Computer Science Department University of California - Los Angeles 5] J. Han, J. Pei, and Y. Yin, "Mining Frequent Patterns without Candidate Generation \(PDF  SIGMOD International Con! on Management of Data SIGMOD'OOj, Dallas, TX, May 2000 6] S. Orlando, P. Palmerini, and R. Perego, "The DCP algoritlnn for Frequent Set Counting", Technical Report CS2001-7, Dip. di Informatica, Universita di Venezia 2001.Available at http://www.dsi.unive.itl?orlando/TR017.pdf 7] MD. Mamun-Or-Rashid, MD.Rezaul Karim, "Predictive item pruning FP-tree algoritlnn", The Dhaka University  Journal of Science, VOL. 52, NO. 1, October,2003, pp. 3946 8] Park, J. S., Chen, M.-S., and Yu, P. S, "An Effective Hash Based Algoritlnn for Mining Association Rules", Proc ofthe 1995 ACM-SIGMOD Con! on Management of Data 175-186 9] Brin, S., Motwani, R., Ullman, J., and Tsur, S, "Dynamic Itemset Counting and Implication Rules for Market Basket Data", In Proc. of the 1997 ACM-SIGMOD Conf On Management of Data, 255-264 10] Zaki, M. J., Parthasarathy, S., Ogihara, M., and Li, W New Algoritlnns for Fast Discovery of Association Rules In Proc. of the Third Int'l Con! on Knowledge Discovery in Databases and Data Mining, 283-286 11] Lin, D.-I and Kedem, Z. M., "Pincer-Search: A New Algoritlnn for Discovering the Maximum Frequent Set", In Proc. of the Sixth European Conf on Extending DatabaseTechnology, 1998 12] R. Ramakrishnan, Database Management Systems University of Wisconsin, Madison, WI, USA; International Edition 1998 pre></body></html 


tors such as union, di?erence and intersection are de?ned for pairs of classes of the same pattern type Renaming. Similarly to the relational context, we consider a renaming operator ? that takes a class and a renaming function and changes the names of the pattern attributes according to the speci?ed function Projection. The projection operator allows one to reduce the structure and the measures of the input patterns by projecting out some components. The new expression is obtained by projecting the formula de?ning the expression over the remaining attributes [12 Note that no projection is de?ned over the data source since in this case the structure and the measures would have to be recomputed Let c be a class of pattern type pt. Let ls be a non empty list of attributes appearing in pt.Structure and lm a list of attributes appearing in pt.Measure. Then the projection operator is de?ned as follows ls,lm c id s m f p ? c, p = \(pid, s, d,m, f In the previous de?nition, id ing new pids for patterns, ?mlm\(m projection of the measure component and ?sls\(s ned as follows: \(i s usual relational projection; \(ii sls\(s and removing the rest from set elements. The last component ?ls?lm\(f computed in certain cases, when the theory over which the formula is constructed admits projection. This happens for example for the polynomial constraint theory 12 Selection. The selection operator allows one to select the patterns belonging to one class that satisfy a certain predicate, involving any possible pattern component, chosen among the ones presented in Section 5.1.1 Let c be a class of pattern type pt. Let pr be a predicate. Then, the selection operator is de?ned as follows pr\(c p Join. The join operation provides a way to combine patterns belonging to two di?erent classes according to a join predicate and a composition function speci?ed by the user Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE Let c1 and c2 be two classes over two pattern types pt1 and pt2. A join predicate F is any predicate de?ned over a component of patterns in c1 and a component of patterns in c2. A composition function c pattern types pt1 and pt2 is a 4-tuple of functions c cStructureSchema, cDataSchema, cMeasureSchema, cFormula one for each pattern component. For example, function cStructureSchema takes as input two structure values of the right type and returns a new structure value, for a possible new pattern type, generated by the join. Functions for the other pattern components are similarly de?ned. Given two patterns p1 = \(pid1, s1, d1,m1, f1 p2 = \(pid2, s2, d2,m2, f2 p1, p2 ned as the pattern p with the following components Structure : cStructureSchema\(s1, s2 Data : cDataSchema\(d1, d2 Measure : cMeasureSchema\(m1,m2 Formula : cformula\(f1, f2 The join of c1 and c2 with respect to the join predicate F and the composition function c, denoted by c 1   F  c  c 2   i s  n o w  d e  n e d  a s  f o l l o w s    F  c  c 2     c  p 1   p 2   p 1    c 1  p 2    c 2  F   p 1   p 2     t r u e   5.1.3. Cross-over database operators OCD Drill-Through. The drill-through operator allows one to 


Drill-Through. The drill-through operator allows one to navigate from the pattern layer to the raw data layer Thus it takes as input a pattern class and it returns a raw data set. More formally, let c be a class of pattern type pt and let d be an instance of the data schema ds of pt. Then, the drill-through operator is denoted by c c Data-covering. Given a pattern p and a dataset D sometimes it is important to determine whether the pattern represents it or not. In other words, we wish to determine the subset S of D represented by p \(p can also be selected by some query the formula as a query on the dataset. Let p be a pattern, possibly selected by using query language operators, and D a dataset with schema \(a1, ..., an ible with the source schema of p. The data-covering operator, denoted by ?d\(p,D responding to all tuples in D represented by p. More formally d\(p,D t.a1, ..., t.an In the previous expression, t.ai denotes a speci?c component of tuple t belonging to D and p.formula\(t.a1, ..., t.an instantiated by replacing each variable corresponding to a pattern data component with values of the considered tuple t Note that, since the drill-though operator uses the intermediate mapping and the data covering operator uses the formula, the covering ?\(p,D D = ?\(p not be equal to D. This is due to the approximating nature of the pattern formula 5.1.4. Cross-over pattern base operators OCP Pattern-covering. Sometimes it can be useful to have an operator that, given a class of patterns and a dataset, returns all patterns in the class representing that dataset \(a sort of inverse data-covering operation Let c be a pattern class and D a dataset with schema a1, ..., an pattern type. The pattern-covering operator, denoted as ?p\(c,D all patterns in c representing D. More formally p\(c,D t.a1, ..., t.an true Note that: ?p\(c,D p,D 6. Related Work Although signi?cant e?ort has been invested in extending database models to deal with patterns, no coherent approach has been proposed and convincingly implemented for a generic model There exist several standardization e?orts for modeling patterns, like the Predictive Model Markup Language \(PMML  eling approach, the ISO SQL/MM standard [2], which is SQL-based, and the Common Warehouse Model CWM  ing e?ort. Also, the Java Data Mining API \(JDMAPI 3] addresses the need for a language-based management of patterns. Although these approaches try to represent a wide range of data mining result, the theoretical background of these frameworks is not clear. Most importantly, though, they do not provide a generic model capable of handling arbitrary cases of pattern types; on the contrary only a given list of prede?ned pattern types is supported To our knowledge, research has not dealt with the issue of pattern management per se, but, at best, with peripheral proximate problems. For example, the paper by Ganti et. al. [9] deals with the measurement 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


