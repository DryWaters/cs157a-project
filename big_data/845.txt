Conducting Web-Based Surveys of Government Practitioners in Social Sciences: Practical Lessons for E-Government Researchers  J. Ramon Gil-Garcia*, Sara A. Berg**, Theresa A. Pardo**, G. Brian Burke**, and Ahmet Guler Centro de Investigación y Docencia Económicas, Mexico Center for Technology in Government, University at Albany, SUNY, USA joseramon.gil@cide.edu    Abstract  Although the use of surveys in social science research is not new, growing computerization and widespread availability of Internet access has made it increasingly possible to conduct these surveys online 
However, populations in the social sciences and particularly in e-government are not always well defined, and their boundaries are fuzzy.  Therefore additional challenges need to be considered in the research process, extending from the initial planning stages to the survey administration and beyond Based on the existing literature and our experience conducting a national Web-based survey on information sharing with public health and criminal justice practitioners, this paper highlights some of these challenges and presents a series of lessons useful for digital government research. The lessons 
highlight aspects related o the nature of Web-based surveys, as well as particularities of working with government practitioners  1. Introduction  Surveys are conducted by many organizations in both private and public sectors.  In general, most survey research is resource consuming, whether this is time, money, or personnel T h is is es pecial l y  true if primary data collection is involved, where researchers are not limited by what others have done Thus while the project itself will take longer from start to finish than if secondary data analysis is done 
using an already existing survey data source\e quality of the data to the specific project will be improved Digital government research requires that researchers engage with government practitioners about the work they do.  Although this can be done through qualitative mechanisms, in order to be able to generalize digital government researchers need to reach beyond to a broader but representative population, which has great challenges \(distributed difficult to identify comparability, dynamic, etc Web-based surveys are a way to overcome some of 
the challenges of this environment, especially in terms of surveying government professionals about their work This paper is based on our experience conducting a Web-based survey about government information sharing. Information is a valuable resource in government, with information sharing being key if government is to work effectively across organizational boundaries [22  I nfo r m a t i o n s h a r i n g  could be defined as building systems, instituting formal standards, and changing business processes to 
allow organizations to share data and information with many other organizations 12, p.121; 3, 8, 14  Agencies at the federal, state, and local levels must bring information together and integrate it so that it can be used to solve important public problems, but the information needed to plan, make decisions, and act is often held by multiple organizations maintained in disparate formats, and used for widely different purposes eref ore beco m e s  v e r y  important for digital government researchers to examine the ways in which these complex 
phenomena occur.  One way this can be done is through a survey. Based on the existing literature and our experience conducting a national Web-based survey of information sharing in public health and criminal justice, this paper highlights some advantages and challenges to this type of research and offers a series of lessons useful for digital government researchers The paper is organized in five sections, including the foregoing introduction.  Section Two showcases some advantages and disadvantages of Web-based 
surveys as derived from the existing literature Section Three provides an overview of the digital government project we are using to exemplify the use of Web-based surveys.  Section Four describes our own experience conducting a Web-based survey with government employees. Finally, Section Five presents our conclusions and proposes some Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 1 978-0-7695-3450-3/09 $25.00 © 2009 IEEE 


lessons based on the existing literature and our experience doing a Web-based survey  2. Web-based Survey Research  The use of surveys for social science research is not new.  Most commonly, surveys are administered in print via the postal mail, face-to-face \(in person or by telephone.  However, growing computerization and widespread availability of Internet access has made it increasingly possible to conduct these surveys online.  Researchers can send out the survey via e-mail and have respondents complete and return it in the same way, or the survey can be administered via the World Wide Web, essentially creating an online version of a print survey  2.1. Advantages of Web-based surveys  There are a number of benefits that Web-based surveys provide \(see, for exampl  17 21    27 2 8 T h ey are dis c us s e d belo w     Lowering the cost of administering It would be extremely costly to administer a geographicallydisparate survey in person, where potential respondents are located in multiple states or countries.  Online administration is cheaper than either in-person administration or doing a mailbased survey; both options require printing out the survey instrument itself, with the latter also including the cost of mailing it to potential respondents and having them mail it back   Automated data entry Data files, usually in Excel/CSV or sometimes in SPSS format, can be downloaded as needed directly from the survey vendor.  Although some manipulation of these files is often necessary prior to beginning of analysis, the data entry itself is not required. For a survey with hundreds or even thousands of variables and respondents, it would be extremely time-consuming to enter results by hand into Excel or other statistical software package Moreover, the automated data entry capability of Web-based surveys reduces possible coding errors during manual entry of the traditional paper-based survey   Less latency Respondents can receive access to a Web-based survey faster via e-mail than postal mail and do not need to mail their completed surveys back, so data is more immediately available after survey completion.  Similarly, it is also faster to send reminders to participants about completing the survey via e-mail than via postal mail.  Problems that arise during the course of administration, such as responses mistakenly being submitted too soon or having missing data due to technical difficulties can be quickly assessed.  This allows researchers to look at data even before the survey is closed to new responses and to contact respondents if there is a problem with their survey   Better controlling the survey population  Although some Web-based surveys are open to any who choose to participate, researchers can also control their sample list by restricting completion to certain individuals who have been invited.  Each potential respondent receives a unique survey link that will be used to track their access and progress, making it possible to view their status \(e.g., link clicked, partially completed submitted etc.\n real time.  This allows for the keeping of detailed records of every person in the sample during the entire administrative process instead of needing to wait until the end Furthermore, Web-based surveys provide opportunity to access unique populations such as online groups in cyberspace o h e lps to  reach groups that normally are difficult to identify and survey   Better instrument design Finally, there is more control over the format than with a paper-based survey.  The online instrument can be designed to appear differently to different participants with regards to color, text size, question order, etc Similarly, participants could be shown different parts of the survey \(e.g., different follow-up questions\ed on their responses to previous questions, in contrast to a print-based survey which presents all possible questions up front  2.2. Disadvantages of Web-based surveys  There are also a number of challenges and limitations of online surveys \(see, for exam     F o l l o w i ng  w e di s c us s s o m e of them   Errors There are several potential errors that cause biased results in Web-based surveys Umbach m e n tions f o u r poten tial errors th at  affect the results of Web-based surveys negatively: coverage error, sample error measurement error, and nonresponse error. First coverage error occurs in Web-based surveys when there is a disparity between the general population and the sample population. This might happen due to not defining the sample population appropriately or not having accurate knowledge about respondents Internet use. Second, sampling error occurs in Web-based survey because it is not Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 2 


possible to have a perfect random sample due to the uncertainty of Internet access among the population. If the survey is open to a general population, the actual sample of respondents may not be representative depending on who chooses to complete it. This causes self-selection bias Third, measurement error happens because of people s mode differences between  a Web-based survey and a traditional paper-based survey.  In addition, formatting differences also cause measurement error. The survey could appear visually different depending on a participant s web browser, monitor size, and display settings creating an unwanted difference separate from intentional format choices Finally, nonresponse error occurs when the characteristics of respondents are different from the survey sample   Lower response rate Web-based surveys may have lower response rates than paper-based ones  P e ople w h o are n o t co m f ortable w i t h  computer technologies may not be willing to respond to a Web-based survey for fear of encountering technical problems. In addition while people can see the length of a survey in the traditional paper-based format, they generally do not have this ability in Web-based survey. As a result, respondents may become impatient while taking the survey and stop taking it or be hesitant to start it in the first place if they do not have a general idea of the time commitment involved based on the survey length   Potential lost of data and responses There is a higher risk to lost data in Web-based surveys than traditional paper-based surveys [1 r in g th e  data on computers or using the online survey tools incurs the risk of lost data due to intentional or inadvertent deletion or corruption and system failure. In addition, any technical problems that occur could result in lost or missing responses possibly without the researcher knowing about them   Information Security Also, although more a consideration than a true limitation, it is important to keep in mind information security requirements when it comes to the storage of confidential data Web-based surveys require planning for both the logical \(digital\security of electronic data files as well as the physical security of any printed material \(i.e., data results, respondent information\ and the media on which the electronic files rest \(e.g., PCs, servers, or USB drives   Confidentiality Similarly, the confidentiality of participants is also key, especially when those participants are government practitioners who could potentially be involved in sensitive projects This is a difference between online administration and other types of administration, where participants may have more assurances of anonymity. Complete anonymity is impossible when an invitation system is used to control who the respondents can be; although the researchers can tie specific responses to specific individuals care must be taken to ensure that outsiders are not able to do so  3. The MIII project: A Brief Overview  This section briefly describes a project titled Modeling the Social and Technical Processes of Interorganizational Information Integration MIII and highlights one of its components that consisted in administering an online survey.  Our survey, based on a digital government study conducted by The Center for Technology in Government and supported by a grant from the National Science Foundation 1 is the culmination of the 6-year-long MIII project that began using a case study methodology to create the ground work for understanding information sharing across government agencies and across government levels.  Its purpose was to develop and test dynamic models of information integration in multiorganizational government settings.  The study employs a multi-method research approach, a powerful way to examine complex social phenomenon, especially those which are not yet well underst s e qu e n ti al ex ploratory re s earch design was selected for the project, in which qualitative data collection and analysis would be succeeded by quantitative data collection and analysis, with an objective that the quantitative analysis would be used to test theory that emerged from the qualitative findings  Following a systematic and rigorous qualitative analysis process using grounded theory techniques 15, 16, 25 e res earch t e a m iden ti f i ed critical  factors and processes involved in sharing information across levels and agencies in government and across organizations from different sectors. The research team also identified how those factors and processes varied for different types and degrees of information sharing icall y  sy ste m d y n a m i c s  modeling \(emphasizing the temporal and feedback aspects of the process\ and social process modeling emphasizing the way collaboration and shared meanings are developed\ere used to produce   1 National Science Foundation grant number ITR0205152 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 3 


models representing the social and technical processes and factors at work.  During the third phase, which we discuss in-depth for this paper, an online \(Web-based\antitative survey was administered to more fully explore the relationships among the factors in these empirically grounded process models and test their generalizability  4. The MIII survey: Our experience and lessons learned  The goal of our quantitative survey was thus to test the a high-level information sharing model that was developed out of the case studies, in conjunction with aspects of information sharing from other similar models \(i.e., knowledge networking in the public s In th is section   w e dis c us s t h e  step-by-step process by which we undertook survey development and administration.  Although some of the information presented is general to large-scale survey projects of this type, we also include our specific experiences as they relate to each step During the entire development and administration process, a key element was the collaboration between academics and government practitioners.  Although we do not elaborate on this relationship until Section 4.7, its importance is highlighted throughout Section 4 as a whole.  Without the participation of these individuals, our survey would not have been as successful as it was; their time and input were invaluable to the entire project  4.1. Selection of Web-based survey environment  After the choice is made to conduct a Web-based survey, researchers must decide what type of environment to use for actual administration.  There are three primary environments that can be selected The survey could be created and hosted internally the researchers do everything\d hosted externally \(an outside vendor does everything created internally and hosted externally \(an outside vendor does the hosting and has created the software in which to design the survey, but the researchers will build it themselves\e opted to choose the latter type of environment Here it helps to have a research team member with IT experience who can talk to the survey technical support people if needed.  Certain issues or bugs may be more technically complex to handle making it useful to have someone who can speak that language.  Even if this team member is not a programmer, it is a plus if they at least know basic HTML and web design, since part of the survey design process is the visual look of the instrument in a Web browser Based on the Center s recent experience with using online survey tools for research, plus some additional investigation into other available survey tools, we found eight possible options for vendors All were compared on cost and various functionalities, including the availability of e-mail address list management \(invitations\nd an automated reminder system; in general, most of the features offered were the same between companies In the end we decided to use Survey Gizmo  www.surveygizmo.com aluating them through their free version.  The Enterprise account level enabled us to access both telephone and e-mail technical support, gave us up to 50,000 responses per month, and allowed us to create an unlimited number of surveys. Of note, other online surveys conducted by the Center have used another tool: Survey Monkey www.surveymonkey.com   4.2. Sampling  In creating our sample, we had two major challenges: delimiting a population of government professionals from which to sample and collecting current contact information for sampled individuals A goal of social science research is to have final results which are representative of a population, so random sampling measures are most-often employed to obtain a representative sample.  However, we were unable to use a random sampling method because, as many other digital government research projects, our complete population was unknown; while the sample included government practitioners in the criminal justice and public health policy domains, no complete list of all these individuals is in existence.  As such we employed a combination of purposeful sampling techniques We also had difficulties in finding valid e-mail addresses for some sampled individuals.  This was especially tough because the sample included government workers whose jobs may be dependant on the current political climate, resulting in turnover Because we would be contacting participants via email, it was extremely important that we had up-todate contact information for each of them. Given these two considerations, our sampling strategy primarily followed the five steps listed below 1  Create the initial sample contact list drawn from previous Center case studies and Google searches 2  Update the sample list to include individuals from all 50 states, at both state and local levels of Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 4 


government, for both criminal justice and public health agencies 3  Update the sample list to check for current e-mail addresses or that the correct individuals were listed for government positions 4  Take recommendations from sampled individuals to include others in our contact list 5  Work with relevant professional government associations as much as possible Overall, sampling was an extremely iterative process with constant updates to ensure that our contact information was up-to-date and correct.  This was necessary because of the transient nature of an already undefined population of government employees; we wanted to be as thorough as possible in making sure that our sample would be complete This thoroughness, although a time-consuming and expensive activity in terms of resources, was nonetheless necessary to ensure that sampling was done well and that the resulting contact list would be adequate  4.3. Instrument design  Because our survey was going to be conducted online, instrument development not only needed to take into account good practices concerning the creation of questions but also how the material would translate from a paper version to a Web-based one We also had the challenge of determining how to focus our respondents on thinking of a single crossboundary information sharing initiative when answering survey questions Item design The survey included two kinds of questions; those that captured demographic data about the individual and about the initiative on which the respondent focused their responses, and those that captured reactions to the theoretical model.  Most of the design time was spent on the model questions These questions were written as Likert items, where a participant is asked to evaluate his/her level of agreement or with a statement \(Bryman\.  This scale of agreement typically includes 5 levels \(though more can be used as well\ ranging from strongly disagree  to strongly agree including a neutral option in the middle.  In deciding how many points to include on the scale, our research team reviewed journal articles published in the areas of public administration and information science to see what other studies used \(see, for exampl In general, the larger the scale the more precise the responses.  While it is possible to collapse categories e.g., a response of 1 or 2 could become a single category\ during the post-administration data cleaning stages if no qualitative difference between answers is found, you cannot go backwards and add more categories out of fewer \(i.e., turn a 5-point scale used in the survey into a 7-point scale during the cleaning stage\o use a 7-point scale to maximize possible variation while still giving us the flexibility to collapse the size if needed Translation from paper to Web While this was a broad concern that informed our decision-making process along the way, it was not difficult to implement.  Our biggest changes here were to eliminate wordiness in response choices where possible, but otherwise the text stayed the same in translating the survey from paper to Web.  Generally the need to make the online survey visually accessible was handled by selecting colors, fonts, and font sizes that would be both pleasing to the eye and easily readable Establishing a specific initiative focus It was also a challenge in crafting the wording to ask about a cross-boundary information sharing initiative on which the respondent had worked the opening question to the survey.  Typical government practitioners are involved with many initiatives especially those who have been working in their field for a number of years, but we needed to get them focused on one project.  Similarly, these types of large-scale projects do not normally end and instead move into a new phase, so we needed to make respondents think about a single, current \(at the time of survey-taking\ phase.  In order to retain participant focus while they worked through the survey, we chose to use the Survey Gizmo functionality that allowed piping of responses; after choosing an initiative that they would use for their responses and entering its name into the survey, that initiative name appeared at the top of all subsequent survey pages  4.4. Invitation letter design  It was very important to the research team to create an interesting yet informative invitation letter a critical step in establishing a relationship with government practitioners and securing their buy in  We looked at how other survey invitation letters were crafted before drafting ours, paying close attention to their choice of wording.  The goal of this letter was to provide some brief background information about the research project in a way that would appeal to the interest of our sample and entice them to complete the survey.  As mentioned previously, some individuals were on our contact list because of a prior relationship with the Center and the MIII project \(i.e they were a part of the qualitative focus groups/interviews\, but the vast majority were unconnected.  Thus it became important to Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 5 


familiarize them with the Center and this particular research being done Invitation letters are a common component of any survey research project.  However, delivering them online via e-mail is a major difference between other methods and creates additional challenges.  An individual who receives an unwanted piece of printed mail can throw it out.  Similarly, unwanted e-mail  spam be deleted after its read, deleted prior to being read, or deleted ever before it reaches a user s inbox.  Thus it was essential that we take steps to ensure that members of our sample actually received our initial invitation e-mail and entice them to read it and choose to participate. We also sent all contact emails from an academic server \(.edu domain\ it would be less likely to be deleted by government spam filters than e-mails coming from a .com or .net address  4.5. Survey administration process  We were very cognizant of the importance of both thoroughly testing the instrument and the administration process prio r to releasing the full survey.  As such, the survey went through three complete stages from start to finish: pre-testing, pilot testing, and full administration.  These three stages took approximately seven months \(October 2007  April 2008\rom start to finish; this does not count the time spent on sampling, instrument design invitation letter design, and building the survey into the online tool  4.5.1. Pre-test The goal of our pre-testing process was to get early feedback about the first implementation of the web-based instrument  The survey was sent to 23 individuals to pre-test including 3 members of the research team, and we received comments back from 9 of them. We discussed the comments and made some revisions to the survey questionnaire  4.5.2. Pilot Pilot tests are primarily used to ensure that survey questions operate well but also that the instrument as a whole is functioning as design we also wanted to examine, using a small group of government employees, if the full administration process planned would be feasible with a large sample.   The survey was piloted using 80 contacts approximately 10% of our sample at the time of pilot administration.  After eliminating individuals whom we were unable to contact \(mainly due to missing or incorrect e-mail addresses, or because they left their position during the course of the pilot administration period\, we had 70 contacts remaining.  Five chose to opt-out between the time of sending out the initial survey link and sending the final reminder 5/70=7.1%\eople completed the survey in full for a response rate of 28.6%.  We also had 3 partial responses, so our rate for any responses was 32.9% or almost one-third.  This became our goal for the full administration.  Unfortunately our n from the pilot testing was too low to do any kind of statistical analysis, but we did use descriptive statistics to examine these results for basic trends. Again, our communication with public health and justice professional associations throughout the survey design and sampling processes was critical. This relationship helped us to refine our message to appeal to the government professionals we were surveying One of the major determinations made from this was that the Likert item questions were generally skewed towards the right \(i.e., the higher end\ On a strongly agree we consistently had means of 5 or 6.  We thought that the wording used for the scale itself might be causing a response bias that would make respondents answer favorably and so we modified it during the post-pilot tweak period to being 1 \(not at all\ 7 \(to a great extent\his decision allowed us to retain the wording on nearly all of the individual question items and still use a single measurement scale for the entire instrument.  Similarly, there seemed to be a selfselection bias, as respondents were mostly choosing initiatives that had succeeded, and this could also skew scale responses.  To remedy this, we revised some of the language in the survey instruction text to encourage participants to think about ALL kinds of initiatives in which they were involved and not only ones with positive outcomes  4.5.3. Full administration The full administration of the survey began by e-mailing invitations to 815 contacts, 361 individuals in criminal justice agencies and 454 individuals in public health agencies. It contained a description of the survey project and background about the previous research leading up to the survey.   Members of our sample were informed that the link to the survey itself would be mailed the following week and gave them the opportunity to opt-out if they so chose; similarly they could offer names of contacts who would replace them or suggest additional contacts to add. Thus this invitation e-mail served two purposes 1  Provide the opportunity for sampled individuals to opt-out of the survey prior to receiving the link Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 


2  Check for working e-mail addresses: if the initial e-mail message delivery failed, we searched for an updated address and resent the invitation A follow-up e-mail containing an individual s unique survey link was sent approximately one week later.  This ensured that a single survey would be tied to a single respondent.  Reminders were e-mailed to non-completers two weeks, four weeks, five weeks and six weeks after the survey link was first sent. In total, our final sample size was 617 government professionals. We had 71 opt-outs without replacements, for a rate of 11.5%.  We had 173 completed surveys, for a rate of 28.0%. The remaining 373 individuals did not complete the survey, for a 60.5% non-response rate; this also included four individuals who started the survey but did not finish/submit it for whatever reason We were also generally satisfied with the final representation concerning the sample make-up \(see Figure 1\omparing the respondents who completed the survey with the non-respondents, the percentages were very similar by policy domain; this was also true when we compared the respondents who completed the survey with the entire sample However, for both comparisons, we found that the percentages were very different by level of government.  Accordingly, we can say that our final results will be representative among our sample as a whole, but not necessarily for the entire population of public health and criminal justice government employees at the state and local levels   Sample Statistics by Policy Domain and Level of Government 0 20 40 60 80 100 Health-Local Health-State Justice-Local Justice-State Non-Respondents Completes Opt-Outs  Figure 1. Respondents by Policy Domain  4.6. Increasing survey response rate  A challenge with any survey is finding ways to increase the response rate as much as possible.  With the MIII survey, as with many other digital government projects, this was especially necessary given that we were essentially cold calling our potential respondents.  At every contact point beginning with the initial invitation letter, to the letter sent with the survey link, to the letter sent with each survey reminder, as well as to the instruction text for the survey itself our goals were to draw these individuals further into the survey process by convincing them not to opt-out and to have them move deeper into the instrument itself once they clicked on a survey link.  Ultimately we wanted them complete the survey and contribute to high response rates As part of our early survey planning, we had looked at published journal articles to see what other researchers were reporting by way of their response rates and survey techniques.  This allowed us to have our own internal target for what we hoped the response rate would be, for both the pilot and then the full administration.  We used four major steps prior to and during the administration of the full survey to maximize the number of completed responses 1  Pre-test After the initial survey instrument was created, a small group of University at Albany faculty, Center for Technology in Government staff members, and public health/criminal justice practitioners reviewed \(pre-tested\or content and clarity.  Recommendations from these participants were used to revise the instrument This included making edits to our wording for clarification purposes, breaking single questions into multiple questions, and changing response options 2  Pilot test Next, we engaged in pilot test administration. We selected 10% of individuals n=80\rom the contact list available in midOctober 2007 to develop our pilot sample.  This was done by sorting our contact list in alphabetical order by first name, then selecting every 10 th person from both the criminal justice and public health lists.  The administration process was designed to mimic what we planned to do for the full survey.  Invitations were sent out asking these individuals if they wished to participate.  The following week a survey link was e-mailed.  Reminders were sent at the start of weeks 3 and 4, then a final reminder the day before the survey was set to close.  The survey instrument was revised based on comments from respondents, as well as on results from basic survey analysis that was done. We also learned that reminders were very useful 3  Full administration This was begun by sending out 815 invitations asking sampled individuals to complete the survey.  The use of a survey invitation allowed us to eliminate members of the sample who wanted to opt-out and not complete the survey, as well as update our Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


contact list with either replacements \(if someone else from an agency would be completing it instead\mendations for new contacts We were also able to remove individuals we were unable to contact \(i.e., non-contacts 4  Reminders Finally, the use of reminders was key in increasing response rate.  Reminders were sent at the start of weeks 2, 4, and 5, then a final reminder sent two days before the official survey close date.  \(However we kept the survey open one extra week since a number of agencies were closed on the end date; this produced four additional completes.\After each point of contact with sampled individuals \(the initial survey link plus the four reminders\, we had a spike in the number of completed surveys on either the day of contact itself or on the following day \(see Figure 2   Number of Completes by Day 0 5 10 15 20 25 2  7 20 0 8 2  1 4  2 00 8 2  2 1  2 008 2  2 8 2008 3 6 2008 3/13/20 0 8 3  2 0/2 00 8 3  2 7  2 008  Figure 2. Effect of Reminders  4.7. Academic-Practitioner Collaboration  For our entire project, the partnership between academic researchers and government practitioners was critical.  Engaging practitioners at the early project stages, from design onwards, ensured that key pieces were not missed when developing the models or in designing the survey items and companion text Both parties have expertise and viewpoints that can be shared with each other and that each other can learn from  Buy in from top government practitioners in a certain domain is important for encouraging the participation of others, both in survey research and in research projects more generally.  As highlighted earlier in this paper, we spoke with the directors or heads of various national-level criminal justice and public health professional associations, some of whom agreed to pre-test the survey.  Our hope was to obtain names and contact information for their members \(to add to our sample list\nd to have them publicize the survey through their channels \(e.g mailing lists, national conferences, etc.\his also took away some of the cold calling aspect once the invitations went out, since many potential respondents did have some advance knowledge These organizational leaders played a role in legitimizing the survey.  If they did not feel that it was an effort worthy of their time, they would not have agreed to offer pre-test feedback or to share its existence with the members of their organization Therefore it was critical to obtain their support However, like with any collaborative endeavor, there were also some costs [6    5. Conclusions  As the area of digital government grows, it becomes incumbent upon academics and practitioners to work together to empirically examine this complex and multifaceted phenomenon.  Surveys are excellent ways to gather essential data for empirical analysis and Web-based surveys in particular are powerful alternatives to mail or telephone ones. However, in order to overcome the challenges of conducting Webbased surveys, researchers should consider the disadvantages of Web-based surveys mentioned above when designing and administering their own surveys. There are several general recommendations to maintain the quality of Web-based surveys in the literature \(see, for exam  2 4 all the target population of a survey should be analyzed carefully in order to assess their ability to access and participate in online surveys d th e desig n  of a Web survey should be respondent-friendly  This includes considering participants technological competencies and the congruence between the logic of how computers operate and respondents  expectations for completing survey T h ird, in  order to increase the response rate, personalized email cover letters, pre-notification to invite participating a survey, and follow-up reminders are importan a st b u t n o t least, to e n s u re confidentiality and privacy of respondents and to have less sampling error the survey should not be open to the general public and passwords and PIN numbers are highly suggested  The MIII project, although extensive and timeconsuming from start to finish, was an extremely worthwhile and unique initiative.  We can pave the way for future digital government studies through our demonstration of how Web-based surveys using an unbounded population of government employees can be conducted in this field of research.  Below, we offer a list of five lessons learned specifically from this digital government project Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


1\inate with professional organizations and government agencies This is one of our most important lessons learned. Given the growth of email communication, problems with spam and the resulting fact that government practitioners are less inclined to read \(let alone respond to\mail from unknown senders, being able to get support from professional associations and government agencies is critical. These organizations do not need to endorse the survey, as none of them did for our project However, if they feel the survey is relevant to their members or employees and that results will provide value to them as well, they are more willing to communicate the existence and purpose of your survey  2\ interdisciplinary and diverse research team Since digital government is an interdisciplinary field, it is extremely valuable to have a project staff with a wide variety of disciplinary backgrounds and skills.  Include individuals with strengths in research methods statistics, IT, project management, and writing for wide audiences.  Even if practitioners are not on the team itself, get their feedback starting at early stages of the research process was very positive.  Our work would not have been as successful as it was without having a varied mix of people involved  3\a flexible plan and timeline Although we created a timeline of work very early in the research process, doing Web-based surveys with government practitioners continually modified our initial timeline It is important to be aware of both internal and external factors that may cause you to need extra time. For our project, at certain steps we needed more time to tweak our survey instrument or handle feedback from practitioners.  Once we began building the instrument in the online tool, sometimes we hit technical difficulties.  During pilot testing we ran into both Thanksgiving and Christmas on the calendar which influenced our administration period Although we had hoped to start the full administration by early to mid-January 2008, it was not until the end of January/early February 2008 that we could begin 4\ Have patience and understand the government context Similarly, it s important to recognize that this is a long process.  It takes time to engage in new research that involves primary data collection with government practitioners.  Team members may come and go, or other projects may pull resources away Members of the sample may be in their position during one point of contact and gone to another government agency the next. Surveys can be a slow frustrating, and often tedious line of work; while the outcome will ideally be the production of a rich data set that can provide any number of contributions to both academic and practitioner audiences, it can be hard to see the light at the end of the tunnel while you re going through it 5\ Keep backups and have a contingency plan  This is not only a good practice for digital government research, but a good IT practice in general.  You never know when the important file you need is discovered to have a mistake, been deleted, or have gotten corrupted, especially with Web-based surveys.  Having backup versions for redundancy ensures that if something goes wrong, it can be fixed quickly.  Because we handled general administration tasks manually, instead of relying solely on Survey Gizmo s invitation manager, the project consisted of numerous data \(spreadsheet\files to keep track of contacts at each stage of the process If there was an error, backup files were key to sort out corrections In conclusion, the use of online surveys offers a powerful yet risky research mechanism. The ability to survey a large population of geographically politically \(i.e., multiple levels of government\d professionally diverse government professionals within a quick turnaround timeframe and with relatively little cost provides a lot of value. The risk comes in part from the ease and prevalence of the communication method: e-mail. While a Web-based survey may be easier to administer than a paper one the fact is that the response rate and quality of responses may be significantly lower than from a paper version. Government professionals, like many others, are inundated with e-mail communication; a significant amount of it which folks consider to be spam or generally a nuisance for other reasons This forces digital government researchers first to build a relationship with key members of the relevant government professional communities. This also requires making the case for your research and why anyone should open another unsolicited e-mail and participate in another survey they do not have time for.  Therefore, if possible an e-mailed survey link should not be the first time that the government professional learns about your organization and the research for which you re collecting. Overall, Webbased surveys are valuable for digital government researchers, but potential challenges should be considered carefully  6. Acknowledgments  This work was partially supported by the National Science Foundation under Grant No. ITR-0205152 Any opinions, findings, and conclusions or recommendations expressed in this material are those Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


of the authors and do not necessarily reflect the views of the National Science Foundation  7. References  1 Boy e r, K  K O l s o n, J  R   J a c k s on, E. C Electronic surveys: Advantages and disadvantages over traditional print survey  Decision Line 32\(4\, 2001, 4-7 2 B ry man A  Social Research Methods \(2 nd ed Oxford University Press, New York, 2004 3 Caf f r e y  L  Ed Information Sharing Between Within Governments Commonwealth Secretariat, London 1998 4 Curra ll S C Ju dg e  T  A  Measuring trust between organizational boundary role persons  Organization Behavior and Human Decision Processes, 64 2\95 151-170 5 D a w e s  S. S Interagency Information Sharing Expected Benefits, Manageable Risks  Journal of Policy Analysis and Management, 15 3\, 1996, 377-394  6 D a w e s  S. S H e lbig N   Building a ResearchPractice Partnership: Lessons from a Government IT Workforce Study Proceedings of the 40th Hawaii International Conference on System Sciences 2007 7 D a w e s  S. S  P a rd o, T  A  200 2  Building Collaborative Digital Government Systems: Systematic Constraints and Effective Practices In W. J. McIver & A K. Elmagarmid, \(Eds Advances in Digital Government Technology, Human Factors, and Policy pp. 259-273 Kluwer Academic Publishers, Norwell, MA, 2002  8 D a w e s  S. S  P r  f onta i ne   L  Understanding New Models of Collaboration for Delivering Government Services  Communications of the ACM, 46 1\03, 4042 9 Dillm a n D.A T o rtora R.D  Bow ke r, D P r inc i ple s  for constructing web surveys. Working paper. Retrieved August 20, 2008 from http://www.sesrc.wsu.edu/dillman/papers/websurveyppr.pd f 10 Eg le ne  O D a w e s  S. S  Sc hne ide r  C. A  Authority and Leadership Patterns in Public Sector Knowledge Networks  The American Review of Public Administration, 37 1\2007, 91-113  11 G il-G a rc a J  R P a r do T  A  Multi-Method Approaches to Digital Government Research: Value Lessons and Implementation Challenges Proceedings of the 39th Hawaii International Conference on System Sciences 2006. \(Creswell, J. W Research Design Qualitative, Quantitative, and Mixed Methods Approaches  SAGE Publications, Thousand Oaks, CA, 2003 12 G il-G a rc a J  R., Che ng a l urSm ith, I D u c h e s s i  P Collaborative E-Government: Impediments and Benefits of Information Sharing Projects in the Public Sector  European Journal of Information Systems, 16 2\, 2007 121-133 13 G il-G a rc a J  R., P a r do T  A  Burk e  G  B Government Leadership in Multi-Sector IT-Enabled Networks: Lessons from the Response to the West Nile Virus Outbreak Proceedings of the Leading the Future of the Public Sector: The Third Transatlantic Dialogue Workshop \(May 31 June 2, 2007\niversity of Delaware, Newark, Delaware, USA  14 G il-G a rc a J  R., Sc hne ide r   C P a rd o, T  A   Cresswell, A. M Interorganizational Information Integration in the Criminal Justice Enterprise: Preliminary Lessons from State and County Initiatives Proceedings of the 38th Hawaii International Conference on System Sciences 2005 15 G l a s e r B. G  Basics of Grounded Theory Analysis Emergence vs. Forcing Sociology Press, Mill Valley, CA 1992 16 G l a s e r B. G  Stra us s  A  L  Discovery of Grounded Theory: Strategies for Qualitative Research  Aldine Transaction, Chicago, IL, 1967 17 G r a n e llo, D  H   W h e a t on J  E  Online  Data Collection: Strategies for Research  Journal of Counseling Development  82 4\, 2004, 387-393 18 G un H  Web-based Surveys: Changing the Survey Process  First Monday 7\(2\, 2002 1 Jarven p aa S  L   Ives B   Executive involvement and participation in the management of information technology  MIS Quarterly, 15 2\991, 205-227 20 L a n f r e d C. W  The Paradox of Self-Management Individual and Group Autonomy in Work Groups  Journal of Organization Behavior, 21 5\, 2000, 563-585 21 L e f e v e r, S., D a l, M  Ma tt hia s d ttir, A Online Data Collection in Academic Research: Advantages and Limitations  British Journal of Educational Technology 38 4\, 2007, 574-582 22  P a r d o  T  A G ilG a rc  a J  R Burk e  G  B Sustainable Cross-Boundary Information Sharing In H Chen et al. \(Eds Digital Government: E-Government Research, Case Studies, and Implementation pp. 421-438 Springer, New York, 2008  23  Se g a rs A   G r ov e r V  Strategic Information Systems Planning and Success: An Investigation of the Construct and its Measurement  MIS Quarterly, 22 2 1998, 139-163 24 n n o n D  M J o hns on T  E Se a r c y S., L o tt, A  Using electronic surveys: Advice from survey professionals  Practical Assessment, Research Evaluation 8 \(1 25 us s  A  Cor bin  J    Basics of Qualitative Research. Techniques and Procedures for Developing Grounded Theory Sage Publications, Thousand Oaks, CA 1998 26 U m ba c h  P  D  Web surveys: Best practices  New Directions for Institutional Research 121, 2004, 23-38 27  V a n Se lm M J a nk ow s k i N  W  Conducting online surveys  Quality & Quantity 40\(3\, 2006, 435456  2 W r i g h t  K B   Researching Internet-based populations: Advantages and disadvantages of online survey research, online questionnaire authoring software packages, and web survey services  Journal of ComputerMediated Communication 10\(3\, 2005, 1-19  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


11 nI KK t nn nn nd nd nd nd X yz yz y y z z a      13 The heading estimate  can then be calculated geometrically using the relative displacement of the lateral components of the intersection line in the navigation frame x = East and y = North 14        min,max min,max,atan2  xx yy II II?  \(14 4. THEORETICAL PERFORMANCE USING SYNTHESIZED FEATURES Simulation Procedure This section will describe a simulation that was developed to assess the theoretical heading accuracy possible from ALS data and a perfect synthesized feature.  The first step of the simulation was to define the scanner parameters that reflect those likely to be used in a flight test: 30 lines per second, 10,000 pulses per second, and a  30 deg field of view.  ALS angles, times, and ranges were then simulated using these parameters.  A ground feature was synthesized by replacing the simulated ranges at the correct ground positions with those calculated according to the desired feature parameters \(i.e. plane equations synthesis and plane intersection procedure described in this section is summarized in the block diagram shown in Figure 8  Figure 8: Feature Synthesis and Plane Intersection Block Diagram Table 1: Simulation Parameters Parameter Value Aircraft Altitude 350, m Points in Surface 1 127 Points in Surface 2 248 Feature Length 25.79, m Feature Width 51.12, m The synthesized feature for this paper was based on airborne measurements of the size, location, and roof slope parameters of the AEC hangar at the Ohio University Airport.  For an aircraft at ~350m altitude in the simulation the synthesized hangar would be illuminated by 127 points 


on surface 1 and 248 points on surface 2 contained within a feature footprint with dimensions of 25.79 m long x 51.12 m wide as summarized in Table 1.  The flight profile from a January 2005 flight test [7] was used to establish the aircraft position and orientation over the two second flight duration needed to extract the simulated feature.  For the data collection phase, the hangar building was over flown while collecting GPS, INS, and ALS data.  In the synthesis phase actual GPS and INS information were used, but the ALS data was replaced with simulated data so that the ALS measurement errors could be controlled.  The heading derived from the measured ALS flight data will be compared with the simulation in the next section Two existing simulators were used to generate the ALS scan angle and ranges measurements [9].  An angle simulator was used to produce an array of angles and times that correspond to the laser scanner parameters.  A range simulator was used to produce range estimates by calculating the geometric difference between the ALS height and the terrain crossing \(simulated using a Digital Terrain Elevation Data \(DTED terrain crossing was found by increasing the range until it intersected the terrain and then iterated until the height accuracy was within some threshold \(1 ?m in this case The simulated range accuracy was determined from the height accuracy using the geometric relationship described in \(15 7 cos dHdR =  \(15 where dR = Range accuracy dH = Height accuracy ALS scan angle The top subplot of Figure 9 illustrates the range accuracy threshold used by the simulator \(determined from the height error and \(15 can be seen in the bottom subplot of Figure 9 because of the range iteration technique.  A 1 ?m height accuracy \(iteration threshold sufficiently small \(i.e. approximating true range influence the standard deviation of the final simulated range measurement \(i.e. true range + sensor noise better represent the capabilities of the true ALS, 25 mm of Additive White Gaussian Noise \(AWGN range measurement as shown in the top subplot of Figure 10 following with the Gaussian distribution function shown in the bottom subplot.  The DTED accuracy was not important for this analysis since only the range measurements to the simulated feature were required for the orientation calculations.  The range to the DTED merely provides terrain measurements for height contrast in the feature extraction algorithms  Figure 9: Range Error Distribution from the Simulator   Figure 10: Range Error + Noise Distribution with 25 mm Noise The feature was synthesized by calculating its height using the planar equations for surface 1 and surface 2 of the AEC hangar roof and the illuminated ground pulse positions as shown in \(16 2 22 11   BH BH   


 Surface Surface V V 16 where V = Vertical height H = Horizontal position vector   B = Plane parameters \(Surface 1 or Surface 2 Pulses that fall within a predetermined space \(represented by white space in Figure 11 points.  The DTED terrain heights at these positions were replaced with feature heights from a feature database prior to range simulation. The synthesized feature is shown in Figure 12  Figure 11: Synthesized Feature Footprint   Figure 12: Synthesized Feature Extraction Simulation Results for Typical System Performance Once a feature has been simulated, the feature extraction and plane intersection algorithms described in the previous section are used to determine the feature orientation.  The result of the least squares plane fit is shown in Figure 13 Notice that a gap is present near the ridgeline in Figure 13 This is due to the exclusion of the actual ridge point Because the peak of the roof contains a ridge cap, inclusion of these points in surface 1 or surface 2 would distort the results.  By using the plane intersection method, these few points \(16 hundred other points would remain in the plane.  The height residuals after subtracting the plane fit are shown in Figure 14.  As expected, these height residuals reflect the same accuracy  25 mm 1 shown previously in Figure 10.  The plane residuals shown in Figure 14 can be used to measure the range accuracy directly from the data.  The next section will describe how to use this information to predict the heading accuracy 8  Figure 13: Data Point Segregation and Plane Fit   Figure 14: Plane Fit Residuals The plane parameters, 1B  and 2B  are then used to find the equation of the intersection line as is shown in Figure 15 The intersection line contains heading information for comparison with some reference data.  In this case, the reference was derived from the same range simulation without noise  Figure 15: Plane / Plane Intersection Line Multiple repetitions of the synthesized feature extraction and plane intersection algorithm were used to determine the repeatable accuracy of the technique over 5000 Monte Carlo runs as shown by the block diagram in Figure 8.  The resulting 1? heading accuracy is shown in Figure 16 for  25 mm 1? ALS range accuracy  Figure 16: Heading Accuracy for 25 mm Range Error The error performance is summarized in Table 2.  The bias term is statistically zero \(as would be expected from the zero mean AWGN added to the simulated range heading error is better than a tenth of a degree Table 2: Heading Error Summary Error Type Magnitude Heading Error Mean 4.0778 x 10-4, deg 1? Heading Standard Deviation 0.0395, deg 0.69, mrad 


1? Heading Standard Deviation 0.0395, deg 0.69, mrad Sensitivity Analysis for Varying System Performance In order to understand how the heading error varies with parameters such as laser ranging accuracy, position accuracy, and heading bias, a sensitivity analysis was conducted.  For computational efficiency the number of repetitions at each parameter setting was reduced to 1000 For each set of 1000 repetitions of the plane intersection algorithm, the one parameter was linearly increased and its affect on the heading accuracy was assessed.  The range accuracy will be considered first, whereby the simulated range noise was increase in 46 steps from 0.001m until 0.31m.  The heading error is computed by comparing the noisy simulated feature intersection with a noise-free feature intersection.  Figure 17 shows the increase in heading standard deviation as the range standard deviation increases while Figure 18 shows the rate of change of the standard deviation per unit of range noise.  Although not shown here the heading resolution bias is zero-mean up to 15 cm and increases to almost 0.6 deg for 30 cm of range noise 9  Figure 17: Heading Error Standard Deviation  Figure 18: Heading Error Standard Deviation Growth Rate per Unit of Range Noise The results shown in Figure 17 illustrate a fairly linear increase in heading error growth for small range errors \(less than 12 cm rate of error growth is constant \(~1.53 deg/m range error.  The results shown in the previous section can be directly estimated using these figures.  If a range noise standard deviation of 25 mm is multiplied by a 1.53 deg/m error growth rate, the predicted heading accuracy would be 0.668 mrad.  This predicted accuracy is approximately the same as the accuracy found from simulation summarized in Table 2 The next parameter to be considered was a constant heading bias.  For the algorithm presented in this paper, the heading bias must be consistent with the position error as would be the case for an IMU operating in a dead reckoning mode Consequently, a heading error was simulated from 0 to 2 degrees in the inertial measurements and then the ENU position was corrupted using the erroneous heading in the direction cosines matrix.  Figure 19 illustrates the residual heading resolution error bias after the simulated heading bias has been removed.  Figure 20 illustrates the standard deviation of the heading resolution error  Figure 19: Heading Resolution Error Bias as a Function of Injected Heading Bias   Figure 20: Heading Resolution Error Standard Deviation as a Function of Injected Heading Bias Up to 2 deg of injected heading bias, there is not discernable effect on the heading resolution accuracy in either the mean or the standard deviation.  This illustrates that the algorithm can detect a constant heading bias without sensitivity to its magnitude The final parameter to be considered was position noise The position noise was varied linearly from 0 to 10 cm in 48 steps.  The response of the error mean to position noise is shown in Figure 21 while the response of the error standard deviation to position noise is shown in Figure 22 10  Figure 21: Heading Resolution Error Bias as a Function of Injected Position Noise   Figure 22: Heading Resolution Error Standard 


Figure 22: Heading Resolution Error Standard Deviation as a Function of Injected Position Noise As the injected position noise increases, the heading error bias remained nearly constant at the mm-level as would be expected since the injected position noise was AWGN Similarly, the heading error standard deviation increases as the injected position noise increases.  As with the laser range noise, the position noise will also determine the heading determination standard deviation.  Inertial positions are generally low noise \(better than 1 cm source is not felt to be as significant of a contributor \(&lt; 0.04 deg As an aside, it is interesting to note that this error will increase slightly as the aircraft height increases.  This is thought to be due to the reduced number of laser pulses that comprise each plane in the intersection equation.  For example, if the aircraft height increases from 350m to 400 m, the pulse count decreases to 68 pulses in surface 1 and 141 pulses in surface 2.  The heading accuracy will decrease by approximately 0.01 degrees 5. FLIGHT TEST PROOF OF CONCEPT  Figure 23: Flight Path over AEC Hangar The flight test data was collected in January 2005 and provided ALS measurements containing several features at the Ohio University Airport including the AEC hangar [7 The flight path over the hangar is shown in Figure 23.  The ALS settings during this flight test were more optimal for this application than for mapping since the gaps between scan lines was large compared to the gap between pulses in a line.  In September 2005, a static GPS survey was conducted \(relative to the KUNI GPS Continuously Operating Reference Station the AEC hangar.  The GPS survey provided an absolute reference to compare with the ALS measurements.  The lateral measurement accuracy of this survey was thought to be on the order of  20 cm because of antenna placement uncertainty Planes were fit to the ALS data from each side of the hangar roof as described previously as shown in Figure 24.  The plane fits are shown in lighter colors \(cyan and magenta than the measured data \(blue and red  Figure 24: Two Planes fit to the Data  Figure 24 is only intended to provide an overview since the plane fit residuals contain more information as shown in Figure 25.  The plane fit residuals indicate 9 cm of height accuracy \(1 11  Figure 25: Plane Fit Residuals The resulting plane intersection line was overlaid on the ALS measurements as shown in Figure 26.  The line was a close visual match, but when comparing the calculated line with the surveyed line, slight errors can be observed  Figure 26: Line Formed by Plane Intersection A closer examination of the two lines in the horizontal plane reveals their differences in the heading as shown in Figure 27  Figure 27: Intersection Line Comparison As mentioned previously, the measured plane-fit residuals provide an indication of the heading accuracy that could be expected from the real data if the survey was perfect.  Since the flight test residuals were within the linear region of the empirical error curves shown in Figure 17, the expected heading accuracy should be predictable.  Since the measured range accuracy is 9 cm \(from the plane-ft residuals heading accuracy is expected to be 0.1377 deg \(at 1.53 


deg/m to 2.4 mrad.  This accuracy is better than the typical heading alignment accuracy of a commercial navigation grade INS The actual heading angle of the two vectors is summarized in Table 3 Table 3: Hangar Ridge Vector Comparison Heading Survey 60.9436, deg Intersection 61.8772, deg Difference 0.9336, deg 16.29, mrad The importance of this proof of concept was to demonstrate the technique and to show that the accuracy can be predicted.  In this case there are several sources of uncertainty that might explain part of the performance degradation The reference survey introduced lateral antenna placement errors of  20cm.  The antenna placement uncertainty is expected to be the dominant error source in the measured data presented here.  Over a 25 m baseline, a  20 cm placement error will become a 0.916 deg pointing error worst case prediction using flight test data Further effort is needed to make a more accurate ground feature survey, but the concept of heading determination from ALS plane intersections has been demonstrated 6. CONCLUSIONS This paper presented results that leverage the accuracy and high number of \(i.e., oversampled measurements along with a priori surveyed features to determine airborne platform heading.  Plane fitting has the affect of averaging the oversampled measurements and can deliver mrad-level heading observations.  The ALS-derived heading information shown in this paper could be used to periodically calibrate tactical grade IMU heading biases while in flight, to perform an ALS / IMU calibration prior to a terrain aided landing, or simply to stabilize the inertial heading measurements when GPS is unavailable.  A simulation was presented to show the theoretical performance of heading determination from plane intersections using typical ALS parameters.  With a perfect survey, the heading accuracy was better than 1 mrad.  Using the same simulation software, a theoretical sensitivity analysis was conducted to determine the effect of range noise, heading bias, and position noise on the heading accuracy.  The heading standard deviation was shown to be a function of the range measurement and position standard deviations.  Inertial positions are generally low noise, so this error source was not deemed to be as significant of a contributor as the range noise.  The heading error magnitude did not change the performance in a noticeable way. This theoretical performance assessment was then compared with 12 flight test data.  While the flight test performance was not as good as expected, the error sources were justified.  Even with large survey uncertainties, the heading measurement performance was still within 1 deg.  If a better survey was available, the flight test results were expected to be accurate to within a few mrad Operationally, there are many things than can be done to improve these results.  This paper discussed flight over a single building.  Accuracy can potentially be improved by flying over multiple buildings of larger sizes and at lower altitudes or other types of features.  Additionally, the heading error growth rates could be predicted by tracking the heading change while flying over a building or between two buildings.  With careful attention to calibration and measurement accuracy, these heading alignments can potentially be made at a higher accuracy than a navigation grade INS can align its heading REFERENCES 1] Schenk, T  Modeling and recovering systematic 


1] Schenk, T  Modeling and recovering systematic errors in airborne laser scanners  Proceedings of the OEEPE workshop on Airborne Laserscanning and Interferometric SAR for Detailed Digital Elevation Models, OEEPE Publication no. 40, 2001, pp. 40-48  2] Crombaghs, M.J.E., Brugelmann, R., and d Min, E.J  On the Adjustment of Overlapping Strips of Laseraltimeter Height Data  International Archives of Photogrammetry and Remote Sensing, 2000, 33 B3/1   3] Dickman, J., and Uijt de Haag, M  Aircraft Heading Measurement Potential from an Airborne Laser Scanner Using Edge Extraction  Proceedings of the IEEE Aerospace Conference, March 3-10, 2007  4] Kraus, K., Pfeifer, N  Determination of Terrain Models in Wooded Areas with Airborne Laser Scanner Data  ISPRS Journal of Photogrammetry and Remote Sensing, 1998, 53, pp. 193-203  5] Maas, H., Vosselman, G  Two Algorithms for Extracting Building Models from Raw Laser Altimetry Data  ISPRS Journal of Photogrammetry and Remote Sensing, 1999, 54, pp. 193-203  6] Venable, D., Campbell, J., and Uijt de Haag, M  Feature Extraction and Separation in Airborne Laser Scanner Terrain Integrity Monitors  Digital Avionics Systems Conference, 2005  7] Campbell, J. L., M. Uijt de Haag, van Graas, F  Terrain Referenced Precision Approach Guidance   Proceedings of the ION National Technical Meeting 2005, San Diego, CA, January 24-26, 2005, pp. 643653  8] http://www.geom.umn.edu/software/download/COPYI NG.html, accessed May 2007  9] V. Nguyen, et. al  A Comparison of Line Extraction Algorithms Using 2D Laser Rangefinder for Indoor Mobile Robotics  Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, Aug. 2-6, 2005, pp. 1929-1934  10] Vadlamani, A  Preliminary Design and Analysis of a LIDAR Based Obstacle Detection System   Proceedings of the 24th Digital Avionics Systems Conference  13 BIOGRAPHY Jeff Dickman graduated from Ohio University in 2008 with a Ph.D in Electrical Engineering His research emphasizes navigation system integration and sensor stabilization.  He has also been involved with GPS landing system research and antenna design and measurement.  He is presently working on vision-aided navigation systems.  He is a member of the IEEE ION, AIAA, Eta Kappa Nu, and Tau Beta Pi  Maarten Uijt de Haag is an Associate Professor of Electrical Engineering at Ohio University and a Principal Investigator with the Ohio University 


Investigator with the Ohio University Avionics Engineering Center.  He earned his Ph.D. from Ohio University and holds a B.S.E.E. and M.S.E.E from Delft University of Technology located in the Netherlands.  He has been involved with GPS landing systems  research, advanced signal processing techniques for GPS receivers, GPS/INS integrated systems, and terrain-referenced navigation systems.  The latter includes the development of terrain data base integrity monitors as an enabling technology for Synthetic Vision Systems and autonomous aircraft operation      pre></body></html 


           7 C ur re nt ly n ot w or ki ng 2  0 0 5  0 0 8 0 04 0 04 0 0 1 0 16  0 16             8 C ur re nt ly in e du ca tio n2   0 6 


6 7  0 01 0 1 9 0 08  0 03 0 6 8 0 0 7 0 3 2           9 C ur re nt ly w or ki ng 2  0 2 8  0 03 0 18  0 1 1 0 0 3 0 64  0 00 0 1 4 0 8 9   


        10 E du ca tio n ac hi ev ed 3  3 5 7 1 5 2  0 04 0 02 0 2 1 0 1 2 0 16  0 02 0 1 6 0 13  0 0 6         11 D is pe ns ab le in co m e   


  21 0 9 2 72 7  0 14  0 0 1 0 09  0 08  0 2 0 0 00 0 0 4 0 18  0 1 6 0 0 1        In te rn et u sa ge                     


  12 A ct iv e in te rn et u sa ge 1  0 0 2 0 9 6 0 2 1 0 25  0 11  0 12  0 10  0 0 4 0 05  0 0 8 0 0 5 0 0 1 0 12        13 H ou rs o nl in e h ou rs 


rs   2 6 5 3 0 3  0 04 0 12  0 1 1 0 0 3 0 40  0 0 7 0 0 7 0 4 7 0 5 3 0 07  0 1 1 0 07       14 W illi ng ne ss to p ay 1  1 8 3 0 6 3  0 03 0 10 


10  0 07  0 08  0 0 2 0 0 4 0 0 1 0 01  0 00 0 0 5 0 14  0 04 0 05      G am e sp ec ifi c va ria bl es                      15 T en 


ur e w ee ks   2 8 2 3 5 2 0 2 6 0 31  0 0 9 0 01 0 12  0 0 4 0 02 0 0 9 0 0 9 0 07  0 02 0 13  0 08  0 0 4    16 C ro ss o ve r on o ffl in e 4  0 1 5 


5 1 1 1 0 1 9 0 11  0 13  0 18  0 2 0 0 1 4 0 0 7 0 14  0 1 1 0 0 4 0 08  0 15  0 0 5 0 01 0 07    17 S at is fa ct io n1   18 7 5 1 3 16  0 18  0 00 


00 0 44  0 52  0 1 4 0 0 3 0 02 0 07  0 0 9 0 1 4 0 10  0 08  0 0 6 0 09  0 0 1 0 13   18 C om m itm en t1  0 6 2 0 8 3 0 3 1 0 13  0 37  0 39  0 0 7 


7 0 0 6 0 02 0 03  0 0 4 0 1 3 0 14  0 17  0 0 5 0 09  0 07  0 19  0 58  S ou rc e O w n ca lc ul at io n N ot e N  1 3 89 o bs er va tio ns S ig ni fic an ce le ve ls 


ls  p  0 05 S D  S ta nd ar d de vi at io n 1 5 po in t L ik er t s ca le ra ng in g fro m 2 to 2  2 du m m y va ria bl e 3 o rd in al v ar ia bl e ra ng in g fro m v oc at io na l e du ca 


tio n to P h D 4 n um be r o f c on ta ct s   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


