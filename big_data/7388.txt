An Improved MapReduce Algorithm for Mining Closed Frequent Itemsets Yaron Gonen The Department of Computer Science Ben-Gurion University of the Negev Beêer-Sheva Israel Email yarongon@cs.bgu.ac.il Ehud Gudes The Department of Computer Science Ben-Gurion University of the Negev Beêer-Sheva Israel Email ehud@cs.bgu.ac.il 
Mining closed frequent itemsets is a key objective in the eld of data mining due to its wide range of applications Given a database of transactions the task is to nd closed subsets 
Abstract 
which appear frequently in different transactions This subject has been studied thoroughly and many efìcient algorithms had been presented however most of them were designed for a nondistributed setting The exponential growth of data in current times forces storing it in a distributed setting meaning that most algorithms no longer apply MapReduce is an acclaimed programming paradigm for processing large-scale distributed data In this paper we present an efìcient algorithm for mining closed frequent itemsets using the MapReduce paradigm In addition to its novelty of running in a distributed setting it also makes the duplication elimination step a common step to all 
existing algorithms redundant 
I I NTRODUCTION Mining of frequent itemsets for association rules has been studied thoroughly in centralized static datasets and data streams settings A major branch in this research eld is mining closed frequent itemsets instead of frequent itemsets for discovering non-redundant association rules A set of closed frequent itemsets is proven to be a complete yet compact representation of the set of all frequent itemsets Mining closed frequent itemsets instead of frequent itemsets saves computation time memory usage and produces a compacted output Many algorithms like Closet Closet 
CHARM and FP-Close ha v e been presented for mining closed frequent itemsets in centralized datasets Mining closed frequent itemsets in big distributed data is more challenging than mining centralized data in the following aspects 1 The distributed settings is a shared-nothing environment one can of course share data however it is very expensive in terms of communication meaning that assumptions like shared memory and shared storage that lie at the base of most algorithms no longer apply 2 Data transfer is more expensive than data processing meaning that performance measurements change 3 The data is huge and cannot reside on a single node 
MT and D-Closed 5 are tw o parallel algorithms for mining closed frequent itemsets however both suffer from a few drawbacks see section II and as will be shown our scheme overcomes these drawbacks 
MapReduce 
MapReduce is a programming paradigm for parallel processing of large-scale distributed data using a computer cluster This paradigm was developed to answer the rising need for big data processing due to the exponential growth in stored digital data The idea that lies at the core of the model is to use many low-priced commodity hardware rather than few high-end high-priced computers and storage 
systems The paradigm is designed in a way that relieves the programmer of the tedious job of synchronizing threads and handling processes on nodes the programmer needs only to implement two functions 
Our Contribution 
and  and the framework takes care of all the rest The most popular implementation of the MapReduce model is Hadoop de v eloped by Y ahoo Labs and now maintained by Apache In this paper we present a novel algorithm for mining closed frequent itemsets in big distributed data settings using the MapReduce paradigm Using 
map reduce 
MapReduce makes our algorithm very pragmatic and relatively easy to implement maintain and execute In addition our algorithm does not require a duplication elimination step which is common to most known algorithms it makes both the mapper and reducer more complicated but it gives better performance II R ELATED W ORK The rst algorithm for mining closed frequent itemsets A-Close was introduced in It presented the concept of 
a set of items that generates a single closed frequent itemset A-Close implements an iterative generation 
generator 
and-test method for nding closed frequent itemsets On each iteration generators are tested for frequency and non-frequent generators are removed An important step is duplication elimination generators that create an already existing itemset are also removed The surviving generators are used to generate the next candidate generators A-Close was not designed to work in a distributed settings 
MT is a parallel algorithm for mining closed frequent itemsets It uses a divide-and-conquer approach on the input data to reduce the amount of data to be processed at 
A State of the Art 
2016 IEEE International Conference on Software Science, Technology and Engineering 978-1-5090-1018-9/16 $31.00 © 2016 IEEE DOI 10.1109/SWSTE.2016.19 76 
2016 IEEE International Conference on Software Science, Technology and Engineering 978-1-5090-1018-9/16 $31.00 © 2016 IEEE DOI 10.1109/SWSTE.2016.19 77 


          
                        3       
t a c d e f t a b e t c e f t a c d f t c e f 
i i   i x x t t  t i x sup D x sup x x minSup x sup x minSup x f g f i t i t g x t i x i t f g x g x x x x g x g x sup x g x h f g x h x x x x minSup a b c d e f minSup c t t t t sup c minSup c f c c sup c f sup c a c f e c e f 
1 2 1 2 1 2 1 2 1 2 1 3 4 5 
1 2 3 4 5 
m n 
I   002I D   D I I D D D D 003 T\002D D T  004I|\005 004T 004   004D|\005 004 004  T D 002 006 007   010 D D D D D I   D                     
TABLE I E XAMPLE DATABASE  TID IS THE TRANSACTION IDENTIFIER  III P ROBLEM D EFINITION Let 
each iteration However its parallelism feature is limited MTClosed is a multi-threaded algorithm designed for a multi-core architecture Though superior to a single-core architecture a multi-core architecture is still limited in its number of cores and its memory is limited is size and must be shared among the threads In addition the input data is not distributed and an index-building phase is required D-Closed is a shared-nothing en vironment distrib uted algorithm for mining closed frequent itemsets It is similar to MT-Closed in the sense that it recursively explores a sub-tree of the search space in each iteration a candidate is generated by adding items to a previously found closure and the dataset is projected by all the candidates It differs from MT-Closed in providing a clever method to detect duplicate generators it introduces the concepts of pro-order and anti-order and proves that among all candidates that produce the same closed itemset only one will have no common items with its antiorder set However there are a few drawbacks to D-Closed 1 it requires a pre-processing phase that scans the data and builds an index that needs to be shared among all the nodes 2 The set of all possible items needs also to be shared among all the nodes and 3 the input data to each recursion call is different meaning that iteration-wise optimizations like caching cannot be used Wang et al ha v e proposed a parallelized AFOPT close algorithm and ha v e implemented it using MapReduce Lik e the previous algorithms it also works in a divide-and-conquer way rst a global list of frequent items is built then a parallel mining of local closed frequent itemsets is performed and nally non-global closed frequent itemsets are ltered out leaving only the global closed frequent itemsets However they still require that nal step of checking the globally closed frequent itemsets which might be very heavy depending on the number of local results Moens et Al describe an algorithm for mining frequent itemsets using MapReduce however they mention the mining of closed frequent itemsets as a post-processing step and do not utilize this in their algorithm which is therefore potentially less efìcient than a closed frequent itemsets algorithm In general there may be several performance measures for evaluating the performance of an algorithm in the MapReduce model such as clock time or sum of all tasks run time In this study we will follow the communication-cost model proposed by Afrati and Ullman in to measure the efìciency of an algorithm in the MapReduce model A is a single map or reduce process executed by a single computer in the network The of a task is the size of the input to this task Note that the initial input to a map-task meaning the input that resides in le is also counted as input Also note that we do not distinct map-tasks from reduce-tasks for this matter The is the sum of the communications costs of all the tasks in the MapReduce job TID Transaction be a set of with some lexicographic order An is a set of items such that A isa set of itemsets each called a  Each transaction in is uniquely identiìed with a transaction identiìer TID and assumed to be sorted lexicographically The difference between a transaction and an itemset is that as itemset is an arbitrary subset of while a transaction is a subset of that exists in and identiìed by its id  The of an itemset in  denoted  or simply when is clear from the context is the number of transactions in that contain sometimes it is the ratio of transactions Given a user-deìned denoted  an itemset is called if  Let be a subset of transactions from and let be an itemset We deìne the following two functions and  Function returns the intersection of all the transactions in  and function returns the set of all the transactions in that contain  Notice that is antitone meaning that for two itemsets and   It is trivial to see that  The function is called the or  An itemset is in iff  It is equivalent to say that an itemset is closed in iff no itemset that is a proper superset of has the same support in  exists Given a database and a minimum support  the is nding all frequent and closed itemsets in  Let  let and let be the transaction database presented in table I Consider itemset  It is a subset of transactions   and  meaning that  which is greater than  However  which is a proper superset of  is also a subset of the same transactions is not a closed itemset since  The list of all closed frequent itemsets is   and  We now present an algorithm for mining frequent closed itemsets in a distributed settings using the MapReduce paradigm 
B MapReduce Communication Cost Model task communication cost total communication cost items itemset transactional database transaction support minimum support frequent Galois operator closure operator closed mining closed frequent itemsets problem A Example 
77 
78 


i i i i i i i i n n i i n i i n 
D 011       D 004 002 004  T 003 
generator map task reduce task minimal 
 1           002 
i C C i C i C 002 i g f C C C p c h p c p c c f c f i C t c C c t t item t c item item c c.generator g g.item g t g g t g g s sum t g t sum g t s   t s n t g s minSup sup g s minSup t  t c 
closure generators generators 
1 0 1 1 1 1 1 1 
IV T HE A LGORITHM Our algorithm is iterative where each iteration is a MapReduce job The inputs for iteration are 1  the transaction database and 2 the set of the closed frequent itemsets found in the previous iteration   the input for the rst iteration is the set containing the empty set The output of iteration is  a set of closed frequent itemsets that have a generator of length If then another iteration  is performed Otherwise the algorithm stops As mentioned earlier each iteration is a MapReduce job comprised of a map phase and a reduce phase The map phase which is equivalent to the function emits sets of items called or simply  The reduce phase which is equivalent to the function nds the closure that each generator produces and decides whether or not it should be added to  Each set added to is paired with its generator The generator is needed for the next iteration The output of the algorithm which is the set of all closed frequent itemsets is the union of all s Before the iteration begin we have discovered that a preprocess phase that nds only the frequent items greatly improves performance even though another MapReduce job is executed and this data must be shared among all mapper tasks This MapReduce job simply counts support of all items and keeps only the frequent one Pseudo-code of the algorithm is presented in Algorithm 1 We provide explanations of the important steps in the algorithm To better understand the algorithm we need some deìnitions Let be an itemset and let be a closed itemset such that  then is called a of  Note that a closed itemset might have more than one generator In the example above both and are the generators of  An execution of a map function on a single transaction is called a  An execution of a reduce function on a speciìc key is called a  A map task in iteration gets 3 parameters as input 1 a set of all the closed frequent itemsets with their generator found in the previous iteration denoted which is shared among all the mappers in the same iteration 2 a single transaction denoted  and 3 the set of all frequent items in again this set is also shared among all the mappers in the same iteration and in all iterations Note that in the Hadoop implementation the mapper gets a set of transactions called and the mapper object calls the map function for each transaction in its own split only For each if then holds the potential of nding a new closed frequent itemsets and we use it to form a new generator For each we check if is frequent If so we concat to the generator of denoted  thus creating we denote that added item as  a potential new generator for other closed frequent itemsets The function emits a message where is the key and the tuple  1 is the value The 1 is later summed up and used to count the support of the itemset Notice that is not only a generator but it is always a generator Concatenating an item not in its closure guarantees to reach another minimal generator More precisely it generates all minimal generators that are supersets of with one additional item and such that supports it Since all transactions are taken every minimal generator with a support of at least 1 is emitted at some point A theorem is proved in Section IV-H Pseudo code of the map function is presented in Algorithm 2 A combiner is not a part of the MapReduce programming paradigm but a Hadoop implementation detail that minimizes the data transferred between map and reduce tasks Hadoop gives the user the option of providing a that is run on the map output on the same machine running the mapper and the output of the combiner function is the input for the reduce function In our implementation we have used a combiner which is quite similar to the reducer but much simpler The input to the combiner is a key and a collection of values the key is the generator which is an itemset and the collection of values is a collection of tuples composed of transactions  all containing and a number indicating the support of the tuple Since the combiner is local by nature it has no use of the minimum support parameter which must be applied in a global point of view The combiner sums the support of the input tuples stores it in the variable and then performs an intersection on the tuples to get  The combiner emits a a message where is the key and the tuple    is the value Pseudo code of the combiner function is presented in Algorithm 3 A reduce task gets as input a key a collection of values and the minimum support The key is the generator which is an itemset the value is a collection of tuples composed of a transaction  all containing and a number indicating the support of the tuple In addition it gets as a parameter the user-given minimum support  At rst the frequency property is checked  If so then an intersection of is performed and a closure denoted  is produced If the 
   002 002 
A Overview B Deìnitions Deìnition 4.1 Deìnition 4.2 Deìnition 4.3 C Map Phase split D Combiner Phase combiner function E Reduce Phase 
78 
79 


0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 
item that was added in the map step is lexicographically greater than the rst item in  then is a duplication and can be discarded Otherwise a new closed frequent itemset is discovered and is added  In other words if the test in line 7 passes then it is guaranteed the the same closure if found and kept is another reduce task the one that will get from its rst minimal generator in the lexicographical order A theorem is proved in Section IV-I Pseudo code of the reduce function is presented in Algorithm 4 In line 5 in the algorithm we perform the function which is actually an intersection of all the transactions in  Notice that we do not need to read all of and store in the RAM can be treated as a stream reading transactions one at a time and performing the intersaction Main Mine Closed Frequent Itemsets   user-given minimum support  the database of transaction  all closed frequent itemsets _     _    Mapper   The set of closed frequent itemsets with their generators found in the previous iteration  A single transaction from _  The set of all frequent items  Key potentially new generator Value transaction that contains the generator and its support    Combiner Key  a generator Values  a collection of tuples such that is a transaction and is its support  Key potentially new generator Value transaction that contains the generator and its support    Reducer Key  a generator Values  a collection of tuples such that is a transaction and is its support  The user-given minimum support  A closed frequent itemset if found       F Run Example Consider the example database in Table I with a minimum support of 2 transactions 0.4 in percentage To simulate a distributed setting we assume that each transaction resides on a different machine in the network mapper node denoted  We track node  Itês input is the transaction and since this is the rst iteration then  For each item in the input transaction we emit a message containing the item as a key and the transaction as a value So the messages that emits are the following     and  According to the MapReduce paradigm a reducer task is assigned to every key We follow the reducer tasks assigned for keys  and  denoted  
  0      1                   
003 002 002 
   002 002 002 002  
Algorithm 1 Input Output Algorithm 2 Input Output do 2 if then 3 do 5 if then 6 Input Output Algorithm 4 Input Output then 3 return then 8 return 1 st Map Phase 1 st Reduce Phase 
 T T T D 012 D 012  012 012 D D 004 002 012  004 004 012 013 014 012 012 013 014 012 012 012  D   013   014 013   014 013   014 013   014 013   014       
findFrequentItems MapReduceJob concat c.generator,item  emit    Applying the function on  i.e intersecting    emit    Applying the function on  i.e intersecting      Duplication elimination   emit c  
i i i i i j j i i n n i i n i i n n n n n i i n i i n n n i i i a c 
1 2 3 4 repeat 5 6 7 8 until 9 return 1 foreach 4 foreach 7 8 end 9 end 10 end 11 end Algorithm 3 1 2 3 1 2 if 4 end 5 6 7 if 9 end 10 
c g c C c c f minSup f items  minSup C 002 i i C  minSup C f items C 002 C C t f items c C c t t t c item t item f _ items g g t g t s   t s n t s sum s f t  t t  t t f t  t g t sum g t s   t s n t s minSup supp s supp  minSup f t   t t   t c f t   t c.generator g g.item  c g t m m t C C 002 m a  a c d e f c  a c d e f d  a c d e f e  a c d e f f  a c d e f a c f R R 
79 
80 


f a c f c ac n j j j j j j l l l l l l l l l l l l l n g 1 g 2 g m 
R R a t t t minSup a a C R c t t t t c f c c f c f C R f t t t t c f R f c f C a a  c f c  e e m c C t c c t c m a c  a c d e f a d  a c d e f a e  a c d e f a f  a c d e f c d  a c d e f c e  a c d e f c f  a c d e f e f  a c d e f R a c t t minSup a c a c d f a c d f p p g p sup p minSup minSup f f g c i  i c i c sup i sup c g i g c h i h c c h i c i c c c l g g l g g g g c g g g c l g c i i c l g 002 g i g g 002 g g g i g l g i g g 002 g c l 002 h g 002 c l c g g 002 g c c g c i i   i g i i   i  m<n h g c g 
G Soundness H Completeness I Duplication Elimination 
a a c f c e e a c d f a c a e a e c e f c e 
                        
2     2                                                        
  D                             004  013   014 013   014 013   014 013   014 013   014 013   014 013   014 013   014         T T 003 T  003 T T 010 004 015 016 004  017  020 020 016     
2 nd Map Phase 2 nd Reduce Phase 
Closed Item set Generator Support 3 4 4 2 2 3 TABLE II C OMPLETE SET OF CLOSED FREQUENT ITEMSETS IN THE EXAMPLE DATABASE FOR A MINIMUM SUPPORT OF 2 TRANSACTIONS  WITH THEIR GENERATORS AND SUPPORT  and 
1 2 4 1 1 3 4 5 1 1 3 4 5 1 1 1 1 1 4 1 1 2 
respectively First consider  According to the MapReduce paradigm this reduce task receives in addition to the key all the transactions in that contain that key  and  First we must test for frequency there are 3 transactions containing the key Since we pass the frequency test and can go on Next we intersect all the transactions producing the closure  The nal check is whether the closure is lexicographically larger than the generator In our case it is not because the generator and closure are equal so we add to  Next consider  This reduce task receives the key  and transactions   and  Since the number of messages is 4 we pass the frequency test The intersection of the transactions is the closure  Finally is lexicographically smaller than soweadd to  Finally consider  The transactions that contain the set are   and  We pass the frequency test but the intersection is  just like in reduce task sowehave a duplicate result However is lexicographically greater than  so this closure is discarded The nal set of all closed frequent itemsets found on the rst iteration is the itemset after the semicolon is the generator of this closure As before we follow node  This time the set of closed frequent itemsets is not empty so according to the algorithm we iterate over all  If the input transaction contains  we add to all the items in  each at a time and emit it So the messages that emits are the following         Consider reduce task  According to the MapReduce paradigm this reduce task receives all the message containing the key  which are transactions and  Since we pass the frequency test Next we consider the key as a generator and intersect all the transactions getting the closure  Final check is whether the added item c is lexicographically larger than the closure minus the generator In our case it is not so we add to the set of closed frequent itemsets The full set of closed frequent itemsets is shown in table II Next we prove the soundness and completeness of the algorithm The mapper phase makes sure that the input to the reducer is a key which is a subset of items  and a set of all transactions that contain  denoted  By deìnition  The reducer rst checks that by checking and then performs an intersection of all the transactions in  which by deìnition is the result of the function  and outputs the result So by deìnition all output is the result of  which is a closed frequent itemset We need to show that the algorithm outputs all the frequent closed itemsets Consider a closed frequent itemset that we are not sure if it was produced Suppose that has no proper subset that is a closed frequent itemset Therefore for all items  and  Therefore  Since then is a generator of  and the algorithm will output at the rst iteration Suppose that has one or more proper subsets each is a closed frequent itemset We examine the largest one and denote it  We also denote its generator  meaning that  Since is antitone and since then  What we show next is that if we add one of the items not in to we will generate  Consider an item  such that  Let  Therefore  Assume that  It implies that is a generator of a closed itemset that is a proper subset of in contradiction to being the largest closed subset of  therefore  meaning that will be found by the mapper by adding an item to see lines 3-4 in algorithm 2 002 As we saw in the run example in Section IV-F a closed itemset can have more than a one generator meaning that two different reduce tasks can produce the same closed itemset Furthermore these two reduce tasks can be in two different iterations We have to identify duplicate closed itemsets and eliminate them The naive way to eliminate duplications is by submitting another MapReduce job that sends all identical closed itemsets to the same reducer However this means we need another MapReduce job for that which greatly damages performance Line 7 in algorithm 4 takes care of that without the need for another MapReduce round In the run example we have already seen how it works when the duplication happens on the same round What we would like to show is that the duplication elimination step does not lose any closed itemset We now explain the method Consider itemset a closed frequent itemset and its generator  such that  According to our algorithm was created by 
80 
81 


j j j 
002\003 004\005\004 006\004\004\007 007\010\011\005 
A Data B Conìguration C The Experiments 
002\003\004\004\005\006\007\010\011\012\007\003\006\013\002\003\014\012\015\003\016\015\012\017\020\015\021\022\023\003\024\007\012\017\004\014\015\003\006\015\025\026\006\012\017\020\012\007\010\015\027\011\012\011 
012\013\014\013\015\016\015\017\020\016\021\021\022\023\024 025\022\015\015\016\014\013\026\027\024\013\022\014\017\025\022\030\024\017\031\013\014\017\015\013\032\032\013\022\014\030\033 011\034\011\011\004\010 011\034\011\011\004 011\034\011\011\006\010 011\034\011\011\006 011 010\011 006\011\011 006\010\011 004\011\011 004\010\011 035\011\011 035\010\011 003\011\011 036\032\037\022\023\013\024 \015  025\032\022\030"\024 036 
        
017    004 004 004 017  015 D 
Fig 1 Comparing our proposed algorithm with a MapReduce adaptations of Closet and AFOPT on the synthetic data The lines represent the communication cost of the three algorithms for each minimum support The bars present the number of closed frequent itemsets found for each minimum support 
002 002 002 
adding an item to a previously found closed itemset We denote that itemset  and the added item such that  Suppose that  In the context of the algorithm it means that would be eliminated We should show that can be produced from a different generator Consider the smallest item in  Since it is frequent and since then surely  meaning that the algorithm will add it to  creating  It is possible that  however if we keep growing with the smallest items we will eventually get  V E XPERIMENTS We have performed several experiments in order to verify the efìciency of our algorithm and to compare it with other renowned algorithms We tested our algorithm on both real and synthetic datasets The real dataset was downloaded from the FIMI repository and is called webdocs It contains close to 1.7 million transactions each transaction is a web document with 5.3 million distinct items each item is a word The maximal length of a transaction is about 71 thousand items The size of the dataset is 1.4 Gigabytes A detailed description of the webdocs dataset that also includes various statistics can be seen in The synthetic dataset was generated using the IBM data generator W e ha v e generated six million transactions with an average of ten items per transaction of a total of 100k items The total size of the input data is 600mb We run all the experiments on the Amazon Elastic MapReduce infrastructure each run w as e x ecuted on 16 machines each is an SSD-based instance storage for fast I/O performance with a quad core CPU and 15 GiB of memory All machines run Hadoop version 2.6.0 with Java 8 We used communication-cost see Section II-B as the main measurement for comparing the performance of the different algorithms the input records to each map task and reduce task were simply counted and summed up and the end of the execution This count is performed on each machine in a distributively manner The implementation of Hadoop provides an internal input records counter that makes the counting and summing task extremely easy Communication-cost is an infrastructure-free measurement meaning that it is not effected by weaker/stronger hardware or temporary network overloads making it our measurement of choice However we also measured the time of execution we run each experiment 3 times and give the average time We have implemented the following algorithms   a naive adaptation of Closet to MapReduce    the AFOPT-close adaptation to MapReduce and   our proposed algorithm All algorithms were implemented in Java 8 taking advantage of its new lambda expressions support We ran the algorithms on the two datasets with different minimum supports and measured the communication cost and execution time for each run The rst batch of runs was conducted on the synthetic dataset The results can be seen in gure 1 The lines represent the communication cost of the three algorithms for each minimum support The bars present the number of closed frequent itemsets found for each minimum support As can be seen our algorithm outperforms the others in terms of communication cost in all the minimum supports In the second batch of runs we run the implemented algorithms on the real dataset with four different minimum supports and measured the communication cost and execution time for each run The results can be seen in gures 2 and 3 The lines represents the different execution times for the different minimum supports As can be seen our algorithm outperforms the existing algorithms VI C ONCLUSION We have presented a new distributed and parallel algorithm for mining closed frequent itemsets using the popular MapReduce programming paradigm Besides its novelty using MapReduce makes this algorithm very easy to implement relieving the programmer from the wearing work of handling concurrency synchronization and nodes management that are part of a distributed environment and focus on the algorithm itself In addition as you recall from section IV-A one of the input parameters to each iteration of the algorithm is  the database This parameter is the dominant one in terms of size 
g g g k k k k k 
f i g f i i  c g c c i c g i c i  g i  f f g f i h g c g c i ii iii 
81 
82 


002\003\004\004\005\006\007\010\011\012\007\003\006\013\002\003\014\012\015\003\016\015\012\017\020\015\021\022\023\003\024\007\012\017\004\014\015\003\006\015\030\020\011\022\015\027\011\012\011 030\005\006\006\007\006\023\015\031\007\004\020\015\003\016\015\012\017\020\015\021\022\023\003\024\007\012\017\004\014\015\003\006\015\030\020\011\022\015\027\011\012\011 
Data Mining and Knowledge Discovery Knowledge and Information Systems Database Theory ICDT 99 Data Mining 2007 ICDM 2007 Seventh IEEE International Conference on Concurrency and Computation Practice and Experience Communications of the ACM Data Mining Workshops ICDMW 2012 IEEE 12th International Conference on FIMI Big Data 2013 IEEE International Conference on Proceedings of the 13th International Conference on Extending Database Technology FIMI 
004\006\010 010\007\011 010\011\004\007 006\010\010\011\005 
012\013\014\013\015\016\015\017\020\016\021\021\022\023\024 025\022\015\015\016\014\013\026\027\024\013\022\014\017\025\022\030\024\017\031\013\014\017\015\013\032\032\013\022\014\030\033 011\034\010 011\034\003 011\034\035 011\034\004 011 006\011\011\011 004\010\011\011 003\011\011\011 010\010\011\011 002\011\011\011 007\010\011\011 006\011\011\011\011 006\004\011\011\011 036\032\037\022\023\013\024 \015  025\032\022\030"\024 036 012\013\014\013\015\016\015\017\020\016\021\021\022\023\024 016\014\014\013\014\037\017'\013\015"\017\031\013\014\017\015\013\014\016\024"\030\033 011\034\010 011\034\003 011\034\035 011\034\004 011 010 006\011 006\010 004\011 004\010 035\011 035\010 003\011 003\010 036\032\037\022\023\013\024 \015  025\032\022\030"\024 036 
Fig 2 Comparing our proposed algorithm with a MapReduce adaptations of Closet and AFOPT on the real data The lines represent the communication cost of the three algorithms for each minimum support The bars present the number of closed frequent itemsets found for each minimum support Fig 3 Comparing the execution time of our proposed algorithm with a MapReduce adaptations of Closet and AFOPT on the real data Since this input is static does not change from one iteration to the next we might use some caching mechanism further increasing the efìciency R EFERENCES  J Han H Cheng D Xin and X Y an Frequent pattern mining current status and future directions  vol 15 no 1 pp 55Ö86 2007  J Cheng Y  K e and W  Ng  A surv e y on algorithms for mining frequent itemsets over data streams  vol 16 no 1 pp 1Ö27 2008  N P asquier  Y  Bastide R T aouil and L Lakhal Disco v ering frequent closed itemsets for association rules in  Springer 1999 pp 398Ö416  C Lucchese S Orlando and R Pere go P arallel mining of frequent closed patterns Harnessing modern computer architectures in  IEEE 2007 pp 242Ö251  C Lucchese C Mastroianni S Orlando and D T alia Mining home toward a public-resource computing framework for distributed data mining  vol 22 no 5 pp 658Ö682 2010  J Dean and S Ghema w at Mapreduce Simpliìed data processing on large clusters  vol 51 no 1 pp 107Ö113 2008  T  A S F oundation Hadoop  http://hadoop.apache.or g  S.-Q W ang Y B Y ang G.-P  Chen Y  Gao and Y  Zhang Mapreducebased closed frequent itemset mining with efìcient redundancy ltering in  IEEE 2012 pp 449Ö453  G Liu H Lu J X Y u W  W ang and X Xiao  Afopt An ef cient implementation of pattern growth approach in  2003  S Moens E Aksehirli and B Goethals Frequent itemset mining for big data in  IEEE 2013 pp 111Ö118  F  Afrati and J Ullman Optimizing joins in a map-reduce en vironment in  ACM 2010 pp 99Ö110  Frequent itemset mining dataset repository   http://ìmi.ua.ac.be/data  C Lucchese S Orlando R Pere go and F  Silv estri W ebdocs a reallife huge transactional dataset in  vol 126 2004  IBM Ibm dataset generator   http://sourcefor ge.net/projects ibmquestdatagen  Amazon Elastic mapreduce emr  https://a ws.amazon.com elasticmapreduce 
82 
83 


0.80 0.76 0.72 0.68 0.64 0.60 0.56 0.52 0.48 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0   Apriori-AP counting speedup Apriori-AP overall speedup  Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time 
0.1 1 10 100 3000 2000 1000 0 1000 2000 3000 4000 5000 
0.6 0.5 0.4 0.3 0.2 0.1 0.0 10 100 
1 10 100 6000 4000 2000 0 2000 4000 6000 8000 10000 
0.20 0.19 0.18 0.17 0.16 0.15 0.14 0.13 0.12 0.11 0.10 0.09 0.08 0.07 0.06 0.05 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0 Apriori-AP counting speedup Apriori-AP overall speedup  Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time  0 10000 5000 15000 c Webdocs Figure 5 The performance results of Apriori-AP on three real-world benchmarks DP time SR time and CPU time represent the data process time on AP symbol replacement time on AP and CPU time respectively Webdocs switches to 16-bit encoding when relative minimum support is less then 0.1 8-bit encoding is applied in other cases 
E vs Eclat Eclat et al Eclat Eclat 
0.80 0.75 0.70 0.65 0.60 0.55 0 100 200 300 400 Computation Time \(s Relative minimum support 1.0X Symbol replacement time 0.5X Symbol replacement time 0.1X Symbol replacement time Figure 7 The impact of symbol replacement time on Apriori-AP performance for Pumsb how symbol replacement time affects the total AprioriAP computation time A reduction of 90 in the symbol replacement time leads to 2.3X-3.4X speedups of the total computation time The reduction of symbol replacement latency will not affect the performance behavior of AprioriAP for large datasets since data processing dominates the total computation time 
Apriori Eclat Equivalent Class Clustering   is another algorithm based on Downward-closure uses a vertical representation of transactions and depth-ìrst-search strategy to minimize memory usage Zhang  proposed a h ybrid depth-ìrst/breadth-ìrst search scheme to expose more parallelism for both multi-thread and GPU versions of  However the trade-off between parallelism and memory usage still exists For large datasets the nite memory main or GPU global memory will become a limiting factor for performance and for very large datasets the algorithm fails While there is a parameter which can tune the trade-off between parallelism and memory occupancy we simply use the default setting of this parameter for better performance Figure 8 shows the speedups that the sequential 
0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05 CPU Apriori Computation Time \(s Speedup DP time SR time CPU time  Relative Minimum Support Time Breakdown 100 50 0 AP counting speedup Apriori-AP overall speedup    Apriori-CPU counting time Apriori-CPU overall time Apriori-AP counting time Apriori-AP overall time  b Accidents 
T40D500K Counting T40D500K Overall T100D20M Counting T100D20M Overall Webdocs5X Counting Webdocs5X Overall Speedup Relative Minimum Support Figure 6 The speedup of Apriori-AP over Apriori-CPU on three synthetic benchmarks 
1 10 100 
a Pumsb 
696 


0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.1 1 10 Pumsb Accidents T40D500K Webdocs Webdocs5X T100D20M ENWiki Speedup Relative Minimum Support 
Figure 8 The performance comparison of CPU sequential and algorithm achieved with respect to sequential Apriori-CPU Though has 8X performance advantage in average cases the vertical bitset representation become less efìcient for sparse and large dataset high trans and freq item ratio This situation becomes worse as the support number decreases The Apriori-CPU implementation usually achieves worse performance than  though the performance boost of counting operation makes Apriori-AP a competitive solution to parallelized  Three factors make a poor t for the AP though it has better performance on CPU 1 requires bit-level operations but the AP works on byte-level symbols 2 generates new vertical representations of transactions for each new itemset candidate while dynamically changing the values in the input stream is not efìcient using the AP 3 Even the hybrid search strategy cannot expose enough parallelism to make full use of the AP chips Figure 9 and 10 show the performance comparison between Apriori-AP 45nm for current generation of AP and sequential multi-core and GPU versions of  Generally Apriori-AP shows better performance than sequential and multi-core versions of  The GPU version of shows better performance in Pumsb Accidents and Webdocs when the minimum support number is small However because of the constraint of GPU global memory Eclat-1G fails at small support numbers for three large datasets ENWiki T100D20M and Webdocs5X ENWiki as a typical sparse dataset causes inefìcient storage of bitset representation in Eclat and leads to early failure of EclatGPU and up to 49X speedup of Apriori-AP over Eclat-6C In other benchmarks Apriori-AP shows up to 7.5X speedup over Eclat-6C and 3.6X speedup over Eclat-1G This gure also indicates that the performance advantage of AprioriAP over GPU/multi-core increases as the size of the dataset grows The AP D480 chip is based on 45nm technology while the Intel CPU Xeon E5-1650 and Nvidia Kepler K20C on which we test are based on 32nm and 28nm technologies respectively To compare the different architectures in the same semiconductor technology mode we show the performance of technology projections on 32nm and 28nm technologies in Figure 9 and 10 assuming linear scaling for clock frequency and square scaling for capacity The technology normalized performance of Apriori-AP shows better performance than multi-core and GPU versions of Eclat in almost all of the ranges of support that we investigated for all datasets with the exception of small support for Pumsb and T100D20M Apriori-AP achieves up to 112X speedup over Eclat-6C and 6.3X speedup over Eclat-1G The above results indicate that the size of the dataset could be a limiting factor for the parallel algorithms By varying the number of transactions but keeping other parameters xed we studied the behavior of Apriori-AP and Eclat as the size of the dataset increases Figure 11 For T100 the datasets with different sizes are obtained by the IBM synthetic data generator For Webdocs the different data sizes are obtained by randomly sampling the transactions or by concatenating duplicates of the whole dataset In the tested cases the GPU version of fails in the range from 2GB to 4GB because of the nite GPU global memory Comparing the results using different support numbers on the same dataset it is apparent that the smaller support number causes Eclat-1G to fail at a smaller dataset This failure is caused by the fact that the ARM with a smaller support will keep more items and transactions in the data preprocessing stage While not shown in this gure it is reasonable to predict that the multicore implementation would fail when the available physical memory is exhausted However Apriori-AP will still work well on much larger datasets assuming the data is streamed in from the hard drive assuming the hard drive bandwidth is not a bottleneck VII C ONCLUSIONS AND THE F UTURE W ORK We present a hardware-accelerated ARM solution using Micronês new AP architecture Our proposed solution includes a novel automaton design for matching and counting frequent itemsets for ARM The multiple-entry NFA based design was proposed to handle variable-size itemsets MENFA-VSI and avoid routing reconìguration The whole design makes full usage of the massive parallelism of the AP and can match and count up to itemsets in parallel on an AP D480 48-core board When compared with the based single-core CPU implementation the proposed solution shows up to 129X speedup in our experimental 
Apriori Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat Eclat F Normalizing for technology Eclat G Data size Eclat Eclat Eclat Apriori 
18 432 
 
697 


0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 1 10 100 1000 10000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm a Webdocs\(1.4GB 0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 10 100 1000 10000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails d Webdocs5X 7.1GB Figure 10 Performance comparison among Apriori-AP Eclat-1C Eclat-6C and Eclat-1G with technology normalization on four large datasets 
0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(second Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm b Accidents 34MB 0.40 0.36 0.32 0.28 0.24 0.20 0.16 0.12 0.08 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm c T40D500K\(49MB Figure 9 Performance comparison among Apriori-AP Eclat1C Eclat-6C and Eclat-1G with technology normalization on three small datasets results on seven real-world and synthetic datasets This APaccelerated solution also outperforms the multicore-based and GPU-based implementations of 0.60 0.55 0.50 0.45 0.40 0.35 0.30 0.25 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails c T100D20M 6.3GB 
ARM a more efìcient algorithm with up to 49X speedups especially on large datasets When performing technology projections on future generations of the AP our results suggest even better speedups relative to the equivalent-generation of CPUs and GPUs Furthermore by varying the size of the datasets from small to very large our results demonstrate the memory constraint of parallel ARM particularly for GPU 
Eclat Eclat 
0.80 0.76 0.72 0.68 0.64 0.60 0.56 0.52 0.1 1 10 100 1000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(s Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm 
0.20 0.15 0.10 0.05 0.00 1 10 100 1000 10000 100000 Eclat-1C \(32nm Eclat-6C \(32nm Eclat-1G \(28nm Computation Time \(second Relative Minimum Support Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Eclat-GPU fails b ENWiki\(3.0GB 
a Pumsb 16MB 
698 


 Frequent pattern mining Current status and future directions  2007  R Agra w al and R Srikant F ast algorithms for mining association rules in  1994  M J Zaki Scalable algorithms for association mining  vol 12 no 3 pp 372Ö390 2000  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation in  2000  Y  Zhang F  Zhang and J Bak os Frequent itemset mining on large-scale shared memory machines in  2011  F  Zhang Y  Zhang and J D Bak os  Accelerating frequent itemset mining on graphics processing units  vol 66 no 1 pp 94Ö117 2013  Y  Zhang  An fpga-based accelerator for frequent itemset mining  vol 6 no 1 pp 2:1Ö2:17 May 2013  P  Dlugosch  An efìcient and scalable semiconductor architecture for parallel automata processing  vol 25 no 12 2014  I Ro y and S Aluru Finding motifs in biological sequences using the micron automata processor in  2014  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases in  1993  H No yes Micronês automata processor architecture Reconìgurable and massively parallel automata processing in  June 2014 keynote presentation  M J Zaki  Parallel data mining for association rules on shared-memory multi-processors in  1996  L Liu  Optimization of frequent itemset mining on multiple-core processor in  2007  I Pramudiono and M Kitsure ga w a P arallel fp-gro wth on pc cluster in  2003  E Ansari  Distributed frequent itemset mining using trie data structure  vol 35 no 3 p 377 2008  W  F ang  Frequent itemset mining on graphics processors in  2009  B Goethals Surv e y on frequent pattern mining  Univ of Helsinki Tech Rep 2003  C Bor gelt Ef cient implementations of apriori and eclat in  2003 p 90  Frequent itemset mining dataset repository   http mi.ua.ac.be/data  J Rabae y  A Chandrakasan and B Nik oli  c  2nd ed Pearson Education 2003 
100 1000 10000 5 50 500 5000 re_sup = 0.12 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails for re_sup = 0.08 re_sup = 0.08 
GPU fails re_sup = 0.42 re_sup = 0.42 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm  Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails re_sup = 0.3 re_sup = 0.3 b T100 Figure 11 Performance prediction with technology normalization as a function of input size implementation In contrast the capability of our AP ARM solution scales nicely with the data size since the AP was designed for processing streaming data With the challenge of the big data era a number of other complex pattern mining tasks such as frequent sequential pattern mining and frequent episode mining have attracted great interests in both academia and industry We plan to extend the proposed CPU-AP infrastructure and automaton designs to address more complex pattern-mining problems A CKNOWLEDGMENT This work was supported in part by the Virginia CIT CRCF program under grant no MF14S-021-IT by C-FAR one of the six SRC STARnet Centers sponsored by MARCO and DARPA NSF grant EF-1124931 and a grant from Micron Technology R EFERENCES  J Han 
et al Data Min Knowl Discov Proc of VLDB 94 IEEE Trans on Knowl and Data Eng Proc of SIGMOD 00 Proc of CLUSTER 11 J Supercomput et al ACM Trans Reconìgurable Technol Syst et al IEEE TPDS Proc of IPDPSê14 Proc of SIGMOD 93 Proc of Fifth International Symposium on Highly-Efìcient Accelerators and Reconìgurable Technologies et al Proc of Supercomputing 96 et al Proc of VLDB 07 Proc of PAKDD 03 et al IAENG Intl J Comp Sci et al Proc of DaMoN 09 Proc FIMI 03 Digital Integrated Circuits 
1 10 100 1000 10000 0.1 1 10 100 1000 
a Webdocs 
699 


