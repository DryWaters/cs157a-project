Maintaining Implicated Statistics in Constrained Environments  Yannis Sismanis  I.B.M Almaden Research Center syannis@us.ibm.com Nick Roussopoulos University of Maryland nick@cs.umd.edu Abstract Aggregated information regarding implicated entities is critical for online applications like network management trafìc characterization or identifying patters of resource consumption Recently there has been a urry of research for online aggregation on streams like quantiles hot items hierarchical heavy hitters but surprizingly the problem of summarizing implicated information in stream data has received no attention As an example consider an IP-network and the implication source  destination  Flash crowds such as those that follow recent sport events like the olympics or seek information regarding 
catastrophic events or denial of service attacks direct a large volume of trafìc from a huge number of sources to a very small number of destinations In this paper we present novel randomized algorithms for monitoring such implications with constraints in both memory and processing power for environments like network routers Our experiments demonstrate several factors of improvements over straightforward approaches 1 Introduction Keeping track of interesting data trends by evaluating various relations between values of different attributes is the focus of a lot of research For example decision support systems are trying to evaluate efcient ways of accomplishing that in an ofine fashion The problem is computationally challenging even for such ofine algorithms but is exacerbated for the case of data streams with high throughputs that are encountered in constrained streaming environments However environments like communication and sensor net 
works security and monitoring applications need accurate and up-to-date statistics in real-time in order to trigger certain actions The class of Distinct Count  statistics are v ery useful for such applications since it provides at any moment the distinct number of values or species in a F o r e xample a typical statistic for a network router is to maintain the distinct number of sources and destinations or even source,destination pairs that the router handles The distinct count problem has been extensively studied in the database literature\(see for a surv e y  a nd has found applications in other areas like selecting a good query plan in query   This material is based upon work supported by or in part by the U.S Army Research Laboratory and the U.S Army Research Ofce under contract/grant number DAAD19-01-1-0494 Prepared through collaborative participation in the Communications and Networks Consortium sponsored by the U S Army Research Laboratory under the Collaborative Technology Alliance Program Coop 
erative Agreement DAAD19-01-2-0011 The U S Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation thereon  Work performed while the author was with University of Maryland Source Destination Service Time S1 D2 WWW Morning S2 D1 FTP Morning S1 D3 WWW Morning 
S2 D1 P2P Noon S1 D3 P2P Afternoon S1 D3 WWW Afternoon S1 D3 P2P Afternoon S3 D3 P2P Night Table 1 Example network trafﬁc data In this paper we focus on the problem of maintaining distinct implication statistics in constrained environments even un 
der the presence of noise Some items imply other items in the sense that they either always or a given percentage of the time appear together We use the term implication to denote such properties between sets of attributes and the term implication count to refer to the number of items that exhibit such implication properties The statistics we collect not only generalize the distinct value statistics we mentioned but also aggregate and complement information gathered from data-mining techniques such as association rules or frequent Such techniques return the set of frequently encountered associated itemsets while the implication statistics we consider here return aggregated information counts averages ratios without returning the involved itemsets In constrained environments like sensor networks where aggregation is important for bandwidth conservation and energy consumption frequent itemsets and association rules techniques cannot be extended in order to provide real-time aggregates and error guar 
antees for the implicated queries we are considering The same stands for the class of heavy which identies the set of objects whose frequency of appearance is above a given threshold The cumulative effect of many objects whose frequency of appearance is less than the given threshold may overwhelm the implication statistics although these objects are not identied To help clarify the meaning and the extend of the statistics that we address consider a simplistic data stream called Network Traffic  a window of which is presented in Table 1 The stream is comprised of the attributes Source Destination Service and Time and is obtained by the trafc a router observes Table 2 contain examples of real-time statistics which are essential for monitoring purposes and that our framework supports A security administrator would like to maintain in real-time the statistic  how many destinations are contacted by just a single source  in order to identify possible intrusion attempts We 
consider the Destinations with the implication property Destination  Source In our example we have that  D 2  S 1  and  D 1  S 2  have the implication property D 2 appears only with S 1 and D 1 only with S 2 and therefore the returned implication count is two Furthermore one might want to consider destinations that 80 of the time are contacted by one single source In that case D3 qualies and the returned count is three Another similar query for this specic dataset is  how many Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


 Example Class How many sources have we seen so far Distinct Count How many destinations are contacted by only one source Implication one-to-one How many sources contact more than ten destinations Implication one-to-many How many destinations are contacted by only one source 80 of the time Implication one-to-one with noise How many sources do not use only the WEB service Complement Implication How many sources contact only one destination during the morning Conditional Implication How many sources contact only one target per service Compound Implication Average number of destinations that 90 of the time are contacted from more than ten sources for the P2P service over a sliding window of 1h Complex Implication Table 2 Classiﬁcation of Example Implication Queries services are being requested from only one source  The returned aggregate in our case is again two because the corresponding implications are  WWW  S 1    FTP  S 2   A small set of all possible implication statistics that someone can keep track for our toy data set in Table 1 is classied in Table 2 based on the denition of the implication on each instance The apparent wide range of implication statistics is the motivation behind this paper In our example such statistics help to keep tracking in real-time various trafc parameters More sophisticated statistics address either conditional or involving oneto-many implications A security-expert may want to keep track of the answer to the following questions  How many sources connect to only one destination during the morning  and  How many destinations are connected from more than ten sources for the P2P service 90 of the time  Similar aggregate queries are very important to users of decision support systems and data warehouses where we assume that all the data can be stored and aggregated and that a lot of computation can be performed ofine with bulk updates during down time Even in this case however the problem of maintaining distinct implication statistics is complicated and requires too many computational and storage resources Although we concentrate on data streams our methods can be applied to ofine query scenarios since our algorithm does not require repeated rescans over the entire database It can run with input the incremental updates to maintain the implication counts as it does for a data stream The complication behind maintaining implication statistics is partially common with the problem of Distinct Value queries where there is a huge number of duplicates that cannot be accommodated by either available memory or processing power In this paper we describe a framework that can be used to estimate a rich variety of distinct implication statistics in the context of streaming environments under constraints on storage and processing power as for example in the case of communication routers or sensor networks In addition our techniques can be applied directly to decision support systems and data warehouses enriching the collection of aggregates that an analyst can use For a data stream that is logically divided into two sets of attributes A and B  we calculate implication statistics of itemsets a i of A that imply some itemsets of B  The problem is more difcult than identifying frequent itemsets in a data stream because the contribution of a large number of infrequent implicated itemsets can be very signicant overwhelming the aggregate count We actually show that current streaming algorithms for frequent cannot be extended and provide error guarantees in the aggregate statistics The context of the environments we are considering forces the following assumptions  There is not enough memory to accommodate the cardinalities of the attributes participating in the query For example consider the case where one attribute is the network address of a client which in IPv6 has an address space of O  2 128    Although some itemsets may not appear frequently enough and therefore may pass undetected by some technique they can seriously affect the total count This is the case for rst hop in distrib uted denial of service attacks where the counts are very small at the rst hop but signicantly contributing to the cumulative effect on the last hop routers  The exact meaning of the implication depends on the nature of the application In most situations a analyst will need to allow some tolerance to avoid noise in the data The work in this paper concentrates on how such implication counts can be accurately estimated in the context described above by using a small amount of memory that holds a summary data structure which makes possible the estimation of the answer The key issue is that the data structure can be kept up to date with a small amount of effort Our technique is based on using certain properties of hash functions We also investigate the error bounds of the estimation and how one can improve those bounds The main contributions of our work are summarized as follows 1 We describe a generalization of implication aggregate queries that frequently arise in the data stream model of data processing and also in other elds of database research 2 We provide memory and processing efcient algorithms for estimating such aggregates within small error bounds typically less than 10 relative error 3 We prove that the complement problem of estimating nonimplication counts can be      approximated under most conditions 4 We extend online that estimate frequent itemsets and prove that they cannot applied directly to the problem of estimating implication counts Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


5 We demonstrate the accuracy of our methods through an extensive set of experiments on both synthetic and real datasets In Section 2 we review applications that can benet from our approach In Section 3 we describe formally and in detail the queries our algorithms can estimate Section 4 explains how one can estimate answers to such queries with bounded error estimation and also presents the memory and processing requirements Section 5 describes why existing algorithms that identify frequent itemsets over a given support cannot be applied directly to the problem of estimating implication counts Section 6 evaluates experimentally the accuracy of the proposed algorithms with both synthetic and real data sets The related work is presented in section 7 and the conclusions are in section 8 2 Applications The following applications can benet from such implication statistics We briey describe them and how the statistics can be applied to solve important problems in them Network trafﬁc monitoring and characterization Accurately measuring aggregate network trafc from one router to another is essential not only for monitoring purposes but also for trafc control rerouting accounting pricing based on usage  detecting denial of service attacks Certain char acteristics of the trafc like router bottlenecks or patterns of resource or e v en ash cro wds[20 and denial of service attacks can be modeled as implication queries One can associate triggers when such implication counts exceed certain thresholds and could for example reroute trafc Approximate Dependencies Functional dependencies have a very strict meaning They leave no room for exception On the other hand association rules are essentially probabilistic and allow room for exceptions which is typical in large databases Approximate attempt to bridge the gap b y dening functional dependencies that almost hold Such approximate dependencies can be validated during updates or on a data-stream by conditions on the aggregate implication counts Multi-dimensional histograms or models A fundamental problem in scenarios like query optimization or query approximation is creating an accurate and compact representation for a multi-dimensional dataset Typical models used are histograms or probabilistic graph or other original approaches[6 In a methodology is proposed where the independence assumption between attributes is waived The histogram synopsis is broken into one model that captures signicant correlation and independence patterns in data and a collection of low-dimensional histograms Estimations of implication counts can be used in a preprocessing step to provide information about signicant dependent or independent areas among certain attributes These counts can then be used to more efciently and accurately construct the model part of the synopsis 3 Problem Deﬁnition In this section we formally describe the problem and the notation We assume that a node in a distributed environment receives a stream of data and wants to maintain a series of statistics about various implicated attributes More specically we are interested in approximating the answer to the general query written in SQLlike format 1  select count distinct A from R where A implies B where R is a relation that models a data stream A  B and C are sets of attributes\(dimensions of the relation R  We assume without loss of generalitythat A  B   0  In order to fully dene the meaning of the predicate implies  we rst introduce the notion of an itemset and then proceed by dening certain implications between itemsets Then we proceed by dening the aggregation of such implications over a distributed environment 3.1 Itemsets and deﬁnitions The projection of a single tuple  of R on the attributes of A is dened as an itemset a  and we denote a    A   For example in the case of the data set in Table 1 if A   Source,Destination  the itemset of the rst tuple is  S1,D2   At any given moment the number of tuples seen so far is denoted by T  The compound cardinality  A  of the set of attributes A is the product of the cardinalities of the attributes of A  In our example the compound cardinality is  A   3  3  9 because there are three different sources and three different destinations For an itemset a of A and b of B  we denote as  a  b  their association and we dene the following Implication Set   a  B   An itemset a of A may appear with more than one itemsets of B  We dene as implication set of an itemset a of A w.r.t B  the set of different itemsets of B it appears with   a  B   b i      A  a      B  b i     R  The cardinality    a  B   of the implication set is called multiplicity of a w.r.t to B  For example the itemset a   S1,D3  of A   Source,Destination  has a multiplicity with B   Service      a  B    2 since it appears with two different services WWW and P2P Support   a   An itemset a of A is said to have support  when it appears at  tuples out of T  For example itemset a   S1,D3  of A   Source,Destination  has a support of four since it appears in four tuples In the literature for example association rules the minimum support is expressed in terms of a ratio over all the tuples In the case of streams which are potentially unbounded in size we chose to dene it in terms of an absolute number of tuples Additionally the relative minimum support has some interesting side-effects that are discussed in Section 5.1.1 Conﬁdence Level   a  b   An itemset a of A and an itemset b of B have a condence level     a  b     a   where   a  b  is the number of tuples where itemsets a and b appear together For example the itemsets a   S1,D3  and b   WWW  of A   Source,Destination  and B   Service  respectively have an condence level   a  b  2  4 since itemsets a and b appear together in two tuples over the four tuples of itemset a  Top-Conﬁdence Level  c  a  B   c    a  B    Assume the sequence of all the condence levels of a  i.e the sequence of  i    a  b i   b i    a  B   We dene as  c  a  1 We are not trying to extend SQL but rather use it to describe the class of queries we are addressing Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


  top c    1   2     k  a  B      where the top c  X  operator returns the c biggest items in sequence X  This metric can be used to keep track of approximate one-to-c implications where one itemset a of A appears with at-most c different itemsets of B in  c  a  B  percent of the tuples where a appears For example for the itemset a   P2P  of A   Service  and B   Source  the condence levels with sources  S1,S2,S3  it appears with are  2  4  1  4  1  4   The top-condence level for c  2 is  2  a  B  2  4  1  4  75 This means that P2P appears with at most c  2 sources in 75 of all the tuples where P2P appears The top-condence level with c  3 in this specic case is 100 i.e service P2P appears with at most three different sources in all the tuples of P2P Similarly the top-condence level with c  1is 50 i.e in half the tuples P2P appears with only one source 3.1.1 Implications between itemsets and attributes An implication of an itemset a of A to B  denoted by a  B  holds for a given maximum multiplicity K  a given minimum support  and a given minimum top-condence level  c  when all of the following implication conditions are met 1 Maximum Multiplicity K     a  B   K 2 Minimum Support     a  012  3 Minimum top-Conﬁdence Level  c  c  a  B  012  c The cardinality S of the set of itemsets a i of A such that a i  B is the implication count for the general query of Section 3 When an itemset a i satises implications conditions 3 then we say that the itemset contributes or participates in the implication count Note that by denition we are interested in counting itemsets that satisfy the implication conditions throughout the life of the stream we lift this restriction using sliding windows and incremental counts in Section 3.2 This has a direct effect on how the condence level is interpreted When an itemset satises the minimum support and maximum multiplicity but does not satisfy the minimum top-condence level then we immediately discard that itemset from the implication count It is possible that later in the stream the same itemset may satisfy the top-condence level condition However since the itemset at least once did not satisfy all the implication conditions then by denition we do not count its contribution to the implication count 3.1.2 Example The above parameters describe a very exible framework for ltering out noise and dening one-to-many implications Consider the network trafc data set described in Table 1 and assume that an analyst is interested in identifying how many services are being used at most two different sources 80 of the time The user may also want to consider all services even if they appear for just one tuple but does not want to consider services that are being used at more than ve sources The corresponding implication conditions are Maximum multiplicity is set to ve a service that is being used by more than ve different sources does not contribute in the returned count Minimum Support is set to one meaning that we take into account services even if they appear in just one tuple of the dataset Top-Conﬁdence Level is set to 80 for c  2 That means that a service P that contributes in the implication count appears with at-most c  2 different sources in at-least 80 of all the tuples where P appears Letís go over all services in Table 1 to see how the above parameters affect the returned count Service WWW appears in two tuples with only S1 and therefore participates in the count FTP appears in only one tuple with S2 and also participates P2P appears in four tuples with three different stores The condence level of  P 2 P  S 1  is 2  4 of  P 2 P  S 2  is 1  4 and of  P 2 P  S 3  is 1  4 Therefore the top-condence level for c  2 is 2  4  1  4  75 and service P2P doesnít satisfy the minimum top-condence level condition The returned count is two for services WWW and FTP The minimum top-condence level corresponds to the fact that the user needs to consider only the services that appear with at most c  2 different stores The value of 80 corresponds to the gravity of this constraint If we change to minimum topcondence level to 75 then P2P is valid and participates in the count The minimum support is used to lter out implications that hold for a very small fraction of the data set For example if the user increases the minimum support to two tuples then the pair  FTP  S 2  is not valid since it appears in only one tuple 3.2 Incremental and Sliding Queries t 1      start ic t 2 stream Figure 1 Incremental maintenance Our framework provides for implication counts given a reference point in the stream where the counting begins and the implication conditions must hold w.r.t to that reference point We relax that constraint by using two techniques The incremental technique can answer queries like How many new sources with some given implication conditions have appears in the last 1h  The sliding window technique generalizes the incremental technique and provides the support for more general aggregates like moving averages In this section we briey describe the ideas behind the techniques due to the paper size constraintsand we refer the reader to the full version of the paper In Figure 1 we demonstrate a count ic at two points t 1 and t 2  In many cases the user is interested in the incremental implication count which is the distinct count of new itemsets that appeared and satisfy the implication conditions t 1   start ic   stream  t 2 start ic stop ic Figure 2 Sliding Windows Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


between t 1 and t 2  This can be derived by ic  t 2   ic  t 1   On the other hand sliding queries where we want to retire old implication counts or compare implication counts with different origins can by supported by maintaining a vector of implications counts with different origins and appropriately retiring old ones as depicted in Figure 2 4 Algorithm for Implication Counts In this section we describe an algorithm that can be used to efciently estimate implication counts The algorithm uses selective sampling driven by hashing techniques and is based on ideas that are used to estimate the count of distinct elements on a stream or relation using limited memory and processing per data item We evaluate analytically the accuracy and describe techniques that can be used to increase the accuracy Finally we describe the complexity requirements of the algorithm both for total space required and time per data item 4.1 Counting Distinct Elements The presence of duplicates in the data can traditionally be handled using sorting sampling or using hash tables Sorting and using hash tables do not scale well under both memory and time constraints as those encountered in a streaming environment Sampling appears as an attractive alternative mechanism Taking a simple random sample and then extrapolating the answer however may introduce an arbitrarily large error or require indexed access to the The alternati v e is to approximate the answer using properties of hash functions 14 26 2 5 4.1.1 Basic Probabilistic Counting One can probabilistically estimate[14 the number of distinct itemsets in a large collection of data denoted as zeroth-frequency moment F 0  in a single pass using only a small additional storage of space complexity O  log  A    where  A  is the compound cardinality of A  The basic counting procedure assumes that we have a hash function that maps itemsets into integers uniformly distributed over the set of binary strings of length L  The function p  y  represents the position of the least signicant 1-bit in the binary representation of y  For each itemset a i that appears in the stream we keep track of the maximum p i  p  hash  a i   Letís consider an initially empty bitmap and dene that an itemset a i is hashed in position p i of the bitmap By assigning the value one to the corresponding bit in the bitmap the maximum p i can then be determined by the position of the most-signicant one bit in the bitmap If the number of distinct elements in M is F 0  then least signicant bit is accessed approximately F 0  2 times approximately F 0  4 times etc This leads to Lemma 1 Lemma 1 The expected number of values that hash in the cell i of the bitmap is F 0  2 i  1  where i  0 corresponds to the least signiìcant bit of the bitmap At any given moment bm i  will almost certainly be zero if i  log F 0 and one if i  log F 0 with a fringe of zeros and ones for i 015 log F 0  The position R of the leftmost zero value in the bitmap is an estimator of log F 0 with expected value E  R  015 log  F 0  14 4.2 Counting Implications The basic probabilistic counting procedure can be extended in a straightforward but inapplicable manner in order to count implications The idea is that the basic procedure can be thought of as recording events  When we are counting distinct elements the recorded event is the existence of an itemset that hashes in a cell of the bitmap and it is recorded by assigning one to the value of that cell Note that we only record events and never erase them When counting implications the recording event is the existence of an itemset that satises the implication conditions Whenever we discover such an itemset we must assign the value of one to the corresponding cell The problem is that we donít know if an itemset will keep on satisfying the implication conditions in the future However we can postpone the assignment of one to a cell for the time when the user requests the implication count We extend the cells of the bitmap so that we can store itemsets in them When an itemset a i hashes in a cell we keep track of all the itemsets of B it appears with postponing the assignment of one or zero to the corresponding cell When the user asks for the count of itemsets a i of A with the property a i  B  we check each cell to see if there is at least one a i such that a i  B and we assign a value of one to the corresponding cells Then as described in Section 4.1.1the position of the leftmost zero is an estimator for the implication count One obvious optimization is that whenever we can determine that some itemset a i   B we can remove it from the cell However the memory requirements of this algorithm is still O  K  A    where K is the maximum multiplicity since we must keep track of every single itemset a i and the K different itemsets of B it appears with 4.3 Counting non-implications Assume that instead of counting the itemsets a i  a i  B we consider the complement problem of estimating the count of itemsets  a i  a i   B  Letís call this problem Non-implication Counting  More specically an itemset a i has the property  a i   B with respect to the implication conditions in Section 3.1.1 when it satises the minimum support requirement but does not satisfy the maximum multiplicity or the minimum top-condence level In this section we describe how we can bound the required memory and still get an estimate of the non-implication count The recording event is the existence of an itemset  a i  a i   B  Unlike the case when counting implications  we can now assign the value of one to a cell as soon as we discover such an itemset Once an itemset does not satisfy the implication conditions we know that it will never satisfy them in the future Below we dene the fringe zone and we show that for all non-implication counts except for very small countsthe size of the fringe zone is quite small We can observe in Figure 3 that the general format of the bitmap while performing the probabilistic counting has three zones Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


 b  a ij      11  00 Zone0 Fringe Zone Zone1  bm[0 p\(hash\(a i Figure 3 Fringe Zone The bitmap consists of cells where the leftmost cell  is the least signicant one Zone-1 is al w ays lled with ones because we found an itemset  a i  a i   B  while Zone-0 is always lled with zeros because the corresponding cells are empty Note that these are the only two cases where we can assign a value of one or zero to a cell The fringe zone lies between Zone-1 and Zone-0 In its boundaries at least we cannot determine the existence of an itemset  a i  i.e all itemsets in the zone so far imply B and therefore we must keep track of all the itemsets a i and the corresponding itemsets of B until we determine there is at least one  a i  4.3.1 Size of the Fringe Zone In order to calculate how big the fringe zone is lets assume that we are interested in estimating the non-implication count  a i   B for some set of attributes A and B  Let F 0  A  be the number of distinct elements of A and let  S be the non-implication count The size in cells of the fringe zone is quantied with very high probability by the following lemma Lemma 2 The size F of the fringe zone is F   log q where q   S F 0  A   Proof This is a direct effect of the way the function p  hash  a i  distributes itemsets a i of A to cells We expect 2 qF 0  A   1 itemsets  a i  a i   B in the rst less signicant cell 2 qF 0  A   2 in the second etc and therefore there are log  qF 0  A  such cells that correspond to the Zone-1 of the bitmap The whole bitmap ignoring Zone-0  holds log  F 0  A  cells and therefore the size of the fringe zone is log  F 0  A   log  qF 0  A    log q  This observation demonstrates that the size of the fringe zone is quite small for almost all non-implication counts and that it logarithmically increases when the ratio q  0 For example all non-implication counts greater than 1  16 of F 0  A  correspond to a fringe zone of only four cells We must point out that the bounds given in Lemma 2 are very pessimistic and that the size of the fringe zone is actually smaller For example in the case where all distinct elements satisfy the implication condition  q  0 then the counting procedure degenerates to the basic probabilistic counting described in where the fringe zone size is quantied with high probability by O  log log F 0  A   4.3.2 Bounding the size of the Fringe Zone This gives rise to the idea of bounding the size of the fringe zone to a specic size This limits the amount of itemsets we must keep in memory In section 4.1.1 we mention that the index of the cell in the bitmap is determined by function p  hash  a i   which represents the position of the least signicant 1-bit in the binary representation of hash  a i   From Lemma 1 we know that as we move to higher-order bits the number of a i s that get hashed in the corresponding cells decreases exponentially For example if the number of distinct a i s is 128 we know that about 64 will get hashed to the rst from left to rightcell 32 to the second  2 to the 6th and only 1 to the 7th cell Note that this distribution is the same regardless the original distribution of a i values or the frequency that a i s appear In there is a discussion about using linear hash functions in order to accomplish that Assume that we arbitrarily choose to dene that the fringe zone has a xed size of four cells We expect that in the rightmost cell of the fringe zone only one a i will get hashed in in the immediate left cell two  and to the leftmost cell of the fringe we expect 2 3  8 different a i s We keep track of every single itemset a i that gets hashed in the cells of the fringe zone as well as all the itemsets of B there itemsets appear with This allows us to check if there is at least one  a i  a i   B in a cell and therefore assign it a value of one If this happens for the leftmost cell in the fringe zone then we oat the fringe zone to the right by increasing the size of Zone-1 by one By limiting the size of the fringe to four cells we bound the amount of memory required to make a decision Note that for each different itemset that hashes in a cell we need to keep at most K different itemsets of B it appears with Therefore in our case where the fringe has a size of four cells we need at most  2 0  2 1   2 3   K i.e O  K   itemsets to be stored in the corresponding cells Note that the actual memory required is much less since we can free all the memory required by cells in the fringe that have been assigned a value of one We can also double the allocated memory keeping the asymptotic requirements unaffected to accommodate deviations from the expected distributions due to inefciencies of the hash function 4.3.3 Estimation Error due to Fringe Size Fixation When there is not enough space in a cell to accommodate an itemset a i we arbitrarily assign a value of one to the cell and shift the oating fringe to the right This can happen under two different situations In the rst one it just happens to hash an itemset to a cell in the fringe zone that already accommodates the expected number of itemsets In the second situation an itemset is hashed to Zone-0 and the fringe zone must oat to the right to accommodate that itemset Remember that by denitionthe rightmost cell of the fringe is always the rightmost cell where an itemset has been hashed As the fringe oats to the right the leftmost cells of the fringe now belong to Zone-1 This step is that xates the length of the fringe zone The only effect that it has is the introduction of an error for small non-implication counts that cannot be managed by the chosen fringe zone size Note that no error is introduced when the fringe zone has a size of at least F   log  q   By limiting the size of the fringe we essentially limit the minimum non-implication count we can estimate If the fringe size is F then the minimum Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


non-implication count we can estimate with the basic probabilistic counting algorithm is 2  F  F 0  A   Smaller non-implication counts are mapped to that specic value For example with a fringe size F  4 we can estimate an implication count accurately if that count is bigger than than 6  25  F 0  A    2  4 015 6  25   Without changing the asymptotic memory requirements one can increase the size of the fringe to eight in order to estimate accurately very small counts  0  4  F 0  A    2  8 015 0  4   Smaller counts than that are mapped to the same value 2  8  F 0  A   4.3.4 Tracking Non-Implication Conditions In this section we describe how we can keep track if an itemset  a i  a i   B given the implication conditions For each itemset a i that hashes in the fringe zone we keep track of all itemsets of B it appears with Therefore the implication count   a i  is known at any moment The support   a i  of that itemset can be represented by a counter that is increased every time itemset a i is hashed in the cell For the condence level   a i  b  with an itemset b of B we use a counter that represents the support   a i  b   Every time the itemset a i appears with b in the stream we increase the corresponding counter At any moment the corresponding condence level is   a i  b    a i  b     a i   which can be determined just by dividing the corresponding counters The top-condence level of an itemset a i can therefore be determined by summing the biggest   a i  b  at any given moment Whenever an itemset a i satises the minimum support condition but does not satisfy the rest of the implication conditions we assign a value of one to the corresponding cell 4.4 Deriving Implication Counts So far we have described how to get an estimate of the nonimplication count  S of A to B  The implication count can be derived by subtracting the non-implication count from the distinct count F sup 0  A  of itemsets a i  A that satisfy the minimum support requirement i.e S  F sup 0  A    S  We can have an estimate of distinct count F sup 0  A  without using any additional memory from the bitmap used to estimate  S  by virtually assigning a value of one to each cell in the fringe zone where at least one itemset of a i  that meets the minimum support condition is hashed in The cells in Zone-1 by denition have at least one itemset that satises the minimum support condition 4.5 Algorithm NIPS/CI Algorithm 1 referred to as NIPS Non-Implication Probabilistic Sampling gives the complete algorithm using a oating fringe for performing the probabilistic sampling for non-implication counts with given implication conditions This algorithm is designed to sample a small O  K  number of pairs  a i  b j   based on the hash representation of a i over a data stream Any time a tuple arrives the bitmap is updated accordingly In line 2 we project the tuple to the attributes of A and B respectively Then we calculate the position i of the cell where itemset a is hashed If the position i is in Zone-0 right of the fringe zone where no item has been hashed yetthen we oat the fringe Algorithm 1 NIPS Non-Implication Probabilistic Sampling Input M:stream of tuples Input A,B  set of attributes of M Input K     C  Implication Conditions State bm bitmap of L cells 1 for each tuple t do 2 a  t  A   b  t  B  i  p  hash  a  3 if i in Zone-0 then 4 oat fringe by making i its rightmost cell 5 end if 6 if i in fringe zone and alue=0 then 7  a  b   bm[i].supp a  b  8  a   bm[i].supp a 1 9 curConf   C 10 if bm[i].supp a    then 11 curConf  Sum top c bm[i].supp  a  b   bm[i].supp a  12 end if 13 if curConf   C  or v ero wed then 14 alue=1 15 free all the memory allocated for the cell 16 if  is the leftmost in the fringe then 17 oat fringe one cell to the right 18 end if 19 end if 20 end if 21 end for zone to the right by making position i its rightmost cell In the process of oating the leftmost cells of the fringe zone that become part of Zone-1 are cleared of all itemsets inside and are set to value 1 As explained in Section 4.3.3 this process introduces an error only when counting very small non-implications and the size of the fringe zone is not appropriately set In lines 7 and 8 the counters that represent the current support of the itemsets a and a  b are increased Line 11 calculates the top-condence level of itemset a  In lines 14 to 17 a cell is assigned the value one if we have found an itemset a that either does not imply B or if there is no room in the corresponding cell Additionally the fringe zone is shifted to the right if necessary Algorithm 2 CI:Counting Implications Input K     C  Same implication conditions used in NIPS Input bm bitmap of L cells used in NIPS 1 R F sup 0  A   0 2 while exists a i in bm R F sup 0  A   s.t supp  a i   K and R F sup 0  A   L do 3 R F sup 0  A   R F sup 0  A   1 4 end while 5 R  S  0 6 while bm R  S  alue=1 and R  S  L do 7 R  S  R  S  1 8 end while 9 return 2 R F sup 0  A   2 R  S Algorithm 2 referred to as CI Counting Implications returns an estimate of the implication count S and is designed to work with the bitmap used in and in parallel with algorithm NIPS Whenever the user wants an estimate of the current implication count she runs CI on the bitmap of NIPS Lines 1 to 3 nd the position R F sup 0  A  that corresponds to the number of distinct elements F sup 0  A  that satisfy the minimum Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


support requirement Lines 5 to 7 similarly locate the position R  S that corresponds to the non-implication count Line 9 returns the estimate of the implication count as described in Section 4.4 4.6 Space and Time Complexities The basic probabilistic counting algorithm has a space complexity in bits of O  log  A   where  A  is the compound cardinality of A  In order to estimate the implication count S  Algorithm NIPS  in addition to the memory O  log log  A   required for the counter for Zone-1 and the O  log  A   memory required for the hash function requires enough memory to accommodate all the counters for the pairs  a  b  that hash in the cells of the fringe zone The distinct number of a s that hash in the fringe zone is bounded by the fringe zone for example for a fringe size of eight we expect about  7 i  0 2 i  255 different a s We can actually double or even triple the expected number of a s without affecting the asymptotic complexities in order to accommodate more a s due to inefciencies of the hash function The number of distinct b s that corresponds to each a is bounded by the maximum multiplicity K  i.e there are at most K different such b s The number of counters for each cell then is O  K   In general we need O  log T  bits to represent each counter where T is the number of tuples of the dataset or the data stream Therefore the total space in bits complexity of algorithm 1 is O  K  log T  log log  A   log  A   where K is the maximum multiplicity T is the number of tuples and  A  is the compound cardinality of A  We do not include the 2 F term since the size F of the fringe zone is xed and usually a value of four is sufcient to estimate very large implication counts as described in Section 4.3.3 By using hash tables to locate a counter in a cell given a pair  a  b  or an a  and a priority queue to handle the top c operator c  K counters for a cell the time complexity of the algorithm per data item is O  K  log K  The number of entries  a i  b j  that NIPS holds in memory is bounded by the fringe zone size and the maximum multiplicity condition For example for F  4 the number of entries in the bitmap is at most 15  K  The above complexities demonstrate the scalability potential of algorithm NIPS One can estimate accurately any implication counts for arbitrarily big attribute cardinalities or number of tuples 4.7 Approximation In this section we discuss how an algorithm approximates a value and we show how existing techniques can be used in order to get more accurate results based on algorithm NIPS/CI A probabilistic algorithm      approximates a value A if it outputs a value  A such that P    A   A     A  012 1   The parameters    are called approximation parameter and conìdence parameter respectively For example if a user requests   10 and   1 then the algorithm should return an estimate  A that is at most 10 relatively off the actual value A with probability at least 99 4.7.1 Approximating Non-Implication Counts The basic probabilistic algorithm is sho to approximate the number of distinct elements F 0 zeroth frequency moment in a different manner P  1  c   F 0 F 0  c  012 c  2 c   c  2 by using a linear hash function In techniques are presented that can be used to      approximate F 0 based on the algorithm in Section 4.1.1 NIPS approximates the non-implication count in exactly the same manner with the basic probabilistic algorithm under the condition that the non-implication count is large enough for the chosen size of the fringe zone The same techniques used in can be applied to get an      approximation of the non-implication count 4.7.2 Approximating Implication Counts The implication count is determined by subtracting two      approximations namely the number of distinct itemsets that satisfy the minimum support condition and the non-implication count This operation however does not maintain      approximation since the relative error can grow arbitrarily large when the non-implication count is very close to the number of distinct elements This essentially means that the relative error for very small implication counts close to zero can be unbounded For a pragmatic approach however this is not an issue We have already made the assumption that the user is not interested in very small non-implication counts in order to xate the size of the fringe zone We can make the assumption that the user is not interested in very small implication counts very close zero as well and in the experiments section we demonstrate that for a wide range of implication counts the estimates returned by algorithm 2 are very accurate 5 Using Frequent Itemsets In this section we extend the algorithms Lossy Counting and Sticky Sampling introduced in so that the y identify itemsets that satisfy given implication conditions Then we discuss the advantages and disadvantages compared to NIPS/CI and point out why they cannot be applied successfully to the problem of estimating implication counts 5.1 Implication Lossy Counting The Implication Lossy Counting\(ILC algorithm is deterministic and requires at most K  log   T  see sampling entries in order to compute an      synopsis 2  that can be used to identify the implicated itemsets for given implications conditions K   rel    It is important to point out that the minimum support condition is required to be specied relatively to the current number of tuples T in the stream and that the approximation parameter  must satisfy    rel  These requirements have some very interesting side-effects discussed in Section 5.1.1 The stream is conceptually divided into buckets of width w   1    The current bucket is denoted by b current  The algorithm samples entries of the form  a i support 012  and   a i  b j  support 012  where 012 is the maximum possible error in the support For each pair  a i  b j  that arrives in the stream we check if there is an 2   1 in this case Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


entry for a i and if it is we update the support of both a i and  a i  b j   Otherwise we create two new entries  a i 1 b current  and   a i  b j  1 b current  The supports of the itemset a i and the pairs  a i  b j  allow us to check if the itemset a i satises the implication conditions as explained in Section 4.3.4 If we determine that an itemset a i satises the minimum support requirement but not one of the remaining implication conditions then we mark the corresponding sample entry as dirty and delete all the pair entries for that itemset a i  At bucket boundaries we prune all non-dirty entries of the form  a i support 012  where support  012  b current  For each non-dirty itemset a i that is deleted we also remove the corresponding entries   a i  b j  support 012  When the user requests the implicated itemsets we output all non-dirty a i s with support 012   rel    T  Algorithm ILC has two differences with the original Lossy Counting algorithm rst we sample supports and errors for both itemsets and pairs of itemsets and second we mark an itemset as dirty and remove all the corresponding pairs from the samplesas soon as we determine that the itemset does not satisfy the implication conditions It is possible to make the same modications to the Sticky  algorithm in order to identify implicated itemsets but the issue with the relative minimum support remains 5.1.1 Relative Minimum Support Issues The ILC algorithm returns the actual itemsets that satisfy any given implication conditions and not just their count Although the complexity appear quite small actually in it is sho wn that the required memory is much less than the worst case which corresponds to a rather pathological situation it does not tell the whole truth More specically one problem is that every single itemset that satises the minimum support  rel has to stay in memory marked dirty even if it doesnít satisfy the rest of the implication conditions This property although desirable for certain applications limits the applicability of ILC due to the amount of memory required In the worst case the number of entries that need to be sampled is in the order of the number of different itemsets in the stream For conditional implications the compound cardinality of the participating attributes can be quite high The only way to limit the memory used is by increasing the  rel minimum support condition which implies that the contribution of many implications that hold for a smaller amount of tuples is totally lost see  for an e xample application The major problem however is the relative nature of the minimum support itself As the stream evolves the number of tuples that correspond to the given implication conditions implicitly increases The side-effect is that the contribution of small implications to the implication count is lost although these specic implication may hold for a quite large and continuously increasing number of tuples The relative nature of the minimum support in the ILC algorithm cannot be removed since the approximation parameter  must remain constant the size of the buckets is a function of   and satisfy    rel  throughout the execution of the algorithm The same holds for the extended sticky sampling algorithm On the other hand the NIPS/CI algorithm returns an accurate estimation of the implication count by keeping only O  K  samples in memory regardless of how small the minimum support requirement is capturing the cumulative effect of small implications throughout the life of the stream 6 Experiments In this section we present an extensive empirical study on the accuracy of the estimation for implication counts The rst section demonstrates the results using articially generated datasets while the second section describes the result obtained from realworld datasets For the synthetic datasets we imposed implication patterns of known count on the datasets and used both the bounded and the unbounded fringe estimator to estimate the strength of the correlations and report the relative error and the deviation For the real datasets we used an exact method based on hash tables for calculating the implication count and compared with the estimation of both the bounded and the unbounded version of our estimator 6.1 Synthetic Dataset One We conducted a series of experiments to verify the error bounds of NIPS/CI with a xed fringe size of four We used a varying cardinality for attribute A and the imposed implications had a variable count between 10 and 90 of  A   for various one-toc implications where c  1  2  4 We increased the accuracy of our estimation to approximately 10 using stohastic av More specically for the accurac y o f 10 we used 64 bitmaps with a fringe zone of size four i.e there was available space for 1920 itemsets in memory We chose a minimum top-condence level  of 90 while the itemsets a s that should participate in the count were imposed to have a top-condence level of 92 The chosen minimum support was 50 tuples The maximum multiplicity was chosen to be equal to c  To test the accuracy of the algorithms with respect to the implication conditions we also imposed a noise Some itemsets a i where created in a way that breaks at least one implication condition and therefore they should not participate in the count For example itemsets that did not participate because of the maximum multiplicity condition were imposed to appear with a number u of different itemsets of B that was uniformly distributed as c  1  u  c  10 Each combination of these parameters was tested one hundred times The experiments were performed by generating random numbers using a random number generator to simulate the itemsets Specically the experiments were organized as follows Pick a cardinality size  A   an implication count S and a c  Generate S different itemsets a i  For each a i create at most c uniformly distributed in the range  1  c   different b j  For each combination  a i  b j  write 50 tuples Then for each a i create four b  j different than all b j s created before And write the four tuples  a i  b  j   This step creates S itemsets a i such that a i  B with a minimum support of 54 tuples and a top-condence level of 50  54  92 Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


 10 20 30 40 50 60 70 80 90 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 100 200 300 400 500 600 700 800 900 Implication Count 0.06 0.08 0.1 Mean Error Bounded Fringe Unbounded Fringe 10 20 30 40 50 60 70 80 90 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 100 200 300 400 500 600 700 800 900 Implication Count 0.06 0.08 0.1 Mean Error Bounded Fringe Unbounded Fringe  A   100  A   1  000  A   100  A   1  000 1000 2000 3000 4000 5000 6000 7000 8000 9000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 20000 40000 60000 80000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 1000 2000 3000 4000 5000 6000 7000 8000 9000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe 20000 40000 60000 80000 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe  A   10  000  A   100  000  A   10  000  A   100  000 Figure 4 DataSet One with c  1 Figure 5 DataSet One with c  2 and therefore these itemsets participate in the implication count The number of tuples created by this step is S  50  c  1   2  4   The rest of the steps create itemsets that should not participate in the count We create three different kind of tuples that break one implication condition The relative weight of each kind is 1  3 Generate   A  S   3 pairs  a i  b j  where each a i is different than all itemsets of A created before As in the previous step for each a i create at most c different b j  For each combination  a i  b j  write 50 tuples Then for each a i create eight b  j different than all b j s created before And write the eight tuples  a i  b  j   This step creates   A  S   3 new itemsets of A that should not participate in the implication count because they do not satisfy the minimum top-condence level although they satisfy both the minimum support and the maximum multiplicity constraint The number of tuples created by this step is   A  S   3  50  c  1   2  8   Generate   A  S   3 pairs  a i  b j  where each a i is different than all itemsets of A previously generated and each one appears with u different b j  where c  1  u  c  10 Write 50 such tuples This step creates   A  S   3 new itemsets of A that should not participate in the implication count because they do not satisfy the maximum multiplicity condition The number of tuples created by this step is   A  S   3  50  c  5  5  Generate   A  S   3 pairs  a i  b j  where each a i is different than all itemsets of A previously generated For each pair  a i  b j  write 40 tuples This step creates   A  S   3 new itemsets of A that should not participate in the implication count because they do not satisfy the minimum support requirement The number of tuples created by this step is   A  S   3  40 Shufe the output le This step just demonstrates that the operation of the algorithm is independent to the ordering of the tuples Estimate the implication count using algorithm NIPS/CI with a fringe size of four and also without a bounded fringe Perform one hundred such experiments and calculate the mean and the standard deviation of both estimations The total number of tuples for each experiment can be derived by adding the partial number of tuples created in each step For example for  A   10000 S  5000 and c  4 the average number of tuples for the corresponding experiment was 015 3  108  333 A minimum support of 50 tuples for this case corresponds to only 015  001 of the tuples demonstrating that in the implication count contribute even implications that hold for a very small number of tuples Figures 4,5 and 6 show the results for c  1  2  4 for varying cardinalities  A   The x-axis corresponds to the actual implication count of the dataset as that was imposed by the creation process The y-axis denotes the mean relative error as it is calculated by running one hundred experiments We used the following formula to estimate the mean relative error relative error   Actual S  Measured S  Actual S  Graphs Bounded Fringe express the experimental results for the case of a fringe with size F  4 while graphs Unbounded Fringe demonstrate the result for the case of an arbitrarily large fringe The error bars correspond to the statistical deviation of the mean error as that was computed by one hundred such experiments.The deviation is generally negligible which means that the error of the estimated S is always very close to the mean error We also observe that the difference between the estimation using a bounded fringe of size four and a unbounded one is negligible for a very wide range of implication counts and therefore a size of four for the fringe zone is sufcient to provide very accurate results for most applications 10 20 30 40 50 60 70 80 90 Implication Count 0.05 0.06 0.07 0.08 0.09 0.1 Mean Error Bounded Fringe Unbounded Fringe Figure 6 Dataset One with c  4 6.2 Real-world datasets  Algorithmic comparison We compare our estimates with the results taken using Distinct Sampling DS which has been sho wn pro vide highlyProceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


accurate estimates for distinct value queries and event reports This algorithm outperforms other estimators that are based on uniform sampling[8 9 e v e n when using much less sample space We also provide comparison with our Implication Lossy Counting ILC algorithm described in Section 5.1 which is based in the Lossy Counting algorithm introduced in For this series of experiments we used a real dataset of eight dimensions which was given to us by an OLAP company whose name we cannot disclose due to our agreement The cardinalities of the dimensions are presented in Table 3 The parameters of the algorithms are presented in Table 5 For NIPS/CI we used 64 concurrent bitmaps with a fringe size of four thus requiring memory enough to hold  2 4  1   64  K  1920 itemsets We expect that the averagingî\([5 o v e r these man y bitmaps will result in an error less than 10 We used the exact same sample space for DS The bound parameter t for DS was set to  1920  50  following the suggestion in F o r ILC we used an approximation parameter   0  01 which increases the memory requirements of ILC relative to those of NIPS/CI or DS On the average ILC used more than twice the memory that NIPS/CI and DS used For example for the experiment in Figure 7\(B it used more than 8,000 entries We evaluate the results of the algorithms with respect to the number of tuples the cardinality of the participating dimensions and the implication conditions To simulate a real data stream scenario we tracked the conditional implication counts of A  B  E  F and the unconditional B  E using the aforementioned algorithms The rst workload corresponds to quite large compound cardinality while the second to very moderate cardinalities Table 4 presents the actual aggregates for various instances of the stream for   5 and  1  60 We believe that most workloads fall somewhere in the middle with respect to the complexity of the wanted implications and the size of the returned counts Figure 7\(A depicts the relative error as the stream evolves for workload A using the algorithms DS NIPS/CI and ILC for different implication parameters In Figure a we show the results for minimum support   5 and  1  60 or  1  80 The different  1 are encoded in the parentheses next to identication of the algorithm in the legend of the graph In Figure b we increased the minimum support to   50 We observe that the behavior of DS varies widely while NIPS/CI remains always below the expected 10 error DS actually keeps a sample of the distinct elements seen so far and tries to scale the implication count that holds for that sample to the whole set of distinct elements In most cases the data in the sample is not representative of the implication The situation for DS is exacerbated when the minimum support increases where quite a lot of samples do not participate in the count making the scaling even more errorprone Algorithm ILC in all cases returned very erroneous results although it used much more space than NIPS/CI and DS since it tries to store not the implication counts but the actual implicated itemsets In these workload the implicated itemsets overwhelm its available memory which is actually larger than the amount given to NIPS/CI and DS In gure 7\(B we present the results of the algorithms for workload B The situation is still in favor of NIPS/CI whose relative error remains always close to the expected 10 unlike DS who returns highly skewed errors even though the domain cardinalities are much smaller and therefore keeps in the sample space much more data As expected from the analysis the error guarantees of NIPS/CI are virtually unaffected by changes in the cardinalities or the number of tuples seen so far in the stream ILC returns very erroneous results although now the cardinalities and the implicated items are much smaller compared to those of workload A The reason is not only because it keeps too much information in memory i.e all the implicated itemsets while both NIPS/CI and DS only hold a mantissa for the count but also because the constraint    rel is broken as the number of tuples increases 7 Related Work There are unique challenges in query processing for the data stream model Most challenges are the result of the streams being potentially unbounded in size Therefore the amount of the storage required in order to get an exact size may also grow out of bounds Another equally important issue is the timely query response required although the volumes of data the need to be processed is continually augmented at a very high rate Essentially the amount of computation per data item received should not add a lot of latency to each item Otherwise any such algorithm wont be able to keep up with the data stream In many cases accessing secondary storage such as disks is not even an option In  there is a discussion of what queries can be answered e xactly using bounded memory and queries that must be approximated unless disk access is allowed Sketching techniques\([14 ha v e been introduced to build summaries of data in order to estimate the number F 0 of distinct elements in a dataset In three algorithms that      approximate the F 0 are described with various space and time requirements Distinct is dri v e n by hashing functions similar to those studied in 14 and provides highly accurate results for distinct value queries compared to those taken by uniform sampling by using only a fraction of their sample size In the algorithms Stick y Sampling and Lossy Counting are introduced that estimate frequency counts with application to association rules and iceberg cubes In a framework for performing set expression on continuously updated streams based on sketching techniques is presented In a general framework over multiple granularities is presented for both range-temporal and spatio-temporal aggregations In a framework for identifying hierarchical heavy hitters i.e hierarchical objects like network addresses whose prexes denes a hierarchy with a frequency above a given threshold is described The effect of impications between columns has been emphasized in the system that identies correlated pairs of columns and soft-dependencies and has been proved very useful in query optimization 8 Conclusions We have presented a generalized and parameterized framework that can accurately and efciently estimate implication counts and can be applied to many scenarios To the best of our knowledge this is the rst practical and truly scalable approach to the problem Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


 Dimension Cardinality A 1557 B 2669 C 2 D 2 E 3363 F 131 G 660 H 693 Table 3 Cardinalities Workload A Workload B Tuples A  B  E  G E  B 134,576 608 50 672,771 12,787 125 1,344,591 34,816 152 2,690,181 84,190 165 4,035,475 132,161 182 5,381,203 187,584 188 Table 4 Impl counts w.r.t tuples NIPS/CI bitmaps 64 NIPS/CI K 2 DS sample size 1920 DS bound t 39 ILC  0.01 Table 5 Algorithm Parameters           0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILC \(.8             0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILS \(.8           0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6 ILC \(.8             0 1 M 2 M 3 M 4 M 5 M 6 M Tuples 0 10 20 30 40 50 60 70 80 90 100 Relative Error NIPS/CI \(.6 NIPS/CI \(.8   DS \(.6   DS \(.8   ILC \(.6   ILC \(.8   a   5 b   50 a   5 b   50 A Workload A B Workload B Figure 7 Relative Error vs stream size of online estimation within small errors of complex implication and non-implication counts between attributes of a data stream under severe memory and processing constraints and even in the presence of noise We prove that the complement problem of estimating non-implication counts can be      approximated when the size of the fringe zone is xed appropriately We demonstrate that existing algorithms for estimating frequent itemsets or sampling cannot be applied to the problem since they lose the cumulative effect of small implications In addition through an extensive set of experiments on both synthetic and real data we have shown that NIPS/CI always remains very close to the actual implication count capturing even very small implications whose total contribution is signicant References  R Agra w al A Evmie vski and R Srikant Information sharing across private databases In ACMÖSIGMOD  2003  N Alon Y  Matias and M Sze gedy  The space comple xity of approximating the frequency moments Journal of Computer and System Sciences  58:137 147 1999  A Arasu B Babcock S Bab u J McAlister  and J W idom Characterizing memory requirement for queries over continuous data streams In Proceedings of the Twenty-ìrst ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems  2002  B Babcock S Bab u M Datar  R  Motw ani and J W idom Models and Issues in Data Stream Systems In Proceedings of the Twenty-ìrst ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems  2002  Z Bar Y ossef T  Jayram R K umar  D  S i v akumar  and L T r e visan Counting Distinct Elements in a Data Stream In RANDOM  pages 1ñ10 2002  A Belussi and C F aloutsos Estimating the selecti vity of spatial queries using the correlation fractal dimension In VLDBê95 Proceedings of 21th International Conference on Very Large Data Bases September 11-15 1995 Zurich Switzerland  pages 299ñ310 1995  J Bunge and M Fitzpatrick Estimating the number of species A r e vie w  Journal of American Statistical Association  88:364ñ373 1993  M Charikar  S  Chaudhuri R Motw ani and V  Narasayya T o w ards estimation error guaranties for distinct values In ACMÖPODS  pages 268ñ279 2000  S Chaudhuri R Motw ani and V  Narasayya Random sampling for histogram construction How much is enough In ACMÖSIGMOD  pages 436 447 1998  G Cormode F  K orn S Muthukrishnan and D Sri v astana Finding hierar chical heavy hitters in data streams In VLDB  2003  M Datar  A  Gionis P  Indyk and R Motw ani Maintaing stream statistics over sliding windows In Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms  2002  A Deshpande M Garof alakis and R Rastogi Independence is Good Dependency-Based Histogram Synopses for High-Dimensional Data In ACM SIGMOD  2001  C Estan S Sa v age and G V a r ghese Automatically inferring patterns of resource consumption in network trafc In ACMÖSIGCOMM  2003  P  Flajolet and N Martin Probabilistic Counting Algorithms for Data Base Applications Journal of Computer and System Sciences  pages 182ñ209 1985  N Friedman L Getoor  D  K oller  and A Pfef fer  Learning probabilistic relational models In IJCAI  pages 1300ñ1309 1999  S Ganguly  M  Garof alakis and R Rastogi Processing set e xpressions o v e r continuous update streams In ACM SIGMOD  pages 265ñ276 2003  P  Gibbons Distinct sampling for highly-accurate answers to distinct v alues queries and event reports In VLDB  pages 541ñ550 2001  P  J Haas J F  Naughton S Seshadri and L Stok es Sampling-based estimation of the number of distinct values of an attribute In VLDB  1995  I Ilyas V Markl P  Haas P  Bro wn and A Aboulnaga CORDS Automatic Discovery of Correlations and Soft Functional Dependencies In ACMÖSIGMOD  2004  J Jung B Krishnamurthy  and M Rabino vich Flash cro wds and denial of service attacks Characterization and implications for cdns and web sites In ACMÖWWW  2002  J Ki vinen and H Mannila Approximate dependenc y inference from relations Theoritical Computer Science  149:129ñ149 1995  G Manku and R Motw ani Approximate frequenc y counts o v e r data streams In VLDB  2002  V  Poosala P  J Haas Y  E Ioannidis and E J Shekita Impro v e d histograms for selectivity estimation of range predicates In ACMÖSIGMOD  pages 294ñ305 1996  S Stolfo W  Lee P  Chan W  F an and E Eskin Data mining-based intrusion detectors An overview of the columbia ids project ACMÖSIGMOD Record  30\(4 2001  H W ang D Zhang and K G Shin Detecting SYN Flooding Attacks In INFOCOM 2002  2002  K Whang B V ander Zander  and H T aylor  A Linear Time Probabilistic Counting Algorithm for Database Applications ACM Transactions on Database Systems  pages 209ñ229 1990  D Zhang D Gunopulos V  Tsotras and B  See ger  T emporal and spatiotemporal aggregations over data streams using multiple time granularities Information Systems  28\(1-2 2003 The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofﬁcial policies either expressed or implied of the Army Research Laboratory or the U S Government Proceedings of the 21st International Conference on Data Engineering \(ICDE 2005 1084-4627/05 $20.00 © 2005 IEEE 


Figure 3a and 3b, is negative since CBA and PART achieved a higher classification rate against this particular dataset A comparison of the knowledge representation produced by our method, PART and CBA has been conducted to evaluate the effectiveness of the set of rules derived. Figure 4 represents the classifiers generated form the hyperheuristic datasets. Analysis of the rules sets indicated that MMAC derives a few more rules than PART and CBA for the majority of the datasets. In particular, the proposed method produced more rules than PART and CBA on 8 and 7 datasets, respectively. A possible reason for extracting more rules is based on the recursive learning phase that MMAC employs to discover more hidden information that most of the associative classification techniques discard, since they only extract the highest confidence rule for each frequent item that survives MinConf Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE 6. Conclusions A new approach for multi-class, and multi-label classification has been proposed that has many distinguishing features over traditional and associative classification methods in that it \(1 that contain rules with multiple labels, \(2 evaluation measures for evaluating accuracy rate, \(3 employs a new method of discovering the rules that require only one scan over the training data, \(4 introduces a ranking technique which prunes redundant rules, and ensures only high effective ones are used for classification, and \(5 discovery and rules generation in one phase to conserve less storage and runtime. Performance studies on 19 datasets from Weka data collection and 9 hyperheuristic scheduling runs indicated that our proposed approach is effective, consistent and has a higher classification rate than the-state-of-the-art decision tree rule \(PART and RIPPER algorithms. In further work, we anticipate extending the method to treat continuous data and creating a hyperheuristic approach to learn  on the fly   which low-level heuristic method is the most effective References 1] R. Agrawal, T. Amielinski and A. Swami. Mining association rule between sets of items in large databases In Proceeding of the 1993 ACM SIGMOD International Conference on Management of Data, Washington, DC May 26-28 1993, pp. 207-216 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rule. In Proceeding of the 20th International Conference on Very Large Data Bases, 1994, pp. 487   499 3] M. Boutell, X. Shen, J. Luo and C. Brown. Multi-label semantic scene classification. Technical report 813 Department of Computer Science, University of Rochester Rochester , NY 14627 &amp; Electronic Imaging Products R &amp D, Eastern Kodak Company, September 2003 4] A. Clare and R.D. King. Knowledge discovery in multilabel phenotype data. In L. De Raedt and A. Siebes editors, PKDD01, volume 2168 of Lecture Notes in Artificial Intelligence, Springer - Verlag, 2001,  pp. 42-53 5] P. Cowling and K. Chakhlevitch. Hyperheuristics for Managing a Large Collection of Low Level Heuristics to Schedule Personnel. In Proceeding of 2003 IEEE conference on Evolutionary Computation, Canberra Australia, 8-12 Dec 2003 6] R. Duda, P. Hart, and D. Strok. Pattern classification Wiley, 2001 7] E. Frank and I. Witten. Generating accurate rule sets without global optimisation. In Shavlik, J., ed., Machine Learning: In Proceedings of the Fifteenth International 


Learning: In Proceedings of the Fifteenth International Conference, Madison, Wisconsin. Morgan Kaufmann Publishers, San Francisco, CA, pp. 144-151 8] J. Furnkranz. Separate-and-conquer rule learning Technical Report TR-96-25, Austrian Research Institute for Artificial Intelligence, Vienna, 1996 9] W. Li, J. Han and J. Pei. CMAR: Accurate and efficient classification based on multiple class association rule. In ICDM  01, San Jose, CA, Nov. 2001, pp. 369-376 10 ] T. Joachims. Text categorisation with Support Vector Machines: Learning with many relevant features. In Proceeding Tenth European Conference on Machine Learning, 1998,  pp. 137-142 11] T. S. Lim, W. Y. Loh and Y. S. Shih. A comparison of prediction accuracy, complexity and training time of thirtythree old and new classification algorithms. Machine Learning, 39, 2000 12] B. Liu, W. Hsu and Y. Ma. Integrating Classification and association rule mining. In KDD  98,  New York, NY, Aug 1998 13] J.R. Quinlan. C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann, San Francisco, 1993 14] J.R. Quinlan. Generating production rules from decision trees. In Proceeding of the 10th International Joint Conferences on Artificial Intelligence,  Morgan Kaufmann San Francisco, 1987, pp. 304-307 15] R. Schapire and Y. Singer, "BoosTexter: A boosting-based system for text categorization," Machine Learning, vol. 39 no. 2/3, 2000, pp. 135-168 16] F. Thabtah, P. Cowling and Y. Peng. Comparison of Classification techniques for a personnel scheduling problem. In Proceeding of the 2004 International Business Information Management Conference, Amman, July 2004 17]Y. Yang. An evaluation of statistical approaches to text categorisation. Technical Report CMU-CS-97-127 Carnegie Mellon University, April 1997 18] X. Yin and J. Han. CPAR: Classification based on predictive association rule. In  SDM  2003, San Francisco CA, May 2003 19]CBA:http://www.comp.nus.edu.sg/~dm2/ p_download.html 20] Weka: Data Mining Software in Java http://www.cs.waikato.ac.nz/ml/weka 21] M. J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. New algorithms for fast discovery of association rules. In Proceedings of the 3rd KDD Conference, Aug. 1997 pp.283-286 Proceedings of the Fourth IEEE International Conference on Data Mining \(ICDM  04 0-7695-2142-8/04 $ 20.00 IEEE pre></body></html 


Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 R=1000 1e-4 0.2 High 1.153e3 1.154e3 3.180e3 2.698e3 45 45.0 597 0.214 0.9900 0.0071 1.0000 0.0162 2e-4 2e-4 Low 0.4017 0.4024 109.291 71.994 4 4.0 39.7 0.996 0.9960 0.0075 0.9995 0.0012 2e-3 1.0 1e-3 Medium Medium 0.4128 0.4129 17.488 19.216 6 6.0 29.6 0.992 0.9938 0.0018 0.9987 0.0034 R=0.1 0.01 4e-3 High 0.4612 0.4722 4.1029 5.410e3 12 12.0 28.0 0.992 0.9902 0.0045 1.0000 0.0132 0.04 1e-8 Low 0.0250 0.0272 253.917 155.263 3 3.0 38.0 1.002 0.9991 0.0001 0.9994 0.0008 1e-3 1e4 1e-7 High Medium 0.0338 0.0290 17.303 12.837 3 3.0 14.5 0.332 0.9900 0.0058 0.9951 0.0045 R=1e-5 1e-2 1e-6 High 0.0918 0.0557 1.6071 1.225e5 8 8.0 9.9 0.992 0.9906 0.0034 0.9994 0.0142 1e-3 TABLE II SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=100, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 2e-4 Low 0.0408 0.0407 2.9255 2.0060 5 5.0 30.3 0.996 0.9933 0.0016 0.9998 0.0020 0.02 1.0 1e-3 Medium Medium 0.0446 0.0446 0.5143 333.138 10 10.0 27.8 0.996 0.9900 0.0042 0.9995 0.0093 R=0.01 0.1 4e-3 High 0.0763 0.1062 0.1580 3.402e3 84 84.0 90.9 0.793 0.9900 0.0099 1.0000 0.0334 0.4 TABLE III SIMULATION RESULTS FOR: TRACKING REGIME PD=0.9, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.166e3 1.167e3 1.073e4 3.357e4 11 12.2 257 0.542 0.9900 0.0089 1.000 0.0073 5e-5 1e-4 0.1 Low Medium 1.167e3 1.167e3 5.509e3 1.481e4 21 23.2 309 0.858 0.9900 0.0067 1.0000 0.0052 R=1000 1e-4 0.2 High 1.168e3 1.170e3 3.180e3 6.979e3 46 50.6 475 0.798 0.9900 0.0089 1.000 0.0022 2e-4 3] T. Fortmann, Y. Bar-Shalom, Y. Scheffe, and S. B. Gelfand  Detection Thresholds for Tracking in Clutter- A Connection Between Estimation and Signal Processing  IEEE Trans Auto. Ctrl., Mar 1985 4] S. B. Gelfand, T. Fortmann, and Y. Bar-Shalom  Adaptive Threshold Detection Optimization for Tracking in Clutter  IEEE Trans. Aero. &amp; Elec. Sys., April 1996 5] Ch. M. Gadzhiev  Testing the Covariance Matrix of a Renovating Sequence Under Operating Control of the Kalman Filter  IEEE Auto. &amp; Remote Ctrl., July 1996 6] L. C. Ludeman, Random Processes: Filtering, Estimation and Detection, Wiley, 2003 7] L. Y. Pao and W. Khawsuk  Determining Track Loss Without Truth Information for Distributed Target Tracking Applications  Proc. Amer. Ctrl. Conf., June 2000 8] L. Y. Pao and R. M. Powers  A Comparison of Several Different Approaches for Target Tracking in Clutter  Proc Amer. Ctrl. Conf., June 2003 9] X. R. Li and Y. Bar-Shalom  Stability Evaluation and Track Life of the PDAF Tracking in Clutter  IEEE Trans. Auto Ctrl., May 1991 10] X. R. Li and Y. Bar-Shalom  Performance Prediction of 


10] X. R. Li and Y. Bar-Shalom  Performance Prediction of Tracking in Clutter with Nearest Neighbor Filters  SPIE Signal and Data Processing of Small Targets, July 1994 11] X. R. Li and Y. Bar-Shalom  Detection Threshold Selection for Tracking Performance Optimization  IEEE Trans. on Aero. &amp; Elect. Sys., July 1994 12] D. Salmond  Mixture Reduction Algorithms for Target Tracking in Clutter  SPIE Signal and Data Processing of Small Targets, Oct. 1990 13] L. Trailovic and L. Y. Pao  Position Error Modeling Using Gaussian Mixture Distributions with Application to Comparison of Tracking Algorithms  Proc. Amer. Ctrl. Conf., June 2003 4323 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207ñ216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intíl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intíl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





