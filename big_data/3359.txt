Analysis of Relationships between Color and Emotion by Classiﬁcation based on Associations Shangfei Wang 002  Rui Ding 002 Yongjie Hu 002 and Haibao Wang  002 Key Lab of Computing and Communicating Software of Anhui Province Department of Computer Science University of Science and Technology of China Hefei Anhui 230027 P.R.China sfwang@ustc.edu.cn rding@mail.ustc.edu.cn hyjie111@mail.ustc.edu.cn  Department of Radiology The First Hospital of Anhui Medical University Hefei Anhui 230027 P.R.China School of Life Science,University of Science and Technology of China whblqh@mail.ustc.edu.cn Abstract This paper studied the relationships between color attributes and emotional dimensions by classi\036cation based on associations First psychological experiments were designed to gather users emotional response to color images 52 color samples chosen from CIE Lab Lch color spaces were displayed on the screen 20 subjects were asked to report their emotional response in the form of valence and arousal on the SelfAssessment Manikin Secondly after preprocessing 42 cases were used to train by classi\036cation based on associations to 036nd rules between color attributes and emotional dimensions Then these rules were used to predict subjects emotional response to a new color A good result was obtained with correct prediction rate of 90.00 for arousal and 80.00 for valence I I NTRODUCTION Color affects our emotions Some colors make people happy while others make people depressive It would be interesting and worthwhile to study these relationships between color attributes and emotional dimensions and nd some potential rules which may be applied in many elds such as fashion and Website design So color-em otion has been increasingly researched in recent years J H Xin conducted visual experiments in 3 different regions namely Hong Kong Japan and Thailand A set of 218 color samples were used to compare 12 pairs of adjectives such as warm-cool were adopted to represent user’s emotional responses He found that the inﬂuences of lightness and chroma is much more important than that of the hue on the color emotions and there are good correlations of color emotions among these 3 regions Li-Chen Qu studied the color emotions for two-color combinations 11 pairs of adjectives were investigated on 190 color pairs with British and Chinese observers His experimental results showed that there existed gender difference but no signiﬁcant cultural difference between British and Chinese cultural and conﬁrmed an additivity relationship between singlecolor and color-combination emotions[2  He also d e v e lo p e d 3 color preference models for single colors 50 subjects from various age groups were asked to match a list of adjectives with the proper color samples in Banu Manav’s study The result indicated that there was a strong dependence between the choice and use of color at residence they also found that emotional responses to colors changed with lightness and saturation levels In Nag KAYA studies 10 fully saturated chromatic colors chosen from the Munsell color system were used to examine color-emotion association Apart from these ten hue groups three achromatic colors white b lack and gray were also used 98 volunteered college students participated in experiment The results revealed that a color-related emotion is highly dependent on personal prefere nce and past experi 253 color pairs generated from a set of 23 single colors were used in Suchitra Sueeprasan’s study 34 observers identiﬁed color sensation induced by each color pair using 14 opponentword pairs Their results show ed that there were some relationships between two color sensation scales Xiao-Ping Gao’s study investig ated the relationship between color attributes and color emotions from 7 different region groups 214 color samples were used to evaluate on 12 emotional variables The anal ysis showed that chroma and lightness are the most important factors on color emotion whereas the inﬂuences of hue and cultural background were very limited[7  Dekan carried out 4 experiments to describe emotional responses to color and analyze the relationship between color attributes and emotional dimensions One-way repeated measurements ANOVA and correlation analysis were used to analyze the results he found that chroma is positively correlated with all emotional dimensions in four experiments From above it can be seen that current studies except Dekan use opponent adjectives to describe subjects emotional response However the semantic meaning of an adjective is user-depend while valence and arousal may reﬂect users emotional response more directly Further more all of these researches only obtain some qualitative results which can not be used to predict subjects emotional response to a new color In this paper valence and arousal were adopted to describe users emotional responses to color samples in our psychological experiments 52 color stimuli chosen from CIE Lab Lch color spaces were displayed on the screen 20 subjects were asked to report their emotional response After psychological experiments the method of classi cation based on associations 
2008 International Conference on Computer Science and Software Engineering 978-0-7695-3336-0/08 $25.00 © 2008 IEEE DOI 10.1109/CSSE.2008.1207 269 


were used to analyze the relationships between color attributes and emotional dimensions Several quantitative rules were obtained Then these rules were used to predict subjects emotional response to a new color stimulus A Psychological Experiments 52 color stimuli chosen from the CIE Lab Lch color space which is a device-independent were used to induce subject’s emotional response 25 color stimuli of dekan’s study listed in Table I were  b u t t hes e col o r s ampl es di d not i n cl ude s o me common colors such as gray white and black Therefore we added other twenty-seven color samples as showed in Table II TABLE I 25 CHROMATIC COLOR STIMULI FORM D EKAN  SSTUDY hue L C L C L C L C L C red:30 30 30 30 45 40 90 50 40 70 30 yellow:80 60 30 60 70 80 90 80 60 80 40 green:160 30 30 40 45 50 60 40 40 70 20 blue:260 30 20 40 30 40 45 60 35 70 25 violet:320 20 25 30 35 40 40 50 30 70 20 TABLE II 27 COLOR STIMULI WHICH WE DESIGNED hue L C H L C H L C H L C H red 26 61 38 53 104 40 68 53 25 orange 32 53 54 62 90 53 79 43 59 yellow 51 57 102 97 97 103 98 61 105 green 46 72 136 88 120 136 91 77 140 72 94 137 indigo 48 30 194 91 51 195 93 37 196 75 40 197 blue 13 80 306 33 134 306 60 71 297 30 98 304 violet 30 69 329 60 114 329 72 75 327 25 31 27 gray 27 0 297 41 0 197 78 0 297 66 0 197 black 0 0 0 white 100 0 297  is the training stimuli To assess the two dimensions of valence and arousal the Self-Assessment Manikin SAM an affective rating system devised by Lang was us A 9 pointed rating s cale f or each dimension were selected 9 repres ents highest and 1 represents lowest B Training stimuli To make subjects understand the meaning of the valence and arousal 7 pictures chosen from I APS International Affective Picture System listed in table III were also used as stimuli In addition 4 another color stimuli listed in table II were selected In order to make subjects get familiar with these experiments the subjects were asked to rate emotional responses TABLE III T HE VALENCE ADN AROUSAL OF 7IAPS PICTURES NO 2071 2102 1019 2095 1120 1230 7010 Valen ce 7.86 5.16 3.95 1.79 6.93 4.03 1.97 Arousal 5.00 3.03 5.77 5.25 3.789 4.61 4.92 C Subject 20 college students including 18 males and 2 females participated in experiment whose ages ranged from 20 to 29 and the averaged age was 23.95 None of them had achromatopsia D Procedure Firstly the subjects were told the meaning of the valence and arousal and the relation between their distribution and 4 basic emotions happy fear sad and calm Secondly 7 IAPS pictures were displayed on CRT monitor Thirdly 4 training color stimuli were displayed on the same monitor the subject was asked to write down the value of valence and arousal to each color sample on SAM Lastly 52 color stimuli were randomly displayed on monitor each color stimulus was displayed only once till the subject pressed any key to display the next There was no time limit between two stimuli The subject was asked to write down the value of valence and arousal to each color stimulus on SAM After the experiment 20 subjects’s emotional responses to each color stimulus were aver aged according to the follow equations Thus a sample sets including 52 cases were obtained V j  Num 002 i 1 SAM ij 1 Num j 1  2   52 Num 20 SAM ij 1 is the i th subject 002 s valence to the j th color A j  Num 002 i 1 SAM ij 2 Num j 1  2   52 Num 20 SAM ij 2 is the i th subject 002 s arousal to the j th color II A NALYSIS BY CBA The CBA is a data mining tool focusing on mining class association rules based on frequent items It had been used for mining rules from varies kinds of data sets efﬁciently In CBA there are two types of mining strategies which are Classiﬁcation Mining and Association Mining In this paper the CBA was used to mine the rules between color attributes and emotional dimensions not the interrelationship among color attributes Therefore we adopted the Classiﬁcation Mining to exert our experiments and took the two emotional dimensions as the class attribut e respectively The generated rules were used to predict the emotional response to new color stimuli In total 52 cases 42 cases were selected to train by CBA to generate the rules and the left 10 cases were used to 
270 


predict using the generated these rules The framework of our experiment is showed in Fig 1 Fig 1 The framework of our experiment A Preprocessing In the analysis by CBA classiﬁcation techniques can be applied to the relation database table which has a set of attributes In the experiment lightness chroma and hue were attributes valence and arousal were the class First all these attributes and classes should be discretized Lightness was divided into 5 groups 20 degree per group so it had 5 attribute values which were 1 2 3 4 5 Chroma was divided into 7 groups 20 degree per group so it had 7 attribute valueswhichwere1,2,3,4,5,6,7.Huewasdividedinto10 groups by their semantic according to the order of red orange yellow green blue indigo violet gray black and white Because the 9-pointed rating scale was too meticulous to distinguish the emotion the 9-pointed rating scale was changed into the 3-pointed rating scale In 3-pointed rating scale level 1 was less than 4 which denoted negative and sleeping level 2 ranged from 4 to 6 which denoted neutral level 3 was more than 6 which denoted exciting and happy Fig 2 was the histogram of the number of valence and arousal which were divided in to the 9-pointed rating scale From Fig 2 it could be seen that valence ranges from 3 to 6 while arousal from 3 to 7 Fig 3 was the histogram of the number of valence and ar ousal which were divided into 3 scales As depicted in Fig 3 it can seen that most of the distributed in level 1 and 2 Compared to Fig 2 and Fig 3 it can be seen that distributions in these the 9-pointed rating scale and the 3-pointed rating s cale were coincident which means that the impacts of the change to our experiment were acceptable In the analysis by CBA the minimum conﬁdence was set to 50.00 and minimum support was set to 1.00 For minimum support it was more complex because it had a strong effect on the quality of the classiﬁer produced If it was set to high those possible rules that cannot satisfy minimum sport but with conﬁdence would not be incl I n some study it was set to 1%˜2 we set it to 1 If the conﬁdence is greater than minimum conﬁdence we say the rule is B The relationships between valence and 3 color attributes 42 cases which included lightness chroma hue and valence were input to the CBA After learning 6 rules listed in table 1 2 3 4 5 6 7 8 9 0 5 10 15 20 25 30 35 valence number The number of 9 point of valence 1 2 3 4 5 6 7 8 9 0 2 4 6 8 10 12 14 16 18 20 arousal number The number of 9 point of arousal Fig 2 The valence and arousal distribution in 9-pointed rating scale 1 2 3 0 5 10 15 20 25 30 35 3 levels of valence number The number of 3 levels of valence 1 2 3 0 5 10 15 20 25 3 levels of arousal number The number of 3 levels of arousal Fig 3 The valence and arousal distribution in 3-pointed rating scale IV had been discovered  TABLE IV T HE RULES BETWEEN LIGHTNESS  CHROMA  HUE AND VALENCE No Rule conf Sup Rule 1 Lightness=5,Hue  3   class  2 100.00 9.52 Rule 2 Lightness=3   class  2 91.67 26.19 Rule 3 Chroma=4   class  2 90.00 21.43 Rule 4 Lightness=2   class  1 87.50 16.67 Rule 5 Hue=4   class=2 85.71 14.29 Rule 6 Hue=7   class  2 85.71 14.29 In this table a rule listed as the follow format conditions 025  result which means that if the condition was satisﬁed then the result is acquired and conf was used to indicate the conﬁdence and the sup was the support As show in Table IV if chroma had value 3 and lightness had value 5 or lightness had value 3 or chroma had value 4 or hue had value 4 or hue had value it would lead to the class 2 If lightness had value 2 it would lead to the class 1 Next these rules were used to predict the left 10 cases 
271 


were used to test its performance then 8 cases were correctly predicted the over correct rate was 80.00 C The relationships between arousal and 3 color attributes 42 cases which included lightness chroma hue and arousal were input to the CBA After learning 17 rules listed in the Table V had been discovered  TABLE V T HE RULES BETWEEN LIGHTNESS  CHROMA  HUE AND AROUSAL No Rule conf Sup Rule 1 Hue=8   class=1 100.00 7.14 Rule 2 Lightness  4,Chroma  1   class  1 100.00 7.143 Rule 3 Chroma=6   class=3 100.00 4.76 Rule 4 Lightness=4   class=2 100.00 4.76 Rule 5 Chroma=3,Hue=4   class=1 100.00 4.76 Rule 6 Lightness=3,Hue=7   class=1 100.00 4.76 Rule 7 Lightness=4,Hue=3   class=1 100.00 4.76 Rule 8 Chroma=3,Lightness=5   class=2 100.00 4.762 Rule 9 Hue=4,Chroma=7   class=3 100.00 2.38 Rule 10 Hue=3,Chroma=5   class=3 100.00 2.381 Rule 11 Chroma=2   class=1 90.00 21.43 Rule 12 Chroma=4   class=2 85.71 14.29 Rule 13 Lightness=2   class=1 83.33 11.91 Rule 14 Lightness=3 chroma=3   class=3 71.43 11.91 Rule 15 Lightness=5   class=2 62.50 11.91 Rule 16 Chroma=3   class=1 61.54 19.05 Rule 17 Hue=6   class=2 57.14 9.52 As showed in Table V if the hue had value 8 or lightness had value 4 and chroma had value 1 or hue had value 4 and chroma had value 3 or hue had value 7 and lightness had value 3 or hue had value 3 and lightness had value 4 or chroma had value 2 or 3 or lightness had value 2 it would lead to class 1 If the lightness had value 4 or chroma had lightness 3 and lightness had value 5 or chroma had value 4 or hue had value 6 it would lead to class 2 If the chroma had value 6 or chroma had value 3 and lightness had value 3 it would lead to class 3 Next these rules were used to predict the left 10 cases were used to test its performance then 9 cases were correctly predicted the over correct rate was 90.00 It can been seen that the two correct prediction rates is higher than 50.00 That is to say the relationships between color attributes and emotional dimension are not random In general some relationships do exist between color attributes and emotional dimension III C ONCLUSION Color affects our emotions and feelings It is worthwhile to study the relationships between color attributes and emotional dimensions In this paper psychological experiments were designed to gather user’s emotional response to color samples CBA were used to discover rules between color attributes and emotional dimensions then these rules were used to predict valence and arousal of emotion response to a new color stimulus A good result was obtained with correct prediction rate of 90.00 for arousal and 80.00 for valence This paper analyzed the relationships between color attributes and user’s emotional response Next we will further analyze the impacts of the color attributes to the user’s physiological responses These result could be directly applied in many elds A CKNOWLEDGMENT The authors would like to thank Peter J.Lang who provided IAPS pictures and all the obser vers who participated in the experiments The work in this paper was supported by an open project of computing and communicating software key lab in Anhui province and funded by Nature Science Foundation of Anhui Province research grant 070412056 R EFERENCES  J  H  X in K  M  C heng G T a yl or T.Sato A Hansuebsai Cross-Regional Comparison of Colour Emotion Part I Quantitative Analysis Color Research and Application 29 451-457 2004  L i-Chen Ou M  R onnier L uo Andre W oodcock Angela W r ight A study of colour emotion and colour preference Part II Colour emotions for two-colour combinations  Color Research and Application Volume 29 Issue 4  Pages 292 298\(2004  L i-Chen Ou M  R onnier L uo Andre W oodcock and A ngela W r ight A Study of Colour Emotion and Colour Preference  Part III Colour Preference Modeling Color Research and Application Volume 29 Issue 5  Pages 381 389\(2004  B  M a n a v  Color-Emotion Associations and Color Prefernces A Case Study for Residences  Color Research and Application Volume 32 Issue 2  pages 144 150 2007  N a z KA Y A a n d H e l e n H E PPS Color-emotion association Past experience and personal preference  AIC 2004 Color and Paints Interim Meeting of the International Color Association Proceedings  S uc hi t r a S ue e p ra sa n Pi sut S ri m o rk A ra n H a n sue b sa i  T e t s uya Sa t o  Pontawee Pungrassamee Quantitative Analysis of Thai Sensation on Colour Combination  Proceedings of the AIC 2004 Color and Paint Porto Alegre Brazil 35-38 2004 11  X iao-Ping Gao J ohn H Xin T e ts uya Sato Aran Hansuebsai Marcello Scalzo Kanji Kajiwara Shing-Sheng Guan J Valldeperas Manuel Jos Lis Monica Billger Analysis of Cross-Cultural Color Emotion  Color Research  Application 32 223-229 2007 6 8 D ekan COLOR AND EMOTION A Study on the Affective Judgment of Color Across Media and in Relation to Visual Stimuli   L ang P   B radle y  M   Cuthbert B  International affective picture system iaps instruction manual and affective ratings  Technical report A-6 University of Florida 2005  Bing L i u W ynne Hs u Y i m i ng Ma Integrating Classiﬁcation and Association Rule Mining  1998 American Association for Artiﬁcial Intelligence  Y i n X  H an J  CPAR Classiﬁcation based on Predictive Association Rules  Proceedings of the 2003 SIAM International Conference on Data Mining SDM03 San Francisco CA May 2003 
272 


4 Itemsets and Decision Trees So far we have discussed how to transmit binary data by using decision trees In this section we present how to selec t the itemsets representing the dependencies implied by the decision trees We will use this link in Section 5 A similar link between itemsets and decision trees is explored in 27 although our setup and goals are different Given a leaf L  the dependency of the item a t is captured in the coding table of L  Hence we are interested in 002nding itemsets that carry the same information That is itemsets from which we can compute the coding table To derive the codes for the leaf L it is suf\002cient to compute the probability q D  a t  1 j L   q D  a t  1  L  q D  L   1 Our goal is to express the probabilities on the right side of the equation using itemsets In order to do that let P be the path from L to its root Let pos  L  be the items along the path P which are tested positive Similarly let neg  L  be the attributes which are tested negative Using the inclusion-exclusion principle we see that q D  L   q D  pos  L   1  neg  L   0  X V 022 neg  L   000 1 j V j f r  pos  L   V   2 We compute q D  a t  1  L  in a similar fashion Let us de\002ne sets  L  for a given leaf L to be sets  L   f V  pos  L  j V 022 neg  L  g  f V  pos  L   f a t g j V 022 neg  L  g  Combining Eqs 1ñ2 we see that the collection sets  L  satis\002es our goal Proposition 6 The coding table associated with the leaf L can be computed from the frequencies of sets  L   Example 7 Let L 1  L 2  and L 3 be the leaves from left to right of T 4 in Figure 1\(d Then the corresponding families of itemsets are sets  L 1   f a ac g  sets  L 2   f b ab bc abc g  and sets  L 3   f  a b ab c ac bc abc g  We can easily see that the family sets  L  is essentially the smallest family of itemsets from which the coding table can be derived uniquely Proposition 8 Let G 6  sets  L  be a family of itemsets Then there are two data sets say D 1 and D 2  for which q D 1  a t  1 j L  6  q D 2  a t  1 j L  but f r  G  D 1   f r  G  D 2   Given a tree T we de\002ne sets  T  to be sets  T   S L 2 lvs  T  sets  L   We also de\002ne sets  T   S i sets  T i  where T  f T 1      T K g is a decision tree model 5 Choosing Good Itemsets The connection between itemsets and decision trees made in the previous section allows us to consider an orthogonal approach to identify good itemsets Informally our goal is to construct decision trees from a family of itemsets F  selecting the subset from F that provides the best compression of the data More formally our new approach is as follows given a downward closed family of itemsets F  we build a decision tree model T  f T 1      T K g providing a good compression of the data with sets  T  022 F  Before we can describe our main algorithm we need to introduce some further notation Firstly given two trees T p and T n not using attribute c  we de\002ne J OIN T REE  c T p  T n  to be the join tree with c as the root node T p as the positive branch of c  and T n as the negative branch of c  Secondly to de\002ne our search algorithm we need to 002nd the best tree bt  a t  S F   arg min T f c  T  j t  T   a t  src  T  022 S sets  T  022 F g  that is bt  a t  S F   returns the best tree for a t for which the related sets are in F and only splits on attributes in S  To compute the optimal tree bt  a t  S F   we use the exhaustive method presented originally in 27  g i v e n i n A l gorithm 2 The algorithm is straightforward it tests each valid item as the root and recurses itself on both branches Algorithm 2 G ENERATE algorithm for calculating bt  a t  S F   that is the best tree T for a t using only S as source and having sets  T  022 F  1 B  S   S F   2 C  T RIVIAL T REE  a t   3 for b 2 B do 4 G  f X 000 b j b 2 X 2 F g  5  D p  D n   S PLIT  D b   6 T p  G ENERATE  a t  G  S D p   7 T n  G ENERATE  a t  G  S D n   8 C  C  J OIN T REE  b T p  T n   9 end for 10 return arg min T f c  T  j T 2 Cg  We can now describe the actual algorithm for constructing decision tree models with a low cost Our method automatically discovers the order in which the attributes can be transmitted most succinct For this it needs to 002nd sets of attributes S i for each attribute a i such that these should be encoded before a i  The collection S  f S 1      S K g should de\002ne an acyclic graph and the actual trees are bt  a i  S i  F   We use c  S  as a shorthand for the total complexity P i c  bt  a i  S i  F  of the best model built from S  
592 
592 


We construct the set S iteratively At the beginning of the algorithm we have S i   and we increase the sets S i one attribute at a time We allow ourselves to mark the attributes The idea is that once the attribute a i is marked then we are not allowed to augment S i any longer At the beginning none of the nodes are marked To describe a single step in the algorithm we consider a graph H   v 0      v K   where v 1      v K represent the attributes and v 0 is a special auxiliary node We start by adding edges  v i  v 0  having the weight c  bt  a i  S i  F   thus the cost of the best tree possible from F using only the attributes in S i  Then for each unmarked node v i we 002nd out what other extra attribute will help most to encode it succinct To do this we add the edge  v i  v j  for each v j with the weight c  bt  a i  S i  f a j g  F   Now let U be the minimum directed spanning tree of H having v 0 as the sink Consider an unmarked node v i such that  v i  v 0  2 E  U   That node is now the best choice to be 002xed as it helps to encode the data best We therefore mark attribute a i and add a i to each S j for each ancestor v j of v i in U  This process is repeated until all attributes are marked The details of t he algorithm are given in Algorithm 3 Algorithm 3 The algorithm S ET P ACK constructs a decision tree model T given a family of itemsets F such that sets  T  022 F  Returns a DAG a family S   S 1      S K  of sets of attributes The trees are T i  bt  a i  S i  F   1 S   S 1      S K             2 r   r 1      r K    false      false   3 V  f v 0      v K g  4 while there exists r i  false do 5 E    6 for i  1      K do 7 E  E   v i  v 0   8 w  v i  v 0   c  bt  a i  S i  F   9 if r i  false then 10 for j  1      K do 11 T  bt  a i  S i  f a j g  F   12 if c  T  024 w  v i  v 0  then 13 E  E   v i  v j   w  v i  v j   c  T   14 end if 15 end for 16 end if 17 end for 18 U  dmst  V E  f Directed Min Spanning Tree g 19 for  v i  v 0  2 E  U  and r i  false do 20 r i  true  21 for v j is a parent of v i in U do 22 S j  S j  a i  23 end for 24 end for 25 end while 26 return S  The marking of the attributes guarantees that there can be no cycles in S  In fact the marking order also tells us a valid order for transmitting the attributes Further as at least one attribute is marked at each step this guarantees that th e algorithm terminates in K steps Let S be the collection of sources The following proposition tells us that the augmentation performed by S ET P ACK does not compromise the optimality of collections next to S  Proposition 9 Assume the collection of sources S  f S 1      S K g  Let O  f O 1      O K g be the collection of sources such that S i 022 O i and j O i j 024 j S i j  1  Let S 0 be the collection that Algorithm 3 produces from S in a single step Then there is a collection S 003 such that S 0 i 022 S 003 i and that c  S 003  024 c  O   Proof Let G be the graph constructed by Algorithm 3 for the collection S  Construct the following graph W  For each O i such that O i  S i add the edge  v i  v 0   For each O i 6  S i add the edge  v i  v j   where f a j g  O i 000 S i  But W is a directed spanning tree of G  Let U be the directed minimum spanning tree returned by the algorithm Let S 003 i  S 0 i if  v i  v 0  2 E  U  and S 003 i  S 0 i  f a j g if  v i  v j  2 E  U   Note that S 003 de\002nes a valid model and because U is optimal we must have c  S 003  024 c  O   Corollary 10 Assume that F is a family of itemsets having 2 items at maximum The algorithm S ET P ACK returns the optimal tree model Let us consider the complexity of the algorithms The algorithm S ET P ACK runs in a polynomial time By using dynamic programming we can show that G ENERATE runs in O  j F j 2  time We also tested a faster variant of the algorithm in which the exhaustive search in G ENERATE is replaced by the greedy approach similar to the ID3 algorithm We call this variant S ET P ACK G REEDY  6 Related Work Finding interesting itemsets is a major research theme in data mining To this end many measures have been suggested over time A classic measure for ranking itemsets is frequency for which there exist ef\002cient search algorithms 2 15  O t h e r m e a s u r e s i n v o l v e c o m p a r i n g h o w much an itemset deviates from the independence assumption 1,3,4,11  I n y e t o t h e r a p p r o a c h e s m o r e 003 e x i b l e m o d els are used such as Bayes networks 17 18  M a x i m u m Entropy estimates 24 31  R e l a t e d a r e a l s o l o w e n t r o p y sets itemsets for which the entropy of the data is low 16  Many of these approaches suffer from the fact that they require a user-de\002ned threshold and further that at low thresholds extremely many itemsets are returned many of which convey the same information To address the latter 
593 
593 


problem we can use closed 28 o r n o n d e r i v a b l e  6  i t e m sets that provide a concise representation of the original itemsets However these methods deteriorate even under small amounts of noise Alternative to these approaches of describing the pattern set there are methods that instead pick groups of itemsets that describe the data well As such we are not the 002rst to embrace the compression approach to data mining 12  R e cently Siebes et al 30 i n t r o d u c e d t h e M D L b a s e d K RIMP algorithm to battle the frequent itemset explosion at low support thresholds It returns small subsets of itemsets th at together capture the distribution of the data well These code tables have been successfully applied in classi\002cation 22  m e a s u r i n g t h e d i s s i m i l a r i t y o f d a t a  3 3   a n d d a t a generation 34  W h i l e t h e s e a p p l i c a t i o n s s h o w s t h e p r a c ticality of the approach K RIMP can only describe the patterns between the items that are present in the dataset On the other hand we consider the 0 s and the 1 s in the data symmetrically and hence we are able to provide more detailed descriptions of the data including patterns betwee n the presence and absence of items More different from our methods are the lossy data description approaches These strive to describe just part of the data and as such may overlook important interactions Summarization 7 i s a c o m p r e s s i o n a p p r o a c h t h a t i d e n t i 002 e s a group of itemsets such that each transaction is summarized by one set with as little loss of information as possible Yet different are pattern teams 20  w h i c h a r e g r o u p s o f m o s t informative lengthk itemsets 19  s e l e c t e d t h r o u g h a n e x ternal interestingness measure As this approach is computationally intensive the number of team members is typically  10  Bringmann et al 5 p r o p o s e d a s i m i l a r s e l e c tion method that can consider larger pattern sets However it also requires the user to choose a quality measure to which the pattern set has to be optimized unlike our parameterfree and lossless method Alternatively we can view the approach in this paper as building a global model for data and then selecting the itemsets that describe the model This approach then allows us to use MDL as a model selection technique In a related work 32 t h e a u t h o r s b u i l d d e c o m p o s a b l e m o d e l s i n o r d e r to select a small family of itemsets that model the data well The decision trees returned by our methods and particularly the DAG that they form have a passing resemblance to Bayes networks 9  H o w e v e r  a s b o t h t h e m o d e l c o n struction and complexity weighing differ strongly so do th e outcomes To be more precise in our case the distributions p  x par  x  are modeled and weighted via decision trees whereas in the Bayes network setup any distribution is weighted equally Furthermore we use the correspondence between the itemsets and the decision trees to output local patterns as opposed to Bayes networks which are traditionally used as global models 7 Experiments This section contains the results of the empirical evaluation of our methods using toy and real datasets 7.1 Datasets For the experimental validation of the two packing strategies we use a group of datasets with strongly differing stati stics From the LUCS/KDD repository 8 w e t o o k a n u m b e r of often used databases to allow for comparison to other methods To test our methods on real data we used the Mammals presence database and the Helsinki CS-courses dataset The latter contains the enrollment records of students taking courses at the Department of Computer Science of the University of Helsinki The mammals dataset consists of the absence/presence of European mammals 25 in geographical areas of 50x50 kilometers 1 The details of these datasets are provided in Table 1 Table 1 Statistics of the datasets used in the experiments Dataset j D j K  of 1's anneal 898 71 20.1 breast 699 16 62.4 courses 3506 98 4.6 mammals 2183 40 46.9 mushroom 8124 119 19.3 nursery 12960 32 28.1 pageblocks 5473 44 25.0 ticñtacñtoe 958 29 34.5 7.2 Experiments with Toy Datasets To evaluate whether our method correctly identi\002es in\dependencies we start our experimentation using two arti\002cial datasets of 2000 transactions and 10 items For both databases the data is generated per transaction and the presence of the 002rst item is based on a fair coin toss For the 002rst database the other items are similarly generated However for the second database the presence of an item is 90 dependent on the previous item As such both datasets have item densities of about 50 If we apply G REEDY P ACK  our greedy decision tree building method to these datasets we see that it is unable to compress the independent database at all Opposing the dependently generated dataset can be compressed into only 1 The full version of the dataset is available for research pur poses upon request http://www.european-mammals.org  
594 
594 


Table 2 Compression number of trees and numbers of extract ed itemsets for the greedy algorithm G REEDY P ACK K RIMP Dataset c  T b  bits c  T  bits c  T  c  T b    trees  sets minñsup  sets  bits ratio  anneal 23104 12342 53.4 71 1203 1 102 22154 34.6 breast 8099 2998 37.0 16 17 1 30 4613 16.9 courses 76326 61685 80.8 98 1230 2 148 71019 79.3 mammals 78044 50068 64.2 40 845 200 254 90192 42.3 mushroom 442062 115347 26.1 119 999 1 424 231877 20.9 nursery 337477 180803 53.6 32 3409 1 260 258898 45.5 pageblocks 15280 7611 49.8 44 219 1 53 10911 5.0 ticñtacñtoe 25123 14137 56.3 29 619 1 162 28812 62.3 50 of the original number of bits Inspection of the resulting itemsets show that the resulting model correctly de scribes the dependencies in detail The resulting 19 itemsets are f a 1      a 10  a 1 a 2      a 9 a 10 g  7.3 The Greedy Method Recall that our goal is to 002nd high quality descriptions of the data Following the MDL principle the quality of the found descriptions can objectively be measured by the compression of the data We present the compressed sizes for G REEDY P ACK in Table 2 The encoding costs c  T  include the size of the encoded data and the decision trees The initial costs as denoted by c  T b   are those of encoding the data using na®\021ve single-node T RIVIAL T REE s Each of these experiments required 1ñ10 seconds runtime with an exception of 60 s for mushroom  From Table 2 we see that all models returned by G REEDY P ACK strongly reduce the number of bits required to describe the data this implicitly shows that good models are returned The quality can be gauged by taking the compression ratios into account In general our greedy method reduces the number of bits to only half of what the independent model requires As two speci\002c examples of the found dependencies in the courses dataset the course Data Mining was packed using Machine Learning  Software Engineering  Information Retrieval Methods and Data Warehouses  Likewise AI and Machine Learning were used to pack the Robotics course Like discussed above our approach and the K RIMP 30 algorithm have stark differences in what part of the data is considered However as both methods use compression and result good itemsets it is insightful to compare the algorithms For the latter we here allow it to compress as well as possible and thus consider candidates up to as low min-sup thresholds as feasible Let us compare between the outcomes of either method For K RIMP these are itemsets for ours it is the combination of the decision trees and the related itemsets We see that K RIMP typically returns fewer itemsets than G REEDY P ACK  However our method returns itemsets that describe interactions between both present and absent items Next we observed that especially the initial K RIMP compression requires many more bits than ours and as such K RIMP attains better compression ratios However if we disregard the ratios and look at the raw number of bits the two methods require we see that K RIMP generally requires twice as many bits to describe only the 1's in the data than G REEDY P ACK does to represent all of the data 7.4 Validation through Classi\014cation To further assess the quality of our models we use a simple classi\002cation scheme 22  F i r s t  w e s p l i t t h e t r a i n i n g database into separate class-databases We pack each of these Next the class labels of the unseen transactions wer e assigned according to the model that compressed it best We ran these experiments for three databases viz mushroom  breast and anneal  A random 90 of the data was used to train the models leaving 10 to test the accuracy on The accuracy scores we noted resp 100 98.0 and 93.4 are fully comparable to and for the second even better than the classi\002ers considered in 22  7.5 Choosing Good Itemsets In this subsection we evaluate S ET P ACK  our itemset selection algorithm Recall that this algorithm selects item sets such that they allow for building succinct encoding decision trees The difference with G REEDY P ACK is that in this setup the resulting itemsets should be a subset of a given candidate family Here we consider frequent itemsets as candidates We set the support threshold such that the experiments with S ET P ACK were 002nished within 1 2 2 hours with an exception of 23 hours for considering the 
595 
595 


Table 3 Compressed sizes and number of extracted itemsets f or the itemset selection algorithms Candidate Itemsets S ET P ACK S ET P ACK G REEDY K RIMP Dataset min-sup  sets c  T  c  T  c  T b    sets c  T  c  T  c  T b    sets  bits  sets anneal 175 8837 20777 89.9 103 20781 89.9 69 31196 53 breast 1 9920 5175 63.7 42 5172 63.9 49 4613 30 courses 55 5030 64835 84.9 268 64937 85.1 262 73287 93 mammals 700 7169 65091 83.4 427 65622 84.1 382 124737 125 mushroom 1000 123277 313428 70.9 636 262942 59.5 1225 474240 140 nursery 50 25777 314081 93.0 276 314295 93.1 218 265064 225 pageblocks 1 63599 11961 78.3 92 11967 78.3 95 10911 53 ticñtacñtoe 7 34019 23118 92.0 620 23616 94.0 277 28957 159 large candidate family for mushroom  For comparison we use the same candidates for K RIMP  We also compare to S ET P ACK G REEDY  which required 1ñ12 minutes 7 minutes typically with an exception of 2 1 2 hours for mushroom  Comparing the results of this experiment Table 3 with the results of G REEDY P ACK in the previous experiment we see that the selection process is more strict now even fewer itemsets are regarded as interesting enough Large candidate collections are strongly reduced in number up to three orders of magnitude On the other hand the compression ratios are still very good The reason that G REEDY P ACK produces smaller compression ratios is because it is allowe d to consider any itemset Further the fact alone that even with this very strict selection the compression ratios are generally well below 90 show that these few sets are indeed of high importance to describing the major interactions in the data If we compare the number of selected sets to K RIMP  we see that our method returns in the same order as many itemsets These descriptions require far less bits than tho se found by K RIMP  As such ours are a better approximation of the Kolmogorov complexity of the data Between S ET P ACK and S ET P ACK G REEDY the outcomes are very much alike this goes for both the obtained compression as well as the number of returned itemsets However the greedy search of S ET P ACK G REEDY allows for much shorter running times 8 Discussion The experimentation on our methods validates the quality of the returned models The models correctly detect dependencies in the data while ignoring independencies Only a small number of itemsets is returned which are shown to provide strong compression of the data By the MDL principle we then know these describes all important regularities in the data distribution in detail ef\002ciently and witho ut redundancy This claim is further supported by the high classi\002cation accuracies our models achieve The G REEDY P ACK algorithm generally uses more itemsets and obtains better packing ratios than S ET P ACK  While G REEDY P ACK is allowed to use any itemset S ET P ACK may only use frequent itemsets This suggests that we may able to achieve better ratios if we use different candidates  for example low-entropy sets 16  The running times of the experiments reported in this work range from seconds to hours and depend mainly on the number of attributes and rows of the datasets The exhaustive version S ET P ACK may be slow on very large candidate sets however the greedy version S ET P ACK G REEDY can even handle such families well Considering that our curren t implementation is rather na®\021ve and the fact that both methods are easily parallelized both G REEDY P ACK and S ET P ACK G REEDY are suited for the analysis of large databases The main outcomes of our models are the itemsets that identify the encoding paths However the decision trees from which these sets are extracted can also be regarded as interesting as these provide an easily interpretable view o n the major interactions in the data Further just consideri ng the attributes used in such a tree as an itemset also allows for simple inspection of the main associations In this work we employ the MDL criterion to identify the optimal model Alternatively one could consider using either BIC or AIC both of which can easily be applied to judge between our decision tree-based models 9 Conclusions In this paper we presented two methods that 002nd compact sets of high quality itemsets Both methods employ compression to select the group of patterns that describe all interactions in the data best That is the data is considere d symmetric and thus both the 0s and 1s are taken into account in these descriptions Experimentation with our methods 
596 
596 


showed that high quality models are returned Their compact size typically tens to thousands of itemsets allow fo r easy further analysis of the found interactions References 1 C  C  A g g a r w a l a n d P  S  Y u  A n e w f r a m e w o r k f o r itemset generation In Proceedings of the ACM SIGACTSIGMOD-SIGART symposium on Principles of Database Systems PODS  pages 18ñ24 ACM Press 1998 2 R  A g r a w a l  H  M a n n i l a  R  S r i k a n t  H  T o i v o n e n  a n d A  I  Verkamo Fast discovery of association rules In Advances in Knowledge Discovery and Data Mining  pages 307ñ328 AAAI 1996 3 S  B r i n  R  M o t w a n i  a n d C  S i l v e r s t e i n  B e y o n d m a r k e t baskets Generalizing association rules to correlations In ACM SIGMOD International Conference on Management of Data  pages 265ñ276 ACM Press 1997 4 S  B r i n  R  M o t w a n i  J  D  U l l m a n  a n d S  T s u r  D y n a m i c itemset counting and implication rules for market basket data In ACM SIGMOD International Conference on Management of Data  pages 255ñ264 1997 5 B  B r i n g m a n n a n d A  Z i m m e r m a n n  T h e c h o s e n f e w  O n identifying valuable patterns In IEEE International Conference on Data Mining ICDM  pages 63ñ72 2007 6 T  C a l d e r s a n d B  G o e t h a l s  M i n i n g a l l n o n d e r i v a b l e f r e quent itemsets In Proceedings of the 6th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases  pages 74ñ85 2002 7 V  C h a n d o l a a n d V  K u m a r  S u m m a r i z a t i o n c o m p r e s s i n g data into an informative representation In Proceedings of the IEEE Conference on Data Mining  pages 98ñ105 2005 8 F  C o e n e n  T h e L U C S K D D d i s c r e t i s e d  n o r m a l i s e d A R M and CARM data library 2003 9 G  F  C o o p e r a n d E  H e r s k o v i t s  A B a y e s i a n m e t h o d f o r the induction of probabilistic networks from data Machine Learning  9:309ñ347 1992 10 T  C o v e r a n d J  T h o m a s  Elements of Information Theory 2nd ed John Wiley and Sons 2006 11 W  D u M o u c h e l a n d D  P r e g i b o n  E m p i r i c a l b a y e s s c r e e n i n g for multi-item associations In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 67ñ76 2001 12 C  F a l o u t s o s a n d V  M e g a l o o i k o n o m o u  O n d a t a m i n i n g  compression and kolmogorov complexity In Data Mining and Knowledge Discovery  volume 15 pages 3ñ20 Springer 2007 13 P  D  G r  u n w a l d  The Minimum Description Length Principle  MIT Press 2007 14 J  H a n  H  C h e n g  D  X i n  a n d X  Y a n  F r e q u e n t p a t t e r n mining Current status and future directions In Data Mining and Knowledge Discovery  volume 15 Springer 2007 15 J  H a n a n d J  P e i  M i n i n g f r e q u e n t p a t t e r n s b y p a t t e r n growth methodology and implications SIGKDD Explorations Newsletter  2\(2\:14ñ20 2000 16 H  H e i k i n h e i m o  E  H i n k k a n e n  H  M a n n i l a  T  M i e l i k  a i nen and J K Sepp®anen Finding low-entropy sets and trees from binary data In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 350ñ359 2007 17 S  J a r o s z e w i c z a n d T  S c h e f f e r  F a s t d i s c o v e r y o f u n e x p ected patterns in data relative to a bayesian network In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 118ñ127 2005 18 S  J a r o s z e w i c z a n d D  A  S i m o v i c i  I n t e r e s t i n g n e s s o f frequent itemsets using bayesian networks as background knowledge In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 178ñ186 2004 19 A  J  K n o b b e a n d E  K  Y  H o  M a x i m a l l y i n f o r m a t i v e k itemsets and their ef\002cient discovery In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 237ñ244 2006 20 A  J  K n o b b e a n d E  K  Y  H o  P a t t e r n t e a m s  I n Proceedings of the 10th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases  pages 577ñ584 2006 21 P  K o n t k a n e n a n d P  M y l l y m  a k i  A l i n e a r t i m e a l g o r i t h m for computing the multinomial stochastic complexity Information Processing Letters  103\(6\:227ñ233 2007 22 M  v a n L e e u w e n  J  V r e e k e n  a n d A  S i e b e s  C o m p r e s s i o n picks the item sets that matter In Proceedings of the 10th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases  pages 585ñ592 2006 23 M  L i a n d P  V i t  a n y i  An Introduction to Kolmogorov Complexity and its Applications  Springer-Verlag 1993 24 R  M e o  T h e o r y o f d e p e n d e n c e v a l u e s  ACM Trans Database Syst  25\(3\:380ñ406 2000 25 A  J  M i t c h e l l J o n e s  G  A m o r i  W  B o g d a n o w i c z  B Krystufek P J H Reijnders F Spitzenberger M Stubb e J B M Thissen V Vohralik and J Zima The Atlas of European Mammals  Academic Press 1999 26 K  V  S  M u r t h y  On growing better decision trees from data  PhD thesis Johns Hopkins Univ Baltimore 1996 27 S  N i j s s e n a n d  E Fromont Mining optimal decision trees from itemset lattices In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 530ñ539 2007 28 N  P a s q u i e r  Y  B a s t i d e  R  T a o u i l  a n d L  L a k h a l  D i s c o vering frequent closed itemsets for association rules Lecture Notes in Computer Science  1540:398ñ416 1999 29 J  R i s s a n e n  F i s h e r i n f o r m a t i o n a n d s t o c h a s t i c c o m p l e xity IEEE Transactions on Information Theory  42\(1\:40ñ47 1996 30 A  S i e b e s  J  V r e e k e n  a n d M  v a n L e e u w e n  I t e m s e t s t h a t compress In Proceedings of the SIAM Conference on Data Mining  pages 393ñ404 2006 31 N  T a t t i  M a x i m u m e n t r o p y b a s e d s i g n i 002 c a n c e o f i t e m s e t s Knowledge and Information Systems KAIS  2008 Accepted for publication 32 N  T a t t i a n d H  H e i k i n h e i m o  D e c o m p o s a b l e f a m i l i e s o f itemsets In Proceedings of the 12th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases  2008 33 J  V r e e k e n  M  v a n L e e u w e n  a n d A  S i e b e s  C h a r a c t e r i s i ng the difference In ACM SIGKDD Conference on Knowledge Discovery and Data Mining  pages 765ñ774 2007 34 J  V r e e k e n  M  v a n L e e u w e n  a n d A  S i e b e s  P r e s e r v i n g privacy through data generation In Proceedings of the IEEE Conference on Data Mining  pages 685ñ690 2007 
597 
597 


