html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">2004 IEEE  lntemational Conference on Systems, Man and Cybemeticb An Efficient Algorithm to Discover Calendar-based Temporal Association Rules 4 Wan-Jui Lee, Jung-Yi Jiang and Shie-Jue Lee Department of Electrical Engineering National Sun Yat-Sen University, Taiwan wrleel jungyil leesj} @water.ee.nsysu.edu.tw Abstract - This work is to discover all calendar-based temporal association rules that may occur over any rime interval in a temporal database. A user-given calendar schema, e.g.. yea&lt; month, and da ifL the interesting rime intervals as calendarpattems. Then in every time interval, the frequent 2-itemsets are discovered along with their 1-star calendar pattems. After that, infor mation of the rest k-star calendar pattems of the frequent 2-itemsets are levelwisely aggregated from their 1-star cal endarpattems. A minimal set of candidate calendarpattems are generated and counted in the jrs t  scan of database. To avoid multiple scans over the database, all candidate item sets are generated from frequent 2-itemsets and the Apri o n  downward property is utilized to reduce the number of candidate calendar patrems. Finall with their frequent calendar pattems are discovered in one shot. Calendar-based ternporal association rules are then obtained. Experimentol results have shown that our method is more efJicient than others Keywords: calendar-based temporal association rules, peri odic association rules, temporal data mining 1 Introduction With the explosive growth of data available from various sources, automatic discovery of useful knowledge from large databases is becoming more and more popular. Association rule mining [I] finds interesting association or correlation relationships among data items continuously being collected and stored, and many industries are becoming interested in mining association rules from their databases. Traditional mining techniques discover unordered correlations between items from a given database of transactions each of which contains a collection of items activated at a certain time I ,  31. However, temporal patterns which reveal ordered cor relations between transactions are attracting more and more attention. For example, customers might look for chocolates and flowers together on February 14th in every year. This  Supported by the National Science Council under the granls NSC 92-2213-E-110-006 and NSC-92-2213-E-110-007 to .7803-8~66  1 /~$zo .~  @ 2004 IEEE kind of periodic behavior may he recorded in a temporal database. Periodic temporal patterns have been expressed in terms of various forms, e.g., periodical association rules [ 2 cyclic association rules [51, and calendric association rules 4, 61. While periodical and cyclic patterns are basically in terms of a single time granularity, calendar patterns are based on a framework with multiple time granularities. Human ac tivities are usually related to time granularities, e.g., months days, and hours. Therefore, system support and reasoning in volving calendars with multiple granularities have been rec ognized to he an important issue recently In [41, a levelwise Apriori-based algorithm named Temporal-Apriori is the first work proposed to discover all calendar-based temporal association rules. Two interleaving steps are repeated in generating candidate patterns of each level in the method. The first step generates candidate item sets of the current level in every time interval, and the second step checks the support and confidence rates of each candi date itemset. If a candidate itemset passes the support and confidence rates in one time interval, it will he recorded as a frequent itemset in all candidate calendar patterns of the time interval. Frequent itemsets that hold in an enough number of time intervals covered by candidate calendar patterns will he regarded as frequent itemsets in frequent calendar pat 


he regarded as frequent itemsets in frequent calendar pat terns. Then, calendar-based temporal association rules are obtained In this paper, we propose a more efficient method to dis cover all calendar-based temporal association rules. An example of calendar-based temporal association rules is  gloves and scarves are frequently purchased together on every day of December in every year  Unlike levelwise Apriori-based approaches, our method scans the underly ing database at most twice. Moreover, a 1-star aggregation mechanism is proposed to derive the minimal set of candi date calendar patterns that need to he counted in the first scan of database. Our method can he briefly introduced as fol lows. A user-given calendar schema [4], e.g., year, month and day, is firstly adopted to specify the interesting time in tervals as calendar pattems. Then, in every time interval the frequent 2-itemsets are discovered along with their 1-star calendar patterns. After that, information of the rest k-star 3122 calendar patterns of the frequent 2-itemsets are levelwisely aggregated by the 1-star aggregation mechanism. F  utther more , all candidate itemsets are generated from disc:overed frequent 2-itemsets and the Apriori downward property is utilized to reduce the number of their candidate calendar pat terns. Finally, all frequent itemsets along with their frequent calendar patterns are discovered in one shot. Calende-based temporal association rules are then obtained The rest of the paper is organized as follows. In Section 2 we briefly describe the concept of the calendar schema. A formal description about calendar-based temporal associa tion rules is also given. In Section 3, we describe our method in detail. Simulation results are presented in Section 4, and finally, a conclusion is given in Section 5 2 Calendar Schema A calendar is, in general, a structured collection of time intervals. To construct a calendar, the hierarchy of time gran ularities, e.g. day, month, and year, has to be  determined to handle descriptions of multiple time granularities 2.1 Basic Calendars For each time granularity, a Boolean function which de scribes the distribution of all the time intervals in the time granularity can be specified, and the Boolean description forms a basic calendar. For example, workdays of ii week can be described with a Boolean function as shown in Fig ure 1, A basic calendar, A,  characterizes a proposition about the collection of time intervals in a time granularity U ,  de scribed by a Boolean function 6~ where 6A : Ti + {o, 1 for every time interval Ti E U. The function value \(?*\(Ti indicates whether Ti is included in .4. Some exampl,: basic calendars are shown in Figure 2. Usually, the Boolean func tions describing time granularities are arbitrarily speci5ed by the user 2.2 Complex Calendars In real life, complicated temporal expressions, such as the rst day of every month, are required and it is importint for users to easily describe these temporal requirements. Given m time granularities, i.e., U1, U,, ..., U, a complex calen dar, B, characterizes a proposition about the collection of time intervals in multiple time granularities, described by a Boolean function 6~ where 6~ : Ti -+ { 0 , 1 for every time interval T, E U1 x U, x ... x U,. The: func tion value ~ B \( T simplicity, a complex calendar pattern of m time graiiulari ties, U,, U2, ..., U, is represented as &lt; R I ,  Rz, ..., B, &gt where Ri, 1 5 i _&lt; m, can he either a particular integt:r or a wild card symhol    If Ri is an integer, one specific t i a e  in terval in U, is indicated. On the other hand, a    symtrol for Ri indicates all time intervals in U,. Therefore, the time in tervals or periodic cycles can he easily described by calendar 


tervals or periodic cycles can he easily described by calendar patterns with appropriate calendar schemas. For example given a calendar schema \(year : [2001, ..., 20041, month 11, ..., 121, day : [I, ..., 31 be represented with the calendar pattern &lt; *: 2,14 &gt;. Note that, in the calendar schema, each granule of U, is required to be uniquely contained in a granule of Ui-l, V i  For exam ple, the hierarchy of time granularities in Figure 2 is allowed since each day is covered by a month and each month is cov ered by a year. Furthermore, a calendar pattern with exactly k wild-card symbols is called a k-star calendar pattern while a calendar pattern without any wild-card symbol is regarded as a basic time interval under the calendar schema Property 1 The information of a \( k  + 1 tem, &lt; *, *, *, *, *, ..., *, R \( k + q ,  ..., R, &gt;, can be aggre gated from the information of all k-star calendar patterns in lt; *,*, *, *, *, ..., *, R\(k+l k  + 1 Proof Assume that there are totally T time intervals in U\(k+l granularity are indicated with a    symbol. There fore, the aggregation of all k-star calendar patterns &lt I, *, ..., *, R\(k+l k+l 2 &lt; *:*,*,*,*, ..., *&gt;j ,R\(k+Z 1 3=1 lt; *, *, *,*&gt; *&gt; ..., *&gt; *, R \( k + 2 which is the \(k + 1 mation of every k-star calendar pattern is known, the information of the \(k + 1 derived  k+S 2.3 Calendar-based Temporal Association Rules The purpose of our method is to find, from temporal databases, calendar-based temporal association rules which holds in the calendar schema specified by the user. Let Z = {il,i2, ..., zl} be a set of items. Let D be a temporal database of transactions where each transaction t is associ ated with an identifier TID, a time information tt indicating the time when the transaction occurred, and a set of items t ,  such that t ,  C 1. Let D be  divided into a sequence of n partitions, PI,  Pz, . _  _, and P,, each Pi containing a set of transactions occurring in the corresponding time interval Ti with the duration being that of the smallest time granu larity. Mining calendar-based temporal association rules in a database is to discover interesting patterns with calendar based periodicity in D. That is, to discover every associa tion rule which holds in an enough number of time intervals given by the corresponding calendar pattern. An association rule with respect to a time interval, Ti, is an implication of 3123 B zleyr dthB1BBL Figure 1: A Boolean function describing time intervals which are workdays of the week. m:ril ;-I5 5 a, % /. -., 0, a &gt; . I . . . . , 1 3 1 S d.P - I_ Figure 2: Basic calendars associated with the time granularity of \(a h c  year the form x J T d  Y \(2 where X 2 I, Y C_ I ,  and X n Y = 0. Let IP;\(I number of transactions containing itemset I in partition Pi The association rule X  Y is said to have supports% in the partition Pi if IPi\(XUY 3 where lPil denotes the number of transactions in partition Pi For an association rule X J T * Y ,  let IPi\(XUY X 4 The rule is said to hold in Pi or 2  with confidence c%. For a 


The rule is said to hold in Pi or 2  with confidence c%. For a given pair of  confidence and supportthresholds, e% and s and a given time interval Ti, association rules in Ti are those which have confidence and support greater than or equal to c% and s% in Pi, respectively Furthemiore, a calendar-based temporal association rule with respect to a calendar pattern, U ,  is an implication of the form x Y. \(5 Assume that lul time intervals are covered by U  If an asso ciation rule holds in at least m x IuI time intervals covered by U ,  where m is a user-defined match ratio \(0 &lt; m 5 l is said to he a calendar-based temporal association rule that holds in U 3 OurMethod Roughly, our method for discovering all calendar-based temporal association rules can he divided into three phases i.e., discovery of frequent 2-itemsets along with their 1-star candidate calendar patterns, generation of candidate itemsets along with all their k-star candidate calendar patterns, and discovery of frequent itemsets along with their frequent cal endar patterns. In phase l ,  frequent 2-itemsets, Lz, in each partition are discovered with their respecting 1-star candi date calendar patterns V  Note that Lz is empty initially In partition Pi, every frequent 2-itemset in PI is computed and inserted into La.  Also, for each itemset I in La, its first I-star candidate calendar pattern U:  is kept in V  of I and its repeating count in U: is set to 1. In the rest of parti tions, Pz, ..., P,, discovery of frequent 2-itemsets along with their 1-star candidate calendar patterns is iterated as follows Frequent 2-itemsets are firstly computed with three different cases. In case 1, a frequent 2-itemset, I, is not currently in Lz,  and therefore it is inserted into La. Also, U: is kept in V  of I and the repeating count in U: is set to 1. In case 2 the frequent 2-itemset, I, is already in Lz,  but a new 1-star calendar pattern, U:, with respect to the current partition is 31 24 generated. In this case, U! is inserted into V  and I  s repeat ing count in ul is set to 1. In case 3, the frequent 2-itemset I ,  is already in Lz ,  and the respecting 1-star calendar pat tern, u t ,  has also been in V  of I .  We simply increase I  s repeating count in ut by 1 in this case In phase 2, Property 1 is firstly used to aggregate ail other k-star candidate calendar patterns of itemsets in Lz from 1 star candidate calendar patterns. For an itemset I in L2,  its repeating counts in 1-star candidate calendar pattern:;, $k have been derived from phase I ,  and thus its repeating counts in 2-star candidate calendar patterns  s, can be easily ob tained. Similarly, its repeating counts in 3-star candidate cal endarpattems  s, can be aggregated from that in u  s, and so on. Instead of directly generating all k-star candidate cal endar patlerns in phase l ,  our method generates and scans only 1-star candidate calendar patterns in the first scan of the database. Therefore, a smaller number of candidate cal endar patterns are generated and counted in the process of scanning database. Once all candidate calendar pattims of itemsets in LZ are derived, candidate itemsets C k ,  for k 2 3 along with their candidate calendar patterns can funher be generated. Note that two kinds of candidates are generated in this phase, i.e., the candidates of frequent itemsets \(candi date itemsets for short dar patterns \(candidate calendar pattems for short of candidate k-itemsets, i.e., C k  of k &gt;= 3,  are generxted as follows where * is the JOINT operation given in [ I Let the set of candidate calendar patterns, VI. of I he v, = U\(U,21u  7 k&gt;1 Intuitively, the candidate calendar patterns of an itemset I; in C3 can be derived by intersecting the candidate calendar pat terns of itemsets which are the subsets of I  in L2 as shown 


terns of itemsets which are the subsets of I  in L2 as shown below vr: = n\({vr; I VI; I 8 However, by utilizing I  s  repeating counts in Vr, V I  E L2 we can find the minimal set of candidate calendar patterns for each itemset in C,. From E   count of a candidate calendar pattern of an itemset I  in C3 uI;.count, will never be larger than any u p  .count  JI  c I  Therefore, the maximal value of uI;.count, GI:.,iount can be obtained by 9 Furthermore, for each I:, uI: is removed if GI;.cozmt 5 m x IvpI k &gt;= 4, &amp;e obtained by ur:.count = min\({vp.count I VI  c I vrL = n\({v,;-. I v1j-l c I IO In the final phase, to discover the frequent patterns from candidates, all candidate itemsets along with their candidate calendar patterns are counted in the database in one shot Note that a candidate itemset is only scanned in the time intervals covered by its candidate calendar patterns. As a result, a frequent itemset passes the match ratio of a calendar pattem can be found. Calendar-based temporal association rules are then obtained 4 Experimental Results We compare the performance of our method with that of the  Temporal-Apriori  algorithm proposed in [4] by run ning them on several experiments with a PC with 2.2 GHz CPU and 1.OG memory. The technique introduced in [ I is used to generate four synthetic datasets, T10.14.D400K T10.14.D600K, T10.14.D800K and T10.14.D1000K, to form input databases to the algorithms in the experiments where T is the mean size of a transaction, I is the mean size of potential maximal large itemsets, and D is the num her of transactions in units of K, i.e. 1000. The calendar schema \(year : [2001, ..., 20041, month : [ l ,  ..., 121, day l, ..., 311 is 0.8. These four datasets are divided into, 400, 600, 800 and 1000, partitions, respectively, where each partition con tains 1000 transactions and is corresponding to one basic time interval in the calendar schema. In the following, we firstly give one experiment to compare the performance of Temporal-Apriori and our method with different scales on data size and support threshold. Then, to demonstrate the effectiveness of our 1-star aggregation mechanism, another experiment is given to compare the performance of generat ing all k-star and only 1-star candidate calendar patterns of LZ in the first database scan 4.1 Experiment 1 In experiment 1, to compare Temporal-Apriori and our method, we use both methods to discover calendar-based temporal association rules in four datasets with different sizes mentioned above on different scales of support thresh old. In Figure 3, the execution time of Temporal-Apriori applying to different datasets with support thresholds, 0.05 0.06,0.07,0.08,0.09 and 1 ,  respectively, are drawn in dotted lines while our method are drawn in solid lines. Moreover in Figure 4 and Figure 5 ,  the black pillars represent the av erage number of candidate itemsets and candidate calendar patterns generated by Temporal-Apriori in these four differ ent databases, respectively. The gray pillars are those gen erated by our method. As for the white ones, they indicate the average number of frequent itemsets and frequent cal endar patterns discovered by both methods in the database From these three figures, we can see that though our method generates slightly more candidates than Temporal-Apriori we have better performance with smaller support thresholds For smaller support thresholds, the length of maximal fre quent itemsets will be longer. Therefore, the number of database scans in Temporal-Apriori increases when the sup 3125 


x lo 0.05, 0.06 0.07 0.m om 0.1 support t h m h o l d Figure 3: Performance comparison between Temporal Apriori and our method port threshold decreases. Meanwhile, our method always scans the database at most twice. As a result, our method is efficient in all kinds of support thresholds, but the perfor mance of Temporal-Apriori varies dramatically for different cases 4.2 Experiment 2 To demonstrate the effectiveness of our 1-star aggrega tion mechanism, we compare the difference between apply ing and not applying the mechanism into discovering candi date calendar patterns in the first scan of database. Figure 6 shows the average number of 1-star and all k-star calendar patterns discovered in the first database scan with four differ ent datasets. Since 1-star calendar patterns is the subset of all k-star patterns, it is clear that the number of 1-star calendar patterns will be smaller than that of all k-star patterns. For the calendar schema used in our experiments. the number of 1-star calendar patterns is close to 90% of all k-star pattems But the percentage of 1-star calendar patterns will be smaller if a more complex calendar schema, e.g., a schema with more time granularities or more time intervals in one granularity is used. Though the number of 1-star calendar patterns is ev idently smaller than that of all k-star calendar patterns, the efficacy of the mechanism can not be directly perceived eas ily. Thus, the execution time for discovering all candidate calendar patterns of large 2-itemsets by scanning 1-star and all star candidate calendar patterns in the database, respec tively, are given in Figure 7. The results obtained by utilizing 1-star calendar pattern mechanism are obviously better than those obtained by not applying the mechanism 5 Conclusion We have proposed an algorithm to discover all calendar based temporal association rules that occur over any time 1200 0 07 SUPP Figure 4: Average number of candidate itemsets and frequent itemsets generated by Temporal-Apriori and our method om 007 008 om 0 1 suppon threshold Figure 5: Average number of candidate calendar patterns and frequent calendar patterns generated by Temporal-Apriori and our method 31 26 io I I 1 star candidate calendar patterns I all 611, candtdats calendar p a t l e d  '7 n i om DO9 0 1  I uppod threshold Figure 6: Average number of 1-star and all star carididate calendar patterns generated in the first database scan Figure I: Execution time for discovering candidate calendru pattems of large 2-itemsets by scanning I-star and a l l  star candidate calendar patterns in the database, respectively interval in a temporal database. An example of a calendar based temporal association rule is "chocolates and flowers are frequently purchased together on February 14th in ev ery year." A user-given calendar schema, e.g., year, month and day, is firstly adopted to specify the interested time in tervals as calendar pattems. Then, in every time interval the frequent %itemsets are discovered along with their 1-star calendar patterns. After that, the information of the rest k star calendar pattems of the frequent 2-itemsets are aggre gated from their 1-star calendar patterns. Thus, the mini mal number of calendar pattems are generated and counted in the database. Further, to avoid multiple scans over the database, all candidate itemsets are generated from discov ered frequent 2-itemsets and the Apriori downward property 


ered frequent 2-itemsets and the Apriori downward property is utilized to generate the minimal number of their candidate calendar patterns. Finally, all frequent itemsets and their cal endar patterns are discovered in one shot. Calendar-based temporal association rules are then obtained. Experimental results have shown that our method is more efficient than others References 11 R. Agrawal and R. Srikant. Fast Algorithms for Min ing Association Rules. In Proceedings of the Inrema tional Very Large Database Conference , pages 487 499,1994 2 ]  J. Han, G. Dong, and Y. Yin. Efficient Mining of Par tial Periodic Patterns in Time Series Databases. In Pro ceedings of the Inremational Conference on Data En gineering, pages 106-1 15, 1999 3] C. H. Lee, C. R. Lin and M. S. Chen. Sliding-Window Filtering: An Efficient Algorithm for Incremental Min ing. In Pmceedings of the ACM 10th Intemational Conference on Information and Knowledge Manage ment, pages 263-270,2001 4] Y. Li, P. Ning, X. S. Wang and S .  Jajodia. Dsicovering Calendar-based Temporal Association Rules. Data and Knowledge Engineering, Vo1.44, No.2, pages 193-21 8 2003 5] B. Ozden, S. Ramaswamy, and A. Silberscbatz. Cyclic Association Rules. In Proceedings of the 15th Inter national Conference on Data Engineering, pages 41 2 421,1998 6] S. Ramaswamy, S. Mahajan, and A. Silberschatz. On the Discovery of Interesting Patterns in Association Rules. In Proceedings of the International Very Large Database Corference , pages 368-379,1998 7] J. F. Roddick and M. Spiliopoulou. A Survey of Tem poral Knowledge Discovery Paradigms and Methods IEEE Trans. Knowledge and Data Engineering, Vol 14, Issue 4., pages 75C!-767,2002 3127 pre></body></html 


decreased. However, refer to Fig.6, it brings the following problem  110 100 010 100 100 011 001 111 100 011 100 34 33 34 DSD u u u                         u              101 100 001 100 100 010 011 111 100 011 100 


100 34 33 34 DSD u u u                          u             Fig.6: Over Hiding problem of setting  1 in S No matter the left-hand or right-hand equation, the support of {1, 2} in D' is 0. That is, item 1 and item 2 never appear toge ther, and they are mutual exclusive! This situation almost never happens in the normal database. The attackers may interest in this situation and infer that {1, 2} is hidden deliberately. To hide the sensitive patterns, only need to make their supports smaller than minimum support and need not to decrease their support to 0. To solve the problem, we inject a probability ? which is called Distortion probability into this approach. Distortion probability is used only when the column j of the sanitization matrix S contains only one  1  i.e. Sjj = 1 0 1 d   m k k j i k  S D  m j n i j i  d d d d   1  1     D  i j  h a s   j probability to be set to 1 and 1  j probability to be set to 0 Lemma 1: Given a minimum support ?, and a level of confidence c. Let {i, j} be a pattern in Marked-Set, nij be the support count of {i, j}. ? is the Distortion probability of column j Without loss of generality, we assume that Sij  1. If ? satisfies    D n i j  u  u  V U   a n d    


   c x n xijnx D x ij t          u    1 0 UU V where D| is the number of transactions in D, we can say that we are c confident that {i, j} isn  t frequent in D Proof: If probability policies are not considered and Sij  1, that is, while a row \(transaction according to the matrix multiplication discussed in the previous section, D'tj will be set to 0. We can view the nij original jtiD jtjtjt ijn DDD 21     f o r  e a c h  r o w  t i  c o n t a i n s   i   j   i n  D  a s  realizations of nij independent identically distributed \(i.i.d om variables ijn X X X      2 1      e a c h  w i t h  t h e  s a m e  d i s t r i b u  tion as Bernoulli distribution, B \(1 X i    1  a n d  t h e  f a i l u r e  i s  d e f i n e d  a s  X i    0     i    1  t o  n i j   T h e  p r o b  ability ? is attached to the success outcome and 1?? is attached to the failure outcome. Let X be a random variable and could be viewed as the number of transactions which include {i, j} in D such that jin     2 1    T h u s   t h e  d i s t r i b u t i o n  o f  X is the same as Binomial distribution, X?B \(nij              otherwise nx x n XP ij xnxij x ij 0 1,0     U the mean of X = nij?, and the variance of X = nij?\(1 If we want to hide {i, j} in D', we must let its support in D' be smaller than ?. Therefore, the expected value of X must be s m a l l e r  t h a n     D u V    T h a t  i s     m u s t  s a t i s f y     D n i j  u  u  V U   Moreover, the probability of the number of success which is 


Moreover, the probability of the number of success which is equal to or smaller than     1     u  D V   s h o u l d  b e  h i g h e r  t h a n  c  Therefore, U must satisfy    c x n xijnx D x ij t           u    1 0 UU V  If ? satisfies the two equations, we can say that we are c confident that {i, j} isn  t frequent in D When nij &gt; 30, the Central Limit Theorem \(C.L.T used to reduce the complexity of the equation and speed up the execution time of the sanitization process Moreover, if several entries in column j of S are equal to  1 such as Sij  1, Skj  1, Smj  1  etc, several candidate Dist ortion probabilities such as iU , kU , mU , etc are gotten. The Dis tortion probability of column j, ? j, is set to the minimal candid ate Distortion probability to guarantee that all corresponding pair-subpatterns have at least c confident to be hidden in D 3.3. Sanitization Algorithm Fig.7: Sanitization Algorithm According to the probability policies discussed above, the sanitization algorithm D'n  m = Dn  m  Sm  m is showed in Fig.7 Notice that, if the sanitization process causes all the entries in row r of D' equal to 0, randomly choose an entry from some j Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE where Drj = 1, and set D'rj = 1. It is to guarantee that the size of the sanitized database equals the size of the original database 4. Performance Evaluation 4.1. Performance Quantifying There are three potential errors in the problem. First of all some sensitive patterns are hidden unsuccessfully. Secondly some non-sensitive patterns cannot be mined from D'. And the third, new artificial patterns may be produced. However, the third condition never happens in both of our approach and SWA Besides, we also introduce the other two criteria, dissimilarity and weakness. All of the criteria are discussed as follows Criterion 1: some sensitive patterns are frequent in D'. This condition is denoted as Hiding Failure [10], and measured by     DP DP HF H H , where XPH represents the number of patterns contained in PHwhich is mined from database X Criterion 2: some non-sensitive patterns are hidden in D'. It is also denoted as Misses Cost [10], and measured by      DP DPDP MC 


MC H H H      w h e r e        X P H  r e p r e s s ents the number of patterns contained in ~PH which is mined from database X Criterion 3: the dissimilarity between the original and the sanitized database is also concerned, and it is measured by      n i m j ij n i m j ijij D DD Dis 1 1 1 1    Criterion 4: according to the previous section, Forward Inference Attack is avoided while at least one pair-subpattern of a sensitive pattern is hidden. The attack is quantified by        DPDP DPairSDPDP Weakness HH HH    where DPairS is the set of sensitive patterns whose pair subsets can be completely mined from D 4.2. Experimental Results Table 1: Experiment factor The experiment is to compare the performance between our approach and SWA which has been compare with Algo2a [4 and IGA [10] and is so far the algorithm with best performances published and presented in [12] as we know. The IBM synthetic data generator is used to generate experimental data. The dataset contains 1000 different items, with 100K transactions where the average length of each transaction is 15 items. The Apriori algorithm with minimum support = 1% is used to mine the dataset and 52964 frequent patterns are gotten Several sensitive patterns are randomly selected from the frequent patterns with length two to three items to be seeds With the several seeds, all of their supersets are included into the sensitive patterns set since any pattern which contains sensitive patterns should also be sensitive 0 0.05 0.1 0.15 0.2 0.25 0.3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern H id 


id in g F ai lu re SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern W e ak n es s SP SWA Fig.8: Rel. bet. RS and HF. Fig.9: Rel. bet. RS and weakness Fig.8, Fig.9, Fig.10, Fig.11 and Fig.12 show the effect of RS by comparing our work to SWA. Refer to Fig.8, because the level of confidence in our sanitization process takes the minim um support into account, no matter how the distribution of the sensitive patterns, we still are C confident to avoid hiding failure problems. However, there is no correlation between the disclos ure threshold in SWA and the minimum support. Under the sa me disclosure threshold, if the frequencies of the sensitive patte rns are high, the hiding failure will get high too. In Fig.9, beca use our work is to hide the sensitive patterns by decreasing the supports of the pair-subpatterns of the sensitive patterns, the val ue of weakness is related to the level of confidence. However according to the disclosure threshold of SWA, when all the pair subpatterns of the sensitive patterns have large frequencies, it may cause serious F-I Attack problems. Hiding failure and wea kness of SWA change with the distributions of sensitive patterns 0 0.1 0.2 0.3 0.4 0.5 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern M is se s C o st SP SWA 0 0.005 0.01 0.015 0.02 0.025 0.03 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern D is si m 


m il ar it y SP SWA Fig.10: Rel. bet. RS and MC.  Fig.11: Rel. bet. RS and Dis In Fig.10 and Fig.11, ideally, the misses cost and dissimilar ity increase as the RS increases in our work and SWA. However if the sensitive pattern set is composed of too many seeds, a lot of victim pair-subpatterns will be contained in Marked-Set. And as a result, cause a higher misses cost, such as x = 0.04 in Fig.10 Moreover, refer to the turning points of SWA under x = 0.0855 to 0.1006 in Fig.10 and Fig.11. The reason of violation is that the result of the experiment has strong correlation with the distri bution of the sensitive patterns. Because the sensitive patterns Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE are chosen randomly, several variant factors of the sensitive patt erns are not under control such as the frequencies of the sensiti ve patterns and the overlap between the sensitive patterns if the overlap between the sensitive patterns is high, some sensitive patterns can be hidden by removing a common item in SWA Therefore, decrease the misses cost and the dissimilarity 0 0.5 1 1.5 2 2.5 3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern E x ec u ti o n T im e h  SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set H id in g F a il u re SP SWA 


SWA Fig.12: Rel. bet. RS and time. Fig.13: Rel. bet. RL2 and HF Fig.12 shows the execution time of SWA and our approach As shown in the result, the execution time of SWA increases as RS increases. On the other hand, our approach can be separated roughly into two parts, one is to get Marked-Set which is strong dependent on the data, the other is to set sanitization matrix and execute multiplication whose execution time is dependent on the numbers of transactions and items in the database. In the experi ment, because the items and transactions are fixed, therefore, the execution time of our approach is decided by the setting of Marked-Set which makes the execution time changing slightly 0 0.05 0.1 0.15 0.2 0.25 0.3 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set W ea k n es s SP SWA 0 0.1 0.2 0.3 0.4 0.5 0.6 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set M is se s C o st SP SWA Fig.14: Rel. bet. RL2 and weakness. Fig.15: Rel. bet.RL2 and MC 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set D is si m il a ri ty SP SWA Fig.16: Rel. bet. RL2 and Dis Fig.13, Fig.14, Fig.15and Fig.16 show the effect of RL2 Refer to Fig.13 and Fig.14, our work outperforms SWA no matter what RL2 is. And our process is almost 0% hiding failure 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


