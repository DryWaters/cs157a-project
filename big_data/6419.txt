Online Mining of data to Generate Association Rule Mining in Large Databases    Archana Singh  Megha Chaudhary  Dr \(Prof.\ Ajay Rana Gaurav Dubey P h.D Scholar   M.tech\(CS&Engg  Ph.d\(Comp Science&Engg Ph.d Scholar  A mity University   Amity University  Amity University Amity Univeristy N OIDA \(U.P   NOIDA \(U.P 
 NOIDA \(U.P NOIDA\(U.P  91-9958255675   91-981811756 919958759459 
archana.elina@gmail.com  
nicemegha@gmail.com ajay _ rana@amity.edu gdubey1977@gmail.com    ABSTRACT  Data Mining is a Technology to explore data  analyze the data and finally discovering patterns from large data repository. In this paper, the problem of online mining of association rules in large databases is discussed. Online association rule mining can be applied which helps to remove 
redundant rules and helps in compact representation of rules for user  In this paper, a new and more optimized algorithm has been proposed for online rule generation. The advantage of this algorithm is that the graph generated in our algorithm has less edge as compared to th e lattice used in the existing algorithm. The Proposed algorithm generates all the essential rules also and no rule is missing. The use of non redundant association rules help significantly in the reduction of irrelevant noise in the data mining process. This graph theoretic approach, called adja cency lattice is crucial for online mining of data. The adjacency lattice could be stored either in main memory or secondary memory. The idea of 
adjacency lattice is to pre store a number of large item sets in special format which reduces disc I/O required in performing the query   Index Keywords   Adjacency lattice, Associatio n Rule Mining, Data Mining   I   INTRODUCTION   Data Mining is a process of analysis the data and summarizing it into useful information. In other words technically, data mining is the process of finding pattern among dozens of fields in large relational databases. Data mining software is one of a number of analytical tools for analyzing data. It allows users to analyze data from many different dimensions or angles, categorize it, and summarize 
the relationships identified   A. Overview of the Work done    Association rule mining, as suggested by R. Agrawal basically describes relationships between items in data sets. It helps in finding out the items, which would be selected provided certain set of items ha ve already been selected. An improved algorithm for fast rule generation has been discussed Agrawal et. al \(1994\Two algorithms for generating association rules have been discussed in \221Fast Algorithms for Mining Association Rules\222 by Rakesh Agrawal and Srikant \(1994  
 The online mining of data is performed by pre-processing the data effectively in order to make it suitable for repeated online queries. An online association rule mining technique discussed  by Charu C Agrawal at al\(2001 uggests a graph theoretic approach, in which the pre -processed data is stored in such a way that online processing may be done by applying a graph theoretic search algorith m. In this paper concept of adjacency lattice of itemsets has been introduced   This adjacency lattice is crucial in performing effective online data mining. The adjacency lattice could be stored either in a main memory or on secondary memory. The idea of 
adjacency is to pre-store a number of item sets at a level of support. These items are stored in a special format \(called adjacency lattice\which reduces the disk I/O required in order to perform the query   Online generation of the rules deals with the finding the association rules online by changing the value of the minimum confidence value. Problems with the existing algorithm is that the lattice has to be constructed again for all large itemsets, to generate the rules, which is very time consuming for online generation of rule. The number of edges would be more in the generated lattice as we have edges for a frequent itemset to all its supersets in the subsequent levels 
  This paper aims to develop a new algorithm for online rule generation. A weighted directed graph has been constructed and depth first search has been used for rule generation. In the proposed algorithm, online rules can be generated by generating adjacency matrix for some confidence value and the generating rules for confidence measure higher than that used for generating the adjacency matrix        


A new algorithm has been developed to overcome these difficulties. In this algorithm the number of edges graph generated is less than the adjacency lattice and it is also capable of finding all the essential rules   This paper is divided further into sections as : Section 2 describes the work done by Charu C Agarwal\(2001\Section 3 describes the new proposed algorithm. Section 4 discusses the illustration of Existing and proposed algorithm. In the last para, the comparison between two algorithms with their complexity is found   II EXISTING ALGORITHM FOR ONLINE   RULE GENERATION   The aim of Association Rule Mining \(Rakesh et. al, 1994\is to detect relationships or patterns between specific values of categorical variables in large data sets. Rakesh suggests a graph theoretic approach. The main idea of association rule mining in the existing algorithm is to partition the attribute values into Transaction patterns Basically, this technique enables analysts and researchers to uncover hidden patterns in large data sets. Here the pre-processed data is stored in such a way that online rule generation may be done with a complexity proportional to the size of the output. In the existing algorithm, the concept of an adjacency lattice of itemsets has been introduced. This adjacency lattice is crucial to performing effective online data mining. The adjacency lattice could be stored either in main memory or on secondary memory. The idea of the adjacency lattice is to prestore a number of large itemsets at a level of support possible given the available memory. These itemsets are stored in a special format \(called the adjacency lattice\which reduces the disk I/O required in order to perform the query. In fact, if enough main memory is available for the entire adjacency lattice then no I/O may need to be performed at all   A Adjacency lattice   An itemset X is said to be adjacent to an itemset Y if one of them can be obtained from the other by adding a single item Specifically, an itemset X is said to be a parent of the itemset Y if Y can be obtained from X by adding a single item to the  set X It is clear that an itemset may possibly have more than one parent and more than one child. In fact, the number of parents of an itemset X is exactly equal to the cardinality of the set X This observation follows from the fact that for each   Element i r in an itemset X, X -i r is a parent of X In the lattice if a directed path exists from the vertex corresponding to Z to the vertex corresponding to X in the adjacency lattice, then   X    Z In such a case X is said to be a descendant of Z and Z is  said to be an ancestor of X   B. The Existing Algorithm   There are three steps in the Existing algorithm explained by \(Agarwal et al. 1994   STEP 1 Generation of adjacency lattice   The Adjacency lattice is created using the frequent itemsets generated using any standard algorithm by defining some minimum support. This support value is called primary  threshold value. The itemsets obtained above are referred as prestored itemsets  and can be stored in main memory or secondary memory. This is beneficial in the sense that we need not to refer dataset again and again from different value of the min. support and confidence given by the user  The adjacency lattice L is a directed acyclic graph. An itemset X is said to be adjacent to an itemset Y if one of them can be  obtained from the other by adding a single item. The adjacency lattice L is constructed as follows   Construct a graph with a vertex v  I or each primary itemset I Each vertex I has a label corresponding to the value of its  support. This label is denoted by S\(I For any pair of vertices corresponding to itemsets X and Y a directed edge exists from v\(X to v\(Y if and only if X is a parent of Y Note that it is not possible to perform online mining of association rules at levels less than the primary threshold    STEP 2  Online Generation of Itemsets  Once we have stored adjacency lattice in RAM. Now user can get some specific large itemsets as he desired. Suppose user want to find all large itemsets which contain a set of items I  and satisfy a level of minimum support s, then there is need to solve the following search in the adjacency lattice. For a given itemset I find all itemsets J such that v\(J is reachable from v\(I by a directed path in the lattice L and satisfies S\(J  s STEP 3 Rule Generation  Rules are generated by using these prestored itemsets for some user defined minimum support and minimum confidence  III PROPOSED ALGORITHM The algorithm by Charu et al. \(2001\s discussed in previous section. Detailed discussion of the proposed algorithm has been done in the current section. Graph theoretic approach has been used in the proposed algorithm. The graph generated is a directed graph with weights associated on the edges. Also the number of edges is less comp ared to that in the algorithm suggested by Charu et. al   A. Algorithm   The algorithm has two steps explained below. The first step is explained in the section 3\(A\ in which we will explain that how are we going to construct th e graph. The second step is explained in section 3\(B\ in which rule generation is explained   Construction of adjacency lattice   


The large itemsets obtained by applying some traditional algorithm for finding frequent itemsets \(like Apriori stored in one file and corresponding values of support is stored in another file. Using these two files we can store the item and their corresponding support in a structure say S  Now create an array of structure s\(i, j having two fields itemsets and support. This array of structure is used to store the different length of large itemsets in different dimensions In the field itemsets of structure s i, j we will store 1-itemsets in s\(1, j 2-itemsets in s\(2, j 3-itemsets in s\(3, j and so on   We have written a function for this purpose named as Initialize \( \he pseudo code for the Initialize   Algorithm Initialize \(S   Begin for each large itemset S do Item1 = s\(i\mset  Item2 = s\(i+1\.itemset M1 = length\(item1 length\(item2 s\(j,k\.itemsets = item1 s\(j,k\.support s\(i\.support; Increment k   If\(diff of lengths of consecutive items!=0 put itemsets in the next row of s  return s; End   Now to calculate the weight of the edge between itemset X  and itemset Y where X-Y itemset, calculate the value support\(X\/support\(Y if this value is >= minimum  confidence then we can have an edge between the itemset X  and the itemset Y and this edge will have weight  support\(X\/support\(Y Now a function is required to generate  the adjacency matrix using the structure S and s. This function will take one large itemset from s\(i, j and compare with all the items in s\(i+1, j If any subset of this itemset in s\(i, j is present in s\(i+1, j then it is required to find that  whether there will be link between them and if there will be link then what will be the weight of the link  Let an itemset X from structure s\(i, j is taken and searched in the S\(i When index of itemset X say index 1 in the structure S is obtained, we can easily get the support of this  itemset X Now search all subset of this itemset in s\(i+1, J There is need to find the support for each itemset Y which is present in the s\(i+1, j and also subset of the itemset X  present in s\(i, j The index of the itemset Y, index2 is obtained by searching it in structure S\(i Now weight  S\(index1\.support/S\(index2 support is calculated if it is  greater than or equal to minimum confidence then in the adjacency matrix,say a, a[index1, index2 is assigned value equal to the weight. The pseudo code for gen_adj_lattice B  is given in the following   Algorithm gen_adj_lattice\(S,s   Begin   For each row of s do  Item 1 = s\(I,j\.itemsets   Index1 = find_index\(item1,s   finding all subsets of item1 in s\(i+1,j For each itemset in s\(i+1   Item2 = s\(i+1,k\.itemsets If \(item1 is superset of item2   Index2 = find_index\(item2,3   Confidence = s\(index2\.support/s\(index1\.support If\(Confidence >= minconf  adj_lat\(index1,index2\Confidence Return adj_lat   End  In the above gen_adj_lattice function there is a sub-function to search an element in the structure S which returns the index of that itemset in the structure. Using this index we can  get the support of the corresponding large itemset  Let an itemset X is to be searched in the S\(i firstly find the length of the itemset X Now take start traversing the structure S if the length of the current itemset is equal to the  length of the itemset to be searched then only compare the two itemsets. If all the items of the both itemsets are matching then return the index. This pseudo code for find_index is given in the following   Algorithm find_index\(item,S Begin   N1 = length\(item1   For each itemset in S do Item2= S\( r\sets N2 = length\(item2   If\(length of the itemsets are equal If\(Each item matched   index= r return index  End   The graph generated will be directed graph in which largest itemsets will be at the first level and 1-large itemsets will be at lowest level. And the direction of the edges will be from n-1\h level to nth level And the weight will be equal to the  support of the itemset in the n-1\h level divided by the support of the itemset at the nth level   B. Generation of Rules   Each node in the directed graph is chosen for rule generation Call that node starting node and do depth first search in the directed graph. And generate the rules from the visited node  and starting node if and only if it satisfies all the condition which are required to generate essential rule  Conditions   1  Product of the confidence of the path between the starting node and the visited mode must be greater than or equal to minimum confidence  2  To reduce simple redundancy: We generate set of all children of the visited node and then this set of child nodes is compared with the nodes that have already been used by the same starting node for rule generation. If any one of the child nodes is found there from this visited node no rule can be generated Since, this rule will be redundant  The pseudo code for find_allChild is given       


Algorithm find_allChild\(adj_lat,i Begin   C1=C=NULL C1=C=child\(adj_lat,i   while C1 = NULL do For each c C1 do   C1 = Child\(adj_lat,c C = C C1   return C End   We have a structure, say G, which stores nodes that have already been used for generating rules. They are stored in such a way that we can get the required nodes just by reaching the corresponding index. The pseudo code for the same is given in the following   Algorithm node_gen_rule\(nodeset S,G\ Begin  generated Set = NULL for each node S\(i S do   generated Set \226 generated Set G\(S\(i return generated Set  End   To reduce strict redundancy   A  We have generated s of all Parents of the starting node and then for all these parent nodes we have to find out all the nodes which have been used for Rule Generation by these parent nodes. Then this set of node is compared with the visited node. If this visited node is found then from this visited node no  rule can be generated. Because this rule will be strictly redundant The pseudo code for  find_allParents is given in the following    B  We generate set of all Childs of the visited node and the set of all Parents of the starting node and then for all these parent\222s nodes we have to find out all the nodes, which have been used for Rule Generation by these parent nodes. Then this set of node is compared with the set of all child. If any of child of this visited node is found there then from this visited node no rule can be generated. Because this rule will be strictly redundant   Algorithm find_allParents\(adj_lat,i Begin   P1=P=NULL P1=P=Parents\(adj_lat,i   While P1 is not equal to NULL do For Each P P1 dp  P1 = Parents\(adj_lat,P\ P = P P1  return P   End   Algorithm Generate Rule \(Starting node: X, Visited node Y, Min Conf: c, G   Begin   RuleSet=NULL  C1=weighted product of the path\(X,Y If\(c1>=c  If\(~compare\(find_allChild\(adj_mat,Y\,node_gen_rule\(X,G  If\(~compare\(node_gen_rule\(find_allParents\(adj_lat,X\,G\,Y      If\(~compare\(find_allChild\(adj_lat,Y\,node_gen_rule\(find_all Parents\(X  RuleSet = RuleSet U\(Y->\(X-Y Return ruleSet   End   IV. ILLUSTRATION OF EXISTING AND PROPOSED ALGORITHMS   Now we are going to illustrate both the algorithms by taking example. The Market Basket Data sets taken shown below in Table 4.1. This dataset has five transactions and five itemsets  Let the minimum support be 0.4 and minimum confidence is  0.67 Various large itemset obtained b having support value greater than 0.4, along with the support value are shown in the Tables 4.2 to 4.4   Table 4.1 : 1-large itemsets   ITEMS SUPPORT   A=Bread  0.8   B=Milk 0.8   C=Beer 0.6   D=Diaper  0.8   F=Cock 0.4     Table 4.2 : 2-large itemsets   AB 0.6   AC 0.4   AD 0.6   BC 0.4   BD 0.6   BF 0.4   CD 0.6   DF 0.4            


Table 4.3 : 3- large itemsets   ABD  0.4   ACD  0.4   BCD  0.4   BDF  0.4    A. Rule Generation from the proposed algorithm   Weights of edges between frequent 1-itemset to frequent 2itemset and between frequent 2-itemset to frequent 3-itemsets are shown in Table 4.4 . The weights of edges are calculated in the following manner. Let X be k-itemset and Y be the k+1 itemset, then the weight of the edge form X to Y is equal to the confidence of the rule X      \(Y- X   Table 4.4: Weights of the edges between 1-itemset to 2-itemsets   Edges Weights A \226 AB 0.75 A \226 AC 0.5 A \226 AD 0.75 B \226 AB 0.75 B \226 BC 0.5 B \226 BD 0.75 B \226 BF 0.5 C \226 AC 0.67 C \226 BC 0.67 C \226 CD 1.0 D \226 DF 0.5 D \226 AD 0.75 D \226 BD 0.75 D \226 CD 0.75 AB-ABD 0.67 AC-ACD 0.67 AD-ABD 0.67 AD-ACD 0.67 BC-BCD 1.0 BD-BCD 0.67 BF-BDF 1.0 CD-ACD 0.67 CD-BCD 0.67 BF-BDF 1.0 The lattice generated for the above example               Figure 4.1 Lattice Structure   The resultant graph is shown below                Figure 4.2: Graph generated for the rule generation   We can see that there are more edges in the lattice generated for the same example. These edges are shown by dotted edge              Figure 4.3  Generating the rules for the large item sets ABD   Applying depth first search starting from the node ABD the node A will be the first visited node but the weighted product  0.67*0.75\ the path obtained from A to ABD is less than minimum confidence. So the node A will not participate in rule generation. Node B will be second visited node but this also will not participate in rule generation because of similar reason. Now the next visited node is AB and the weighted product of the path from AB to ABD is  0.67 which is equal to the minimum confidence. The children nodes of AB are not generating any rule, and also AB is not used by any of the parent nodes of ABD Thus all the three conditions are satisfied for rule generation. So we will generate the rule         


from AB  AB = > D Now the next visited node will be D  but weighted product of the path from D to ABD is less than minimum confidence hence no rule will be there and we have to go to next visited node AD as satisfies all the three conditions so there will be rule , A D => B    The next visited node will BD and this node satisfies all the three conditions, thus we have rule BD => A   Similarly, Generating the rules for large itemset ACD, BCD BDF,BDF,AB,AD,BD,AC,BC,CD,BF,DF We are getting the  following rules shown in the Table 4.5 below   Table 4.5: The rules generated   1 AB => D 2 AD => B 3 BD => A 4 C => AD 5 AD => C 6 C => BD 7 BD => C 8 F => BD  9 BD => F 10 A => B 11 B => A 12 A => D 13 D => A 14 B => D 15 D => B 16 D => C  B.  Rules Generated from the Existing algorithm   Generating the rules for large itemsets ABD   Chose all the ancestors of ABD which has support less than or equal to the   Value = {support \(ABD\/c 0.4  0.67\6   AB, AD and BD will be selected. So we will have following  lattice. We can easily see that AB, AD and BD are the maximal ancestor of the directed graph shown in the figure Hence we will have two rules   AB => D , AD => B,  BD => A          Figure 4.4 : Directed Graph in Adjacency   Total number of 16 rules generated in both algorithms. It was found that no essential rules are missing in proposed algorithm and also there is no redundancy in the rules generated   C.  Comparison of Algorithms   Complexity of graph search algorithm is proportional to the size of output      Theorem: The number of edges in the adjacency lattice is equal to the sum of the number of parents of each primary itemset   Let N\(I, s\he number of primary itemsets in R\(I, s\hus size of output in this case = N\(I, s\ . h\(I, s\ Complexity of existing algorithm is proportional to N\(I, s\ h\(I, s\ the proposed algorithm there are some edges left which are not visited by their parents Let those nodes are denoted by L\(I s\.This size of output in this case = N\(I, s\ . h\(I, s\ \226 L\(I, s Complexity of proposed algorithm is proportional to N\(I, s h\(I, s\\(I, s   CONCLUSION AND FUTURE WORK   In this paper, data mining and one of important technique of data mining is discussed. The issues related with association rule mining and then online mining of association rules are introduced to resolve these issues. Online association mining helps to remove redundant rules and helps in compact representation of rules for user. A new algorithm has been proposed for online rule generation. The advantage of this algorithm is that, the graph generated in our algorithm has less edge as compared to the lattice used in the existing algorithm. This algorithm generates all the essential rules also and no rule is missing   The future work will be implementing both existing and proposed algorithms, and then test these algorithms on large datasets like the Zoo Dataset  Mushroom Dataset and Synthetic Dataset   REFERENCES   1 A g r a w al  R  I m ie l i nski T S w am i, A  223 M i n ing  asso ci at io n  rules between sets of items in large databases.\222\222 SIGMOD-1993 pp. 207-214  2  Charu C. Agrawal and Philip S. Yu, \223A New Approach to Online Generation of Association Rules\222\222 IEEE, vol. 13 No. 4, pp. 327-340, 2001 3  Dao-I Lin, Zvi M.Kedem, ``Pincer search: An  Efficient algorithm to find maximal frequency item set IEEE trans On knowledge and data engineering vol. No.3 pp. 333-344, may/june,2002   4  Ming Liu, W.Hsu, and Y. Ma, \223Mining association rules with multiple minimum supports.\222\222 In Proceeding of fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 337-341 N.Y., 1999. ACM Press  5  R. Agrawal, T. Lmielinksi, and A. Swami 223Mining association between sets of items in Large databases\222\222 Conf. Management of  Data, Washington, DC, May 1993 6  Ramakrishna Srikanth and Quoc Vu and Rakesh Agrawal, \221\222Mining association rules with itemsets constraints.\222\222 In Proc. Of the 3 rd  International Conference on KDD and Data Mining \(KDD 97 Newport Beach, California August 1997 7  Rakesh Agrawal and Ramakrishna Srikanth, \223Fast Algorithm for Mining Association Rules\222\222 In Proc 20  Int Conf. Very Large Data Base, VLDB, 1994  8  Data Mining: Concepts and Techniques By \223Jiawei Han Micheline Kamber\222\222. Academic press 2001  


history is contained in the system: a set of items with their valuations and this helps to identify their interests. In the following, we use valuation to refer to an implicit or explicit rating. The usage of implicit and explicit data is not the same in the different modules B. Collaborative Filtering Module 1 performed by computing association rules. Association rule mining is commonly used to perform market based analysis 15]. It is used to discover patterns linking items purchased 5A user profile is an instantiation of the user model 2010 10th International Conference on Intelligent Systems Design and Applications 471 together by customers. For example, a possible association rule may detect that most customers who buy cheese also buy red wine. Formally, an association rule is an expression X ? B where X ? I and B ? \(I \\ X respectively called body and head In the context of recommender systems, using association rule mining aims at discovering items that might interest the user based on the previous items he has rated and which have also been rated similarly by a group of users. Typically they discover sequential patterns between session profiles The problem is that a session profile is relevant in certain domains only \(webmarket for example a movie rental platform, a session profile has no meaning This is why, we expend the session profile to a short-term profile, which allows domain-independence There is a great deal of research dealing with association rule mining in recommender systems [22]. Association rule mining allows one to compute the rules off-line, which is significant as it reduces the recommendation process time Furthermore, model-based algorithms such as association rule mining, are more robust when dealing with profile injection attacks6 than memory-based algorithms \(k nearest neighbors whole set of users to generate recommendations while memory-based ones only use the most similar users who can have in this case attacker profiles [13]. Finally, association rule mining deals well with the scalability problem. Actually it is fast to implement and execute and it manages well large sets of data. This is why it represents the core of our collaborative module 


To be preserved, an association rule must satisfy two measures The support of an itemset A, denoted s\(A the frequency of the itemset A among the set of transactions T \(users history items in the context s\(A The con?dence of a rule X ? B, denoted c\(X ? B is defined by: c\(X ? B X?B s\(X Association rules are mined using the Apriori algorithm 15]. The liked and disliked items are first distinguished in the user history using their item ratings. This is done in order to gain a global view of the liked and disliked items For ratings between 1 and 5, values strictly lower than 4 are considered to represent disliked items. On the other hand values equal to or higher than 4 represent items that fit the users taste. Thus, items can be labeled with two values l like dislike users who have rated some of these items, we can obtain the extraction of the rules This module doesnt depend on a particular user. The computation is conducted generally for all users and is 6The insertion of multiple profiles under false identities in order to promote or denote the recommendation of a particular item performed periodically 2 aims at personalizing the association rules for the user The association rules are filtered according to the user by applying the following rules in the following order a of items contained in the users history \(this is a strong rule which can be relaxed  see Sect. V b the head because there is no sense in recommending an item he has already consulted c because there is no sense in recommending to the user an item he will not appreciate d as a consequence of \(b c Example: Let us consider movie recommendation Given a user u who rated and liked the movies F1, F3 and 


F5 denoted F l1, F l3 and F l5. The following rules are obtained r1 : {F l1, F l3} ? {F l5 r2 : {F l1, F l2} ? {F l4 r3 : {F l3, F l5} ? {F d6 r4 : {F l1, F l5} ? {F l3, F l7 By the application of \(b d kept. The rule r2 is also not kept by the application of \(a r3 is not kept after passing \(c d r?4 : {F l1, F l5} ? {F l7} because of \(b rule r?4 This result is justified because r1 leads to the recommendation of an item already consulted by the user, which is useless. r2 doesnt fit the user because he hasnt consulted all of the items contained in the body so we cannot make any conclusions. The recommendation of r3 is not accurate because the item is predicted as disliked. Finally, we can extract rule r4 which results in a possible recommendation the item F7 C. Semantic-Based Module This module is based on an ontology. An ontology is an explicit specification of a shared conceptualization [23]. It is used to formally describe domains by means of concepts and properties. Typically, concepts are classes and properties are relations characterizing the concepts. A property has a domain and a range. For example, the domain of the property hasActor\(Film,Actor Instances of concepts and properties are called individuals7 In the following, we denote C as the set of the concepts of the the ontology, and Rel as the set of properties \(relations We consider that an item is characterized by the concepts hes related to. Moreover, each concept has its proper influence on each user. This is why, the interest of a user 7The terms instance and individual are used without distinction 472 2010 10th International Conference on Intelligent Systems Design and Applications in an item is defined by his interest in the concepts related to the item. The user preference for a concept over another is learned by analyzing his profile. We do not explain this process in this paper Once the association rules concerning the user are identified, we use the ontology to compute the interests of each item in the head. This helps to refine the obtained results from the previous module. Indeed, before applying 


the semantic module, the recommendation is issued from collaborative filtering. While in some cases these recommendations can fit the user, it is possible that they dont suit his other interests. This semantic module performs the required adjustments and takes into account the user as a unique entity The prediction of the interest of these features of item f uses the following parameters for the user u the users valuations for the items which have at least one shared feature with f the number of occurrences of each feature of f among those rated by u This is done in two steps: the computation of the instance interest and then the computation of the concept interest 1 Definition: Given a user u, the interest of u for an instance i is a prediction based on the past valuations of u for the items connected to i by a certain relation R ? Rel An instance i is connected to 0 to n items in the users history. Thus, 0 to n valuations related to this instance are obtained. The interest of i is then estimated according to the valuations connected with it. This is done by computing the median of these valuations. Actually, median allows one to avoid extreme and unusual values which add some noise when computing a basic average of a set of values instanceInterest\(u, i median\({eval\(u, f f, i 1 where eval\(u, f f 2 Definition: Given a user u, the interest of u according to a concept c for an item f is based on the instance interests of the individuals of the concept c connected to f by a certain relation R ? Rel conceptInterestc\(u, f   i?{i? |?R?Rel R\(f,i i instanceInterest\(u, i ratioocc\(u, i   


i?{i? |?R?Rel R\(f,i i ratioocc\(u, i 1 2 where ratioocc\(u, i instance i among the instances of the same concept in the users history D. Frequency Module This module aims at detecting the frequent instances and the frequent associations of instances. Indeed, such a frequency depicts an important interest of the user for the concerned instances. Consequently, it is relevant to recommend items with these characteristics to the user 1 considers the profile of the user. It aims at detecting the most important features of interest for the user Regardless of the estimated interest of an instance for a user, we consider that if the user has in his history a significant percentage of items which have as a feature that particular instance, the interest of this instance is significant Unlike the previous computation, this computation ignores the users ratings for the items which have the instance as a feature Example: A user who has watched 80% of the films interpreted by the actor Tom Hanks should get the recommendation of the other 20% he has not seen even if some of the films of Tom Hanks in his history are badly rated 2 case with frequent instances described above, this part of the module deals not only with the profile of the user, but also with the set of items. It aims at discovering frequent associations between the features in the user history. It detects the features that often occur together in order to discover new recommendations. To achieve this, frequent sets of the instances related to the items in the users history are computed. Then, items with such instances are recommended to the user Example: A possible frequent association is the actor Johnny Depp and the director Tim Burton. A user who is interested in these two instances will be recommended the other films related to them E. Recommendation and explanation 


As explained in Sect. II, the collaborative and the semantic modules are in cascade. Consequently, the result is a set of recommendations rec1 which is mixed with the recommendations of the frequency module rec2 such that rec1 is presented before rec2 to the user. This order can be inverted according to user feedback. Concerning the explanation of the recommendations, this is done by highlighting the instances which have highly scored the interest of the user for the items of rec1, and by highlighting the frequent instances in the items of rec2 F. Example In this section, we illustrate the recommendation process for any user u, in the movie domain. We will simplify to preserve the clarity of the example 2010 10th International Conference on Intelligent Systems Design and Applications 473 Table I EXTRACT OF THE PROFILE OF THE USER u Film Rating Transformation Psycho 5 Psychol Rear Window 4 Rear Windowl Four Weddings and a Funeral 4 Four Weddings and a Funerall Monty Pythons Life of Brian 5 Monty Pythons Life of Brianl Carrie 3 Carried Stephen Kings The Langoliers 1 Stephen Kings The Langoliersd Pulp Fiction 4 Pulp Fictionl Dr. Strangelove 2 Dr. Strangeloved A Clockwork Orange 1 A Clockwork Oranged Let us consider u who has rated the movies in Tab. I Collaborative Filtering Module: Let us assume that the association rule mining result is r1 : {Psychol, Pulp Fictionl} ? {The Shiningl r2 : {Pulp Fictionl,Monty Pythons Life of Brianl Monty Python and the Holy Graill r3 : {Monty Python and the Holy Graill, Jurassic Parkl Indiana Jones and the Last Crusadel According to the rules introduced in Sect III-B2, we only keep the association rules r1 and r2 Semantic-Based Module: In this step, the interest of the user for each movie in the head of each rule from the last module is computed. The concerned movies are The Shining and Monty Python and the Holy Grail For The Shining, we obtain the following interest re 


sults conceptInterestActor \(u, TheShining conceptInterestDirector \(u, TheShining conceptInterestWriter \(u, TheShining conceptInterestGenre \(u, TheShining Unlike the prediction of the previous modules, it seems that The Shining is not a good recommendation for u Actually, this film shares its director with Dr. Strangelove and A Clockwork Orange which are negatively rated by u Moreover, it has a writer in common with Carrie and The Langoliers which are also movies disliked by u. The same reasoning is made about the concepts Actor and Genre Concerning Monty Python and the Holy Grail, the interests by concept are conceptInterestActor \(u, HolyGrail conceptInterestDirector \(u, HolyGrail conceptInterestWriter \(u, HolyGrail conceptInterestGenre \(u, HolyGrail This recommendation is a good one. The film shares its actors, writers and director with Monty Pythons Life of Brian which is highly rated by the user. The recommendation is thus justified Figure 3. Extract of the movie ontology Frequency Module: Let us assume that the user rated 60% of Alfred Hitchcocks films \(in Tab. I, Psycho and Rear Window are some of them recommended to u IV. EXPERIMENTAL EVALUATION A. Ontology Description For the experimentation, we built the ontology manually see Fig. 3 IMDB8. We focused only on a set of data which led to the concepts Film, Person, Actor, Director, Writer and Genre The connections between these concepts are Each movie is related to a certain number of persons who can be actors, directors or writers but it can also be related to other movies \(Example: Free Willy 2: The Adventure Home and Free Willy 3: The Rescue are related A person and a movie have a genre \(Action, Adventure Animation, Children, Comedy, Crime, etc B. Experimentation and Evaluation 


We use a subset of the dataset provided by MovieLens, the recommender system of GroupLens Research. The dataset contains a set of users, the set of items they have evaluated with a rating between 1 \(for the least liked for the most liked framework, we deal with a set of 86 movies, 934 users and 13 053 ratings. The dataset contains 3593 actors, 77 directors, 275 writers and 17 genres Using a 65% confidence and a 5% support, association rule mining resulted in 1472 rules after running the collaborative module. We evaluated the results obtained from the system by eighteen 20-50-year-old volunteers. The evaluation consisted exclusively in explicit valuations \(ratings 8http://www.imdb.com 474 2010 10th International Conference on Intelligent Systems Design and Applications Figure 4. Users evaluation of the system between 1 and 5 rated at the beginning between 11 and 31 films. For each recommended item, the user rates it as liked or disliked. If an item is rated as liked, the recommendation is considered as accurate. Otherwise, the system explains the reason why this item is recommended. The user can then agree with this explanation or not. Explanation of recommendation can be effective in convincing users in their appreciation of the items [24]. In our approach, the explanation aims at discovering if the detected patterns in the recommended item are accurate or not. Let us consider the following explanation in the recommendation of a film: This film may interest you because you frequently watched Tim Burtons films with Johnny Depp. If the user agrees with the explanation, that means that the association \(Tim Burton - Johnny Depp relevant but this particular film do not appeal to the user Otherwise, we consider that the detected association was purely a coincidence. In this case, the system will be able to ignore this pattern for this user in the future The results of this evaluation are depicted on Fig. 4. We can see that 84,9% of the recommendations satisfy the users Concerning the recommendations rated as disliked, 59,4% of the explanations are approved by users. Finally, 93,9% of the recommendations are satisfying or approved An average of 5 recommendations is obtained by running the collaborative and the semantic-based modules \(which is 


acceptable due to the low number of movies  86  in the dataset frequency module. This difference is due to the fact that the cascading modules \(collaborative and semantic-based are limited by the unique usage of the ratings to compute the association rules. The frequency module, on the other hand, is based on a statistical analysis of the item contents Consequently, it does not suffer from the sparsity of the user rating matrix like the previous modules The collaborative module results in some recommendations which are not liked by users. Fortunately, such recommendations are eliminated by the semantic-based module Other recommendations are eliminated by the semanticbased module though they appeal to users. We explain this because the concerned items dont share any features with the ones in the users history. This is why, we aim at introducing a semantic similarity measure to alleviate this problem \(see Sect. V module recommendations and 58,1% of the explanations of the disliked recommendations, satisfy the users. We can conclude that the combination of all the modules results in better recommendations V. CONCLUSION AND FUTURE WORK In our work, we propose a hybrid recommender system that combines collaborative filtering and semantic analysis of the items. The approach is based on many modules that refine the rules which progressively lead to a recommendation. A process targeting users with various interests is described. First, the collaborative filtering step is achieved using association rule mining which is a flexible way to classify the user. His history is then used to make the results more adapted to him. The semantic module aims at refining the recommendation issued from the rules. Finally, a frequency module is used to discover other items of interest for the user. Using distinct modules allows us to explain the recommendations to the user The results we have obtained from the evaluation experiments are promising. The combination of the collaborative and semantic modules improves the quality of the recommendations and the frequency module adds new ones. 93,9 of the recommendations are satisfying or approved In near future, we aim at defining the approach to learn 


the user profile in order to adapt the combination of the recommendation modules. We also plan to improve the semantic module by defining the semantic similarity between instances [25], [26]. Thus, when computing the interest by instance, those which are semantically similar to the current instance can be used when the instance is not present in the users history This similarity could also be used during the personalization of the association rules. The personalization rule \(a which consists in only keeping the rules which have a body composed of items contained in the users history, can be relaxed if the items violating \(a the items in the users history. The advantage of the semantic similarity is that it can be computed off-line which does not slow down the recommendation process Another improvement we want to introduce is the use of implicit data collected and based on the users behavior e.g. his search history, the time he spent looking at an item and his navigational patterns. This will help to increase the knowledge about the user and, in turn, lead to a better understanding of his expectations Finally, we plan to experiment the framework on other domains to confirm the domain-independence of the system REFERENCES 1] G. Adomavicius and A. Tuzhilin, Toward the next generation of recommender systems: A survey of the state-of-the-art and 2010 10th International Conference on Intelligent Systems Design and Applications 475 possible extensions, IEEE Trans. Knowl. Data Eng., vol. 17 no. 6, pp. 734749, 2005 2] R. Burke, Hybrid recommender systems: Survey and experiments, User Modeling and User-Adapted Interaction vol. 12, no. 4, pp. 331370, 2002 3] K. Lang, Newsweeder: Learning to filter netnews, in Proceedings of the 12th International Machine Learning Conference \(ML, 1995 4] M. J. Pazzani and D. Billsus, Content-based recommendation systems, in The Adaptive Web, P. Brusilovsky, A. Kobsa, and W. Nejdl, Eds., 2007, vol. 4321, pp. 325341 5] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and J. Riedl Grouplens: An open architecture for collaborative filtering of netnews, in Proceedings of ACM 1994 Conference on Computer Supported Cooperative Work, Chapel Hill, North 


Carolina, 1994, pp. 175186 6] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl, Itembased collaborative filtering recommendation algorithms, in Proceedings of the 10th international conference on World Wide Web \(WWW 295 7] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, Analysis of recommendation algorithms for e-commerce, in Proceedings of the 2nd ACM conference on Electronic commerce \(EC Minneapolis, Minnesota, USA, 2000, pp. 158167 8] M. Balabanovic and Y. Shoham, Fab: content-based, collaborative recommendation, Commun. ACM, vol. 40, no. 3, pp 6672, 1997 9] D. Billsus and M. J. Pazzani, User modeling for adaptive news access, User Modeling and User-Adapted Interaction vol. 10, no. 2-3, pp. 147180, 2000 10] M. J. Pazzani, A framework for collaborative, content-based and demographic filtering, Artif. Intell. Rev., vol. 13, no. 5-6 pp. 393408, 1999 11] S. Castagnos, A. Brun, and A. Boyer, Probabilistic association rules for item-based recommender systems, in Proceedings of the Fourth Starting AI Researchers Symposium STAIRS 12] W. Lin, Association rule mining for collaborative recommender systems, Masters thesis, Faculty of the Worcester Polytechnic Institute, 2000 13] J. J. Sandvig, B. Mobasher, and R. Burke, Robustness of collaborative recommendation based on association rule mining, in Proceedings of the 2007 ACM conference on Recommender systems \(RecSys 14] B. Mobasher, H. Dai, T. Luo, and M. Nakagawa, Effective personalization based on association rule discovery from web usage data, in Proceedings of the 3rd international workshop on Web information and data management \(WIDM Georgia, USA, 2001, pp. 915 15] R. Agrawal, T. Imielinski, and A. Swami, Mining association rules between sets of items in large databases, in Proceedings of the 1993 ACM SIGMOD international conference on Management of data, New York, NY, USA, 1993, pp. 207 216 16] B. Liu, W. Hsu, and Y. Ma, Integrating classification and association rule mining, in Knowledge Discovery and Data 


Mining, New York City, New York, USA, Aug. 1998, pp 8086 17] Y. Blanco-Fernandez, J. J. P. Arias, A. Gil-Solla, M. R Cabrer, M. L. Nores, J. G. Duque, A. F. Vilas, R. P. D Redondo, and J. B. Munoz, A flexible semantic inference methodology to reason about user preferences in knowledgebased recommender systems, Knowl.-Based Syst., vol. 21 no. 4, pp. 305320, 2008 18] S. E. Middleton, H. Alani, N. R. Shadbolt, and D. C. D Roure, Exploiting synergy between ontologies and recommender systems, in Semantic Web Workshop 2002 At the Eleventh International World Wide Web Conference, 2002 19] M. Zanker and M. Jessenitschnig, Case-studies on exploiting explicit customer requirements in recommender systems User Modeling and User-Adapted Interaction, vol. 19, no 1-2, pp. 133166, 2009 20] N. Ducheneaut, K. Partridge, Q. Huang, B. Price, M. Roberts E. H. Chi, V. Bellotti, and B. Begole, Collaborative filtering is not enough? experiments with a mixed-model recommender for leisure activities, in Proceedings of the 17th International Conference on User Modeling, Adaptation, and Personalization \(UMAP 21] H. Nguyen and P. Haddawy, The decision-theoretic interactive video advisor, in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence \(UAI 494501 22] B. Mobasher, Data mining for web personalization, in The Adaptive Web, ser. Lecture Notes in Computer Science P. Brusilovsky, A. Kobsa, and W. Nejdl, Eds. Springer Berlin Heidelberg, 2007, vol. 4321, ch. 3, pp. 90135 23] T. R. Gruber, A translation approach to portable ontology specifications, Knowl. Acquis., vol. 5, no. 2, pp. 199220 1993 24] N. Tintarev and J. Masthoff, The effectiveness of personalized movie explanations: An experiment using commercial meta-data, in Proceedings of the 5th international conference on Adaptive Hypermedia and Adaptive Web-Based Systems AH 25] R. Albertoni and M. D. Martino, Asymmetric and contextdependent semantic similarity among ontology instances Journal on Data Semantics, vol. 10, pp. 130, 2008 26] X. Jin and B. Mobasher, Using semantic similarity to 


enhance item-based collaborative filtering, in 2nd IASTED International Conference on Information and Knowledge Sharing, Scottsdale, Arizona, 2003 476 2010 10th International Conference on Intelligent Systems Design and Applications 


Basi Association Rles Basi c  Association R u les Association is basically connecting or tying up occurrences of Association is basically connecting or tying up occurrences of events Ol dib t f ilt t O n l y d escr ib e se t s o f s i mu lt aneous even t s Cannot describe patterns that iterate over time e g  itemset a  0  b  0  g    Eg If you sense higher data rates on the downlink than normal AND New Route generated Implies high chances of Intrusion AND New Route generated Implies high chances of Intrusion Associative IDS for NextGen Frameworks Dr S Dua LA Tech 20 


Enhanced Inte r transaction Association Rules Enhanced Inter transaction Association Rules Enhanced Inter transaction Association Rules Extension of association rules Conditional relationships at multiple different time steps e.g itemset a\(0 0 1 2 You sense Higher data rate than normal AND You see New Route g enerated AND 1 minute a g o you detected checksum gg error packets AND 2 minutes ago your encountered wrong checksum   Implies High Chance of Intrusion Enhanced Rules and Confidence Associative IDS for NextGen Frameworks Dr S Dua LA Tech 21 


Complex Spatio temporal Association Complex Spatio temporal Association Rules Further extension of inter transaction association rules Describe event durations e.g itemset a\(0,X j,Y k,Z Eg  You sense high data rates for X seconds AND new route generated j minutes ago task completed in Y AND new route generated j minutes ago task completed in Y seconds AND checksum error packets received k minutes ago for Z seconds High Chance of Intrusion With highest confidence level in association rules  association rules  Associative IDS for NextGen Frameworks Dr S Dua LA Tech 22 


DMITAR Al ith ARD DMITAR Al gor ith m  ARD Problem Domain Problem Statement and Challenges Aiti Miig bd IDS A ssoc i a ti ve Mi n i n g b ase d IDS  Introduction to data mining Association rule in data mining DMITAR Algorithm  ARD New research Associative IDS for NextGen Frameworks Dr S Dua LA Tech 23 


DMITAR Algorithm DMITAR Difference Matrix Based Inter Transaction Association Rule Miner developed in DMRL Uses vertical data format Differences of the transaction IDs are used to generate extended itemsets Windowless mechanism Associative IDS for NextGen Frameworks Dr S Dua LA Tech 24 


Deep into the Mechanism The DMITAR algorithm is based on lhilii comp l ex mat h emat i ca l assoc i at i ve formulation and proofs Four major parts Four major parts Frequent 1 itemset generation Frequent 2 itemset generation Frequent k itemset generation k>2 Spatio temporal rule formation Associative IDS for NextGen Frameworks Dr S Dua LA Tech 25 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


