Market-Basket Analysis with Principal Component Analysis An Exploration Kitty S.Y Chiu Robeti W.P Luk Keith C.C Chan and Koms F.L Chung Department of Computing Hong Kong Polytechnic University Hong Kong China cssychiu, csrluk cskcchan cskchung Abshee Market-basket aonlyiir Is a well-known business problem which can be partially solved computationally using association roles mined from transaction data to maximize cross-selling effeets Here we model the market-basket aaalyrls 81 a finite mixtnre density of human eoasomptian behavior according to social and cultural events This leads to 
thc use of principle component analysis and possibly miitore density analysis of transaction data wbleh was not apparent before We compare PCA and association ruler mined from B set of benebmark transaction data to explore similarities and differences between these two data esploration tools KeywordF market-basket association rules PCA mixture density I INTRODUCTION The market-basket problem is a well known business problem in which consumers buying behavior is influenced by alternative prcducts and related products With the availabiliiy of accurate and efficient capture of transaction data wmputational analysis 
of hansaction data can discover interesting consumer spending patterns Mining association rules I21 is a well known and important tool to discover dependencies in product sales By discovering these dependencies, it is possible to maximize the cross-selling effect 3 of related prodwts increasing transaction volume and therefore increasing total profit The appropriate use of cross-selling capabilities has implications in other business activities like inventoryharehouse management enhancing user satisfaction etc Hence it is important to discover prohct dependencies 41 While association rules have been quite successful in solving the market-basket problem 
and dismvering dcpendencie higher-order dependencies are hard to rind because of the nature of the discovery algorithm e.g Apricri in which successive higher order association tules are pruned SI due to computational cost and the lack of evidence to supput such higher order association rules hincipal componen analysis PCA and the more general techniques of finding mixture densities are well know multivan'ate data exploration techniques 6 but it is not apparent how they could be applied to the market basket problem Previous work used PCA for quantitative association des for estimating errors 7 
as well as for load balancing parallel association rule mining mechanisms SI Recently Cadez Smfi and Mauuila 9 examined the use of mix models for profiling the bansaction data of indivi&als Here we will suggest a model of consumer spending behavior originated from \(social or cultural events These lend themselves to the use of PCA 11 MOOELING GNSUMER SPENDING PATTERNS A market is based on supply and demaud Shops are typical market places where suppliers provide products to satisfy the demand of consumers We conjecture that the demand of consumers typically depend on \(social or cultural events. For 
example almost every family participates in the breakfast event in which thm is a naW distribution of products desired for those events Certain events are seasonally like Christmas while other events may recur like breakfast Each household will require certain amount of products to be bought in order to satisfy the participation of the events by the househdd We assume that each event has a quasi stationary distribution of how likely certain product is bought For example suppov there are three products: bread butter and hammer The likelihood of a household to buy bread and butter for the breakfast event is much higher 
than that to buy beer for breakfast Hence the spending pattern is dependent on the nature of the event Obviously the spending pattern of each household depend on many other factors but for a large numba of households we assume that the probability distribution de  p\(iJ  p\(iJ of products ti  iJ for a particular event will k reflecled in the aggregate demand of pro&cts Since there are many events that a household is participatilg the aggregate demand Dp of these households is h.r This demand may be 
reflected in the transaction data representing the eventual spending pattern Obviously there are product substitution effects etc. However, for simplicity we assume that the consumer only buys the desired product or not Hence one cansider that dh is basically an influence on the consumption reflected in the transaction data T of a consumer h In general the influence of such a distribuion di,c is sumnmizcd in some function say GO However for simplicity the influence is simply modeled as additive components i.e 0 2002 IEEE SMC TAlFZ 


although more sophisticatedmodels using bgit transformatbn for discrete data can be used Since the events are those that may induce cross-selling effects it would be important to discover the events and the associated likely products to buy Hence it would be interesting to find de xp\(h h Assuming that each household is equally likely to consume i.e.p\(h\is a constant However, what we can observe is only TA Hence wr task is to discover the underlying distribution de Figure 1 is the schematic diagram of our suggested transaction generation process Notice that the discovering process may not be able to identify the original numbu n of events i.e rn f n it is possible to verify which social event that a product is used for by asking the consumer It is also possible that a single product is used for a number of social events so that the decision to buy a ype of product may be based on some aggregate influence for all events that needed the oroduct Figure 1 Schematic diagram of discovering the underlying distribution d _ 4J of consumer spending patterns to participate in different social and/or cultural events From a practical point of view it may not be disastrous even if the discovered distribution differed from the underlying distributbn as long as the distribution found can readily identify products that are sold together in the same transmion for maximizing cross-selling effects. In this vais it does not matter if two underlying distributions of two different events merged together as long as in the transactions the consumers bought products to support both events. From the point of view of scientific modeling and fmm other practical point of views obtaining the actual underbing distribution is impomt to predict when cetiain events are known to occur, for example seasonal events. However as an initial exploration in this area we will confine to the case whae readily avdilable techniques are used 111 PRINCIPAL COMPONENT ANALYSIS Principle component analysis \(PCA\is a well-known method in statistical multivariate data analysis and it has been applied widely First it was used as a means to reduce the dimensionality of the problem IO by reducing the number of variables to a few components or latent variables at certain level of accepted information loss Dimension reduction however is not the main objective to apply PCA to the market-basket analysis problem, here Second PCA was used to project data onto a transformed space that is invariant to certain linear transformation, to measure better similarity or dissimilarity of two points This has been applied in infomation retrieval and is called latent semantic indexing I I Again this is not our objective to apply PCA to the market-basket analysis problem We apply it i.e the discovery process in Figure 1 is the PCA in the market-basket analysis problem see Figure 2 as a means to discover not the underlying distributions but the significant variables which are associated positively or negatively to certain events These events are interpreted as the principal components of the data i.e they account for the variability that we obme fmm the data The acceptable level of informdon loss can be considd as noise or underlying uncenainty in the data that cannot be accounted by the principal components or social events These could be interpted as a kind of impulsive buying behavior althmgh this type of explanation is anecdote in nature Figure 2 shows the updated discovely process with a noise component Note that the noise source is added to the function G since it may not be an additive noise Figure 2 PCA discovery process with noise Formally let us define PCA and interpret the PCA in the context of market basket analysis PCA assumes that a principal component Z is a linear combination of a set of random variables In this case these random vanables are 


categorical variables indicating the presence or absence of an item i bougld in a transaction TA Hence 2 cl xil end xi where cJ are the coefficients in he linear combination of the items In principal component analysis the number of components is at most n i.e 0  k 5 n These principal components can be expressedin matrix notation form i.e Z CXT where C is the coefficient matrix cj,h 7 is the transpose of T The covariance of Z is COvar\(Z CxZ\(T where Zm rems the square matrix with the main diagonal equals to the eigenvalues I of X and all offdiagd elements are zero In PCA the coefficients q,d are related to the correlation between the obenable random variables g and the latent Variables ZJ in the following manner simulated transaction data is generated by Quest I21 with 20 items A set of association rules are obtained by the Apriori algorib I31 with minimum support of 20 and minimum confidence of 30 In total 1,157 association rules were discovered and there are 320 unique frequent itemsets Figure 3 is the scatter diagmn of thc wntidence values and the suppolt values of the mined association rules ne confidence values of all the association rules are larger than or equals to their corresponding support values Only a number  10 of cases called outlien in the Figure\that their support values and ther corresponding conli&nce values are the same For PCA the standardized variables 2 are used which requires the computation of correlations between two categorical variables Since each variable is a probability of occumfy'e the conelation r between the presence and absence of itemi, and i in a transaction is If 25 are standvdizcd variabls Covar\(Z is the correlation matrix between the latent variables and the correlation betwen observable random variables i and latent variables We refer to this correlation as the principal component or PC correlation me high the FC correlation r\(ij 2 is the higher the co-ocnmence of item  with the latent variable Zh If there are a number of itans or obsavable raniam variables carrehe with the same principal component then these items would likely cocorrelate or CO-occur with each other as well. Hence we expea that there should be a significant amoum of transactions with these co-correlate items Specifically if a transaction is classified as being influenced by the principal component or observatde event, then the co-correlate items should appear in the transaction In PCA a principal component 2 is uncorrelatd with any other principal component Z From the point of view of modeliug this suggested that social and cdu events are uncorrelated which is unlikely to be the case However for certain general events like those related to a meal and those related to same mechanical work may have little correlation Hence this is a strong assumption that PCA made which are unlikely to be the case in practice w NlTlALEXPLORATlON A Set Up We carried out a pilot study to examine the potential of using PCA for mining association patterns A set of 10,000 90  80 70 eo 540 530 20 10 50 0 O%-l  I 0 20 40 60 80 SuPllOsn Figure 3 kana diagram of the supott and confidence values of the sct af association mles mined by the Apriori algorithm Both thejointprobabilitiesp\(in iJ and indinidual probabilities p\(iJ are estimated by relative frequency counts The eigenvalues U of the carrelationmatrix is shown in Table 1 The information gain IG refers to the amount of data that can be explained by a particular principal component \(say i-th which is IC xlOO 4 D According to the initial results the largest principal component is 20 which accounts for 12.3 of the data and which can be expressed as a linear weighted sum of the items as follows Zzo  O.21SXt 0.OlSXz 0.046X3 0.339 0.242XI O.lOO 0.289X7 0.085 0.45X9 0.035XlO 0.190X,l 0.277Xlz 0.262Xt3 0.319X14 0.099X1 0.124X16 0.3S7X17 O0.16SXls 0.097X19 0.037Xzo 


The coefficients in the above equation can be converted into a set of correlation values between Z and indiviU items using equation I as in Table 2 If we consider those correlation wefficierts larger than 20 as strong then item 7 9 II 12 14 17 and 18 are strongly correlated with the largest principal component This if a transaction i5 wincipally influenced by the largest principal component then it is likely to observe these correlated items although they may not necessarily simultaneously OCNI in the same transaction related frequent itemsets since we conjecture that the smng PC correlations suggest cross buying behavior Related to the data generation model ow assumption is that the buyer driven by some social or cultural event needs to buy multiple items to participate in the event Hence the bansadion would reflect the CO-O~\(U~~~~CE of items supporting specific events These co-occurrences would be related to the frequent itemsets during the association rule mining process and individual association rules are simply the ratio of the probability of two itemsets where one itemset is a subset of the other i.e confidence value confc of the association rule for itemsetX  Y is Component Eigenvaiue Informatwn Gain I 0.876 4.38 2 0.944 4.72 3 0.942 4.71 4 0.881 4.40 5 0.89 4.45 6 0.901 4.50 7 0.955 4.77 8 0.917 4.58 9 0.963 4.81 10 0.861 4.30 II 0.856 4.28 12 0.982 4.91 13 0.837 4.18 14 0.988 4.94 I5 0.817 4.08 16 1.007 5.03 17 0.809 4.04 I8 1.034 5.17 19 1.076 5.38 20 2.465 1232 Table I  Eigenvalues of the principal components Table 2 Correlation between items and the largest principal component B Comparison To compare whether there are any relationships between mined association rules and the principal components we examine whether the strong PC correlations for a given component match with any of the association rules and where Xand Y are disjoint for the Apriori algorithm Therefore an initial comparison between the principal components and the mining of assodation rules is to examine whether the itemsets of both algorithms are the same Since the iiequent itemsets of the association rules are always correct, they are used as a reference for comparison For PCA there is a need to define what are strong PC correlations since those are considered to be Co-ocCUrring for a givm event 1nste.d of defining a threshold for strong PC correlatiors we examine how the matching performance varies with different level of threshold values so that the tendency and characteristics of matching in relation to the different threshold values can be examined For matching we expect that as long as all the items in a fiequent itemset has strong correlations, then there is a match i.e for all items i in th itemset X r\(i Z is strong for some principal component Z since it is possible to recover the frequent itemset by funher analysis Since there is no threshold to define smng correlation, for the itemset X to be discovered by PCA the threshold has to be below or equals to the minimm PC correlation value of all the items in X for the principal compnent 2 Since thee are more than one principal component, if any principal component can match with the itemset X then X can be discovered by funher analysis Hence, the threshold can set to the maximum of all the different minimum PC correlation values of the itemsets of different principal componens We call this threshold below which the itemset X can be discovered by PCA the min-max correlation value for itemset X and formally it is defined as min max\(m  rnaxb{r\(i 2 where Z is a principal component Figure 4 shows the min-max conelation values against the maximum confidence value of the frequent itemsets This maximum confidence value is the maximum confidence values of all assodation rules X  Y such that itemset W  X U Y i.e 


0  I Figun 4 Scam diagram of the min-max melation values definition 2 of itemsets against their corresponding maximum confidence value def~tion 3 derived from related associatan rules see text rm 2 8 862 Bm Em  0  lo ox 0 10 20 30 40 50 PC CorrslaUDn Threshold Figure 5 The effect of PC correlatim theshold on the recall of itemsets The maximm was used because if the itemset W leads to any quality association rules W should be retained rather than filtered According to Figure 4 in almost all cases except 2 that mar-conflry  min-mar\(r\(l Hence we expect the discovered itemsets to have a better confidenee value than the min-max correlation value which may be used as a kind of approximate lower bound to discover quality association rules Figure 5 shows the impact of filteringPC correlation values using different threshdds on the recall of itemsets as discovered hy the association rules As the threshold increases the recall dropped dramatically Whether this impact is desirable depends on whether the filtered itemsets are low quality i.e. have low mar-confvalues We observe the effects of setting different PC carrelation threshold in Figure 6 on fdtering the quality itemsets As the thresholdvalue increases the minimumma-confvalues for all the itemsets increass semi-monotonically and drastically after 35 and the maximum mar-conf values for all the itemsets dropped by I from 84 to 83 This shows that certain quality assmiation rules are retained with increasing threshold value In additios the average m-conf values steadily wnverge towards the maximum mar-cod values as e threshold increases Average F s minimum g 7zzzE3 10 0 0 1W 20 30 40 50 60 PC Comlatlon Threshold Figure 6 PC correlation threshold against the itemsets discovered Figure 7 shows that the amount of PC correlation values that are filtered against the quality ofthe itemsets measured by the ma*-co$values set by different threshold With just 10 of the top correlation values retamed the itemsets leading to the quality association rules can be found using PCA In this case since there are 400 i.e 20 x 20 PC correlations IO represents retaining only 40 PC correlation coefficients and only 2 correlation coefficients per principal component 90 80 e 70 60 50 40 30 I Minimum e 20  10 0  I 0 20 40 60 80 100 Y NoRI.Io PC Cwr.Ulb Figure 7 The amount of nonzero PC correlations filtered against the quality of the itemsets found measured by the mer-conf values Figure 8 shows the size of itemsets discovered hy PCA against the different percentages of nomm PC correlations of all the principal components The general trend is that the less the amount ofpercentages the smaller the itemset size In panicular even though the percentage of nonzero PC correlatims is close to 0 the maximum item sim discovered by PCA can still be as large as 4 Since certain principl components have no noms0 PC correlations when the 


percentage is close to 0 these principal componenb can be discarded without any fwthpr data exploration   2 I4 is 6 16 2 6 10 6 b4 82 0 20 xM4&%mpc&4 BOX IWY Figure 8 The maxitnum minimum and average munber of nonzero PC correlations per principal component against the different percentages of nom PC correlations for all the principal components Another enmuraging aspect of using PCA is that the average number of itemset size is only 15 compared with 20 because some of the PC correlations are zems inherently. When the PC correlation threshold is set at 0.0 the recall of items is 100 Hence there may be some potential in saving processingspgd using less number of items V SUMMARY In this paper we have explored the use of finite mixture densities to model the consumer spending patterns Products hcught for specific social and/or cultural events were considaed to be the underlying driving forces of cross selli effects as multiple related items are needed for consumrs to participate into those events We applied the principal component analysis PCA to discover these events as principal components taking a simplistic view of the discovery process for this initial exploration We used a set of 10,000 tansaction data generated by Quest  121 to examine haw PCA discovered c-occurring items may relate to associatim rules mined using the Apriai algorithm Our initial understanding is that the frequent itemsets of association rules may relate to the CO-odng items discovered by PCA. These co-occuning items are thought to he the smng correlations between the principal component and the specific item Instead of using a threshold to define s-g cornlatiom we use a novel measure called the min max correlation to illushate graphically how PCA mining of patterns relate to the fiquent itemsets discovered by the Apriori algorithm Our initial study show that as the conelation threshold is increased the average, minimum and maximm confidenoe values of the itemsets discovered by the Aprim algorithm and the PCA converges to high confidence value \(around 80 However, this process would loose some high mfidence value rules Nevertheles certain high confidence and nontrivial association rules related to the itemsets remain Further work is nccessary to demonstrate the extend of the utility of PCA Aekoowledgement This research is supported in part by project funding for the postgraduate study Reference I Agrawal R T Imielinsld and A Swami Mining associationrules between sets of items in large database In Proceedings of fhe I993 Infenarional Conferrpnce on Management of Dora SIGMOD 93 pages 207-216 May 1993 2 Agrawal R and R Srikant Fasf ulgorifhmsfor mining arrocimion mla in large daatases In YLDB 94 September 1994 31 Russell G.J and A Petersen Analysis of cross category dependence in market basket selection Journd of Refai!ing 67\(3 367-392.2000 4 Meo R Theory of dependence values ACM Trans on Dofabase Sysremr 25\(3 380406,2000 5 Bayardo Jr R Ef\200iciently mining long patterns km databases In Proc I998 Infsnmional Confexnce on Management of Dofa SIGMOD 98 pages 85-93.1998 6 Bryan F.J Manly Mdtiwniufe Sfarirtiml Mefhds A Primer Chapman and Hall 1986 7 F Korn A Labrinidis Y Kotidis C Faloutsas A Kaplunwich and D Perkovic Quantifiable data mining using principal component analysis CS-TR-3754 and UMUCSTR-97-13 techniculreportr Febmuy 1997 SI Manning A.M and J.A Keane Induing load balancing and efficient data distribution prior to association Rule Discovery in a Parallel Ennvimment In Pro Europron Co&ence on Puralld Processing pages 1460-1463 1999 9 Cadez I.V Smyul P and Mannila H Probabilistic modeling of transaction data with applications of profiling, visualization and prediction In Proc ACM KDD Confereno pages 37-46.2001 IO Levin A.U ken T.K and Moody I.E Fast pruning using principal components Advances in Neural Information Processmg Systems 6 3542.1994 I11 AgganvaZ C.C On the effects of dimensionality reduction on high dimmsimal similarityearch In ACM PODS Cmfererce 2001 I21 Agrawal R M Mehta J Shafer R Srikant A Amhg and T Bollinger The Quest data mining system In Proc 2nd Int Con5 Knowledge Discovery and Dam Mining pages 244249,1996 I31 Han J M Kamber Data mining concepts and techniques Morgnnffiufmnnn Publishers 2001 


4 If large k-itemset is empty the algorithm termi nates Otherwise k  k  1 and the coordinator broadcasts large k-itemsets to all the processors and goto 2231\224 3.5 HPA with Extremely Large Itemset Duplication  HPA-ELD In case the size of candidate itemset is smaller than the available system memory HPA does not use the remaining free space However HPA-ELD does utilize the memory by copying some of the itemsets The itemsets are sorted based on their frequency of ap pearance HPA-ELD chooses the most frequently oc curring itemsets and copies them over the processors so that all the memory space is used which contributes to further reduce the communication among the pro cessor In HPA it is generally difficult to achieve a flat workload distribution If the transaction data is highly skewed that is, some of the itemsets appear very fre quently in the transaction data the processor which has such itemsets will receive a much larger amount of data than the others This might become a system bottleneck In real situations the skew of items is easily discovered In retail applications certain items such as milk and eggs appear more frequently than others HPA-ELD can handle this problem effectively since it treats the frequently occurring itemset entries in a special way HPA-ELD copies such frequently occurring item sets among the processors and counts the sup port-count locally like in NPA In the first phase when the processors generate the candidate k-itemset using the large \(k-1 if the sum of the support val ues for each large itemset exceeds the given threshold it is inserted in all the processor\222s hash table The re maining candidate itemsets are partitioned as in HPA The threshold is determined so that all of the available memory can be fully utilized using sort After reading all the transaction data all processor\222s support-count are gathered and checked whether it satisfies the min imum support condition or not Since most of the algorithm steps are equal to HPA we omit a detailed description of HPA-ELD 4 Performance Evaluation Figure 6 shows the architecture of Fujitsu APlOOODDV system, on which we have measured the performance of the proposed parallel algorithms for mining association rules NPA SPA HPA and HPA ELD APlOOODDV employs a shared-nothing archi tecture A 64 processor system was used where each processor, called cell, is a 25MHz SPARC with 16MB local memory and a 1GB local disk drive Each pro t15.14 t20.14 T-net 15 4 2048K 145MB 20 4 2048K 187MB Figure 6 Organization of the APlOOODDV system Name I It1 I 111 I ID1 I S ize tlO.14 I 10 I 4 I 2048K I lOOMB Table 4 Parameters of data sets cessor is connected to three independent networks T net B-net and S-net The communication between processors is done via a torus mesh network called the T-net and broadcast communication is done via the B-net In addition a special network for barrier syn chronization called the S-net is provided To evaluate the performance of the four algo rithms synthetic data emulating retail transactions is used where the generation procedure is based on the method described in 2 Table 4 shows the mean ing of the various parameters and the characteristics of the data set used in the experiments 4.1 Measurement of Execution Time Figure 7 shows the execution time of the four pro posed algorithms using three different data sets with varying minimum support values 16\(4 x 4 proces sors are used in these experiments Transaction data is evenly spread over the processor\222s local disks In these experiments, each parallel algorithm is adopted only for pass 2 the remaining passes are performed using NPA, since the single processor\222s memory can not hold the entire candidate itemsets only for pass 2 and if it fits NPA is most efficient HPA and HPA-ELD significantly outperforms SPA 25 


tlO.14 16 processors 1m 1 161 t 1 the number ofall the transactions 127  CpP 10 I 0 0.2 0.4 0.6 0.8 1 1.2 1.4 minimum supwrt  t15.14 16 processors 0 0.2 0.4 0.6 0.8 1 1.2 1.4 rrrrm SUDPOIT  t20.14 16 processors looOa 1 A SPA  0 0.2 0.4 0.6 0.8 1 1.2 1.4 U SUDWrt  Figure 7 Execution time varying minimum support value Since all transaction data is broadcast to all of the processors in SPA its communication costs are much larger in SPA than in HPA and HPA-ELD where the data is not broadcasted but transfered to just one pre cessor determined by a hash function In addition SPA transmits the transaction data while HPA and HPA ELD transmit the itemsets which further reduces the communication costs In NPA the execution time increases sharply when the minimum support becomes small Since the can didate itemsets becomes large for small minimum sup port the single processor's memory cannot hold the entire candidate itemsets NPA has to divide the can didate itemsets into fragments Processors have to scan the transaction data repetitively for each frag ment which significantly increases the execution time 4.2 Communication Cost Analysis Here we analyze the communication costs of each algorithm Since the size of the transaction data is usually much larger than that of the candidate item set we focus on the transaction data transfer In NPA the candidate itemsets are initially copied over all the processors which incurs processor communication In addition during the last phase of the processing, each processor sends the support count statistics to the co ordinator where the minimum support condition is ex amined This also incurs communications overhead But here we ignore such overhead and concentrate on the transaction data transfer for SPA and HPA in sec ond phase In SPA, each processor broadcasts all transaction data to all the other processors The total amount of communication data of SPA at pass IC can be expressed as follows p=l i=l N It x N  1 x ID1 1 where the number of items in i-th transaction of pth processor the number of pth Drocessor's transactions In HPA the itemsets of the transaction are trans mitted to the limited number of processors instead of broadcasting The number of candidates is dependent on the data synthesized by the generator The total 26 


amount of communication for HPA at pass IC can be expressed as follows CAN M p=l i=l the amount of the candidate itemset in bytes the size of main memory of a single pre cessor in bytes One transaction potentially generate t,p ck candi dates However in practice most of them are filtered out as is denoted by the parameter CY Since a is usually small4 MkSPA  MFPA Since it is difficult to derive a we measured the amount of data received by each processor Figure 8 shows the total amounts of received messages of SPA HPA and HPA-ELD where t15.14 transaction data was used with 0.4 minimum support As you can see in Figure 8 the amount of messages received of HPA is much smaller then that of SPA In HPA-ELD the amount of messages received is further reduced since a part of the candidate itemset is handled separately and the itemsets which corre spond to them are not transmitted but just locally processed SPA HPA HPA-ELD Figure 8 the amount of messages received pass 2 4.3 Search Cost Analysis In the second phase the hash table which consists of the candidate itemsets are probed for each transac tion itemset 41f the number of processors is very small and the number of items in transaction is large then McPA could be larger than MzPA With reasonable number of processors, this does not happen as you can see in Figure 8 We are currently doing experiments on mining association rules with item\222s classifica tion hierarchy, where combination of items becomes much larger than the ordinary mining association rules When ak increases McPA tends to increase as well we will report on this case in a future paper In NPA the number of probes at pass IC can be expressed as follows p=l i=l 11 ItlCk x lak x ID x N 4 In HPA and HPA-ELD the number of searches at pass IC can be expressed as follows p=l i=l E ltlck x lakl x 5 The search cost of HPA and HPA-ELD is always smaller than SPA It is apparent that SFPA  SfPA Not only the communication cost but also search cost also can be reduced significantly by employing hash based algorithms which is quite similar to the way in which the hash join algorithm works much better than nested loop algorithms. In NPA the search cost depends on the size of the candidate itemsets If the candidate itemset becomes too large SrPA could be larger than SfPA But if it fits SFPA N SZPA  SfPA that is the search cost is much smaller than SPA and almost equal to HPA Figure 9 shows the search cost of the three algorithms for each pass where the t15.14 data set is used under 16 processors with the minimum support 0.4 In the experimental results we have so far shown all passes except pass 2 adopts NPA algorithm We applied different algorithms only for pass 2 which is computationally heaviest part of 27 


the total processing However here in order to focus on the search cost of individual algorithm more clearly each algorithm is applied for all passes The cost of 500 400 300 200 100 0 1 2 3 4 oass number    Figure 9 the search cost of SPA NPA and HPA NPA changes drastically for pass 2 The search cost of NPA is highly dependent on the size of available main memory If memory is insufficient, NPA's performance deteriorates significantly due to the cost increase at pass 2 In Figure 9 the search cost of NPA is less than SPA However as we explained before it incurred a lot of additional 1/0 cost Therefore the total execution time of NPA is much longer than that of SPA 4.4 In this section the performance comparison be tween HPA and HPA-ELD is described In HPA-ELD we treat the most frequently appearing itemsets sepa rately In order to determine which itemset we should pick up we use the statistics accumulated during pass 1 As the number of pass increases the size of the candidate itemsets decreases Thus we focused on pass 2 The number of the candidate itemsets to be separated is adjusted so that sum of non-duplicated itemsets and duplicated itemsets would just fit in the available memory Figure 10 shows the execution time of HPA and HPA-ELD for t15.14 varying the minimum support value on a 16 processors system HPA-ELD is always faster than HPA The smaller the minimum support the larger the ratio of the difference between the execu tion times of the two algorithms becomes As the min imum support value decreases the number of candi date itemsets and the count of support increases The candidate itemsets which are frequently found cause large amounts of communication The performance of HPA is degraded by this high communications traffic Comparison of HPA and HPA-ELD  0 0.5 1 1.5 2 minimum supwrt  Figure 10 Execution time of HPA and HPA-ELD at pass 2 Figure 11 shows the number of probes in each pro cessor for HPA and HPA-ELD for t15.14 using a 16 processor system for pass 2 We picked up an exam ple which is highly skewed Horizontal axis denotes processor ID In HPA the distribution of the number 14 9 c g 12 g 10 s U  2 6 4t 2 Y 0 2 4 6 8 101214 processor ID Figure 11 The number of search of HPA and HPA ELD at pass 2 of probes is not flat Since each candidate itemset is allocated to just one processor the large amount of messages concentrate at a certain processor which has many candidate itemsets occurring frequently In HPA-ELD the number of probes is compara tively flat HPA-ELD handle certain candidate item sets separately thus reducing the influence of the data skew However as you can see in Figure 11 there still remain the deviation of the load amongst processors If we parallelize the mining over more than 64 proces sors we have to introduces more sophisticated load 28 


18001  balancing mechanism which requires further investi gation 4.5 Speedup Figure 12 shows the speedup ratio for pass 2 vary ing the number of processors used, 16 32 48 and 64 where the curve is normalized with the 16 processor execution time The minimum support value was set to 0.4 4.5 0.5 1 1 0 I 10 20 30 40 50 60 70 number of mxessors Figure 12 Speedup curve NPA HPA and HPA-ELD attain much higher lin earity than SPA HPA-ELD an extension of HPA for extremely large itemset decomposition further in creases the linearity HPA-ELD attains satisfactory speed up ratio This algorithm just focuses on the item distribution of the transaction file and picks up the extremely frequently occurring items Transferring such items could result in network hot spots HPA-ELD tries not to send such items but to process them locally. Such a small mod ification to the original HPA algorithm could improve the linearity substantially 4.6 Effect of increasing transaction Figure 13 shows the effect of increasing transac tion database sue as the number of transactions is increased from 256,000 to 2 million transactions We used the data set t15.14 The behavior of the results does not change with increased database size The minimum support value was set to 0.4 The num ber of processors is kept at 16 As shown each of the parallel algorithms attains linearity 5 Summary and related work In this paper we proposed four parallel algorithms for mining association rules A summary of the four database size Sizeup 0 I 0 500 loo0 1500 uxw amount of transaction thousands Figure 13 Sizeup curve algorithms is shown in Table 5 In NPA the candi date itemsets are just copied amongst all the proces sors Each processor works on the entire candidate itemsets NPA requires no data transfer when the supports are counted However in the case where the entire candidate itemsets do not fit within the mem ory of a single processor the candidate itemsets are divided and the supports are counted by scanning the transaction database repeatedly Thus Disk 1/0 cost of NPA is high PDM, proposed in 6 is the same as NPA which copies the candidate itemsets among all the processors Disk 1/0 for PDM should be also high The remaining three algorithms SPA HPA and HPA-ELD partition the candidate itemsets over the memory space of all the processors Because it better exploits the total system's memory, disk 1/0 cost is low SPA arbitrarily partitions the candidate itemsets equally among the processors Since each processor broadcasts its local transaction data to all other pro cessors the communication cost is high HPA and HPA-ELD partition the candidate itemsets using a hash function which eliminates the need for transac tion data broadcasting and can reduce the comparison workload significantly HPA-ELD detects frequently occurring itemsets and handles them separately which can reduce the influence of the workload skew 6 Conclusions Since mining association rules requires several scans of the transaction file its computational requirements are too large for a single processor to have a reasonable response time This motivates our research In this paper we proposed four different parallel algorithms for mining association rules on a shared nothing parallel machine and examined their viabil 29 


Table 5 characteristics of algorithms ity through implementation on a 64 node parallel ma chine the Fujitsu AP1000DDV If a single processor can hold all the candidate item sets parallelization is straightforward It is just suf ficient to partition the transaction over the proces sors and for each processor to process the allocated transaction data in parallel We named this algo rithm NPA However when we try to do large scale data mining against a very large transaction file the candidate itemsets become too large to fit within the main memory of a single processor In addition to the size of a transaction file a small minimum support also increases the size of the candidate itemsets As we decrease the minimum support computation time grows rapidly but in many cases we can discover more interesting association rules SPA HPA and HPA-ELD not only partition the transaction file but partition the candidate itemsets among all the processors We implemented these al gorithms on a shard-nothing parallel machine Per formance evaluations show that the best algorithm HPA-ELD attains good linearity on speedup by fully utilizing all the available memory space which is also effective for skew handling At present we are doing the parallelization of mining generalized association rules described in 9 which includes the taxonomy is-a hierarchy Each item belongs to its own class hierarchy In such mining associations between the higher class and the lower class are also examined Thus the candidate itemset space becomes much larger and its computation time also takes even longer than the naive single level association mining Parallel pro cessing is essential for such heavy mining processing Acknowledgments This research is partially supported as a priority research program by ministry of education We would like to thank the F\221ujitsu Parallel Computing Research Center for allowing us to use their APlOOODDV sys tems References l R.Agrawal T.Imielinski and ASwami 223Min ing Association Rules between Sets of Items in Large Databases\224 In Proc of the 1993 ACM SIGMOD International Conference on Manage ment of Data pp207-216 May 1993 2 R.Agrawal and RSrikant 223Fast Algorithms for Mining Association Rules\224 In Proc of the 20th International Conference on Very Large Data Bases pp.487-499 September 1994 3 J.S.Park M.-S.Chen and P.S.Yu 223An Effec tive Hash-Based Algorithm for Mining Associ ation Rules\224 In Proc of the 1995 ACM SIG MOD International Conference on the Manage ment of Data SIGMOD Record Vo1.24 pp.175 186 June 1995 4 H.Mannila H.Toivonen and A.I.Verkamo 223Ef ficient Algorithms for Discovering Association Rules\224 In KDD-94:AAAI Workshop on Knowl edge Discovery in Databases pp.181-192 July 1994 5 A.Savasere, E.Omiecinski and S.Navathe 223An Effective Algorithm for Mining Association Rules in Large Databases\224 In Proc of the 21th International Conference on Very Large Data Bases pp.432-444 September 1995 6 J.S.Park M.-S.Chen and P.S.Yu 223Efficient Parallel Data Mining for Association Rules\224 In Proc of the 4th International Conference on In formation and Knowledge Management pp.31 36 November 1995 7 T.Shintani and M.Kitsuregawa 223Considera tion on Parallelization of Database Mining\224 In Institute of Electronics Information and Com munication Engineering Japan SIG CPS Y95 88 Technical Report Vo1.95 No.47 pp.57-62 December 1995 8 T.Shimizu T.Horie and H.Ishihata 223Perfor mance Evaluation of the APlOOO Effects of message handling broadcast and barrier syn chronization on benchmark performance-\224  In S WO PP 22292 9.2 ARC 95 Information Processing Society of Japan Vo1.92 No.64 1992 9 R.Srikant and R.Agrawal 223Mining Generalized Association Rules\224 In Proc of the 21th Inter national Conference on Very Large Data Bases pp.407-419 September 1995 30 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


