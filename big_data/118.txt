024 Discover Relaxed Periodicity In Temporal Databases 1 Tang Changjie  Yu Zhonghua  Zhang Tianqing Computer Department, Sichuan University 610064,Chengdu City, China E-mail: chjtang@scuu.edu.cn 1 6XSSRUWHG E 7KH 1DWLRQDO 6FLHQFH RXQGDWLRQ RI KLQD 1R\021 031\034\032\032\026\023\030\024 Abstract The Relaxed-Periodicity Pattern describes loose`cyclic behavior of objects while allowing uneven stretch or shrink on time axis, limited noises, and inflation /deflation of attribute values. To discover Relaxed-Periodicity from Temporal Databases, we propose the concepts of Attribute Trend, Trend Inertia, Peak-Valley Pattern, Inertia Algorithm with Anti-noise ability, as well as the Peak-Valley Algorithm, and show that the implementation prototype is efficient  1 Introduction In the real world, many objects are associated with time and periodic patterns. To support time in databases efforts have been put on the designing of Time Related Database such as the 13 Temporal Data Models \(TDB described in [1 an d H b as e w h ich is th e f i r s t TD B prototype implemented in China [2,3  T o  di s c o v e r periodic pattern, some fruitful researches have been made by researchers. Ozden,B. et al. studied the mining of cyclic association rules [4  J i a w ei H a n  et a l  in te g r at ed data cube and Apriori data mining techniques for mining segment-wise periodicity in regard to a fixed length period and showed that data cube provides an efficient structure and convenient way for interactive mining of multiple-level periodicity [5  H o w e v e r  m a n y  c y c lelik e phenomena do not fit strict periodicity because blending with noises and inflation /deflation, or stretching unevenly on the time axis. We refer this kind of phenomena as Relaxed Periodicity. Some previous studies investigated this issue: R. Agrawal et. al. propose fast Similarity search Method in the presence of Noise, Scaling, and Translati  S a k o e  H    a n d  Ch i b a  S p ropos e d  dy na m i c programming algorithm t o  s o l v e  t h e  s t r e t c h a n d  s hri nk in the time axis, but only consider the full matching in voice recognition. Few attentions were paid to mining Relaxed- periodicity   This paper discusses the techniques for mining relaxedperiodicity from Temporal Databases The main contributions include the following z  Different from previous studies that rely on the every precise attribute value, we propose the Trend pattern  including concepts of Attribute Trend such as UP,DOWN, PEAK, and VALLEY Trend Inertia and Inertia Algorithm which can ignore small noise and determine the trend of attribute values. The idea is based on the observation that a moving object with inertia can overcome resistance and keep its moving direction z  With the new concept of Peak-Valley Chain our PeakValley Algorithm can search relaxed periodicity while disregarding uneven stretch of time axis and inflation/ deflation The remainder of the paper is organized as follows Section 2 gives the concepts related to RelaxedPeriodicity. Section 3 defines the model for Relaxedperiodicity. Section 4 describes our algorithms for mining relaxed periodicity from temporal database. Section 5 discusses the application of relaxed periodicity, such as inflation/deflation. Finally a conclusion is given in Section 6 2. Preliminaries and Basic Concepts 2.1 The related concepts and operations in TDB 2.1.1 Chronon and Life Span As defined in [1 Chronon is the indivisible time duration supported by temporal system. Time interval is the set of finite ordered chronons {t 1 t 2 205t n denoted as [t 1 t n   Th e len g t h  o f  ti me 


025 interval is the number of chronons in the interval. The time granularity hierarchy is a user defined series of time interval, such as \(Chronon, Second, Minute, Hour Day, Month, Year\. The current time granularity is the smallest time interval to expand temporal information The life span of record \(or attribute\ is the time interval in which the value of record \(or attribute\ can be accessed The function GetLifeSpanBegin\(t\ return the start time of a life span containing t 2.1.2 TDrill and Troll Temporal databases have build-in operations to modify the current time granularity such as TDrill /Troll in Hbase Expand/Coalesce in HSQL Fold/Unfold in IXRM and Nest/Unnest , Pack/Unpack in other TDB models 1 They are similar to the Drillingdown/ Rolling-up in data mining, and will be refereed as TDrill/TRoll in this paper Example 1 Consider a temporal relation with schema Name, Income, LBegin, LEnd> in Hbase model, where Lbegin, LEnd i s t h e l i fe s p a n of re c ord  G i v e t r e e records with time granularity of month   r1= <John, 3000,1975:01:, 1975:01   r2= <John, 4000,1975:02:, 1975:02   r3= <John, 5000,1975:03:, 1975:03 The operation TRoll-up to SEASON will produce a record r4= <John, 120000, 1975:Spring, 1975: Spring The inverse operation TDrill on r4 will Drill-down and get record r1,r2 and r3 c 2.1.3 Retrieve Time Related Attribute Value In temporal database, given time t and attribute A, it is easy to get a record r such that the life span of r contains t, and return the value of r[A   Th is r etr ie v a l me ch a n is m h a s various implementation, such as the function Get_Attri_At\( A, T\ used  in this paper Fig .1 Monthly passengers information h 1000 Transaction Number Season Passengers  h 1000 1 95 Spring 40 2 95 Summer 20 3 95 Autumn 25 4 95 Winter 20 5 96 Spring 45 6 96 Summer 25 7 96 Autumn 35 8 96 Winter 26    Fig.2 Seasonally Passengers information 2.2 The Observations Following example gives intuitions to Relaxed Periodicity Pattern Example 2 Consider the passenger information for an airport from January 1995 to December 1996. In the daily routine, the chronon is MINUTE. For the simplicity, we Roll-up to MONTH \(as shown in Figure 1\ and SEASON as shown in Figure 2  It is easy to get the following observations 1  The tendency There are five kinds of tendencies  that is, PEAK \(such as Sep.1995, Aug.1996 etc\ VALEY such as May 1995, Sep.--Nov.1996 etc.\ UP \(such as Jan.1995,Aug,1995 etc\,  DOWN \(such as April 1995\and EdgeInit \(such as 1995.1 2  Noises and the Inertia for Trend Assume that the system can tolerate noises not greater than 1 Consider the trend at 1996.4 in Fig.1. The point 1996.4,9\ is slightly lower than point \(1996.5, 10 but the difference is not greater than 1. By assumption  the DOWN trend at April of 1996 will be continued to May of 1996 by Inertia Thus the overall trend for interval [1996.2, 1996.6 i s D O W N  The above method to figure out the trend is called Inertia Algorithm that will be discussed later 3  Inflation and uneven stretch on time dimension Consider the peak points P1="Sep.1995 P3="Aug.1996 " and the valley points V1="May 1995", V3="June.1996 ". The Length from P1 to P3 is 11 months, Length form V1 to V3 is 13 months Thus there exist uneven shrinks or stretch on time dimension. Comparing the corresponding months of 1995 and 1996, the inflation is clear 265 3 Model for Relaxed-Periodicity Pattern To formalize above observations, we now define the Trend , the Time Array and the Trend Array  for given attribute   024\025 024\030 024\026 033 030 032\032 033 024\023 033 030 032 024\027 024\032 024\027 034 024\023 031 024\023 024\032 033\033\033 024\023 023 030 024\023 024\030 025\023 034\030\021\024 034\030\021\027 034\030\021\032 034\030B\024\023 034\031\021\024 034\031\021\027 034\031\021\032 034\031B\024\023 


026 Definition 1  Let [t, t\222 b e  th e l i f e s p an o f  at tr ib u t e  A  g is the current time granularity, TrendType be the enumerate set  Up, Peak, Down, Valley, EdgeInit 1  The trend function of Attribute A is the map  form the Trend of Attribute to Trend Type AttriTrend: [t, t\222  027 TrendType Its value AttriTrend\(t 310 A is called the trend of A at time t 2  L= Length\([t, t  g i s c a l l e d t h e  nu m b e r of c h e c k points for attribute A. The Set TimeArray={t k t k t+gk,0<=k<=L is called the Time array for attribute A.  The set TrendArray={A k  A k  AttriTrend \(t k A\,0<=k<L is called the Trend Array of A, its k-th member is denoted as AttriTre  c   In the real world, objects possess inertia with which it overcomes disturbs \(or noises\ while trying to keep its original status. For example, a ball rolling down from hill can overcome a small concave and continue rolling down moreover, the longer duration of rolling, the larger inertia it has. To formalize this observation, we have Definition 2  Let [t, t\222  an d T r en d T y p e b e as  ab o v e  an d InertiaDom be a subset of real numbers 1  The inertia function of attribute A is the map from TrendType h 2 t,t to InertiaDom  GetInertia TrendType h 2 t,t 027 InertiaDom   Its value denoted as GetInertia \(TheTrend, Durat    where the Durat is the duration of TheTrend 2  The inertia evaluation function is the map form TrendType h InertiaDom to NoiseDom, denoted as InertiaToAntiNoise or I_To_N I_To_N:: TrendType h InertiaDom 027 NoiseDom Its value is denoted as I_To_N \(TheTrend InertiaValue c   The inertia evaluation function transfer inertia to the the capacity to overcome noises. The separation of two functions \(GetInertia. and InertiaToAntiNoise\ can increases flexibility in process. The design quality of the functions will influence the mining process Example 3 In the Hbase implementation, Users can design their own GetInertia function to fit the context of application. The comments give intuitive illustrations Int Hbase GetInertia TheTrend, Duratition a user-defined inertia function in C switch \(TheTrend   case PEAK : return 0    no inertia at peak.\(hence the Peak state is unsteady case VALLEY: return 0.2 inertia at valley can overcome noise of size 0.2 It is relatively steady    case UP: x = Duration; return -a*x 2 bx a>0,b>0, The inertia of UP trend is a hump-like  function. It is increasing by x for x< b/2a, and decreasing by x for b/2a < x hump-like function is useful for many real objects    case DOWN: return 0.2 * Ln\(\(Duration The inertia of down trend is in proportion with the logarithm duration of the trend Default: return EdgeInit  c Example 4 This is a simple user-defined inertia evaluation function for Hbase NoiceDom Hbase InertiaToAntiNoise TheTrend InertiaValue swich \(TheTrend  PEAK, VALLEY AntiNoiseCapacity = InertiaValue /10 DOWN: AntiNoiseCapacity = InertiaValue UP:  AntiNoiseCapacity = 1.1*InertiaValue allow 10% inflation return AntiNoiseCapacity  The trend inertia functions and inertia evaluation function depend on the properties of underlying objects For example, different stocks may have different inertia functions and inertia evaluation The deep analysis on trend inertia and inertia evaluation functions  will be discussed elsewhere  4  Mining Relaxed-Periodicity From Temporal  Database   In the following algorithms, the function SimpleTrend V1,V2,V3\ompares attribute values at continuous time points t1,t2 and t3, and returns the trend at t2. The detail is omitted because the simplicity. The function GetLifeSpanBegin is a typical and simple operation in Temporal Database. Its meaning is as indicated by name and illustrated in Section 2. The function GetInertia and InertiaToAntiNoise are as illustrated in example 3 and 4 The algorithm is written in C++ style for the convenience to illustrate data structure, key programming techniques and loop analysis Algorithm 1  Inertia Trend Algorithm Input Attribute A MaxNoise \(the upper bound of noise Time array TimeArray[L 310 L>=3 Output the Attribute trend array TrendArray[L  void HBase GetTrendWithInertia A MaxNoise TimeArray 


027 K=0 327 Duration=0 PrevTrend = EdgeInit Initialize current trend and duration  While \( K< L-2  Step1  Get values at three continuous chronons for \( J=0; J <=3; J  T [J   G e t L i f e S pa nB e g i n   T i m e A rr a y[K  J  get start time of life span V[J G et _ A ttr _ A t A  T J    V  is  a  b u f f e r  ar r a y  f o r  a ttr ib u t e v a lu es end of for if \(K=0 && PrevTrend ==EdgeInit      at the first chronon of TimeArray, force the      trend as same as the trend at next time point TrendArra  P re vT r e nd SimpleTrend\( V  V 1  V 2  by trivial algorithm  continue;  // back to the loop entrance, note K=0 310 but PrevTrend  is defined end of if  Step2 :Get and transfer Trend inertia PrevInertia= GetInertia\(A 310 PrevTrend , Duration Get current inertia, see Example 3   MaxiNoise   Transfer to anti noise capacity InertiaToAntiNoise\(PrevTrend, PrevInertia 327 Step 3: Computing current Trend while using inertia as anti-noise capacity Switch \(PrevTrend case  Down:  // It is down at V[0  if \(\( V    V 1]-M a x N o i s e  1 V 2*M a x N o i s e  such as Sep.1996 in Fig.1 V[0  1 V a x N o i s e    V  2]-2*M a x N o i s e   CurrTrend = Down keep going down at V b y  i n e r t i a  else  CurrTrend =Valley; // such as Oct.1996 in Fig.1 break case  Up  I it is up at V[0 if \( \(V 0 V  M a x N o i s e    V 2    2  M a xN o i s e  Noise less than anti-noise capacity V   V 1  M a xN oi s e  0  V 2*M a x N o i s e   CurrTrend  = Up ;  // by inertia, keep going up    else CurrTrend = Peak ; //V[1 s  p e a k break case Valley if  \( V[1 1 V M a xN o i s e  Curr T r e n d V a l l e y   CurrTrend =Up  such as Nov..1996 in Fig.1 break case  Peak if \( V1 0 V2 +MaxNoise\CurrTrend=Peak else CurrTrend = Down break otherwise  AskUsersHelp complicated noises case, ask user for help end of  case TrendArray [K+1  Curr T r e n d 327 If \(PrevTrend== CurrTrend\ Duration increate the duration of current status else Duraion=0; // start a new status PrevTrend= CurrTrend  save it , will be used in next loop K 327 end of while Step 4 : output trend array TrendArray [LE d g e Ini t     the trend at last chronon depends on future\222s data return TrendArray  Theorem 1   Let L be the size of TimeArray of attribute A.. Then computing complexity of Algorithm 1 is O\(L Proof  In the switch statement of the Inertia Algorithm Algorithm 1\, each CASE  only simply compares attribute values, Although the functions GetInertia and InertiaToAntiNoise may contain some basic mathematical functions such as logarithm, square root etc. as shown in Example 3 and Example 4, but they are independent from L  Hence the above computation work can be evaluated by a constant C. In the WHILE statement the total loops is L - 2, therefore the total computation complexity is C h L-2\=O\(L c We now define the chain structure to keep the time series information for all peaks and valleys Definition 3  Peak-Valley Chain Let A be the attribute concerned, L be the size of the time array of A and  0<k<L 1  The Peak-Valley Record of A is the 3-tuple S  PV H,T\, where H is the value of attribute A at time T PV= PEAK if the trend of A at T is PEAK , or PV VALLEY, if the trend of A at T is VALLEY 2  The Peak-Valley Chain or Peak-Valley Pattern of Attribute  A is the time series chain S 1 316 S 2  316 205 316 S n  where S i is called the i-th Peak-Valley Record, and n is the length of chain c    Following algorithm describes the techniques to pick up peaks and valleys form Trend Array Algorithm 2 Peak-Valley Algorithm 


030 Input   TrendArray[L i  e   t h e ou t p u t of  A l gor i t h m 1 Output PeakValleyChain[L   i e th e P e a k V a l l e y C h ai n of attribute A Hbase Get_Valley_Peak As a member function of  Hbase int K=0; ListLen=0  while \( K<L-1\ //copy the data of peak or valley from TrendArray to PeakValleyChain  if  TrendArray [K  P e a k   TrendArray [K   V a l l e y    PeakValleyChain[ListLen  P V PeakValleyChain{K}; //is Peak or Valley Temp_T= TimeArray[K  PeakValleyChain[ListLen  T   T emp _ T  as life span start PeakValleyChain[ListLen  H  Get_Attr_At\(A, Temp_T \; //attribute values ListLen t++; // count add one end_of _if K end_of_while output PeakValleyChain[L    c   Note that, the size of Peak-Valley chain is much smaller than the size of Trend Array In Trend Array we only store the trend value, but do not store the attribute value This saves space. In the Peak-Valley Chain we store the value of attribute, since it is the part of the tendency pattern   We need a structure to store the aggregation data for all peaks and valleys Definition 4  Let the context be as same as Definition 1, MaxPV<L is an integer defined by user as loop control 1  Let K>1,S j1 S j2 205S jk be k continuous peaks \(or k continuous valleys\. The time difference S jk T-S j1 T is called k-peak-span \(or k-valley-span\ started at j1 The k-peak-span and k-valley-span are also referred as k-span. The set of k-span ordered by the start point called k-span-array denoted as SpanArray k  For coherence we define SpanArray k 0} when k=0 and k=1 2  For a fixed k>1, denote MaxS = Max\( SpanArray k  MinS=Min\(SpanArray k AveS = Average SpanArray k and  DiffS= MaxS-MinS. The 4tuple \(MaxS, MinS, AveS, DiffS\ is called the kspan aggregation Record 3  The Peak-Valley aggregation Array is defined as PV_AggregateArray[MaxPV  w h e r e  P V _ AggregateArray k is  th e k-span Aggregation Record MaxPV is an user defined constant for loop control Theorem 2 \(Computing K-Span Array Let A be the attribute  considered  Let L be the number of the check points of A,  S 1 316 S 2  316 205 316 S n be  the Peak-Valley Chain produced by Algorithm 2, and n>2 1  If i \226 j >1 and i - j is an odd number. then S i PV S j PV 2  The k-span array SpanArray k can be computed as following If k 0 1, SpanArray k j  0  f or a l l  j    0   If k>1, SpanArray k j   S 2k-2+j T - S j T. for j >0 3  For n>2,k>1, the number of elements in SpanArray k is n+2-2k Proof 1\The Peak-Valley Chain is constructed based on the attribute trend array. In the step 3 of Algorithm 1 we can see that: DOWN precedes VALLEY, PEAK precedes DOWN, UP precedes PEAK and VALLEY precedes UP. Thus between two VALLEYS there exists at least one PEAK. By the same reason, there exists at least one VALLEY between two PEAKS. Hence the PEAK's and VALLEY's must be one follows another. Thus, S 1 PV S 3 PV = S 5 PV = ..., and  S 2 PV = S 4 PV =  S 6 PV as desired.  In another word, the output of Algorithm  1 is consistent with our observation to the real world   2\The cases for k=0 and k=1 are trivial. Now consider k-span array for k>1. Without loss generality, we may assume S j PV is PEAK,. Now we find integer x, such that S x PV is PEAK and there are k PEAK\222s \( including S j and S x are between S 1 and S x By \(1\, there are k-1 VALLEY\222s are between S j and S x Hence x= k +\(k-1\j 1=2k-2+j as desired 3\By \(2\ S j and S 2k-2+j produces on k-span. Note that 1 0 j and 2k-2+j 0 n ,hence j 0 n+2-2k. From j=1 to j=n+22k, each j is a starting point of a k-span. Thus the  total number of k-span in SpanArray k is n+2-2k  c Example 4 Let  S 1 316 S 2  316 205 316 S 9 be  the Peak-Valley Chain produced by Algorithm 2,  S 1 PV= S 3 PV S 5 PV  205.= S 9 PV=PEAK and  S 2 PV= S4.PV= S6.PV S 8 PV=VALEEY. It is easy to verify that 1\pan={ S 2x2-2+j T - S j T | for j >0,j<9 S 3 T-S 1 T,  S 4 T-S 2 T, S 5 T-S 3 T S 6 T-S 4 T,  S 7 T-S 5 T S 8 T-S 6 T,  S 9 T-S 7 T 2\pan has 9+2-2 h 2=7 elements 3\pan={ S 2x3-2+j T - S j T | for j >0,j<9 S 5 T-S 1 T,  S 6 T-S 2 T, S 7 T-S 3 T S 8 T-S 4 T,  S 9 T-S 5 T 4\pan has 9+2-2 h 3=5 elements c Theorem 3 The Computing complexity to compute kspan Array is O\(n 2 where n is the size of  Peak-Valley Chain Proof If n is odd number, it computes one more k-span i.e. the case of k= \(n+1\/2 \ than the case of n-1. This does not affect to the complexity, Thus without loss accuracy, we may assume that n is even number and n=2m  By \(3\ of Theorem 2, the members in the m-span m-1\pan,\205,2-span are 2,4,\205\(n+2-4\respectively. The 


031 computing for each span needs one subtraction, the total number of subtractions is 2+4+\205+\(n-2\=n\(n-2\4. Thus the complexity is O\(n 2  c  It is easy to see that based on the Peak-Valley Chain The Aggregation array PV_AggregateArray[MaxPV c a n be constructed by calling simple functions such as Max Min, Average in a loop. The implementation in Hbase is G et_PV_AggregateArray Its details were omitted because simplisity. It will be used in following algorithm Algorithm 3 Mining Relaxed-Periodicity Input Temporal Database\(TDB attribute A loop control number MaxPV Output The Relaxed-periodicity for the values of attribute A in TDB Procedure z  Step1 Select appropriate time granularity, Use temporal operation such as projection and  TDrill TRoll, prepare time series data  including TimeArray of size L  as the inputs of Algorithm 1 z  Step2 By Inertia Algorithm Algorithm1  produce TrendArray[L for a t t r i b ut e  A by i n e r t i a pr i n c i p l e  z  Step3 By Peak-Valley  Algorithm produce PeakValley Chain z  Step4 For k=2 to MaxPV, computing the k-Span Array by Theorem 2. Then construct k-th PeakValley Aggregation Record as PV_ AggregateArray  Find integer j such that PV_ AggregateArray[j  D i f f S is m i ni mum  for 2 0 j 0 MaxPV z  Step5 Output PV_AggregateArra  A v e S a s Relaxed-periodicity of Attribute A c Example 5 Mining the Relaxed-periodicity from the data in Figure 2 1  Set parameters as following   The current granularity  is MONTH  MaxNoise=1 loop control number MaxPV=3   Trend inertia function is GetInertia \(A , TrendType 310 Duration 0.01*Duration Inertia transfer function is InertiaToAntiNoise \(TheTrend, InertiaValue InertiaValue 2\pply our algorithms 1, get the TrendArray as Fig .4 3\ Algorithm 2, based on the TrendArray and    scanning the temporal database ,get the Peak -valley Chain The chain is as following S1= \(Peak, 15, 95. 02 316 S2= \(Valley, 5, 95.05 316 S3= \(Peak, 10, 95.09 316 S4= \(Valley, 5, 95.11 316 S5= \(Peak , 17, 96.02 316 S6= \(Valley, 6, 96.04 316 S7= \(Peak , 17, 96.08 316 S6= \(Valley,8, 96.10  Time 1995 Trend Time 1996 Trend 95.1 EdgeInit 96.1 Up 95.2 Peak 96.2 Peak 95.3 Down 96.3 Down 95.4 Down 96.4 Valley 95.5 Valley 96.5 Up 95.6 Up 96.6 Up 95.7 Up 96.7 Up 95.8 Up 96.8 Peak 95.9 Peak 96.9 Down 95.10 Down 96.10 Valley 95.11 Valley 96.11 Up 95.12 Up 96.12 EdgeIni t   Fig .3  The trend array 4\For 2 0 k 0  MaxPV=3, construct k-span array the results are SpanArray 2 7,6,5,6,6.4 SpanArray 3 12,12,11,11 By simple aggregation function, get Peak-Valley Arregation Array   Its contents is shown in Fig. 4  with current time granularity = MONTH Agg K MaxSpan MinSpan AveSpan DiffSpan  274 5.63 3 12 11 11.5 0 .5   Fig. 4 The statistics for k-Span Array 5\Note that PV_AggregateArra h a s m i n i m u m DiffSpan value. Thus the relaxed periodicity of attribute A is the AveSpan in PV_AggregateArra i  e  11  5  Werounded 11.5 to 12  \( since time granularity is not dividable \. Thus the periodicity. of the specified attribute is 12 months c 5 Discussion   1\ Note that our mining process is based on the PeakValley Chain \(or Peak-Valley pattern\ regardless the inflation or deflation. Once we get the Relaxedperiodicity p, we can figure out the average inflation at k+1\th period \(with respect to k-th period \y the following procedure 


032 Procedure 2  GetInflation k \ // inflation at \(k+1\th segment  TotalInflation =0 For \(i=kp; i<\(k+1\*p; i TotalInflation  Get_Attr_at\(A,k*p+p+i\Get_Attr_at\(A, k*p+i    AverageInflation = TotalInflation / L Return AverageInflation c 2\e apply our algorithms to the Data Warehouse for Earthquake information of Sichuan Province. The test data keep the transmogrification information for the AnNing River Area measured  by stain gauge from 1970 to 1995. The total numeber of records  is 10000 The result is consistent with historical facts. The performance study \(not included here for lack of space shows that the implementation of our methods in Hbase is acceptable for temporal databases with practical scale 6 Summary We discuss the phenomena with Relaxed Periodicity  introduce the concepts of attribute trend, trend inertia and Peak-Valley Chain propose the Inertia Algorithm and Peak-Valley Algorithm    Note that, the Trend Inertia is the essential property of object. Once we construct a good inertia function and a good inertia evaluation function, we can calculate the trend of temporal attributes just like what happens in the real world. The Peak-Valley Chain \(or Peak-Valley pattern disregards the noises and inflation, gives global trend of attributes. The pattern itself is a kind of knowledge, for example, when we talk about a pattern of career \223with tree valley and tree peaks, and each peak is higher than previous peak\224, people may remember a great statesman in the contemporary era of China. The further study on Peak-Valley Chain, Inertia function and inertia transfer function will be discussed elsewhere References 1   T e m po r a l D a t a ba s e s  T h eor y  D e s i gn a n d   I m p l e m en t a t i on  A Tansel, J.Clifford, S.    Dadia,.Jajodia,  A.Segev and R.Snodgrass, The Benjamin Cummings Publishing Company 1993 2  Tang Changjie and Xiong Min 000 The Temporal Mechanisms in  HBase 001 Journal of Computer Science and  Technology,  Vol.11, No.4 July 1996, P365\227371 304 3  Tang Changjie, Yu Zhonghua, Zhang Tainqing, Xu DaiGangang and Yang Feng: \223Storage By Separate Historical Segment in Temporal Database\224, in Proceedings of The 15-th Conference   on Database of China \(CDBC 98 \.Nanjing city, Oct.1998 4  Ozden,B., Ramaswamy, S. And Silberschatz, A. 1998. Cyclic association rules, In Proc. Of 1998 international Conference Data Engineering \(ICDE\22298\, p412-421 5  Jiawei Han, WanGong anf Yiwen Yin :Mining Segment-wise Periodic Pattern in Time Related   Databases, Proc. Of 1998 of International Conf. On Knowledge Discovery and Data   Mining\(KDD\22298\ New York City, NY. Aug 1998 6  Rakesh Agrawal, King-Ip Ling, Harpreet S. Sawhney Kyuseok Shim,  Fast Similarity Search    in the presence of Noise, Scaling, and Translation in Time-Series Databases Procedings of    the 21-st VLDB Conference Zurich Switzerland 1995 7  Sakoe,H., and Chiba,S.1990, Dynamic Programming Algorithm Optimization for  Spoken   Word Recognition In Readings  In speech Recognition eds. Waibel,A. And Lee,K., 159-  165.San Mateo,CA: Morgan Kaufman Publishers, Inc 8  Usama M.Fayyad,Grerory Piatetsky-Shapiro, Ed Advanced in Knowledge Discover and  Data Mining, AAAI Press The MIT Press 1996.p1-25 


Ministry of Education Science Sports and Culture of Japan References l R Agrawal and R Srikant Fast algorithms for mining association rules In Proc of the 20th International Conference on Very Large Data Bases pages 487-499 Santiago, Chile 1994 2 Alta Vista Company Alta vista http://www.altavista.com 31 C Chen Information Visualization and Virtual En vironment Springer 1999 4 U M Fayyad G Piatetsky-Shapiro P Smyth, and R Uthurusamy Advances in Knowledge Discovery and Data Mining AAAI/MIT Press 1996 5 R Feldman Practical text mining In Second Sym posium on Principles of Data Mining and Knwoledge Discovery PKDD-97 Nantes, France 1998 fresheye http://www.fresheye.com 6 Fresheye Cooporation  Google Inc google http://www.google.com 181 A Gupta and S Mumick Materialized Views Tech niques Implementations and Applications The MIT press 1998 9 J Han S Nishio H Kawano and W Wei Generalization-based data mining in object-oriented databases using an object cube model Data and Knowledge Engineering 25 1998 lo M Kawahara and H Kawano Performance evaluation of bibliographic navigation system with association rules from roc convex hull method Transactions of the IPSJ:Database 40\(SIG3\(TOD1 1999 I 11 M Kawahara and H Kawano Performance evaluation of bibliographic navigation system with materialized association rules Transactions of the IEICE D-I J82 1121 Wf Kawahara and H Kawano The other thresholds in the mining association algorithm In Proc of The 14th International Conference on Systems Engineer ing ICSE 2000 volume 1 Coventry UK Sep 2000 I31 M Kawahara H Kawano and T Hasegawa Data mining technologies for bibliographic navigation sys tem Transactions of the ZPSJ 39\(4 I41 H Kawano Mondou Web search engine with textual data mining In Proc of IEEE Pacific Rim Confer ence on Communications, Computers and Signal Pro cessing pages 402-405 1997 15 H Kawano Text and web mining panel discussion In International Workshop and Web Knowledge Dis covery and Data Mining WKDDM\2222000 in conjunc tion with PAKDDOO Keihanna Japan 2000 To see or not to see is that the query In Proc of 14th ACM/SIGIR pages 134-141 Chicago IL USA 1991 D-I\(1 1999 16 R R Korfhage 17 M Koster Guidelines for robot writers 18 Y Matsumoto S Kurohashi T Utsuro Y Myoki and M Nagao Japanese morphological analysis sys tem juman manual version 1.0 Nara Institute of Sci ence and Technology 1993 19 R Michalski I Bratko h4 Kubat and eds Ma chine Learning and Data Mining Methods and Appli cations John Wiley  Sons 1998 20 NTT-X goo http://www.goo.ne.jp 21 Members of the Clever Project Hypersearching the web June 1999 22 F Provost and T Fawcett. Analysis and visualization of classifier performance Comparison under imprecise class and cost distributions In Proceedings of 3rd Int 222I Conference on Knowledge Discovery and Data Mining KDD-97 pages 43-48 1997  Yahoo Inc Yahoo http://www.yahoo.com 24 1 


FIGURE 5 Execution time and rules returned versus minimum coverage for the various algorithms FIGURE 6 Execution time of dense_0002 as minconf is varied for both data-sets. Minimum coverage is fixed at 5% on pums and 1% on connect-4 FIGURE 7 Maximum confidence rule mined from each data-set for a given level of minimum coverage   1 10 100 1000 10000 100000 0 10 20 30 40 50 60 70 80 90 Execution time \(sec Minimum Coverage connect-4 apriori_c  dense_0002   dense_002   dense_02    1 10 100 1000 10000 100000 1e+06 0 10 20 30 40 50 60 70 80 90 Number of Rules Minimum Coverage connect-4 apriori_c  dense_0002   dense_002   dense_02    1 10 100 1000 10000 100000 0 10 20 30 40 50 60 70 80 90 Execution Time \(sec Minimum Coverage pums apriori_c  dense_0002   dense_002   dense_02    1 10 100 1000 10000 100000 1e+06 1e+07 0 10 20 30 40 50 60 70 80 90 Number of Rules Minimum Coverage pums apriori_c  dense_0002   dense_002   dense_02    0 500 1000 1500 2000 2500 3000 3500 20 25 30 35 40 45 50 55 60 65 Execution time \(sec minconf pums  connect-4  1 10 100 1000 10000 100000 1e+06 20 25 30 35 40 45 50 55 60 65 Number of Rules minconf pums  connect-4    0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100 Highest Rule Confidence Minimum Coverage pums  connect-4 


8.2  Effects of minimum confidence The next experiment \(Figure 6\ws the effect of varying minconf while fixing minimp and minsup to very low values. With connect-4, we used a minimum coverage of 1%, and with pums, a minimum coverage of 5%. Minimp was set to .0002 with both data-sets. As can be extrapolated from the previous figures, the number of rules meeting these weak minimp and minsup constraints would be enormous As a result, with these constraints alone, Dense-Miner exceeds the available memory of our machine The efficiency of Dense-Miner when minimum confidence is specified shows that it is effectively exploiting the confidence constraint to prune the set of rules explored. We were unable to use lower settings of minconf than those plotted because of the large number of rules. As minconf is increased beyond the point at which fewer than 100,000 rules are returned, the run-time of Dense-Miner rapidly falls to around 500 seconds on both data-sets 8.3  Summary of experimental findings These experiments demonstrate that Dense-Miner, in contrast to approaches based on finding frequent itemsets achieves good performance on highly dense data even when the input constraints are set conservatively. Minsup can be set low \(which is necessary to find high confidence rules as can minimp and minconf \(if it is set at all\This characteristic of our algorithm is important for the end-user who may not know how to set these parameters properly. Low default values can be automatically specified by the system so that all potentially useful rules are produced. Refinements of the default settings can then be made by the user to tailor this result. In general, the execution time required by Dense-Miner correlates strongly with the number of rules that satisfy all of the specified constraints 9.     Conclusions We have shown how Dense-Miner exploits rule constraints to efficiently mine consequent-constrained rules from large and dense data-sets, even at low supports. Unlike previous approaches, Dense-Miner exploits constraints such as minimum confidence \(or alternatively, minimum lift or conviction\ and a new constraint called minimum improvement during the mining phase. The minimum improvement constraint prunes any rule that does not offer a significant predictive advantage over its proper sub-rules. This increases efficiency of the algorithm, but more importantly it presents the user with a concise set of predictive rules that are easy to comprehend because every condition of each rule strongly contributes to its predictive ability The primary contribution of Dense-Miner with respect to its implementation is its search-space pruning strategy which consists of the three critical components: \(1\functions that allow the algorithm to flexibly compute bounds on confidence, improvement, and support of any rule derivable from a given node in the search tree; \(2\proaches for reusing support information gathered during previous database passes within these functions to allow pruning of nodes before they are processed; and \(3\ item-ordering heuristic that ensures there are plenty of pruning opportunities. In principle, these ideas can be retargeted to exploit other constraints in place of or in addition to those already described We lastly described a rule post-processor that DenseMiner uses to fully enforce the minimum improvement constraint. This post-processor is useful on its own for determining the improvement value of every rule in an arbitrary set of rules, as well as associating with each rule its proper sub-rule with the highest confidence. Improvement can then be used to rank the rules, and the sub-rules used to potentially simplify, generalize, and improve the predictive ability of the original rule set References 1 w a l  R.; Im ie lin ski  T   a n d S w a m i, A. 1 9 9 3   M i n i ng As so ciations between Sets of Items in Massive Databases. In Proc of the 1993 ACM-SIGMOD Int\222l Conf. on Management of Data 207-216 2 raw a l R.; M a n n ila, H Sri k an t  R T o i v o n en  H.; an d  Verkamo, A. I. 1996. Fast Discovery of Association Rules. In Advances in Knowledge Discovery and Data Mining AAAI Press, 307-328 3 K Ma ng a n a r is S a n d Sri k a n t, R 19 97  P a rtia l Cl a ssif i cation using Association Rules. In Proc. of the 3rd Int'l Conference on Knowledge Discovery in Databases and Data Mining 115-118 4 a rd o  R. J 1 9 9 8  Ef f i c i en tly Min i n g  Lo n g  P a ttern s fro m  Databases. In Proc. of the 1998 ACM-SIGMOD Int\222l Conf. on Management of Data 85-93 5  Mi c h ae l J. A a n d  Lin o f f G  S 1 9 9 7  Data Mining Techniques for Marketing, Sales and Customer Support John Wiley & Sons, Inc 6 Bri n, S  M o t w a n i, R.; Ullm a n J.; a n d  Tsu r S. 19 9 7 Dyn a m i c  Itemset Counting and Implication Rules for Market Basket Data. In Proc. of the 1997 ACM-SIGMOD Int\222l Conf. on the Management of Data 255-264 7 h e n  W   W   1 9 9 5 F a st Ef fecti v e Ru le In d u ctio n   In  Proc. of the 12th Int\222l Conf. on Machine Learning 115-123 8 In tern atio n a l Bu sin e s s Mac h in e s   1 9 9 6  IBM Intelligent Miner User\222s Guide Version 1, Release 1 9 m e t tin e n M   Ma nn ila  P  Ro nk a i ne n  P   a n d V e rk a m o  A  I. 1994. Finding Interesting Rules from Large Sets of Discovered Association Rules. In Proc. of the Third Int\222l Conf. on Information and Knowledge Management 401-407 10  Ng   R  T    L a k s hm ana n   V   S    Ha n  J   an d P a ng A  1 9 9 8   Exploratory Mining and Pruning Optimizations of Constrained Association Rules. In Proc of the 1998 ACM-SIGMOD Int\222l Conf. on the Management of Data 13-24 11 Ry mo n  R 1 9 9 2   Search  t h ro u g h Sy s t e m atic S e t En u m era tion. In Proc. of Third Int\222l Conf. on Principles of Knowledge Representation and Reasoning 539-550 1  Sha f e r  J  A g r a w a l R   an d Me ht a M 19 98  SPR I N T   A  Scalable Parallel Classifier for Data-Mining. In Proc. of the 22nd Conf. on Very Large Data-Bases 544-555 13  S m y t he P  and  Go od man   R  M 19 92 An I n f o r m at i o n Th eo retic Approach to Rule Induction from Databases IEEE Transactions on Knowledge and Data Engineering 4\(4\:301316 14  S r i k a n t   R    V u  Q an d Ag r a w a l  R  19 97 M i ni ng  A ssoc i a tion Rules with Item Constraints. In Proc. of the Third Int'l Conf. on Knowledge Discovery in Databases and Data Mining 67-73 15 W e bb, G. I 1 9 9 5 OP U S An Ef f i c i e n t Adm i ssible Algo rit h m for Unordered Search. In Journal of Artificial Intelligence Research 3:431-465 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


