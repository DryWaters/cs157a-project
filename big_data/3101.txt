Using Data Mining Techniques in Development of MURA Geometric Prediction Module for Large Area Photomask  WEN-HSING KAO, JASON C. HUNG, AND VICTORIA HSU Department of Information Technology  The Overseas Chinese Institute of Technology Taiwan Email jhung@ocit.edu.tw  S9618207@ocit.edu.tw    Abstract  Data Mining is concerned with the different applications for discovery of a priori unknown relationships such as associations groupings, and classifiers from data. The solution of MURA problems in producing to the large photomask industry is finding the association rules or the algorithms for learning classifiers from relational data. In this paper, we designed and developed a MURA related association rules which suitable for the MURA model requirements and we named MURA Risk rating system Our purpose is to figure out the effective application of data mining algorithms in monitoring and control of complex Large Area Photomask systems. We combine the Data Mining into MURA risk management It is indeed saving time from standby machine, and expectation the photomask processing graphic of MURA problems. By our scheme and MURA risk rating system we can shorten the time and reduce the MURA problems  Keyword Large Photomask, MURA, Data Mining, Data Mining application  1. Introduction  The effective and efficient management and use of increasing amounts of stored data and in particular the transformation of these data into information and knowledge, is considered a key requirement in modern information systems.[1  concerned with the different applications for discovery of a priori unknown relationships such as associations, groupings, and classifiers from data. Data Mining is one component of the broader process known as Knowledge Discovery in Databases \(KDD 2  In the re cently the Intelligent Miner of  IBM and the Enterprise Miner of SAS are the old well-known enterprise data mining tools. SAS, Inc. developed the process for Amazon.com to use data mining providing customers with purchase suggestions which has increased sales by 15% [3 and the Federal Reserve use data mining to investigate the flow of money. Federal 
2008 IEEE Asia-Pacific Services Computing Conference 978-0-7695-3473-2/08 $25.00 © 2008 IEEE DOI 10.1109/APSCC.2008.76 957 
2008 IEEE Asia-Pacific Services Computing Conference 978-0-7695-3473-2/08 $25.00 © 2008 IEEE DOI 10.1109/APSCC.2008.76 957 


agencies use data mining to monitor cell phone communications via satellite Compaq uses data mining to examine calls made to customer service to find patterns of complaints. [4   The goal of Data Mining is to discover knowledge hidden in data repositories According to this concept which fallows as expert-select, database and optimization mining [5   The solut ion of M U RA  problems in producing to the large photomask industry is finding the association rules or the algorithms for learning classifiers from relational data. In this paper, we designed and developed a MURA related association rules which suitable for the MURA model requirements and we named MURA Risk rating system Our purpose is to figure out the effective application of data mining algorithms in monitoring and control of complex Large Area Photomask systems. In order to enhance the quality of Large Area Photomask, this paper will concentrate on how to connect the Data Mining Techniques with MURA problems. The organization of this paper is as following. We present the Large Area Photomask simulation and MURA prediction module in the next section. Then, we describe our scheme and MURA geometric and graphic terms in section 3 and 4. Before we concluding this paper, we display the MURA Risk rating system in section 5  2. The MURA simulation in Large Area Photomask  Standards for MURA are defined by the experts [3   The traditional data processing can not predict what kind of geometric would lead to MURA problems when a mask machine painted. And it is also due to the difficulties of data processing  Thronging Data Mining to predict the MURA modules, it could establish a set of feed-back system. The MURA prediction module will be more accurate judgments than human experience. And because of its feed-back system, when the scheme run more and more over again, the database warehouse will not only become more precision but also adjust this module from its history database warehouse  The high level of FPD mask painted machine almost produced by Micronic Company. And the main differences in high-end machine specifications are the size of the mask depicted, and most specifications of the machines are the same Fig.1.\6    Fig.1. The tendency of FPD Photomask technology  This study is intended to find a model that fit for different machines and by using similar method to develop their own module of their own machine 
958 
958 


We will through the CATS program and PERL language to simulate the adjustment of the light exposure when the graphics depicted. This part of the graphics format will be converted into vector, and it could provide a database for our Data Mining scheme. By our scheme, when the MURA problems adjust at the plant, we can pre-know the risk value of MURA. With this module can provide a graphic adjuster a reference value, in order to get the best risk-adjusted of MURA  3. MURA Scheme   During the manufacture of Photomask we can read the Layout format file with these information like the customers graphics, bar code, title, serial number, the name of the company, the date of manufacture and other vector data files Fig.2  Fig.2. The information from mask   The laser writer has lapped over each array graphics to build a larger area photomask [3   The program has to co mpute  the depicting of start point position, and output the overlap location plan \(Fig.3.\ We try to simulate the same overlap graphics because of no need for a Fracturing on the computing procession it should be faster than the original operation  Fig.3. The overlap of graphic  If the first time adjustment of overlap will result of poor graphics, we can only restart the starting point position till it’s qualify to best required. Since we do this step again we will easily adjust the overlap in soon without the further fracturing for another once. And it will save more time among 30 to 60 minutes to know the results When it need to change the direction of painted, we could repeat the simulate step again to output the GDSII file which transfer to 90 degrees Celsius or 270 degree rotation\(Fig.4.\When we get another new start point position, we can know the new overlap result and do not need to wait for the different redirection of mask layout  Fig.4. Fracturing Rotated  After the simulated has been done, the all information will become our Data Mining 
959 
959 


database. We organized all the information from the standard operating of Photomask Layout, and transfer the Layout format into the program can read one\(Fig.5  Fig.5.The Layout data format  The first part of our scheme is to shorten the adjustment time of MURA graphics and the output time of vector graphics in MURA adjustment. We will work through the CATS program and PERL language to simulate the adjustment of the light exposure when the graphics depicted. Because of this part, it will provide a good source of information to use in Data Mining which is composed in our second part The second part is combining the Data Mining Techniques into the MURA simulation. As a photomask adjust at the plant, we can pre-know the risk value of MURA. It is involved in graphic analysis and graphic mining which we will discuss in next section. With this second port, it can provide a graphic adjuster a reference value in order to get the best risk-adjusted of MURA  4. The graphic Geometric in MURA Scheme   The photomask file is one type of graphic geometric. As we know the graphic is hard to be calculated in algorithm. To overcome this drawback, we make the point line represent the overlap region which we have discussed in section 3\(Fig.6  Through the first part of a simulation study, we can gather the GDSII format when mask graphic exposure for the MURA adjustment at the plant. For its vector format the available source of data mining could be established  Fig.6. The represent in MURA scheme  The laser painted machine’s pixel size is 0.25 um, address grid is 0.01 um, sweep length is 210 um, and CD Uniformity is 0.07 um. If the overlapping regional graphics are too complex, it is not only to control different laser energy in every pixel 
960 
960 


but also necessary to control the good overlap of the exposure in the Y-axis precision regional at the plant To identify overlap in this region of vector graphics, we have to analyze the overlapping coordinates region. From this information, we proposed to separate the regional grid within the 100-area ratio which is distributing the second part of the input data mining file \(Fig.7  Fig.7. The two lines show the overlapped region  The general expectation is overlap on the simple geometric shapes region. But with the limit of graphic design, a mask usually unable placed over the region on the simple geometry panel. It is also a MURA main issue in photomask  5. MURA Risk rating system  In this section, we evaluate the MURA graphic in our scheme by a number of degrees. We named this part as a MURA Risk rating system in our scheme. In section 4, we distribute the graphics into deferent degrees. This is the way we define the result of our output in Data Mining  The bevel edge in overlap area is a highly-risk of MURA. In this study, we separate the overlap area into 100-area ratio Fig.8.\n, we calculate the cut area of graphics. By analyzing the cut area of overlap, we could figure out the MURA related association rule in this scheme According to the advantage of our feed-back system, the more information has been collected the more accurate of this system  Fig.8. The separated overlap area and The cut area of overlap  We separate the overlap area into 100 species, and all the lattice point area can be calculate as ratio type. For example, the superficial measure of this lattice in fig8, the ratio is 0.35. It is settle down between 0.341~0.35 interval. When the calculated value is true, we take it as 1; on the contrary we take it as 0. And that make our importation become a MURA risk rating from 0.1 to 0.9\(Fig.9  Fig.9. MURA Risk rating system  This module is helpful for company to do the MURA risk management. If the risk level of MURA adjusted could be aware in advance. We may change the laser overlap 
961 
961 


start line to reduce the MURA problems which caused by the overlap area  6. Conclusion  The photomask is rendering on high-purity quartz and it painted the design by the high-precision laser machine. The laser has to double the graphic design line to compose whole large photomask. That is how it causes of MURA problems during the photomask rendering. We combine the Data Mining into MURA risk management It is indeed saving time from standby machine, and expectation the photomask processing graphic of MURA problems In this paper, we proposed a MURA scheme to solve the phenomenon of uneven brightness in display. By our scheme and MURA risk rating system, we can shorten the time and reduce the MURA problems. In the future, we need more input data to support our theory, and make our system more précised  7. Acknowledgement   The authors would like to thank the PKLT CO, LTD, Taiwan for technical supporting this research. Besides, the authors would like to acknowledge Kevin Chang for his assistance in making this system possible  8. References  1 http://www datamininggrid.or g 2 Zantinge. D, “Data Mining.” Addison-Wesley Longman, 1996  3 SAS, Eddie Bauer Uses SAS to T a ilor Customer Relationships. Retrieved on September 8, 2004, from http://www.sas.com/success/eddiebauer.html 4 P a tricia B. Cerrito, Depart m e n t of  Mathematics Jewish Hospital Center for Advanced Medicine, “A Data Mining Applications Area in the Department of Mathematics” ,July, 2003 5  W e n-Hsing Kao, Jason C. Hung and Victoria Hsu,” The MURA Graphics problems in Large Area Photomask for concept-based Data Mining Techniques” ,Proceedings 2008 The First IEEE Inter Conference on Ubi-Media Computing and Workshop,2008 6 h ttp://www pklt.com t w/chinese/a9 _news html   
962 
962 


Used-for references in the LCSH into holonym/meronym relations in our WKB  In the experiments we assume that each topic comes from an individual user We attempt to evaluate our model in an environment that covers great range of topics However it is not realistic to expect a participant to hold such great range of topics in personal interests Thus for the 50 experimental topics we assume each one coming from an individual user and learn her his personalized ontology An LIR is collected through searching the subject catalogue of Queensland University of Technology QUT Library 3 by using the title of a topic Librarians have assigned title table of content summary and a list of subjects to each information item e.g a book stored in QUT library The assigned subjects are treated as the tags in Web documents that cite the knowledge in the WKB  In order to simplify the experiments we only use the librarian summarized information title table of content and summary to represent an instance in an LIR  All these information can be downloaded from QUT's Web site and are available to the public Once the WKB and an LIR are ready an ontology is learned as described in Section 3.3.1 and personalized as in Section 3.3.2 The user con\002dence rates on the subjects are speci\002ed as in Section 3.3.3 A document d i in the training set is then generated by an instance i  and its support value is determined by support  d i   X s 2 021  i  s 2S sup  s Q  14 where s 2 S in O  Q  are as de\002ned in De\002nition 5 As sup  s Q   0 for s 2 S 000 according to Eq 11 the documents with support  d   0 go to D 000  whereas those with support  d   0 go to D   4.4 Performance Measures The performance of the experimental models are measured by three methods the precision averages at eleven standard recall levels 11SPR the mean average precision MAP and the F 1 Measure They are all based on precision and recall the modern IR evaluation methods The 11SPR is reported suitable for information gathering and is used in TREC evaluations as a performance measuring standard An 11SPR v alue is computed by summing the interpolated precisions at the speci\002ed recall cutoff and then dividing by the number of topics P N i 1 precision 025 N  025  f 0  0  0  1  0  2      1  0 g  15 N is the number of topics and 025 are the cutoff points where the precisions are interpolated At each 025 point an aver3 http://library.qut.edu.au Figure 2 Experimental 11SPR Results age precision value over N topics is calculated These average precisions then link to a curve describing the recallprecision performance The MAP is a stable and discriminating choice in information gathering evaluations and is recommended for measuring general-purpose information gathering methods The average precision for each topic is the mean of the precision obtained after each relevant document is retrieved The MAP for the 50 experimental topics is then the mean of the average precision scores of each of the individual topics in the experiments The MAP re\003ects the performance in a non-interpolated recall-precision fashion F 1 Measure is also well accepted by the information gathering community which is calculated by F 1  2 002 precision 002 recall precision  recall  16 Precision and recall are evenly weighted in F 1 Measure For each topic the macro F 1 Measure averages the precision and recall and then calculates F 1 Measure whereas the micro F 1 Measure calculates the F 1 Measure for each returned result and then averages the F 1 Measure values The greater F 1 values indicate the better performance 5 Results and Discussions The experiments attempt to evaluate our proposed model by comparing to an implementation of mental model We expect that the ONTO model can achieve at least the close performance to the TREC model The experimental 11SPR results are illustrated in Fig 2 At recall point 0.3 the TREC model slightly outperformed the ONTO model but at 0.5 and 0.6 the ONTO model achieved better results than the TREC model subtly At all other points their 11SPR results are just the same For the MAP results shown on Table 1 the ONTO model achieved 0.284 which is just 0.006 below the TREC model 2 
512 
516 


TREC ONTO p-value Macro-FM 0.388 0.386 0.862 Micro-FM 0.356 0.355 0.896 MAP 0.290 0.284 0.484 Table 1 Other Experimental results downgrade For the average macroand microF 1 Measures also shown on Table 1 the TREC model only outperformed the ONTO model by 0.002 0.5 in macro F 1 and 0.001 0.2 in micro F 1  The two models achieved almost the same performance The evaluation result is promising The statistical test is also performed on the experimental results in order to analyze the evaluation's reliability As suggested by we use the Student's Paired T-Test for the signi\002cance test The null hypothesis in our T-Test is that no difference exists in two comparing models When two tests produce substantially low p-value usually  0.05 the null hypothesis can be rejected In contrast when two tests produce high p-value usually  0.1 there is not or just little practical difference between two models The T-Test results are also presented on Table 1 The pvalue s show that there is no evidence of signi\002cant difference between two experimental models as the produced pvalue s are quite high  p-value 0.484\(MAP 0.862\(macroFM and 0.896\(micro-FM far greater than 0.1 Thus we can conclude that in terms of statistics our proposed model has the same performance as the golden TREC model and the evaluation result is reliable The advantage of the TREC model is that the experimental topics and the training sets are generated by the same linguists manually They as users perfectly know their information needs and what they are looking for in the training sets Therefore it is reasonable that the TREC model performed better than the ONTO model as we cannot expect that a computational model could outperform a such perfect manual model However the knowledge contained in TREC model's training sets is well formed for human beings to understand but not for computers The contained knowledge is not mathematically formalized and speci\002ed The ONTO model on the other hand formally speci\002es the user background knowledge and the related semantic relations using the world knowledge base and local instance repositories The mathematic formalizations are ideal for computers to understand This leverages the performance of the ONTO model As a result as shown on Fig 2 and Table 1 the ONTO model achieved almost the same performance as that of the TREC model 6 Conclusions In this paper an ontology-based knowledge IR framework is proposed aiming to discover a user's background knowledge to improve IR performance The framework consists of a user's mental model a querying model a computer model and an ontology model A world knowledge base is used by the computer model to construct an ontology to simulate a user's mental model and the ontology is personalized by using the user's local instance repository The semantic relations of hypernym/hyponym holonym/meronym and synonym are speci\002ed in the ontology model The framework is successfully evaluated by comparing to a manual user model The ontology-based framework is a novel contribution to knowledge engineering and Web information retrieval References   C Buckley and E M Voorhees Evaluating evaluation measure stability In Proc of SIGIR 00  pages 33–40 2000   R M Colomb Information Spaces The Architecture of Cyberspace  Springer 2002   D Dou G Frishkoff J Rong R Frank A Malony and D Tucker Development of neuroelectromagnetic ontologies\(NEMO a framework for mining brainwave ontologies In Proc of KDD 07  pages 270–279 2007   S Gauch J Chaffee and A Pretschner Ontology-based personalized search and browsing Web Intelligence and Agent Systems  1\(3-4 2003   X Jiang and A.-H Tan Mining ontological knowledge from domain-speci\002c text documents In Proc of ICDM 05  pages 665–668 2005   J D King Y Li X Tao and R Nayak Mining World Knowledge for Analysis of Search Engine Content Web Intelligence and Agent Systems  5\(3 2007   D D Lewis Y Yang T G Rose and F Li RCV1 A new benchmark collection for text categorization research Journal of Machine Learning Research  5:361–397 2004   Y Li and N Zhong Mining Ontology for Automatically Acquiring Web User Information Needs IEEE Transactions on Knowledge and Data Engineering  18\(4 2006   H Liu and P Singh ConceptNet a practical commonsense reasoning toolkit BT Technology  22\(4 2004   A D Maedche Ontology Learning for the Semantic Web  Kluwer Academic Publisher 2002   S E Robertson and I Soboroff The TREC 2002 002ltering track report In Text REtrieval Conference  2002   M D Smucker J Allan and B Carterette A Comparison of Statistical Signi\002cance Tests for Information Retrieval Evaluation In Proc of CIKM'07  pages 623–632 2007   X Tao Y Li and R Nayak A knowledge retrieval model using ontology mining and user pro\002ling Integrated Computer-Aided Engineering  15\(4 2008   X Tao Y Li N Zhong and R Nayak Ontology mining for personalzied web information gathering In Proc of WI 07  pages 351–358 2007   T Tran P Cimiano S Rudolph and R Studer Ontologybased interpretation of keywords for semantic search In Proc of the 6th ICSW  pages 523–536 2007   Y Y Yao Y Zeng N Zhong and X Huang Knowedge retrieval KR In Proc of WI 07  pages 729–735 2007 
513 
517 


TESTS IN SECOND t INDICATES nl WAS LOWERED TO 2 Training BSTC Top-k RCBT 7 OC Holdout Validation Results RCBT outperforms BSTC on the single test it could finish by more then 7 although it should be kept in mind that RCBT's results for the 24 unfinished tests could vary widely Note that BSTC's mean accuracy increases monotonically with training set size as expected At 60 training BSTC's accuracy behaves almost identically to RCBT's 40 training accuracy see Figure 6 4 Ovarian Cancer OC Experiment For the Ovarian Cancer dataset which is the largest dataset in this collection the Top-k mining method that is used by RCBT also runs into long computational times Although Top-k is an exceptiounally fast CAR group upper bound miner it still depends on performing a pruned exponential search over the training sample subset space Thus as the number of training samples increases Top-k quickly becomes computationally challenging to tune/use Table VI contains four average classification test run times in seconds for each Ovarian Cancer\(OC training size As before the second column run times each give the average time required to build both class 0/1 BSTs and then use them to classify all test's samples with BSTC Note that BSTC was able to complete each OC classification test in about 1 minute In contrast RCBT again failed to complete processing most classification tests within 2 hours Table VI's third column gives the average times required for Top-k to mine the top 10 covering rule groups upper bouhnds for each training set test with the same 2 hour cutoff procedure as used for PC testing The fourth column gives the average run times of RCBT on the tests for which Topk finished mining rules also with a 2 hour cutoff Finally the  RCBT DNF column gives the number of tests that RCBT was unable to finish classifying in  2 hours each THE OC TESTS THAT RCBT FINISHED Training BSTC RCBT 40 92.05 97.66 60 95.75 96.73 80 94 12 98.04 1-133/077 9380 96.12 1070 cJ CZ C 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 BSTC RCBT d Median Median  Mean 260 Near outliers  Far outliers 40 Training 60 Training 0.90.80.70.6BSTC RCBT a 80 Training 1-52/0-50 Training 0.9DNFI 0.80.70.6BSTC RCBT b 1 u0.9DNFI 0.80.70.6BSTC RCBT  RCBT DNF 40 30.89 0.6186 273.37 0/25 60 61.28 41.21  5554.37 19/25 80 71.84  1421.80  7205.43 t 21/22 TIMES FOR THE OC 9 Mean 0 Near outliers  Far outliers 1.01 11 01 1.0 d Fig 6 PC Holdout Validation Results BSTC RCBT a Fig 0.80.8 0.8BSTC RCBT BSTC RCBT b c c i DNF cJ CZ C 40 Training 60 Training 80 Training 1-133/0-77 Training 0.95 DNF DNF DNF 0.9 0.90.90.90.85 0.8 BSTC RCBT TABLE VI AVERAGE RUN 1 133/0-77 70.38  1045.65  6362.86 t 20/23 over the number of tests for which Top-k finished Because RCBT couldn't finish any 80 or 1-133/0-77 tests within 2 hours with nl  20 we lowered nl to 2 Classification Accuracy Figure 7 contains boxplots for BSTC on all four OC classification test sets Boxplots were not generated for RCBT with 60 80 or 1-133/0-77 training since it was unable to finish all 25 tests for all these training set sizes in  2 hours each Table VII lists the mean accuracies of BSTC and RCBT over the tests on which RCBT was able to produce results Hence Table VII's 40 row consists of averages over 25 results Meanwhile Table VII's 60 row results are from 6 tests 80 contains a single test's result and 1-133/0-77 results from 3 tests RCBT has better mean accuracy on the 40 training size but the results are closer on the remaining sizes   4 difference over RCBT's completed tests Again RCBT's accuracy could vary widely on its uncompleted tests CAR Mining Parameter Tuning and Scalability We attempted to run Top-k to completion on the 3 OC 80 training and 2 OC 1-133/0-77 training tests However it could not finish mining rules within the 2 hour cutoff Top-k finished two of the three 80 training tests in 775 min 43.6 sec and 185 min 3.3 sec However the third test ran for over 16,000 mnm  11 days without finishing Likewise Top-k finished one of the two 1-133/0-77 tests in 126 min 45.2 sec but couldn't finish the other in 16,000 min  11 days After increasing Top-k's support cutoff from 0.7 to 0.9 it was able to finish the two unfinished 80 and 1-133/0-77 training tests in 5 min 13.8 sec and 35 min 36.9 sec respectively However RCBT with nl 2 then wasn't able to finish lower bound rule mining for either of these two tests within 1,500 min Clearly CAR-mining and parameter tuning on large training sets is TABLE VII MEAN AcCU1ACIES FOR 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


