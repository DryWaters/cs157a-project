Session 12B7 An Adaptable Speech System for MS-DOS Based Personal Computers IEEE Gwong Sun Raymond Wisman Khaled Kame1 Engineering Mathematics  Computer Science University of Louisville Lou i sv i 1 1 e Kentucky ABSTRACT A software system to interface standard personal computer applications to speech synthesis hardware will be presented It has been designed for use with applications utilizing the Microsoft Disk Operating System on International Business Machines Personal Computers and compatibles A variety of phoneme driven speech synthesizers may 
be supported The system is composed of three main software elements 1 an interface to the input and output of the standard application via the operating system 2 a user interface providing control of the system 3 translation of text-to speech Hardware consists of the host computer system and a simple speech synthesizer The speech system is adaptable to various speech synthesizer hardware Adapting the speech system to another synthesizer requires two changes to the IPA-to-synthesizer translation rule file to reflect that specific synthesizer\222s phoneme code representation and to the software that drives 
the synthesizer hardware to reflect the manner in which the hardware interfaces to the computer system I INTRODUCTION Interest in artificial speech has existed for many years and has often been on the threshold of the current technology Computcr synthesized speech research was well under way by the early 1960\222s and the components needed to generate human speech from text were known by 1970 l In current computer applications three types of speech generation prcdominates linear-predictive coding LPC waveform digitization and phoneme synthesis These methods differ considerably in speech quality vocabulary size and area of application Linear-predictive 
LPC coding is based on the frequencies found in speech The vocal tract is modeled using stored filter coefficients amplifier-gain settings and excitation frequencies Waveform digitization technology consists of recording a human voice, applying analog-to-digital conversion, storing the digitized parametcrs of speech then reversing the process to play it back under compute control The speech quality produced is generally quite good but has large memory and data rate requirements Phoneme based speech technology electronically models thc human vocal tract Each of the approximately forty phonemes the basic sounds of speech is individually modeled and produced By stringing together phonemes in the proper time 
and sequence recognizable speech is produced The principal advantage of this method is that unlimited vocabularies may be generated from combinations of these phonemes Since phonemes are basic to all speech foreign languages may also be produced by this method However phoneme based speech quality is often lower than preferable for many applications In general there have been few successes in developing synthetic speech into a viable means of interacting with computer applications The goal of this paper is to explore the problem of producing synthetic speech for standard computer applications Techniques are utilized to enhance 
hardware independence especially that of the speech synthesizer This problem is approached within the general context of converting the usual text generated by the computer and the computer operator interaction into speech The general problem of converting standard textual computer output to spoken output has several stages that diagrammatically appear as in Figure 1 1 2 3 4 Intercept Apply user Translate text Generate text to  constraints  resulting to  speech from console to text from for speech translation Figure 1  Stages of Conversion 
from Textual Output to Speech Output One may consider the processing of a given word from computer generated textual output into speech as essentially a series of sequential operations A word enters stage 1 in text form is processed at each step until reproduced in corresponding speech at stage 4 The first stage involves capturing any text being transmitted to the console from either the operator via the keyboard or as a result of functioning of the computer This should not interfere with the normal textual output or overall functioning of the computer system The second stage is that 
of collecting and applying user constraints to text previously captured before being translated to phonemes This stage provides for control over certain aspects of speech production to allow adjustment to personal preferences The third stage is that of translating form text to a form suitable for speech production by a synthesizer The most desirable situation is where an unlimited vocabulary be produced meaning that textual input be unrestricted in terms of words occurring within that text The translation process must necessarily be rapid enough as to not slow the apparent Proceedings  1989 Southeastcon CH2674-5/89/0000-1163$01.00019891EEE 1163 


overall functioning of the computer system A certain amount of speech reduction is to be expected as a by-product of speaking normal output given it is naturally somewhat slower than normal video textual output The fourth stage is the actual production of the speech from phonemes resulting form the text-to-phoneme translation For purposes of this paper the final stage is implemented in hardware by a general purpose phoneme driven synthesizer For completeness a detailed design of the speech synthesizer circuitry is given in the following It is however part of this project\222s goal to develop a speech system that is largely independent of the device that produces the sounds 11 HARDWARE ENVIRONMENT A number of hardware options are available when implementing a speech system One system that demonstrates the feasibility of replacing computer visual feedback with audio is the Total Talk system which consists of a terminal a special duty microprocessor system a speech synthesizer and a host computer running standard software This configuration is typical of systems that produce speech from the normal terminal-host computer interaction The system chosen for this paper consists of a single computer executing standard application software plus all special software necessary for speech production A wide variety of computer systems, including personal computers, can support such a configuration The synthesizer can be contained within the computer attached to the main system bus or as a stand alone device connected to an external port The specific hardware components used are 1 International Business Machine Personal Computer Model 5150 IBM-PC with 256 kilobytes random access memory 2 360 kilobyte diskettes and graphics video interface 2 Custom internal speech synthesizer board using Silicon Systems Inc SSI-263A speech synthesizer chip 3 8 Ohm speaker A number of initial considerations are required Several are imposed by the goals of the project others are by nature expedient Those of main importance to the synthesizer design are phoneme driven synthesizer  allows production of unlimited vocabulary bus connection  simplifies overall design no independent power supply or inter-system communication devices such as UARTs simple interfacing  reduce the complexity of synthesizer board design The circuit that best satisfies the initial requirements and the one selected is the SSI-263A The SSI-263A synthesizer circuit is phoneme-based 7TL transistor-transistor logic compatible throughout and designed for ease of interfacing with micro-processors It allows for unlimited vocabulary with very low data rates for monotone speech less than 100 bits per second  The circuit contains 5 eight bit registers that allow software control of speech rate pitch pitch movement rate amplitude, articulation rate vocal tract filter response and phoneme selection and duration The synthesizer circuit consists of a single 24-pin CMOS chip \(Complementary Metal-Oxide Semiconductor\powered by a single 5 V source Interfacing the device via the system bus to a computer system involves the decoding of the appropriate addresses and control signals from the computer bus to select the device generation of the proper clock signal and connection to the data lines The synthesizer board connects to the computer bus in any available slot All signals and power requirements are supplied via the computer bus The synthesizer device requires a clock signal to be synchronized with the computer system clock However in this case the clock signal is derived from a system clock having a rate of 14.318 MHz as its base Clocking for the device is derived from the system clock after dividing the pulse by 16 to produce a 89 MHz clock with a 50 duty cycle The analog output of the circuit is filtered and amplified to directly drive an 8-ohm speaker 111 Software Overview In order to convert textual output of standard software into equivalent spoken audio output project software must accomplish the three stages outlines 1 Intercept textual output to computer console  character capture 2 Provide user control over speech process  user interface 3 Translate resulting text into form appropriate for synthesizer  text to speech translation 3.1 As the first stage of the process that leads to speech production from the textual output of standard computer software capture of that text is also the part most subject to the idiosyncracies of the softwarehardware environment The general problem is first to detect when output is being made to the console or the keyboard is being exercised by the user These may be detected at the hardware level or through the operating system software The MS-DOS 2.0 operating system offers several possible methods within the structure of a standard software environment Using MS-DOS the problem becomes one of interfacing MS-DOS console inputioutput functions to those of the speech system character capture functions The software interface between the MS-DOS operating system and all other project software has a rather simple requirement that of trapping either keyboard input or output directed to the screen Characters trapped are first processed by project software and either returned to calling routines when finished or used only internal to the speech system for control purposes such as cursor movement Using the available functions in MS-DOS it is rather simple to store speech system addresses into the interrupt vector to transfer control of a MS-DOS system interrupt to an interfacing routine address is replaced by the address of the character capture routine An input interrupt causes processing control to transfer to the character capture routine The character capture routine calls the BIOS Basic Input and Output System routines  normally used by the MS-DOS When controI is returned to the capture routine interface the input is processed and returns control to the interrupting software Both keyboard and video functions may be treated in a similar manner When an application requires keyboard input or a video function a software interrupt is issued requesting service Normally the request is vectored directly to the BIOS routines serviced and results returned to the application that made the initial request In order to capture the textual output resulting Stage 1  Character Capture Proceedings  1989 Southeastcon 1164 


from the keyboard and video interaction the capture routines arc inserted into the flow of control between the application and thc BIOS operation The character capture portion is necessarily designed for a specific operating system environment using a non-portable language Assembler The remaining major portions of the spccch system software the user interface and text-to-speech translation are coded in the C language and are mainly operating system independent 3.2 Stage 2  User Interface In gcncral terms the user interface provides overall system control to the user via commands entered from the keyboard or otherwise displayed to the console commands are separated in two groups by the criteria of immediate or delayed response to the commands some commands such as cursor positioning must be performed immediately Others such as conversion of the abbreviation Mr to Mister would be delayed until the Mr occurred in text being processed The group of delayed commands controls such functions as 1 Turn text-to-speech translation on/oEf 2 Addldeletelreplace stored commands that control character replacements or skipping of characters sent to text to-speech translator 3 Switch modes to character or word spoken at a time 4 Turn effect of stored commands onioff 5 List speak\stored commands These functions would not normally be used a great deal only to set initial configurations of the speech system and for occasional alterations Consider that one wanted the word Mister spoken when Mr occurred in text This would be iccomplished by storing a command to convert Mr to Mistcr before text-to-speech translation Stored commands control elements of the interface that from the perspective of the user are not dynamic Their effect is automatic once initiated requiring no action by the user Storcd commands are used primarily to set conditions and tailor spokcn output to personal requirements The following Table I lists the stored commands and their functions Table I Stored Command Codes Form COMMAND[stringl]$[stromg2 Note   Ctrl A is default and COMMAND either upper or lowercase s toggles speech onloff w c a adds an abbreviation d deletes an existing abbreviation i ignores text from string1 to string2 1 lists and speaks stored commands All commands that require storage of their parameters make use of this database The stored commands and speech system settings may be listed spoken or written to a file for latcr rctrieval It allows the creation of libraries of commands to control speech functions tailored for a particular application or taste puts system in word mode puts system in character mode The remaining requirement of the user interface is to allow the cursor to be moved about the screen and charactep on the screen processed in the same manner as keyboardhideo output characters and corresponding speech produced This allows text appearing on the video screen to be reviewed Two modes of cursor movement and speech production are utilized moving and speaking either a character or a word at a time Cursor control is accomplished by shifting the standard cursor control keys This is to allow the use of the speech software in conjunction with software that utilize the same keys in the more standard unshifted state The shifted numeric keypad is therefore not useable by other applications in conjunction with the speech software The cursor control keys and their function are illustrated in Table 11 Table I1 Control Key Definitions A Moves up 1 line v Moves down 1 line  Speaks characterfiord to right  Speaks characterfiord to left Home Speaks entire line to left of End Speaks entire line to right of Pg Up Speaks entire screen to left of Pg Down Speaks entire screen to right ESC Ctrl Break control of cursor of cursor cursor cursor cursor of cursor Aborts speech started by cursor Escapes from command entry mode After text has been processed through the user interface it is then passed to the final major stage that of translation from the text output by the standard operation of the computer to phoneme codes serving as input to a speech synthesizer 3.3 Stage 3  Text-To-Speech Translation The method of translation of text into phonemes used in this paper is the Rule Translation  by matching individual or patterns of letters with stored rules to produce corresponding phonemes These rules determine the phonemic result of a given letter pattern Each letter of the alphabet has it's respective list of rules and resultant phonemes The most complex rules are considered first with the very last rule being the rule for the individual letter In that way letter patterns that do not match any rule such as random letters would be spelled out letter by letter The advantages of such a method are small size only about 350 to 400 rules are necessary Simplicity all words are dealt with using the same method of translation And adaptability to change pronunciations or synthesizer support only the rules need be altered no program coding change is needed since rules are stored in a standard text file The basic approach requires two distinct steps that of rule formulation and that of rule application In the rule formulation stage which is done by hand letter combinations Proceedings  1989 Southeastcon 1165 


are assigned a particular phonetic equivalent As a simple example the OOK combination as it occurs in COOK is given the IPA International Phonetic Alphabet 5 equivalent of U k Rule application requires that the text OOk to be matched with the appropriate rule Rules are generally made up of four parts left part the letters preceding the specific letter combination being translated exact part the letter combination exactly identical to the text being translated right part the letters following the letter combination being translated translation result the corresponding phonemes of the translation The left and right parts may be empty The Table I11 lists the rule syntax Table I11 Translation Rule Form All rules have the form oE left right  result where letter  ASCII character vowel  AlElIlOlUlY consonant  B I CI D I FI GI H I J I KI L I M I N I P I Q I R I S I TIUIVIWIXIZ  B 1 D JV 1 G 1 J 1 L I M 1 N 1 RI W 1 Z voiced consonant   ER IED JE IES IING ELY      1 or more vowels  SUffi  S 1 CI G 1 Z 1 XI J I CH 1 SH  TIS I R ID I L I Z I N I TH I CH I SH sibilant long U    EJIJY front vowel   1 or more consonants  1 consonant followed by E or I  1 or more vowels A  1 consonant  combination of letters 1 I left exact  letters right result  IPA phoneme equivalent l+I#l.l*l$lA I I A I  comb of letters I  I  I  I  I A modified thumb-index technique 6 is used for rule storage and access Rules are grouped together by the letter starting the letter combination to be translated e.g AE a z would be grouped with the letter A rules and would translate AE in the order the rules are to be examined This grouping and ordering of the rules is reflected in the data structure used to access the rules during their application to text The rules are read from a file and stored using the starting letter as an index into an array of linked lists The rule would be stored at the end of the linked list maintaining the order in which the rules are read from the file The four components would be left exact=AE right=S and result=a z Due to the difficulty in representing the IPA symbols in practice they are equated to an ASCII character representation Rule application of the text-to-speech rules to computer generated text produces IPA phoneme results To apply the rules to text the rule list for the beginning letter of the text to be translated is searched until either a match is found 200or the right exact and left components of a rule or the end of the rule list for that letter is reached In the former case the resultant phonetic translation is produced In the latter, the default the last rule of the list is used as the translation which has the effect of spelling some words Some commercial synthesizers will not accept phonemes in that form Necessarily a similar translation of the phonemes from the IPA form to that acceptable to a specific synthesizer is required Most synthesizers are generally capable of producing the sound corresponding directly to the IPA phonemic representation through actual phoneme code representation varies It is this two step process of translation to IPA then to synthesizer phoneme representation that provides the adaptability of this method to function with different speech synthesizers In many situations the synthesizer is capable of parameter control not defined by the IPA codes Often a single IPA code may be represented by several synthesizer codes, such as the IPA code of UH has three Votrax representations of UH1 UH2 and UH3 Each synthesizer code produces a UH sound of differing duration The second stage of translation for IPA to-synthesizer codes can utilize synthesizer properties that account for the fact that certain phonemes are voiced differently when in conjunction with other phonemes Selection of the proper synthesizer codes by the translation process results in more understandable speech The translation software is written in the C language which consists of two parts one part that acts as a translation rule preprocessor and a second that applies the translation rules to text Numbers are pronounced as series of single digits 134 would be one three four and punctuation marks as their spelled equivalent  would be pronounced as asterisk Rules are stored on a text file and read in at program initialization Words to be translated are input from keyboard or console with the phonetic translation codes output to the synthesizer IV RECOMMENDATIONS directions for future development The following recommendations are submitted as possible Develop software drivers as defined within MS-DOS to interface to speech synthesizer hardware Used in conjunction with synthesizer translation rules these would allow greater portability to different synthesizers Polish the user interface Provide on-line help Study ways that user could store portions of screen recall and replay at will Also restrict portions of screen to be spoken in a manner similar to video windowing Consider use of speech in multi-tasking personal computer systems as an alternative to console output from background jobs The foreground function could be monitored primarily via video output while other background functions could be monitored via audio Improve speech intelligibility A continuing problem for all applications of synthetic speech Proceedings  1989 Southeastcon 1166 


V REFERENCE 1 Shcnvood Bruce The Computer Speaks IEEE Spectrum August 1979 pp 18-25 2 Blazie Deane B Total Talk a Computer Terminal for the Blind Proceedings of the First National Search for Applications of Personal Computing to Aid the Handicapped October 1981 pp 251-253 3 SSI 263A Phoneme SDeech Synthesizer Data Sheet by Silicon Systems, 14351 Myford Road Tustin CA 4 Roskos J Eric Writing Device Drives for MS-DOS 2.0 Byte Magazine, February 1984 Vol 9 No 2 pp 370-380 5 The Princioles of the International Phonetic Association Intcrnational Phonetic Association Department of Phonetics University College London England 6 Knuth donald E The Art of Computer Programming Volume 3/Sorting and Searching Addison-Wesley Publishing Company Reading MA KHALED KAMEL Dr Kamel is currently a professor of the Engi neering Computer Science at the University of Louis ville He joined the Engineering School in 1979 Prior to this he worked with the Aircraft Engine Group at General Electric Co as an Instrumental Engineer mation System Division IBM University Program Division General Electric Appliance Park the Naval Ordnance Station in Louisville and others He completed the Ph.D degree in Electrical and Com puter Engineering at the University of Cincinnati Dr Kamel is a senior member of IEEE He also worked part time with IBM Infor e Proceedings  1989 Southeastcon 1167 


W 0 000 9 001 6 001 3 000 18 W 1 000 8 001 5 001 2 000 15 and W 2 000 7 001 4 001 1 000 12 The load imbalance is much smaller r t s still present Bitonic Partitioning Single Equivalence Class In 6 w e propose a n e w partitioning scheme called bitonic partitioning  for load balancing that can be applied to the problem here as well This scheme is based on the observation that the sum of the workload due to itemsets i and 002 2 P\000 i 000 1 003 is a constant w i 001 w 2 P\000 i 000 1 000 n 000 i 000 1 001\002 n 000 002 2 P\000 i 000 1 003 000 1 003\000 2 n 000 2 P t 000 1 We can therefore assign itemsets i and 002 n 000 i 000 1 003 as one unit with uniform work 2 n 000 2 P t 000 1 If n mod 2 P 000 0 then perfect balancing results The case n mod 2 P 001 000 0 s handled as described in The 336nal assignment is n s A 0 000 f 0 000 5 000 6 g  A 1 000 f 1 000 4 000 7 g  and A 2 000 f 2 000 3 000 8 000 9 g  with corresponding workload n s W 0 000 9 001 4 001 3 000 16 W 1 000 8 001 5 001 2 000 15 and W 2 000 7 001 6 001 1 000 14 This partition scheme is better than the d scheme and results in almost no imbalance Bitonic Partitioning Multiple Equivalence Classes Above we presented the simple case of C 1  where we only had a single equivalence class In general we may have multiple equivalence classes Observe that the bitonic scheme presented above is a greedy algorithm i.e we sort all the w i the work load due to itemset i  extract the itemset with maximum w i  and assign it to processor 0 Each time we extract the maximum of the remaining itemsets and assign it to the least loaded processor This greedy strategy generalizes to the multiple equivalence class as well 14 the major difference being work loads in different classes may not be distinct 3.1.3 Adaptive Parallelism Let n be the total number of items in the database Then there are potentially 000 n k 001 large k itemsets that we would have to count during iteration k  r in practice the number is usually much smaller as is indicated by our experimental results We found that support counting dominated the execution time to the tune of around 85 of the total computation time for the databases we considered in Section 5 On the other hand for iterations with a large number of k itemsets there was suf\336cient work in the candidate generation phase This suggests a need for some form of dynamic or adaptive parallelization based on the number of large k itemsets If there aren\325t a suf\336cient number of large itemsets then it is better not to parallelize the candidate generation 3.1.4 Parallel Hash Tree Formation We could choose to build the candidate hash tree in parallel or we could let the candidates be temporarily inserted in local lists or hash trees This would have to be followed by a step to construct the global hash tree In our implementation we build the tree in parallel We associate a lock with each leaf node in the hash tree When processor i wants to insert a candidate itemset into the hash tree it starts at the root node and hashes on successive items in the itemsets until it reaches a leaf node At this point it 6 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


acquires a lock on this leaf node for mutual exclusion while inserting the itemset r if we exceed the threshold of the leaf we convert the leaf into an internal node with the lock still set This implies that we also have to provide a lock for all the internal nodes and the processors will have to check if any node is acquired along its downward path from the root This complication only arises at the interface of the s and internal nodes With this locking mechanism each process can insert the itemsets in different parts of the hash tree in parallel r since we start with a hash tree with the root as a leaf there can be a lot of initial contention to acquire the lock at the root r we did not 336nd this to be a signi\336cant factor on 12 processors 3.2 Support Counting r this phase we could either split the database logically among the processors with a common hash tree or split the hash tree with each processor traversing the entire database We will look at each case below 3.2.1 Partitioned vs Common Candidate Hash Tree One approach in parallelizing the support counting step is to split the hash tree among the processors The decisions for computation balancing directly in\337uence the effectiveness of this approach since each processor should ideally have the same number of itemsets in its local portion of the hash tree Another approach is to keep a single common hash tree among all the processors There are several ways of incrementing the count of itemsets in the common candidate hash tree Counter per Itemset Let us assume that each itemset in the candidate hash tree has a single count 336eld associated with it Since the counts are common more than one processor may try to access the count 336eld and increment it We thus need a locking mechanism to provide mutual exclusion among the processors while incrementing the count This approach may cause contention and degrade the performance r since we are using only 12 processors and the sharing is very 336ne-grained at the itemset level we found this approach to be the better than using private or separate counters 1  3.2.2 Partitioned vs Common Database We could either choose to logically partition the database among the processors or each processor can choose to traverse the entire database for incrementing the candidate support counts Balanced Database Partitioning In our implementation we partition the database in a blocked fashion among all the processors r this strategy may not result in balanced work per processor This is because the work load is a function of the length of the transactions If l t is the length of the transaction t  then during iteration k of the algorithm we have to test whether all the 1 r all the databases we looked at on our system the overhead of contention was within 4 which leads us to conclude that contention is not a big problem Other mechanisms like separate counters to eliminate locking and local counters to eliminate false sharing were studied t not shown to be bene\336cial 14 7 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


000 l t k 001 subsets of the transaction are contained in C k  Clearly the complexity of the work load for a transaction is n s O 000 min 000 l t k 000l t l t 000 k 001\001  i.e it is polynomial in the transaction length This also implies that a static partitioning won\325t work r we could devise static heuristics to approximate a balanced partition r example one static heuristic is to estimate the maximum number of iterations we expect say T  We could then partition the database based on the mean estimated work load for each transaction r all iterations n s 000 P T k 000 1 000 l i k 001 001 001T  Another approach is to re-partition the database in each iteration In this case it is important to respect the locality of the partition by moving transactions only when it is absolutely necessary We plan to investigate different partitioning schemes as part of future work 3.3 Parallel Data Mining Algorithms Based on the discussion in the previous section we consider the following algorithms for mining association rules in parallel 000 Common Candidate Partitioned Database CCPD This algorithm uses a common candidate hash tree across all processors while the database is logically split among them The hash tree is built in parallel see section 3.1.4 Each processor then traverses its local database and counts the support see section 3.2.1 for each itemset Finally the master process selects the large itemsets 000 Partitioned Candidate Common Database PCCD This has a partitioned candidate hash tree t a common database In this approach we construct a local candidate hash tree per processor Each processor then traverses the entire database and counts support for itemsets only in its local tree Finally the master process performs the reduction and selects the large itemsets for the next iteration Note that the common candidate common database\(CCCD approach results in duplicated work while the partitioned candidate partitioned database PCPD approach is more or less equivalent to CCPD r this reason we did not implement these parallelizations 4 Optimizations In this section we present some optimizations to the association rule algorithm These optimizations are bene\336cial for both sequential and parallel implementation 4.1 Hash Tree Balancing Although the computation balancing approach results in balanced work load it does not guarantee that the resulting hash tree is balanced Balancing C 2 No Pruning  We\325ll begin by a discussion of tree balancing for C 2  since there is no pruning step in this case We can balance the hash tree by using the bitonic partitioning scheme described  We simply replace P  the number of processors with the fan-out F for 8 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


the hash table We label the n large 1-itemsets from 0 o n 000 1 n exicographical order and use P 000 F to derive the assignments A 0 000 001\001\001 000 A F\000 1 for each processor Each A i is treated as an equivalence class The hash function is based on these equivalence classes which is simply n as h 001 i 002\000 A i  for i 000 0 000 001\001\001 000 F  The equivalence classes are implemented via an indirection vector of length n  For example let L 1 000 f A\000 D 000 E 000 G\000 K 000 M 000 N 000 S\000 T 000 Z g  We 336rst label these as f 0 000 1 000 2 000 3 000 4 000 5 000 6 000 7 000 8 000 9 g  Assume that the fan-out F 000 3 We thus obtain the 3 equivalence classes A 0 000 f 0 000 5 000 6 g  A 1 000 f 1 000 4 000 7 g  and A 2 000 f 2 000 3 000 8 000 9 g  and the indirection vector is shown in table 1 Furthermore this hash function is applied at all levels of the hash tree Clearly this scheme results in a balanced hash tree as compared to the simple g 001 i 002\000 i mod F hash function which corresponds to the d partitioning scheme from section 3.1.1 Label 0 1 2 3 4 5 6 7 8 9 Hash Value 0 1 2 2 1 0 0 1 2 2 Table 1 Indirection Vector Balancing C k 001 k\001 2 002  Although items can be pruned for iteration k 002 3 we use the same bitonic partitioning scheme for C 3 and beyond Below e show that n n this general case bitonic hash function is very good as compared to the d scheme Theorem 1 below establishes an upper and lower bound on the number of itemsets per leaf for the bitonic scheme Theorem 1 Let k 002 1 denote the iteration number I 000 f 0 000 002\002\002\000 d 000 1 g the set of items F the fan-out of the hash table T 000 f 0 000 002\002\002\000 F\000 1 g the set of equivalence classes modulo F  T 000 T k the total number of leaves in C k  and G the family of all size k ordered subsets of I  i.e the set of all k itemsets that can be constructed from items in I  Suppose d 2 F is an integer and d 2 F 000 F\002 k  De\336ne the bitonic hash function h  I\003 T by h 001 i 002\000 i mod F if 0 004 001 i mod 2 F 002 003 F and 2 F\000 1 000 001 i mod 2 F 002 otherwise and the mapping H  G 003 T from k itemsets to the leaves of C k by H 001 a 1 002\002\002\000 a k 002 000 001 h 001 a 1 002 000\002\002\002\000h 001 a k 002\002  Then for every leaf B 000\001 b 1 000\002\002\002\000b k 002 005T  the ratio of the number of k itemsets in the leaf  k H 000 1 001 B 002 k  o the average number of itemsets per leaf  kG k 004 kT k  s bounded above and below by the expression e 000 k 2 d\000 F 004 k H 000 1 001 B 002 k kG k 004 kT k 004 e k 2 d\000 F 002 A proof of the above theorem can be found in W e also obtain the same lo wer and upper bound for the d hash function also r the two functions behave differently Note that the average number of k itemsets per leaf kG k 004 kT k is 000 2 w F k 001 004 F k 006 000 2 w 001 k k   Let 005 001 w 002 denote this polynomial We say that a leaf has a capacity close to the average if its capacity which is a polynomial in w of degree at most k  s f the form 000 2 w 001 k k  003 006 001 w 002  with 006 001 w 002 being a polynomial of degree at most k 000 2 9 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


r the bitonic hash function a leaf speci\336ed by the hash values 000 a 1 000\000\000\001 a k 001 has capacity close to 002 000 w 001 if and only if a i 000 002 a i 000 1 for all i 1 001 i 001 k 002 1 Thus there are F 000 F\002 1 001 k 000 1 such leaves and so 000 1 002F 000 1 001 k 000 1 fraction of the s ave capacity close to 002 000 w 001  Note also that clearly 000 1 002F 000 1 001 k 000 1 approaches 1 On the other hand for the d hash function a leaf speci\336ed by 000 a 1 000\000\000\001 a k 001 has capacity close to 002 000 w 001 if and only if a i 000 002 a i 000 1 for all i  and the number of i such that a i 003a i 000 1 is equal to 000 k 002 1 001 004 2 So there is no such leaf if k is even r odd k 003 3 the ratio of the 322good\323 s decreases as k increases achieving a maximum of 2 004 3 when k 002 3 Thus at most 2 004 3 f the s achieve the average From the above discussion it is clear that while both the simple and bitonic hash function have the same maximum and minimum bounds the distribution of the number of itemsets per leaf is quite different While a signi\336cant portion of the s are close to the average for the bitonic case only a ew are close in the simple hash function case 4.2 Short-circuited Subset Checking  Candidate Hash Tree \(C 3 Hash Function: h\(i DEPTH 0 DEPTH 1 DEPTH 2 01 35 7101113 12986 24 LEAVES ABE ADE CDE A,C,EB,D B,D B,D B,D B,D B,D B,DA,C,E A,C,E A,C,E A,C,E A,C,E A,C,E ABD ACD ACEBCEBCDBDE ABC Figure 2 Candidate Hash Tree  C 3  Recall that while counting the support once we reach a leaf node we check whether all the itemsets in the leaf are contained in the transaction This node is then marked as VISITED to avoid processing it more than once for the same transaction A further optimization is to associate a VISITED 337ag with each node in the hash tree We mark an internal node as VISITED the 336rst time we touch it This enables us to preempt the search as soon as possible We would 10 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


expect this optimization to be of greatest bene\336t when the transaction sizes are large r example if our transaction is T 000 f A\000 B 000 C\000 D\000 E g  k 000 3 fan-out 000 2 then all the 3-subsets of T are f ABC,ABD,ABE,ACD,ACE,ADE,BCD,BCE,BDE,CDE g  Figure 2 shows the candidate hash tree C 3  We ave to increment the support of every subset of T contained in C 3  We egin with the subset AB C  and hash to node 11 and process all the itemsets In this downward path from the root we mark nodes 1 4 and 11 as visited We then process subset AD B  and mark node 10 Now consider the subset CDE  We see in this case that node 1 has already been marked and we can preempt the processing at this very stage This approach can r consume a lot of memory r a n fan-out F  for iteration k  e need additional memory of size F k to store the 337ags In the parallel implementation we have to keep a VISITED 336eld for each processor bringing the memory requirement to P\000F k  This can still get very large especially with increasing number of processors In we sho w a mechanism by which further reduces the memory requirement to only k 000F  The approach in the parallel setting yields a total requirement of k 000F 000P  5 Experimental Evaluation Database T I D Total Size T5.I2.D100K 5 2 100,000 2.6MB T10.I4.D100K 10 4 100,000 4.3MB T15.I4.D100K 15 4 100,000 6.2MB T20.I6.D100K 20 6 100,000 7.9MB T10.I6.D400K 10 6 400,000 17.1MB T10.I6.D800K 10 6 800,000 34.6MB T10.I6.D1600K 10 6 1,600,000 69.8MB Table 2 Database properties 5.1 Experimental Setup All the experiments were performed on a 12-node SGI Power Challenge shared-memory multiprocessor Each node is a MIPS processor running at 100MHz There\325s a total of 256MB of main memory The primary cache size is 16 KB 64 bytes cache line size with different instruction and data caches while the secondary cache is 1 B 128 bytes cache line size The databases are stored on an attached 2GB disk All processors run IRIX 5.3 and data is obtained from the disk via an NFS 336le server We used different synthetic databases with size ranging form 3MB to 70MB 2  and are generated using the procedure described in These databases mimic the transactions in a retailing en vironment Each transaction has a unique ID followed by a list of items bought in that transaction The 2 While results in this section are only shown for memory resident databases the concepts and optimization are equally applicable for non memory resident databases In non memory resident programs I/O becomes an important problem Solutions to the I/O problem can be applied in combination with the schemes presented in this paper These solutions are part of future research 11 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


  0 500 1000 1500 2000 2500 0 2 4 6 8 10 12 Number of Large Itemsets Iterations Large Itemset at Support = 0.5 222T5.I2.D100K\222  222T10.I4.D100K\222   222T15.I4.D100K\222   222T20.I6.D100K\222   222T10.I6.D400K\222   222T10.I6.D800K\222   222T10.I6.D1600K\222  Figure 3 Large Itemsets per Iteration data-mining provides information about the set of items generally bought together Table 2 shows the databases used and their properties The number of transactions is denoted as jD j  average transaction size as j T j  and the average maximal potentially large itemset size as j I j  The number of maximal potentially large itemsets j L j 000 2000 and the number of items N 000 1000 We refer the reader to for more detail on the database generation All the e xperiments were performed with a minimum support value of 0.5 and a leaf threshold of 2 i.e max of 2 itemsets per leaf We note that the  improvements shown in all the experiments except where indicated do not take into account initial database reading time since we speci\336cally wanted to measure the effects of the optimizations on the computation Figure 3 shows the number of iterations and the number of large itemsets found for different databases In the following sections all the results are reported for the CCPD parallelization We do not present any results for the PCCD approach since it performs very poorly and results in a speed-down on more than one processor 3  5.2 Aggregate Parallel Performance Table 3 s actual running times for the unoptimized sequential and a naive parallelization of the base algorithm Apriori for 2,4 and 8 processors without any f the techniques descibed in sections 3 and 4 In this section all the graphs showing  improvements are with respect to the data for one processor in table 3 Figure 4 presents the speedups obtained on different databases and different processors for the CCPD parallelization The results presented on CCPD use all the optimization discussed 3 Recall that in the PCCD approach every processor has to read the entire database during each iteration The resulting I/O costs on our system were too prohibitive for this method to be  12 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


