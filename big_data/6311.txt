FAARM: Frequent association action rules mining using FP-Tree  Djellel Eddine Difallah eXascale Infolab University of Fribourg Switzerland djelleleddine.difallah@unifr.ch  Ryan G. Benton, Vijay Raghavan Center for Advanced Computer Studies University of Louisiana at Lafayette, LA rbenton,vijay}@cacs.louisiana.edu  Tom Johnsten Computer & Information Sciences Univ. of South Alabama Mobile, AL tjohnsten@usouthal.edu Abstract Action rules mining aims to provide recommendations to analysts seeking to achieve a specific change. An action rule is constructed as a series of changes or actions, which can be made to some of the flexible characteristics of the information system that ultimately triggers a change in the targeted attribute. The existing 
action rules discovery methods consider the input decision system as their search domain and are limited to expensive and ambiguous strategies. In this paper, we define and propose the notion of action table as the ideal search domain for actions, and then propose a strategy based on the FPTree structure to achieve high performance in rules extraction Keywords- action rules; recommendation; association mining; action table; FP-Tree I   I NTRODUCTION  In association rule mining, the rules extracted from an information system are handed to the domain experts who need to filter what information is interesting or trivial Action rules were introduced in [3 as a n e w class o f ru l e  discovery that provides hints on possible actions a business should take to achieve a desired target Mining action rules is defined as the process of identifying patterns in a decision system capturing the 
possible changes to certain object attributes that may lead to a change in the decision value [3  G e n e ral ly act ion  ru le  mining operates on a decision system [13 i t h ob j ect s  having three classes of attributes: stable, flexible and decision. The stable attributes are attributes that cannot be changed or, in some approaches, require a prohibitive high cost to change them [4  Ex a m ples of s t a b l e  att ri bu tes a r e  the date of birth or weather conditions. Conversely flexible attributes are attributes on which the analysts have a certain degree of freedom in manipulating such as color saleês percentage, and so forth. The decision is the attribute that the analyst would like to see changed; an example would be the profit of a company Existent action rules discovery methods use a decision table as their primary search domain; the employed strategies are limited to candidate generation-and-test. In our approach, the discovery of action rules is based on a 
domain of actions that we create from the decision system called the action table. The main contribution of this paper is to reformulate the action rules mining problem into the association-mining problem framework using the action table as the new search domain. A particularly suited approach is to use an FP-Tree structure to store the action table and the FP-Growth algorithm to extract association action rules The rest paper is organized as follows. Section 2 briefly surveys previous works on action rules mining Section 3 reviews the key concepts and definitions Sections 4 and 5 present the action table and our strategy through a detailed example. Finally we experimentally compare the performance of our solution with existent algorithms II  O VERVIEW AND RELATED WORK  Previous works on action rules assume a generalization 
of an information system S as introduced in [1  w h ere S   X, A, V   X: a nonempty, finite set of objects   A: a nonempty, finite set of attributes   V = {V a a 002 A} all the attributes values Additionally, a : X 003 V a is a function for any a 002 A that returns the value of the attribute of a given object. The attributes are divided into different categories: a stable set A st flexible set A fl and a decision set D of attributes where A= A st  004 A fl 004 D For example, Table I. represents a decision table with eight objects a 
is stable b and c are flexible d is the decision attribute TABLE I  E XAMPLE OF A DECISION TABLE   a b c d x1 a1 b1 c1 l x2 a2 b1 c1 l x3 a2 b2 c1 h 
x4 a2 b2 c2 h x5 a2 b1 c1 l x6 a2 b2 c1 h x7 a2 b1 c2 h x8 a1 b2 c2 l 
 In order to extract action rules, [3, 7, 8, 9, 10 w e r e  based on an existing set of classification rules. Certain pairs of these rules were combined to reclassify objects to a targeted state. One problem that arose, as argued in [4  is  some meaningful action rules should be missed in these classification-based techniques and thus existing algorithms cannot specify when and how the correct and complete underlying action rules are discovered  Instead they were the first to propose an inductive approach for mining directly from the decision system. The approach is formulated as a search problem based on a supportconfidence-cost framework and an Apriori-like algorithm 
2011 11th IEEE International Conference on Data Mining Workshops 978-0-7695-4409-0/11 $26.00 © 2011 IEEE DOI 10.1109/ICDMW.2011.82 398 


4  Fu rth er w o rk t o ex t r a c t a c ti on  ru l e s d i r ect ly f r om th e  decision system followed In [5 th e p r o p o s e d alg o rith m  calle d A c ti o n Ru les  Discovery \(ARD\, builds rules for a given target decision using an iterative marking strategy. It considers the change in an attribute value as an atomic-action-term of length one, and then an action-term is a composition of atomicaction-terms. ARD starts by generating all atomic-actionterms for a given set of attribute values and assigning a mark \(unmarked, positive, negative\ based on support and confidence measures. The unmarked terms are placed into the candidate list. Next, it generates all possible actionterms of length two by combining terms in the candidate list and the atomic terms. The process continues iteratively, creating terms of greater length, until the candidatesê list is empty. The action-terms marked as positive are used to construct the action rules In [6  au th o r s p r es en te d an  a s s o ci ati on ty pe  of  ac ti on  rules and used an Apriori like strategy to find frequent action sets to induce action rules. Like ARD, the algorithm AAR \(Association Action Rule\considers atomic action sets being the fine granule used to construct longer rules similar to items and item sets in association mining\. The Apriori algorithm is directly used with few modifications the main changes are mostly driven \(a\ by modifications to the definition of support and confidence and \(b\y the calculation of the measures directly from the input decision system Although these approaches have different definitions for objective measures like support and confidence, they use the same idea of atomic-action set, action set and Standard Interpretation  III  P RINCIPLES OF ACTION RULES  In this section, we provide a quick overview of the key concepts in action rules literature.  We particularly focus on [6 as i t is th e m o s t  r ecen t w o rk on th e s u b j e c t an d i s  the most related to this work in the sense that both try to map association rules mining into action rules mining A  Atomic action set Defined as the expression a a1\002a2\ where a is an attribute in A and a1, a2 are values of a If the attribute is stable or did not change its value then the atomic action set is expressed as a a1\. The domain of an atomic action set is its attribute Dom a a1\002a2 a Example: Consider Rate a flexible attribute with values V rate 30%, 10%, 50%}. The atomic action set Rate  30 002 10%\eans changing the value of Rate from 30 to 10 B  Action sets Constructed as the conjunction of atomic action sets with the composition operator \(∑\. If t1, t2 are two atomic action sets with different attributes, then t=t1∑ t2 is an action set. The domain of the action set t is the set of attributes from all its atomic action sets, here Dom t m\(t1 004 Dom\(t2  Example: Consider Age as  a stable attribute with values V age 25, 50, 70} and Credit a flexible attribute with values V credit good, bad}. An action set could be the composition Age 50   Rate 30 002 10   Credit  bad 002 good h ich cou ld be read as f o llo w s f o r customers of Age 50, change the Rate from 30% to 10 and the C redit from good to bad C  The standard interpretation \(noted Ns The introduction of the Standard Interpretation is the basis of measures like support and confidence. In association mining, the support of an itemset is simply the count of objects. For action rules, we need to consider two sets. The first set is all the objects with attributes value equal to the initial state of the action; the second set respectively, is all the objects having attributes values equal to the values of the final state of the action Example: The Standard interpretation of the action set Ns Age 50   Rate 30 002 10   Credit bad 002 good  A1,A2 Where A1= { x 001\031 X: age\(x\ = 50 005 rate\(x\0 005 credit\(x bad A2= { x 001\031 X: age\(x\ = 50 005 rate\(x\0 005  credit\(x\=good D  The support of an action set Assume t an action set with standard interpretation Ns t  A1  A2  h e s u pport Supp of t is considered in AAR as Supp t in{card\(A1\, card\(A2 In ARD as  Supp\(t\card\(A1 Hence, for AAR, for the two states, the support is concerned only with the state having the lowest occurrences.  For the ARD, it was only in terms of number of occurrences of the initial state. It should be noted that these definitions lead to very different results.  With respect to ARD, a rationale for the definition was not provided; however, the definition allows for A2 to be 0 and/or below minimum support. AAR, however, ensures that both A1 and A2 are guaranteed to satisfy the minimum support threshold E  Action rule An action rules r is expressed as r t1  002 t2  w h ere t 1  an d  t2 are two action sets. Typically t2 is the action comprising only the decision attribute Example Age 50   Rate 30 002 10   Credit  bad 002 good 002  Profit low 002 high The support is calculated similarly to action sets by considering the t1  t2 as an action set itself 
399 


F  The confidence measure The confidence of an action rule r=[t1 002 t2   Considering Ns\(t1\=[A1, A2 a nd N s t 2  Z1, Z2  w i t h  A1 and A2 not empty conf  r   card  A 1 006 Z 1 card  A 1  card  A 2 006 Z 2 card  A 2 1 In AAR, generating action rules is similar to association rule mining where frequent item sets are first extracted. The algorithm, which is based on Apriori generates actions sets with support that exceeds a specified threshold value: minimum support \(minSup\ any action set that meets this criterion is a frequent action set. An action rule is constructed and tested as following   If t is a frequent action set and t1 is a subset of t  then r =[ t  t1 002 t1    If Conf\(r\\003 minConf where minConf is the minimum confidence specified, r is a valid rule Like Apriori, the AAR method can generate a large number of rules.  The process does not constrain what the decision attribute may be; in fact, it does not even require a decision attribute to be specified.  As a side benefit unlike ARD, the user does not have to supply what the targeted decision should be.  For example, ARD would require the user to state \(Rate, 30%\00210%\ the decision of interest On the other hand, AAR suffers from two problems First, like Apriori, the number of rules may be overwhelming to the user.  Second, if the user is interested in a particular change, like \(Rate, 30%\00210%\ere is no guarantee that the AAR method will generate the required rules.  For instance, if minimum support is 8, and \(Rate 10%\ has a support of 5, then no rules containing \(Rate 10%\ would be generated As an aside, a justification of the defined confidence measure was provided for the ARD method.  In [5  th e  authors indicated that the definition of confidence should be considered as an optimistic confidence.  It requires that the card\(A1\ \0040, card\(A2\ \0040, card\(A1 006 Z1\ 0 and card\(A2 006 Z2\ \004 0.  In effect, this definition was required due to ARDês definition of support.  Without it, action rules could be generated that for cases in which A2 006 Z2 never occurred Interestingly, for AAR, a similar claim was made.  In 6   th e def in i ti on  w a s de cla re d op tim is tic b ecau s e  card\(A1\0 and card\(A2\ \0040.  However, since AAR uses minSup, as long as minSup is greater than 0, it is guaranteed that card\(A1\0 and card\(A2\ \0040 IV  T HE A CTION TABLE  As mentioned previously, the AAR and ARD methods operate within the search space represented by the decision table; we refer to this as the information space.  As a result, the creation \(and tracking\ of the various flexible atomic action sets is intertwined with the action set generation. Hence, one would need to calculate the support of Rate 30%\nd Rate 10%\ first in order to obtain the support of Rate 30 002 10%\. This operation needs to be performed for all k iterations.  The first iteration generates frequent action sets composed of 1 element, the second iteration generated action sets composed of 2 elements and so forth.  For instance, assume you have stable attribute Gender with values {m,f}.  Then, to calculate the action set Gender f   Rate 10 002 30   on e w o u l d f i rs t n e e d  to calculate the support of Gender f Rate 10 an d   Gender  Rate 30 However, this could be avoided if, instead of working within the information space, we were able to work within an èaction spaceê, where the possible flexible action mappings are already known and represented.  In this paper, this is done via the introduction of the action table The key idea is explicitly enumerating how each object in S can be converted from the undesired, or initial, state to the desired one, or final. That information can be captured as a set of action sets within a table called the action table This creates a limitation, compared to the AAR approach by requiring the targeted decision to be known before hand; this limitation, however, is similar to that required by the ARD approach For example, if we consider Table I as our decision table and the targeted decision is \(d l 001\027 h then   L={x 001\031 X: d\(x l i.e. all the objects with decision value l    H={x 001\031 X: d\(x h i.e all the objects with decision value h  The action table will contain the necessary action sets to move every object in L to an object in H. The action table will contain exactly card\(L\card\(H\action sets Each row in the action table reflects the necessary action sets for transforming an object from low l to high  h This operation is formalized as following 007 x 002 L, y 002 H: x\002y is a possible transition describing a new action set t and 007 a 002 A   If a is flexible and a\(x\ a\(y\: a t a\(x\\002a\(y   If a is stable and a\(x\ a\(y\, then a\(t\ is contradicting and is therefore discarded   y\, then a t a\(x Example To generate the action table we start from the decision Table I and organize it into two subtables, as shown in table II, with respect to the decision attribute d  TABLE II  T HE D ECISION TABLE ORGANIZED WITH RESPECT TO THE DECISION ATTRIBUTE D   a b c d L x1 a1 b1 c2 l x2 a2 b1 c1 l x5 a2 b1 c1 l x8 a1 b2 c2 l H x3 a2 b2 c1 h x4 a2 b2 c2 h x6 a2 b2 c1 h x7 a2 b1 c2 h  
400 


Then we make a cross product from L into H and construct the action sets for each transition. If there is a change in the stable attribute we simply discard it. This process results in table III TABLE III  T HE ACTION TABLE FOR THE TARGET  D  L 002 H   a b c x1\002x3 b1\002b2 c2\002c1 x1\002x4 b1\002b2 c2 x1\002x6 b1\002b2 c2\002c1 x1\002x7 b1 c2 x2\002x3 a2 b1\002b2 c1 x2\002x4 a2 b1\002b2 c1\002c2 x2\002x6 a2 b1\002b2 c1 x2\002x7 a2 b1 c1\002c2 x5\002x3 a2 b1\002b2 c1 x5\002x4 a2 b1\002b2 c1\002c2 x5\002x6 a2 b1\002b2 c1 x5\002x7 a2 b1 c1\002c2 x8\002x3 b2 c2\002c1 x8\002x4 b2 c2 x8\002x6 b2 c2\002c1 x8\002x7 b2\002b1 c2  Note: the atomic action set with the decision attribute d will always be \(d, l\002h\ and therefore would be redundant in the action table A  Implications of the Action Table First, unlike the original decision table, the action table now explicitly includes change values as part of the attribute definition. Second, the decision cardinality card d l 001\027 h}\ is equal to the number of rows in the action table. Third, the action table does not allow us to recover the individual counts of Z1 or Z2; nor, for that matter, the individual counts of A1 or A2.  This information is not needed, then support for t, where t=t1∑t2 and t2 d l 001\027 h}, is given by Supp  t   card  A 1 006 Z 1  card  A 2 006 Z 2 2 This calculation can take place by performing a count count operation on the action table What is attractive about this definition is it is the same support measure used in the traditional Association Mining [14   Now, if we look at the confidence measure used in traditional Association Mining [14   w e see  conf  r   Supp  r  Supp  t 1 3 One potential problem arises if the traditional association mining confidence measure is utilized. While the support of the rule can be directly calculated from the action table, the Supp t1\ cannot be calculated from the action table. This is because the action table does not contain all occurrences of A1 and A2.  However, the support for A1 and A2 can be easily calculated from the decision table; a simple pass through the table, performing a basic count operation is required. More formally, the support of an action set t1 with standard interpretation Ns t1 A1, A2   Supp  t 1  card  A 1  card  A 2 4 V   T HE FAARM STRATEGY  Using the concept of action table and the resulting changes to support and confidence, we believe that any association-mining algorithm can be used to generate action rules. Here, we demonstrate the use of the FPGrowth algorithm [2 o g e n e r a te th e a c ti on  ru les  W e  cal l  the resultant approach FAARM \(Frequent Association Action Rule Mining The FP-Growth algorithm is a divideÖandÖconquer approach that is considered to be an order of magnitude faster than Apriori  It re l i es  on a s p ec ial tr ee d a ta  structure called FP-Tree[2 w h ich is  o b tain e d  b y o r d e rin g  the transaction attributes values by their frequency pruning those that do not meet a given minimum support and then inserting the transaction, or action sets in our case, into a tree. The result is a condensed data structure that avoids expensive database scans and is especially tailored for dense datasets [2   The same concepts of atomic action set, action set and Standard Interpretation are borrowed from previous work However, the definitions of support and confidence used are those introduce in section IV.A Now, we present FAARM, the proposed process of extracting action rules from  FAARM  M ETHOD  1  Specify the decision target 2  Generate all the atomic action sets from A 3  Calculate the frequency of each atomic set 4  Build the action table 5  Prune and reorder the action table 6  Build the FP-Tree from the action table 7  Run FP-Growth on the FP-tree a  Extract frequent action sets b  Build and test the action rules  To better explain our proposed FAARM strategy, we go through the example on the decision table described in Table I. We use minSup 4 and the minConf 80  1\ Set the targeted decision   d l 001\027    2\ Generate the atomic sets  The atomic sets are all the possible transitions for every attribute b,b2\, \(b,b1 001\027 b2 b,b2 001\027 b1\, \(c,c1\, \(c,c2\ \(c,c1 001\027 c2\ \(c,c2 001\027 c1  3\ Calculate the frequency of each atomic set In order to calculate the support of each atomic action set with regard to a decision target, it is sufficient to scan the 
401 


decision table once and count the occurrence of each attribute with respect to decision's left and right values here l and h Table IV\. Then, using the support formula 4\ and the minimum support criteria, we can calculate the exact support of all the possible atomic action sets \(Table V TABLE IV  T HE FREQUENCY OF ATTRIBUTES IN V FOR THE ACTION d, l 001\027 h FROM THE DECISION TABLE IN ONE PASS   l h a1 2 0 a2 2 4 b1 3 1 b2 1 3 c1 2 2 c2 2 2 TABLE V  S UPPORT OF ALL ATOMIC ACTION SETS  MIN S UP   4   Atomic action set Support with regards to \(d, l\002h a,a1 0 \(Does not meet min support a,a2\ 8 b,b1 3 \(Does not meet min support b,b2 3 \(Does not meet min support b,b1\002b2\ 9 b,b2\002b1 1 \(Does not meet min support c,c1\ 4 c,c2\ 4 c,c1\002c2\ 4 c,c2\002c1\ 4  For a later use, the list of atomic action sets is ordered by descending support List b1 001\027 a2\ \(c1\,\(c2\, \(c1 001\027 c2\, \(c2 001\027 c1 4\ Generate the action table  see section IV for the example 5\ Prune and reorder the action table Once the action table is generated, we can use List to order action sets by descending support and remove atomic sets that does not meet the minimum support. This operation facilitates the creation of the FP-Tree  6\ Build the FP-Tree from the action table  Generating the action table and storing it in memory is expensive time and space complexity is O\(n 2 where n is the number of transactions; to alleviate the space complexity we propose to use the FP-Tree to store the table. To build the FP-Tree we insert each action set, from the ordered and pruned action table, into the tree using the atomic action sets as the nodes. Each time a node is inserted or reused we increment its local count. If the node has been inserted somewhere else in the tree we create a link to the last inserted one \(Figure 1  7a\ Extract Frequent action sets  FP-Growth receives an FP-Tree as an input and does its traditional job for extracting frequent patterns given minimum support criterion  7b\ Generating Association Action Rules  The following frequent action sets are extracted from the FPTree using the FP-Growth algorithm [2   t1  a a2 b b1\002b2 c c1 d  l\002h  Supp  t1 4=minSup t2  a a2 c c1\002c2 d l\002h Supp  t2 4=minSup Finally, we can easily construct the association action rules and check their confidence. This operation takes exactly one scan of the Table 1 for each frequent action set r1  a a2 b b1\002b2 c c1   002  d l\002h supp r1 4=minSup, conf r1 4/4 >minConf r2= \(a a2 c c1\002c2   002  d l\002h   supp r2 4=minSup, conf r2 4/8<minConf Only r1 meets the minimum confidence for a valid association action rule extracted from S   Figure 1  FP-Tree generated from the action table VI  E XPERIMENTS  The experimental goal is to compare the performances of our algorithm FAARM to other action rules discovery algorithms that do not use pre-existing classification; the ones selected are AAR and ARD. We also considered datasets used in previous action rules discovery literature namely Hepatitis [9 an d Nu rs ery  1 2   th e  d a tase ts c a n  b e  obtained from the UCI Machine Learning Repository [1   Finally, we used the same classification \(Stable, Flexible Decision\ for the attributes as used in [9, 12   A  Description of the datasets A brief description of each dataset is provided in this section 1  Hepatitis dataset This dataset contains clinical data of patients affected by the Hepatitis disease. It has 155 records and 19 attributes, not including the decision attribute.  The attributes are decomposed into 2 stable attributes and 17 flexible attributes.  Each flexible attributes is composed 9 values or less; the majority are composed of only 2. The patients are classified into: Die, Live. Our target is to find rules to change the likelihood of this classification i.e targeted decision effect is  class die 001\027 live  
402 


2  Nursery dataset This dataset is composed of the evaluation forms of applications to nursery schools. The dataset has 12960 records and 8 attributes, not including the decision attribute. The attributes are decomposed into 4 stable attributes and 4 flexible attributes.  Each flexible attributes is composed of 3 values or less. Our target is to find rules to enhance the chances of having an application go from being not recommended to priority i.e. targeted decision effect is  rank not_recom 001\027 priority B  Experimental Methodology In order to achieve fair performance comparison, we have implemented a version of ARD and AAR with the following modifications   The same definition of support is used as in FAARM: using different definition led to different set of rules, which is expected, as using different definitions would results in different action sets to be discarded/kept   AAR: First, we initially prune atomic-sets of the class attribute that are different from our targeted decision. Second, we stop the Apriori iterations if none of the frequent action sets contains the targeted decision   Because the search space is different in the three algorithms \(N in AAR and ARD, N 2 in FAARM the final support value for an action set in AAR and ARD must be squared With these modifications, the three algorithms will now produce the same rules.  Thus, the purpose of this comparison is to determine which method, if any, is faster In particular, as action set generation is more expensive than rule generation, we examined the impact of changing the support value.  For this study, we use a minimum confidence threshold of 80% for all experiments C  Results The results in Figure 2 show that FAARM achieves better performances on both Hepatitis and Nursery datasets with both high and low support values. With FAARM generating the action table from the Nursery would take 4320\*\(4266\operations, these values are the number of records with status not_recom and status priority  respectively. This operation is expensive, but once generated and encoded, the FP-Tree could be mined quickly  AAR performs better on Hepatitis, mainly because of the lower number of candidates. ARD, on the other hand showed the worst performance. During the experiments we noticed that ARD was fast when finding shortest rules and slower finding longer rules.  Furthermore, ARD is very poor at finding a good stopping point; in fact, it actually goes through all the iteration phases \(the maximum number of iteration being the length of the transaction\ The reason is that, for each generation, there are always some candidates to test, even if a positive mark isnêt achievable An interesting aspect is the extent of FP-Tree compression over the action table. Figure 4 shows the size of the FP-Tree constructed from the action table in the Nursery experiment with varying minimum support values. While the action table size is always 250Mb, the size of the FP-Tree is 3.5Mb even for very small minimum support. The size of the FP-Tree shrinks even further for increasing minimum support values and this is due to the pruning at this level   Figure 2  Speed comparison of FAARM, AAR and ARD extracting action rules on Hepatitis dataset with varying minimum support threshold  Figure 3  Speed comparison of FAARM, AAR and ARD extracting action rules on Nursery dataset with varying minimum support threshold 
403 


 Figure 4  Compression of the action table from Nursery dataset of size 250Mb into FP-Tree with varying minimum support values VII  C ONCLUSION AND FUTURE WORK  In this paper, we propose the action table as the ideal search domain for action rules mining. The action table transforms the complex problem of finding action rules from a plain decision table, into finding action rules from an action table. As a result, the problem of action rules mining is reformulated into association-mining In practice, we applied FAARM on the Hepatitis and Nursery datasets and compared the results and performances with AAR and ARD. Although the space and time complexity associated with generating the action table are O\(n 2 experiments show that FAARM has a better execution time on relatively small dataset, over ARD and AAR Generating the action table directly into the FP-Tree could mitigate the space complexity associated with action table. As a future work, we propose to look at parallel implementation of Apriori and FP-Growth to test the scalability of using the action table with large datasets R EFERENCES  1  Z. Pawlak, çInformation systems - theoretical foundationsé, Information Systems Journal, Elsevier, Vol 6, 1981, 205-218 2  J. Han, J. Pei, and Y. Yin, çMining frequent patterns without candidate generationé, ACM SIGMOD International Conference on Management of Data, 2000, 1 12 3  Z.W. Ras and A. Wieczorkowska,  "Action-Rules: How to Increase Profit of a Company",  The Fourth European Conference on Principles and Practice of Knowledge Discovery in Databases, 587-592 4  Z. He, X. Xu, S. Deng, R. Ma, çMining action rules from scratché, Expert Systems with Applications, Elsevier, Vol 29, No. 3, 2005, 691-699 5  Z.W. Ras and A. Dardzinska,  "Action Rules Discovery without Pre-existing Classification Rules",   The Sixth International Conference on Rough Sets and Current Trends in Computing, 2008, 181-190 6  Z.W. Ras, A. Dardzinska, L. Tsay,  and H. Wasyluk Association Action Rules",   IEEE International Conference on Data Mining Workshops, 2008, 283-290 7  Qiang Yan, Jie Yin, Charles Ling, Tielin Chen,"Postprocessing Decision Trees to Extract Actionable Knowledge", IEEE International Conference on Data Mining, 2003,  685-688 8  Z.W. Ras and L. Tsay,  "Discovering Extended ActionRules \(System DEAR\,   International IIS IIPWM'03 Conference, 2003, 293-300 9  L. Tsay and Z.W. Ras,  "Action rules discovery: system DEAR2, method and experiments",   Journal of Experimental & Theoretical Artificial Intelligence, 2005 119-128   Z.W. Ras, E. Wyrzykowska,  and H. Wasyluk,  "ARAS Action Rules Discovery Based on Agglomerative Strategy",   Third International Workshop on Mining Complex Data, 2007, 196-208   http://archive.ics.uci.edu/ml/datasets   S. Im and Z.W. Ras,  "Action Rule Extraction from a Decision Table: ARED",   International Syposium on Methodologies for Intelligent Systems, 2008, 160-168   J. S. Deogun, V. V. Raghavan, and H. Sever, çRough set based classification methods and extended decision tables International Workshop on Rough Sets and Soft Computing, 1994, 302-309   R. Agrawl and R. Srikant, çFast algorithm for mining assocation rules,é International Conference on Very Large Data Bases, 1993, 487-499  
404 


a  Figure 1.  The original Share-struct and the steps of S?s change C. Share-FPM algorithm In this subsection, we will develop an efficient algorithm for mining all frequent patterns Algorithm 2 Share-FPM Input: S?, the Share-struct constructed based on Algorithm 1 and s, the minimum support threshold Output: The complete set of frequent patterns Method:Call Share-FPM \(S?,?, s Procedure Share-FPM \(S? ,?, s 1 for each entry si in S 2   if \(si.local-count<s and si->new! =NULL 3     add all si ->news children to the new fields of relevant entries in S 4   else ? =?si; ?-SD = ?-SD 5     if \(si ->new != NULL 6       if si is the only active entry 7         ?-SD=?-SDsi 8         add all children of si to relevant entries in S 9         si ->old= S 10      else ?-Postfix = ?-Postfix ?si 11        if \(si ->old == S 12          call Inherit\(si, S TID Items Bought \(Ordered 100 f, a, c, d, g, i, m, p f, c, a, m, p 200 a, b, c, f, l, m, o f, c, a, b, m, l, o 300 b, f, h, j, o f, b, o 400 b, c, k, s, p c, b, p 500 a, f, c, e, l, p, m, n f, c, a, m, p, l 1431 13        else call Initialize \(S?, si->new, si->old 14        if \(si ->old == NULL 15          flag_upload =TRUE 16        si->old = S 17        call Share-FPM \(S?, ?, s 18        if \(flag_upload ==TRUE 19          call Upload \(S?, S 20    else if \(si ->old ? S 21      ?-SD=?-SD?the items that appear after ? in ?-SD 22      ?-Postfix=the items that appear after ? in ?-Postfix 


23      for each pi in ?-Postfix 24        ? = ??pi 25        ?-SD = the items that appear before pi in ?-SD 26        call Share-FPM \(pi->old, ?, s 27      call Generate-FP \(?, ?-SD, ?-Postfix 28 call Generate-FP \(?,?-SD, ?-Postfix the end of Share-FPM The procedures Inherit, Upload and Generate-FP are shown in the following Procedure Inherit \(s, S if s->old can be released then //memory management S? = s->old else create a new Share-table S? by inheriting s->old call Initialize \(S?, s->new  Procedure Upload \(S?, S for each entry si in S upload all new and old fields in S? to old fields in S add si.local-count in S? to si.local-count in S  Procedure Generate-FP \(?,?-SD, ?-Postfix for each nonempty combination ? of the items in ?-SD generate pattern ?? ? with support minimum support of items in it for each item pi in ?-Postfix for each combination ? of the items which appear before pi generate pattern ?? ? ? pi with support minimum support of items in it  D. Share-UFPM Algorithm According to our mining model description, each utility frequent pattern is also frequent. After Share-FPM algorithm finds all frequent patterns, the Share-UFPM algorithm scans the database once to check whether each frequent pattern candidate algorithm is as follow Algorithm 3 Share-UFPM Input:   S?, s Output:   UFP, utility frequent patterns in DB Method:  Call Share-UFPM\(S?, s 


Procedure Share-UFPM\(S?, s UFP FP = Share-FPM \(S?,?, s for each transaction Ti ? DB for each candidate c ? FP if \(c ? Ti and u \(c, Ti c.support for each candidate c ? FP if \(c.support ? s UFP = UFP + c return UFP  IV. PERFORMANCE STUDY To evaluate the efficiency and effectiveness of our algorithms, we have done extensive experiments on various kinds of datasets with different features. The experiments are based on a 2.4GMHz Pentium IV PC with 512MB main memory and 60 GB hard driver, running on Microsoft Windows 2000 Server. All the programs are written in Microsoft/Visual C++6.0 The measured performance is algorithms execution time on the datasets with different minimum support threshold. The execution time only includes the disk reading time \(scan datasets output frequent patterns speed of disk writing A. Datasets and characteristics We use real world and synthetic data for our performance study. The basic characteristics of datasets are listed in the following The real world dataset called Retail is achieved from a retailing company. Retail contains products from various categories. There are 16469 items and 88162 transactions in the dataset. Each transaction consists of the products purchased by a customer at a time point. Its average transaction size and average maximal potentially frequent patterns size are 10.3 and 3. The size of this dataset is 4M The synthetic data sets which were used for the experiments were achieved from the online FIMI repository See the RUL: http://fimi.cs.helsinki.fi/. The data sets are T10I4D100K and T40I10D100K. In T10I4D100K, the average record size and average maximal potentially frequent patterns 


size are 10 and 4. In T40I10D100K, they are 40 and 10. The numbers of transactions in both two dataset are set to 100K There are exponentially numerous frequent patterns when the support threshold goes down B. Experimental results In order to mine the utility frequent patterns, we randomly generate the count of each item between 1 and 6. In fact, most items are in the low profit range, we synthetically generate utility values of each item from 0.01 to 10.00, using a log normal distribution. For instance, Fig.2 shows the distribution of the utility values of items in T10I4D100K 1432 0 1 2 3 4 5 6 7 8 9 10 0 20 40 60 80 100 120 140 160 180 N um be r o f i te m s Utility value  Figure 2.  Utility value distribution in T10I4D100K For selecting appropriate utility thresholds, we use the average transaction utility value to constraint the utility threshold instead of randomly choosing it. For example, in Table 1, where the average transaction utility value is 40. If the utility threshold is equal to 25%, it represents that ? = 10 40  25% =10 We compare the performance of Share-FPM with BUUFM [5], an up to date algorithm for utility frequent patterns 


mining. Fig.3 through Fig.5 show the performance curves of two algorithms on three datasets respectively. We can see that the Share-UFPM algorithm outperforms BU-UFM on all datasets, and the performance gap becomes significant when the minimum support threshold drops low enough 0.0 0.2 0.4 0.6 0.8 1.0 1.2 0 10 20 30 40 50 60 70 80 90 100 110 120 Ti m e S ec on ds  Minimum support \(%0 Share-UFPM BU-UFM Utility threshold=5  Figure 3.  Fig.12 Utility frequent patterns mining on Retail 0.0 0.2 0.4 0.6 0.8 1.0 0 50 100 150 200 250 300 350 


400 450 500 550 600 Ti m e S ec on ds  Minimum support \(%0 Share-UFPM BU-UFM Utility threshold=5  Figure 4.  Fig.13 Utility frequent patterns mining on T10I4D100K 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 0 50 100 150 200 250 300 350 400 450 500 550 Ti m e S ec on ds  Minimum support Share-UFPM 


BU-UFM Utility threshold=5  Figure 5.  Utility frequent patterns mining on T40I10D100K V. CONCLUSIONS In this paper, we introduce a utility frequent pattern mining model based on a share strategy to find the combination of items with high frequencies and utilities. This model first find all patterns with a given minimum support threshold. In this step, a share strategy gives a way to share most of the results from the previous mining process instead of separating them distinctively, thereby dramatically reducing the cost of computation. And then all patterns that do not satisfy a minimum utility threshold are pruned The extension of our technique, for maintenance of the already mined utility frequent patterns when updating databases, is an interesting topic for future research REFERENCES 1] R. Agrawal, T. Imielinski, A. Swami, Mining association rules between sets of items in large databases, In: Proceedings of the 1993 ACM-SIGMOD, Washington, DC, 1993, 207216 2] J. Han, H. Cheng, D. Xin, X. Yan, Frequent pattern mining: current status and future directions, Data Min knowl Disc 2007, 55-86 3] Y. Liu, W. Liao, A.Choudhary, A two-phase algorithm for fast discovery of high utility itemsets, Lecture Notes in Artificial Intelligence 2005, 3518:689-695 4] Y. Liu, W. Liao, A. Choudhary, A fast high utility itemses mining algorithm, In: Proceeding of the 2005 ACM SIGKDD workshop on utility-based data mining, Chicago, Illinois, USA, 2005, 90-99 5] J. Yeh, Y. Li, C. Chang, Two-phase algorithms for a novel utilityfrequent mining model, Lecture Notes in Artificial intelligence 2007 4819: 433-444 6] C, Aaron, F. John, Association mining, ACM Computing Surveys 2006, 38\(2 7] H.Yao, H. Hamilton, C. Butz, A foundational approach to mining itemset utilities from databases, In: Proceeding of the 4th SIAM International Conference on Data Mining, Lake Buena Vista, Florida 2004, 428-486 8] H. Yao, H. Hamilton, L. Geng, A unified framework for utility based measures for mining itemsets, In: Proceedings of ACM SIGKDD 2nd workshop on utility-based data mining, New York, NY, 2006, 28-37 9] J. Han, M. Kamber, Data mining: concepts and techniques, 2nd edn 


Morgan Kaufmann. 2006 10] J. Han, J. Pei, Y. Yin, Mining frequent patterns without candidate generation, In: Proceeding of the 2000 ACM-SIGMOD international conference on management of data, Dallas, TX, 2000, 112 


2] S. Brin, R. Motwani, J. D. Ullman, and S. Tsur Dynamic Itemset Counting and Implication Rules for Market Basket Data," in Proceedings of the 1997 ACM SIGMOD international conference on Management of data, Tucson, Arizona, United States 1997, pp. 255-264 3] J. S. Park, M. S. Chen, and P. S. Yu, "An Effctive Hash based Algorithm for mining association rules in Prof. ACM SIGMOD Conf Management of Data New York, NY, USA, 1995, pp. 175 - 186 4] R. Agrawal, T. ,PLHOL?VNL DQG $. Swami, "Mining Association Rules between Sets of Items in Very Large Databases," in Proceedings of the 1993 ACM SIGMOD international conference on Management of data, Washington, D.C., 1993, pp. 207-216 5] H. Mannila, H. Toivonen, and A. I. Verkamo Efficient Algorithms for Discovering Association Rules," in AAAI Workshop on Knowledge Discovery in Databases, 1994, pp. 181-192 6] R. Srikant and R. Agrawal, "Mining Generalized Association Rules," in In Proc. of the 21st Int'l Conference on Very Large Databases, Zurich Switzerland, 1995 7] R. Srikant, Q. Vu, and R. Agrawal, "Mining association rules with item constraints," in In Proc 3rd Int. Conf. Knowledge Discovery and Data Mining, 1997, pp. 67--73 8] A. Savasere, E. Omiecinski, and S. B. Navathe, "An Efficient Algorithm for Mining Association Rules in Large Databases," in Proceedings of the 21th International Conference on Very Large Data Bases 1995, pp. 432 - 444 9] H. Mannila, "Database methods for data mining," in The Fourth International Conference on Knowledge Discovery and Data Mining, 1998 10] B. Liu, W. Hsu, and Y. Ma, "Mining Association Rules with Multiple Minimum Supports.," in SIGKDD Explorations, 1999, pp. 337--341 11] H. Yun, D. Ha, B. Hwang, and K. H. Ryu, "Mining association rules on significant rare data using relative support.," Journal of Systems and Software archive vol. 67, no. 3, pp. 181 - 191, 2003 


12] M. Hahsler, "A Model-Based Frequency Constraint for Mining Associations from Transaction Data Data Mining and Knowledge Discovery, vol. 13, no 2, pp. 137 - 166, 2006 13] L. Zhou and S. Yau, "Association rule and quantitative association rule mining among infrequent items," in International Conference on Knowledge Discovery and Data Mining, San Jose, California 2007, pp. 156-167 14] C. Ordonez, C. Santana, and L. d. Braal, "Discovering Interesting Association Rules in Medical Data," in Proccedings of ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, 2000, pp. 78-85 15] L. J. Sheela and V. Shanthi, "DIMAR - Discovering interesting medical association rules form MRI scans," in 6th International Conference on Electrical Engineering/Electronics, Computer Telecommunications and Information Technology 2009, pp. 654 - 658 16] C. Ordonez, N. Ezquerra, and C. A. Santana Constraining and summarizing association rules in medical data," Knowledge and Information Systems vol. 9, no. 3, pp. 259 - 283, September 2005 17] H. Pan, J. Li, and Z. Wei, "Mining Interesting Association Rules in Medical Images," Lecture Notes In Computer Science, vol. 3584, pp. 598-609, 2005 18] S. Doddi, A. Marathe, S. S. Ravi, and D. C Torney Discovery of association rules in medical data Medical Informatics and the Internet in Medicine, vol 26, no. 1, pp. 25-33, January 2001 86 


the time needed for execution exceeded 100000 seconds Thus, from this analysis we see that FPrep, which uses FCM clustering, clearly outperforms the CLARANS and CURE based methods on the basis of speed. The execution times for CLARANS and CURE mentioned in fig. 7 and Table II do not include the time required to create fuzzy sets, and calculate the membership value  for each numerical data point in every fuzzy set for the numerical attribute under consideration. These times also do not take into account the time required to transform crisp numerical attributes to fuzzy attributes, and derive the fuzzy dataset from the original crisp dataset The fuzzy partitions generated for each of the five numerical attributes for the USCensus1990raw dataset are shown in Table III. Coincidentally, generating three fuzzy partitions for each numerical attribute seemed a perfect fit In addition to the superior speeds achieved by FPrep, as illustrated in fig. 7 and Table II, Table III indicates the semantics and the quality of the fuzzy partitions generated by FPrep. Moreover, the number of frequent itemsets generated by a fuzzy ARM algorithm \(like fuzzy ARMOR and fuzzy Apriori minimum support threshold, is illustrated in fig. 8   Fig. 7. Algorithm, numerical attribute comparison based on speed \(log10 seconds   Fig. 8. Number of frequent itemsets for various minimum support values  B. Results from Second Dataset We have also applied FPrep on the FAM95 dataset http://www.stat.ucla.edu/data/fpp transactions. Of the 23 attributes in the dataset, we have used the first 18, of which six are quantitative and the rest are binary. For each of the six quantitative attributes, we have generated fuzzy partitions using FPrep. A thorough analysis with respect to execution times, has already been performed on the USCensus1990raw dataset \(which is manifolds bigger in size than the FAM95 dataset both on the basis of number of transactions and number of unique values for numerical 


attributes dataset has been done solely to provide further evidence of the quality and semantics of the fuzzy partitions generated by FPrep. The details of the same are in Table IV. In this case, the number of fuzzy partitions is different for different numerical attributes. Thus, the number and type of fuzzy partitions to be generated is totally dependent on the attribute under consideration. A graphical representation of the fuzzy partitions generated for the attribute Age has already been provided in fig. 5, and clearly shows the Gaussian nature of the fuzzy partitions. The nature and shapes of fuzzy partitions for the rest of the attributes are also similar. Last, the number of frequent itemsets generated for different minimum support values is illustrated in fig. 8  C. Analysis of Results With FPrep, we can analyze and zero in on the number and type of partitions required based on the semantics of the numerical attributes, which the methods detailed in [19 20] do not necessarily facilitate. Then, FPrep, backed by FCM clustering, takes care of the creating the fuzzy partitions, especially assigning membership values for each numerical data point in each fuzzy partition. In section 8.A we have already shown that FPrep is nearly 9 to 44 times faster than the CURE-based method, and 2672 to 13005 times faster than the CLARANS-based method. FPrep is not only much faster than other related methods, but also generates very high quality fuzzy partitions \(Table III and IV much user-intervention. We have created a standard way of representing any fuzzy dataset \(converted from any type of crisp dataset efficacy of the same is corroborated by the successful implementation of Fuzzy Apriori and Fuzzy ARMOR on the fuzzy dataset \(converted from crisp version of FAM95 dataset an initial implementation of Fuzzy ARMOR, are very encouraging. FPrep, when used in conjunction with these fuzzy ARM algorithms, generates a pretty good number of high-quality frequent itemsets \(fig. 8 frequent itemsets generated for a particular minimum support is same, irrespective of the fuzzy ARM algorithm 


used IX. CONCLUSIONS In this paper we have highlighted our methodology, called FPrep, for ARM in a fuzzy scenario. FPrep is meant for seamlessly and holistically transforming a crisp dataset into a fuzzy dataset such that it can drive a subsequent fuzzy ARM process. It does not rely on any non-fuzzy techniques and is thus more straightforward, fast, and consistent. It facilitates user-friendly automation of fuzzy dataset 1 0 1 2 3 4 5 Age - 91 Hours - 100 Income3 4949 Income2 13707 Income1 55089 Ti m e lo g1 0 se co nd s Numerical Attribute - Number of Unique Values FCM CURE CLARANS 0 500 1000 1500 2000 2500 


3000 0.075 0.1 0.15 0.2 0.25 0.3 0.35 0.4 N o o f F re qu en t I te m se ts Minimum Support USCensus1990 FAM95 generation through FCM, and subsequent steps in preprocessing with very less manual intervention and as simple and straightforward manner as possible. This methodology involves two distinct steps, namely creation of appropriate fuzzy partitions using fuzzy clustering and creation of fuzzy records, using these partitions, to get the fuzzy dataset from the original crisp dataset FPrep has been compared with other such techniques, and has been found to better on the basis of speed. We also illustrate its efficacy on the basis of quality of fuzzy partitions generated and the number of itemsets mined by a fuzzy ARM algorithm which is preceded by FPrep. This preprocessing technique provides us with a standard method of fuzzy data \(record that it is useful for any kind of fuzzy ARM algorithm irrespective of how the algorithm works. Furthermore, this pre-processing methodology has been adequately tested with two disparate fuzzy ARM algorithms, Fuzzy Apriori and Fuzzy ARMOR, and would also work fine with other fuzzy ARM algorithm REFERENCES 1] Zadeh, L. A.: Fuzzy sets. Inf. Control, 8, 338358 \(1965 2] Chen G., Yan P., Kerre E.E.: Computationally Efficient Mining for Fuzzy Implication-Based Association Rules in Quantitative Databases. International Journal of General Systems, 33, 163-182 


2004 3] Hllermeier, E.: Fuzzy methods in machine learning and data mining Status and prospects. Fuzzy Sets and Systems. 156, 387-406 \(2005 4] De Cock, M., Cornelis, C., Kerre, E.E.: Fuzzy Association Rules: A Two-Sided Approach. In: FIP, pp 385-390 \(2003 5] Yan, P., Chen, G., Cornelis, C., De Cock, M., Kerre, E.E.: Mining Positive and Negative Fuzzy Association Rules. In: KES, pp. 270-276 Springer \(2004 6] De Cock, M., Cornelis, C., Kerre, E.E.: Elicitation of fuzzy association rules from positive and negative examples. Fuzzy Sets and Systems, 149, 7385 \(2005 7] Verlinde, H., De Cock, M., Boute, R.: Fuzzy Versus Quantitative Association Rules: A Fair Data-Driven Comparison. IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics 36, 679-683 \(2006 8] Dubois, D., Hllermeier, E., Prade, H.: A systematic approach to the assessment of fuzzy association rules. Data Min. Knowl. Discov., 13 167-192 \(2006 9] Dubois, D., Hllermeier, E., Prade, H.: A Note on Quality Measures for Fuzzy Association Rules. In: IFSA, pp. 346-353. Springer-Verlag 2003 10] Hllermeier, E., Yi, Y.: In Defense of Fuzzy Association Analysis IEEE Transactions on Systems, Man, and Cybernetics - Part B Cybernetics, 37, 1039-1043 \(2007 11] Agrawal, R., Imielinski, T., Swami, A.N.: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Record, 22, 207216 \(1993 12]  Agrawal, R., Srikant, R.: Fast Algorithms for Mining Association Rules. In: VLDB, pp. 487-99. Morgan Kaufmann \(1994 13] Han, J., Pei, J., Yin, Y.: Mining Frequent Patterns without Candidate Generation. In: SIGMOD Conference, pp. 1-12. ACM Press \(2000 14] Han, J., Pei, J., Yin, Y., Mao, R.: Mining Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree Approach. Data Mining and Knowledge Discovery, 8, 5387 \(2004 15] Pudi V., Haritsa J.R.: ARMOR: Association Rule Mining based on Oracle. CEUR Workshop Proceedings, 90 \(2003 16] Dunn, J. C.: A Fuzzy Relative of the ISODATA Process and its Use in Detecting Compact, Well Separated Clusters. J. Cyber., 3, 32-57 1974 17] Hoppner, F., Klawonn, F., Kruse, R, Runkler, T.: Fuzzy Cluster Analysis, Methods for Classification, Data Analysis and Image Recognition. Wiley, New York \(1999 


18] Bezdek J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms. Kluwer Academic Publishers, Norwell, MA \(1981 19] Fu, A.W., Wong, M.H., Sze, S.C., Wong, W.C., Wong, W.L., Yu W.K. Finding Fuzzy Sets for the Mining of Fuzzy Association Rules for Numerical Attributes. In: IDEAL, pp. 263-268. Springer \(1998 20] Kaya, M., Alhajj, R., Polat, F., Arslan, A: Efficient Automated Mining of Fuzzy Association Rules. In: DEXA, pp. 133-142. Springer \(2002 21] Mangalampalli, A., Pudi, V. Fuzzy Association Rule Mining Algorithm for Fast and Efficient Performance on Very Large Datasets In FUZZ-IEEE, pp. 1163-1168. IEEE \(2009 22] Kaya, M., Alhajj. Integrating Multi-Objective Genetic Algorithms into Clustering for Fuzzy Association Rules Mining. In ICDM, pp. 431434. IEEE \(2004  Table II. Algorithm, numerical attribute comparison based on speed \(seconds  Algorithm Age - 91 Hours - 100 Income3 - 4949 Income2 - 13707 Income1 - 55089 FCM 0.27 0.3 3.13 6.28 79.4 CURE 0.25 0.25 28.67 163.19 3614.13 CLARANS 1.3 1.34 8363.53 78030.3 Table III. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions Age Old Middle Aged Young Hours More Average Less Income1 High Medium Low Income2 High Medium Low Income3 High Medium Low  Table IV. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions AGE Very old Around 25 Around 50 Around 65 Around 35 HOURS Very High Zero Around 40 Around 25 INCHEAD Very less Around 30K Around 50K Around 100K INCFAM Around 60K Around 152K Around 96K Around 31K Around 8K TAXINC Around 50K Around 95K Around 20K Very less FTAX Around 15K Very less Around 6K Very high Around 33K  


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


