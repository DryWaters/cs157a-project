Improving Performance of the K-Nearest Neighbor Classifier by Tolerant Rough Sets Yongguang Bao Xiaoyong Du2 Naohiro Ishii Department of Intelligence and Computer Science, Nagoya Institute of Technology Gokiso-cho, Showa-ku Nagoya 466-8555 Japan baoyg ishii} @egg ics.nitech.ac.jp School of Information, Renmin University of China 100872 Beijing, China Duyong mail ruc. edu. cn I 2 Abstract In this paper we report our efforts in improving the pelformance of the 
k-nearest neighbor classification by introducing the tolerant rough set We relute the tolerant rough relation with object similarity Two objects are called similar if and only if these two objects satisfy the requirements of the tolerant rough relation Hence the tolerant rough set is used to select objects from the training data and constructing the similarity function The GA algorithm is used for seeking the optimal similarity metrics Experiments have been conducted on some artificial and real world data the results show that our ulgorithni 
can improve the performance of the k-nearest neighbor classification and get the higher accuracy compared with the C4.5 system 1 Introduction Classification is a primary data mining method Given a set of classes it assigns a point XER to one of those classes Classification has been applied in many applications such as the credit approval pattern recognition part classification in computer vision and so on Many induction algorithms have been proposed for pattern classification problem For example ID3, C4.5 k Nearest Neighbor Naive-Bayes T2 
Neural-network association rules etc However improving accuracy and performance of classifiers is still attractive to many researchers In this paper we focus on the k-nearest neighbor classification method The k-nearest neighbor method is a predictive technique Given a decision table it scans the all table to find the most similar object with a new one and then assign the class label of the selected object to the new object Some algorithms use training data as the decision table directly But in many cases it is not appropriate because there 
may exist noise missing or redundant data in the training data Moreover a big decision table will reduce the efficiency of the classification Hence, conceptually a k-NN classification algorithm can be viewed as two parts constructing a decision table from the training data and finding the k nearest neighbor for an unseen object. Theoretically these two steps are independent Many researchers have developed extensions to the nearest neighbor algorithm  171 In this paper we use the tolerant rough set to select a set of objects 
from the training data that have the same classification power as the original data set It can reduce the size of decision table and make the algorithm no sensitive to irrelevant and redundant attributes The k-nearest neighbor algorithm needed to construct a similarity metrics Similarity metrics greatly affects the accuracy of the nearest neighbor classification Even though we use the same training data set, the algorithms with different distance function will result in completely different predictions Similarity metrics is usually represented by a distance function based on the attribute 
values Hence determination of the weight of each attribute and the similarity threshold are the most important for the nearest neighbor method In this paper we use tolerant rough set to measure the classification power of each attribute and CA algorithm to find the optimal threshold for similarity and weights of each attribute It means also that we find the optimal tolerant rough relations between objects that provide as small classification error as possible Experiments have been conducted on some artificial and real world data The results 
show that our method can improve the accuracy of k-nearest neighbor classification and get the higher accuracy compared with C4.5 system The remainder of this paper is organized as follows Section 2 introduces the relevant concept of the tolerant rough set and similarity measure used for classification In Section 3 we describe how to determine the features weight and similarity threshold optimally by using the CA Section 4 presents our classification method and 0-7695-1 128-7/01 10.00 0 2901 IEEE 167 


Section 5 presents the results of the experiments that compare our algorithm with the C4.5 system and no weighted k-nearest neighbor method A short conclusion is given in the final section 2 Tolerant Rough Set Rough set theory introduced by Z Pawlak in the early 198Os[l is a new mathematical tool to deal with vagueness and uncertainty in the areas of machine learning knowledge acquisition decision analysis knowledge discovery from databases expert systems decision support systems, inductive reasoning and pattern recognition The rough set is based on the indiscernibility relation that satisfies reflexivity symmetry and transiti vity However in the problem of data classification it is inconvenient to describe the similarity among data with the indiscerniblity relation because similarity relation is not transitive Two data x and z cannot be guaranteed in the same class even though x and y are in one class and y and z are in other class So we introduce a tolerant relation as follows Let R  U Au\(d be a decision table where U is a set of elements objects examples A is a set of condition attributes d is the decision attribute Each attribute aEA has a set of values V called domain of the attribute and let r\(d be a number of decision values Let SA  R  R,E V x V n a\200 A be a set of tolerant relations where each such tolerance relation satisfies Reflexive  V VE V  v R v Symmetric  vI R,vz v2 R VI We say that two objects x and y are similar with respect to the attribute a when the attribute values a\(x and a\(y satisfy y Further we say that two objects x and y are similar with respect to all attributes A if a\(x y denoted as x rA y A tolerance set TS\(x of an object x is defined as Now we can define the lower approximation rA Y  and the upper approximation rA Y  of a set YE U that have the tolerance relation with respect to all attributes A as TS\(X   X rA y  rA Y  YEU TS\(y EY rA*\(Y  yEU:TS\(y nY The meaning of two approximations in the tolerant rela tion is similar to that in the indiscernibility relation Let Yi=\(x\200U  d\(x  The set is called rA positive region of partition  Yi ,i=1,2   r\(d\The coefficient is called the quality of approximation of classification  Yi i=l,z,...,r\(d It expresses the ratio of all FA correctly classified objects to all objects in the table POS\(TA  d  U TS\(X  3 i TS\(X Yi  y\(r d  card\(POS\(TA d U To construct a tolerance relation among the data we need to define a similarity measure that can quantify the closeness between attribute values of objects Let the similarity measure with respective to the attribute a between two objects x and y be S,\(X y Then two objects x and y are called similar with respect to the attribute a if S,\(x y 5 t\(a where t\(a is a similarity threshold value of the attribute a whose value is in the interval of So we can relate the tolerance relation with the similarity measure as a\(x y iff X Y 5 t\(4 In the classification problem the commonly used similarity measure is based on a normalized distance function as Sa\(x Y  d\(a\(x y where d is the maximum value of distance between two attribute values a\(x and a\(y The choice of distance function depends on the type of application. In this work we select the absolute difference between attribute values as d\(a\(x y  la\(x  a\(y due to its computational simplicity Next we can extend the similarity measure SA\(x,y between two objects x and y with respect to all attributes by an weight average of similarity measures of all attributes as SA y  Ca,Sa\(x,y where a is the number in 0,1 and Ca l In the case of considering all attributes A at the same time we can relate the tolerance relation with the similarity measure as X rA y SA y 5 t\(A where t\(A 0,1 is a similarity threshold One of the most important tasks in the pattern classification using the similarity measure defined above is the optimal determination of the similarity threshold a and t\(A because its proper determination affects the classification performance greatly In this work we apply the GA to solve this optimization problem 3 Determination of Similarity Thresholds GAS is any population-based iterative adaptive algori thm that uses selection recombination and mutation operations based on natural selection and biological genetics GAS has been proven to be a powerful method in search, optimization and machine learning They encode a potential solution to a specific problem on a simple chromosome-like data structure and apply recombination operators to these structures to achieve optimization 3.1 Chromosome Representation When we apply the CA to determine the optimal similarity threshold values, the inputs into the GA are the information table A  U Au\(d and the similarity measure and the output from the GA is a set of optimal 168 


similarity features weight a and threshold values t\(A So when an object is represented by n attributes the chromosome for the GA consists of n+l consecutive real numbers aa1  aa2  aan t\(A where aai i  1 2  n represents the feature weight for the ith attribute the sum of all a is equal to 1 The last value t\(A represents the similarity threshold that defines the tolerance relation when all attributes A are considered together 3.2 Initial Population Generation and Fitness Function The initial gene values in the chromosome are obtained by generating n random real numbers in the interval of 0,1 as the initial weights and one random real numbers in 0.001 OS as the initial similarity threshold We complete initial population by repeating the above operation 21AI times\(we set the population size to 21AI where IAl denote the number of attributes in A For preventing card TS\(x 3 i TS\(X Y  for most or all of objects we set the fitness as Fimess\(rA card\(u\(TS\(x  3 i TS\(X Y and card\(TS\(x    E x card\(u TS\(x  cnrd\(TS\(x 1    U O<E 1  We can see Fitness\(TA 5 y\(rA d 3.3 Genetic Operations The initial populations are then evolved by appropriate genetic operations in order to find a set of optimal similarity thresholds for the pattern classification The detailed explanation about the genetic operations used for the determination of the optimal similarity thresholds is given as follows 3.3.1 Selection All individuals in the population are sorted by their fitness values and the first individual is the best In our method all individuals are selected for mating We perform crossover to the ith individual and the i+l individual to generate IAl individuals of the next population where i=l,2,..IAI If the fitness value offspring is less than that of ith parent then adding the parent to the next population instead of the offspring By this way the individuals with higher fitness could be generated because the fitness of parents is higher Also we perform crossover to the jth individual and the 2IAl+l-j individual to generate IAl individuals of the next population where j 1,2,..,lAl then the average fitness of individuals of the next population could be increased because the fitness of one parent is higher and the other is lower 3.3.2 Crossover Two selected individuals are called parent and parent-2 and assume the fitness value of parent-1 is higher or equal to that of parent-2 For the t\(A set the average value of that in two parents to the offspring For weights every crossover results in one offspring by the following steps 1 For a gene o in parent-1 and the a gene q at the same position as a gene o in parent-2 if q c then set q to the same position in the offspring 2 Let the number of unset genes in the offspring be m a if m<4 then for every unset position in the offspring set the average value of genes at the same position in parents to the offspring b If m\(4 then select m+1]/2 genes from parent-1 randomly and set them to the offspring Let the sum of these genes and the genes set by step 1 be sum1 and then let the number and sum of genes in parent-2 at the unset position be n\221 and sum2 For any unset position in the offspring if sum2  0 then set 1-suml to the position else set g 1 suml where g is the gene in parent-2 at the unset position so that the sum of all weigh genes in offspring is 1 3.3.3 Mutation Select two weight genes for mutation randomly replacing a gene pair gl,g2 with a new one gl\222,g?\222 where gl+g2=gl\222+g2\222 for eeping the sum of weighs unchanged For the t\(A we generate a random number t in 0.02,0.02 and set it as t  t\(A 3.3.4 Differentiation of the same individuals After crossover and mutation the differentiation of the same individuals will be carried out for variety of individuals in the population Mutation will operate on the individuals in the population that is same as another until it is different from all other individuals 4 Classification based on the Tolerant Rough Set and Nearest Neighbor\(TRS-NN Now we describe our hybrid algorithm in detail For convenience we denote DTS\(x y\(TS\(x d\(x y in the following Let U be all of training instances p be an threshold in 0,1 for generate objects\(T0 based on tolerant relation and PRUNE the flag of pruning for TO Step 1 Determine the optimal similarity threshold values using GA 1 Read training data 2\Define the similarity measure 3 Generate initial population 4 Perform the genetic algorithm 5 Determine the optimal similarity threshold values Step 2 Determine the objects for the k-nearest neighbor classification TO1   XE U  card\(DTS\(x TS\(x 1  TO2   XEU  I>card\(DTS\(x TS\(x 2 p 169 


If PRUNE TO1   xeTO1 no y~T01 s.t DTS\(X DTS\(y TO2   XE T02 no YE TO2 s.t DTS\(X DTS\(y TO  TO 1 uT02 Step 3 Classification For unseen object U we use the smallest k as possible to classify U by using k-nearest neighbor That is mean if there is only one instance x with the minimal value SA\(x U in TO then set U to class d\(x And if there is two instances with the minimal value SA\(x U  and this two instances is in different class we should check instances with the secondary minimal value SA\(x U in TO If we cannot determine class too we should continue the process until we can determine the class of the object U End if Reproduction  Pselect  Crossover probability  P  Mutation urobabilitv P 1 5 Experiment Results 0.10 0.70 0.20 For evaluating the classification accuracy of our algorithm the TRS-NN algorithm was compared with the no-weighted k-nearest neighbor classification NW-K and the C4.5 system release 8 The data sets used in experiments are downloaded from the UCI Machine Learning Repository Each dataset with no test data set was divided into two parts training data set \(two-third of the original size in every class and testing data set. The basic characteristics of the data sets contains the size of training data and test data, class number feature number missing instance number and noise instance number in the following table Tablel Basic characteristics of the data sets used in Table 2 shows the execution parameters of the CA that is used for the optimal similarity threshold value And for the fitness we set E  0 at firstly After evolving 100 times we get the fitness If fitness 0.8 we set E  0.67 otherwise we set E  0.67 fitness Table 3 shows the execution results Because for the same fitness the similarity threshold may be little different so we run the TRS-NN 10 times for each data set and Table 2 Execution parameters for GA Excution parameters I Values Pomlation I xlAl Number of generations I 2x~ize\(train Der boundarv of t\(A I 0.50 take the average In the process of simulation for the nominal attributes we just change them to number ones artificially For example  a,b,c}==>\( 1,2,3\for the missing value feature we take the possible biggest distance as the distance between this two objects To analysis the TRS-NN we gave the result of TRS-NN in the case of p=O 0.66 and 1 respectively When compared with NW-NN and C4.5 system \(tree pruning tree rules 170 


in the most of datasets we get the higher accuracy and in the average we get the higher accuracy too Now let us analysis the affection of p In the case of p  0 it is mean that we use all the objects to nearest-neighbor selection And in the case of p  1 we just use the objects of T01 We can see that in most of case the set TO1 is just parts of all objects On the average when p=1 we can get 85.28 accuracy use 64.39 objects and get 85.08 accuracy use 37.9 objects after pruning And when p=0.66 we can get 86.03 accuracy use 90.22 objects and get 86.09 accuracy use 55.82% objects And use all objects we get 85.16 So we can get the same higher classification accuracy just used some necessary objects And from the table 3 we can see it is it is no sensitive to irrelevant and redundant attributes and it is also useful for some missing or noise data too As the result it is showed that TRS-NN developed in this paper is effectual 6 Conclusions In this paper we have presented a new approach for classification It is based on the tolerant rough set and nearest neighbor method For the better classification it is important to find the optimal tolerant relations between objects that provide as small classification error as possible We use a very simple distance function like the absolute difference of attribute between two objects due to its low computational cost and use the genetic algorithm to determine the optimal similarity threshold values. After finding the optimal features weight and similarity threshold we compute the objects reduction Based on these well-selected objects we determine the class for unseen objects using the nearest neighbor approach Simulation results show that we improve the performance of the k-nearest neighbor classification and get higher classification accuracy than C4.5 system on some artificial and real word data One important feature of TRS-NN in our view is that we can just use objects reduction for prediction using K It can be used for continuous feature directly it is no sensitive to irrelevant and redundant attributes and it is also useful for some missing or noise data too Moreover, from the feature weights in tolerant relations we can know the importance of feature And when applied for real project you can select the adequate distance function We believed that increasing the generation number of CA can improved the perfor mance As the result we can say that TRS-NN developed in this paper is an efficient algorithm Acknowledgement We would like to thank Professor J.R Quinlan for the newest C4.5 system References I Z.Pawlak ROUGH SETS \(Theoretical Aspects of Reasoning about Data 1991 2 D Nejman  A Rough Sets Based Methods of Handwritten Numerals Classification ICs Research Reports No 18 Warsaw University of Technology, Warsaw, 1994 3 M Kretowski and J Stepaniuk Selection of Objects and Attributes  A Tolerance Rough Set Approach ICs Research Reports No 54 Warsaw University of Technology Warsaw 1995 4 H.Liu and H.Motoda Feature Selection for Knowledge Discovery and Data Mining Kluwer Academic Publi shers 1998 5 T.M Cover and P.E Hart Nearest Neighbor Pattern classification IEEE Transactions on Information Theory, Vol 6 J.R Quinlan Induction of Decision Trees Machine Learning 1\(1 81-106,1986 7 J.R Quinlan C4.5:Programs for Machine Learning Morgan Kaufmann, 1993  M.Mitchell An introduction to Genetic Algorithms The MIT Press 1996  Y.G.Bao X.Y Du and N Ishii Using Rough Sets and Class Mutual Information for Feature Selection 6th inter national Conference on soft computing IIZUKA2000 452-458 IO Y Wang and N Ishii Learning Feature Weights from Similarity Information International Journal on Artificial Intelligence Tools, Vol.7,No 1 I 998 3 1-41 I I K.ohavi and D Sommerfield Data Mining using MLC Tools with AI 1996 234-245  T.Y.Lin and Y.Y.Yao, "Mining Soft Rules Using Rough Sets and Nearest Neighborhoods in Proceeding of the Symposium on Modeling Analysis and Simulation Computational Engineering in Systems Applications CESA96 IMASACS Multi-conference Lille France 9-1 2 July 1996  121 Y.Y.Yao Relational Interpretations of Neighborhood Operates and Rough Set Approximation Operators Information Science 1 1 1 1 998 239-259  131 Y.Y.Yao  Information Tables with Neighborhood Semantics in Data Mining and Knowledge Discovery Theory Tools and technology 11 Belur V Dasarathy editor procee dings of SPIE Vol 4057\(2000 108-1 16  141 H.A Guvenir and A.akkus, "Weighted K-Nearest Neigh bor Classification on Feature Projections in: proceeding of the 12th Intel Symposium on Computer and Information Sciences\(ISCISP7  Antalya,Turkey 1997,44-5 1 I51 J.D.Kelly and L.Davis A Hybrid Genetic Algorithm for Classification in proceeding of the 12th International joint Conference on Artificial Intelligence 1991 645-650  161 J.D.Kelly and L.Davis, "Hybridizing the Genetic Algorithm and the K Nearest neighbors Classification Algorithm in proceeding of the 4th International Conference on genetic Algorithms 1991, 377-383  171 D R. Wilson and T.R. Martinez An Integrated Instance Based Learning Algorithm Computational Intelligence Volume 16 Number 1 2000,pp. 1-28  181 Wettschereck, Dietrich and T.G. Dietterich An Experi mental Comparison of Nearest-Neighborand Nearest-H yperrec tangle Algorithms Machine Learning 19 1 I 995 5-28 13 No.1 1967 21-27 1095-1 100 171 


Pr o of Sketch  By sho wing that the total coun tofac hild no de is alw a ys at least equal to the paren t in the item set lattice and hence the sum total should also b e the same for all paren ts 6 Suciency and Correctness of R ULES W e no w pro ceed to sho w that the rules presen ted in the program R ULES are sucien t and correctly compute the large item sets and th us the asso ciation rules F or an y transaction kno wledge base K w ekno w that it is sucien t to compute the total coun ts of all transaction no des in K  F rom lemma 5.1 and corollary 5.1 w e also kno w that w e can a v oid computing virtual no des that are redundan t and from corollaries 5.2 and 5.3 w e kno w that w em ust compute the non-redundan t virtual no des that do not app ear in K  First w e compute the total coun ts of all transaction no des as follo ws Through rule r 1  w e compute the gr oup facts so that w e can refer to a transaction as a set of items instead of a item at a time The frequency the prop ortion to be exact of eac h distinct item v in gr oup  i.e the parameter t of v t c  is computed through rule r 3  This is accomplished b y taking a coun t of transaction IDs for a group of transactions that ha v e exactly the same item sets Then rule r 4 mak es it p ossible to inherit the transaction coun ts of sup erset transaction no des to a transaction no de for whic h they are sup ersets to b e subsequen tly summed b yrule r 6  Lemma 6.1 F or an y kno wledge base K  the rules r 1 through r 3  together with rules r 5 and r 6 compute the total coun t of all transaction no des Pr o of Sketch  By sho wing that the rules actually collect all the subsets inclusiv e of the sup erset no de of a transaction no de for whic h there is another transaction no de in K  and carries the sup erset's transaction coun t This when group ed together and summed giv es the total coun t of that no de Unfortunately  computing the signature coun ts of transaction no des is not sucien t as demonstrated in the example in gure 6 b ecause there ma y b e non-redundan t virtual no des that are of in terest to us This scenario will occur only when for an y pair of transaction no des the least common descendan t is not a transaction no de e.g gure 6 Suc h no des cannot b e redundan tb y corollary 5.3 F urthermore these t w o transaction no des m ust not b e related b y degreek subsets relation These observ ations follo w from lemma 5.1 and corollaries 5.1 through 5.3 Since these non-redundan t virtual no des do not app ear in the kno wledge base K w em ust compute them explicitly  The expression in rule r 5 computes the total coun tof nonredundan t virtual no des b y nding the in tersection of a pair of transaction no des suc h that they are not related b y the  relationship Lemma 6.2 F or an y kno wledge base K  the rules r 3 and r 4 along with r 6 compute the total coun t of all non-redundan t virtual no des Pr o of Sketch  By sho wing that the expression actually collects all the non-redundan t virtual no des and then from lemma 6.1 the pro of follo ws The follo wing theorem follo ws immediately  Theorem 6.1 Let K be a transaction kno wledge base and  m be the minim um supp ort threshold Then rules r 1 through r 8 correctly compute all the non-redundan t asso ciation rules en tailed b y K  T o pro v e this theorem w e need y et another theorem b elo w that establishes the fact that the set of large item sets is a subset of all transaction no des and non-redundan t virtual no des in the K mapping of K  whic h is actually not to o dicult to pro v e Theorem 6.2 Suciency Let K b e a transaction kno wledge base and  m b e the minim um supp ort threshold Then the set of transaction no des and non-redundan t virtual no des are sucien t to nd the non-redundan t large item sets of K  7 Multiset Pro cessing Though w e normally exp ect the system R ULES to w ork ne it breaks do wn in most practical cases This situation can b e explained with the example in gure 1 Recall that w e exp ect the system to compute the fr e q facts sho wn in the leftmost column in the gure 7 b elo w from the facts gr oup using rules r 3 through r 5  Since a b ottom-up deductiv e database will only k eep distinct facts set seman tics it will pro duce the set sho wn in the middle column of the gure 7 In the pro cess the system will thro wa w a y the b o xed fact in the rst column and th us ultimately pro duce the bo xed c and fact sho wn in the righ tmost column re\015ecting an incorrect computation of supp ort for the item set f a g  freq  f a b c g  33  freq  f a b c g  33  cand  f a b c g  33  freq  f a d g  16  freq  f a d g  16  cand  f a d g  16  freq  f a g  33  freq  f a g  33  cand  f a g  49  freq  f a g  33  freq  f a g  16  cand  f b c e g  16  freq  f a g  16  freq  f b c g  50  cand  f b c g  50  freq  f b c e g  16  freq  f b c g  50  Figure 7 Incorrect execution due to set seman tics of Datalog T o remedy this system p eculiarit y  w e can mo dify the R ULES system as follo ws The critical observ ation here is that ev ery fr e q fact con tributes to w ards the supp ort coun t of item sets and hence none of them are redundan t ev en though as a predicate they ma y b e iden tical to other predicates The issue no w is to force the database system to treat facts as m ultisets bag b efore w e start pro cessing rule r 5 rather than a set adopt a m ultiset seman tics at least sim ulate it Since the fr e q facts are deriv ed from unique transactions in the kno wledge base K w e can exploit this fact and utilize a system dened or in terpreted function to generate a new id ev ery time a fr e q fact is created and include it as an argumen tof fr e q as sho wn in gure 8 In this w a y  w e are able to force the inclusion of ev ery deriv ation as eac h predicate will be unique due to the inclusion of a unique id as an argumen t and later coun t their con tributions T o accomplish this goal w e mo dify the 6 


r 0 5  freq  genid  I 1 C 2   inh  I 1 C 1   inh  I 2 C 2  I 1  I 2  r 0 6  cand  I sum  C    freq  P I  C   Figure 8 Mo died rules that sim ulates m ultiset seman tics in Datalog rules r 5 through r 6 to obtain rules r 0 5 through r 0 6 as sho wn in gure 8 In these rules w eha v e used a system dened function called genid  that returns a unique iden tier ev ery time it is called Notice that the rules r 3 and r 4 in R ULES are the only rules that are recursiv e and that they are safe F urthermore it is imp ortan t that w e main tain a set seman tics while w e complete pro cessing these t w o rules b ecause w e need unique deriv ations of the meet irreducible elemen ts in inh  The mo died system R ULES no w b eha v es as exp ected and computes the correct supp ort for item sets in an y kno wledge base K  including our example kno wledge base T  It is imp ortan t to note here that the articial x w e ha v e prop osed ab o v e to sim ulate m ultiset op eration in set based framew ork through the use of genid  function is not necessary in man y systems including CORAL and RelationLog deductiv e database systems F or example CORAL supp orts m ultiset relations bags through multiset declaration Finally  grouping using set v alued terms are also allo w ed in CORAL and RelationLog 8 Wh y the System W orks The reader ma yha v e noticed that the R ULES system did not rely on generating candidate item sets in the w a y apriori has to Unlik e apriori it also do es not rely on a lev el wise computation Instead it uses a few critical observ ations that man y systems fail to notice 4  W e summarize belo w t w o critical observ ations that w e exploit in our system These observ ations follo w from the formal prop erties of transaction kno wledge bases that w e ha v e presen ted in section 5 and section 2  Item sets that are large can be computed from the database in t w o principal w a ys Either they app ear as transactions in the kno wledge base or they are computable from the transactions as follo ws Item sets in the transaction table that are not related b y a subset sup erset relationship in tersect with eac h other to pro duce in tersection virtual no des in the item set lattice meets These in tersection no des in turn in tersect un til they b ecome me et irr e ducible elements  Only a subset of these in tersection no des will b e large item sets These elemen ts can b e generated from the kno wledge base b y computing the least xp oin t of the pairwise in tersection of the elemen ts in the transaction kno wledge base Hence there is no need to generate 4 Zaki and sev eral others also ha v e made similar observ ations in their w ork on closed sets and concept lattices But there are imp ortan t dierences b et w een our observ ations and the manner in whic h w e utilize these observ ations His observ ations and tec hniques rely on a searc h based algorithm for CHARM 25 whic h is essen tial in order to compute the so called closed sets and th us ha v e to b e completely pro cedural an y candidate item sets articially as the w a y apriori do es  All other p ossible item sets are either not large item sets or are redundan t and can b e computed from the other large item sets found in the t w o t yp es of sets computed as ab o v e These observ ations can be in tuitiv ely understo o d from the example belo w Consider another kno wledge base T as sho wn in gure 9 tr ans  t 1 a   tr ans  t 1 b   tr ans  t 1 c   tr ans  t 2 a   tr ans  t 2 b   tr ans  t 2 d   tr ans  t 3 d   tr ans  t 3 e   tr ans  t 4 a   tr ans  t 4 c   tr ans  t 4 d   tr ans  t 5 a   tr ans  t 5 b   tr ans  t 5 c   Figure 9 A new kno wledge base T  Application of rules r 1 and r 3 will pro duce the gr oup and inh facts sho wn in gure 10 group  t 1  f a b c g   inh  f a b c g  40  group  t 2  f a b d g   inh  f a b d g  20  group  t 3  f d e g   inh  f d e g  20  group  t 4  f a c d g   inh  f a c d g  20  group  t 5  f a b c g   Figure 10 Execution trace of T  F ollo wing the con v en tions of lattice building in previous sections w e construct the K mapping in gure 11 for the item set lattice corresp onding to the example kno wledge base T in gure 9 Notice that in gure 11 no de ab 0 3 is an in tersection of no des abc 2 2 and abd 1 1 whic h inherits the transaction coun t of all its ancestors 2 from abc and 1 from abd  to record its total coun t as 3 Notice that its transaction coun t is still zero as it is a virtual no de not app earing in T and becauseitw as created through an in tersection Recall that the total coun t of a virtual no de cannot b e less than an yof its paren ts from whic hitw as created In fact it is alw a ys higher than its paren ts coun t refer to lemma 5.2  de 1 1 ac 0 3  acd 1 1 ad 0 2  0 3 c  0 2 d                     abc 2 2 abd 1 1 0 bd 0 1 cd 0 1 ab 0 3 null 5 0 0 3 b 0 1 e 0 4 a bc 2 acd 1 1 ad 0 2 0 3 c 0 2 d transaction nodes Intersection of transaction nodes Redundant nodes Meet irreducible element l-envelope 40 l-envelope 60 Figure 11 K mapping of the kno wledge base T  F urthermore the in tersection of the transaction no des abc 2 2  abd 1 1 and acd 1 1 could not cross the lev el of 2 item sets i.e ab  ac and ad  as in tersections alw a ys pro duce the meet alw a ys the largest p ossible common subset of 7 


the paren ts In particular the in tersection of these three no des cannot yield a 0 4  T o pro duce a 0 4  w e need to tak e another round of in tersection of the new in tersection no des pro duced in the rst round from T  It turns out that a 0 4 is a meet irreducible elemen tinthe K mapping of T  and hence no further in tersection in v olving a 0 4 is required In general w e need to compute the least xp oin t of the pairwise in tersection pro cess to compute all the in tersection no des and stop only when the set generated at the nal stage are all meet irreducible elemen ts Suc h a least xp oin t computation will only generate no des ab  ac  ad  a and d with abd and acd actually in the rst round In particular the least xp oin t will nev er compute the no des with b  c and bc as sho wn under the so called l-en v elop e in gure 11 Recall that ev ery no de under this en v elop e is a large item set Consequen tly  the l-en v elop e in this example assumes a 40 supp ort for large item sets But notice that the no de b 0 3 has an iden tical total coun t with one of its non-redundan t paren t ab 0 3  Hence b 0 3 is a redundan tnode and th us not computing or generating this no de do es not result in the loss of an y information b ecause w e can infer b 0 3 from ab 0 3  in case w e need to Since w e do not ha v e to generate the redundan t large item sets the R ULES systems w orks just ne But if w e wish to create all the large item sets similar to apriori w e m ust add another rule to ac hiev e this goal as w e do not explicitly compute them as a view d lar ge  The addition of the follo wing rule whic hessen tially copies the coun tof a large item set I to all its subsets X if X do es not exist as a large item set already  will do the tric k r 9  d lar g e  X C   lar g e  I C  X  I  lar g e  X C 2  There is a subtle issue that w e w ould lik e to poin t out here Consider the K mapping sho wn in gure 12 corresp onding to another kno wledge base T not sho wn from whic h w e ha v e remo v ed all the redundan t no des and sho wn only the transaction and in tersection virtual no des A t a rst glance one ma y think that it is p ossible to compute the total coun t of no des or item sets in a lev el wise manner and sa v e time b y not redoing certain computations F or example consider computing the total coun t of no de abc and recall that initially  the no de abc will read as abc 2 0  Assume that w e compute abc 2 3 from abc 2 0 and abcd 1 1 b y adding the total coun tof abc and abcd  Recall that abc 2 0 re\015ects the fact that abc app ears t wice in the database whereas abc 2 3 represen ts the fact that abc app ears t wice as a database transaction and app ears once 3-2=1 as a sub item in another transaction i.e abcd Let us assume for a momen t that w e compute the total coun t of ev ery no de in this fashion starting from no de abcd in a lev el wise fashion  compute the total coun tof eac hnode b y adding the total coun ts of all its paren ts No w for the third lev el from the top to compute the no de coun ts for ac  w e add the total coun tof its paren ts 3+2 giving 5 But as can b e seen from the gure coun t 5 is not really accurate This discrepancy resulted b ecause w e added total coun ts of paren ts instead of the transaction coun ts of ancestors to compute the total coun t of the no de ac  Notice that the coun t corresp onding to ac in abcd w as accoun ted for t wice in no de ac via t w o distinct branc hes as sho wn Similarly ifw e con tin ue with the same sc heme w e will compute a 0 11 for no de a instead of a 0 5 whic h in realit y is the correct total coun tfor a  1 ac 1 ac a 1 a 1 a 1 a 1 a 1                  abd 1 2 null 5 0 0 5 a 1 1 abcd abc 2 3 acd 1 2 ab 0 4 ac 0 4 ad 0 3 Figure 12 K mapping of a new database T sho wing incorrect inheritance of transaction coun t if total coun t of paren ts are used to compute total coun toflo w er lev el no des instead of transaction coun t Our rule system w ork ed correctly b ecause w e either inherited the transaction coun ts in the fr e q rules from a no de that is related via subset-sup erset relationship or b y rst generating the in tersection no de once initializing the transaction coun t to zero rule r 4   and using this in tersection no de to inherit the transaction coun ts whic h no w is in a subset-sup erset relationship with its ancestors Finally  w e added the transaction coun ts with a grouping op eration follo w ed b y a coun t op eration whic hb y denition is the total coun tfor an ynode 8.1 Breaking the Barrier of Pro ceduralit y W ew ould lik e to highligh t here that the three observ ations w e ha v e made early in this section w ere critical in dev eloping a mo del theoretic and declarativ ec haracterization of the large item set computing pro cess as it did not dep end on pro cedural concepts suc h as candidate generation The observ ation that w e only need to generate and test the intersection no des help ed us visualize the pro cess as a sort of Cartesian pro duct of the kno wledge base with itself and compare eac h transaction tuple with the other tuples in the kno wledge base and see if they w ere unrelated b y subsetsup erset relationships Recall that suc h pairs are p oten tial con tributors to an in tersection no de The least xp oin tof the in tersection pro cess help ed b ecause w e kno w that w e ha v e computed all the meet irreducible elemen ts b y no w and no other in tersection no des exists There are sev eral w orks that ha v ein v estigated the issue of declarativ e asso ciation rule mining using SQL 7  23  19  16  11 Most of these w orks sp ecially 23  19 attempt to sim ulate apriori in SQL giving rise to a complicated and a wkw ard metho d They do not exploit the inheren t declarativ e prop erties of transaction databases as w eha v e iden tied in this pap er The inheren t pro ceduralit yoftheir prop osed expressions app ears to be a ma jor b ottlenec k While it is ob viously p ossible to dev elop op erators that hide the complexit y of these expressions the system nonetheless is a wkw ard unnatural and pro cedural whic h ma y ha v e eciency related dra wbac ks F urthermore b y sp ecifying the seman tics in pro cedural terms they compromise the query optimization asp ects of the system The reason for this loss of opp ortunit y is the fact that the pro cess has already b een co ded in to the declarativit y of SQL and th us database system m ust no w consider only lo cal optimization 8 


of the query expression without ha ving the global view of the in ten tion There is a big c hance that the enco ded pro cedure ma y not b e the b est w a y to compute the rules dep ending on the database instance F urthermore as w e understand it their prop osals require p oten tially large n um ber of name generation for relations and attributes The names that are needed are usually database dep enden t and th us p ossibly cannot b e gathered at query time An additional pro cess needs to b e completed to gather those v ariables b efore actual computations can b egin 5  9 Optimization Issues While it w as in tellectually c hallenging to dev elop a declarativ e expression for asso ciation rule mining from deductiv e databases there are sev eral op en issues with great promises for resolution In the w orst case the least xp oin tneedsto generate n 2 tuples in the rst pass alone when the database size is n  Theoretically  this can happ en only when eac h transaction in the database pro duces an in tersection no de and when they are not related b y subset-sup erset relationship In the second pass w e need to do n 4 computations and so on The question no w is can w e a v oid generating and p erhaps scanning some of these com binations as they will not lead to useful in tersections F or example the no de b 0 3 in gure 11 is redundan t A signican t dierence with apriori lik e systems is that our system generates all the item sets top do wn in the lattice without taking their candidacy as a large item set in to consideration Apriori on the other hand do es not generate an y no de if their subsets are not large item sets themselv es and thereb y prunes a large set of no des Optimization tec hniques that exploit this so called an ti-monotonicit y prop ert y of item set lattices similar to apriori could mak e all the dierence in our setup The k ey issue w ould b e ho ww e push the selection threshold minim um supp ort inside the top do wn computation of the no des in the lattice in our metho d F or the momen t and for the sak e of this discussion let us consider a higher supp ort threshold of 60 for the database T of gure 9 No w the l-en v elop e will b e the one sho wn in ligh ter dashed lines in gure 11 and the no des under this line will b e the large item sets Notice that no ww eha v eto discard no des ad 2 0 and d 0 2 to o This raises the question is it p ossible to utilize the supp ort and condence thresholds pro vided in the query and prune candidates for in tersection an y further Ideas similar to magic sets transformation 3  24 ma y be b orro w ed to address this issue The only problem is that pruning of an y no de dep ends on its supp ort coun t whic h ma y come at a later stage By then all no des ma y already ha v e b een computed and th us pushing selection conditions inside aggregate op erator ma y b ecome non-trivial Sp ecial data structures and indexes ma y also aid in dev eloping faster metho ds to compute ecien t interse ction joins that w e ha v e utilized in this pap er W e lea v e these questions as op en issues that should be tak en up in the future F ortunately though there has been a v ast b o dy of researc h in optimizing Datalog programs including recursiv e programs suc h as the one w e ha v e used in this pap er and hence the new questions and researc h 5 Recall that their prop osal requires one to express the mining problem to the system using sev eral queries and up date statemen ts that utilizes information ab out the database con ten ts to ac hiev e its functionalit y  c hallenges that this prop osal raises for declarativ e mining ma y exploit some of these adv ances Needless to emphasize a declarativ e metho d preferably a formal one is desirable b ecause once w e understand the functioning of the system w e will then be able to select appropriate pro cedures dep ending on the instances to compute the seman tics of the program whic hw e kno wis in tended once w e establish the equiv alence of declarativ e and pro cedural seman tics of the system F ortunately  w e ha v e n umerous pro cedural metho ds for computing asso ciation rules whic h complemen t eac h other in terms of sp eed and database instances In fact that is what declarativ e systems or declarativit y buy us  a c hoice for the most ecien t and accurate pro cessing p ossible 10 Conclusion Our primary goal for this pap er has b een to demonstrate that mining asso ciation rules from an y rst-order kno wledge base is p ossible in a declarativ ew a y  without help from an y sp ecial to ols or mac hinery  and that w e can no wha v ea v ery in tuitiv e and simple program to do so W eha v esho wn that it is indeed p ossible to mine declarativ ekno wledge b y exploiting the existing mac hinery supp orted b ycon temp orary inference engines in programming languages e.g Prolog or kno wledge base systems e.g RelationLog XSB LD L  CORAL All w e require is that the engine b e able to supp ort set v alued terms grouping aggregate functions and set relational op erators for comparison functionalities whic hmostofthesesystemscurren tly supp ort W e ha v e also demonstrated that our formalism is grounded on a more mathematical foundation with formal prop erties on whic h the seman tics of the R ULES system rely  W e ha v e also raised sev eral op en issues related to eciency and query optimization whic h should b e our next order of business As future researc h w e plan to dev elop optimization tec hniques for mining queries that require non-trivial lo ok ahead and pruning tec hniques in aggregate functions The dev elopmen ts presen ted here also ha v e other signican t implications F or example it is no w p ossible to compute c hi square rules 4 using the building blo c ks pro vided b y our system Declarativ e computation of c hi square rules to our kno wledge has nev er b een attempted for the man y pro cedural concepts the computation of c hi square metho d relies on In a separate w ork 2 w e sho w that the coun ting metho d prop osed in this pap er can be eectiv ely utilized to generate the exp ectations needed in order to compute suc h rules rather easily  These are some of the issues w e plan to address in the near future The motiv ation imp ortance and the need for in tegrating data mining tec hnology with relational databases has b een addressed in sev eral articles suc h as 12  13 They con vincingly argue that without suc h in tegration data mining tec hnology ma y not nd itself in a viable p osition in the y ears to come T o b e a successful and feasible to ol for the analysis of business data in relational databases suc htec hnology m ust b e made a v ailable as part of database engines and as part of its declarativ e query language Our prop osal for declarativ e mining bears merit since it sheds ligh t on ho w rst order databases can be mined in a declarativ e and pro cedure indep enden t w a y so that the optimization issues can b e delegated to the underlying database engine Once suc h argumen ts are accepted sev eral systems 9 


related issues b ecome prime candidates for immediate atten tion F or example traditionally database systems supp orted declarativ e querying without the necessit y to care ab out the pro ceduralit y of the queries In this pap er w eha v e actually demonstrated that asso ciation rule mining can b e view ed as a Datalog query  It is immediate that a direct mapping from the Datalog expressions presen ted in this pap er to SQL can be dev elop ed with no problem at all W e can then rely on ecien t database pro cessing of the query in an optimized fashion Hence w ecomeclose to the essence of the visions expressed b y the leading database researc hers and practioners 12  References 1 Rak esh Agra w al and Ramakrishnan Srik an t F ast algorithms for mining asso ciation rules in large databases In VLDB  pages 487{499 1994 2 Anon ymous A declarativ e metho d for mining c hisquare rules from deductiv e databases T ec hnical rep ort Departmen t of Computer Science Anon ymous Univ ersit y USA F ebruary 2001 3 C Beeri and R Ramakrishnan On the po w er of magic In Pr o c e e dings of the 6th A CM Symp osium on Principles of Datab ase Systems  pages 269{283 1987 4 Sergey Brin Ra jeev Mot w ani and Craig Silv erstein Bey ond mark et bask ets Generalizing asso ciation rules to correlations In Pr o c A CM SIGMOD  pages 265 276 1997 5 D Chimen ti et al The LD L system protot yp e IEEE Journal on Data and Know le dge Engine ering  2\(1 90 1990 6 Jia w ei Han Jian P ei and Yiw en Yin Mining frequen t patterns without candidate generation In Pr o c A CM SIGMOD  pages 1{12 2000 7 Marcel Holsheimer Martin L Kersten Heikki Mannila and Hann uT oiv onen A p ersp ectiv e on databases and data mining In Pr o c of the sixth A CM SIGKDD Intl Conf  pages 150{155 Mon treal Queb ec 1995 8 Flip Korn Alexandros Labrinidis Y annis Kotidis and Christos F aloutsos Ratio rules A new paradigm for fast quan tiable data mining In Pr o c of 24th VLDB  pages 582{593 1998 9 Brian Len t Arun N Sw ami and Jennifer Widom Clustering asso ciation rules In Pr o c of the 3th ICDE  pages 220{231 1997 10 Mengc hi Liu Relationlog At yp ed extension to datalog with sets and tuples In John Llo yd editor Pr oc e e dings of the 12th International L o gic Pr o gr amming Symp osium  pages 83{97 P ortland Oregon Decem ber 1995 MIT Press 11 Rosa Meo Giusepp e Psaila and Stefano Ceri An extension to SQL for mining asso ciation rules Data Mining and Know le dge Disc overy  2\(2 1998 12 Amir Netz Sura jit Chaudh uri Je Bernhardt and Usama M F a yy ad In tegration of data mining with database tec hnology  In Pr o c e e dings of 26th VLDB  pages 719{722 2000 13 Amir Netz Sura jit Chaudh uri Usama M F a yy ad and Je Bernhardt In tegrating data mining with SQL databases In IEEE ICDE  2001 14 Ra ymond T Ng Laks V S Lakshmanan Jia w ei Han and Alex P ang Exploratory mining and pruning optimizations of constrained asso ciation rules In Pr o c A CM SIGMOD  pages 13{24 1998 15 Jong So o P ark Ming-Sy an Chen and Philip S Y u An eectiv e hash based algorithm for mining asso ciation rules In Pr o c A CM SIGMOD  pages 175{186 1995 16 Karthic k Ra jamani Alan Co x Bala Iy er and A tul Chadha Ecien t mining for asso ciation rules with relational database systems In Pr o c e e dings of the International Datab ase Engine ering and Applic ations Symp osium  pages 148{155 1999 17 R Ramakrishnan D Sriv asta v a and S Sudarshan CORAL  Con trol Relations and Logic In Pr o c of 18th VLDB Confer enc e  pages 238{250 1992 18 Konstan tinos F Sagonas T errance Swift and Da vid Scott W arren XSB as an ecien t deductiv e database engine In Pr o c of the A CM SIGMOD Intl Conf  pages 442{453 1994 19 Sunita Sara w agi Shib y Thomas and Rak esh Agra w al In tegrating mining with relational database systems Alternativ es and implications In Pr o c A CM SIGMOD  pages 343{354 1998 20 Ashok a Sa v asere Edw ard Omiecinski and Shamk an tB Nav athe An ecien t algorithm for mining asso ciation rules in large databases In Pr o c of 21th VLDB  pages 432{444 1995 21 Pradeep Sheno y  Ja y an t R Haritsa S Sudarshan Gaura v Bhalotia Ma y ank Ba w a and Dev a vrat Shah T urb o-c harging v ertical mining of large databases In A CM SIGMOD  pages 22{33 2000 22 Abraham Silb ersc hatz Henry F Korth and S Sudarshan Datab ase System Conc epts  McGra w-Hill third edition 1996 23 Shib y Thomas and Sunita Sara w agi Mining generalized asso ciation rules and sequen tial patterns using SQL queries In KDD  pages 344{348 1998 24 J D Ullman Principles of Datab ase and Know le dgeb ase Systems Part I II  Computer Science Press 1988 25 Mohammed J Zaki Generating non-redundan t association rules In Pr o c of the 6th A CM SIGKDD Intl Conf  Boston MA August 2000 1 0 


OM OM 006 OD8 01 012 014 016 018 02 022 False alarm demity Figure 9 Percentage of tracks lost within 200 seconds using three-scan assignment with PD  0.9 TI  O.ls Figure 11 T2  1.9s and T  Is ij  20 and 0  0.1 24 1 22  20  E fls 0  8l 16 0 n 14  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 T1/12 PD Average track life of three-scan assignment with PD varying TI  0-ls T2  1.9s T  Is X  0.02 ij LO and   0.1 mareuvenng index Figure 12 Percentage of lost tracks of 4-D assipment in 200 seconds with maneuvering index varying X  0.01 Ti  0.1 T2  1.9s and T  IS PD  0.98 Figure 10 Percentage of lost tracks of 4-D assignment in 200 SeoDllCls with TI and T2 varying PD  0.98 X  0.02 q 20 and 0  0.1 4-1607 


Figure 13 Average gate size for Kalman filter Figure is relative as compared to nq and curves are parametrized by ij/r with unit-time between each pair of samples 1.2 Iy I 1.1 0.5 I A CRLB for he unifm samiina I  0.4 0.35 d 3 03 i7 3 0.25 0 0.M 0.04 0.06 008 0.1 0.12 0.14 0.16 0.18 0.2 False A!am DemW V I    Figure 14 CramerRao Lower Boundfor Mean Square Error of uniform and nonuniform sampling schemes with Ti  O.ls T2  1.9s T  IS PD  0.9 ij  5 and U  0.25 1 unifon sampling r-ls ked i non-uniform sampling loge inlewi I ti non-uniform sampling shod interva I 0.9 0.8 I Figure 15 MSE comparison of three-scan assignment with Ti and T2 varying I'D  1 X  0.01 ij  20 and U  0.1 4-1608 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


