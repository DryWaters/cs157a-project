Ad Hoc Association Rule Mining as SQL3 Queries  ansaction Hasan M Jamil Member ACM and IEEE j amilOacm org tab1 Abstract Although there have been several encouraging attempts at developing methods for data mining using SQL simplicity and eficiency still remain significant impediments for further development In this paper we propose a significantly new approach and show that any object relational database can be mined for association rules without any restructuring or preprocessing using only basic SQL3 constructs 
and functions and hence no additional machineries are necessary In particular we show that the cost of computing association rules for a given database does not depend on support and confidence thresholds More precisely the set of large items can be computed using one simple join query and an aggregation once the set of all possible meets least fixpoint of item set patterns in the input table is known The principal focus of this paper is to demonstrate that several SQLS expressions exists for the mining of association rules 1 Introduction The motivation 
importance and the need for integrating data mining with relational databases has been addressed in several articles such as 3 41 They convincingly argue that without such integration data mining technology may not find itself in a viable position in the years to come To be a successful and feasible tool for the analysis of business data in relational databases such technology must be made available as part of database engines and as part of its declarative query language Some of the benefits of using existing relational machinery may include opportunity for query optimization declarative language support 
selective mining, mining from non-transactional databases and so on From these standpoints it appears that research into data mining using SQL or SQL-like languages bear merit and warrant attention In this article we demonstrate that there is a simpler SQLS expression for association rule mining that does not require candidate generation such as in 6 51 or any implementation of new specialized operators such as in 2 We also show that we can simply add a mine by operator to SQL with an optional having clause to facilitate filtering of 
unwanted derivations in a fashion similar to the cube by operator proposed for data warehousing applications The striking feature of our proposal is that we can exploit the vast array of optimization techniques that already exists and possibly develop newer ones for better performance These are some of the advantages of our proposal over previous proposals in addition to its simplicity and intuitive appeal 0-7695-1 119-8/01 17.00 0 2001 IEEE 609 2 A Set Theoretic Perspective of Data Mining In order to introduce a few ideas let us consider a database called the transaction table 
T as shown in figure 1 Based on the traditional understanding of association rule mining we expect to obtain the 223non-redundant\222\222 large item set table Ltable and the rules table \(r-table shown in figure 1 below from the table T once we set the support threshold at 25 The reasoning process of reaching to the large item set and rules tables can be explained as follows le Items a b b f b f a b b e d f d C C C 1-table r-table 0.29 0.66 b f 
0.29 0.40 b,c a 0.29 0.66 association rules table Figure 1 Source transaction database T is shown as t-table large item set table as I-table and finally the association rules as r-table We can think of T as the set of complex tuples shown in nested table n-table in figure 2 once we nest the items on transaction numbers If we use a group by on the Items column and count the transactions we will compute the frequency table f-table that will show how many times a 
single item set pattern appears in the transaction table \(t-table in figure 1 Then let us assume that we took a cross product of the frequency table with itself and selected the rows for which 0 the Items column in the first table is a proper subset of the Items column in the second table and finally projected out the Items column of the first table and Support column of the second table\222 or 221This will give us  b j 1  and  d 1  


0 the Items columns are not subset of one another and we took the intersection of the Items of both the tables created a new relation int-table called the intersection table with distinct tuples of such Items with Support 0 and then finally computed the support counts as explained in step 1 now with the frequency table and intersection table This will give us the inheritance table i-table as shown in figure 2 Finally if we took a union of the frequency table and the inheritance table and then do a group by on the Items column and sum the Support column we would obtain the count table c-table of figure 2 n-table Tranid Items Ia,b,c t5 e d,f t7 nested table 1 frequency table ifable 1 inheritance table c-table Support 2 1 3 2 1 1 5 3 2 able Figure 2 n-table t-table after nesting on Tranid f-table n-table after grouping on Items and counting i-table gener ated from f-table and c-table grouping on Items and sum on Support on the union of i-table and f-table The entire process of large item set and association rule generation can be conveniently explained using the so called item set lattice found in the literature once we enhance it with some additional information Let us consider placing the transactions with item set I appearing in the frequency table with their support count t as a node in such a lattice as shown in figure 3 Notice that in the lattice each node is represented as I where it denotes the fact that I appears in exactly t transactions in the source table and that I also appears as a subset of other transactions n number of times such that c  n  t t is called the transaction count or frequency count and c is called the total count of item set I In the lattice of figure 3 the nodes marked with a solid rectangle are the nodes or the item sets in T nodes identified with dotted rectangles are called the intersection nodes or the virtual nodes as they do not appears in T explicitly and the nodes marked with dotted ellipses are redundant The solid ellipse nodes are redundant as well as 2The result of this will be tuple  b c 3   b 5  and  f},3  in this example Note that the intersection table will contain the tuples  b,c},O   b},O  and  f},o  and that these patterns are not part of the frequency table in figure 2 The union of step 1 and step 2 processed with the intersection table will now produce the inheritance table in figure 2 Figure 3 Lattice representation of item sets in the database T they are not large item sets The nodes below the dotted line called the large item set envelope or 1-envelope are the large item sets Notice that the node bc is a large item set but is not a member of T while bc f  df and be are yet they are not included in the set of large item sets of T We are assuming here a support threshold of 25 So basically we would like to compute only the nodes abc bc 6 f  6 d and f from T This set is identified by the sandwich formed by the I-envelope and the zero-envelope or the z-envelope that marks the lowest level nodes in the lattice The concept of node redundancy and consequently large item set and rule redundancy are formalized as follows Definition 2.1 Let 7 be a transaction table over item sets 2 I Z be an item set and n be a positive integer Also let n represent the frequency of the item set I with which it appears in 7 Then the pair I n is called a frequent item set, and the pair is called a large item set if n 2 S where 6 is the minimum support threshold If for any large item set I its frequency n can be determined from other large item sets, then I is redundant Formally Definition 2.2 Redundancy'of Large Item Sets Let L be a set of large item sets of tuples of the form Iu,n such that Vz,y\(s  Iz,n  Iy,ny E LA I  Iy 3 n  ny and let U  Iu nu be such a tuple Then U is redundant in L if 3v\(v E L v  I n I C_ I  nu  nu The importance of the definition 2.2 may be highlighted as follows For any given set of large item sets L and an element 1  Ii,nl E L If is unique in L The implication of anti monotonicity is that for any other w  I n E L such that If C I holds n 5 nf because an item set cannot appear in a transaction database less number of times than any of its supersets But the important case is when n  n1 yet If c I This implies that If never appears in a transaction alone, i.e it always appeared with other items It also implies for all large item sets s  Is,ns E L of I such that I 2 I if it exists nu  ns too As if not nf should be different than n which it is not according to our assumption It also implies that If is not involved in any other sub-superset relationship chains other that I Because redundant large item sets exists traditional mining algorithms such as apriori generates all possible rules some of which are essentially redundant For example let a  b\(%,y and ab  c be two rules discovered from a any tracsaction table Ff where sx and m represent respectively the frequency of an item set X in 610 


the item set table and the number of transactions in the item set table S Then it is also the case that the set of discovered rules contains another rule transitive implication a  bc F Notice that this last rule is a logical consequence of the first two rules that can be derived using the following inference rule where X Y Z C Z are item sets x  Y y x U y  z\(sxumyuz   x  Y U Z sxumyuz sx"~u sx Written differently using only symbols for support 6 and confidence q the inference rule reads as follows x U Y  Z\(62,?l2 x  Y\(61.171 x  Y U Z\(S2,q  72 Based on these observations we take the position and claim that the table I-table shown in figure 1 is an information equivalent table of the large item set table produced by apriori on T which essentially means that these tables faithfully imply one another assuming identical support thresholds 3 SQL3 Expressions for Computing Association Rules Now that we have explained what non-redundant large item sets and association rules mean in our framework we are ready to discuss cornputing them using SQL The reader may recall from our discussion in the previous section that we have already given this problem a relational face by presenting thein in terms of nested tables We will now present a set of SQL3 sentences to compute the tables we have discussed earlier We must mention here that it is possible to evaluate the final table in figure 1 by mimicking the process using a lesser number of expressions than what we present below But we prefer to include them all separately for the sake of clarity For the purpose of this discussion we will assume that several functions that we are going to use in our expressions are available in some SQL3 implementation such as Oracle DB2 or Informix Recall that SQL3 standard requires or implies that in some form or other these functions are supported3 In particular we have used a nest by clause that functions like a group by on the listed attributes but returns a nested relation as opposed to a first normal form relation returned by group by We have also assumed that SQL3 can perform group by on nested columns columns with set values Finally we have also used set comparators in where clause and set functions such as intersect and setminus in the select clause which we think are natural additions to SQL3 once nested tuples are supported As we have mentioned before we have for now used user defined functions UDFs by treating set of items as a string of labels to implement these features in Oracle create view n-table as select Tranid Items from 1-table nest by Tranid create view f-table as select Items count as Support from n-table group by Items 3Although some of these functions are not supported right now once they are we will be in a better shape Until then we can use PL/SQL codes to realize these functions The two view definitions above prepare any first normal form transaction table for the mining process Note that these view definitions act as idempotent functions on their source So redoing them does not harm the process if the source table is already in one of these forms These two views compute the n-table and the f-table of figure 2 Before we can compute the i-table we need to know what nodes in the imaginary lattice will inherit transaction counts from some of the transaction nodes in the lattice  Support value of Items in the f-table Recall that nodes that are subset of another node in the lattice inherit the transaction count of the superset node towards its total count We also know that only those non-redundant nodes which appear in the f-table or are in the least fixpoint of the nodes in f-table will inherit them So we compute first the set of intersection nodes implied by f-table using the newly proposed SQL3 create view recursive statement as follows create view recursive int-table as select distinct intersect 1 Items p Items 0 from f-table as t f-table as p where t.Items  p.llems and p.Items  t.Items and not exists select  from f-table as f where f Items  intersect t.Items p Items union select distinct intersect 1 Ite.ms p.Items 0 from int-table as 1 int-table as p where t.ltems  p.ltems and p.1tem.s  t.Items and not exists select  from f-table as f where f.Items  intersect\(t.ltems, p.ltems We would like to mention here again that we have implemented this feature again using PL/SQL in Oracle Notice that we did not list the int-table we create below in figure 1 or 2 because it is regarded as a transient table needed for the computation of i-table It is really important that we create only distinct set of intersection items and only those ones that do not appear in the f-table for the purpose of accuracy in support counting Take for example three transactions in a new frequency table f'-table represented as abck bcd bcfd bc Assume that we compute the set of intersections of the entries in this table If we do not guard against the cautions we have mentioned we will produce the set bc bc bc using the view expression for int-table  which is not desirable Because these three will inherit Support from abck bcd bcfd giving a total count of 10 i.e bcio The correct total count should have been bc If we just ensure the uniqueness of a newly generated item set but not its absence in the f-table through meet computation we still derive bc instead of an empty set which is incorrect This means that not including the following condition or its equivalent in the above SQL expression will be a serious mistake not exists select  from f-table as f where f.Items  intersect\(t.Items p.Items Once we have computed the int-table the rest of the task is pretty simple The i-table view is computed by copying the Support of a tuple in f-table for any tuple in the collection of f-table and int-table which is a subset of the tuple in the f-table Intuitively, these are the nodes that need to inherit the transaction counts of their ancestors in f-table 611 


create view i-table as select t. Items p.Support from f-table as p select  from f-table union select  from int-table as t where t.Items C p.Items From the i-table a simple grouping and sum operation as shown below will give us the count table or the c-table of figure 2 create view c-table as select t. Items sum t.Support as Support from select  from f-table union select  from i-table as t group by t.ftems The large item sets of 1-table in figure 1 can now be generated by just selecting on the c-table tuples as shown next Notice that we could have combined this step with the c-table expression above with the help of a having clause create view l-table as select Items Support from c-table where Support 2 hm Finally the non-redundant association rules of figure 1 are computed using the r-table view below The functionality of this view can be explained as follows Two item sets ftems and v[fte,ms in a pair of tuple U and in the I-table implies an association rule of the form u[ftems  v[ftems  u[~tems]\(v[~~pport M only if ul~ternsl c w11temsl and there does not exist any intervening item set z in the 1-table such that z is a superset of Items and is a subset of ltems as well In other words in the lattice Items is one of the immediate ancestors of Items In addition the ratio of the Supports i.e sa must be at least equal to the minimum confidence threshold qm create view r-table as select a Items c Items\\a Items c.Support from 1-table as a 1-table as c where a.ftems c c.Items and c.Items/a.Items 2 qm c Support/a Support and not exists select Items from 1-table as i where a Items c a Items and a Items C c.Ztems The readers may verify that these are the only 223generic\224 SQL3 expressions or their equivalent that are needed to mine any relational database assuming proper name adapta tions for tables and columns The essence of this relational interpretation of the problem of mining as demonstrated by the SQL3 expressions above is that we do not necessarily need to think procedurally  in terms of number of iterations, can didate generation, space time overhead and so on Instead we can now express our mining problems on any relational database in declarative ways and leave the optimization is sues with the system and let the system process the query using the best available method to it recognizing the fact that depending on the instance of the database the choice of best methods may now vary widely In another recent research l we have demonstrated that to implement apriori in SQL3, one need not be constrained by the complexity and availability of combination and GatherJoin 7 61 operators Without much discussion we present below the set of SQL3 queries see l for details that correctly implement apriori Again a comparison with the works in 6 21 will expose the strength and simplicity of our proposal create view f-table as select Items count\(Items as Support from t-table group by Items create sequence seq increment by 1 start with 1 create view recursive 1-table as select Items sum\(Support as Support from flatten\(se1ect sub Items  1 as Items Support from f-table group by Items having sum\(Support  6 union all select t Items sum t.Support as Support from f-table as U flatten distinct\(se1ect sub\(f.ftenis LItems i Degvee as Items select Seq Nextval as Degree from itera.tion as i and 1.Items c f.ftems as t from f-table as f 1-table as 1 where sizeof\(l.Items  i Degree  1 where t.Items 5 u.Items group by t.Items having sum\(t.Support  bm References Hasan M Jamil On the equivalence of top-down and bottom-up data mining in relational databases In Da WaK Munich, Germany 2001 Rosa Meo Giuseppe Psaila and Stefan0 Ceri An extension to SQL for mining association rules DMKD Amir Netz Surajit Chaudhuri Jeff Bernhardt and Usama M Fayyad Integration of data mining with database technology In Proceedings of 26th VLDB pages Amir Netz Surajit Chaudhuri Usama M Fayyad and Jeff Bernhardt Integrating data mining with SQL databases In IEEE ICDE 2001 Karthick Rajamani Alan Cox Bala Iyer and Atul Chadha Efficient mining for association rules with relational database systems In IDEAS pages 148-155 1999 Sunita Sarawagi Shiby Thomas and Rakesh Agrawal Integrating mining with relational database systems Alternatives and implications In Proc A CM SIGMOD pages 343-354 1998 Shiby Thomas and Sunita Sarawagi Mining generalized association rules and sequential patterns using SQL queries In KDD pages 344-348 1998 2\(2 195-224 1998 719-722 2000 612 


Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 descendant cell cg\222 of cg is processed from this expansion, derive LiveSet\(c,\222 from LiveSet\(c It is worth mentioning that in our study, the probe cells in LiveSet\(c are assumed to be stored in the ascending order according to the measures Thus if the measure of one probe cell cp can not satisfy the constraint Cgrad\(cg,cp\then all the probe cells following it will not satisfy the gradient constraints function and hence can be pruned from LiveSet\(c In the case that the LiveSet\(c is very large set the hash table method can be used to fast access the Liveset cg  5 Experiments In this section we present our experimental results on the performance \(in terms of time of mining constrained cube gradient from a data cube All experiments are conducted on a PC platform with an Intel PentiumIII 500M CPU 218M RAM and Windows 2000 OS Two mining algorithms are compared in our experiments The first algorithm is eLiveSet that is constructed on a condensed cube that is computed by the BU-BST algorithm. The other is LiveSet that is constructed on a general cube that is computed by BUC algorithm. The performance of mining cube gradient from cube with the two algorithms is compared All experiments are performed using synthetic algorithmically generated\datasets of 1M \(=1,024\tuples that is uniformly distributed and the number of dimension is set to 9 and the cardinality of all attributes is set to 100. The aggregate function used in the cube computation algorithm is SUM function. The performance of the algorithms with different constraints is shown in Figl In Figl the significance constraints C,,,>O and the gradient constraints Cd=m\(c cp The value of the non value dimension in probe constraint C is set to 1,50 The number of non-* value dimension is varied from 1 to 9 The non value dimensions in cprb are fixed on the first k dimensions 1%9 respectively The larger the number of non value dimensions, the smaller number of probe cells that satisfy the cprb In Figl the scalability of the two algorithms over probe cells is shown The runtime of the two algorithms is less with the increase of the number of non value dimensions in C because the number of non  value dimensions in Cp,,,\222 larger and the probe cells satisfied Cph are larger However eLiveSet algorithm is faster than LiveSet algorithm The reason is that many cells are condensed into the condensed cube and the search space of the cells including the probe cells and the gradient cells to be handled reduces dramatically in eLiveSet algorithm Similarly eLiveSet algorithm is also faster than LiveSet with Cslg and Cgd Due to the limitation of space the corresponding figures of results are not given P 1300.00 P 1100.00 2 2 900.00 2 700.00 500.00 a2 3 300.00 100.00 123456789 The number of non value dimension of Cprb Fig 1 The scalability with probe cells 6 Conclusions In this paper we study the problem of mining constrained cube gradient using the condensed cube approach A new algorithm, eLiveSet is developed by the extension of the existing efficient algorithm LiveSet The experiment results show eLiveSet is efficient and scalable There are many interesting issues in our future work for example, instead of computing the cube gradient on a non materialized cube, as shown in this paper, we can compute the cube gradient on a materialized cube since a materialized cube is always pre-computed in a datawarehouse environment In addition, further enhancing the performance of eLiveSet algorithm is also our interested topic Acknowledgements This paper is supported by the e-government project of National Science and Technology Ministry of China No 2001BA110B01 References K.Beyer and R.Ramakrishnan Bottom-up computation of sparse and iceberg cubes ACM SIGMOD 1999 J.Han J.Pei G.Dong and K.Wang Efficient computation of iceberg cubes with complex measures ACM SIGMOD 2001 T.Imielinski L.Khachiyan and A.Abdulghani Cubegrades Generalization Association Rules Tech Rep Dept Computer Science Rutgers University Aug. 2000 Wei Wang, Jianlin Feng Hongjun Lu and Jeffrey Xu Yu Condensed Cube An Effective Approach to Reducing Data Cube Size IEEE ICDE 2002 Jim Gray Adam Bosworth Andrew Layman and Hamid Pirahesh Data Cube A Relational Aggregation Operator Generalizing Group-By, Cross Tab, and Sub-Totals. IEEE ICDE 1996 1032 


Proceedmgs of the First International Conference on Machme Learning and Cybernetics Beijing 4-5 November 2002 161 171 81 191 Guozhu Dong, Jiawei Han, Joyee Lam Jean Pei and Ke Wang Mining Multi-Dimensional Constrained Gradients in Data Cubes VLDB 2001 R.Ng L.V.S.Lakshmanan J.Han and A.Pang Exploratory mining and pruning optimizations of constraned association rules. ACM SIGMOD 1998 R.Agrawal T.Imielinski and ASwami Fast algorithms for mining association ruels. VLDB 1994 Jiawei Han, Jianyong Wang Guozhu Dong, Jian Pei Ke Wang CubeExplorer online exploration of data cube ACM SIGMOD 2002 1033 


4.1 Simulation model In the section we evaluate the performances of the three algorithms including BASIC 1121 Cumulate 12 and GMAR on a DELL PowerEdge 4400 Server with Intel" Xeon Processor and 756MB main memory running Windows 2000 server All the experimental data are generated randomly and stored on a local 30GB SCSI Disk Ultra 160 with a RAID controller The relative simulation parameters are shown in Table 1 To make our data representative, we generate two types of databases in the experiments i.e DENSE databases and SPARSE databases. Each item in the DENSE database is randomly generated from a pool P called potentially frequent itemsets with size 300 while each item in the SPARSE database is randomly generated from a pool N i.e the set of all the items\with size 1000 Since the items in the DENSE database are more clustered than those in the SPARSE database, larger frequent itemsets will probably be produced in the DENSE database for the same minimum support Besides we use the notations T for average number of items per transaction I for average number of items in a frequent itemset, and D for number of transactions. For example the experiment labeled with 7lOB.DIK represents the simulation environment with IO items on the average per transaction 3 items on the average in a frequent itemset, and 1000 transactions in total Table 1 Simulation parameters with default values ID INumber of transactions 1000-500,000 7 INumber ofthe items per transaction 15-15 P INumber of potentially frequent 1300  litemsets I I Number of the items in a frequent 12-5 4.2 Experimental results Experiment 1 In the experiment we explore the execution time of BASIC Cumulate and GMAR algorithms for the environment 7lO.L3,DIK under different minimum support and minimum confidence pairs as shown in Figure 9 In the figure, we find that our algorithm GMAR is almost faster 2-16 times than BASIC especially for larger minimum support and minimum confidence pairs whereas Cumulate is only faster 1.3-1.5 times than BASIC although R Srikant and R Agrawal claimed that Cumulate runs faster 2-5 times than BASIC 1121 In general the larger the minimum support and minimum confidence pair is the faster the execution time of the three algorithms becomes To be fair to all algorithms, we have added the extra time of generating original frequent itemsets and association rules for GMAR However the time is helow 1 of total execution time thus we do not show it in the figure h n  B a 3 i.m.26 i.ma 1.~1.3 i.mm 1.740.3 I.w Figure 9 Execution time for different pairs minimum support  Confidence Experiment 2 In the experiment we extend Experiment 1 by fixing the minimum support IS and observe their variations For the minimum support 1.5 all the algorithms except GMAR are not sensitive to the changes of the minimum confidences as shown in Figure IO The reason is that larger minimum confidences will make GMAR prune more irrelevant rules. Nevertheless, GMAR is still in the first rank D  2 4M cm 22 CUlIIUlStt  gm BASIC 8 Irn 0 0 0.26 0.28 0.3 0.32 0.34 0.3 minimum confidence Figure 10 Execution time for different minimum confidences Experiment 3 In the experiment, we explore the execution time of the three algorithms for the environment ZlO.I3.DxK i.e different numbers of transactions generated in the SPARSE database and in the DENSE database as shown in Figure Il.\(a and b respectively. Both cases have the same minimum confidence 0.3 However to get comparable number of frequent itemsets, we set a smaller minimum support 1 in the SPARSE case and a larger 233 


minimum support 2 in the DENSE case As expected GMAR is still the hest one among them in the SPARSE and DENSE case especially when there are a huge amount of transactions From the both cases we find that much more frequent itemsets are generated in the DENSE database than in SPARSE database so that BASIC and Cumulate are not practicable candidates there B Ixa 8 Imo 222ixa 80 0 Imo 3033 m m loo30 number of transactions Figure 11 a Execution time for different numbers of transactions in the SPARSE database cumh?t  6 loo30  rma 2230 lorn m 5m 7cw IwD3 number of transactions Figure 1 l.\(b Execution time for different numbers of transactions in the DENSE database 5 Conclusions In the paper we try to find the association rules between the items at different levels in the taxonomy tree under the assumption that original frequent itemsets and association rules have already been generated beforehand The primary challenge is how to make use of the original frequent itemsets and association rules to directly generate new generalized association rules rather than rescanning the database In the proposed algorithm GMAR we use join methods and pruning techniques to generate new generalized association rules Through several comprehensive experiments we find that the GMAR algorithm is much better than BASIC and Cumulate algorithms since it generates fewer candidate itemsets and furthermore prunes a large amount of irrelevant rules based on the minimum confidence 6 Acknowledgments This research was supported in part by the National Science Council Taiwan under contract NSC-90-22 13-E-224-026 7 References I R Agrawal T Imielinski and A Swami 223Mining Association Rules between Sets of Items in Large Databases,\224 Pmc ACM International Conference on Mananement of Data  1993 pp 207-216 121 R Anrawal and R Srikant 223Fast Alaorithms for Mmine Adsociation Rules,\224 Pmc 2Vh Internationaiconference on Ve Large Data Bases 1994 pp 487-499 131 Yong-Jian Fu 223Data Mining,\224 IEEE Potentials Yol 16 No 4 1997 pp 18-20 141 Jia-Wei Han and Yong-Jian Fu 223Mining Multiplelevel Association Rules in Large Databases,\224 IEEE Transactions on Knowledge and Data Engineering Yo 11 No 5 1999 pp 798-805 5 Iia-Wei Han and Micheline Kamber Data Mining Concepts and Techniques Morgan Kaufmann Publishers 2001 6 Iia-Wei Han lian Pei and Yi-Wen Yin 223Mining Frequent Patterns without Candidate Generation,\224 Pmc ACM International Conference on Management o Data 2000 pp 1-12 7 Mon-Fong Jim Shian-Shyong Tseng and Shan-Yi Lia 223Data Types Generalization for Data Mining Algorithms,\224 Pmc IEEE International Conference on stems Man and Cybernetics 1999 pp 928-933 8 Bing Liu Wynne Hsu and Yi-Ming Ma 223Minin Association Rules with Multiple Minimum Supports,\224 Pmc 5 ACM International Conference on Knowledne Discovery and B DataMining 1999 pp 337-341 191 J S Park M S Cben and P S Yu 223An Effective  H&h-based Algorithm for Mining Association Rules,\224 Pmc ACM Internotional Conjerence on Mamgement o Data 1995 pp 175-186 IO A Savasere E Omiecinski, and S Navathe 223An Efficient Algorithm for Mining Association Rules in Large Databases,\224 P 21\224 lnternationk Conference on Very La Data Bases 1995 pp 432-443 Ill Pradeep Shenoy layant Haritsa S Sudarshan Gaurav Bhalotia, Mayank Bawa and Devavrat Shah 223Turbo-charging Vertical Mining of Large Databases,\224 Pmc ACM International Conference on Management of Data 2000 pp 22-33 I21 R Srikant and R Agrawal 223Mining Generalized Association Rules,\224 Pme 21\224 International Conference on Very Large DataBases 1995 pp 407-419 I31 S Y Sung K Wag and L. Chua 223Data Mining in a Large Database Environment,\224 Pmc IEEE International Conference on Systems Man and Cybernetics 1996 pp 988-993 I41 H Toivonen 223Sampling Large Databases for Association Rules,\224 Pmc 2T\221 International Conference on Very Large Data Bases 1996 pp 134-145 I51 Ming-Cheng Tseng Wen-Yang Lin and Been-Chian Chien 223Maintenance of Generalized  Association Rules with Multiple Minimum Supports,\224 Pmc 9th IFSA World Congress and 20th NAFIPSInternational Conference 2001 pp 1294-1299 234 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


