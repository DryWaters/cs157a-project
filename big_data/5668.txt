Clustering Association Rules Brian Lentt Arun Swami 247 Jennifer Widom t Department of Computer Science Stanford University Stanford, CA 94305 lent widom}@cs.stanford.edu 247 arunecs  s tanf ord edu Abstract We consider the problem of clustering two-dimensional as sociation rules in large databases We present a geometric based algorithm, BitOp for performing the clustering, em bedded within an association rule clustering system ARCS Association rule clustering is useful when the user desires to segment the datu We measure the quality of the segment ation generated by ARCS using the Minimum 
Description Length MDL principle of encoding the clusters on several databases including noise and errors Scale-up experiments show that ARCS using the BitOp algorithm scales linearly with the amount of data 1 Introduction Data mining or the efficient discovery of interesting pat terns from large collections of data, has been recognized as an important area of database research The most commonly sought patterns are association rules as introduced in 3 Intuitively an association rule identifies a frequently occur ing pattern of information in a database Consider a super market database where 
the set of items purchased by a single customer at the check-stand is recorded as a transaction The supermarket owners may be interested in finding 223asso ciations\224 among the items purchased together at the check stand An example of such an association is that if a cus tomer buys bread and butter then it is likely he will buy milk Given a set of transactions where each transaction is a set of items an association rule is an expression X j Y where k\222 and y are sets of items The meaning of this 
rule is trans actions that contain the items in X also tend to contain the items in Y Generally, association rules are used in conjunction with transaction or basket type data, but their use is not limited to this domain When dealing with customer demographic data for example, the database schema defines a fixed set This research was initiated by the first two authors while at Silicon Graphics Computer Systems Inc Continuing support for the first author has been provided by a graduate fellowship of the Department of Defense Office of Naval Research of attributes for each record, and each customer is repres 
ented by one record Each record contains a value for each attribute, i.e attribute  value By replacing the sets of items in the traditional definition of association rules with conjunctions of attribute  value equalities we can gen eralize the above definition of association rules to include this type of non-transactional data For example age  40 A Salary  50,000 When mining association rules from this type of non transactional data we may find hundreds or thousands of rules 9 corresponding to specific attribute 
values We therefore introduce a clustered association rule as a rule that is formed by combining similar, \223adjacent\224 association rules to form a few general rules Instead of a set of attribute  value equalities for clustered rules we have a set of value ranges using inequalities For example the clustered rule 40 5 age  42 ownhome  yes could be formed from the two association rules age  40 own-home 1 yes and age  41 d ownhome  yes 
The problem we consider is how to efficiently mine clustered association rules from large databases using an as sociation rule mining algorithm as one step in the overall process In practice it is very important that rules mined from a given database are understandable and useful to the user Clustered association rules are helpful in reducing the large number of association rules that are typically computed by existing algorithms thereby rendering the clustered rules much easier to interpret and visualize A practical use of these clusters is to perform segmentation on large customer oriented databases 
As an example, consider a marketing company that sends a direct mail catalog to its current cus tomer base promoting its products Orders are taken for each customer and placed in a transactional database, recording a list of those items purchased along with some demographic information about the purchaser At some point the com pany decides to expand its current customer base By group ing its existing customers by total sales for instance into groups of 223excellent\224 223above average\224 and 223average\224 prof itability the company could use clustered association rules own-home  yes 220 1063-6382/97 10.00 0 1997 IEEE 


on the attributes of the demographic database to segment its customers The company can then use this segmentation in selecting the new customers it should target for future mail ings who are most likely to respond Hence we would like a segmentation that defines a specific criterion e.g custom ers rated 223excellent\224 as a function of the other attribute val ues This corresponds to considering clustered association rules of the form AlAAzA  AA,*G where G is one of 223excellent\224 223above average\224 or 223av erage\224 and the A are attributeranges such as 40 5 age  42 In this paper we consider association rule clustering in the two-dimensional space where each axis represents one attribute from the database used on the left-hand side LHS of a rule We feel two attribute segmentation is easily un derstandable by the user however in Section 5 we discuss possibilities for handling higher dimensionality data Be fore processing by our algorithms, the two LHS attributes A and A are chosen by the user Statistical techniques for identifying the most influential attributes for a given dataset such as factor analysis lo and principal component ana lysis 1111 have been well-studied and could also be used The user also selects a value from the third attribute as a cri terion to be used for segmentation We use an adjacency definition that ensures that the clusters are rectangular re gions of the data This helps with readability of the final clustered association rules Clustering in general is a difficult problem and as a re search topic has been studied for quite some time 119 211 Even under the two-dimensional attribute assumptions we make the problem of identifying the fewest clusters in a two-dimensional grid is a specific instance of the k-decision set-covering problem which has been shown to be NP complete 5 Our clustering approach begins by taking source data in tuple form and partitioning those attributes that take values from a continuous domain We then perform a single pass over the data using an association rule engine to derive a set of association rules Next we cluster all those two-attribute association rules where the right-hand side of the rules sat isfies our segmentation criteria Our approach to the clus tering problem is heuristic based on the geometric proper ties of a two-dimensional grid and produces an efficient lin ear time approximation to an optimal solution. This cluster ing produces the desired segmentation We test the segment ation for accuracy and if necessary modify certain system parameters to produce a better segmentation and repeat the process 1.1 Related Work The problem of taking sets of attribute-value pairs such as Age=x, Salary=y as points in two-dimensional space and then finding optimal regions with respect to some spe cified criteria, within this space was introduced in 7 The authors in 7 considered two types of regions, rectangles and connected x-monotone but focus on the latter We fo cus on rectangular regions because they usually are more understandable Unlike 171 we do not require the user to choose any of the parameters necessary for generating asso ciation rules, but instead provide a fully automated system for identifying clustered association rules In 22 the authors consider the problem of mining quant itative attributes, in contrast to previous algorithms designed for transaction-type items The authors in 22 partition attributes using equi-depth bins where each bin contains roughly the same number of tuples and introduce a new measure to help the user to decide how many partitions there should be for each attribute They address issues of com bining adjacent intervals and use a 223greater-tham-expected value\224 interest measure for the rules. One component of our work also requires partitioning quantitative attributes but differs in that we are using the partitioned quantitative attrib utes as a means to form clusters that ultimately define a seg mentation of the data Further our system is fully automated and does not require any user-specified thresholds as does the work in 1221 In addition we have provided an alternat ive definition of an 223interesting\224 association rule that gives an intuitive means of segmenting the data Finally, whereas 1221 uses user-specified parameters and a partiid complete ness measure to determine attributepartitioning we use geo metric properties of the attribute space to form clusters A related problem is class$cution where data is categor ized into disjoint groups based upon common characterist ics of its attributes Techniques for classification include instance-based classifiers, decision trees artificial neural networks, genetic algorithms and various statistical tech niques  171 Classification has been studied primarily in the AI community and the computational complexity of these algorithms generally inhibits the performance efficiency ne cessary when mining large databases 1121 furthermore, the algorithms do not scale well with increasing database size In the database community the work in I 13 201 has fo cused on designing efficient classification algorithms for large databases The goal of classification is to compute a predictive model for each of the groups. The emphasis is to produce the most accurate model that can predict the group to which an unseen instance belongs An understanding of the model that the classification algorithm computes is usu ally of secondary importance By contrast in cur work we want to segment the space of attribute values based on some criterion grouping attribute, into manageable segments that are understandable to the user Also related is the area of image processing Although a rich literature exists, traditional image segmentation al gorithms tend to focus on obtaining exact boundaries for 22 1 


clusters and as a result do not limit themselves to rectangu lar clusters that we feel are necessary for understandability by the user 8 Our system has been designed however so that it would be a simple matter of exchanging our clustering algorithm with any other 1.2 Paper Organization In Section 2 we present formal definitions and an over view of the work Section 3 describes our Association Rule Clustering System, ARCS and describes the BitOp al gorithm for clustering We also discuss a preprocessing step to smooth the grid and dynamic pruning that removes un interesting clusters. Statistical measures for setting the ini tial threshold values are also described in Section 3 In Sec tion 4 we present experimental results on synthetic data and show the scalability of ARCS for large databases Using a classifier as an alternative way of segmenting the data we compared a well-known classifier to our approach that in stead uses clustered association rules for segmentation. Sec tion 5 concludes and presents our ideas for future work 2 Preliminaries In this section we present formal definitions and termin ology for the problem of mining clustered association rules and then we give an overview of our approach 2.1 Terminology An attribute can be either categorical or non-categorical Categorical attributes are those that have a finite number of possible values with no ordering amongst themselves Ex amples of categorical attributes include \223zip code\224 223hair color\224 223make of car\224 etc Non-categorical attributes which we will call quantitative attributes do have an im plicit ordering and can assume continuous values usually within a specified range. Examples of quantitativeattributes include \223salary\224, \223age\224, \223interest rate\224, etc Let D be a database of tuples where each tuple is a set of attribute values called items of the form attributei  value An association ruEe is an expression of the form X Y where X and Y are sets of attribute=value\items such that no attribute appears more than once in X U y We will refer to X as the left-hand side \(LHS of the rule and y as the right-hand side \(RKS of the rule Two common numeric measures assigned to each asso ciation rule are support and conjidence Support quantifies how often the items in X and Y occur together in the same where JDJ denotes the total number of tuples Confidence quantifies how often X and Y occur together as a fraction of the number of tuples in which X occurs, or 221>\222I When generating a set of association rules using any o F the known algorithms, the user must specify minimum threshold values for both support and confidence tuple as a fraction of the total number of tuples or  lXYl ID1 The following is an example of an association rule Age  32 A Salary  38,500 Rating  good Because quantitative attributes will typically assume a wide range of values from their respective domains we par tition these attributes into intervals, called bins In this pa per we consider only equi-width bins \(the interval size of each bin is the same Other choices are possible such as equi-depth bins \(where each bin contains roughly the same number of tuples or homogeneity-based bins \(each bin is sized so that the tuples in the bin are uniformly distributed 14 231 Our approach can easily be extended to handle these cases as well When a quantitative attribute is parti tioned into bins, the bins are mapped to consecutive integers when attribute values are mapped to bins their value is re placed with the corresponding integer for that bin. For cat egorical attributes we also map the attribute values to a set of consecutive integers and use these integers in place of the categorical values Since this mapping happens prior to run ning an association rule mining algorithm, the binning pro cess is transparent to the association rule engine. This bin ning and mapping approach is also used in 221 and 15 Clustering as defined here is the combination of adja cent attributes values, or adjacent bins of attribute values For example, clustering Agedo and Age4l results in 40 5 Age  42 A clustered association rule is an ex pression of the form XC  Yc XC and YC are items of the form \(Attribute  value\or \(bini L Attribute  bini+l where binj denotes the lower bound for values in the ith bin Clustered association rules will always have a support and confidence of at least that of the minimum threshold levels 2.2 Overview of Approach In this paper we consider the problem of clustering as sociation rules of the form A A B  C where the LHS attributes A and B are quantitative and the RHS attribute C is categorical.\222 The RHS attribute could be quantitative but would first require binning with the resulting bins then treated as categorical values We define a segmentation as the collection of all the clustered association rules for a spe cific value C of the criterion attribute Given a set of two-attribute association rules over binned data we form a two-dimensional grid where each axis cor responds to one of the LHS attributes On this grid we will plot, for a specific value of the RHS attribute all of the cor responding association rules An example of such a grid is shown in Figure 1 Our goal is to find the fewest number of clusters, shown as circles in the figure that cover the asso ciation rules within this grid These clusters represent our clustered association rules and define the segmentation 221We consider only quantitative LHS attributes because the lack of or dering in categorical attributes introduces additional complexity We are currently extending our work to handle categorical LHS attributes 222 


60-$65k 55-$60k 50-$55k 45-$5Ok 40-$45k  35-$40k 30-$35k 25-$30k 20-$25k S l5-$20k lO-$15k O-$lOk m 20 21 22 23 24 25 26 21 28 29 30 31 Age Figure 1 Sample grid with clustered associ ation rules The algorithm we introduce to cluster association rules using a 2D grid solves only a part of the overall problem Recall that in order to mine a collection of association rules we must first define minimum thresholds for both support and confidence Further for the quantitative attributes we must determine the number of bins over which to partition the attribute domain Finding the values for each of these parameters that gives us the best segmentation is a difficult combinatorial optimization problem We present a unifying framework that uses heuristics for searching this space ef ficiently and has very good results Figure 2 shows a high level view of our entire system to compute the clustered as sociation rules which we now describe While the source data is read the attribute values are par titioned \(by the binner as described earlier While the num ber of bins can be changed by the user doing so restarts the system The association rule engine is a special-purpose al gorithm that operates on the binned data The minimum sup port is used along with the minimum confidence to generate the association rules Once the association rules are discovered for a particu lar level of support and confidence we then form a grid of only those rules that give us information about the group RHS we are segmenting We apply our BitOp algorithm Section 3.3 to this grid along with certain other techniques described in Sections 3.4 and 3.5 to form clusters of adja cent association rules in the grid These clustered associ ation rules are then tested for their accuracy by the verijer against a sample of tuples from the source database The ac curacy is supplied to a heuristic optimizer Section 3.7 that adjusts the minimum support threshold and/or the minimum confidence threshold and restarts the mining process at the association rule engine These heuristic adjustments con tinue until the verifier detects no significant improvement in the resulting clustered association rules or the verifier de termines that the budgeted time has expired 3 Record Data I  of x-bins Binner yjarrayofhinned data  criteria Engine I asssociation rules Heuristic Optimizer conversion Clustering  222 I I I 1  clustered association rules i   test data   Figure 2 Architecture of the Association Rule Clustering System Association Rule Clustering System ARCS The ARCS framework was shown in Figure 2 We now detail components of the system 3.1 Binning Data The binner reads in tuples from the database and re places the tuples\222 attribute values with their coriesponding bin number as previously described We first determine the bin numbers for each of the two LHS attributes A and A Using the corresponding bin numbers bin and bin we index into a 2D array where for each bin,,bin pair, we maintain the number of bin,,bin tuples having each pos sible RHS attribute value as well as the total number of bin,,bin tuples Thesizeofthe2Darray is n,*ny*\(nseg 1 where n is the number of x-bins ny is the number of y bins and nSeg is the cardinality of the RHS segmentation attribute In our system we assume this array can fit in main memory Although we are typically interested in only one value of the segmentation criteria at a time e.g 223customer-rating  excellent\224 by maintaining this data structure in memory we can compute an entirely new segmentation for a different value of the segmentation criteria without the need to re-bin the original data If memory space is at a premiurn, however we can set nSeg  1 and maintain tuple counts for only the one value of the segmentation criteria we are inlerested in 223 


3.2 The Association Rule Engine While it is possible to use any of the existing association rule mining algorithms to mine the binned data we describe a more efficient algorithm for the special case of mining two dimensional association rules using the data structure con structed by our binning process Deriving association rules from the binned data in the BinArray is straightforward. Let Gk be our RHS criterion attribute Every cell in the BinAr ray can be represented by an association rule whose LHS values are the two bins that define the BinArray cell and whose RHS value is Gk where X  i represents the range bin 5 X  binktl and U  j represents the range bin 5 Y  bin;+l and bin and bin are the lower bounds of the ith x-attribute bin and theith y-attribute bin respectively The support for this rule is and the confidence is w where N is the total number of tuples in the source data I i j I is the total number of tuples mapped into the BinArray at location i j and I\(i j Gk is the number of tuples mapped into the BinArray at location i j with criterion attribute value Gk To derive all the association rules for a given support and confidence threshold we need only check each of the oc cupied cells in the BinArray to see if the above conditions hold If the thresholds are met we output the pair i j cor responding to the association rule on binned data as shown above The i j pairs are then used to create a bitmap grid that is used by the BitOp algorithm, described in the follow ing section, to locate clusters of association rules Our algorithm, shown in Figure 3 requires only a single pass through the data in contrast to using some existing al gorithms that may need to make several passes to find all association rules It is also very important to note that by maintaining the BinArray data structure, we can apply dif ferent support or confidence thresholds without reexamin ing the data, making the \223re-mining\224 process dramatically faster than with previous algorithms In our system, chan ging thresholds is nearly instantaneous x  i A Y  j j Gk 3.3 Clustering We begin by presenting a very simple example of the clustering problem to illustrate the idea Consider the following four association rules where the RHS attribute 223GroupAabel\224 has value 223A\224 Input 11 The BinArray computed from the binning component 12 The value Gk we are using as the 13 The min-support threshold  14 The min-confidence threshold  15 N the total number of tuples in the source data 16 n the number of x-bins 17 ny the number of y-bins 01  A set of pairs of bin numbers i j criterion for segmentation output representing association rules of the form x  i A Y  j  Gk Procedure GenAssocationRules minsupport-count  N  min-support  Association rule generation from the binned data  for\(i=l i n i do begin for\(j=l j n j do begin if BinArray[i j Gk 2 min-supportzount and BinArray[i j   2 min-conf Output ij end-for end-for Figure 3 The association rule mining al gorithm Age  u3 A Salary  s5  Group-label  A Age  u4 A Salary  s6  Group-label  A Age  u4 A Salary  s5 Group-label  A Age  u3 A Salary  86  Group-label  A We represent these four rules with the grid in Figure 4.2 Clearly linear, adjacent cells can be combined to form a line segment and we can naturally extend this idea to rectangu lar regions In doing so all four of the original association rules are clustered together and subsumed by one rule a3 5 Age  a4 ss 5 Salary  sg  GroupAabel  A Assuming the bin mappings shown in Figure 4, the final clustered rule output to the user is 40 5 Age  42 A 40,000 5 Salary  60,000  GroupAabel  A Intuitively starting with a grid as in Figure 4 we are searching for the rectangular clusters that best cover all of the occupied cells For example, given the grid in Figure 5 our algorithm would select the two clusters shown if we are interested in finding the fewest number of clusters The problem we investigate is how to find these non Age  40 A Salary  42,350  GroupAabel  A Age  41 A Salary  57,000  Grouplabel  A Age  41 A Salary  48,750  Grouplabel  A Age  40 A Salary  52,600 a Groupdabel  A If theLHS Age bins areal u2   I and theLHS Salary bins are SI sz    then these rules are binned to form the corres ponding binned association rules overlapping clusters Or more Precisely, how to find a good The grid is constructed directly from the a sJ  pairs output by the association rule engine 224 


Age Figure 4 Grid showing the four association rules mask Figure 5 clustering of rules given some criterion An example of a simple criterion is minimizing the total number of clusters in Section 3.6 we will describe the criterion we use The process of clustering is made difficult by noise and outliers Noise occurs in the data from those tuples that belong to other groups than the group we are currently clustering For example, a customer database could contain data about cus tomers whose group label RHS attribute would be one of several values customerxating is 223average\224 223above aver age\224 or 223excellent\224 of which 223excellent\224 may be our cri terion value for segmenting the data using clusters Tuples having any other value will be considered noise that will af fect both the support and confidence of the clusters we com pute. Outliers are those tuples that belong to the same group but lie outside of any existing clusters for that group We will give examples of these occurrences and show how our clustering system can mitigate their effects in the following sections 01 1101 1 1 11 10 10 3.3.1 The BitOp Algorithm The algorithm we present enumerates clusters from the grid We select the largest cluster from the enumerated list it eratively applying the algorithm until no clusters remain It has been shown in 5 that such a greedy approach pro duces near optimal clusters in O 15\2221 time where C is the final set of clusters found We experimented with pure Bitmap Masks starting at row 1 The first mask mask is defined to be the same as the first row while the second mask mask is the first mask bitwise ANDed with the second row Since mask is not identical to mask we evaluate mask for clusters finding a 2-by-1 cluster, indicated by the solid circle VVe see that mask identifies a 1-by-2 cluster 1 since a single bit is set 225 


and 2 since it is in the second mask meaning the cluster extends two rows This cluster is indicated by the dashed circle Finally mask identifies no clusters since the mask contains no set bits, signifying there are no clusters that be gin at I-ow l and extend through row 3 We now repeat this process beginning with the second row of the bitmap, producing the two clusters as shown Bitmap Masks starting at row 2 row3 1 0 0 mask row2 mask row 1 0 1 1 The process ends when the mask for the last row is com puted Our algorithm can be implemented efficiently since it only uses arithmetic registers, bitwise AND and bit-shift machine instructions We assume that the size of the bitmap is such that it fits in memory which is easily the case even for a 1000x1000 bitmap 3.4 Grid Smoothing As a preprocessing step to clustering we apply a 2D smoothing function to the bitmap grid In practice, we have often found that the grids contain jagged edges or small 223holes\224 of missing values where no association rule was found A typical example is shown in Figure 7\(a These features inhibit our ability to find large, complete clusters To reduce the effects caused by such anomalies we use an image processing technique known as a low-pass jilter to smooth out the grid prior to processing 8 Essentially a low-pass filter used on a two-dimensional grid replaces a value with the average value of its adjoining neighbors thereby 223smoothing\224 out large variations or inconsistencies in the grid The use of smoothing filters to reduce noise is well known in other domains such as communications and computer graphics Details of our filtering algorithm are omitted for brevity, but Figure 7\(b nicely illustrates the res ults Experiments using the association rule support values instead of binary values were also performed yielding prom ising results See Section 5 3.5 Cluster Pruning Clusters that are found by the BitOp algorithm but that do not meet certain criteria are dynamically pruned from the set of final candidate clusters Typically we have found that clusters smaller than 1 of the overall graph are not use ful in creating a generalized segmentation Pruning these smaller clusters also aids in reducing \223outliers\224 and effects from noise not eliminated by the smoothing step In the case where the BitOp algorithm finds all of the clusters to be suf ficiently large no pruning is performed. Likewise if the al Input 11 R the number of bins for attributex 12 C the number of bins for attribute Y 13 BM, the bitmap representation of the grid  is the ith row of bits in the bitmap output 01  clusters of association rules row  1 while row i R do begin RowMask  set all bits to 222 1\222 PriorMask  BM[row height=O for r=row riR r do begin RowMask  RowMask  BM[row if RowMask  0 do begin I Locate clusters in PriorMask I process_row\(PriorMask,height  break end-if if RowMask  PriorMask do begin processxo w\(PriorMask,height  PriorMask  RowMask end-if height  height+l I extend height of possible clusters I end-for processJow\(PriorMask,height end-while Figure 6 The BitOp algorithm  8 B 1 e m attribute X 4 Figure 7 A typical grid \(a\prior to smoothing b after smoothing 226 


computed cluster Function 2 false-positives 1 age  40 A 50K L salary 5 1OOK j GroupA 2 40 5 age  60 A 7511\221 5 salary 5 125K GroupA 3 age 2 60 A 25K 5 salary 5 751q 3 Group A LI B Y else  Group other 0 a E false-negatives   Y a 0 Figure 8 The function used to generate syn thetic data gorithm cannot locate a sufficiently large cluster it termin ates The idea of pruning to reduce error and to reduce the size of the result has been used in the AI community espe cially for decision trees  171 3.6 Cluster Accuracy Analysis To determine the quality of a segmentation by a set of clustered association rules we measure two quantities i the number of rules computed and ii\the summed error rate of the rules based on a sample of the data. The two quantities are combined using the minimum description length MDL principle  181 to arrive at a quantitative measure for determ ining the quality compared to an 223optimal\224 segmentation We first describe our experiments then we describe our error measure for a given rule then we describe our application of the MDL principle In 2 a set of six quantitativeattributes salary, commis sion age, hvulue hyears loan and three categorical attrib utes educationdevel cur zip code for a test database are defined. Using these attributes 10 functions of various com plexity were listed We used Function 2 shown in Figure 8 to generate the synthetic data used in our experiments. The optimal segmentation for this data would be three clustered association rules each of which represents one of the three disjuncts of the function. The clustering process is made dif ficult when we introduce noise, random perturbations of at tribute values and error due to binningof the input attributes age and salary An intuitive measure of the accuracy of the resulting clustered rules would be to see how well the rectangular clusters overlap the three precise disjuncts of the function in Figure 8 We define the notion of false-positives and fulse-negatives graphically as shown in Figure 9 and seek to minimize both sources of error In Figure 9 the light grey rectangle represents an actual cluster according to the function and the dark-grey rectangle represents a computed cluster. Note that in general an optimal cluster need not be rectangular The false-positive results are when the com puted cluster incorrectly identifies tuples outside of the op timal cluster as belonging to the specified group whereas the false-negatives are tuples that should belong to the group but are not identified as such by the computed cluster. The total summed error for a particular cluster is the total false Figure 9 Error between overlapping rc-g\222  ions positives  false-negatives However unless the optimal clustering is known beforehand such as here where a func tion is used to generate the data, this exact measure of error is not possible. Because we are interested in real-world data where the optimal clustering is not known we instead se lect a random sample of tuples from the database and use these samples to determine the relative error of the com puted clusters The relative error is only an approximation to the exact error since we are counting the number of false negatives and false-positives based only on a sample of the original database In order to get a good approximation to the actual error we use repeated k out of n sampling a stronger statistical technique The strategy we use to measure the quality of a segmenta tion given a set of clustered association rules is b<asecf on the MDL principle We are using a simplified model of MDL that has worked in practice The MDL principle states that the best model for encoding data is the one that minimizes the sum of the cost of describing the model and the cost of describing the data using that model The goal is to find a model that results in the lowest overall cost with cost typ ically measured in bits In the context of clustering, the models are the descrip tions of the clusters and the data is the sampled data de scribed above The greater the number of clusters used for segmentation, the higher the cost necessary to describe those clusters The cost of encoding the sampled data using a given set of clusters \(the model\is defined to be the sum of all errors for the clusters The intuition is that if a ruple is not an error, then it is identified by a particular cluster and hence its cost is included with the description of the cluster Otherwise if the tuple is an error, then we must specifically identify it as such and this incurs a cost We use the follow ing equation to determine the cost of a given set of clustered association rules cost  w log,\(ICI  we log,\(errors where IC is the number of clusters and errors is the sum of false-positives  false-negatives\for the clusters C The logarithmic factor is used because having more clusters re quires a logarithmically increasing number of bits to enu 227 


merate and the logarithmic factor provides a favorable non linear separation between close and near-optimal solutions Based on empirical evidence we made the simplifying as sumption the clusters themselves have a uniform encoding cost. The constants wc and we allow the user to impart a bias towards 223optimal\224 cluster selection, providing greater flex ibility in finding a representation of the segmentation that is the most usable If w is large, segmentations that have many clusters will be penalized more heavily since they will have a higher associated cost and segmentations of the data that have fewer clusters will have a greater probability of be ing 223optimal\224. Likewise if we is large, the system will favor segmentations where the error rate is lowest If both con stants are equal wc  we  1 as in thedefault case, neither term will bias the cost Our heuristic optimizer recall Figure 2 by means de scribed in the following section, seeks to minimize the MDL cost 3.7 Parameter Heuristics In this section we describe the algorithm our overall sys tem uses to adjust the minimum support and confidence thresholds based upon the accuracy analysis from the previ ous section. \(currently the number of bins for each attribute is preset at 50 We discuss this issue more in the following section We desire values for support and confidence that will result in a segmentation of the data that optimizes our MDL cost function. The search process involves successive iterations through the feedback loop shown in Figure 2 We identify the actual support and confidence values that appear in the binned data and use only these values when adjusting the ARCS parameters We begin by enumerating all unique support thresholds from the binned data with one pass and then all of the unique confidence thresholds for each of these support thresholds with a second pass A data structure sim ilar to that shown in Figure 10 is used to maintain these val ues. Note that as support increases, there become fewer and fewer cells that can support an association rule and we have found a similar decrease in the variability of the confidence values of such cells Given a choice to either begin with a low support threshold and search upwards or begin with a high sup port threshold and search downwards we chose the former since we found most 223optimal\224 segmentations were derived from grids with lower support thresholds If we were using a previous association rule mining algorithm, for efficiency it might be preferable to start at a high support threshold and work downwards, but our efficient mining algorithm al lows us to discover segmentations by starting at a low sup port threshold and working upwards. Our search starts with a low minimum support threshold so that we consider a lar ger number of association rules initially, allowing the dy namic pruning performed by the clustering algorithm to re Confidence List  I I I I I I 5 rl 12 I 25 I 40 I 56%I 90 Sumort I List\222 Wra*luoal Figure 10 Ordered support thresholds    lists of confidence and move unnecessary rules The support is gradually increased to remove background noise and outliers until there is no im provement of the clustered association rules \(within some E 4 Experimental Results The ARCS system and the BitOp algorithm have been implemented in C comprising approximately 6,300 lines of code To assess the performance and results of the al gorithms in the system we performed several experiments on an Intel Pentium workstation with a CPU clock rate of 120MHz and 32MB of main memory running Linux 1.3.48 We first describe the synthetic rules used in generating data and present our initial results We then briefly compare our results with those using a well-known classifier C4.5 to perform the segmentation task Finally we show perform ance results as the sizes of the databases scale 4.1 Generation of Synthetic Data We generated synthetic tuples using the rules of Func tion 2 from Figure 8 Several parameters affect the distri bution of the synthetic data These include the fraction of the overall number of tuples that are assigned to each value of the criterion attribute aperturbation factor to model fuzzy boundaries between disjuncts and an outlier percentage that defines how many tuples will be assigned to a given group label but do not match any of the defining rules for that group. These parameters are shown in Table 1 4.2 Accuracy and Performance Results We generated one set of data for Function 2 with ID1  50,000 and 5 perturbation and a second set of data for the same function but with 10 outliers, i.e 10 of the data are outliers that do not obey the generating rules In every ex perimental run we performed ARCS always produced three clustered association rules each very similar to the gener ating rules and effectively removed all noise and outliers 228 


Attribute 1 Value salary age 1 uniformly distributed from 20,000 to 150k uniformly distributed from 20 to 80 PI fracA fracother P U Number of tuples 20,000 to 10 million Fraction of tuples for \223Group A\224 40 Fraction of tuples for \223Group other\224 60 Perturbation factor 5 Outlier percentage 0 and 10 Table 1 Synthetic data parameters from the database. The following clustered association rules were generated for Group A clusters from the set of data containing outliers and with a minimum support threshold of 0.01 and a minimum confidence threshold of 39.0 20 5 Age 5 39 A 48601 5 Salary 5 100600 j Grp A 40 5 Age 5 59 A 74601 5 Salary 5 124000  Grp A 60 5 Age 5 80 A 25201 5 Salary 5 74600  Grp A The reader can compare the similarity of these rules with those used to generate the synthetic data in Figure 8 We measured the error rate of ARCS on these databases and compared it to rules from C4.5 C4.5 is well known for building highly accurate decision trees that are used for classifying new data and from these trees a routine called C4.5RULES constructs generalized rules  171 These rules have a form similar to our clustered association rules and we use them for comparison both in accuracy and in speed of generation. Figures 11 and 12 graph the error of both sys tems as the number of tuples scale using the two sets of gen erated data The missing bars for C4.5 on larger database sizes are due to the depletion of virtual memory for those ex periments, resulting in our inability to obtain results \(clearly C4.5 is not suited to large-scale data sets From Figure 11 we see that C4.5 rules generally have a slightly lower error rate than ARCS clustered association rules when there are no outliers in the data However, with 10 of the data appearing as outliers the error rate of C4.5 is slightly higher than ARCS as shown in Figure 12 C4.5 also comes at a cost of producing significantly more rules as shown in Figures 13 and 14 As mentioned earlier we are targeting environments where the rules will be processed by end users so keeping the number of rules small is very im portant The primary cause of error in the ARCS rules is due to the granularity of binning The coarser the granularity, the less likely it will be that the computed rules will have the same boundary as the generating rules To test this hypo thesis we performed a separate set of identical experiments using between 10 to 50 bins for each attribute We found a general trend towards more 223optimal\224 clusters as the number of bins increases 100 10002000 4000 8000 10000 Number of Tuples in 222000s Figure 15 Scalability of ARCS Table 2 Comparative execution timers sec 4.3 Scaleup Experiments To test scalability we ran ARCS on several databases with increasing numbers of tuples. Figure 15 shows that ex ecution time increases at most linearly with the size of the database Because ARCS maintains only the ElinArray and the bitmap grid ARCS requires only a constant amount of main memory regardless of the size of the database \(assum ing the same number of bins\This actually gives our system significantly better than linear performance as can be seen by close inspection of Figure 15 since some overhead ex ists initially but the data is streamed in faster from the U0 device with larger requests For example when the num ber of tuples scales from 100,000 to 10 million a factor of loo the execution time increases from 42 seconds to 420 seconds a factor of lo In comparison C4.5 requires the entire database, times some factor, to fit entirely in main memory This results in paging and eventual depletion of virtual memory which prevented us from obtaining execu tion times or accuracy results for C4.5 rules from databases greater than 100,000 tuples\Both C4.5 alone and C4.5 to gether with C4.5RULES take exponentially higher execu tion times than ARCS as shown in Table 2 229 


10 r  ______ 20 18 7 20 50 100 200 500 1000 Number of Tuples in 221000s Figure 11 Error rate with U  0 20 50 100 200 500 1000 Number of Tuples in 221000s Figure 12 Error rate with U  10 35 4 30 II 6 25  v   L 3 20  5 15  z 10  5 OT 12 16 31 3 20 50 100 200 500 1000 Number of Tuples in 221000s Figure 13 Number of rules produced U  0 5 Conclusions and Future Work In this paper we have investigated clustering two attribute association rules to identify generalized segments in large databases The contributions of this paper are sum marized here 0 We have presented an automated system to compute a clustering of the two-attribute space in large databases 0 We have demonstrated how association rule mining technology can be applied to the clustering problem We also have proposed a specialized mining algorithm that only makes one pass through the data for a given partitioning of the input attributes and allows the sup port or confidence thresholds to change without requir ing a new pass through the data e A new geometric algorithm for locating clusters in a two-dimensional grid was introduced Our approach has been shown to run in linear time with the size of 16  14  z 12 O 10 kl 5 z6 4 2 0 Lc 20 50 100 200 500 1000 Number of Tuples in 221000s Figure 14 Number of rules produced U  10 the clusters Further, parallel implementations of the algorithm would be straightforward 0 We apply the Minimum Description Length MDL principle as a means of evaluating clusters and use this metric in describing an 223optimal\224 clustering of associ ation rules 0 Experimental results show the usefulness of the clustered association rules and demonstrates how the proposed system scales in better than linear time with the amount of data The algorithm and system presented in the paper have been implemented on several platforms including Intel DEC and SGI So far we have performed tests only us ing synthetic data but intend to examine real-world demo graphic data We also plan on extending this work in the fol lowing areas 0 It may be desirable to find clusters with more than two attributes One way in which we can extend our pro posed system is by iteratively combining overlapping 230 


sets of two-attribute clustered association rules to pro duce clusters that have an arbitrary number of attrib utes Handle both categorical and quantitative attributes on the LHS of rules To obtain the best clustering we will need to consider all feasible orderings of categorical at tributes Our clustering algorithm has been extended to handle the case where one attribute is categorical and the other quantitative and we achieved good results By using the ordering of the quantitative attribute we con sider only those subsets of the categorical attribute that yield the densest clusters Preliminary experiments show that segmentation can be improved if the association rule support values rather than binary values are considered in the smooth ing filter and more advanced filters could be used for purposes of detecting edges and corners of clusters It may be beneficial to apply measures of information gain 16 such as entropy when determining which two attributes to select for segmentation or for the op timal threshold values for support and confidence The technique of factorial design by Fisher 6 41 can greatly reduce the number of experiments necessary when searching for 223optimal\224 solutions This tech nique can be applied in the heuristic optimizer to re duce the number of runs required to find good values for minimum support and minimum confidence Other search techniques such as simulated annealing can be also be used in the optimization step Acknowledgements We are grateful to Dan Liu for his work in extending functionality in the synthetic data generator and for the ex periments using C4.5 References I R Agrawal S Ghosh T Imielinski, B. Iyer and A. Swami An interval classifier for database mining applications In Proceedings of the 18th International Conference on Very Large Data Bases Vancouver Canada, 1992 2 R Agrawal T Imielinski, and A. Swami. Database mining A performance perspective In ZEEE Transactions on Know ledge and Data Engineering volume 5\(6 pages 914-925 Dec 1993 3 R Agrawal T Imielinski and A Swami Mining associ ation rules between sets of items in large databases In Pro ceedings of the I993 ACM SIGMOD International Confer ence on Management of Data Washington D.C 1993 4 G E Box W Hunter and J S Hunter Statistics for Ex perimenters An Introduction to Design, Data Analysis and Model Building John Wiley and Sons 1978 5 T Cormen, C. Lieserson and R Rivest Introduction to Al gorithms The MIT Press 1990 6 R A Fisher The Design of Experiments Hafner Publishing Company 1960 71 T Fukuda Y Morimoto S Morishita and lr Tokuyama Data mining using two-dimensional optimized association rules Scheme algorithms, and visualization In Proceed ings of the 1996ACM SIGMOD International Conference on Management of Data Montreal Canada June 1996 8 R C Gonzalez and R Woods Digital Image Processing Addison-Wesley 1992 9 M Klemettinen H Mannila P Ronkainen and H Toivonen Finding interesting rules from large sets of discovered association rules In 3rd International Confer ence on Information and Knowledge Management CIKM Nov 1994 IO J B Kruskal. Factor analysis and principle components Bi linear methods. In H Kruskal and J. Tanur editors Interna tional Encyclopedia of Statistics Free Press 1978  111 D N Lawley Factor Analysis as a Statistical Method American Elsevier Publishing second edition 1971 I21 D J Lubinsky Discovery from databases A review of ai and statistical techniques In IJCAI-89 Workshop on Know ledge Discovery in Databases pages 204218,1989 I31 M Mehta, R. Agrawal and J Rissanen Sliq A fast scal able classifier for data mining In Proceedings of the 5th In ternational Conference on Extending Databaste Technology EDBT Avignon France Mar 1996 I41 M Muralikrishna and D DeWitt Statistical pirofile estima tion in database systems ACM Computing Suweys 20\(3 Sept. 1988  151 G. Piatetsky-Shapiro Discovery, analysis and presentation of strong rules Knowledge Discovery in Databases 1991 16 J Quinlan. Induction of decision trees In MachineLearning volume 1 pages 81-106,1986  171 J Quinlan C4.5 Programs for Machine Learning Morgan Kaufmann San Mateo California 1993  J Rissanen Stochastic Complexity in Statistical Inquiry World Scientific Publishing Company, 1989 I91 H C Romesburg Cluster Analysis for Researchers Life time Learning Publications-Wadsworth Inc 1984 20 J Shafer R Agrawal and M Mehta Fast serial and paral lel classification of very large data bases In Proceedings of the 22nd International Conferenceon Very Luge Databases Bombay, India, 1996 21 H Spath Cluster AnalysisAlgorithms for data reductionand classijication ofobjects Ellis Horwood Publishers, 1980 22 R Srikant and R Agrawal Mining quantitative association rules in large relational tables In Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data Montreal Canada June 1996 23 K Whang S Kim and G Wiederhold Dynamic mainten ance of data distribution for selectivity estimation VLDB Journal 3\(1\Jan 1994 231 


