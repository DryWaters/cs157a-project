Big Data Analysis Using Apache Hadoop Jyoti Nandimath Dept. of Computer Engineering SKNCOE, Pune, India jyotign@gmail.com Ankur Patil Dept. of Computer Engineering SKNCOE, Pune, India ankurpatil23@gmail.com Ekata Banerjee Dept. of Computer Engineering SKNCOE, Pune, India koool.ekata@gmail.com   Pratima Kakade Dept. of Computer Engineering SKNCOE, Pune, India pratimakakade@gmail.com 
Saumitra Vaidya Dept. of Computer Engineering SKNCOE, Pune, India saumitravaidya28@gmail.com Abstract  The paradigm of processing huge datasets has been shifted from centralized architecture to distributed architecture As the enterprises faced issues of gathering large chunks of data they found that the data cannot be processed using any of the existing centralized architecture solutions. Apart from time constraints, the enterprises faced issues of efficiency 
performance and elevated infrastructure cost with the data processing in the centralized environment With the help of distributed architecture these large organizations were able to overcome the problems of extracting relevant information from a huge data dump. One of the best open source tools used in the ma rket to harness the distributed architecture in order to solve the data processing problems is Apache Hadoop.  Using Apache Hadoopês various components 
such as data clusters, map-reduce algorithms and distributed processing, we will resolve vari ous location-based complex data problems and provide the relevant information back into the system, thereby increasing the user experience Index Terms  Hadoop,Big data,Map Reduce, Data processing I INTRODUCTION Amount of data generated ev ery day is expanding in drastic manner.Big data is a popu lar term used to describe 
the data which is in zetta bytes. Government , companies many organisations try to acqu ire and store data about their citizens and customers in order to know them better and predict the customer behaviour . social networking websites generate new data every second and handling such a data is one of the major challenges companies are facing. Data which is stored in data warehouses is causing disruption because it is in a raw format ,proper analysis and processing is to be done  in order to produce usable information out of it  New tools are being used to handle such a large 
amount of data in short time.ApacheHadoop is java based programming framework which is used for processing large data sets in distributed computer environment. Hadoop is used in system where multiple nodes are present which can process terabytes of data.hadoop uses its own file system HDFS which facilitates fast transfer of data which can sustain node failure and avoid system failure as whole Hadoop uses MapReduce algorithm which breaks down the big data into smaller chunks and performs the operations on it Various technologies will come in hand-in-hand to 
accomplish this task such as Spring Hadoop Data Framework for the basic foundations and running of the Map-Reduce jobs, Apache Maven for distributed building of the code, REST Web services for the communication and lastly Apache Hadoop for distributed processing of the huge dataset I II RELATED WORK Hadoop framework is used by many big companies like Google, yahoo, IBM , for applic ations such as search engine advertising and information gathering and processing 
A Apache Hadoop goes realtime at Facebook At Facebook, Hadoop has traditionally been used in conjunction with Hive for storage and analysis of large data sets. Most of this analysis occu rs in offline batch jobs and the emphasis has been on maximizing throughput and efficiency. These workloads typically read and write large 700 IEEE IRI 2013, August 14-16, 2013, San Francisco, California, USA 978-1-4799-1050-2/13/$31.00 ©2013 IEEE 


amounts of data from disk sequentially. As such, there has been less emphasis on making Hadoop performant for random access workloads by providing low latency access to HDFS. Instead, we have used a combination of large clusters of MySQL databases and caching tiers built using memcache  m a ny ca s es, results  f r o m Had oop are  uploaded into MySQL or memcached for consumption by the web tier Recently, a new generation of applications has arisen at Facebook that require very high write thro ughput and cheap and elastic storage, while simultaneously requiring low latency and disk efficient sequential and random read performance MySQL storage engines are proven and have very good random read performance but typically suffer from low random write throughput. It is difficult to scale up our MySQL clusters rapidly while maintaining good load balancing and high uptime. Administration of MySQL clusters requires a relatively high management overhead and they typically use more expensive hardware. Given our high confidence in the reliability and scalab ility of HDFS we began to explore Hadoop and HBase for such applications B Yelp : uses AWS and Hadoop Yelp originally depended upon giant RAIDs to store their logs, along with a single local instance of Hadoop When Yelp made the move Amazon Elastic MapReduce they replaced the RAIDs with Amazon Simple Storage Service Amazon S3\d immediately transferred all Hadoop jobs to Amazon Elastic MapReduce Yelp uses Amazon S3 to store daily logs and photos generating around 100GB of logs per day. The company also uses Amazon Elastic MapReduce to power approximately 20 separate batch scripts, most of those processing the logs. Features powered by Amazon Elastic MapReduce include 1 People Who Viewed this Also Viewed 2 Review highlights 3 Auto complete as you type on search 4 Search spelling suggestions 5 Top searches 6 Ads Yelp uses MapReduce. MapReduce is about the simplest way you can break down a big job into little pieces. Basically, mappers read lines of input, and spit out tuples of \(key, value\. Each ke y and all of its corresponding values are sent to a reducer  III T HE P ROPOSED S CHEMES As mentioned earlier we overcome the problem of analysis of big data using Apache Hadoop . The processing is done in four steps which includes creating a servers of required configuration using Amazon web services using their own cloud formation language and configuring it using chef . Data on the cluster is stored using MongoDB which stores data in the form of key:value pairs which is advantageous over relational database for managing large amount of data Various languages allows writing scripts for importing data from collections in Mongodb to the json file which then can be processed in Hadoop as per userês requirement Hadoop jobs are written in spring framework this jobs implement MapReduce algorithm for data processing. Six jobs are implemented in STS for a data processing in a location based social networking application The record of the whole session has to me maintained in log file using aspect \(AOP\ programming in spring The output produced after data processing in the hadoop job, has to be exported back to the database . the old values to the database has to be updated immediately after processing in order to prevent loss of valuable data The whole process is automated by using java scripts and tasks written in ant build tool  for executing executable JAR files A Importing data from database to Hadoop Data is stored in mongo DB which is a NoSQL database. Its stored in the form of key value pairs which makes it easier for latter upd ation and creating the index than using traditional relational database.   JSON JavaScript Object Notation is a data interchangeable format which is easy for humans to read and write and its easy for machines to parse and generate. Itês a text format which is completely language independent  but it is understandable for c, java programmer Using json files with Hadoop Is beneficial as it stores information in key value format. Map reduce is designed in such a way that it takes key value pairs as an input. Hence there is no additional cost of conversion Groovy scripts can be written to make a connection to remote database  and copy collection to the json file which then will passed to HDFS to perform required operations .groovy scripts are called and run in an application using groovy class loader File system of hadoop is different than the file system the OS .it uses single name node and multiple  data nodes to store data and perform jobs. hence json file is to be transferred the HDFS from local file system . java script is included in batch file to automate this process B Performing jobs in Hadoop  Map/Reduce is a programming paradigm that was made popular by Google where a task is divided into small portions and distributed to a large number of nodes for processing \(map\he results are then summarized into the final answer \(reduce\oop also uses Map/Reduce for data processing. Hence different functions for the processing are written in the form of Hadoop job. A Hadoop job consists of mapper and reducer functions. These jobs are written in spring 701 


framework i.e. STS is used as the integrated development environment \(IDE\ Hadoop jobs are implemented in spring framework. These jobs ar e required to process data for a location based social networki ng application. The input to these jobs is a JSON file containing different information fields about various locations The jobs and their implementation are explained below Hadoop jobs are written for calculating 1.Average Rating:- Ratings for a location are added from different records and average is calculated. Higher rating specifies that a particular location is more popular 2.Total Recommendations:- Total number of recommendations given by user for a particular location are calculated 3.Total Votes:- Total number of votes given by user for a particular lo cation are calculated 4.Total Suggestions:- A user can suggest a location to other users who visit the web site. Total suggestions are calculated 5.Unique Tags:- Unique tags corresponding to a location are calculated C Exporting data back to the database The output produced after data processing in the hadoop job, has to be exported back to the database. The old values in the database has to be updated immediately after processing, in order to prevent loss of valuable data Exporting of data is done in the following steps 1.The output of the hadoop job is in a key value format. The key contains the location_id and value is the specific value pertaining to the data processed 2.There are 6 different jobs, each job will have its own export program, that will copy the specific output to the mongo database. Consider the following example Calculating the average rating fo r a particular location\(say barbeque nation\ is a hadoop job that we have implemented. Now members from all around the globe will post their ratings and comments regarding barbeque nation We need to give a final ab solute rating to barbeque nation for our users interest. This com es under the data processing part which will be implem ented in the hadoop job The final output of the average ratings of all places is produced in a comma separated format output file. This file in turn is used in the Groovy script to update the records in the database The key value pairs are extracted from the output file and a query is generated to update the database. Hadoop and mongodb are connected using the hadoop-mongodb connector D.Logging and automation The whole session of such a large process has to be maintained and logged in to a file for a record. Aspect\(AOP Programming allows us to put an aspect on a class\(ie to intercept a class 1 Before Run before the method execution 2 After - Run after the method returned a result 3 Around - Run around the method execution combine all three advices above Aspect can be implemented using following steps 1.To add Project Dependencies To enable AspectJ, you need aspectjrt.jar aspectjweaver.jar and spring-aop.jar in pom.xml file 2.Spring Bean Normal bean, with few methods, later intercept it via AspectJ annotation 3.Enable AspectJ In Spring configuration file, put ç<aop:aspectj autoproxy />ç, and define your Aspect \(interceptor and normal bean Aspect Types AspectJ @Before The logBefore\(\ethod will be executed before the execution of  a class method AspectJ @After The logAfter\(\ method will be executed after the execution of class method AspectJ @Around The logAround\(\ method will be executed before the class method To overcome the shortage of ti me and to run such a large process manually the whole process has to be automated Ant is a build tool where task can be written to automated various task 1\Making excuted JAR file 2\ompliation of  java class 3\ Clean Ant IV C ONCLUSION The application performs the operation on big data like counting average ratings, total recommendations, unique tags etc,  in optimal time and producing an output with 702 


minimum utilization of resources .  The data analysis and processing is used in a social networking application. Thus providing the necessary information to the application users with least effort V A CKNOWLEDGMENT We would like to thank Prof. Jyoti Nandimath for guiding us throughout the selection and implementation of the topic .we would also like to thank Mr.Divyansh Chaturvedi for his guidance and providing the resources for making this paper possible VI R EFERENCES  1  H a d o o p i n A c t i on  by C h uc k L a m 2  H a d o op t h e de f i ni ti v e g u i d e  by T o m  W h i t e pu bl is he r  O  REILLY 3  P r o Ha d o o p build scalable distributed applications in the cloudé by Jason Venner  4  Michael G Noll tuto rial s A p p l ied Rese arch Big Da ta  Distributed Systems. website: http://www.michael-noll.com 5 Exp e rt One-o n One J2EE Design and Developmenté by Rod Johnson 6  MK Yo n g Sp ri n g T u to rials 7 G ro o v y in A c tio n b y D ierk Konig with Andrew Glover, Paul KingguiIllaume Laforge, and Jon Skeet 8  Map R ed u ce: Sim p lif ied Da ta P r o cessin g o n L a rg e Clusters.Available at http://labs.google.com/papers/mapreduce osdi04.pdf  Mo ng oDB T h e Definitive Guide Sep 2010  rin g Hadoop ref e ren c e m a nu al b y  C o s t in  L e au  703 


are workloads from cloudrank sztod and hotregion are developed by ourselves. Group in the left consists of nine workloads, and all of them are from HiBench. The IPC of these sixteen workloads are range from 0.72 to 0.96, with an average value of 0.85. Wordcount has the lowest IPC value and hotregion has highest value among these workloads While the workloads are task parallel, the IPC is less than 1 This indicated the parallelism of these workloads is not fully exploited B  Cache miss ratio Instruction cache and Translation Look-side Buff \(TLB are two important components, and they provide fetch unit the capability to accelerate instruction fetch Figure 2 shows the L1 instruction cache miss ratios of sixteen workloads. The cache miss ratios of these typical workloads are range from 3.9% to 19.8%, with an average value of 8.9 Wordcount has the lowest L1 instruction cache miss ratio and ibcf has the highest L1 instruction cache miss ratio Figure 3 presents the L2 instruction cache misses of each workload, the cache misses value of these are range from 47.4% to 87.0%. On average, workloads from cloudrank in right side have larger L2 instruction miss rate then workloads from HiBench in the left side. Overall, the L2 cache is ineffective in our experiment platform The LLC is the largest on-chip component; its capacity has increased across each processor generation. Our results in Figure 4 show that LLC are one of the key limiters of these MapReduce based applications in our experiment. Both wordcount and svm have almost 100% LLC cache miss ratio C  Branch miss Prediction per instruction Branch miss prediction ratio affects the performance directly. Modern out-of-order processors use a functional unit to predict the next branch to avoid pipeline stalls. The pipeline continues whether or not depends on the result of branch predict. If the branch miss prediction occurs, it will cause a dozens of cycles waste due to the pipeline must flush the wrong instructions and fetch the correct ones Figure 5 presents the branch miss prediction ratio of each workload, these ratios are range from 1.5% to 5.6 with an average value of 2.7 Pagerank has the lowest branch miss prediction ratio while nutch indexing has the highest branch miss prediction ratio. The results show that the branch predictor of our processor matches these typical MapReduce based applications D  Off-chip bandwidth utilization Over the past decade; while the off-chip memory latency has slowly improved, off-chip bandwidth has sharply improved. The speed of the memory bus has increased from dozens of MHz to 1GHz, raising the peak theoretical bandwidth form 544MB/s to 17GB/s. Figure 6 plot the percore off-chip bandwidth utilization of these typical workloads we have chosen. As the figure 6 depicted, among these workloads we evaluated, terasort is the only one that has the highest utilization ratio with a value of 14%. Overall in our experiment platform, processors significantly overprovision off-chip bandwidth for these typical workloads V  W ORKLOADS S IMILARITY  In this section, we choose seven micro-architecture metrics to analyze similarity in typical workloads. These metrics are calculated by their performance counters respectively, detail information of these metrics for each workload on four different machines are as follows: \(1 2\L1 instruction cache miss per instruction; \(3\2 instruction cache miss per instruction; \(4 L3 cache miss per instruction; \(5\ Off-chip bandwidth utilization; \(6\ number of branches per instruction; and \(7 number of mispredicted branches per instruction. We measure fundamental workloads characteristics related to their data locality, branch predictability, and instruction locality by hardware performance counters. Since our experiments are carried out on four different nodes, we take each metric-node pair as a variable, in our experiment; we have 28 variables due to there are four nodes and seven metrics for each workload Firstly, we use PCA remove the correlations between this metrics. And then apply hierarchical clustering algorithm to metrics of these workloads. Finally, we analyze the results according to two dendrograms A  Pricinpal component analysis Considering twenty-eight variables for each workload from different metric-machine pair, it is impossible to simultaneously look at all experiment data and draw meaningful conclusions from them. Therefore, PCA is used for analyzing the data. Firstly, the data is normalized to a unit normal distribution for isolating the effect of varying ranges of each parameter PCA helps to reduce the dimensionality of a data set without too much original information loss. PCA computes a new variable set; these new variables are uncorrelated to each other in this set, new variable in this set are linear combinations of original variables, generally, we take each new variable as a principal component [9 The number of principal components is less than or equal to the number of original variables. This transformation is defined in such a way that the first principal component has the largest possible variance \(that is, accounts for as much of the variability in the data as possible\, and each succeeding component in turn has the highest variance possible under the constraint that it be orthogonal to \(i.e., uncorrelated with the preceding components. Principal components are guaranteed to be independent if the data set is jointly normally distributed. PCA is sensitive to the relative scaling of the original variables [19   B  Clustering and Kmeans There are two popular used clustering techniques hierarchical clustering and K-means clustering. Both of them can be used to group workloads with similar features  122 


 sort-30G sort-60G sort-15G terasort-100G terasort-50G bayes terasort-25G sztod-98G pagerank hotregion-17G hotregion-35G grep-80G hive-join sztod-49G svm-40G grep-20G hotregion-70G hive-aggre wordcount-15G wordcount-30G wordcount-60G k-means sztod-24G ibcf-8G hmm-16G ibcf-4G hmm-32G hmm-8G svm-20G grep-40G ibcf-2G svm-10G fpg nutchindexing 2 3 4 5 6 7 8 Linkage Distance a b c sort terasort bayes grep pagerank hotregion hive-join k-means hive-aggre hmm wordcount svm fpg sztod ibcf nutchindexing 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 Linkage Distance    Figure 7  Dendrogram showing similarity between typical MapReduce workloads K-means clustering is a method of vector quantization originally from signal processing. K-means clustering aims to partition n workloads into k clusters in which each workload belongs to the cluster with the nearest mean, where K is a value specified by user. Therefore, we need to cluster workloads for different values of K and then select the best fit due to different grouping possibilities [20   Hierarchical clustering is a "bottom up" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. It is useful in simultaneously looking at multiple clustering possibilities and use can use a dendrogram for selecting desired number of clusters. It start with a matrix of distance between N cases or workloads, this distance is the Euclidean distance between the workloads metrics [20   In our experiments, we use hierarchical clustering algorithm to find out a representative subset of these typical workloads aforementioned C  Workloads similarity Generally, researchers and customers partially use workloads of a specific benchmark suite due to simulation constraints, system call or hardware circumstance issues It would be beneficial to cut down the simulation or evaluation time by finding out a valid minimal subset of a representative, which includes the same characteristics as that benchmark suite dose and without missing diversity  This section demonstrates the results of analyzing similarity in typical MapReduce based workloads. Figure 7 shows a dendrogram for typical workloads listed in Table I after applying PCA and Hierarchical Clustering on the seven metrics. We measure dissimilarity between workloads by Euclidean distance and create a dendrogram by singlelinkage distance. In Figure 7 the horizontal axis shows the linkage distance indicating the similarity and dissimilarity between workloads. Workloads that are outliers have larger linkage distances with the rest of the clusters formed in a hierarchical way. Workloads are positioned close to each other when the distance is smaller, and the ordering on the Y-axis does not have particular significance  Figure 8  Dendrogram showing similarity between input size for each workload We can clearly see from Figure 7, a sort and terasort as a workload pair has the smallest distance to each other in these workloads, as well as bayes and grep We can infer that both of them share many characteristics. b sztod as typical program in our internal project based on trajectory data processing, which has almost largest linkage distances with the rest workloads in the clusters. We can conclude that this program have inherent properties, which make it dissimilarity to the rest workloads Researchers and users can select a representative subset of these workloads by using this dendrogram. For example if researchers want to reduce a subset includes just ten members; they can draw a vertical line at linkage distance of 4, it will yield a subset of ten workloads Figure 8 shows a dendrogram for typical workloads with different data volume. We apply three different input data sets to nine workloads due to constraint of experiment time This workloads are a sort  terasort and wordcount which included in HiBench; b hotregion and sztod which included in our internal project; c grep  hmm  ibcf and svm  which provided by cloudrank. We perform on this data to find similarity between input data for sets these workloads Workloads with three data sets are represented by the name combine with input size As this figure 8 depicted, on the one hand, in some workloads, different input data sets appear clustered together. For example, the largest linkage distance in three input sets of sort is less than 2\(as the vertical line a marked terasort with different input sets in a same cluster when linkage distance is less than 3\(as the vertical line b marked On the other hand, some input sets are very different from the other input sets of the same workloads. For example, three input sets of hmm is dissimilarity to each 123 


other according to their linkage distance, as well as svm and ibcf  We can conclude that a few workloads have inherent property and they are tending towards stability when the data volume increases to a certain extent VI  W ORKLOADS BASED ON SPATIO TEMPORAL DATA  In this section, we present an initial idea of a big data benchmark suite for spatio-temporal data Shenzhen is a famous international city in south china which covers an area of almost 2000 square kilometers. And there are more than eighteen millions of people living in it Therefore, on each day, an amount of spatio-temporal data is produced from subway transportation system, taxicab transportation system and public bus transportation system In particular, our data center collects and stores more than 90 million of GPS records on a daily basis. These GPS data and smart transaction data offers us the opportunity to investigate and understand the demand pattern of passengers and service level from transit agencies There are five typical workloads in our big data system for analyzing spatio-temporal data. All of them are used for traffic flow analysis and spatio-temporal data mining Currently, we just release two typical workloads due to business confidentiality. These two workloads and their input data are released on the web page http://cloud.siat.ac.cn/trajactory-data\ And we will attempt to release the rest typical workloads with the permission of our collaborator A  Transaction Data of  smart card Sztod there are five different subway lines, 118 subway stations and more than 20 million active smart cards in Shenzhen. On average, we collect 15 million transaction records in one day from subway transportation system. The smart card readers register passengers when they enter or leave a subway station. The information from the readers includes card id, flag of entrance or exit, timestamp, id of smart card reader and subway station name  The algorithm used for data processing is as follows 1. Assign a subway station pair and a specific direction count the number of people who enter the origin station and leave the destination station 2. Repeat step 1 until all pairs of subway station are computed 3. Find the top N station pairs with largest counter B  GPS position of  Taxicab Hotregion at a time interval range from ten seconds to thirty seconds, more than 28000 taxicabs equipped with GPS device report their position information to our system in real time. Each record report from these taxies has four primary fields; these fields are timestamp taxicab id  longitude and latitude The algorithm used for data processing is as follows 1. Divide the area into 10000*10000 grids according to latitude and longitude 2. Assign a time range; count the number of taxicab for each grid 3. Repeat the step 2 until all grids are processed 4. Find the top N grids with largest counter of taxicab VII  C ONCLUSIONS AND FUTURE WORK  The results show that there are characteristic redundancies between these existing MapReduce based workloads under the condition of a specific data volume range. For example sort and terasort are clustered into one cluster due to the workload-pair sharing many same characteristics in micro-architecture level, as well as bayes  and grep  Sztod a workload for processing trajectory data from a subway transportation system, which has inherent proprieties and these properties make it obvious dissimiliar to the rest typical workloads we have chosen A few workloads such as terasort and horegion are tending towards stability in micro-architecture level metrics when the data volume increases to a certain extent In future work, our research steps are as follows First, we will further select typical MapReduce based workloads from various application domains to conduct similarity analysis Second, we would like to present a big data benchmark suite for spatio-temporal data R EFERENCES   1  http://parsa.epfl.ch/cloudsuite/cloudsuite.html 2  http://hadoop.apache.org/mapreduce/docs/current/gridmix.html 3  ICTBench home page. http://prof.ict.ac.cn/ICTBench 4  S. S. Huang, J. Huang, J. Q. Dai, T. Xie, and B. Huang. ìThe HiBench benchmark suite: Characterization of the MapReduce-based data analysisî. Huang, Shengsheng, Jie Huang, Jinquan Dai, Tao Xie and Bo Huang. "" In Data Engineering Workshops \(ICDEW\ 2010 IEEE 26th International Conference on, pp. 41-51. IEEE, 2010 5  Hive home page. http://hive.apache.org 6  DCBench home page. DChttp://prof.ict.ac.cn/DCBench 7  BigDataBench  home page. http://prof.ict.ac.cn/BigDataBench 8  CloudRank  home page. http://prof.ict.ac.cn/BigDataBench 9  A. Phansalkar, A. Joshi, and L. K. John, ìAnalysis of redundancy and application balance in the SPEC CPU2006 benchmark suiteî. 2007 In Proceedings of the 34th annual international symposium on Computer architecture \(ISCA '07\. ACM, New York, NY, USA, 412423   J. Q. Dai et al. ìHitune: dataflow-based performance analysis for big data cloudî. Proc. of the 2011 USENIX ATC \(2011\-100   W. Gao, et al. ìBigDataBench: a Big Data Benchmark Suite from Web Search Enginesî. The Third Workshop on Architectures and Systems for Big Data \(ASBD 2013\in conjunction with  ISCA 2013   C. Luo, et al. ìCloudRank-D: Benchmarking and Ranking Cloud Computing Systems for Data Processing Applications. Fronters of Computer  Science, 2012, 6\(4\: 347ñ362   Z. Chen et al. ìCharacterizing OS behavior of Scale-out Data Center Workloads.  Seventh Annual Workshop on the Interaction amongst Virtualization, Operating Systems and Computer Architecture WIVOSCA 2013\. In Conjunction with ISCA 2013   Z, Jia, et al. ìCharacterizing Data Analysis Workloads in Data Centersî. 2013 IEEE International Symposium on Workload Characterization IISWC-2013   H. Xi et al Characterization of Real Workloads of Web Search Engines 2011 IEEE International Symposium on Workload Characterization IISWC-2011 124 


  Z. Jia et al. ìThe Implications of Diverse Applications and Scalable Data Sets in Benchmarking Big Data Systemsî. Second workshop of big data benchmarking \(WBDB 2012 India\ & Lecture Note in Computer Science \(LNCS   Y. Chen et al, ìWe Donít Know Enough to make a Big Data Benchmark suiteî. Workshop on Big Data Benchmarking. 2012   J. Zhan et al, ìHigh volume computing: Identify and characterizing throught oriented workloads in data centersî. In Parallel and Distributed processing Symposium Workshops & PhD Forum IPDPSW\, 2012 IEEE 26 th International pages 1712-1721. IEEE 2012   http://en.wikipedia.org/wiki/Principal_component_analysis   http://en.wikipedia.org/wiki/K-means_clustering   125 


overhead of job initialization in Hadoop is much larger than cNeural VIII C ONCLUSION AND F UTURE W ORK The past several years have witnessed an ever-increasing growth speed of data To address large scale neural network training problems in this paper we proposed a customized parallel computing platform called cNeural Different from many previous studies cNeural is designed and built on perspective of the whole architecture from the distributed storage system at the bottom level to the parallel computing framework and algorithm on the top level Experimental results show that cNeural is able to train neural networks over millions of samples and around 50 times faster than Hadoop with dozens of machines In the future we plan to develop and add more neural network algorithms such as deep belief networks into cNeural in order to make further support training large scale neural networks for various problems Finally with more technical work such as GUI done we would like to make it as a toolbox and open source it A CKNOWLEDGMENT This work is funded in part by China NSF Grants No 61223003 the National High Technology Research and Development Program of China 863 No 2011AA01A202 and the USA Intel Labs University Research Program R EFERENCES  C Bishop Neural networks for pattern recognition  Clarendon press Oxford 1995  J Collins Sailing on an ocean of 0s and 1s  Science  vol 327 no 5972 pp 1455Ö1456 2010  S Haykin Neural networks and learning machines  Englewood Cliffs NJ Prentice Hall 2009  R Hecht-Nielsen Theory of the backpropagation neural network in Proc Int Joint Conf on Neural Networks,IJCNN IEEE 1989 pp 593Ö605  Y  Loukas  Artiìcial neural netw orks in liquid chromatography Efìcient and improved quantitative structure-retention relationship models Journal of Chromatography A  vol 904 pp 119Ö129 2000  N Serbedzija Simulating artiìcial neural netw orks on parallel architectures Computer  vol 29 no 3 pp 56Ö63 1996  M Pethick M Liddle P  W erstein and Z Huang P arallelization of a backpropagation neural network on a cluster computer in Proc Int Conf on parallel and distributed computing and systems PDCS  2003  K Ganeshamoorthy and D Ranasinghe On the performance of parallel neural network implementations on distributed memory architectures in Proc Int Symp on Cluster Computing and the Grid CCGRID  IEEE 2008 pp 90Ö97  S Suresh S Omkar  and V  Mani P arallel implementation of back-propagation algorithm in networks of workstations IEEE Trans Parallel and Distributed Systems  vol 16 no 1 pp 24Ö34 2005  Z Liu H Li and G Miao Mapreduce-based backpropagation neural network over large scale mobile data in Proc Int Conf on Natural Computation ICNC  vol 4 IEEE 2010 pp 1726Ö1730  M Glesner and W  P  ochm  uller Neurocomputers an overview of neural networks in VLSI  CRC Press 1994  Y  Bo and W  Xun Research on the performance of grid computing for distributed neural networks International Journal of Computer Science and Netwrok Security  vol 6 no 4 pp 179Ö187 2006  C Chu S Kim Y  Lin Y  Y u  G  Bradski A Ng and K Olukotun Map-reduce for machine learning on multicore Advances in neural information processing systems  vol 19 pp 281Ö288 2007  U Seif fert  Artiìcial neural netw orks on massi v ely parallel computer hardware Neurocomputing  vol 57 pp 135Ö150 2004  D Calv ert and J Guan Distrib uted artiìcial neural netw ork architectures in Proc Int Symp on High Performance Computing Systems and Applications  IEEE 2005 pp 2Ö10  H Kharbanda and R Campbell F ast neural netw ork training on general purpose computers in Proc Int Conf on High Performance Computing HiPC  IEEE 2011  U Lotri  c and e a Dobnikar A Parallel implementations of feed-forward neural network using mpi and c on  net platform in Proc Int Conf on Adaptive and Natural Computing Algorithms  Coimbra 2005 pp 534Ö537  Q V  Le R Monga and M e a De vin Building high-le v e l features using large scale unsupervised learning in Proc Int Conf on Machine Learning ICML  ACM 2012 pp 2Ö16  J Ekanayak e and H e a Li T wister a runtime for iterati v e mapreduce in Proc of the 19th ACM International Symposium on High Performance Distributed Computing  ACM 2010 pp 810Ö818  Y  Bu B Ho we M Balazinska and M D Ernst Haloop Efìcient iterative data processing on large clusters Proc of the VLDB Endowment  vol 3 no 1-2 pp 285Ö296 2010  M Zaharia M Cho wdhury  T  Das A Da v e  J  Ma M McCauley M Franklin S Shenker and I Stoica Resilient distributed datasets A fault-tolerant abstraction for in-memory cluster computing in Proc USENIX Conf on Networked Systems Design and Implementation  USENIX Association 2012 pp 2Ö16 384 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


