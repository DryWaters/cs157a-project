 Received January 6 2017 accepted February 3 2017 date of publication February 24 2017 date of current version March 13 2017 Digital Object Identifier 10.1109/ACCESS.2017.2669243 Dynamic Model Adaptive to User Interest Drift Based on Cluster and Nearest Neighbors BAOSHAN SUN AND LINGYU DONG School of Computer Science and Software Engineering Tianjin Polytechnic University Tianjin 300387 China Corresponding author B Sun 050sunbaoshan@tipu.edu.cn\051 This work was supported in part by the National Natural Science Foundation of China under Grant 61173032 and in part by the Science and Technology Commissioner Project of Tianjin under Grant 15JCTPJC58100 ABSTRACT A recommendation system provides personalized recommendations on products and services to users In the traditional recommendation system the user interest is regarded as constant over time while in fact the user interest changes over time Hence tracking the user interest drift becomes key in designing the dynamic recommendation system However it is a challenge to 034nd an accurate and effective method that can predict the user interest drift To solve the prediction problem of the user interest drift this paper adopts clustering and time impact factor matrix to monitor the degree of user interest drift in the class and more accurately predict an item's rating We add a time impact factor to the original baseline estimates and use the linear regression to predict the user interest drift Our comparative experiments are conducted on three big data sets MovieLens100K MovieLens1M and MovieLens10M The experimental results show that our proposed approach can ef\034ciently improve the prediction accuracy INDEX TERMS Collaborative 034ltering recommender systems concept drift time weight MovieLens I INTRODUCE With the development of information technology and Internet people gradually entered an information-explosion era from the past information-scarce one Everyday people will get a lot of information from different ways but most are useless noise and valuable information is mixed in the information noise In order to improve the ef\034ciency people need to 034lter out useless information by some 034ltering techniques When user has clear demands classi\034ed directory and search engine can be a good way to solve the problem of information overload Ho we v er  most user demands are not clear in real life recommendation system came into being Recommendation system uses user modeling and user historical data to predict the user's favorite information This method speeds up transmission ef\034ciency of useful information and highlights the individuation comparing with friend recommendation At the same time recommendation system is advantageous to the information producer to carry on the user population localization Recommendation system is often divided into several categories Content-based the user will be recommended the items that are similar to his favorite items in the past Collaborative 034ltering the user will be recommended the items that are liked by people who have the same interests and hobbies with the user Hybrid it combines two recommendation models to make recommendations Collaborative 034ltering recommendation system has received more attention in the past ten years because it doesn't require too much professional knowledge and has the ability to discover models which are more complex and dif\034cult to be discovered Collaborative 034ltering system has two classical models nearest neighbor model and latent factor model Nearest neighbor model uses neighbor relationships between people or between items In user-based neighbor model the user's nearest neighbors are used to predict the item's rating that the user will give In item-based neighbor model the user's rating for the item is predicted by the item's nearest neighbor ratings Nearest neighbor model pays more attention to the relationships between people or between items instead of the relationships between people and items thereby it reduces a lot of complex calculations But nearest neighbor model has its own shortcoming it doesn't carefully analyze the users rating so that this model is good at recommending item and not good at predicting rating Because use nearest neighbors are generated by clustering If user u doesn't rate item i  most neighbors of user u don't rate item i  We can't get more accurate rating prediction by nearest neighbor method 1682 2169-3536 012 2017 IEEE Translations and content mining are permitted for academic research only Personal use is also permitted but republication/redistribution requires IEEE permission See http://www.ieee.org/publications_standards/publications/rights/index.html for more information VOLUME 5 2017 
 


B Sun L Dong Dynamic Model Adaptive to User Interest Drift Based on Cluster and Nearest Neighbors  After the Net\035ix Prize latent factor model has been rapidly developed Compared with nearest neighbor model latent factor model pays more attention to the implied semantics of the data set The most common one is the matrix decomposition model Rating matrix is decomposed into users features and items features two matrices It can predict the ratings by using the matrix multiplication Latent factor model try to use user features and item features to explain the reason of user ratings Latent factor model is an effective prediction model that can be applied to the most recommender systems However the Latent factor model is dif\034cult to produce detailed recommendation reasons because user features and item features are dif\034cult to express Initial latent factor methods such as SVD SVD CC  have a fatal 035aw that it can't capture the user interest drift These methods simply consider the ratings without considering the time Therefore we call these methods are static methods The user's preference for item is always changing with time or place which leads to frequently rede\034ne the user's interest Dynamic recommendation system become a new trend of the present recommendation system design And the concept of user interest drift becomes important Man y scientists have begun to study the dynamic recommendation system For example Keoren created capturing time drift model on the basis of the original papers to improving accuracy of recommenders in 2009 He considers user interest drift in the aspects of user item nearest neighbor and matrix decomposition which makes the model quite complicated And Koren believes items that are rated by same user are related and these relationships can be calculated in data set but it's not the truth Wei et al  proposed a time-aw are collaborati v e 034ltering framework for making recommendations based on user feedback data collected over time Several effective time weighting techniques are applied to the prediction algorithm But time weights only affect nearest neighbor model and its time decay model is too simple In 2014 Lin and Liu 034nished the user model prediction with an item clustering method But they use a single time function and they use an inappropriate method to prevent over-\034tting in their algorithm Motivated by above problems we fully consider interest drift characteristic and utilize clustering and time decay model to overcome the problems In this paper we propose a model that uses the time impact factor matrix to predict user's rating we call it dynamic time drift model 050DTDM\051 The contributions of this paper are summarized as follows 017 We apply the short-term long-term and periodic effects to the time impact factor matrix to improve the prediction accuracy 017 We present a novel idea that uses time distribution of dataset to calculate the time decay function 017 We improved the existing clustering algorithms improved clustering algorithm makes the element numbers relatively average in each class in order to achieve better prediction for user interest drift 017 We propose a new method to prevent over-\034tting to replace the least-square method in our algorithm 017 We experimentally show that our proposed algorithm signifcantly improve prediction effect compared with some mainstream recommendation algorithms The rest of this paper is organized as follows In section II we introduce some notations and basic methods In section III we introduce the overall structure of the DTDM algorithm In section IV we show the formula of core algorithm In section V through the contrast test results we carry out some detailed analyses and put forward some new ideas In section VI we adjust the parameters and the results of our algorithm are compared with some mainstream recommendation algorithms In section VII we describe the conclusion and future work II PRELIMINARIES A MEANING OF SYMBOL Our recommendation system uses movie rating data set we assume that users will use 1-5 ratings to represent their liking for items Our work is to establish a model that can accurately predict users ratings on the test set Movies dataset suppose m users and n items The size of the rating matrix is m 003 n  We use special symbols to distinguish between users items and time for users u  v  for items i  j  and for time t  Rating r u  i 050 t 051 indicates the user u for item i rating at time t  the higher the rating the more like We use the set S D f 050 u  i  t 051 j r u  i is known g to represent the information that is known The time stamp t is used to calculate the time impact factor B BASELINE ESTIMATES In the previous collaborative 034ltering system research a large number of users and items data have some characteristics Some people will give higher ratings than others and some of the items will get higher ratings than the similar items Keoren team calculate these impacts with a linear method and experiments show that this method is effective As shown in Eq.\0501\051 026 indicates the average rating of the whole data set b u  i indicates a linear estimate for unknown ratings b u  i D 026 C b u C b i 0501\051 The parameters b u and b i are expressed separately the observed deviation of user u and item i  In order to facilitate the calculation Keoren uses a simple method to calculate b u and b i  b i D P u V 050 u  i 051 2 S 050 r u  i 000 026 051  014 1 C jf u j 050 u  i 051 2 S gj 0502\051 b u D P i V 050 u  i 051 2 S 050 r u  i 000 026 000 b i 051  014 2 C jf i j 050 u  i 051 2 S gj 0503\051 014 1 and 014 2 are smoothing factors these parameter are required to adjust to achieve the best results We will use b u  i to 034ll sparse matrix VOLUME 5 2017 1683 
 


B Sun L Dong Dynamic Model Adaptive to User Interest Drift Based on Cluster and Nearest Neighbors  FIGURE 1 Algorithm framework of DTDM C SOLVING METHOD OF THE PSEUDO INVERSE MATRIX In the main algorithm we use the method of solving the analytical solution E.g R D T 001 X 0504\051 Rating matrix R and time impact factor matrix T are known we need to solve the time factor coef\034cient matrix X  Time impact factor matrix T is obtained by time set and rating set The calculation of T is described in section 4.1 When T is a square matrix we can use the adjoint matrix to 034nd the inverse matrix A 000 1 D A 003  j A j 0505\051 Then using Eq.\0506\051 X can be obtained T 000 1 001 R D T 000 1 001 T 001 X D X 0506\051 When T and the R matrix are non-square matrix we can't simply use the adjoint matrix to solve the inverse matrix and need to be based on the T matrix is the row full rank matrix or the column full rank matrix to solve the pseudo inverse matrix Suppose A is full rank matrix of m 003 n  A C is the pseudoinverse matrix if m  n V A C D A T 050 AA T 051 000 1 if m  n V A C D 050 AA T 051 000 1 A T III ALGORITHM FRAMEWORK OF DTDM In this section we describe algorithm framework shown in Fig.1 After reading the data set the value of b u  b i is calculated from the known rating set S  Eq.\0502\051 and Eq.\0503\051 Then the algorithm will complete the work of the item clustering At the same time we use time set to 034t time decay function Then algorithm establishes the time impact factor matrix T based on the item clustering result and time decay function We obtain the time factor coef\034cient matrix X by analytic solution In the optimization process the algorithm searches for user nearest neighbors and uses its time factor coef\034cient matrix X to prevent over-\034tting We will discuss details in the following steps A ITEM CLUSTERING Because of the movies set characteristics we can hardly 034nd the same user repeated ratings for a movie Without multiple  Algorithm 1 K-Mean Improved Algorithm  Require n vectors r 1  r 2  001 001 001  r n representing n items class number k c Ensure k c categories of items 1 Randomly initialize 3 003 k c class centers 2 Repeat 3 Allocate each item to the nearest center 4 For each class if having no item then randomly pick one item from the largest class to it 5 Recalculate the class centers 6 Until converged 7 Repeat 8 Incorporating A class that has the least amount of elements to B class that is nearest to A 9 Until the class number reduces to k c  ratings of the movie we can't judge whether the user still interested in this movie So we design another method to replace the observation of a movie that rated many times this method use the item clustering similar items will be in the same class using item neighbor ratings to indirectly predict user interest drift degree This prediction method will have a certain error In order to reduce the error two problems should be overcome in the item clustering 1 In the results of the item clustering elements are not very similar in each class so that the core part of our algorithm can't have its due effect 2 Elements in some classes are so scarce that it is dif\034cult for us to 034nd out the user interest drift  The main reason for the 034rst problem is the sparsity of the rating matrix We can use the MovieLens 100k data set as an example This data set has 100000 data 943 users and 1682 movies On average each user comments on 106 movies it accounts for 6.3 of total movies And each movie is commented by 59 users it accounts for 6.25 of total users In other words about 93.7 of the matrix is free If the rating matrix is directly used for clustering many similar movies may be not in the same class our algorithm may partly failure In order to solve this problem we intend to use the 034lling method as follows r  u  i 050 u  i 051  2 S D 026 C b u C b i 0507\051 r  u  i indicates linear prediction rating to 034ll empty rating meaning and calculation of 026  b u  b i have been mentioned in Eq.\0501\051 S D f 050 u  i  t 051 j r u  i is known g  We have found that this formula can achieve better result in comparative experiments In section V all expressions of 034lling method were compared and analyzed The second problem is that there are too few elements in a class to predict well the user interest drift We use a Kmeans improved algorithm shown in Algorithm 1 to solve the problem This algorithm is actually a combination of K-means algorithm and agglomerative hierarchical Algorithm We 034rst 1684 VOLUME 5 2017 
 


B Sun L Dong Dynamic Model Adaptive to User Interest Drift Based on Cluster and Nearest Neighbors  generate 3 003 k c classes every time we set the class containing the fewest elements incorporated into its nearest class until class number condensed into k c  The algorithm can effectively guarantee that the element number in each class is relative average B ESTABLISHING THE TIME IMPACT FACTOR MATRIX This part is the core of our algorithm we will explain it in section 4.1 and 4.2 but considering the continuity of the depiction we brie\035y describe here From Eq.\0501\051 rating can be divided into overall average 026  users deviation b u and items deviation b i  But it doesn't completely 034t the user rating They have great difference we assume that the difference comes from the time factor So we de\034ne R u  i D r u  i 000 026 000 b u 000 b i 0508\051 R u  i indicates the rating that is affected by the time factor This part of the rating can be positive or negative positive value indicates that user u loves item i more than the linear prediction negative value indicates that user u doesn't love item i more than the linear prediction In some ways we will use the method of matrix decomposition for function 034tting we take SVD as an example R u  i D p u 001 q i 0509\051 p u is a f dimensional feature vector related to the user u  q i is a f dimensional feature vector related to the item i  f is an arti\034cial parameter that needs to be adjusted But this model has a fatal 035aw the signi\034cance of p u and q i can not be fully explained and people can't change these parameters properly based on experience to reduce the over\034tting For example we make an assumption that the value of N th dimension is 3 in the p u  but you can't 034gure out what it means and you can't change this value arti\034cially In this paper we presents a time impact factor matrix that can be adjusted The advantage of the matrix is that we can further reduce the prediction error by manual adjustment it is different from the SVD method that the vector parameters can not be explained and can not be adjusted We have generated a non-square matrix T u  t composed of time impact factors So we use analytical solution instead of matrix decomposition to solve the problem Note the method of solving T C u  t we have talked about in 2.4 R u  i 050 t 051 D T u  t 001 X u  t 05010\051 T C u  t 001 R u  i 050 t 051 D T C u  t 001 T u  t 001 X u  t D X u  t 05011\051 C USER NEAREST NEIGHBOR SELECTION First we explain why to select the user neighborhood After solving the time factor coef\034cient matrix X u  t in using Eq.\05011\051 The process of obtaining analytical solution did not join the regularization constraints that may lead to over\034tting So we need to use the nearest neighbors time factor coef\034cient matrix and the original matrix to prevent over\034tting and make the prediction results more accurate In this part we use the k-nearest neighbor algorithm the general idea is to select a similarity calculation method then it calculates similarity between a user and other users and select the k n highest similarity users as the user's nearest neighbor elements k n is an arti\034cial value There are many kinds of similarity calculation methods in our test results we found that the performance of the Pearson correlation coef\034cient is better By using the Pearson correlation coef\034cient we obtain user u nearest neighbor similarity set S nei  and calculate the user u all nearest neighbors similarity weight w uv D s uv  P v 2 S nei s uv 05012\051 s uv indicates the similarity between u and v  w uv indicates the weight of v in the user u nearest neighbor set We will explain in detail the use of w uv in 3.4 D SOLVING THE PROBLEM OF OPTIMAL SOLUTION In solving the optimization problem the international leading method is using the least square method We take SVD as an example r u  i D 026 C b u C b i C p u 001 q i 05013\051 p u  q i are vectors with f parameters Every user have his own p u  and every item have its own q i  This algorithm eventually generate 050 m C n 051 003 f parameter space To prevent over-\034tting the mainstream algorithms adopt the method of adding penalty term as shown in Eq.\05014\051 min X 050 u  i 2 S 051 050 r u  i 000 026 000 b u 000 b i 000 p u 001 q i 051 2 C 025 1 050 j p u j 2 C j q i j 2 C b 2 u C b 2 i 051  05014\051 025 1 is a regularization parameter After adding a penalty term the algorithm not only is required to high 034tting for the rating but also has some restrictions on p u  q i  b u  b i  In the least square method algorithm requires multiple iterations to reach an optimal solution and achieve the accurate 034tting Our algorithm use the analytical solution method the advantage of analytic solution is that it doesn't need multiple iterations algorithm can achieve a relatively stable value after one iteration However the analytical solution method also has its own problem that it can't use the Eq.\05014\051 to prevent over-\034tting Therefore we used an alternative approach to prevent over-\034tting this method use the user nearest neighbors weighting mentioned in 3.3 This method is proved to be effective on preventing over-\034tting in the experiment Neighbor weighting formula is as follows N X u  t D 0501 000 013 051 001 X u  t C 013 001 X v 2 S w uv X v  t 05015\051 X u  t indicates the time factor coef\034cient matrix of user u calculated by Eq.\05011\051 X v  t indicates the time factor coef\034cient matrix of user v that is the neighbor of user u  w uv indicates the weight of user v for user u that can be calculated by Eq.\05012\051 Using Eq.\05015\051 we can get an adjusted N X u  t matrix VOLUME 5 2017 1685 
 


B Sun L Dong Dynamic Model Adaptive to User Interest Drift Based on Cluster and Nearest Neighbors  Adjusted N X u  t is closer to the real situation It can prevent the over-\034tting problem caused by the analytical solution 013 is an adjustable parameter of 0-1 IV ALGORITHM CORE In this section we show the core part of our algorithm In 3.2 we just do a simple introduction In this section we will divide it into two parts to make complete explanation A TIME IMPACT FACTOR CALCULATION FORMULA In 3.2 we mentioned the time impact factor matrix but did not refer to the form of this matrix and the calculation formula First of all we give a detailed explanation of the Eq.\05010\051 R u  i 050 t 051 D T u  t 001 X u  t 05010\051 R u  i 050 t 051 indicates the rating that user u give the item i in time t  Note that the R u  i 050 t 051 is obtained according to the Eq.\0508\051 and is the time-related rating T u  t is the time impact factor matrix which indicates the preference of user u for each class at t time X u  t is the time factor coef\034cient matrix which provides us a linear combination coef\034cient This matrix is obtained by solving the analytic solution Here we explain generating method of T u  t  We assume that there are three models affecting the user rating for the item they are time interval number interval and seasonal impact For the three models we come up with three formulas to calculate the degree of impact Three time factors are calculated as follows respectively T 1 u  k  t D P i 2 k R u  i 050 a 001 e 000 025 j t 000 t 0 j C b 051  P i 2 k 050 a 001 e 000 025 j t 000 t 0 j C b 051 05016\051 In Eq.\05016\051 we calculate the impact of time interval for user u  T 1 u  k  t indicates the time interval impact factor of user u to class k in time t  t 0 indicates the time of rating item i  a  b and 025 are adjusting parameters these will be introduced in details in the section 4.2 When there are a lot of items belonging to the same class k  the longer the time interval between t and t 0 is the smaller the impact on the user's preferences will be the shorter the time interval of t and t 0 is the greater the impact on the user's preferences will be T 2 u  k  l D P i 2 k R u  i 050 a 001 e 000 025 j l 000 l 0 j C b 051  P i 2 k 050 a 001 e 000 025 j l 000 l 0 j C b 051 05017\051 In Eq.\05017\051 we calculate the impact of number interval for user u  T 2 u  k  l indicates the number interval impact factor of user u to class k  We sort items which the user u commented with the time sequence According to the location of the item in the user's time sequence algorithm arranges the corresponding position parameters l to each item In our framework T 1 u  k  t and T 2 u  k  l are complementary to each other then the algorithm can achieve a relatively stable prediction T 1 u  k  t is more in favor of a long-term stable time factor it is a function that change over time and is not affected by the user rate the other items so it has its own shortcomings After user u watched a movie of k category,user u watched 5 movies of the k C 1 category But T 1 u  k  t does not have a mechanism for the number interval the user interest maybe have changed but the model does not detect the change in time The T 2 u  k  l can solve this problem it is more in favor of a short time factor it also has its own problems it can't be monitored in time interval When T 1 u  k  t and T 2 u  k  l are combined the combination of temporary interest drift and long term interest drift is completed On this basis we try to 034nd the periodic trend of rating so we made another time impact factor function T 3 u  k  t  T 3 u  k  t D P i 2 k R u  i 001 cos 0502 031 001 t 000 t 0  86400 003 365 051  P i 2 k cos 0502 031 001 t 000 t 0  86400 003 365 051 05018\051 In Eq.\05018\051 we calculate the impact of seasonal factors for user u  T 3 u  k  t indicates the seasonal impact factor of user u to class k  We assume that u users rated some items that belong to k class in time t  then on the same day next year T 3 u  k  t will have higher value but after six months from the time t  the T 3 u  k  t will have the lower negative weight This formula guarantees that the function has the cyclical changes so that it can predict some item categories that has periodic characteristics We can de\034ne the time impact factor matrix T u  t under the impact of three factors T u  t D  T 1 u  1  t  T 2 u  1  l  T 3 u  1  t  T 1 u  2  t  T 2 u  2  l  T 3 u  2  t  001 001 001  T 1 u  k  t  T 2 u  k  l  T 3 u  k  t  Through the time impact factor matrix that we give we can see that each class have their own T 1 u  k  t  T 2 u  k  l  T 3 u  k  t  so each class will have their own X 1 u  k  t  X 2 u  k  l  X 3 u  k  t  The following is the expression of the time factor coef\034cient matrix X u  t  X u  t D  X 1 u  1  t  X 2 u  1  l  X 3 u  1  t  X 1 u  2  t  X 2 u  2  l  X 3 u  2  t  001 001 001  X 1 u  k  t  X 2 u  k  l  X 3 u  k  t  Note that T u  t  X u  t are for the user u  In other words each user has its own T u  t and X u  t vector B TIME DECAY FUNCTION FITTING In 4.1 Eq.\05016\051 and Eq.\05017\051 mentioned three parameters a  b  025  but we did not give their exact values because these three parameters are automatically adjusted based on the data sets and algorithm We propose a 034tting algorithm for the time decay function The main function is as follows f 050 t 051 D a 001 e 000 025 t C b 05019\051 1686 VOLUME 5 2017 
 


B Sun L Dong Dynamic Model Adaptive to User Interest Drift Based on Cluster and Nearest Neighbors  FIGURE 2 Ebbinghaus forgetting curve  Algorithm 2 Time Decay Factor Algorithm  Require Rating Set,Time Set Ensure a  b  025 1 Computing time span of the data set 2 Establish items-time matrix for storing the comment number of each item in every day 3 Fill items time matrix by the rating set and time set 4 Set a weight 015  N D 015 001 n 5 Calculate TOP 000 N items of everyday 6 Calculate the similarity of T day and T C t day as shown in Eq.\05020\051 7 Calculate the average similarity of the same time span as shown in Eq.\05021\051 8 Using the Exponential approximation 034tting function  In order to obtain the time decay function we made three reasonable assumptions 1 User interest drift will have some relationships with users time memory we take the German psychologist Ebbinghaus's forgetting curve as the core of the model We research the change characteristics of time forgetting curve model as shown in Fig 2 We think that the user's interest in the movie is the biggest and the interest value is 1 when he has just commented on this movie With the change of time the weight begin to decreased signi\034cantly and then tend to a steady value b  2 People interest have different drift speed in different data sets which leads to a result that different data sets have different timeliness For example People interest have different drift speed in the news data set and the movie data set 3 Interest drift speed for data set can be well re\035ected in data set daily TOP-N So we can use the similarity of TOP-N to estimate the regression function model in the algorithm to achieve the purpose that algorithm can automatically generate function model Based on three assumptions we made a relatively reasonable time decay factor algorithm which is given in Algorithm 2 In the algorithm we have given a parameter 015  The main purpose of this parameter is to control the value of N in the TOP 000 N  This value will be taken as a certain percentage of item number in the date set No matter how large or small data set is N will be a relatively reasonable value without major changes In algorithm step 6 we assume T can be any day in the time span and t indicates the number of time span between two speci\034ed days We need to show our similarity calculation formula We use TOPN T to represent the TOP 000 N list of the T day S T  T C t D j TOPN T 134 TOPN T C t j  j TOPN T  TOPN T C t j 05020\051 In algorithm step 7 we calculate the average similarity of the same time span as shown in Eq.\05021\051 A simple understanding that time span of the 034rst and second day is 1 time span of second and third days is 1 the algorithm calculates the average similarity of all time span 1 to make the algorithm result more reliable S t indicates the average similarity of the t time span S t D maxT 000 t 000 1 P T D 1 S T  T C t  j S T  T C t j 05021\051 At the end we get a series of data and a formula model but don't have parameter We use an exponential approximation to adjust the parameters V ALGORITHM DETAIL ANALYSIS In the previous part we have introduced the whole algorithm but the detail problems are not analyzed In this section we try to solve these problems through experiments we put forward some solutions to every problem Some of them are new ideas to solve these problems here to show you A RATING FILL METHOD In 3.1 we have shown that we use Eq.\0507\051 to 034ll matrix Due to the 034lling work the matrix becomes very dense and item clustering effect is improved but if you use a wrong 034lling method it doesn't improve the effectiveness of clustering We believe that the four formulas below is very effective r u  i 050 u  i 051  2 S D N r i 05022\051 r u  i 050 u  i 051  2 S D N r u 05023\051 N r i indicates the average rating of item i  N r u indicates the average rating of user u  Mean can represent the users general attitude to the items In our experiments it is found that these simple methods can obtain better results than the global mean 034lled or not 034lled The N r u 034lling will get better effect than N r i  Through our analysis the N r u can re\035ects the user's attitude better than the N r i in the item clustering r u  i 050 u  i 051  2 S D 026 C b u C b i 0507\051 Further we think out the Eq.\0507\051 to solve the 034lling matrix problem we found that this 034lling method has a better clustering effect After our analysis we believe that the personalized rating 034lling is a good way to enhance the clustering effect In Eq.\0507\051 the rating 034lling already has a very high personalization which will make the item clustering effect VOLUME 5 2017 1687 
 


B Sun L Dong Dynamic Model Adaptive to User Interest Drift Based on Cluster and Nearest Neighbors  more prominent With this research direction we found that this method is not the best personalized rating because the b u and b i aren't the best way to 034t We have come up with a weighted method of nearest neighbors rating r u  i 050 u  i 051  2 S D X v 2 U nei w uv r v  i 05024\051 U nei indicates the set of the user's nearest neighbors In our experiments we found that the Eq.\05024\051 is not very effective Through our research for the experimental data we think the problem is that we didn't use the 034lling method in the process of user clustering it lead to a bad result If user u has not rated item i  many nearest neighbors of user u has not rated item i  That leads to inaccurate rating so that the algorithm effect is not as good as the previous three algorithms Through this part of the research we have a certain understanding to 034ll method personalized prominent and stable 034ll method is easier to get a good clustering results B ITEM CLUSTERING METHOD In the item clustering we mainly solve 2 problems 1.Similarity algorithm of clustering 2 The clustering algorithm 1\051 SIMILARITY ALGORITHM OF CLUSTERING First we compare the similarity mainstream clustering algorithm respectively Euclidean distance cosine similarity Pearson correlation Euclidean formula as in Eq.\05025\051 E ij D v u u t  n X u D 1 050 r u  i 000 r u  j 051 2 05025\051 Cosine similarity formula as in Eq.\05026\051 sim 050 i  j 051 D P n 1 r u  i 001 r u  j  q  P n 1 r 2 u  i 001 q  P n 1 r 2 u  j 05026\051 Pearson correlation formula as in Eq.\05027\051 sim 050 i  j 051 D P n 1 050 r u  i 000 N r i 051 001 050 r u  j 000 N r j 051  q  P n 1 050 r u  i 000 N r i 051 2 001 q  P n 1 050 r u  j 000 N r j 051 2 05027\051 Comparing these algorithms in the experiments we found that Pearson correlation algorithm is better than the cosine algorithm and Euclidean distance After our analysis we think that the cosine algorithm does not take into account the relationship between the rating and the average rating it only considers the angle problem instead of the real neighborhood For example i 1  i 2  i 3 three items respectively get the same three users ratings ratings vector is i 1 0504,4,4\051 i 2 0501,1,1\051 i 3 0504,4,5\051 It is obvious that i 2 and i 3 are more similar but the cosine algorithm calculate that i 1 and i 2 are more similar Because this method doesn't take into account the relationship between the rating and the average rating which can show user attitude for the movie Euclidean distance appears similar problem Without considering the user attitude the clustering is not accurate Instead we can 034nd that Pearson coef\034cient take into account the problem of average rating Following this direction we 034nd another algorithm Adjusted Cosine Similarity Like Pearson correlation The range of Adjusted Cosine Similarity is 1 to 1 The difference between the adjusted Cosine Similarity and the Pearson correlation is the selection of the average rating We have mentioned above in fact the users attitude will be a great in\035uence on clustering Finally the experimental result of the adjusted Cosine Similarity is better than Pearson correlation Adjusted Cosine Similarity is given by Eq.\05028\051 sim 050 i  j 051 D P n 1 050 r u  i 000 N r u 051 001 050 r u  j 000 N r u 051  q  P n 1 050 r u  i 000 N r u 051 2 001 q  P n 1 050 r u  j 000 N r u 051 2 05028\051 2\051 THE CLUSTERING ALGORITHM In 3.1 we have mentioned that this algorithm is the combination of K-means algorithm and agglomerative hierarchical Algorithm But in the popular aggregation algorithm the algorithm will combine the two closest class At the beginning we used this aggregate algorithm In the process of the experiment we found that this aggregation algorithm is not very suitable for our algorithm because what we need is relatively big data amounts in each class that can be used to observe user interest drift If we use the popular aggregation algorithm we can 034nd that there are isolated point or small groups monopolizing a class which is not conducive to the observation of interest drift On this basis we use the mode of merging minimal class and its nearest class to replace another mode of merging the nearest class The advantage of this method is that the element numbers in each class are relatively average it facilitate the observation of interest drift C CALCULATION OF TIME IMPACT FACTOR In 4.1 we put forward three calculation formulas about time factor and give the time impact factor matrix We think that the three time impact models are effective But in the experiment we found that is not the case In the MovieLens data set the effect of using T 1 u  k  t  T 2 u  k  l is better than using T 1 u  k  t  T 2 u  k  l  T 3 u  k  t  Based on the results of this test the reasons are analyzed We believe that the main reasons for the T 3 u  k  t failure are as follows 1 The long time periodic characteristics of movie data set may be not exist  the majority of users will not change their own interests over season or month Therefore periodic change does not help predict very well 2 The periodic change model T 3 u  k  t is too simple or it is not suitable for the interest drift prediction after clustering D TIME DECAY FUNCTION FITTING In 4.2 we discuss a new method for obtaining time decay function In the hypothesis we think this is a good algorithm it can regulate time decay function based on different databases in order to better 034t the data set But the effect of the algorithm isn't good in the experiment We found that the time decay function can't achieve better results compared with taking a D 1  b D 0  025 D 1 directly Through the 1688 VOLUME 5 2017 
 


B Sun L Dong Dynamic Model Adaptive to User Interest Drift Based on Cluster and Nearest Neighbors  analysis of the experimental results we found that the following problems may affect the 034nal experimental result 0501\051 User selection MovieLens data sets are cleaned data sets users who rate less than 20 movies completely deleted in this data set In this case the statistics of TOP 000 N have a certain error this error can interfere the 034nal 034tting result 0502\051 Item selection MovieLens data sets not only clean users but also screen movies with certain rules this type of screening is arti\034cial We found that many movies already exist before the initial starting time of the data set The data set has a certain degree of closure There is a problem after the movie exists a year or a few years the number of evaluation will become stable it will affect the results and cause error 034tting 0503\051 Data set we choose relatively stable movie data sets rather than news data set that has more real-time effect In this stable data set the algorithm is very sensitive to the weight 015 of TOP 000 N  especially like MovieLens 100k that is small closed and stable data set The adjustable parameter of 015 became a problem remaining to be solved 0504\051 Algorithm in addition to the Eq.\05020\051 and Eq.\05021\051 we don't have a better method to calculate the similarity This similarity calculation uses average In the large data set the average similarity will lose its due effect because the similarity degree will be more closer to 015 after many days 0505\051 Model we have used the Eq.\0508\051 to remove the nontime impact factor In the process of eliminating We may have removed the part that is affected by the parameter b in Eq.\05019\051 which led to the parameters b can't play its effect VI PARAMETER ADJUSTMENT AND EXPERIMENTAL COMPARISON In this section we will do the work of Adjustment parameter and we will compare our algorithm with some mainstream algorithms A EVALUATION METRICS AND DESCRIPTION OF TEST SET To estimate the quality of each algorithm and compare these algorithms we have used internationally accepted measurement methods RMSE 050Mean Squared Error Root\051 and MAE 050Mean Absolute Error\051 RMSE D v u u t  P 050 u  i 051 2 S 050 r u  i 000 O r u  i 051 2  j S j 05029\051 MAE D P 050 u  i 051 2 S j r u  i 000 O r u  i j  j S j 05030\051 Compared with MAE method RMSE method increases the punishment of the large error r u  i indicates the actual rating of the item O r u  i indicates the predictive value of the item B DATASET PREPARING Our experiments are based on three well-known movie rating data sets MovieLens 100k MovieLens 1M and MovieLens 10M The three data sets were collected by the Uni v ersity TABLE 1 Characteristics of the datasets FIGURE 3 When clustering number k c is 5 Neighbor number and RMSE of Minnesota as a research project The advantage of using these data sets is that these data sets have been cleaned in advance which can ensure the low noise of the data set It helps to better observe the effectiveness of the algorithm We describe the related data of these data sets in table 1 In test set we use 034ve latest data of each users as the test set elements So MovieLens 100k test set has 4715 elements MovieLens 1M test set has 30200 elements MovieLens 100k test set has 357835 elements These data sets have considerable test sets which can detect the effectiveness of the algorithm C ADJUSTMENT OF PARAMETER In the section V we made the experiment method comparisons and also give the reasons for the effect improvement In the parameter adjustment stage we can only adjust parameters based on experimental results we 034rst made adjustments in a large range then we adjust further on these effective intervals There are two important parameters to be adjusted User neighbor number k n of k nearest neighbor algorithm and the clustering number k c of items clustering algorithm 1\051 ADJUSTMENT WORK OF k n In the choice of neighbor number k n  we 034rst tested from 0 to 50 with interval of 5 When the neighbor number k n is 0 we found the RMSE is 1.034084199 that is not a good prediction effect This result also indirectly proves that we can use the nearest neighbor model to prevent over-\034tting We found a small value when k n D 15 We have done experiments for 10-20 The results are shown in Figure 3 Finally we get the RMSE minimum 0.960140846 when k n D 17 2\051 ADJUSTMENT WORK OF k c In the choice of neighbor number k c  we 034rst tested from 1 to 15 with interval of 1 We found that the algorithm will reach a relatively stable value in clustering number k c is 3-7 The results are shown in Figure 4 After several tests we found that RMSE will reach the minimum value 0.921690948 when the k c D 4 VOLUME 5 2017 1689 
 


B Sun L Dong Dynamic Model Adaptive to User Interest Drift Based on Cluster and Nearest Neighbors  FIGURE 4 When Neighbor number k n is 17,Clustering number and RMSE D COMPARISON WITH MAINSTREAM ALGORITHMS In this section we will compare the DTDM algorithm with some mainstream static and dynamic algorithms 1\051 FUNK-SVD MODEL The whole rating is decomposed into two matrices which are user feature and item feature and using matrix multiplication predict unknown rating Its formula is as follows r u  i D p u 001 q i 05031\051 2\051 BiasSVD MODEL This is a model that is a combination of Linear estimate model and Funk-SVD model r u  i D 026 C b u C b i C p u 001 q i linear estimate model contains the global average 026  user deviation b u and item deviation b i  3\051 SVD CC MODEL On the basis of BiasSVD model koren added feedback of user's historical behavior r u  i D 026 C b u C b i C q T i 001 050 p u C 1  p  j N 050 u 051 j X j 2 N 050 u 051 y j 051 N 050 u 051 indicates movie set of user ratings y j is a vector that is used to measure the relationship between i and j  Note that the former three methods are static methods which don't have any time factors These methods do not obtain different predictions in different time But the dynamic method will consider the time parameters so we introduce to you the three mainstream dynamic methods 4\051 ITEM TIME MODEL This model is mainly to consider the user interest drift over time in the item level r u  i D 026 C b u C b i C b i  bin 050 t 051 In this model b i  bin 050 t 051 is divided into several stages In different stages the value is not the same so that this model has some dynamic characteristics TABLE 2 Experimental results of RMSE TABLE 3 Experimental results of MAE 5\051 TIME LINEAR MODEL This model considers not only the user interest drift in the item but also the user interest drift in the user r u  i D 026 C b u C 013 u dev u 050 t 051 C b i C b i  bin 050 t 051 013 u is a adjustment parameter dev u 050 t 051 calculation formula is as follows dev u 050 t 051 D sign 050 t 000 t u 051 001 j t 000 t u j 014 The timestamp t u is the user u rating time 014 is a adjustment parameter 6\051 TimeSVD CC MODEL This algorithm is a more complex dynamic model based on SVD CC model r u  i D 026 C b u 050 t 051 C b i 050 t 051 C q T i 001 050 p u 050 t 051 C 1  p  j N 050 u 051 j X j 2 N 050 u 051 y j 051 Through the formula below we can learn the calculation of b u 050 t 051  b i 050 t 051 and p u 050 t 051 b u 050 t 051 D b u C 013 u dev u 050 t 051 C b u  bin 050 t 051 C b u  period 050 t 051 dev u 050 t 051 D sign 050 t 000 t u 051 001 j t 000 t u j 014 b i 050 t 051 D b i C b i  bin 050 t 051 C b i  period 050 t 051 p u 050 t 051 D p u C 013 u dev u 050 t 051 C p u  t b u  period 050 t 051 and b i  period 050 t 051 indicate the periodic changes of users and items p u captures the stationary portion of the factor 013 u 001 dev u 050 t 051 approximates a possible portion that changes linearly over time and p u  t absorbs the very local day-speci\034c variability In three data sets we will compare our algorithm with the mainstream algorithms the comparison of our results are show in table 2 and table 3 After the experiment we found that our algorithm is better than some mainstream algorithms in these data sets But at the same time we also found that SVD CC model achieve better results than mainstream dynamic algorithms in MovieLens 1M and MovieLens 10M In our research we found that some 1690 VOLUME 5 2017 
 


B Sun L Dong Dynamic Model Adaptive to User Interest Drift Based on Cluster and Nearest Neighbors  users who rate less than 20 movies have not been deleted in MovieLens 1M and MovieLens 10M All of the dynamic algorithms are dependent on the time variation If the personal data set is very small the dynamic algorithm may not show better results than the static algorithm VII CONCLUSIONS In this paper we propose a new dynamic recommendation algorithm DTDM This algorithm classi\034es all items and predicts unknown ratings on each class in order to better rating prediction results we use the pseudo inverse matrix method to optimize DTDM algorithm and use the nearest neighbor method to prevent over-\034tting In the experiment we did some comparison of algorithm details At last we adjust the parameters and compare with some mainstream algorithms on three big data sets The experimental results show that the DTDM algorithm has excellent performance In the next step we need to 034nd more effective cycle impact factors and decay function 034tting algorithms in order to enhance the performance of the algorithm again ACKNOWLEDGEMENTS The authors would like to sincerely thank the reviewers for their insightful comments and very valuable suggestions REFERENCES   P Resnick N Iacovou M Suchak P Bergstrom and J Riedl GroupLens An open architecture for collaborative 034ltering of netnews in Proc ACM Conf Comput Supported Cooperat Work  1994 pp 175\025186   Y Zhang J Callan and T Minka Novelty and redundancy detection in adaptive 034ltering in Proc 25th Annu Int ACM SIGIR Conf Res Develop Inf Retr  2002 pp 81\02588   D Goldberg D Nichols B M Oki and D Terry Using collaborative 034ltering to weave an information tapestry Commun ACM  vol 35 no 12 pp 61\02570 1992   Y Koren Factorization meets the neighborhood A multifaceted collaborative 034ltering model in Proc ACM SIGKDD Int Conf Knowl Discovery Data Mining  Las Vegas NV USA Aug 2008 pp 426\025434   G Widmer and M Kubat Learning in the presence of concept drift and hidden contexts Mach Learn  vol 23 no 1 pp 69\025101 1996   Y Koren Collaborative 034ltering with temporal dynamics in Proc ACM SIGKDD Int Conf Knowl Discovery Data Mining  2009 pp 89\02597   S Wei N Ye and Q Zhang Time-aware collaborative 034ltering for recommender systems in Pattern Recognition  Berlin Germany Springer 2012   K Lin and D Liu Category-based dynamic recommendations adaptive to user interest drifts in Proc 6th Int Conf Wireless Commun Signal Process  2014 pp 1\0256   Y Koren Factor in the neighbors Scalable and accurate collaborative 034ltering ACM Trans Knowl Discovery Data  vol 4 no 1 2010 Art no 1   H Ebbinghaus Memory A contribution to experimental psychology Ann Neurosci  vol 20 no 4 pp 155\025156 2013   F M Harper and J A Konstan The MovieLens datasets History and context ACM Trans Interact Intell Syst  vol 5 no 4 2016 Art no 19   L Banda and K K Bharadwaj Evaluation of collaborative 034ltering based on tagging with diffusion similarity using gradual decay approach in Advanced Computing Networking and Informatics  vol 1 Basel Switzerland Springer International 2014 pp 421\025428   J Gaillard and J M Renders Time-sensitive collaborative 034ltering through adaptive matrix completion in Proc Eur Conf Inf Retr  Springer 2015 pp 327\025332   N Koenigstein G Dror and Y Koren Yahoo Music recommendations Modeling music ratings with temporal dynamics and item taxonomy in Proc ACM Conf Recommender Syst 050Recsys\051  Chicago IL USA Oct 2011 pp 165\025172   C Luo X Cai and N Chowdhury Probabilistic temporal bilinear model for temporal dynamic recommender systems in Proc Int Joint Conf Neural Netw 050IJCNN\051  Jul 2015 pp 1\0258   W Sen Z Xiaonan D Yannan A collaborative 034ltering recommender system integrated with interest drift based on forgetting function Int J u-and e-Serv Sci Technol  vol 8 no 4 pp 247\025264 2015   M Yedla S R Pathakota and T M Srinivasa Enhancing K-means clustering algorithm with improved initial center Int J Comput Sci Inf Technol  vol 1 no 2 pp 121\025125 2010   Y Zhang and Y Liu A collaborative 034ltering algorithm based on time period partition in Proc 3rd Int Symp Intell Inf Technol Secur Inform 050IITSI\051  2010 pp 1\0254   Y Zhang et al  Daily-aware personalized recommendation based on feature-level time series analysis in Proc 24th Int Conf World Wide Web ACM  2015 pp 1373\0251383   F Zhao Y Xiong X Liang X Gong and Q Lu Privacy-preserving collaborative 034ltering based on time-drifting characteristic Chin J Electron  vol 25 no 1 pp 20\02525 2016 BAOSHAN SUN received the Ph.D degree from Tianjin Polytechnic University China He joined the School of Computer Science and Software Engineering Tianjin Polytechnic University as an Associate Professor He has accomplished two scienti\034c research projects at national three projects at ministerial level and three transverse projects He has published more than ten papers in major academic journals participated in the publishing of three textbooks of computer science and won three patents for his inventions He has undertaken three national research projects and two provincial and ministerial projects His current research interests include the studies of machine learning national language processing and computer networks LINGYU DONG is currently pursuing the M.Eng degree with the School of Computer Science and Software Engineering Tianjin Polytechnic University He is currently involved in machine learning algorithms for recommendation systems His research interests include machine learning neural network data mining and recommendation systems VOLUME 5 2017 1691 
 


2327-4662 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/JIOT.2017.2664072, IEE\E Internet of Things Journal   en-US s   en-US  en-US  en-US  en-US  en-US      ia a a ng  12 6   T h e  n ee d  t o  co lle ct m ass iv e am o u n ts  e  at g ood 4  an d   1 2 7      en-US e    s          by   s    s     er      to en  3  1 2 8   is  o n e  o f  th e        0   7 3    T h e p r e p r o c es s ed  d ata  a r e  s av e d  in  C o u ch b ase   9  e  s  It   re ty    0   1 3 1    en-US   e   2   S p a r k  is  ch o s en  as pr    3  1 3 4      


2327-4662 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/JIOT.2017.2664072, IEE\E Internet of Things Journal   en-US       r ni t 13 7   en-US         en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US   M SED I O T PROC ESS  d  t  en-US  a  en-US     en-US  n  en-US  n  en-US    s  en-US    en-US  en-US     s  S   s  m n   t   e     e s   se   it  e s   mat y    s e  3    1 3 8    1 3 9    en-US  e d  s is  s    d s  o es      a  y a  3   1 3 8    en-US  t   e s  0    7 4    1 4 0    en-US   n   v   s  s    me e   s  c 0   7 3   7 4     en-US  si     ss t    n  n  en-US   d   en-US  n  en-US  s  en-US   en-US ff   en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US n   en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US   en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US   en-US DB  en-US ag e  en-US k    en-US  en-US n en-US  en-US  en-US  en-US n en-US  en-US ta  en-US ts  en-US  en-US  en-US w ta  en-US rs  en-US ta re g  en-US  en-US  en-US   en-US  en-US  en-US  en-US  


2327-4662 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/JIOT.2017.2664072, IEE\E Internet of Things Journal   en-US s        c 4    141    en-US   S S  en-US      e      en-US    en-US   e  en-US   en-US   en-US     en-US      e e        e fi                ies    e           en-US    ll          en-US   D  FOR M  ED I O  r  t  en-US s  en-US  s  s   e e  k  en-US s  en-US  s  Z  en-US m  en-US S  en-US t y  en-US i p  sh  k  en-US y  en-US c  en-US e  en-US 300m  en-US R e  en-US s  en-US  e  en-US O  r ies  en-US me  en-US s  en-US l  en-US  w d  l  en-US n  en-US 6 de  


2327-4662 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/JIOT.2017.2664072, IEE\E Internet of Things Journal   en-US s  en-US e  r  en-US g  en-US   h    c  en-US t  en-US t  en-US n  en-US b P  l  en-US s  en-US    en-US m  en-US k  e  en-US  ns  en-US c  l y  en-US y  en-US S s n  ms  n  en-US  r  en-US ja  en-US A T  en-US         en-US R ES  en-US 1  R r  y   en-US 2          en-US 3  san  P ss  s  t   en-US 4    y  en-US 5  J   y  2  en-US 6      en-US 7  r    s   en-US 8       en-US 9      pp  en-US 0  a       en-US 1    s   en-US 2  l     2  en-US 3  e   A v a i l a b l e   h t t p    w w w  z i g b e e  o r g   en-US 4      e       en-US 5   A  P      en-US 6    S   s  en-US 7    s  EEE    en-US 8  J      en-US 9      en-US 0  N       en-US 1  y  r  r     en-US 2  M a  e   o    s   en-US 3   s   en-US 4   P mas    s  en-US 5    o  n g   en-US 6   To   9    en-US 7     n    en-US 8  n  g   mat    en-US 9  M     en-US 0   Xu     s   en-US 1  L    EEE   en-US 2     of as    


2327-4662 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/JIOT.2017.2664072, IEE\E Internet of Things Journal   en-US   en-US 3  T   A of    s 1 5  en-US 4   I M       A  d   T       en-US 5  s   of T   s  0 15  en-US 6      4  en-US 7     en-US 8        en-US 9  h  A    en-US 0     5  en-US 1  G  i on   5  en-US 2  C   n   7  en-US 3    s 7  en-US 4  n    s    en-US 5   W  R     s   en-US 6  m       en-US 7  se   s  N o  en-US 8    s    en-US 9        2  en-US 0  n   g    en-US 1        en-US 2  z      en-US 3        en-US 4  S       en-US 5    g   en-US 6   En     en-US 7  K v   E    en-US 8      t  8  en-US 9  J   g    en-US 0  z  e   en-US 1  sal   N h  en-US 2    g  en-US 3  G  D    s   en-US 4     2   en-US 5     g  s  4   en-US 6    ms    pp  en-US 7      s 16  en-US 8     w   en-US 9     l  s  6   


2327-4662 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/JIOT.2017.2664072, IEE\E Internet of Things Journal   en-US 0  i     en-US 1        en-US 2       en-US 3     i ng   3  en-US 4        en-US 5  G      e  7   en-US 6    s    en-US 7      T   en-US 8   A  A    EW    en-US 9   g   en-US 0   e    en-US 1     4   en-US 2         en-US 3       62   en-US 4  H        en-US 5   S    en-US 6  A  Lo o   Pro     en-US 7  e  e  sch       en-US 8        en-US 9    t EEE   en-US 0       en-US 1  JP  N    en-US 2      5  en-US 3      e  ss  5  en-US 4  l    th a l  7  en-US 5   d    3 rd  g    en-US 6  s t   s    211  en-US 7      M  EEE  0  en-US 8  J       en-US 9   J  s EEE  en-US 0  o    s   en-US 1      en-US 2       en-US 3    n  en-US 4  D  S A   g    134   en-US 5    s  s in   en-US 6  r 366n   s    en-US 7  M  A  M b    


2327-4662 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/JIOT.2017.2664072, IEE\E Internet of Things Journal   en-US OS 1 19   en-US 8  b  R B   s    en-US 9  M  H  R   t     en-US 0     1  en-US 1  t     en-US 2   r     en-US 3    r     en-US 4   y   en-US 5   In y  EEE n     0   en-US 6    O a    A  2 0    en-US 7  a     en-US 8  P   d   P    en-US 9  A  B     n    en-US 0      en-US 1  N     y   en-US 2  u   L    50  en-US 3  j    D  EEE   6   en-US 4  P    of a   en-US 5    a I EEE  2  en-US 6    S    en-US 7    D    en-US 8    x     en-US 9   en-US 0      en-US 1   w e   A v a i l a b l e  h t t p    w w w  c o u c h b a se  c o m n o sq l  en-US 2  i   k e   A v a i l a b l e    en-US 3  ml en-US  en-US 4   k       en-US 5    g    en-US 6   y   en-US 7  s    k    en-US 8      en-US 9    e    en-US 0  i  re     en-US 1   A  X g   s    en-US  en-US                  en-US  en-US  en-US en-US     en-US en-US     en-US en-US   en-US en-US    


2327-4662 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/JIOT.2017.2664072, IEE\E Internet of Things Journal   en-US  g            en-US  en-US n     f  s U.K stle upo r     s  rs  en-US  en-US  en-US n               en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  


This article has been accepted for inclusion in a future issue of this j\ournal. Content is final as presented, with the exception of pagination EEE   19 S Gong Cheng om Xidian  in 2007 and the M.S. and technical  3   He is currently an Associate Professor with Northwestern Polytechnical University. His main research interests are computer vision and pat tern recognition ei Han ently techni ch The ersity cher at the Uni His omputer vision, multi and brain imaging analysis. He es such as IEEE T C t T IONS  ON P A t t T ERN  A A YSIS  AND M CHINE  I I N t T ELLIGENCE AMI I I N t T ER NA t T IONAL J OURNAL  OF  C C O m p MP U t T ER V ISION V T C t T IONS  ON  I I m M GE P SSING  TIP C C ONFERENCE  ON  C C O m p MP U t T ER V ISION  AND P A t t T ERN  R R OGNI t T ION VPR I I N t T ERNA t T IONAL  C C ONFERENCE  ON  C C O m p MP U t T ER V ISION V I I N t T ERNA t T IONAL J OIN t T  C C ONFER ENCE  ON  A A R t T IFICIAL  I I N t T ELLIGENCE IJCAI Prof. Han is an Associate Editor of the I E E E IEEE T RANSAC t T IONS  ON  H H U m M AN M ACHINE  S S YS t T E m M S  Neurocomputing   Processing and Machine Vision and Applications  u ently f  tor ests include emote sensing om e eas  international journal, including Neurocomputing Elsevier Cognitive  Computation Springer International Journal of Image and Graphics  World of Scientific 


