The Optimization and Improvement of the Apriori Algorithm  Yiwu Xie, Yutong Li, Chunli Wang, Mingyu Lu Information Science Technology Department Dalian Maritime University, Dalian, Liaoning, 116026, China dearliyutong@163.com   Abstract  Through the study of Apriori algorithm we discover two aspects that affect the e fficiency of the algorithm. One is the frequent scanning database, the other is large scale of the candidate itemsets. Therefore, IApriori algorithm is proposed that can reduce the times of scanning database optimize the join procedure of frequent itemsets generated in order to reduce the size of the candidate itemsets. The results show that the algorithm is better than Apriori algorithm  1. Introduction  Data mining also known as the knowledge discovery in database, it is a forecasting method to extract the hidden knowledge from the large-scale database or the data warehouse  Its main methods include: classification clustering, association analysis and so on  Mining association rules is one of the main contents of data mining research at present and emphasis particularly is finding the relation of different items in database. Apriori 3 is the most famous and basic method in mining association rules. The principle of Apriori algorithm is to find the valuable association rules whose support and confidence must satisfy the minimum support and confidence confirmed by user aforehand  2. Apriori algorithm description  Apriori algorithm is the most effective method that candidate k+1-itemsets may be generated from frequent k-itemsets according to the nature of Apriori algorithm that any subset of frequent itemsets are all frequent itemsets. The classical Apriori algorithm  L 1 large 1-itemsets for \(k=2; L k-1 002\003 k++\in C k apriori-gen\(L k-1  new candidate itemsets generated for all transactions t 004 D do begin C t subset\(C k t transaction t contains in the candidate itemsets for all candidates c 004 C t do c.count end L k c 004 C k c.count 005 minsup end Answer=\037 k L k   First, find the frequent 1-itemsets L 1 L 2 is generated from L 1 and so on, until no more frequent k-itemsets can be found and then algorithm ceases. In the algorithm each L k generated must scan database one time  Second, C k  is generated from L k-1 Every itemset in C k is tested whether its all k-1 subsets constitute a large k-1 itemset or not. Therefore, scanning large database many times and generating so many useless candidate itemsets are the bottleneck of the Apriori algorithm  3. Problem description  Apriori algorithm has three shortcomings below  First, because there are lots of transactions in the database, computing the frequencies of candidate itemsets must scan database frequently and spend much more time Second, C k is generated by joining two frequent itemsets that belong to L k-1 If we can reduce the operating frequencies, we can improve the efficiency of Apriori algorithm. Third, the expending in time and space for frequent itemsets is too much. For example, if we have 10 4 frequent 1-itemsets, we can get 10 7 frequent 2itemsets. So reducing the scale of C k can improve the efficiency of the Apriori algorithm greatly Therefore, it is necessary to delete the useless transactions in the database in order to reduce the scale of database and reduce itemsets generated from C k by the join procedure. This paper presents an improved Apriori algorithm called IApriori algorithm according to the analysis of   Apriori algorithm above  4. Improved apriori algorithm  Improved Apriori algorithm related in [9 s  database to compute the frequencies of candidate itemsets at the same time to mark the deleting tag if the size of 
International Symposium on Intelligent Information Technology Application Workshops 978-0-7695-3505-0/08 $25.00 © 2008 IEEE DOI 10.1109/IITA.Workshops.2008.170 1101 


transaction t is less than k. In [10, 11 sactio n  t d o e s  not contain any element of candidate itemsets C k the algorithm determines to mark the deleting tag on t, so we can skip over the record in next database scanning. The method that reduces the operating frequencies generating C k by joining two frequent itemsets that belong to L k-1 is proposed in T h e i m prov e m e n t i n 13] l i m i t s t h e s cal e  m+n \ of  C k in advance according to the numbers of rules that contain the total items which are respectively m and n Due to the generalization of algorithm, IApriori algorithm proposed in this paper does not input more thresholds to improve efficiency, only has improved from some aspects 1\provement method of optimizing the join procedure to reduce the size of candidate itemsets. When C k+1 is generated from L k  L k each item of the first L k is joining with every item in the second L k in classical Apriori algorithm. Since the k th item of the first L k is the same as the k th item of the second one, the k th item of the first L k is joining with the k+1 th item of the second one when C k+1 is generated from L k  L k Only in this way can we spend less time than before For example: L 1 I 1 I 2 I 3 I 4 Apriori algorithm needs computing 16\(4*4\times but it only needs 6\(3+2+1\imes in IApriori algorithm 2\Improvement method of reducing the scale of database. Through the process obtaining C k+1 from L k if the size of transaction t is less than k, we can say that it is useless for generating C k+1 if transaction t does not contain any subset of candidate itemsets C k mark the transaction t the deleting tag IApriori algorithm reduces the scale of database and optimizes the join procedure, so it improves the efficiency of algorithm greatly Moreover, the scale of L 1 must be small as soon as possible to reduce the scale of C 2 C 3 and so on. Make sure the proper minsupport; also we can choose interested items for the frequent itemsets generated IApriori algorithm is described as follows  1  L 1 large 1-itemsets 2  for \(k=2; L k-1  k 3   4  C k apriori-gen\(L k-1  generate new candidate itemsets 5  for all transactions t  D  and  t.delete=0 6   7  if  t.count<k  then if the size of transaction t is less than k, t is useless for C k generated 8  t.delete=1 mark t the deleting tag to skip over the record in next database scanning 9  else 10   11  C t subset\(C k t candidate itemsets contained in transcaion t 12  if  C t   then if t does not contain any subset of candidate itemsets C k mark the deleting tag 13  t.delete=1 14  else 15   16  for all candidates c  C t  17  c.count 18   19   20   21  L k c  C k c.count  minsup 22   23  Answer k L k   To implement the improvement, IApriori algorithm is described as follow steps 1\ the database to get C 1 make sure the proper minsupport to get frequent itemset L 1 k=1 2\erate C k+1 though joining two frequent itemsets that belong to L k The m th item in the first L k should join with the m+1 th item of the second L k  3\If the size of transaction t is less than k, we give transaction t to mark the deleting tag; if not, compute C t  that contains subsets of candidate itemsets C k in transaction t 4\Judge whether t comprises any subset of C k if not compute the frequencies of C k Otherwise, mark t the deleting tag 5\dd item of C k to L k+1 if the support of the item is greater than minsupport 6\ L k+1   the algorithm ceases. Otherwise k=k+1, continue to the second step in circle until L k+1     5. IApriori algorithm performance test  To analyse the relative performance of the IApriori and Apriori algorithms, we use a small part data from real store database stored 10000 transactions. Figure 1 demonstrates the relative performance of these algorithms. Five experiments are carried out accomplished using the same database with different minimum support factors. The experiment is in WindowsXP Professional operating system, CPU with Intel \(R\Hz, memory with 512MB, the algorithm language used in C Experiments results show that the time needed IApriori algorithm is less than Apriori algorithm under the same support condition. So we can have the conclusion that the proposed algorithm outperforms the Apriori algorithm in computational time 
1102 


 Figure 1  Relative performance under different minsupport  6. Conclusions  In this paper we discuss the problems exist in scanning database frequently and the large scale of candidate itemsets in Apriori algorithm, present an improved algorithm IApriori algorithm It not only decrease the times of scanning database but also optimize the process that generates candidate itemsets. Experiments results show that the proposed algorithm outperforms the Apriori algorithm in computational time  Acknowledgement  This research was supported by Natural Science Foundation of China \(No.60473115, No.60773084, No 60603023\d Doctoral Fund of Ministry of Education of China \(No. 20070151009   References  1  Jiawei Han, M. Kamber Data Mining-Concepts and Techniques Morgan Kaufmann Publishers, Sam Francisco 2001 2  S. Brin, R. Motwani, C. Silverstein, “Beyond Market Basket: Generalizing Association Rules to Correlation In Proc.1997ACM-SIGMOD Int. Conf. Management of Data \(SIGMOD97 Tucson, AZ. 1997, pp.265-276 3  M.B. Elsen, P.T. Speliman, and P.O. Brown, “Cluster Analysis and Display of Genome-wide Expression Patterns Proc. Natl. Acad. Sci USA, 1998, pp.1486314868 4  Agrawal, Rakesh, “Fast Algorithms for Mining Association Rules in Large Databases Proceedings of the ACM SIGMOD International Conference Management of Data  Washington, 1993, pp.207-216 5  ZAKI, J. Mohammed, PARTHASARATHY, Srinivasan Wei Li, “New Algorithms for Fast Discovery and Data Rules In 3rd Intl. Conf. On Knowledge Discovery and Data Mining 1997 6  A. Savasere, E. Omiecinski, and S. B. Navathe, “Mining for Strong Negative Associations in a Large Database of Customer Transactions Proceeding of the 14th International Conference on Data Engineering Orlando Florida, USA: IEEE Computer Society Press, 2004 pp.494-502 7  Xiao-ping Yang. “Improvement of Apriori Algorithm for Association Rules Journal of Zhejiang Ocean Universty Natural Science 2006, V01.25, No.2, pp.176-182 8  Jie Gao, Shao-jun Li, “A Method of Improvement and Optimization on Association Rules Apriori Algorithm Proceedings of the 6th World Congress on Intelligent Control and Automation 2006, pp.5901-5905 9  Sheng Chai, Jia Yang, and Yang Cheng, “The Research of Improved Apriori Algorithm for Mining Association Rules Proceedings of the Service Systems and Service Management 2007 10  Zhen Zhu, Jing-yan Wang, “Book Recommendation Service by Improved Association Rule Mining Algorithm Proceedings of the 6th International Conference on Machine Learning and Cybernetics 2007, pp.3864-3869 11  Peng Gong, Chi Yang, and Hui Li, “The Application of Improved Association Rules Data Mining Algorithm Apriori in CRM Proceedings of 2nd International Conference on Pervasive Computing and Applications  2007 12  Jun-Peng Yuan, Dong-Hua Zhu, “A Survey of Association Rules of Apriori Algorithms Computer Science Vol.31 No.1, pp.114-117 13  Sandro Da Silva Camargo, “MiRABIT: A New Algorithm for Mining Association Rules Proceedings of the 12th International Conference of the Chilean Computer Science Society 2002  
1103 


Lemma 5 Let T be a fuzzy set of FAIs and A B 2 L Y  De\002ne an ordinary set c T  of FAIs by c T   f A  T  A  B   B j A B 2 L Y and T  A  B   B 6  Y g  Then we have Mod T   Mod\(c T   15 jj A  B jj  T  jj A  B jj  c T   16 Proof Directly using Lemma 4 Furthermore Lemma 4 enables us to reduce the concept of semantic entailment from a theory to the concept of entailment from a crisp theory Lemma 6 For A B 2 L Y and a fuzzy set T of fuzzy attribute implications we have jj A  B jj  T  W f c 2 L j jj A  c  B jj  T  1 g  Proof Using Lemma 4 Therefore we have Corollary 7 For A B 2 L Y  T and c T  as in Lemma 5 jj A  B jj  T  W f c 2 L j jj A  c  B jj  c T   1 g  We use Corollary 7 when proving a graded version of completeness theorem for our logic 3 Non-redundant bases In this section we describe non-redundant bases of tables h X Y I i with data attributes De\002nition 8 A set T of FAIs is called complete in h X Y I i if jj A  B jj  T  jj A  B jj  h X;Y;I i for each attribute implication A  B  If T is complete and no proper subset if T is complete then T is called a non-redundant basis of h X Y I i  It follows that if T is complete every A  B from T is valid in h X Y I i to degree 1 and for any other C  D  the degree to which C  D is valid in h X Y I i equals the degree to which T entails C  D  That is non-redundant sets are minimal sets of FAIs with complete information about validity of FAIs in the data The following theorem characterizes complete sets Theorem 9 T is complete iff Mod T   Int X   Y   I   Proof Omitted due to lack of space De\002nition 10 Given h X Y I i  P 022 L Y is called a system of pseudo-intents of h X Y I i if for each P 2 L Y we have P 2 P iff P 6  P  and jj Q  Q  jj  P  1 for each Q 2 P with Q 6  P  In what follows P denotes a system of pseudo-intents We need the following two lemmas proofs omitted Lemma 11 Let T  f P  P  j P 2 Pg  Then Mod T  022 Int X   Y   I   Lemma 12 For any A M 2 L Y we have jj A  A  jj  M  1 for each A 2 L Y and M  M   Theorem 13 T  f P  P  j P 2 Pg is complete Proof We show that jj A  B jj  T  jj A  B jj  Int X  Y  I  for each FAI A  B  Completeness of T is then a consequence of 13 By Lemma 12 each intent M 2 Int X   Y   I  is a model of T  proving the 024 part The 025 part follows from Lemma 11 003 The following theorem is the main result of this section It says that in order to get a non-redundant basis of h X Y I i  it is suf\002cient to compute a system of pseudointents of h X Y I i cf also Section 5 Theorem 14 T  f P  P  j P 2 Pg is a non-redundant basis Proof By Theorem 13 T is complete Now we are going to show the non-redundancy Take T 0 032 T  Clearly there must be P 2 P s.t P  P  does not belong to T 0  In addition to that we have jj Q  Q  jj  P  1  Q 2 P  Q 6  P  by De\002nition 10 i.e P 2 Mod T 0   On the other hand P  2 Mod T  since jj P  P  jj  P  S  P P  6  1  That is jj P  P  jj  h X;Y;I i  jj P  P  jj  T 6  jj P  P  jj  T 0 i.e T 0 is not complete showing the non-redundancy of T  003 If the scale L of grades is 002nite and 003 is globalization the non-redundant basis T given by pseudo-intents is the smallest one in terms of the number of FAIs it contains Theorem 15 Let L be a 002nite residuated lattice with 003 being globalization Let T 0 be complete in h X Y I i  where Y is 002nite Then j T j 024 j T 0 j  where T  f P  P  j P 2 Pg Proof Omitted due to lack of space 
249 


4 Completeness theorems In this section we introduce an axiomatic system for our logic and prove completeness theorems First we introduce deduction rules and a notion of a proof from a set T of FAIs Second we prove that A  B is provable from a set T of FAIs iff A  B semantically follows from T in degree 1 ordinary completeness Third we introduce a concept of a degree j A  B j  T of provability of A  B from a fuzzy set T of FAIs and show that j A  B j  T  jj A  B jj  T graded completeness 4.1 Deduction rules Our axiomatic system consists of the following deduction rules  Ax infer A  A  B  DCut from A  B and B  C  D infer A  C  D  Sh from A  B infer c 003  A  c 003  B for each A B C D 2 L Y  and c 2 L  Rules Sh are to be understood as usual deduction rules having FAIs which are of the form of FAIs in the input part the part preceding infer of a rule a rule allows us to infer in one step the corresponding FAI in the output part the part following infer of a rule Ax is a nullary rule axiom which says that each A  A  B  A B 2 L Y  is inferred in one step Rules Sh resemble Armstrong rules from database theory Remark 1 If 003 is globalization Sh can be omitted Indeed for c  1  we have c 003  1 and Sh becomes from A  B infer A  B  which is a trivial rule for c  1  we have c 003  0 and Sh becomes from A  B infer Y  Y  which can be omitted since Y  Y can be inferred by Ax A  B is called provable from a set T of FAIs using a set R of deduction rules written T  R A  B  if there is a sequence  1       n of fuzzy attribute implications such that  n is A  B and for each  i we either have  i 2 T or  i is inferred in one step from some of the preceding formulas i.e  1       i 000 1  using some deduction rule from R  If R consists of Sh we say just provable     instead of provable    using R  and write just T  A  B instead of T  R A  B  A deduction rule from  1       n infer     i   are fuzzy attribute implications is said to be derivable from a set R of deduction rules if f  1       n g  R   Again if R consists of Sh we omit R  We omit the proof of the next lemma Lemma 16 The following deduction rules are derivable from Ax and DCut Ref infer A  A  Wea from A  B infer A  C  B  Add from A  B and A  C infer A  B  C  Pro from A  B  C infer A  B  Tra from A  B and B  C infer A  C  for each A B C D 2 L Y  4.2 Ordinary completeness In this section we show that deduction rules Sh are sound and we prove their completeness A deduction rule from  1       n infer   is said to be sound if for each M 2 Mod f  1       n g  we have M 2 Mod f  g   i.e each model of all of  1       n is also a model of   The following lemmas are needed to prove our completeness theorems The proofs are technically involved and we omit them due to lack of space Lemma 17 Each of the deduction rules Sh is sound A set T of FAIs is called semantically closed if jj A  B jj  T  1 iff A  B 2 T  i.e if T  f A  B j jj A  B jj  T  1 g  syntactically closed if T  A  B iff A  B 2 T  i.e if T  f A  B j T  A  B g  The following lemma is almost immediate Lemma 18 A set T of fuzzy attribute implications is syntactically closed iff we have Ax A  A  B 2 T  DCut if A  B 2 T and B  C  D 2 T then A  C  D 2 T  Sh if A  B 2 T then c 003  A  c 003  B 2 T for each A B C D 2 L Y  and c 2 L  Lemma 19 Let T be a set of fuzzy attribute implications If T is semantically closed then T is syntactically closed Lemma 20 Let T be a set of fuzzy attribute implications let both Y and L be 002nite If T is syntactically closed then T is semantically closed Corollary 21 If L and Y are 002nite a set T of FAI T is syntactically closed iff T is semantically closed Theorem 22 ordinary completeness Let L and Y be 002nite Let T be a set of fuzzy attribute implications Then T  A  B iff jj A  B jj  T  1  
250 


Proof Sketch Denote by syn  T  the least syntactically closed set of fuzzy attribute implications which contains T  It can be shown that syn  T   f A  B j T  A  B g  Furthermore denote by sem  T  the least semantically closed set of fuzzy attribute implications which contains T  It can be shown that sem  T   f A  B j jj A  B jj  T  1 g  To prove the claim we need to show syn  T   sem  T   As syn  T  is syntactically closed it is also semantically closed by Corollary 21 which means sem  syn  T  022 syn  T   Therefore by T 022 syn  T  we get sem  T  022 sem  syn  T  022 syn  T   In a similar manner we get syn  T  022 sem  T   showing syn  T   sem  T   The proof is complete 003 4.3 Graded completeness We now turn to a graded version of the completeness theorem Note that Theorem 22 can be read as providing a syntactic characterization of entailment in degree 1  i.e of jj A  B jj  T  1  However entailment comes in degrees in general jj A  B jj  T is a degree not necessarily equal to 0 or 1  Our aim is to capture jj A  B jj  T syntactically For this purpose we introduce a notion of a degree j A  B j  T of provability of A  B from a fuzzy set T of FAIs Then we show that j A  B j  T  jj A  B jj  T  which can be understood as a graded completeness completeness in degrees Note that graded completeness was introduced by Pavelka see e.g 7 for detailed information For a fuzzy set T of FAIs and for A  B we de\002ne a degree j A  B j  T 2 L to which A  B is provable from T by j A  B j T  W f c 2 L j c T   A  c 012 B g  17 where c T  is de\002ned as in Lemma 5 This makes use of the reduction of general entailment from fuzzy sets of FAIs to entailment in degree 1 from crisp sets of FAIs established in Corollary 7 The we get Theorem 23 graded completeness Let L and Y be 002nite Then for every fuzzy set T of fuzzy attribute implications and A  B we have j A  B j  T  jj A  B jj  T  5 Further issues We provided several results regarding a logic of attribute containment for graded attributes Main results established in the paper are a description of non-redundant bases of data tables with fuzzy attributes and completeness theorems for such logic We omitted several issues which we plan to include in the full version Most importantly model-theoretic results related to an ef\002cient checking of entailment computation of non-redundant bases and illustrative as well as real-world examples References  S Abiteboul et al  The Lowell database research self-assessment Communications of ACM  48 pp 111–118 2005  R Belohla v ek Fuzzy Relational Systems Foundations and Principles  New York Kluwer 2002  R Belohla v ek V  Vychodil Fuzzy attrib ute implications computing non-redundant bases using maximal independent sets in AI 2005 Arti\002cial Intelligence  S Zhang and R Jarvis Eds LNAI  3809 Berlin Springer-Verlag pp 1126–1129 2005  R Belohla v ek V  Vychodil  Attrib ute implications in a fuzzy setting in ICFCA 2006 International Conference on Formal Concept Analysis  R Missaoui and J Schmid Eds LNAI  3874 Berlin Springer-Verlag pp 45–60 2006  B Ganter  R W ille Formal Concept Analysis Mathematical Foundations  Berlin Springer 1999  G Geor gescu A Popescu Non-dual fuzzy connections Archive for Mathematical Logic  43 pp 1009 1039 2004  G Gerla Fuzzy Logic Mathematical Tools for Approximate Reasoning  Dordrecht Kluwer 2001  J A Goguen The logic of ine xact concepts Synthese  18 pp 325–373 1968  J.-L Guigues V  Duquenne F amilles minimales d'implications informatives resultant d'un tableau de donn  ees binaires Math Sci Humaines  95 pp 5–18 1986  P  H  ajek Metamathematics of Fuzzy Logic  Dordrecht Kluwer 1998  P  H  ajek T Havr  anek Mechanizing Hypotheses Formation Mathematical Foundations for a General Theory  Berlin Springer 1978  D Maier  The Theory of Relational Databases  Rockville Computer Science Press 1983  J Rauch Logic of association rules Applied Intelligence  22 pp 9–28 2005  C Zhang C S Zhang Association Rule Mining Models and Algorithms  Berlin Springer-Verlag 2002 
251 


                                                                                                                 
456 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efÞcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efÞciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efÞciency When the clustering model is available it is a signiÞcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques SufÞcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efÞcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efÞcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conÞdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conÞdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conÞdence The sufÞcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efÞcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efÞciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufÞciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70Ð81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207Ð216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487Ð499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145Ð154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146Ð153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9Ð15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597Ð600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern ClassiÞcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155Ð162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512Ð521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1Ð12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265Ð278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283Ð304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476Ð482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559Ð563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220Ð231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13Ð24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10Ð17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549Ð550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188Ð201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259Ð283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909Ð921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305Ð345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432Ð444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conÞdence Intelligent Data Analysis  9\(4\:381Ð395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49Ð73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407Ð419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1Ð12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki EfÞciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642Ð658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483Ð490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194Ð203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344Ð353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efÞcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103Ð114 1996 
618 
618 


