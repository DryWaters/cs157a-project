Maintenance of Discovered Association Rules in Large Databases An Incremental Updating Technique  David W Cheungt Jiawei Hant Vincent T Ngtt C.Y Wongj t Department of Computer Science, The University of Hong Kong Hong Kong. Email: dcheung@cs.hku.hk School of Computing Science Simon Fraser University, Canada. Email han@cs.sfu.ca tt Department of Computing Hong Kong Polytechnic University Hong Kong. Email cstyng@comp.polyu.edu.hk t Department of Computer Science 
The University of Hong Kong Hong Kong. Email cywong@cs.hku.hk Abstract An incremental updating technique is developed for maintenance of the association rules discovered by database mining There have been many stud ies on eficient discovery of association rules in large databases However it is nontrivial to maintain such discovered rules in large databases because a database may allow frequent or occasional updates and such up dates may not only invalidate some existing strong 
association rules but also turn some weak rules into strong ones In this study an incremental updating technique is proposed for eficient maintenance of dis covered association rules when new transaction data are added to a transaction database 1 Introduction Database mining has recently attracted tremendous amount of attention in the database research because of its wide applicability in many areas including deci sion support market strategy and financial forecast According to many studies in knowledge discovery 
in databases lo 41 mining knowledge from databases has the following characteristics 1 The size of the database is significantly large, it could scale up to gigabytes terabytes or even larger in some applications 2 The rules discovered is valid only in statistical terms Users are looking for rules that hold for a significant amount of data but not necessarily The research of the first three authors were supported in part by RGC the Hong 
Kong Research Grants Council grant 338 f O65/OO26 The research of the second author was also sup ported in part by NSERC the Natural Sciences and Engineer ing Research Council of Canada research grant OGP0037230 and an NCE the Networks of Centres of Excellence of Canada research grant IRIS-HMI5 for all the data Therefore the number of rules returned from a mining activity could be large 3 The rules discovered 
from a database only reflect the current state of the database To make the rules discovered stable and reliable a large vol ume of data should be collected over a substantial period of time These observations indicate that the promise of database mining lies in the techniques to handle a large amount of data, to manage a substantial number of rules and to maintain the rules over a significantly long period of time Therefore the following two prob lems are essential in order to 
make database mining a feasible technology 1 Design efficient algorithms for mining different types of rules or patterns 2 Design efficient algorithms to update maintain and manage the rules discovered The first problem has been studied substantially with many interesting and efficient database mining algorithms reported e.g see l 2 3 5 6 8 9 111 Such database-oriented knowledge mining al gorithms can be classified into two categories con cept generalization-based discovery and 
discovery at the primitive concept levels The former relies on the generalization of concepts attribute values\stored in databases and then summarization of the data reg ularities at a high concept level One such example is the DBLearn system 3 51 The latter relies on the discovery of strong regularities rules from the database without concept generalization Association rule l 2 91 is an important type of rules discovered by this approach 1063-6382/96 5.00 0 1996 IEEE 106 


However very little work has been done on the second problem A method for handling incremen tal database updates for the rules discovered by the generalization-based approach was briefly discussed in 5 However previous work has not been seen on incremental updating of association rules Since database updates may introduce new association rules and invalidate some existing ones it is important to study efficient algorithms for incremental update of as sociation rules in large databases which is the theme of this paper In the pioneer work l it is shown that the prob lem of mining association rules can be decomposed into two subproblems The first problem is to find out all large itemsets which are contained by a significant number of transactions with respect to a threshold minimum suppori The second problem is to generate all the association rules from the large itemsets found with respect to another threshold the minimum con fidence Since it is easy to generate association rules if the large itemsets are available major efforts in the research community have been focused on finding ef ficient algorithms to compute the large itemsets in re cent studies Among all the algorithms proposed the Apriori and its modifications l and the DHP Direct Hash ing and Pruning 9 algorithms are the two most suc cessful They both run a number of iterations and compute the large itemsets of the same size in each iteration starting from the size-one itemsets In each iteration they first construct a set of candidate item sets and then scan the database to count the number of transactions that contain each candidate set The key for optimization lies on the techniques used to create the candidate sets The smaller the number of candidate sets is the faster the algorithm would be The goal in this work is to solve the efficient update problem of association rules after a nontrivial number of new records have been added to a database As suming that the two thresholds minimumsupport and confidence do not change there are several important characteristics in the update problem 1 The update problem can be reduced to finding the new set of large itemsets After that the new association rules can be computed from the new large itemsets 2 Generally speaking an old large itemset has the potential to become small in the updated database 3 Similarly an old small itemset could become large in the new database 4 In order to find the new large itemsets all the records in the updated database including those from the original database have to be checked against every candidate set One possible approach to the update problem is to re-run the association rule mining algorithm on the whole updated database This approach, though sim ple has some obvious disadvantages All the computa tion done initially at finding out the old large itemsets are wasted and all large itemsets have to be computed again from scratch In this paper an efficient algorithm FUP stands for Fast Update is presented for computing the large itemsets in the updated database We will show that the information from the old large itemsets can be reused Moreover at finding the new large itemsets the pool of candidate sets can be pruned substan tially Some optimization technique for reducing the database size during the update process will also be discussed Extensive experiments have been conducted to study the performance of FUP and compare it against the cases in which either Apriori or DHP is applied to the updated database to find the new large itemsets FUP is found to be 2 to 16 times faster than re-running Apriori or DHP. More importantly the number of can didate sets is found to be about 2-5  of that in DHP This shows that FUP is very effective in reducing the number of candidate sets Also the overhead of run ning FUP on an updated database is measured and found to be only about 5-20 which is very efficient The remaining of the paper is organized as follows A detailed problem description is given in Section 2 The algorithm FUP is described in Section 3 Per formance study is discussed in Section 4 Section 5 discusses the variations of the techniques and Section 6 concludes our study 2 Problem Description 2.1 Mining of association rules Let I  il,iz  im be a set of literals called items Let DB be a database of transactions where each transaction T is a set of items such that T C_ I Given an atemset X E I a transaction T contains X if and only if X E T An associatzon rule is an im plication of the form X j Y where X C I Y C I and X n Y  8 The association rule X j Y holds in DB with confidence c if c of the transactions in DB that contain X also contain Y The association rule X 3 Y has support s in DB ifs of the transactions in DB contain X U Y Given a minimum confidence threshold minconf and a minimum support threshold 107 


minsup the problem of mining association rules is to find out all the association rules whose confidence and support are larger than the respective thresholds We also call an association rule a strong rule to distin guish it from the weak ones i.e those that do not meet the thresholds 6 For an itemset X its support is definited similarly as the percentage of transactions in DB which contain X Given a minimum support threshold minsup an itemset X is large if its support is no less than minsup The problem of mining asso ciation rules is reduced to the problem of finding all large itemsets for a pre-determined minimum support 2.2 Update of association rules Let L be the set of large itemsets in the database DB s be the minimumsupport and D be the number of transactions in DB Assume that for each X E L its support count Xsupport which is the number of transactions in DB containing X is available After some update activities an increment db of new transactions is added to the original database DB and d is the number of transactions in db With respect to the same minimum support s an item set X is large in the updated database DB U db if the support of X in DB U db is no less than s i.e X.support 2 s x D  d Thus the essence of the problem of updating asso ciation rules is to find the set L\222 of large itemsets in DB U db Note that a large itemset in L may not be a large itemset in L\222 on the other hand an itemset X not in L may become a large itemset in L\222 3 Fast Update Algorithm FUP Basically the framework of FUP is similar to that of Apriori and DHP It contains a number of iterations The iteration starts at the size-one itemsets and at each iteration all the large itemsets of the same size are found Moreover the candidate sets at each iter ation are generated based on the large itemsets found at the previous iteration The features of FUP which distinguish it from Apriori and DHP are listed as fol lows 111  At each iteration the supports of the size-k large itemsets in L are updated against the increment db to filter out the losers i.e those that are no longer large in the updated database Only the increment db has to be scanned to do the filtering While scanning the increment a set of candidate sets ck is extracted from the transactions in db together with their supports in db counted Note that the size of db is in general much smaller than that of the original database DB The supports of these sets in ck are then updated against the DB to find the 223new\224 large itemsets More importantly many sets in ck can be pruned away by a simple check on their supports in db before the update against DB starts This check will be discussed in the following The size of the updated database is reduced at each iteration by pruning away some items from some transactions in the updated database These features combinkd together form the core in the design of FUP and make FUP a much faster algorithm in comparison with the rerunning of Apriori and DHP on the updated database Our experimental results show a factor of 2 to 16 improvement in performance in the comparison The following notations are used in the remaining of the paper Lk is the set of all size-k large itemsets called large k-itemsets in DB and Li is the set of all large k-itemsets in DB U db ck is the set of size4 candidate sets in the k-th iteration of FWP Moreover X.supportD X.supportd and x.supportr/D represent the support counts of an itemset X in DB db and DB U db respectively The following is a detailed de scription of the algorithm FUP The first iteration of FUP is discussed followed by the discussion of the re maining iterations 3.1 First iteration Removing size-one losers generating size-one candidate sets and finding size-one winners The following properties are useful in the derivation of the large 1-itemsets for the updated database Lemma 1 An I-itemset X in the original large 1 itemsets L1 is a loser in the updated database DBUdb i.e not an the large I-aternset Li if and only af X.SuppOrtUD  s x D  d Proof Based on the definitions of minimum suppod and large 1-itemset 0 Lemma 2 An I-itemset X not an the original large 1-itemsets L1 can become a winner an the updated database DB U db i.e being included in the large 1 itemset Li only if X.Supportd 2 s x d Proof Since X is not in the original large 1-itemsets L1 X.SuPPOTtD  S X D If X.supportd  s x d then X.supportuD  X.SUppOPtD  X.supportd  s x D  d That is X cannot become a large item in the updated database Thus we have the lemma  108 


n losers candidate set C1 scan DB Figure 1 Processes in the first iteration of FUP Based on these properties the finding of large 1 itemset L in the updated database DBUdb is outlined as follows The steps in the outline are described graphically in Figure 1 for easy understanding 1 Scan the increment db for all itemsets X E L1 update its support count X.supportuD Once the scan is completed all the losers in L1 are found by checking the condition XsupportvD  s x D  d on all X E L1 according to Lemma 1 By removing the losers the itemsets in L1 which remain large after the update are identified 2 In the same scan a set C1 is created to store for each T E db all size-one itemset X T which is not in L1 This becomes the set of candidate sets and their support in db can also be found in the scan More importantly according to Lemma 2 if X E C1 and X.supportd  s x d X can never be large in DBUdb Because of this all the sets in C1 whose support counts are less than s x d are pruned off This gives us a very small candidate set for finding the new size-one large itemsets 3 A scan is then conducted on DB to update the support count X.supportuD for each X E C1 By checking their support count new large itemsets from C1 are found By combining with those iden tified in L1 the set of all size-one large itemsets L is generated Example 1 A database DB is updated with an in crement db such that D  1000 d  100 and s  3 I1,12,I3 and I4 are four items I1 and I2 are the large itemsets in L1 with I1.supportD  32 and Iz.supportD  31 Assume that Il.supportd  4 and I2.supportd  1 After a scan on db we have I1.SUppOrtUD  36  llOOx3%and I2.supportuD  32  1100~3 Hence Iz is a loser and only I1 is included in L i.e remains to be large in the updated database Assume that Is and I4 are two itemsets which are not in L1 but occur in the increment db Both I3 and 14 are potential candidate sets In the scan of db it is found that I3.supportd  6 and I4.supportd  2 Since I4.supportd  s x d  3 x 100 it is removed from the candidate set C1 i.e it is unnecessary to check I4 against the updated database Only I3 is included in C1 Suppose that I3.supporit  28 is obtained in the scan of DB Thus I3.supportu  34  1100 x 3 and 13 is included in Li 0 In comparison with the first iteration of Apriori and DHP FUP first filters out the losers and obtains the first set of winners from the original large 1-itemsets by examining only the incremental database db It also filters out from the remaining candidate set in db those items whose occurrence frequencies are too small to be considered as potential winners Both func tions are performed in a single scan of the incremental database db It then scans the original dattabase DB once to check the remaining potential winners In con trast Apriori and DHP must take all the data items as size-one candidate sets and check them against the whole updated database A much smaller candidate set gives FUP a competitive edge in performance when compared with Apriori and DHP 3.2 Second iteration and beyond Remov ing other losers pruning candidate sets and finding remaining winners The following properties are useful in the derivation of the large k-itemsets where k  1 for the updated database Lemma 3 If{Xl  Xk-l isaloser aithe\(k-1 th iteration i.e the itemset is in Lk-1 but not in Li-l a large k-itemset an Lk for any k containing 109 


the itemset cannot be a winner in the k-th iteration i.e being included in the large k-itemsei LL Proof This is based on the property that all the sub sets of a large itemset must also be large proved in 0 Lemma 4 A k-itemset XI   xk in the original large k-itemsets Lk is a loser ie not in the large k-itemset Lk in the updated database DB U db if and only zf XI   Xk}.supportUD  s x D  d Proof Based on the definitions of minimum support and large k-itemset  Lemma 5 A k-itemset XI    xk not in the orig inal large k-itemsets Lk can become a winner z.e be ing included in the large k-itemset Li in the updated database DB U db only if XI   Xk}.supportd 2 s x d Proof Based on the similar reasoning as for Lemma 2 0 Based on the above properties the finding of large 2-itemset L in the updated database DB U db is out lined as follows 1 Similar to the first iteration losers in L2 will be filtered out in a scan on db The filtering is done in two steps Firstly according to Lemma 3 some losers in Lz can be filtered out without checking them against db The set of losers L1  L have been identified in the first iteration Therefore any set X E L2 which has a subset Y such that Y E LI  Li cannot be large and are filtered out from L2 without checking against db Secondly a scan is done on db and the support count of the remaining sets in L2 are updated and the large itemsets from L2 are identified 2 Similar to the first iteration the second part at this iteration is to find the new size-two large itemsets The key is to generate a small set of candidate sets The set of candidate sets C2 is generated before the above scan on db starts by applying the apriori-gen function on L a The sets in L2 are excluded when creating Cz because they have already been handled The support count of the itemsets in C2 are accumulated in the same scan of db The itemsets in C2 can now be pruned by checking their support count For all X E C2 if X.supportd  s x d X is removed from C2 Based on Lemmas 5 all the removed sets cannot be large in DB U db 3 The last step is to scan DB to update the sup port count for all the itemsets in C2 At the end of the scan all the sets X E C2 whose support count X.supportUD 2 s x D  d are identi fied as the new large itemsets The set L which contains all the large itemsets identified from L2 and C2 above are the set of all the size-two large it emsets Example 2 A database LIB is updated with an in crement db such that D  1000 d  100 and s  3 I1,4 13 and I4 are four items and the size-1 and size 2 large itemsets in DB are L1  II 12,13 and L2   I1 12 I2 13 respectively Also I1 I2 supportr  50 and I213.support  31 Suppose FUP has completed the first iteration and found the new size-1 itemsets Li  II Iz 14 This example illustrates how FUP will find out L in the second iteration Note that I3 E L1  Li therefore the set l2I3 E L2 is a loser and is filtered out For the remaining set I1112 E L2 FUP scans db to update its support count Assume that 1112.supportdb  3 Since 1112.supportu  3+50  3 1100, therefore I1I2 is large in DBUdb and is stored in Li Secondly FUP will try to find out the new large itemsets from db Note that apriori-gen applied on Li generates the candidate set C2  I~12,1114,12~4 Since Ill E L2 has already been handled it is re moved from Cz For the remaining sets 11 I4 and 4 4 in C2 FUP scans db to update their support counts Suppose IlI4.supportd  5 and 1214.supportd  2 Since 1214.supportd  2  3 x 100 it cannot be a large itemsets in DB U db Therefore I214 is removed from the candidate set C2 For the remaining set Ill4 E C2 FUP scans DB to update its support count Suppose I1 I4.support  30 Since I1I4.support  30+5  3 x 1100 it is a large itemset in the updated database Therefore I1 I4 is added into Li At the end of the second iteration 0 FUP first filters out losers from L2 Lk  1112,1114 is returned The same algorithm is applied to the later iterations until no large itemsets is found At the k-th iteration of FUP the whole updated database is scanned once However, for the large k  1 itemsets in the orig inal database they only have to be checked against the small increment db For the new large itemsets their candidate sets are extracted from the increment and are pruned according to their support count in the increment This pool for candidate sets is much smaller than those found by using either Apriori or DHP on the updated database This shows that FUP is a much faster algorithm than the previous rule min ing algorithms on database updates 110 


3.3 The FUP Algorithm is presented as follows Based on the above discussion the FUP algorithm Algorithm 1 FUP A fast update algorithm for maintenance of association rules on database up dates Input 1 DB the original database with its size i.e the total number of transactions equal to D 2 Lk the set of all large k-itemsets in L where k  1    r 3 db an increment database with its size equal to d and 4 s the minimum support threshold Output L The set of all large itemsets in DB U db Method The 1st iteration  find Li the set of all large 1 itemsets in DB U db  w  L1 c  0 L  0 P  0  W winners C candidate sets Li initialized P for optimization  for-all T E db do  scan db  forall 1-itemset X C T do  if X E W then X.supportd else  ifX$C then  C  C U X X.supportd  0  init the support count and add X into C  X supportd    1 forall X E W do put winners into Li  if X.SUPPOT~UD _ s x D  d then L  Li U X if X.SUppOTtd  S X d forall X E C do prune candidate sets in C  then  C C X P  PU{X   P will be used for optimization  forall T E DB do  scan DB  if X E C then X.supportD if X E P then removes X from T  Transaction T is reduced  forall 1-itemset X 2 T do  1 forall X E C do put winners into L if X.supportu 2 s x D  d then Li  Li U X return Li  end of the 1st iteration  The k-th iteration  for IC  2 or larger repeat this program fragment to find L the set of all large k-itemsets in the updated database until either L returned is empty or db  0  W  Lk L  0   W winners Li initialized   the size-k candidate sets   prune off losers in W  forall k-1 Y E Lk-1  L',-l do C  apriori-gen\(li-l Lk forall k-itemset X E W do if Y X then  W  W  X break  forall T E db do   scan db  forall X E Subset\(W T do X.~upp~~td  Subset\(W,T returns all the sets in W contained in T 2  for-all X E Subset\(C T do X.supportd  find support of all X E C  Reduce-db\(T Some items in transactions in db can be removed discussed in next section 1 for211 X E W do put the winners from W into Li  if X.supportv _ s x D  d then L  L U X forall X E C do  prune candidate sets in C  forall T E DB do   scan DB  if X.supportd  s x d then C  C  X forall X E Subset\(C,T do X.supportr Reduce-Db T    Some items in transactions in DB can be removed discussed in next section  for-all X E C do  put the winners from C into L  if X.supportuD 2 s x D  d then L  L U X return L  The end of the k-th iteration  Rationale The algorithm follows the lemmas and dis cussions in Sections 3.1-3.2 Moreover the state ments for the reduction of the size of the database are reasoned at the next subsection Hence the al gorithm correctly finds all the association rules in 0 the updated database and terminates 3.4 Reduction of the size of the updated database FUP applies the techniques used in DHP to reduce the size of the updated database At the first itera tion all the candidate sets which do not have enough support in the increment db are stored in the set P Later during the scan of the original database all items in P can be removed from all the transactions 111 


because they will not appear in any large itemset in the later iteration At any k-th iteration some items in db or DB which is not needed for finding large itemsets in the next iteration can be identified and hence removed At any k-th iteration during the scan in the incre ment db while FUP is counting the support for sets in the candidate sets C and W for each transaction TI the Reduce-db function is called It counts for each I E TI the number of sets in C and W which contain I This number gives an upper bound on the num ber of large k-itemsets that contain I If this number is smaller than k then I cannot belong to any large k+l and hence can be removed from all the transactions Using this number Reduce-db can prune off some items from db After the set C has been pruned against db it can be seen that any items in DB which does not belong to any set in Lk or C will not belong to any large IC  l-itemset Therefore in the scanning of DB to compute the supports of sets in C all items that do not belong to any set in Lk or C can be removed In the FUP algorithm, the function Reduce-DB performs this reduction In FUP we have also integrated the direct hashing technique in 9 which further reduces the number of the candidate sets used in iteration two 4 Performance Study In order to assess the performance of FUP experi ments are conducted to compare its performance with that of Apriori and DMP The experiments were per formed on an AIX system on an RS/6000 workstation with model 410 As will be presented in the follow ing the result shows that FUP is much faster than the most successful mining algorithm with respect to updating association rules FUP performs 2 to 6 times faster than DHP for a moderate size database of 100,000 transactions When the database is scaled up to 1,000,000 transactions the speed-up is 2 to 16 times As explained before the key of the speed-up lies on the much smaller amount of candidate sets In some cases the number of candidate sets generated were counted, and it was found that the amount gen erated in FUP is reduced to the range of 1.5  5 of that in DHP This is a very significant reduction As mentioned above we also tested FUP with some very large databases It was found that FUP actually performs much better in larger databases 4.1 Generation of synthetic data The databases used in our experiments are syn thetic data generated using the same technique intro duced in l and modified in 9 The parameters used ILI Number of transactions in database DB Number of transactions in the increment d Mean size of the transactions Mean size of the maximal potentially large itemsets Number of potentially large itemsets Number of items Table 1 Parameter Table TlO.l4.DlOO.dl 3 8.00 1 1 a E 6.00 E 4.00 0 z 2*oo 3 0.00 6.00 4.00 2.00 1.00 0.75 W Minimum support DHPFUP  pri01-i Figure 2 Performance Ratio are similar to those in 9 except that the size of the increment is an additional parameter Table 1 is a list of the parameters used in our synthetic database In the following we use the notation Tx.Iy.Dm.dn modified from the one used in 9 to denote a database in which D  m thousands d  n thousands IT1  x and 111  y In our experiments we set ILI  2000 N  1000 and the secondary parameters S  5 P  50 and Mj  2000 S is the clustering size used in the generation of potential large itemsets P is the pool size to store potential large itemsets from which transactions will receive their items Mj is the multiplying factor associated with the pool Readers not familiar with these parameters please refer to l The way we create our increment is a straight for ward extension of the technique used to synthesize the database In order to do comparison on a database of size D with an increment of size d A database of size D  d is first generated and then the first D transactions are stored in the database DB and the remaining d transactions is stored in the increment db Since all the transactions are generated from the same statistical pattern, it models very well real life 91 112 


11 0.14.01 0O.dl 5 0.06 a 0.05 f 0.04 Z 8 0.03 C 0 0 5 0.02 g 8 0.01 K 0.00 6.00% 4.00 2.00 1.00 0.75 Minimum Support 0 FUP/DHP FUP/APRIORI i3 E 3 Q 2.5 e 2 1.5 4 15K 25K 75K 125K 175K 250K 350K In creme n t Size Figure 3 Reduction on Candidate Sets Figure 4 Speed Up Ratio vs Increment Size updates 4.2 We have compared the performance of FUP against that of DHP and Apriori The first comparison was done on an updated database T10.14.DlOO.dl The performance ratios between them are shown in Fig ure 2 In our implementation of the DHP a hash table of size 100 is used and hashing is only used in the generation of the size-2 candidate sets This is the same policy used in 9 For small support FUP is 3 to 6 times faster than DHP and 3 to 7 times faster than Apriori For larger support, it is less costly to re-run the mining algorithm on the updated database since the number of large itemsets is relatively smaller In terestingly FUP is still 2 to 3 times faster in this case 4.3 Reduction on the number of candi As explained before FUP substantially reduces the number of candidate sets generated The effect is par ticularly significant at the first iteration In Figure 3 the chart shows the ratio of the number of candidate sets generated by FUP when comparing with the two mining algorithms The amount of reduction ranges from 98 to 95% when FUP is compared to DHP It is even greater when it is compared with Apriori 4.4 Performance of FUP with large incre ment In general the larger the increment is the longer it would take to do the update Also the gain in speed up would slow down Two sets of experiments have been performed to support this analysis A database T10.14.DlOO.dmwith updates of lK 5K and 10K were generated and different updates with different sup ports were done by FUP and DHP For the same sup FUP versus DHP and Apriori date sets port the speed-up ratio decreases when update size increases For example when the support is 2 the ratio decreases from 5.8 to 3.7 We also want to find out whether the decreas ing of the performance ratio as the size increases in the update would eventually bring the performance of FUP down to that of DHP In the same setting of T10.14.DlOO.dm we increase the increment size m from 10K gradually to 350K for comparison The per formance ratio is plotted in Figure 4 A gradually level off only appears when the increment size is about 3.5 times the size of the original database The fact that FUP still exhibits performance gain when the incre ment is much larger than the original database shows that it is very efficient 4.5 Small overhead of FUP We also have done some experiments for the pur pose of analyzing the overhead incurred by the FUP In general if the time to compute the set L\222 from an updated database DB U db is added to the time to compute the original set L of large itemsets from the database DB by a mining algorithm the sum would be larger than that if the same mining algorithm was applied directly on DBUdb to compute L\222 The differ ence of these two time values is a measurement of the overhead of the update If the overhead is small, then it indicates that the update was done very efficiently We have designed some experiments to analyze the overhead of FUP by measuring this difference It was found that the bigger the increment is the smaller this overhead becomes In our experiment what was dis covered is that when the increment is much smaller than the original database, the 0verhea.d percentage 113 


ranges around 10  15 Once the increment is larger than the original size the overhead decreases very rapidly from 10 to 5 This is a very encouraging result because it shows that FUP not only can benefit update with small increment it actually works very well in the case of large increment 4.6 Performance in scaled-up databases Our last experiment is done in a scaled-up database The database is T10.I4.D1000.d10 which contains 1 million transactions The performance ratio between DHP and FUP in this scaled-up database ranges from 3 to 16 The result shows that the gain from FUP will in fact increase if the database becomes larger This shows that FUP is very adaptive to size increase and can be applied to very large databases 5 Discussion and Conclusions We studied an efficient fast incremental updating technique for maintenance of the association rules dis covered by database mining The developed method strives to determine the promising itemsets and hope less itemsets in the incremental portion and reduce the size of the candidate set to be searched against the original large database The method is implemented and its performance is studied and compared with the best algorithms for mining association rules studied so far The study shows that the proposed incremen tal updating technique has superior performance on database updates in comparison with direct mining from an updated database The incremental updating technique is applicable to the databases which allow frequent or occasional updates when new transaction data are added to a transaction database We have also investigated the cases of deletion and modification of a transaction database Recently there have been some interesting stud ies at finding multiple-level or generalized association rules in large transaction databases 6 111 The exten sion of our incremental updating technique for mainte nance of multiple-level or generalized association rules in transaction databases is an interesting topic for fu ture research References R Agrawal T Imielinski and A Swami Mining Association Rules between Sets of Items in Large Databases In Proc 1993 ACM-SIGMOD Int Conf Management of Data 207-216 May 1993 R Agrawal and R Srikant Fast algorithms for mining association rules. In Proc 1994 Int Conf Very Large Data Bases pages 487-499 Santiago Chile September 1994 D.W Cheung A W.-C Fu and J Han Knowledge discovery in databases A rule-based attribute-oriented approach In Proc 1994 Int 2221 Symp on Methodologies for Intelligent Systems pages 164-173 Charlotte North Carolina Octo ber 1994 U M Fayyad G Piatetsky-Shapiro P Smyth and R Uthurusamy Advances in Knowledge Dis covery and Data Mining AAAI/MIT Press 1995 J Han Y Cai and N Cercone Data driven discovery of quantitative rules in relational databases IEEE Trans. Knowledge and Data En gineering 5:29-40 1993 J Han and Y Fu Discovery of multiple-level association rules from large databases In Proc 1995 Int Conf Very Large Data Bases Zurich Switzerland Sept 1995 M Klemettinen H Mannila P Ronkainen H. Toivonen and A I Verkamo Finding inter esting rules from large sets of discovered associa tion rules In Proc 3rd Int\222I Conf on Informa tion and Knowledge Management pages 401-408 Gaithersburg Maryland Nov 1994 R Ng and J Han Efficient and effective cluster ing method for spatial data mining In Proc 1994 Int Conf Very Large Data Bases pages 144-155 Santiago Chile September 1994 J.S Park M.S Chen and P.S Yu An effec tive hash-based algorithm for mining association rules In Proc 1995 ACM-SIGMOD Int Conf Management of Daia San Jose CA May 1995 G Piatetsky-Shapiro and W J Frawley Knowl edge Discovery in Ratabases AAAI/MIT Press 1991 R Srikant and R Agrawal Mining generalized association rules In Proc 1995 Int Conf Very Large Data Bases Zurich Switzerland Sept 1995 114 


The disadvantage of this rule-oriented control strategy is that it imposes a restriction on the mixing of forward and backward chaining rules such that a forward chaining rule cannot read any data written by backward chaining rules STO87 To describe this problem let the following be a series of rules Ra to Rd and the resuls REa to REd derived by these rules Ra Rb Rc Rd DB  R  REb  R  REd Also let Ra and Rb be defined as backward chaining rules and Rc and Rd as forward chaining rules If the original database DB is updated rules Rc and Rd though they are forward chaining rules will not be triggered to update the result REd until someone requests the data of REb Thus REd may be iriconsistent with the base data To overcome this problem we use a result-oriented control strategy in which we specify for each result derived subdatabase whether it is to be pre-evaluated or post evaluated The same rule may follow the forward or backward chaining strategy depending on whether the derived subdatabae is to be pre or post-evaluated To illustrate by the example above assume that REd is defined as pre-evaluated and REb is defined as post evaluated Whenever the database DB is updated the rules Ra Rb Rc and Rd will be triggered in the forward chaining fashion to keep REM which is explicitly stored up-to-date REb on the other hand will be evaluated whenever a retrieval operation is issued against it In this case the rules Ra and Rb that derive REb are applied in the backward chaining fashion Thus Ra and Rb follow one control strategy when deriving RFxl and the other control straregy when deriving REb This technique offers more flexibility and alleviates the restriction in POSTGRES described above 7 Conclusion In this paper we have introduced the induced generalization association construct and presented a deductive rule-based language for object-oriented databases The world of subdatabases is closed under this language which facilitates defining inference chains in which each rule derives a new subdatabase based on the subdatabases derived by previous rules in the chain The transitive closure operation can be specified in our language in the form of looping rather than in a recursive form A result-oriented control strategy to be used as the underlying implementation technique has also been introduced in this paper ACKNOWLEDGEMENTS Research on the rule-based language was supported by the U.S West Advanced Technologies grant number UPN 88071315 Work on the Object-Oriented Query Language OQL was supported by the Navy Manufacturing Technology Program through the National Institute of Standards and Technology formerly the National Bureau of Standards grant number 60NANB4wO17 and by the National Science Foundation grant number DMC-8814989 The development efforts are supported by the Florida High Technology and Industry Council grant number UPN 85100316 BIBLIOGRAPHY ALA89a A.M Alashqur S.Y.W Su and H Lar OQL A Quy Language for Manipulating Object-onented Datah Accepted for Publication the 15th VLDB Int Con 1989 ALA89b A.M Alashqur A Query Model and Query and Knowledge Definition Langwi~es for Object-oriented Databases a Ph.D BAN87 BAT85 cER86 CHA84 COD79 DEL88 DIT86 HS87 FOR88 GAL84 HAM81 m7 JAR84 LAM89 MAI88 RAS88 STO87 SU89 TY88 U85 VAS84 Thesis Univedty if Florida 1989 Jay Banerjee et al Data Model Issues for Object-Oriented Aplications ACM Trans on Ofice Information Systems January 1987 D Batory and W Kim Modeling Concepts for VLSI CAD objects ACM TODS September 1985, pages 322-346 Stefan0 Ceri George Gottlob and Gio Wiederhold Interfacing Relational Databases and Prolog Efficiently Roc of the 1st Intl Con on Expert Database Systems 1986 C L Chang and A Walker PROSQL a PROLOG Programming Interface with SQLDS F'mxdngs of the 1st Intl Workshop on Expert Database Systems 1984 E Codd Extend~ng the Database Relational Model to Capture More Meaning ACM TODS Vol 4 No 4 1979 Lois ML Delcambre and James N Etheredge A self Controlling Interpreter for the relational Production Language Roceedings of ACM SIGMOD Conference on Management of Data 1988, pages 396403 KR Dimich Object-oriented Database Systems the Notion and Issues Roc of rhe Intl Workshop on Object-Oriented Database Systems califomia September 1986 D.H Fishman et al Iris An Object-Oriented Database Management System ACM Transaction on Oftixe Informarion Systems January 1987 Pages 4869 S Ford et al Zeitgeist Database support for object-oriented rogramming in the F  gs of the Second International Workshq on Object-oriented Database Systems 1988 Heme Gallaire Jack Mier and Jean-Marie Nicolas Logic and Databases A Deductive Approach ACM Computing Surveys June 1984 Pages 153-185 M Hammer and D McLeod Database Description with SDM A Semantic Associon Model ACM TODS Sepember 1981 R Hull and R King Semantic Database Modeling Survey Applications and Research Issues ACM Computing Surveys September 1987 Mauhias Jark Jim Clifford and Yannis Vassiliou An Optimizing hlog Front-End to a Relational Query System Roc of ACM SIGMOD Con on Management of Data 1984 H.M Lam S Su and A.M Alashqur Integrating the Concepts and Techniqws of Semantic Data Modeling and the Objectdented wradigm Roc of the 13th Intl Computex Software and ApptiCationS Conference COMSAC 89 1989 Christcphe de Maindreville and Eric Simon A Production Rule Based Approach to Deductive Databases Roc of the 4th Intl Con on Data Engineering California 1988 L Raschid and S.Y.W Su A Transaction-oriented Mechanism to Control Precessing in a Knowledge Base Management System Pmc of the Intl Con on Expert Database Systems 1988 Michael Stonebraker Eric Hanson and Chin-Heng Hong The Design of the POSTGRES Rules System Roc of the 3rd Intl Con on Data Engineering California 1987 S.Y.W Su V KrishnamurIhy and H Lam An Object oriented Semantic Association Model OsAM appearing in A.I in Indus&l Engineering and Manufacturing Theoretid Issues and Applications S Kumara et al eds American Institute of Industrial Engineering 1989 Frederick Ty G-OQL Graphics Interface to the Object Oriented Query Language OQL Master thesis University of Florida 1988 Jeffrey ullman Implementation of Logical Query Languages for Databases ACM TODS September 1985 Y Vassiliou J Clifford and M Jark Access to Specific Declarative Knowledge by Expert Systems The Impact of hg"'ning Decision Suppat Systems 1 1 1984 67 


 s_suppkey s_nationkey ps_partkey ps_suppkey ps_supplycost p_partkey p_name   l_partkey l_discount l_quantity l_orderkey l_suppkey l_extendedprice o_orderkey o_orderdate n_nationkey n_name p_partkey p_name   246\262 1 2 3 4 5 7 6 8 9 10,#11,#12,#13 14 15 16 17 18 1 2 Figure 11 Execution plan of TPC-D query 9 for transposed files 2 0 20 40 60 80 100 0 50 100 150 200 Time [s CPUusage NetSend NetRecv Disk 10 8 6 4 0 Throughput [MB/s CPU usage 8 9 10 13 Figure 12 Execution trace of TPC-D query 9 with transposed files 11 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


of I/O But the resulting speedup compared with the previous plans exceeds 2 which is quite satisfactory Table 3 shows the results of above right-deep rd left-deep ld and transposed file tp methods along with the reported results of other commercial systems for 100GB TPC-D query 9 Because our system lacks the software and maintenance price metrics the overall system price can\220t be determined accurately Hardware components themselves cost less than 0.5M We can observe that our system achieves fairly good performance Above all the execution time with the transposed files is twelve times as short as the most powerful commercial platform These results strongly support the effectiveness of the commodity PC based massively parallel relational database servers  System Exec Time Price Teradata on NCR 5100M 160 000 133MHz Pentium 20GB Main Memory 400 Disk Drives 953.3 17M Oracle 7 n DEC AlphaServer 8400 12 000 437MHz DECchip 21164 24GB Main Memory 84 Disk Drives 1884.9 1.3M Oracle 7 n SUN UE6000 24 000 167MHz UltraSPARC 5.3GB Main Memory 300 Disk Drives 2639.3 2.1M IBM DB2 PE on RS/6000 SP 306 96 000 112MHz PowerPC 604 24GB Main Memory 96 Disk Drives 2899.4 3.7M Oracle 7 n HP9000 EPS30 12 000 120MHz PA7150 3.75GB Main Memory 320 Disk Drives 7154.8 2.2M Our Pilot System 100 000 200MHz Pentium Pros 6.4GB Main Memory 100 Disk Drives rd 193.7 ld 177.2 tp 77.1 see text Table 3 Execution time of 100 GB TPC-D Q9 on several systems 12 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


4 Data mining 4.1 Association rule mining Data mining which is a recent hot research topic in the database field is a method of discovering useful information such as rules and previously unknown patterns existing behind data items It enables more effective utilization of transaction log data which have been just archived and abandoned Among the major applications of data mining is association rule mining so called 217\217basket analysis.\220\220 Each of the transaction data typically consists of a set of items bought in a transaction By analyzing them one can derive some association rule such as 217\21790 of the customers who buy both A and B also buy C.\220\220 In order to improve the quality of obtained rules a very large amount of transaction data have to be examined requiring quite a long time to complete First we introduce some basic concepts of association rule Let 000 000 001 000 1 001\000 2 001\002\002\002\001\000 000 002 be a set of items and 003 000 001 003 1 001\003 2 001\002\002\002\001\003 001 002 be a set of transactions where each transaction 003 002 is a set of items such that 003 002 004\000  n itemset 004 has support 005 in the transaction set 003 if 005  f transactions in 003 contain 004  here we denote 005 000 005\006\007\007\b\t 003 001 004 002 An association rule is an implication of the form 004 005 n  where 004\001 n 004 000  and 004 006 n 000 007  Each rule has two measures of value support and confidence  The support of the rule 004 005 n is 005\006\007\007\b\t 003 001 004 b n 002  The confidence 013 of the rule 004 005 n in the transaction set 003 means 013 of transactions in 003 that contain 004 also contain n  which can be written as 005\006\007\007\b\t 003 001 004 b n 002 f\005\006\007\007\b\t 003 001 004 002  For example let r 1 000 001 1 001 3 001 4 002  r 2 000 001 1 001 2 001 3 001 5 002  r 3 000 001 2 001 4 002  r 4 000 001 1 001 2 002  r 5 000 001 1 001 3 001 5 002 be the transaction database Let minimum  support and minimum confidence be 60 and 70 respectively First all itemsets that have support above the minimum support called large itemsets  are generated In this case the large itemsets are 001 1 002 001 001 2 002 001 001 3 002 001 001 1 001 3 002  Then for each large itemset 004  n association rule 004 t n 005 n 001 n 004 004 002 is derived if 005\006\007\007\b\t 003 001 004 002 f\005\006\007\007\b\t 003 001 004 t n 002 n minimum confidence  The results are 1 005 3 001 005\006\007\007\b\t 003 000 60 001 b\016\017 000\020\021\016\013\021 000 75 002 and 3 005 1 001 005\006\007\007\b\t 003 000 60 001 013\b\016\017 000\020\021\016\013\021 000 100 002  The most well known algorithm for association rule mining is the Apriori algorithm[1 We have studied several parallel algorithms for mining association based on Apriori One of these algorithms called HPA Hash Partitioned Apriori is discussed here Apriori first generates candidate itemsets and then scans the transaction database to determine whether each of the candidates satisfies the user specified minimum support and minimum confidence Using these results the next candidate itemsets are generated This continues until no itemset satisfies the minimum support and confidence The most naive parallelization of Apriori would copy the candidates over all the processing node and make each processing node scan the transaction database in parallel Although this works fine when the number of candidates is small enough to fit in the local memory of a single processing node memory space utilization efficiency of this method is very poor For large scale data mining the storage required for the candidates exceeds the available memory space of a processing node This causes memory overflow which results in significant performance degradation due to an excessive amount of extra I/Os HPA partitions the candidate itemsets among the processing nodes using a hash function as in the parallel hash join which eliminates broadcasting of all the transaction data and can reduce the comparison workload significantly Hence HPA works much better than the naive parallelization for large scale data mining The 022 th iteration pass 022  f the algorithm is as follows 1 Generate the candidate itemsets Each processing node generates new candidate itemsets from the large itemsets of the last  001 022 t 1 002 th iteration Each of the former itemsets contains 022 items while each of the latter itemsets contains 001 022 t 1 002 items They are called 022 itemsets and 001 022 t 1 002 itemsets respectively The processing node applies the hash function to each of the candidates to determine the destination node ID If the candidate is for the processing node itself it is inserted into the hash table otherwise it is discarded 13 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


 30 40 50 60 70 80 90 100 110 120 30 40 50 60 70 80 90 10 0 Execution Time [s Number of Nodes Figure 13 Execution time of HPA program pass 2 on PC cluster 2 Scan the transaction database and count the support count Each processing node reads the transaction database from its local disk 000 itemsets are generated from that transaction and the same hash function used in phase 1 s applied to each of them Each of the 000 itemsets is sent to certain processing node according the hash value For the itemsets received from the other nodes and those locally generated whose ID equals the node\220s ID the hash table is searched If hit its support count value is incremented 3 Determine the large itemset After reading all the transaction data each processing node can individually determine whether each candidate 000 itemset satisfy user-specified minimum support or not Each processing node sends large 000 itemsets to the coordinator where all the large 000 itemsets are gathered 4 Check the terminal condition If the large 000 itemsets are empty the algorithm terminates Otherwise the coordinator broadcasts large 000 itemsets to all the processing nodes and the algorithm enters the next iteration 4.2 Performance evaluation of HPA algorithm The HPA program explained above is implemented on our PC cluster Each node of the cluster has a transaction data file on its own hard disk Transaction data is produced using data generation program developed by Agrawal designating some parameters such as the number of transaction the number of different items and so on The produced data is divided by the number of nodes and copied to each node\220s hard disk The parameters used in the evaluation is as follows The number of transaction is 5,000,000 the number of different items is 5000 and minimum support is 0.7 The size of the data is about 400MBytes in total The message block size is set to be 16KBytes according to the results of communication characteristics of PC clusters discussed in previous section The disk I/O block size is 64KBytes which seems to be most suitable value for the system Note that the number of candidate itemset in pass 2 s substantially larger than for the other passes which relatively frequently occurs in association rules mining Therefore we have been careful to parallelize the program effectively especially in pass 2 so that unnecessary itemsets to count should not be generated 14 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


The execution time of the HPA program pass 2 is shown in figure 13 as the number of PCs is changed The maximum number of PCs used in this evaluation is 100 Reasonably good speedup is achieved in this application as the number of PCs is increased 5 Conclusion In this paper we presented performance evaluation of parallel database processing on an ATM connected 100 node PC cluster system The latest PCs enabled us to obtain over 110Mbps throughput in point-to-point communication on a 155Mbps ATM network even with the so-called 217\217heavy\220\220 TCP/IP This greatly helped in developing the system in a short period since we were absorbed in fixing many other problems Massively parallel computers now tend to be used in business applications as well as the conventional scientific computation Two major business applications decision support query processing and data mining were picked up and executed on the PC cluster The query processing environment was built using the results of our previous research the super database computer SDC project Performace evaluation results with a query of the standard TPC-D benchmark showed that our system achieved superior performance especially when transposed file organization was employed As for data mining we developed a parallel algorithm for mining association rules and implemented it on the PC cluster By utilizing aggregate memory of the system efficiently the system showed good speedup characteristics as the number of nodes increased The good price/performance ratio makes PC clusters very attractive and promising for parallel database processing applications All these facts support the effectiveness of the commodity PC based massively parallel database servers Acknowledgment This project is supported by NEDO New Energy and Industrial Technology Development Organization in Japan Hitachi Ltd technically helped us extensively for ATM related issues References  R Agrawal T Imielinski and A Swami Mining association rules between sets of items in large databases In Proceedings of ACM SIGMOD International Conference on Management of Data  pages 207--216 1993  R Agrawal and R Srikant Fast algorithms for mining association rules In Proceedings of International Conference on Very Large Data Bases  1994  A C Arpaci-Dusseau R H Arpaci-Dusseau D E Culler J M Hellerstein and D A Patterson High-performance sorting on Networks of Workstations In Proceedings of International Conference on Management of Data  pages 243--254 1997  D.S Batory On searching transposed files ACM TODS  4\(4 1979  P.A Boncz W Quak and M.L Kersten Monet and its geographical extensions A novel approach to high performance GIS processing In Proceedings of International Conference on Extending Database Technology  pages 147--166 1996 15 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


 R Carter and J Laroco Commodity clusters Performance comparison between PC\220s and workstations In Proceedings of IEEE International Symposium on High Performance Distributed Computing  pages 292--304 1995  D.J DeWitt and J Gray Parallel database systems  The future of high performance database systems Communications of the ACM  35\(6 1992  J Gray editor The Benchmark Handbook for Database and Transaction Processing Systems  Morgan Kaufmann Publishers 2nd edition 1993  J Heinanen Multiprotocol encapsulation over ATM adaptation layer 5 Technical Report RFC1483 1993  M Kitsuregawa M Nakano and M Takagi Query execution for large relations on Functional Disk System In Proceedings of International Conference on Data Engineering  5th pages 159--167 IEEE 1989  M Kitsuregawa and Y Ogawa Bucket Spreading Parallel Hash:A new parallel hash join method with robustness for data skew in Super Database Computer SDC In Proceedings of International Conference on Very Large Data Bases  16th pages 210--221 1990  M Laubach Classical IP and ARP over ATM Technical Report RFC1577 1994  D.A Schneider and D.J DeWitt Tradeoffs in processing complex join queries via hashing in multiprocessor database machines In Proceedings of International Conference on Very Large Data Bases  16th pages 469--480 1990  T Shintani and M Kitsuregawa Hash based parallel algorithms for mining association rules In Proceedings of IEEE International Conference on Parallel and Distributed Information Systems  pages 19--30 1996  T Sterling D Saverese D.J Becker B Fryxell and K Olson Communication overhead for space science applications on the Beowulf parallel workstaion In Proceedings of International Symposium on High Performance Distributed Computing  pages 23--30 1995  T Tamura M Nakamura M Kitsuregawa and Y Ogawa Implementation and performance evaluation of the parallel relational database server SDC-II In Proceedings of International Conference on Parallel Processing  25th pages I--212--I--221 1996  TPC TPC Benchmark 000\001 D Decision Support Standard Specification Revision 1.1 Transaction Processing Performance Council 1995 16 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


In accordance with 1910.97 and 1910.209 warning signs are required in microwave areas For work involving power line carrier systems this work is to be conducted according to requirements for work on energized lines Comments s APPA objects to the absolute requirement implied by the word ensure regarding exposure to microwave radiation and recommends revision of s l iii to read when an employee works in an area where electromagnetic radiation levels could exceed the levels specified in the radiation protection guide the employer shall institute measures designed to protect employees from accidental exposure to radiation levels greater than those permitted by that guide  I1 an employee must be stationed at the remote end of the rodding operation Before moving an energized cable it must be inspected for defects which might lead to a fault To prevent accidents from working on the wrong cable would require identification of the correct cable when multiple cables are present Would prohibit an employee from working in a manhole with an energized cable with a defect that could lead to a fault However if the cable cannot be deenergized while another cable is out employees may enter the manhole but must protect against failure by some means for example using a ballistics blanket wrapped around cable Requires bonding around opening in metal sheath while working on cable Underaround EIectrical Installations t Comments t This paragraph addresses safety for underground vaults and manholes The following requirements are contained in this section Ladders must be used in manholes and vaults greater than four feet deep and climbing on cables and hangers in these vaults is prohibited Equipment used to lower materials and tools in manholes must be capable of supporting the weight and should be checked for defects before use An employee in a manhole must have an attendant in the immediate vicinity with facilities greater than 250 volts energized An employee working alone is permitted to enter briefly for inspection housekeeping taking readings or similar assuming work could be done safely Duct rods must be inserted in the direction presenting the least hazard to employees and APPA recommends that OSHA rewrite section 7\regarding working with defective cables This rewrite would include the words shall be given a thorough inspection and a determination made as to whether they represent a hazard to personnel or representative of an impending fault As in Subsection \(e EEI proposes the addition of wording to cover training of employees in emergency rescue procedures and for providing and maintaining rescue equipment Substations U This paragraph covers work performed in substations and contains the following requirements Requires that enough space be provided around electrical equipment to allow ready and safe access for operation and maintenance of equipment OSHA's position A2-16 


is that this requirement is sufficiently performance oriented to meet the requirements for old installations according to the 1987 NEW Requires draw-out circuit breakers to be inserted and removed while in the open position and that if the design permits the control circuits be rendered inoperative while breakers are being inserted and removed stated in the Rules and requests that existing installations not be required to be modified to meet NESC APPA recommends that Section u 4 i which includes requirements for enclosing electric conductors and equipment to minimize unauthorized access to such equipment be modified to refer to only those areas which are accessible to the public Requires conductive fences around substations to be grounded Power Generation v Addresses guarding of energized parts  Fences screens, partitions or walls This section provides additional requirements and related work practices for power generating plants  Entrances locked or attended Special Conditions w  Warning signs posted  Live parts greater than 150 volts to be guarded or isolated by location or be insulated  Enclosures are to be according to the 1987 NESC Sections llOA and 124A1 and in 1993 NESC  Requires guarding of live parts except during an operation and maintenance function when guards are removed barriers must be installed to prevent employees in the area from contacting exposed live parts Requires employees who do not work regularly at the substation to report their presence Requires information to be communicated to employees during job briefings in accordance with Section \(c of the Rules Comments U APPA and EEI provide comments as follows Both believe that some older substations \(and power plants would not meet NESC as This paragraph proposes special conditions that are encountered during electric power generation, transmission and distribution work including the following Capacitors  Requires individual units in a rack to be short circuited and the rack grounded  Require lines with capacitors connected to be short circuited before being considered deenergized Current transformer secondaries may not be opened while energized and must be bridged if the CT circuit is opened Series street lighting circuits with open circuit voltages greater than 600 volts must be worked in accordance with Section q\or t and the series loop may be opened only after the source transformer is deenergized and isolated or after the loop is bridged to avoid open circuit condition Sufficient artificial light must be provided where insufficient naturals illumination is present to enable employee to work safely A2-17 


US Coast Guard approved personal floatation devices must be supplied and inspected where employees are engaged in work where there is danger of drowning Required employee protection in public work areas to include the following  Warning signs or flags and other traffic control devices  Barricades for additional protection to employees  Barricades around excavated areas  Warning lights at night prominently displayed Lines or equipment which may be sub to backfeed from cogeneration or other sources are to be worked as energized in accordance with the applicable paragraphs of the Rules Comments w APPA submits the following comments regarding this Special Conditions section Recommends that the wording regarding capacitors be modified to include a waiting period for five minutes prior to short circuiting and grounding in accordance with industry standards for discharging of capacitors For series street light circuits, recommends that language be added for bridging to either install a bypass conductor or by placement of grounds so that work occurs between the grounds Recommends modification of the section regarding personal floatation devices to not apply to work sites near fountains decorative ponds swimming pools or other bodies of water on residential and commercial property Definitions x This section of the proposed Rules includes definitions of terms Definitions particularly pertinent to understanding the proposal and which have not previously been included are listed as follows Authorized Employee  an employee to whom the authority and responsibility to perform a specific assignment has been given by the employer who can demonstrate by experience or training the ability to recognize potentially hazardous energy and its potential impact on the work place conditions and who has the knowledge to implement adequate methods and means for the control and isolation of such energy CZearance for Work  Authorization to perform specified work or permission to enter a restricted area Clearance from Hazard  Separation from energized lines or equipment Comments x The following summarizes the changes in some of the definitions which APPA recommends Add to the definition for authorized employee It the authorized employee may be an employee assigned to perform the work or assigned to provide the energy control and isolation function  Recommends that OSHA modify the definition for a line clearance tree trimmer to add the word qualified resulting in the complete designation as a qualified line clearance tree trimmer Recommends that OSHA modify the definition of qualified employee" to remove the word construction from the definition since it is felt that knowledge of construction procedures is beyond the scope of the proposed rule resulting in APPA's new A2-18 I 


wording as follows more knowledgeable in operation and hazards associated with electric power generation transmission and/or distribution equipment Recommends that OSHA add a definition for the word practicable and replace the word feasible with practicable wherever it appears in the proposed regulations and that practicable be further defined as capable of being accomplished by reasonably available and economic means OTHER ISSUES Clothing OSHA requested comments on the advisability of adopting requirements regarding the clothing worn by electric utility industry employees EEI has presented comments which indicates research is underway prior to establishing a standard for clothing to be worn by electric utility employees However EEI's position is that this standard has not developed to the extent that it could be included in the OSHA Rules Both APPA and EEI state that they would support a requirement that employers train employees regarding the proper type of clothing to wear to minimize hazards when working in the vicinity of exposed energized facilities Grandfathering Due to the anticipated cost impact on the utility industry of the proposed Rules requiring that existing installations be brought to the requirements of the proposed Rules both APPA and EEI propose that the final Rules include an omnibus grandfather provision This provision would exempt those selected types of facilities from modification to meet the new rules EEI states that if the grandfathering concept is incorporated that electric utility employees will not be deprived of proper protection They propose that employers be required to provide employees with a level of protection equivalent to that which the standard would require in those instances in which the utility does not choose to modify existing facilities to comply with the final standard Rubber Sleeves OSHA requests comments from the industry on whether it would be advisable to require rubber insulating sleeves when gloves are used on lines or equipment energized at more than a given voltage EEI states its position that utilities should continue to have the option of choosing rubber gloves or gloves and sleeves to protect employees when it is necessary to work closer to energized lines than the distances specified in the clearance tables Preemuting State Laws EEI requests that the final Rules be clear in their preempting state rules applicable to the operation and maintenance work rules for electric power systems. This is especially critical since some states now have existing laws which are more stringent than the proposed OSHA Rules Examples are 1 in California and Pennsylvania where electric utility linemen are prohibited from using rubber gloves to work on lines and equipment energized at more than certain voltages and 2 in California and Connecticut where the live line bare hand method of working on high voltage transmission systems is prohibited One utility Pacific Gas  Electric has obtained a variance from the California OSHA to perform live line bare-hand transmission maintenance work on an experimental basis Coiiflicts Between the Rilles and Part 1926 Subpart V Since many of the work procedures in construction work and operation and maintenance work are similar and difficult to distinguish between EEI requests that the final order be clear in establishing which rule has jurisdiction over such similar work areas A2-19 v 


IMPACTS ON COSTS AND ASSOCIATED BENEFITS In its introduction to the proposed rules OSHA has provided an estimate of the annual cost impact on the electric utility industry for the proposed des of approximately 20.7 million OSHA estimates that compliance with this proposed standard would annually prevent between 24 and 28 fatalities and 2,175 injuries per year The utilities which have responded to this proposed standard through their respective associations have questioned the claims both of the magnitude of the cost involved and the benefit to the industry in preventing fatalities and lost-time injuries Both EEI and APPA feel that the annual cost which OSHA estimates are significantly lower than would be realized in practice Factors which APPA and EEI feel were not properly addressed include the following OSHA has not accurately accounted for cost of potential retroactive impacts including retrofitting and modifying existing installations and equipment OSHA has not consistently implemented performance based provisions in proposed rules  many portions require specific approaches which would require utilities to replace procedures already in place with new procedures Estimates were based on an average size investor-owned utility of 2,800 employees and an average rural cooperative of 56 employees, which are not applicable to many smaller systems such as municipal systems OSHA has not adequately addressed the retraining which would be necessary with modifying long-established industry practices to be in accordance with the OSHA rules EEI claims that OSHA's proposed clearance requirements would not allow the use of established maintenance techniques for maintaining high voltage transmission systems and thus would require new techniques For an example of the cost which is estimated to be experienced as a result of the new Rules one of the EEI member companies has estimated that approximately 20,000 transmission towers would need to be modified to accommodate the required step bolts in the Rules at an estimated cost of 6,200,000 Additionally this same company estimates that the annual cost of retesting live line tools for its estimated 1,000 tools would be 265,000 Additionally, both EEI and APPA question the additional benefits which OSHA claims would result from implementation of the new Rules APPA questions the estimates of preventing an additional 24 to 28 fatalities annually and 2,175 injuries per year in that it fails to account for the fact that the industry has already implemented in large part safety measures which are incorporated in the Rules EEI and APPA also point out that many preventable injuries cannot be eliminated despite work rules enforcement and safety awareness campaigns since many such accidents which result in fatalities are due to employee being trained but not following the employer's training and policies PRESENT STATUS OF RULES According to information received from the OSHA office in February 1993 the final Rules are to be published no later than July 1993 and possibly as soon as March 1993 OSHA closed their receipt of comments in March 1991 and no further changes in the rules are thought possible A2-20 


CONCLUSION The OSHA 1910.269 which proposes to cover electric utility operation and maintenance work rules affects a multitude of working procedures as are summarized in this paper It is not possible at the present time to assess the final structure of the Rules as may be proposed in 1993 or subsequent years Since the comments from the utility associations APPA and EEI were made following the initial release of the proposed OSHA Rules in 1989 a significant amount of time has elapsed where other events have occurred which may affect the form of the final Rules The 1993 NESC went into effect in August 1992 and includes some of the requirements to which the commenters objected For example a significant requirement in the Part 4 of the 1993 NESC requires that rubber gloves be utilized on exposed energized parts of facilities operating at 50 to 300 volts This requirement is in conflict with EEl\222s proposed change to the OSHA Rules which would still allow working such secondary facilities without the use of rubber gloves Electric utilities are advised to review the January 31 1989 proposed operation and maintenance Rules as summarized in this paper and to review their procedures which would be affected by application of the Rules Many of the procedures proposed in the Rules provide valuable guidance in electric utilities\222 operation and maintenance activities Where the cost impact is not significant, it is recommended that utilities consider implementing such procedures in expectation of the Rules being published in the next few months Also it would be appropriate for electric utilities to review the 1993 edition of the NESC since there are portions of the Rules which have resulted in changes in the NESC These changes mainly occur in Part 4 Rules for the Operation of Electric Supply and Communications Lines and Equipment The concerns which the commenters have addressed regarding the cost impact and the resulting benefits experienced as a result of the promulgation of the Rules are real ones and must be addressed in the final Rules As a result this paper cannot present a conclusion regarding the full impact of the Rules The development of such Rules continue to be an ongoing matter and will undoubtedly require later analysis when the final rules are published A2-21 


