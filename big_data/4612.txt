A Characterization of Big Data Benchmarks   Wen Xiong, Zhibin Yu*, Zhendong Bei, Juanjuan Zhao, Fan Zhang, Yubin Zou, Xue Bai, Ye Li, Chengzhong Xu Center for Cloud Computing, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences 518055 ShenZhen, China corresponding author wen.xiong, zb.yu, zd.bei, jj.zhao, zhangfan, yb.zou, bai.xue, li.ye, cz.xu}@siat.ac.cn  Abstract recently, big data has been evolved into a buzzword from academia to industry all over the world. Benchmarks are important tools for evaluating an IT system. However benchmarking big data systems is much more challenging than ever before. First, big data systems are still in their infant stage and consequently they are not well understood. Second, big data systems are more complicated compared to previous systems such as a single node computing platform. While some researchers started to design benchmarks for big data systems they do not consider the redundancy between their benchmarks. Moreover, they use artificial input data sets rather than real world data for their benchmarks. It is therefore unclear whether these benchmarks can be used to precisely evaluate the performance of big data systems In this paper, we first analyze the redundancy among benchmarks from ICTBench, HiBench and typical workloads from real world applications: spatio-temporal data analysis for Shenzhen transportation system. Subsequently, we present an initial idea of a big data benchmark suite for spatio-temporal data. There are three findings in this work: \(1\undancy exists in these pioneering benchmark suites and some of them can be removed safely. \(2\ The workload behavior of trajectory data analysis applications is dramatically affected by their input data sets. \(3\e benchmarks created for academic research cannot represent the cases of real world applications Keywords: workloads; similarity; micro-architecture metrics mapreduce; trajectory data I   I NTRODUCTION  In recent years, big data has been evolved into a buzzword from academia to industry all over the world Companies such as Google, Facebook, and Baidu already built many big data systems. Governments including USA China, and Japan etc. allocate a lot of funds to support the big data research.  While big data is super hot, it also comes with challenges. One of which is how to evaluate the performance of big data systems. Benchmarks are the most important part of evaluating the performance of an IT system Hence, the above challenge is converted into how to design a benchmark suite for big data systems As a benchmark suite, two requirements must be satisfied 1\ The number of benchmarks in the suite should be as small as possible. \(2\ The benchmarks in the suite should be as diverse as possible. The first requirement aims to reduce the time needed to evaluate an IT system. The second one attempts to represent a wide range of applications. Big data as an emerging area, satisfying the two requirements is extremely challenging. Moreover, the two requirements of a benchmark suite are contradictory, further complicating the big data benchmarking problem Among big data platforms, the MapReduce/Hadoop is one of the most widely used frameworks. Therefore, we focus on benchmark suites for Hadoop systems in this paper Since benchmarking big data systems is utter important, a few of studies have been done to design big data benchmark suites [1 2  3  4   5 V ari o u s  c o m m u n ities an d o r g a n i z a ti o n s  related big data have released  benchmark suites such as Cloudsuite [1  G r i d M i x [2  H i ve p e r f or m a nc e  be nc hm a r k 5  IC TB en ch 3   an d  Hi Ben c h   4   However existing Hadoop benchmark suites can’t properly evaluate Hadoop framework because of their limitations of representativeness and diversity. Generally researchers and end users partially use these benchmark suites due to simulation constraints, system calls or hardware circumstances issues. But a randomly selected subset would not be proper for evaluating their systems [9 o r a s p ec i f i c  application domain, it would be beneficial to cost saving by finding out a valid minimal subset without missing diversity Moreover, this subset includes the same characteristics as these benchmark suites do, and it obviously decreases simulation or evaluation time  Generally, in a benchmark suite design, it needs to meet a number of considerations. These considerations are as follows: \(1\nchmark suite should have workloads that are representative of a wide range of application domains; \(2 workloads in a benchmark suite should have diversity of data characteristics; and \(3\ a benchmark suite should not have redundant workloads in itself. However, existing hadoop benchmark suites providers present suites without analyzing redundancy between their workloads and these already popular typical workloads. It is necessary to conduct similarity analysis in these typical workloads due to the aforementioned reasons MapReduce based benchmarks require big data sets as the inputs to drive their workloads H X i e t al 15 f o und that frequently used distributions cannot capture the key characteristics of real world data. Chen et al. [17 und t h at application traces from larger-scale MapReduce Clusters deployed in Facebook and Cloudera did not fit well-know statistical distributions. In this case, only real data can reflect the real system behaviors and workload characteristics, and hence the real world data is preferred in big data benchmarks However, to obtain real world data is a big challenge because of two primary reasons, these reasons are as follows: \(1\ the owner of real world data would not like to share their big 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 118 


data for business confidentiality and user privacies; and \(2 even though the real world data is available on the internet, it is unacceptable for researchers to download terabyte scale data under the condition of current internet traffic. Therefore we provide real world data with two workloads under the permission of our collaborator. These two workloads are sztod and hotregion both of which are typical programs in our internal project related to trajectory data processing in real world At the beginning of our work, we choose 14 typical workloads form ICTBench and HiBench. Both of which are popular MapReduce related benchmark suites in various communities. Meanwhile, we choose 2 typical workloads from our MapReduce system for processing trajectory data HiBench was proposed by Intel Company. It is a new realistic and comprehensive benchmark suite for Hadoop And it consists of a set of Hadoop programs including both read-world applications and synthetic mircobenchmarks [4 ICTBench is a presented by Institute of Computing Technology \(ICT\, which contains three different benchmark suites, and these benchmark suites are DCBench BigDataBench and CloudRank [3 D C B e n c h co nsi s t s of typical data center workloads; these workloads are different from scientific computing applications; they cover applications in important domains such as search engine and electronic commerce; and each workload equal to a single application [6 B i gD a t aB e n ch w a s p u rpo sed for l a r g e sca l e systems and architecture researches and for characterizing big data applications; each benchmark in BigDataBench is equal to a single big application E a c h b e nc hm a r k o f  CloudRank equal to a group of consolidated data center workloads; and it was purposed for capacity planning system evaluation and researches [8  It is unclear that there is redundancy between workloads from the abovementioned benchmark suites and we can find out a valid subset of these typical workloads, which minimizes the number of typical workloads but includes the same characteristics as these original benchmark suites do We address the problem by using micro-architecture level metrics and use principal component analysis PCA\ and clustering techniques to analyze similarity in these typical workloads. In particular, the main contributions of this paper are as follows  We characterize 16 various typical workloads from HiBench and ICTBench by micro-architecture level metrics  We analyze similarity in these various workloads by statistical techniques such as PCA and clustering, the results show that a few workloads have inherent properties  We release two typical workloads related to trajectory data process in real-world application domain These two workloads have dramatically different behavior compared to HiBench and ICTBench The rest of the paper is organized as follows. Section 2 introduces the related work. Section 3 depicts the Evaluation methodology. Section 4 characterizes various typical workloads. Section 5 describes the similarity analysis of our experimented workloads. Section 6 describes trajectory data processing. Section 7 concludes the paper II  R ELATED WORK  Huang et al. conduct quantitative characterization of different workloads in HiBench from three aspects including data access, system resource utilizations and HDFS bandwidth [4  D ai et al an a l y z e a s e ri es ty pic al Ma pR e d u c e  based workloads by HiTune, which is a distributed profile tool based on dataflow model for Hadoop framework. They characterize these workloads not only from the timeline based CPU, disk and Network utilization, but also from execution process of workloads for analyzing  hotpots [1   Meanwhile we characterize these typical workloads from micro-architecture level metrics, such as instruction per cycle cache miss ratio, and number of braches per instruction Researchers in ICT \(Institute of Computing Technology Chinese Academy of Sciences\ did much work in MapReduce-related benchmarking [1   14  T h e y  release three different benchmark suites, BigDataBench is a big data benchmark suite [11 D CBe nc h i s a d a t a c e nt er  benchmark suite and CloudRank is a cloud computing benchmark suite [6 1 Z h en g  et al ch a r ac te riz e O S  behavior of scale-out data center workloads and investigate how the pipeline front end is affected by OS activities [13   Zhen et al. focus characterization on top three application domains: search engine, social networks and electronic commerce [14  On th e on e h a n d  th ei r w o rk s h ow s th at on ly  one application is not enough to represent various categories of data analysis workloads. On the other hand, their study reveals that a series of workloads share many similarities in terms of micro-architectural characteristics [15 How e v e r   they do not quantitatively analyze similarity in these typical workloads by the micro-architectural metrics they obtained Aashish et al. analyze redundancy in the SPEC CPU2006 benchmark suite using micro-architecture metrics. They draw inference on similarity of the benchmarks and arrived at meaningful subsets; and these subsets are representative of a wide range of applications areas without having many benchmarks with similar characteristics. The research result could obviously decrease simulation time for system architecture researches [9  O n th e  on e h a n d  w e a p p l y th e  same methods to conduct similarity analysis in typical workloads in research domains respectively by microarchitecture level metrics. On the other hand, for a given system, each workload in SPEC CPU2006 was executed as single process in a single physical machine. Meanwhile, in our experiments, each MapReduce based workload was running as a multiple process program in a distributed computing environment, which consists of nine physical machines III  E VALUATION M ETHOD  A  Workloads choosing To keep the objective workloads set minimal and without missing diversity. The selected workloads must meet two primary requirements Firstly, the workloads we choose must be representative There are representative workloads in MapReduce based workloads in various application domains such as search 119 


TABLE I  W ORKLOADS AND B ENCHMARK SUITES  Index Workload Benchmark suite 1 Sort HiBench 2 Wordcount HiBench 3 Grep ICTBench 4 Terasort HiBench 5 Bayes HiBench 6 K-means HiBench 7 Nutch indexing HiBench 8 Pagerank HiBench 9 Hive-aggregate HiBench 10 Hive-join HiBench 11 Hotregion Internal workload 12 Sztod Internal workload 13 SVM \(support vector machine ICTBench 14 Ibcf \(Item based collabrative,recom mandation ICTBench 15 Fpg \(Frequent pattern growth ICTBench 16 Hmm \(Hidden Markov model ICTBench  engine, social network and electronic commerce. These workloads are typical in their own fields; and they are quietly different from each other in the terms of characteristics such as high level data graph and different input/output ratio Secondly, the selected workloads should be popular in various communities and be widely validated by academia and industry Based on these considerations, we choose fifteen typical workloads from HiBench, ICTBench, and our internal project, which are related to trajectory data processing in real-world data Detail introduction of workloads from ICTBench or HiBench can be found in  o r 12 a n d t h e w o r k l o a d s a n d benchmark suites are listed in Table 1 Hotregion and sztod are typical workloads in our big data system for real world spatio-temporal data processing. And these data are produced from the subway transportation system and taxicab transportation in Shenzhen. In particular h otregion is a workload for analyzing GPS data of taxicab Sztod is a workload for processing  transaction data of subway transportation system B  Experiment platform and Hadoop cluster setup In our experiment, we deploy a Hadoop cluster consisting five nodes, which are one master and four slaves All nodes in the cluster have the same configurations and are connected through 1000MB Ethernet network. Each node has two Intel Xeon E5620 processors, and it is equipped with 8TB disk, and 16GB memory. Our Linux is ubuntu 12.04 TABLE II  D ETAILS OF HARDWARE CONFIGURATIONS  CPU type Intel Xeon E5620 cores 8 cores   threads 16 threads Sockets 2 ITLB 4-way set assocuative, 64 entries DTLB 4-way set assocuative, 64 entries L2 TLB 4-way set assocuative, 512 entries L1 DCache 32KB, 8-way set assocuative, 64byte/line L1 ICache 32KB, 4-way set assocuative, 64byte/line L2 Cache 256KB, 8-way set assocuative, 64byte/line L3 Cache 12.8MB, 16-way set assocuative, 64byte/line  and the kernel version is 3.2.0. The detail configuration parameters of each node are listed in Table 1 The version of Hadoop and JDK is 1.0.3 and 1.7.0 respectively. The source Benchmark suites are HiBench-2.2 and CloudRank1.0. The profile tools are HiTune-1.0 and oprofile-0.98. Memory size for child process of task tracker is 400MB, and each slave node has 8 map slots and 8 reduce slots C  Experiment methodology In our experiments, we get OS-level performance data and micro-architectural data by using hardware performance counters. We use operf—a profiling tool for Linux 2.90 based systems by specifying the event number and corresponding mask. In our work, we observe 12 CPU events, and specify single process mode for the operf. L2 cache is a unified cache, but operf can distinguish L2 instruction cache miss event from L2 data cache miss event It provides non-root user the capability to collect data from any child process of task tracker, which is the daemon of MapReduce framework in slave nodes. In addition, we use HiTune—a distributed profiling tool for Hadoop-framework to monitor the progress of high level dataflow graph of each workload and collecting data of timeline-based utilization of system resources such as disk, CPU, memory and network We start collecting performance data after a warm up period for each workload, and profiling tools is working in the whole lifetime of each workload. Based on oprofile toolkit, we implemented a series of scripts, which can automatically execute for collecting data from all slave nodes and processing data. And we conduct experiments for each workload three times and report the mean value IV  W ORKLOADS CHARACTERIZATION  In this section, we focus our experiments on seven microarchitecture level characteristics, which calculated by fourteen hardware performance counters. These characteristics are instruction per cycle \(IPC L1 instruction cache miss ratio, L2 instruction cache miss ratio, last level 120 


  Figure 1  Instruction per cycle of each worklaods  Figure 2  L1 Instruction cache miss ratio of each worklaods  Figure 3  L2 Instruction cache miss ratio cycle of each worklaods In this section, we focus our experiments on seven micro-architecture level characteristics, which calculated by fourteen hardware performance counters. These characteristics are instruction per cycle \(IPC L1 instruction cache miss ratio, L2 instruction cache miss ratio last level cache miss ratio, branch miss prediction per instruction, execute time breakdown and off-chip bandwidth We use these characteristics to conduct similarity analysis  Figure 4  Last level cache miss ratio of each worklaods  Figure 5  Branch miss prediction ratio of  each worklaods  Figure 6  Off-chip bandwidth utilization  of each worklaods of these workloads in next section A  Instruction per cycle \(IPC Instruction per cycle \(IPC\ indicates how many instructions can simultaneously execute in one cycle. Figure 1 shows IPC of these typical workloads, group in the right side includes seven workloads fpg  svm  grep  hmm and ibcf  121 


are workloads from cloudrank sztod and hotregion are developed by ourselves. Group in the left consists of nine workloads, and all of them are from HiBench. The IPC of these sixteen workloads are range from 0.72 to 0.96, with an average value of 0.85. Wordcount has the lowest IPC value and hotregion has highest value among these workloads While the workloads are task parallel, the IPC is less than 1 This indicated the parallelism of these workloads is not fully exploited B  Cache miss ratio Instruction cache and Translation Look-side Buff \(TLB are two important components, and they provide fetch unit the capability to accelerate instruction fetch Figure 2 shows the L1 instruction cache miss ratios of sixteen workloads. The cache miss ratios of these typical workloads are range from 3.9% to 19.8%, with an average value of 8.9 Wordcount has the lowest L1 instruction cache miss ratio and ibcf has the highest L1 instruction cache miss ratio Figure 3 presents the L2 instruction cache misses of each workload, the cache misses value of these are range from 47.4% to 87.0%. On average, workloads from cloudrank in right side have larger L2 instruction miss rate then workloads from HiBench in the left side. Overall, the L2 cache is ineffective in our experiment platform The LLC is the largest on-chip component; its capacity has increased across each processor generation. Our results in Figure 4 show that LLC are one of the key limiters of these MapReduce based applications in our experiment. Both wordcount and svm have almost 100% LLC cache miss ratio C  Branch miss Prediction per instruction Branch miss prediction ratio affects the performance directly. Modern out-of-order processors use a functional unit to predict the next branch to avoid pipeline stalls. The pipeline continues whether or not depends on the result of branch predict. If the branch miss prediction occurs, it will cause a dozens of cycles waste due to the pipeline must flush the wrong instructions and fetch the correct ones Figure 5 presents the branch miss prediction ratio of each workload, these ratios are range from 1.5% to 5.6 with an average value of 2.7 Pagerank has the lowest branch miss prediction ratio while nutch indexing has the highest branch miss prediction ratio. The results show that the branch predictor of our processor matches these typical MapReduce based applications D  Off-chip bandwidth utilization Over the past decade; while the off-chip memory latency has slowly improved, off-chip bandwidth has sharply improved. The speed of the memory bus has increased from dozens of MHz to 1GHz, raising the peak theoretical bandwidth form 544MB/s to 17GB/s. Figure 6 plot the percore off-chip bandwidth utilization of these typical workloads we have chosen. As the figure 6 depicted, among these workloads we evaluated, terasort is the only one that has the highest utilization ratio with a value of 14%. Overall in our experiment platform, processors significantly overprovision off-chip bandwidth for these typical workloads V  W ORKLOADS S IMILARITY  In this section, we choose seven micro-architecture metrics to analyze similarity in typical workloads. These metrics are calculated by their performance counters respectively, detail information of these metrics for each workload on four different machines are as follows: \(1 2\L1 instruction cache miss per instruction; \(3\2 instruction cache miss per instruction; \(4 L3 cache miss per instruction; \(5\ Off-chip bandwidth utilization; \(6\ number of branches per instruction; and \(7 number of mispredicted branches per instruction. We measure fundamental workloads characteristics related to their data locality, branch predictability, and instruction locality by hardware performance counters. Since our experiments are carried out on four different nodes, we take each metric-node pair as a variable, in our experiment; we have 28 variables due to there are four nodes and seven metrics for each workload Firstly, we use PCA remove the correlations between this metrics. And then apply hierarchical clustering algorithm to metrics of these workloads. Finally, we analyze the results according to two dendrograms A  Pricinpal component analysis Considering twenty-eight variables for each workload from different metric-machine pair, it is impossible to simultaneously look at all experiment data and draw meaningful conclusions from them. Therefore, PCA is used for analyzing the data. Firstly, the data is normalized to a unit normal distribution for isolating the effect of varying ranges of each parameter PCA helps to reduce the dimensionality of a data set without too much original information loss. PCA computes a new variable set; these new variables are uncorrelated to each other in this set, new variable in this set are linear combinations of original variables, generally, we take each new variable as a principal component [9 The number of principal components is less than or equal to the number of original variables. This transformation is defined in such a way that the first principal component has the largest possible variance \(that is, accounts for as much of the variability in the data as possible\, and each succeeding component in turn has the highest variance possible under the constraint that it be orthogonal to \(i.e., uncorrelated with the preceding components. Principal components are guaranteed to be independent if the data set is jointly normally distributed. PCA is sensitive to the relative scaling of the original variables [19   B  Clustering and Kmeans There are two popular used clustering techniques hierarchical clustering and K-means clustering. Both of them can be used to group workloads with similar features  122 


 sort-30G sort-60G sort-15G terasort-100G terasort-50G bayes terasort-25G sztod-98G pagerank hotregion-17G hotregion-35G grep-80G hive-join sztod-49G svm-40G grep-20G hotregion-70G hive-aggre wordcount-15G wordcount-30G wordcount-60G k-means sztod-24G ibcf-8G hmm-16G ibcf-4G hmm-32G hmm-8G svm-20G grep-40G ibcf-2G svm-10G fpg nutchindexing 2 3 4 5 6 7 8 Linkage Distance a b c sort terasort bayes grep pagerank hotregion hive-join k-means hive-aggre hmm wordcount svm fpg sztod ibcf nutchindexing 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 Linkage Distance    Figure 7  Dendrogram showing similarity between typical MapReduce workloads K-means clustering is a method of vector quantization originally from signal processing. K-means clustering aims to partition n workloads into k clusters in which each workload belongs to the cluster with the nearest mean, where K is a value specified by user. Therefore, we need to cluster workloads for different values of K and then select the best fit due to different grouping possibilities [20   Hierarchical clustering is a "bottom up" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. It is useful in simultaneously looking at multiple clustering possibilities and use can use a dendrogram for selecting desired number of clusters. It start with a matrix of distance between N cases or workloads, this distance is the Euclidean distance between the workloads metrics [20   In our experiments, we use hierarchical clustering algorithm to find out a representative subset of these typical workloads aforementioned C  Workloads similarity Generally, researchers and customers partially use workloads of a specific benchmark suite due to simulation constraints, system call or hardware circumstance issues It would be beneficial to cut down the simulation or evaluation time by finding out a valid minimal subset of a representative, which includes the same characteristics as that benchmark suite dose and without missing diversity  This section demonstrates the results of analyzing similarity in typical MapReduce based workloads. Figure 7 shows a dendrogram for typical workloads listed in Table I after applying PCA and Hierarchical Clustering on the seven metrics. We measure dissimilarity between workloads by Euclidean distance and create a dendrogram by singlelinkage distance. In Figure 7 the horizontal axis shows the linkage distance indicating the similarity and dissimilarity between workloads. Workloads that are outliers have larger linkage distances with the rest of the clusters formed in a hierarchical way. Workloads are positioned close to each other when the distance is smaller, and the ordering on the Y-axis does not have particular significance  Figure 8  Dendrogram showing similarity between input size for each workload We can clearly see from Figure 7, a sort and terasort as a workload pair has the smallest distance to each other in these workloads, as well as bayes and grep We can infer that both of them share many characteristics. b sztod as typical program in our internal project based on trajectory data processing, which has almost largest linkage distances with the rest workloads in the clusters. We can conclude that this program have inherent properties, which make it dissimilarity to the rest workloads Researchers and users can select a representative subset of these workloads by using this dendrogram. For example if researchers want to reduce a subset includes just ten members; they can draw a vertical line at linkage distance of 4, it will yield a subset of ten workloads Figure 8 shows a dendrogram for typical workloads with different data volume. We apply three different input data sets to nine workloads due to constraint of experiment time This workloads are a sort  terasort and wordcount which included in HiBench; b hotregion and sztod which included in our internal project; c grep  hmm  ibcf and svm  which provided by cloudrank. We perform on this data to find similarity between input data for sets these workloads Workloads with three data sets are represented by the name combine with input size As this figure 8 depicted, on the one hand, in some workloads, different input data sets appear clustered together. For example, the largest linkage distance in three input sets of sort is less than 2\(as the vertical line a marked terasort with different input sets in a same cluster when linkage distance is less than 3\(as the vertical line b marked On the other hand, some input sets are very different from the other input sets of the same workloads. For example, three input sets of hmm is dissimilarity to each 123 


other according to their linkage distance, as well as svm and ibcf  We can conclude that a few workloads have inherent property and they are tending towards stability when the data volume increases to a certain extent VI  W ORKLOADS BASED ON SPATIO TEMPORAL DATA  In this section, we present an initial idea of a big data benchmark suite for spatio-temporal data Shenzhen is a famous international city in south china which covers an area of almost 2000 square kilometers. And there are more than eighteen millions of people living in it Therefore, on each day, an amount of spatio-temporal data is produced from subway transportation system, taxicab transportation system and public bus transportation system In particular, our data center collects and stores more than 90 million of GPS records on a daily basis. These GPS data and smart transaction data offers us the opportunity to investigate and understand the demand pattern of passengers and service level from transit agencies There are five typical workloads in our big data system for analyzing spatio-temporal data. All of them are used for traffic flow analysis and spatio-temporal data mining Currently, we just release two typical workloads due to business confidentiality. These two workloads and their input data are released on the web page http://cloud.siat.ac.cn/trajactory-data\ And we will attempt to release the rest typical workloads with the permission of our collaborator A  Transaction Data of  smart card Sztod there are five different subway lines, 118 subway stations and more than 20 million active smart cards in Shenzhen. On average, we collect 15 million transaction records in one day from subway transportation system. The smart card readers register passengers when they enter or leave a subway station. The information from the readers includes card id, flag of entrance or exit, timestamp, id of smart card reader and subway station name  The algorithm used for data processing is as follows 1. Assign a subway station pair and a specific direction count the number of people who enter the origin station and leave the destination station 2. Repeat step 1 until all pairs of subway station are computed 3. Find the top N station pairs with largest counter B  GPS position of  Taxicab Hotregion at a time interval range from ten seconds to thirty seconds, more than 28000 taxicabs equipped with GPS device report their position information to our system in real time. Each record report from these taxies has four primary fields; these fields are timestamp taxicab id  longitude and latitude The algorithm used for data processing is as follows 1. Divide the area into 10000*10000 grids according to latitude and longitude 2. Assign a time range; count the number of taxicab for each grid 3. Repeat the step 2 until all grids are processed 4. Find the top N grids with largest counter of taxicab VII  C ONCLUSIONS AND FUTURE WORK  The results show that there are characteristic redundancies between these existing MapReduce based workloads under the condition of a specific data volume range. For example sort and terasort are clustered into one cluster due to the workload-pair sharing many same characteristics in micro-architecture level, as well as bayes  and grep  Sztod a workload for processing trajectory data from a subway transportation system, which has inherent proprieties and these properties make it obvious dissimiliar to the rest typical workloads we have chosen A few workloads such as terasort and horegion are tending towards stability in micro-architecture level metrics when the data volume increases to a certain extent In future work, our research steps are as follows First, we will further select typical MapReduce based workloads from various application domains to conduct similarity analysis Second, we would like to present a big data benchmark suite for spatio-temporal data R EFERENCES   1  http://parsa.epfl.ch/cloudsuite/cloudsuite.html 2  http://hadoop.apache.org/mapreduce/docs/current/gridmix.html 3  ICTBench home page. http://prof.ict.ac.cn/ICTBench 4  S. S. Huang, J. Huang, J. Q. Dai, T. Xie, and B. Huang. “The HiBench benchmark suite: Characterization of the MapReduce-based data analysis”. Huang, Shengsheng, Jie Huang, Jinquan Dai, Tao Xie and Bo Huang. "" In Data Engineering Workshops \(ICDEW\ 2010 IEEE 26th International Conference on, pp. 41-51. IEEE, 2010 5  Hive home page. http://hive.apache.org 6  DCBench home page. DChttp://prof.ict.ac.cn/DCBench 7  BigDataBench  home page. http://prof.ict.ac.cn/BigDataBench 8  CloudRank  home page. http://prof.ict.ac.cn/BigDataBench 9  A. Phansalkar, A. Joshi, and L. K. John, “Analysis of redundancy and application balance in the SPEC CPU2006 benchmark suite”. 2007 In Proceedings of the 34th annual international symposium on Computer architecture \(ISCA '07\. ACM, New York, NY, USA, 412423   J. Q. Dai et al. “Hitune: dataflow-based performance analysis for big data cloud”. Proc. of the 2011 USENIX ATC \(2011\-100   W. Gao, et al. “BigDataBench: a Big Data Benchmark Suite from Web Search Engines”. The Third Workshop on Architectures and Systems for Big Data \(ASBD 2013\in conjunction with  ISCA 2013   C. Luo, et al. “CloudRank-D: Benchmarking and Ranking Cloud Computing Systems for Data Processing Applications. Fronters of Computer  Science, 2012, 6\(4\: 347–362   Z. Chen et al. “Characterizing OS behavior of Scale-out Data Center Workloads.  Seventh Annual Workshop on the Interaction amongst Virtualization, Operating Systems and Computer Architecture WIVOSCA 2013\. In Conjunction with ISCA 2013   Z, Jia, et al. “Characterizing Data Analysis Workloads in Data Centers”. 2013 IEEE International Symposium on Workload Characterization IISWC-2013   H. Xi et al Characterization of Real Workloads of Web Search Engines 2011 IEEE International Symposium on Workload Characterization IISWC-2011 124 


  Z. Jia et al. “The Implications of Diverse Applications and Scalable Data Sets in Benchmarking Big Data Systems”. Second workshop of big data benchmarking \(WBDB 2012 India\ & Lecture Note in Computer Science \(LNCS   Y. Chen et al, “We Don’t Know Enough to make a Big Data Benchmark suite”. Workshop on Big Data Benchmarking. 2012   J. Zhan et al, “High volume computing: Identify and characterizing throught oriented workloads in data centers”. In Parallel and Distributed processing Symposium Workshops & PhD Forum IPDPSW\, 2012 IEEE 26 th International pages 1712-1721. IEEE 2012   http://en.wikipedia.org/wiki/Principal_component_analysis   http://en.wikipedia.org/wiki/K-means_clustering   125 


overhead of job initialization in Hadoop is much larger than cNeural VIII C ONCLUSION AND F UTURE W ORK The past several years have witnessed an ever-increasing growth speed of data To address large scale neural network training problems in this paper we proposed a customized parallel computing platform called cNeural Different from many previous studies cNeural is designed and built on perspective of the whole architecture from the distributed storage system at the bottom level to the parallel computing framework and algorithm on the top level Experimental results show that cNeural is able to train neural networks over millions of samples and around 50 times faster than Hadoop with dozens of machines In the future we plan to develop and add more neural network algorithms such as deep belief networks into cNeural in order to make further support training large scale neural networks for various problems Finally with more technical work such as GUI done we would like to make it as a toolbox and open source it A CKNOWLEDGMENT This work is funded in part by China NSF Grants No 61223003 the National High Technology Research and Development Program of China 863 No 2011AA01A202 and the USA Intel Labs University Research Program R EFERENCES  C Bishop Neural networks for pattern recognition  Clarendon press Oxford 1995  J Collins Sailing on an ocean of 0s and 1s  Science  vol 327 no 5972 pp 1455…1456 2010  S Haykin Neural networks and learning machines  Englewood Cliffs NJ Prentice Hall 2009  R Hecht-Nielsen Theory of the backpropagation neural network in Proc Int Joint Conf on Neural Networks,IJCNN IEEE 1989 pp 593…605  Y  Loukas  Arti“cial neural netw orks in liquid chromatography Ef“cient and improved quantitative structure-retention relationship models Journal of Chromatography A  vol 904 pp 119…129 2000  N Serbedzija Simulating arti“cial neural netw orks on parallel architectures Computer  vol 29 no 3 pp 56…63 1996  M Pethick M Liddle P  W erstein and Z Huang P arallelization of a backpropagation neural network on a cluster computer in Proc Int Conf on parallel and distributed computing and systems PDCS  2003  K Ganeshamoorthy and D Ranasinghe On the performance of parallel neural network implementations on distributed memory architectures in Proc Int Symp on Cluster Computing and the Grid CCGRID  IEEE 2008 pp 90…97  S Suresh S Omkar  and V  Mani P arallel implementation of back-propagation algorithm in networks of workstations IEEE Trans Parallel and Distributed Systems  vol 16 no 1 pp 24…34 2005  Z Liu H Li and G Miao Mapreduce-based backpropagation neural network over large scale mobile data in Proc Int Conf on Natural Computation ICNC  vol 4 IEEE 2010 pp 1726…1730  M Glesner and W  P  ochm  uller Neurocomputers an overview of neural networks in VLSI  CRC Press 1994  Y  Bo and W  Xun Research on the performance of grid computing for distributed neural networks International Journal of Computer Science and Netwrok Security  vol 6 no 4 pp 179…187 2006  C Chu S Kim Y  Lin Y  Y u  G  Bradski A Ng and K Olukotun Map-reduce for machine learning on multicore Advances in neural information processing systems  vol 19 pp 281…288 2007  U Seif fert  Arti“cial neural netw orks on massi v ely parallel computer hardware Neurocomputing  vol 57 pp 135…150 2004  D Calv ert and J Guan Distrib uted arti“cial neural netw ork architectures in Proc Int Symp on High Performance Computing Systems and Applications  IEEE 2005 pp 2…10  H Kharbanda and R Campbell F ast neural netw ork training on general purpose computers in Proc Int Conf on High Performance Computing HiPC  IEEE 2011  U Lotri  c and e a Dobnikar A Parallel implementations of feed-forward neural network using mpi and c on  net platform in Proc Int Conf on Adaptive and Natural Computing Algorithms  Coimbra 2005 pp 534…537  Q V  Le R Monga and M e a De vin Building high-le v e l features using large scale unsupervised learning in Proc Int Conf on Machine Learning ICML  ACM 2012 pp 2…16  J Ekanayak e and H e a Li T wister a runtime for iterati v e mapreduce in Proc of the 19th ACM International Symposium on High Performance Distributed Computing  ACM 2010 pp 810…818  Y  Bu B Ho we M Balazinska and M D Ernst Haloop Ef“cient iterative data processing on large clusters Proc of the VLDB Endowment  vol 3 no 1-2 pp 285…296 2010  M Zaharia M Cho wdhury  T  Das A Da v e  J  Ma M McCauley M Franklin S Shenker and I Stoica Resilient distributed datasets A fault-tolerant abstraction for in-memory cluster computing in Proc USENIX Conf on Networked Systems Design and Implementation  USENIX Association 2012 pp 2…16 384 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


