Fast and Memory Efficient Mining of High Utility It emsets in Data Streams   Hua-Fu Li 1 Hsin-Yun Huang 2 Yi-Cheng Chen 2 and Yu-Jiun Liu 2 and Suh-Yin Lee 2  1 Department of Computer Science, Kainan University Taoyuan, Taiwan 2 Department of Computer Science, National Chiao-Tung University, Hsinchu, Taiwan E-mail: hfli@mail.knu.edu.tw; sylee@csie.nctu.edu.t w   Abstract  Efficient mining of high utility itemsets has becom e one of the most interesting data mining tasks with broa d applications In this paper we proposed two effici ent one-pass algorithms MHUI-BIT  and MHUI-TID  for mining high utility itemsets from data streams with in a transaction-sensitive sliding window Two effective  representations of item information and an extended  lexicographical tree-based summary data structure a re developed to improve the efficiency of mining high utility itemsets Experimental results show that the propos ed algorithms outperform than the existing algorithms for mining high utility itemsets from data streams   1. Introduction  Association rule mining ARM has been widely studied over the last decade. ARM model treats all items in a large database by only considering whether an item appeared in a transaction or not. The count of an i temset is not a sufficient indicator of interestingness I t only reflects the number of transactions in a large data base that contains the itemset However it does not reveal t he utility of an itemset. The utility can be measured in terms of cost profit or other expressions of user prefer ences Moreover frequent itemsets may only contribute a s mall portion of the overall profit whereas non-frequent  itemsets may contribute a large portion of profit Therefore a new model of ARM i.e utility mining model 5   i s  p r o p o s e d  t o  a d d r e s s  t h e  l i m i t a t i o n  o f  traditional ARM Based on the utility mining model  utility is a measure of how useful or profitable an itemset X is. The utility of an itemset X  i.e u  X  is the sum of the utilities of itemset X in all the transactions containing X An itemset X is called a high utility itemset if and only if u  X    min_utility where min_utility is a user-defined minimum utility threshold Therefore the goal of u tility mining is to find a set of high utility itemsets fr om a large transaction database Formal definition of utility mining, a theoretical model called MEU and a unified framework were proposed b y Yao et al. [10, 13   I n  t h i s  w o r k   t h e  u t i l i t y  i s  d efined as the combination of utility information in each tran saction and additional resources An efficient utility mini ng algorithm called Two-Phase was proposed by Liu et  al 11   T h e  b a s i c  i d e a  o f  T w o P h a s e  a l g o r i t h m  i s  b a s e d on the MEU But Two-Phase algorithm not only can prune  down the number of candidate itemsets, but also can find a complete set of high utility itemsets A data stream  is an infinite sequence of data elements continuously arrived at a rapid rate 2 4    Frequent pattern mining is one of interesting probl ems of mining data streams 3 6 7 8 9   H o w e v e r   l e ss work were proposed for mining high utility itemsets  from data streams Tseng et al 12  p r o p o s e d  t h e  f i r s t  method called THUI-Mine for mining temporal high utility itemsets from data streams In the framewor k of THUI-Mine a database is divided into a sequence of  partitions In the first scan of database THUI-Min e employs a filtering threshold in each partition to generate a progressive set of itemsets Then it uses databa se reduction method to generate a set of candidate k itemsets where k    2 Finally it needs one more scan over the database to find a set of high utility itemsets fro m these candidate k itemsets There are two problems of THUIMine algorithm a lot of false candidate itemsets and huge memory requirement Therefore, in this paper, we propose two efficient algorithms for mining high utility it emsets from data streams Experiments show that the propos ed algorithms outperform the THUI-Mine algorithm The remainder of this paper is organized as follows  The problem is defined in Section 2 The proposed algorithms are described in Section 3. Section 4 di scusses the experimental results. Finally, we conclude the work in Section 5 2. Preliminaries 2.1. Problem Definition Let I  i 1  i 2  i n be a set of n distinct literals called  items An itemset is a non-empty set of items. An itemset X  i 1  i 2  i k with k items is referred to as k itemset 
2008 Eighth IEEE International Conference on Data Mining 1550-4786/08 $25.00 © 2008 IEEE DOI 10.1109/ICDM.2008.107 881 
2008 Eighth IEEE International Conference on Data Mining 1550-4786/08 $25.00 © 2008 IEEE DOI 10.1109/ICDM.2008.107 881 


and the value k is called the length of X and i j    I for j  1 n A transaction  T TID i 1  i 2  i k consists of a transaction identifier TID and a set of item s  i 1  i 2  i k where i j    I  j 1, 2 k A data stream DS  T 1  T 2   T m  is an infinite sequence of transactions where m  is the TID of latest incoming transaction The transaction-sensitive sliding window  TransSW of DS is a window that slides forward for every transacti on [8   The window at each slide has a fixed number w  of transactions and w  is the size of the TransSW Hence current transaction-sensitive sliding window  CurTransSW  or TransSW N 001 001 001 001 w+1    T N 001 w   1  T N 001 w   2   T N    w h e r e  t h e  i n d e x   N 001 w   1 is the window identifier of current TransSW Example 1  An example data stream DS as shown in Figure 1, is composed of four consecutive TransSWs i.e TransSW 1  T 1  T 2   T 9    T r a n s S W 2   T 2  T 3   T 10    TransSW 3  T 3  T 4   T 11   a n d  T r a n s S W 4   T 4  T 5   T 12    w h e n  t h e  w i n d o w  s i z e   i  e    w 9, is given In the typical framework of frequent itemsets minin g the quantity of item purchased of each transaction is 1 or 0  However in the framework of high utility items ets mining the quantity of item purchased is an arbitr ary number The quantity  is called local transaction utility LTU The LTU denoted as o  i p  T q   represents the quantity of item i p in the transaction T q For example o  a  T 3 12 and o  c  T 1 26 in Figure 1 The external utility EU  i.e profit  of an item i p  denoted as u  i p  is the value associated with item i p in the utility table. For example u  a 3 u  b 10 and u  c 1 The utility  of an item  i p  in transaction  T q   denoted as u  i p  T q   is defined as o  i p  T q  001 001\001 001  u  i p   For example the utility of item a in transaction T 3 is 36, i.e u  a  T 3  o  a  T 3  002  u  a  12  3 36. The utility  of an itemset  X  in transaction  T q  denoted as u  X  Tq  is defined as 001  X i q p p T i u    where X    i 1  i 2  i k is a k itemset and X    T q  For example the utility of 2-itemset  ce  in transaction T 1 is 31, i.e u  ce  T 1  u  c  T 1  u  e  T 1  26   1  1   5  31 and utility of 3-itemset  abe  in transaction T 6 is 48, i.e u  abe  T 6  u  a  T 6  u  b  T 6   u  e  T 6 1  3 + 4  10 + 1  5 = 48 The utility of an itemset  X  in TransSW  denoted as u  X  001    q q T X D T q T X u    is the sum of the utilities of X in all the transactions of TransSW containing X as a subset For example the utility of 2-itemset  bd  in TransSW is 146, i.e u  bd  u  bd  T 2  u  bd  T 4  u  bd  T 8 66 52 + 28 = 146, when the size of TransSW is 9 An itemset X is called a high utility itemset if and only if u  X    min_utility For example, 2-itemset bd is a high utility itemset in TransSW 1  since u  bd   146   min_utility if min_utility is 120 in TransSW 1  Problem Statement Given a data stream DS  the size w of a transaction-sensitive sliding window TransSW and a user-defined minimal utility threshold min_utility  the task of this paper is to mine the set of high utili ty itemsets efficiently from current sliding window Figure 1: An example data stream and its utility ta ble 2.2 TWDC-Property In the framework of Boolean frequent itemset mining  algorithms [1, 3, 6, 7   downward closure property i.e if an itemset is frequent then all its subsets must  be frequent  is usually used to mine all frequent itemsets from a large database However the downward closur e property can not be used for mining high utility it emsets For example the utility of 1-itemset  d  is 72 in TransSW 1  i.e u  d   72 in Figure 1 It is a low utility itemset but its superset 2-itemset  bd  is a high utility itemset since u  bd   146  min_utility  if min_utility  is 120 Hence we need new properties to mine high uti lity itemsets In this section a property called Transaction Weighted Downward Closure Property TWDCProperty  12   i s  u s e d  i n  o u r  p r o p o s e d  a l g o r i t h m s  t o  mine the set of high utility itemsets in streaming transactions with a transaction-sensitive sliding w indow Definition 1 The transaction utility of the transaction T q  denoted as tu  T q  is the sum of the utilities of  all  items in T q  For example tu  T 1   u  c  T 1   u  e  T 1   26*1+1*5  31 For example the transaction utility  of each transaction in the example data stream of Figu re 1 is given in Figure 3 Definition 2  The transaction-weighted utilization  of an itemset X  denoted as twu  X  is the sum of the transaction utilities of all transactions containin g X  For example in TransSW 1  of Figure 1 the transactionweighted utilization of 2-itemset bd is 146, i.e twu  bd   tu  T 2  tu  T 4  tu  T 8 66 + 52 + 28 = 146 Definition 3 An itemset X is called a high transactionweighted utilization itemset HTU-itemset if and only if twu  X    min_utility where min_utility is a user-defined minimum utility threshold For example if min_utility  is 120 the 2-itemset  bd  is a high transaction-weighted utilization itemset since twu  bd  is 146 in TransSW 1  of Figure 1  Property 1 Transaction-Weighted Downward Closure Property Let X be a k itemset and Y be a k 1 
882 
882 


itemset such that Y    X  If X  is a high transactionweighted utilization itemset Y is also a high transactionweighted utilization itemset For example let min_utility  is 120 in TransSW 1  of Figure 1 since 3-itemset  abe  is a high transactionweighted utilization itemset, its subsets i.e  a   b   e   ab   ae   be  are also high transaction-weighted utilization itemsets 3. Efficient Mining of High Utility Itemsets In this section, two algorithms, MHUI-BIT and MHUITID are proposed to mine a set of high utility ite msets from data streams The proposed algorithms consist of two major components i.e item information  discussed in Section 3.1\ and a lexicographical tree-based summary data structure  based on item information discussed in Section 3.2 3.1 Bitvector and TIDlist Representations The first major component of proposed algorithms is  item information i.e effective representations of items  Two effective representations of item information i.e Bitvector  and TIDlist  are developed and used in the proposed methods to restrict the number of candidat es and to reduce the processing time and memory usage needed In this paper we proposed two algorithms t o mine the set of high utility itemsets based on Bitv ector and TIDlist respectively Note that the first prop osed algorithm based on the Bitvector representation is called MHUI-BIT   M ining H igh U tility I temsets based on BIT vector Moreover based on TIDlist representation  the second proposed method is called MHUI-TID  M ining H igh U tility I temsets based on TID list Definition 4 For each item x in the current transactionsensitive sliding window TransSW, a bit-sequence wi th w bits denoted as Bitvector  x  is constructed The construction process of Bitvector is described as f ollows If the item x is in the i th transaction of current TransSW the i th bit of Bitvector  x  is set to be 1 Otherwise the bit is set to be 0 Definition 5 For each item x in the current transactionsensitive sliding window TransSW a sorted list wit h at least w values, denoted as TIDlist  x is constructed. The construction process of TIDlist is described as fol lows. If the item x  is in the i th transaction of current TransSW the value i is stored in the TIDlist x  For example the representations of Bitvector and TIDlist of each item of TransSW 1  and TransSW 2  are given in Figure 2. From this figure, we can find th at item a\ appears in transactions T 3 T 6 T 8 and T 9 of TransSW 1  Hence the Bitvector of item  a  i.e Bitvector  a  and the TIDlist of item a i.e TIDlist  a are <001001011 and 3 6 8 9 in TransSW 1  respectively Furthermore Bitvector  a  and TIDlist  a  within TransSW 2  are 010010110> and {2, 5, 7, 8}, respectively Both algorithms are composed of three phases i.e  window initialization phase  window sliding phase  and high utility itemsets generation phase  These phases are discussed from Section 3.2 to Section 3.4, respecti vely  Figure 2: Bitvector and TIDlist of items  TID Transaction Utility TID  Transaction Utility T 1  31 T 7  105 T 2  66 T 8  37 T 3  42 T 9  53 T 4  52 T 10  62 T 5  22 T 11  42 T 6  48 T 12  21 Figure 3: Transaction utility of each transaction o f Example 1 3.2 Window Initialization Phase The first phase is window initialization phase  The phase is activated while the number of transactions  generated so far in a data stream is less than or e qual to a user-defined sliding window size  w  In this phase the item information i.e Bitvector and TIDlist and transaction utility of each transaction within a cu rrent sliding window are generated For example the representations of Bitvector and TIDlist of Example 1 are given in Figure 2 and the table of transaction utility of each transaction within s liding windows, TransSW 1 and TransSW 2 is shown in Figure 3 While the sliding window is full the proposed lexicographical tree-based summary data structure called LexTree-2HTU  lex icographical tree  with 2 HTU itemsets based on item information is constructed  The procedure of building the LexTree-2HTU is described  as follows First a set of high transaction-weighted utilizati on 1itemsets  1-HTU-itemsets  is generated by using item information and the transaction utility table. Then a set of candidate 2-itemsets, i.e., C 2 are generated by combining the set of 1-HTU-itemsets. As each candidate is gen erated its corresponding transaction-weighted utility is determined immediately by using item-information an d the transaction utility table Finally 1-HTU-items ets and 2-HTU-itemsets whose transaction-weighted utility a re greater than or equal to the minimum utility are maintained in the proposed summary data structure LexTree-2HTU Note that in the MHUI-TID algorithm, the TIDlist of a candidate k itemset is generated by joining the TIDlists of the two k 1\-itemsets, where k   2 and the set of two k 1\-itemsets is a subset of this k itemset In MHUI-BIT 
883 
883 


algorithm the Bitvector of the two  k 1\-itemsets is generated by performing bitwise AND operation on th e Bitvectors of the two k 1\-itemsets For example five items a  b  c  d  and e  are 1-HTUitemsets in the window TransSW 1  of Example 1 First four candidate 2-itemsets  ab  ac  ad  ae  are generated from 1-HTU-itemset a  In the framework of MHUI-TID algorithm, the TIDlist\(ab\ in TransSW 1 is {6, 8, 9} which is generated by intersecting TIDlist a  3 6 8 9 and TIDlist b 2, 4, 6, 7, 8, 9}. In the framework of MHUIBIT algorithm the Bitvector ab  in TransSW 1  is 000001011 and is generated by performing bitwise AND operation on Bitvector a   001001011 and Bitvector b 010101111 Next the transaction-weighted utility of candidate  2itemset ab i.e., twu ab can be obtained by summarizing the corresponding transaction utilities Hence twu  ab   tu\(T 6 tu\(T 8 tu\(T 9 from the transaction utility table as shown in Figure 3 Other candidates  ac  ad  ae  are verified in the same way Finally two 2-HTU-itemse ts i.e  ab  and  ae  are maintained in the LexTree-2HTU with prefix a  as shown in Figure 4 We use the same process to construct the sub-trees with prefixes b  c  d  and e The result of LexTree-2HTU of TransSW 1 is given in Figure 5   Figure 4: LexTree-2HTU after inserting all candidat e 2-itemsets with a prefix item a  Figure 5: LexTree-2HTU after inserting all candidat es of TransSW 1 3.3 Window Sliding Phase The second phase of mining high utility itemsets i e window sliding phase is activated while the window  is full and a new transaction arrives In this phase two operations are performed. The first operation is to update the item-information The second one is to update t he summary data structure LexTree-2HTU Two operations  are described as follows  3.3.1 Update Item Information In the framework of MHUI-TID algorithm, all TIDlist s of items are sliding The sliding means that the va lue of TIDlist is decreased by the number of dropped transactions After sliding new transactions are a dded Hence the TIDlists of items appeared in the new transaction are needed updated For example when window slides from TransSW 1  to TransSW 2  the oldest transaction T 1 is deleted and new incoming transaction T 10  is added in the sliding window. At this time, the T IDlists of items in T 1  and T 10  are updated For instance the TIDlists of items c and e of T 1 are changed from {1, 5, 8 9} to {4, 7, 8} and from {1, 5, 6, 7, 8, 9} to {4 5, 6, 7, 8 respectively In MHUI-BIT algorithm all Bitvectors of items are updated by performing bitwise AND operation for window sliding For example when window slides fro m TransSW 1  to TransSW 2  the oldest transaction T 1  is deleted and new incoming transaction T 10 is added in the sliding window. At this time, the Bitvectors of ite ms in T 1  and T 10 are updated. For instance, the Bitvectors of items  c  and e  of T 1  are changed from 100010011 to 000100110 and from 100011111 to 000111110 respectively After updating item-information we need to classif y the items recorded in the dropped transaction and n ew incoming transaction into three types i.e DeleteItem  InsertItem  and IntersecItem  for updating the proposed summary data structure LexTree-2HTU efficiently Th e set of items in the dropped transaction is called DeleteItems  The set of items in the new incoming transaction is celled InsertItems  The set of items not only in the dropped transaction but also in the new  incoming transaction is called IntersecItems  For example item e  is DeleteItem item b  is InsertItem and item c is IntersecItem in Example 1  3.3.2 Update LexTree-2HTU The updating process of LexTree-2HTU is composed of three operations as described as follows a  Item is a DeleteItem Since the item is only in the oldest dropped transaction the transaction-weigh ted utilities of its child nodes are less than or equal to that of previous window That means its child nodes may be 2HTU-itemsets in previous sliding window but are not  2HTU-itemsets in the current sliding window. In this case we check the child nodes of this item with its item information based on different proposed algorithms and prune these child nodes while their transaction-wei ghted utilities are less than the user-defined minimum ut ility For example since there are no potential candidate  2itemsets generated from the item e  item e is a leaf node  in this case\ in previous sliding window, no itemse ts need to be checked in the current LexTree-2HTU b\ Item is an InsertItem Since the item is only in the new incoming transaction the transaction-weighted utilities of its child nodes are greater than or eq ual to that of previous window That means its child nodes may be not 2-HTU-itemsets in previous sliding window but m ay be 2-HTU-itemsets in the current sliding window Ne w candidate itemsets are generated from this new inco ming transaction and their transaction-weighted utilitie s are 
884 
884 


verified If these new itemsets are high transactio nweighted utilization itemsets HTU-itemsets we in sert these HTU-itemsets into the LexTree-2HTU immediatel y For example item b  is an InsertItem in Example 1 In the new incoming transaction T 10  only one candidate 2itemsets bc is generated from item b i.e., with prefix b  After verifying the transaction-weighted utility of  candidate 2-itemset bc i.e., twu bc tu\(T 8 tu\(T 9  tu\(T 10   152 003 120 minimum utility itemset  bc  is inserted into the LexTree-2HTU as a branch with a p refix item b as shown in Figure 5. Note that the existing child  nodes of item b  i.e  bd  and  be  are not necessary to check their transaction-weighted utilities since t he transaction-weighted utilities of itemsets bd and be are greater than the user-specified minimum utility and  at least the same values as that of previous window T he result is given in Figure 6 c\ Item is an IntersecItem Since the item is not only in the dropped transaction but also in the new inco ming transaction, original 2-HTU-itemset may be not a 2HTUitemset in current sliding window and vice versa I n this case we check the transaction-weighted utilities o f existing nodes appeared in the new incoming transac tion to decide whether or not they need to be deleted fr om the LexTree-2HTU Moreover, in order to decide whether or not new candidate itemsets are need to be inserted into the LexTree-2HTU we also check the transaction-weighte d utilities of these new candidates generated from th e new transaction For example item c  is an IntersecItem in Example 1 since it appears in transactions T 1 c: 26\, \(e: 1 and T 10 b: 6\, \(c: 2\}. In this case, only one existing node i.e  ce  needs to be checked and no new candidate 2itemsets need to be checked. After deleting the tra nsaction T 1 the transaction-weighted utility of 2-HTU-itemset  ce  i.e twu ce  changes from 143 to 112 The new transaction-weighted utility of itemset ce is less than the user-defined minimum utility i.e 120 Hence the  node  ce is deleted from the current LexTree-2HTU. The res ult is shown in Figure 7  Figure 6: Updated LexTree-2HTU after processing the items in DeleteItem and InsertItem of Example 1  Figure 7: Updated LexTree-2HTU after processing the items in DeleteItem, InsertItem and IntersecItem of Example 1  Figure 8: Patterns generated after first and second scan of sliding window for TransSW 1 and TransSW 2 of Example 1 3.4 High Utility Itemsets Generation Phase The phase of generating high utility itemsets is us ually performed periodically or when it is needed. In thi s phase the proposed algorithms use level-wise method to generate a set of candidate k HTU-itemsets, C k from the previous pre-known  k 1\-HTU-itemsets where k  003  2 After that we can immediately find the set of k HTUitemsets by using the item-information Bitvector o f MHUI-BIT algorithm and TIDlist of MHUI-TID algorithm The candidate-generation-testing proces s stops when no candidates are generated For example let the user-defined minimum utility i s 120, an itemset X is a high utility if and only if u X   120 There are five 2-HTU-itemsets generated in TransSW 1 of Example 1 But only one candidate 3-itemset  abe  is generated by combining three 2-HTU-itemsets i.e  ab   ae  and  be  After that in MHUI-TID algorithm TIDlist abe   6 7 8 by joining TIDlist ab  and TIDlist ae Hence, twu abe tu\(T 6 tu\(T 7 tu\(T 8  138 003 120. Therefore, itemset abe is a 3-HTU-itemset Note that in MHUI-BIT algorithm Bitvector abe   000010110> by performing bitwise AND on Bitvector ab  and Bitvector ae  Because no more new candidates are generated the candidate-generation-testing process  stops After all candidate HTU-itemsets are generated one  more scan of sliding window is needed to find high utili ty itemsets of TransSW 1  The result of high utility itemsets generated after first and second scan over each sliding windo w is shown in Figure 8 4. Performance Evaluation All the programs are implemented in C STL and compiled with Visual C++.NET compiler All the programs are performed on AMD Athlon\(tm 64 Processor 3000+ 1.8GHz with 1GB memory and running on Windows XP system. All testing data was generate d by the synthetic data generator provided by Agrawal et al in 1   H o w e v e r   t h e  I B M  g e n e r a t o r  o n l y  g e n e r a t e s  t h e  quantity of 0 or 1 for each item in a transaction In order to adapt the databases into the scenario of utility  mining the quantity of each item and the utility of each i tem are randomly generated. In these experiments, the quant ity of each item in each transaction Q ip  are generated randomly ranging from 1 to 5 The utility of each item U ip  stored in the utility table is synthetically cre ated by 
885 
885 


assigning a utility value to each item randomly ra nging from 1 to 1,000. Observed from real world databases that most items are in the low profit range the utility  value generated using a log normal distribution, as is si milar to the model used in the THUI-Mine algorithm [12     0 20 40 60 80 100 120 T15I10D100K T20I15D100K T30I20D100K Datasets Execution time \(sec THUI-Mine MHUI-BIT MHUI-TID  Figure 9: Execution time of algorithms MHUI-BIT, MH UI-TID, and THUI-Mine under various datasets  Figure 10: Comparisons of the number of candidates  Three datasets T15I10D100K T20I15D100K and T30I20D100K are used to compare the experimental results of these methods in Figure 9 The window si ze is fixed to 30,000. The partition size is fixed to 10 000. The minimum utility threshold is fixed to 1%. From this figure we can see that THUI-Mine only runs successfully in the dataset T15I10D100K However the execution time of  THUI-Mine can not be drawn in this picture since it needs much more execution time.  From this figure, we can find that the relation of execution time needed is MHUI TID MHUI-BIT << THUI-Mine The comparison of the number of candidates generated after 1 st scan is given in Figure 10. From this figure we can find that our algorithm generates le ss candidate itemsets than that of THUI-Mine. Hence, t he proposed algorithms are memory efficient algorithms  for mining high utility itemsets from data streams   5. Conclusions Mining of high utility itemsets is one of the most interesting research problems of data mining. In th is paper we proposed two efficient one-pass algorithms for m ining a set of high utility itemsets form a transactional  data stream Experiments show that the proposed algorith ms are efficient one-pass mining methods and outperfor m the existing algorithms for mining high utility itemset s from data streams Future work includes mining top-k hig h utility itemsets from data streams and mining high utility itemsets from data streams with constraints Acknowledgements The research is supported by National Science Counc il of R.O.C under grant no NSC96-2218-E-424-001 and NSC 95-2221-E-009-069-MY3 References 1  R   A g r a w a l  a n d  R   S r i k a n t   F a s t  A l g o r i t h m s  f o r  Mining Association Rules in Large Database In Proc of th e 20th Intel. Conf. on Very Large Databases \(VLDB\, pp. 48 7- 499 1994 2  Y   Z h u   D   S h a s h a   S t a t S t r e a m   S t a t i s t i c a l  M o n i toring of Thousands of Data Stream in Real Time, In Proc. of the 28th Intel. Conf. on Very Large Databases \(VLDB\, pp. 35 8-369 2002 3  G   M a n k u  a n d  R   M o t w a n i   A p p r o x i m a t e  F r e q u e n c y  Counts over Data Streams In Proc of the 28th Inte l Conf on Very Large Databases \(VLDB\, pp.346-357, 2002 4  L   G o l a b  a n d  M   T   O z s u   I s s u e s  i n  D a t a  S t r e a m  Management In ACM SIGMOD Record 32\(2 pp 5-14 2003 5  R   C h a n   Q   Y a n g   Y   D   S h e n   M i n i n g  H i g h  u t i l i ty Itemsets In Proc of the 3rd IEEE Intel Conf on Data Mining \(ICDM\, 2003 6  H  F   L i   M  K   S h a n   a n d  S  Y   L e e   D S M F I   A n  Efficient Algorithm for Mining Frequent Itemsets in  Data Streams Knowledge and Information Systems KAIS doi 10.1007/s10115-007-0112-4 7  H  F   L i  a n d  S  Y   L e e   M i n i n g  F r e q u e n t  I t e m s e t s over Data Streams using Efficient Window Sliding Techniq ues Expert Systems With Applications ESWA 39\(3 doi  10.1016/j.eswa.2007.11.061 8  H  F   L i   C  C   H o   a n d  S  Y   L e e   I n c r e m e n t a l  Updates of Closed Frequent Itemsets over Continuous Data St reams Expert Systems with Applications ESWA 40\(3 doi  10.1016/ j.eswa.2007.12.054 9  Y   C h i   H   W a n g   P   S   Y u   R   M u n t z   M o m e n t   Maintaining Closed Frequent Itemsets over a Stream Sliding Window, In Proc. IEEE Intel. Conf. on Data Mining ICDM pp.  59-66, 2004 10  H   Y a o   H   J   H a m i l t o n   a n d  C   J   B u t z   A  F o u n dational Approach to Mining Itemset Utilities from Databases  In Proc. of 4th SIAM Intel. Conf. on Data Mining \(SDM 2004 11  Y   L i u   W   L i a o   a n d  A   C h o u d h a r y   A  F a s t  H i g h Utility Itemsets Mining Algorithm, In Proc. of the ACM Inte l. Conf on Utility-Based Data Mining Workshop \(UBDM\, 2005  12  V   S   T s e n g   C   J   C h u   a n d  T   L i a n g   E f f i c i e n t Mining of Temporal High Utility Itemsets from Data Streams  In Proc. of the ACM Intel. Conf. on Utility-Based Data Mining Workshop \(UBDM\, 2006 13  H   Y a o   H   H a m i l t o n  a n d  L   G e n g   A  U n i f i e d  Framework for Utilty-Based Measures for Mining Item sets In Proc of the ACM Intel Conf on Utility-Based D ata Mining Workshop \(UBDM\, pp. 28-37, 2006 
886 
886 


0 10 20 30 40 50 60 3210.75 minimum support execution time \(minutes GMFI GMAR BASIC Cumulate  Fig. 10 Mining time in DENSE databases  7.  Conclusions  Through several comprehensive experiments, we found that the FCET and IFECT can save a larger amount of storage spaces than Apriori, MaxEclat, and CHARM in both SPARSE databases and DENSE databases. Since the FCET stores fewer elements for a long pattern, when matched with GMFI/GMAR algorithm, it also revealed efficient execution time than BASIC and CUMULATE in mining generalized association rules The time complexity to find the maximal itemsets is O\(log2n\ where n is the total number of maximal itemsets. For a long pattern, we used a partition tree to count the SUB_TID of itemsets, and then got their merged results. Although the memory required for the FCET is still exponentially large, through limiting the size of maximal itemsets and the size of clustering to a reasonable memory requirement, we do save a large amount of storage spaces, especially in dense databases  7.    Conclusions  Through several comprehensive experiments, we found that the FCET and IFECT can save a larger amount of storage spaces than Apriori, MaxEclat, and CHARM in both SPARSE databases and DENSE databases. Since the FCET stores fewer elements for a long pattern, when matched with GMFI/GMAR algorithm, it also revealed efficient execution time than BASIC and CUMULATE in mining generalized association rules The time complexity to find the maximal itemsets is O\(log 2 n\ where n is the total number of maximal itemsets For a long pattern, we used a partition tree to count the SUB_TID of itemsets, and then got their merged results Although the memory required for the FCET is still exponentially large, through limiting the size of maximal itemsets and the size of clustering to a reasonable memory requirement, we do save a large amount of storage spaces especially in dense databases 8. References  1 g ra w a l, T  Im ieli n s k i an d A  S w a m i M in i n g  association rules between sets of items in large databases, Proc. ACM International Conference on Management of Data \(1993\, pp. 207-216 2 g ra w a l an d R. Srik an t F a s t alg o rit h m s f o r  mining association rules, Proc. 20th International Conference on Very Large Data Bases \(1994\ pp.487499  J i aW e i Han  J i an  P e i an d YiW en Yi n   Mi n i n g frequent patterns without candidate generation, Proc ACM International Conference on Management of Data 2000\p. 1-12 4 J  S   Pa r k  M  S  C h en  an d P S  Y u     A n ef f e c t iv e hash-based algorithm for mining association rules, Proc ACM International Conference on Management of Data 1995\p.175-186 5 A  Sa va se r e  E  O m i e c i ns ki  a n d S N a va t h e    A n  efficient algorithm for mining association rules in large databases,  Proc. 21st International Conference on Very Large Data Bases \(1995\ pp.432-443 6 Y i n-F u H u a ng a n d  Chi e h M i ng W u   M i n i n g  generalized association rules using pruning techniques Proc. IEEE International Conference on Data Mining 2002\p.227-234 7 a k i a n d C.J  Hsia o   E ff icie n t al g o rith m s f o r  mining closed itemsets and their lattice structure,   IEEE Transactions on Knowledge and Data Engineering, vol 17, no. 4, April \(2005\p. 462-478   Bu rdick  M. C a li m l i m  an d J. Geh r k e  M AF I A a maximal frequent itemset algorithm for transactional databases, Proc. 17th International Conference on Data Engineering, \(2001\p.443-452  a k i S  P a rth a s a rat h y   M. Ogih ara, a n d W. Li   New algorithms for fast discovery of association rules Proc. 3rd ACM International Conference on Knowledge Discovery in Databases and Data Mining, \(1997\pp 283-286 10 M  J  Za ki a n d K  G o ud a  Fa s t ve r t i c a l  mi ni ng usi n g  diffsets, Proc. 9th ACM International Conference on Knowledge Discovery and Data Mining, Aug. \(2003   Mam o u l i s D  W. C h e u ng an d W. L i a n    Similarity search in sets and categorical data using the signature tree, Proc. 19th International Conference on Data Engineering, \(2003 12 R. Srik a n t a n d R Ag ra w a l   M i n i n g g e n e ralized  association rules, Proc. 21st International Conference on Very Large Data Bases, \(1995\.407-419    
571 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efÞcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efÞciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efÞciency When the clustering model is available it is a signiÞcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques SufÞcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efÞcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efÞcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conÞdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conÞdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conÞdence The sufÞcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efÞcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efÞciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufÞciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70Ð81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207Ð216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487Ð499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145Ð154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146Ð153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9Ð15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597Ð600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern ClassiÞcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155Ð162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512Ð521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1Ð12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265Ð278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283Ð304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476Ð482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559Ð563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220Ð231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13Ð24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10Ð17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549Ð550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188Ð201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259Ð283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909Ð921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305Ð345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432Ð444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conÞdence Intelligent Data Analysis  9\(4\:381Ð395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49Ð73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407Ð419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1Ð12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki EfÞciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642Ð658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483Ð490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194Ð203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344Ð353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efÞcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103Ð114 1996 
618 
618 


