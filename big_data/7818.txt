IEEE TRANSACTIONS ON IMAGE PROCESSING VOL 26 NO 3 MARCH 2017 1289 Learning Short Binary Codes for Large-scale Image Retrieval Li Liu Mengyang Yu Student Member IEEE  and Ling Shao Senior Member IEEE Abstract  Large-scale visual information retrieval has become an active research area in this big data era Recently hashing/binary coding algorithms prove to be effective for scalable retrieval applications Most existing hashing methods require relatively long binary codes i.e over hundreds of bits sometimes even thousands of bits to achieve reasonable retrieval accuracies However for some realistic and unique applications such as on wearable or mobile devices only short binary codes can 
be used for efcient image retrieval due to the limitation of computational resources or bandwidth on these devices In this paper we propose a novel unsupervised hashing approach called min-cost ranking MCR specically for learning powerful short binary codes i.e usually the code length shorter than 100 b for scalable image retrieval tasks By exploring the discriminative ability of each dimension of data MCR can generate one bit binary code for each dimension and simultaneously rank the discriminative separability of each bit according to the proposed cost function Only top-ranked bits with minimum cost-values are then selected and grouped together to compose the nal 
salient binary codes Extensive experimental results on largescale retrieval demonstrate that MCR can achieve comparative performance as the state-of-the-art hashing algorithms but with signicantly shorter codes leading to much faster large-scale retrieval Index Terms  Large-scale retrieval short binary codes unsupervised hashing min-cost ranking I I NTRODUCTION R ECENTLY the approximate nearest neighbor ANN search  h as attract ed increasing attention for large-scale visual retrieval applications in which hashing methods are popularly utilized to em bed high-dime nsional data into a similarity-preserved low-dimensional Hamming space Hash codes largely reduce the memory storage requirement and simultaneously expedite computation and search Existing 
hashing techniques can be roughly divided into two groups random projection based methods and learning based methods Manuscript received April 26 2016 revised September 21 2016 and October 27 2016 accepted December 19 2016 Date of publication January 10 2017 date of current version January 30 2017 This work was supported by National Natural Sc ience Foundation of China under Grant 61528106 The associate editor coordinating the review of this manuscript and approving it for publication was Prof Guoliang Fan Corresponding author Ling Shao L Liu and L Shao are with the College of Electronic and Information Engineering Nanjing University of Information Science and Technology Nanjing 210044 China and also with the School of Computing Sciences University of East Anglia Anglia NE4 7TJ U.K e-mail liuli1213@gmail.com ling.shao@ieee.org M Yu is with the Department of Computer and Information Sciences 
Northumbria University Newcastle upon Tyne NE1 8ST U.K e-mail m.y.yu@ieee.org Color versions of one or more of the gures in this paper are available online at http://ieeexplore.ieee.org Digital Object Identier 10.1109/TIP.2017.2651390 For the random projection based hashing the most wellknown hashing technique that preserves similarity information is probably Locality-Sensitive Hashing LSH 7  LSH s im ply employs random linear projections followed by random thresholding to map close data points in the Euclidean space to similar codes The kernelized version of LSH Kernelized Locality-Sensitive Hashing KLSH 8  h as also b een proposed and utilized for large-scale image retrieval and classication To design eff ective and compact hash codes a number of projection learning based hashing methods have 
been introduced Principled linear projections such as PCA Hashing PCAH a nd i t s rot a t i onal v ari a nt 10 ha v e been designed for better quantizati on Additionally another popular technique called Spectral Hashing SpH  w a s p ropos ed to preserve the local relationship of data by keeping the neighbors in the input space as neighbors in the Hamming space After that researchers utilized anchor graphs to obtain tractable low-rank adjacency ma trices for efcient similarity search termed Anchor Graphs Hashing AGH B e yond those recently Self-Taught Hashing STH S pherical Hashing SpherH 14  I ter a ti v e Qu an tizatio n  I T Q  1 0   Random Maximum Margin Hashing RMMH  D i s crete Graph Hashing\(DGH 16  L aten t S tru c tu re Preserv i n g Hashing LSPH  and C ompres s e d H as hi ng C H  18 have also been effectively applied to large-scale information 
retrieval tasks More hashing techniques can also be seen in 2 and  19]–[26 Previous efforts on hashing methods more focus on learning data structure and proposing novel optimization scheme for objective functions However less attention is attracted on how to obtain very short compact bin ary codes less than 100 bits for some unique but realistic scenarios and applications such as image retrievals via wear able or mobile devices with limitation of computational resources or bandwidth Inotherwords notwithstanding the effectiveness of preserving data similarity in the original space by the aforementioned hashing methods relatively long codes i.e over hundreds of bits sometimes even thousands of bits are needed to acquire sufciently 
discriminative power otherwise such hashing methods will lead to low actuaries on the searching problems For instance one of the state-of-the-art hashing methods Iterative Quantization ITQ 1  cannot reach the s atis f actory accuracy when the code length is below 100 bits and achieves better performance with the increase of the code length to a certain level as shown in Fig 1 a and c In fact 1 To be consistent with original paper  P CA is applied p rior to IT Q 1057-7149  2017 I EEE Personal u se is perm itted but republication/redistri bution requires IEEE permission See http://www.ieee.org/publications_standards/publications/rights/index.html for more information 


1290 IEEE TRANSACTIONS ON IMAGE PROCESSING VOL 26 NO 3 MARCH 2017 Fig 1 The left subgure shows the top six retrieved images of four query images returned by MCR SpherH ITQ and RMMH using 20K training images and 16 binary bits on the CIFAR-10 dataset It is observed that when applyi ng short hash codes only 16 bits for image retrieval the proposed MCR-L1 can outperform other state-of-t he-art methods Images marked by red boxes are incorrect re trieval Subgure a and b illustrates the performanc ebyusing Hamming ranking search and subgure c and d illustrates the performance resulted from the hash lookup table sear ch In detail the comparison re sults in subgure a are measured by the average semantic top-200 precision between MCR and ITQ To achieve the competitive retrieval results with using orig inal 512-d Gist features MCR can lead to only one-fourth code length of ITQ The s ubgure b reveals the retrieval time per query via linear scan on the 60K dataset Towards the best performance of each method the black bold line seg ment indicates the time seconds re quirement per query via MCR and the blue one indicates the time requireme nt per query for ITQ In terms of using the hash lookup tabl e search subgure c shows the retrieval precision wi thin 2 Hamming distance radius for ITQ and our MCR and the curves in c have t he similar tendencies with subgure  a In addition in subgure d the corresponding searching complexity i.e possible searching space of th e hash lookup table is explored with the code lengths varying From all resu lts it is obviously observed that in terms of the same performance MCR can achieve large-scale retrieval with less computational complexity than ITQ due to t he shorter code length ITQ needs to generate a 320-bit binary code for each sample to achieve the comparative perform ance compared with original features For an ordinary workstation with 16GB RAM if the database contains 1 billion images it is impossible to load such long binary codes in the memory since about 37GB 2 is required for these 320-bit ITQ codes in total Moreover in terms of the online retrieval phase longer binary codes will also lead to an expensive searching complexity no matter using either the Hamming ranking scheme or the hash lookup table In detail from Fig 1 b we can obviously observe that by using Hamming ranking search the retrieval time will dramatically increase when codes become longer More importantly for practical tasks we always adopt the hash lookup table search instead of Hamming ranking to achieve instant retrieval on large-scale data However shorter than 32-bit codes are usually used to establish the hash lookup table since longer codes will cause a huge searching complexity i.e huge searching s pace For instance there are around 5  10 4 possible locations that need to be searched in a lookup table with 320-bit ITQ codes While for 80-bit MCR codes the possible searching locations would be reduced to 3  10 3 in the hash lookup table as illustrated in Fig 1 d Therefore it is infeasible to use long binary codes in applications with the instant retrieval requirement and limited computation resources which motivates us to learn more compact codes   100 bits for large-scale retrieval and still achieve competitive results with state-of-the-art methods In this paper we propose a novel unsupervised hashing method called Min-cost Ranking MCR to learn very short binary codes i.e usually the code length shorter than 100 bits for large-scale retrieval tasks To better explore the 2 Only about 9GB memory is needed for our proposed MCR but with the same retrieval performance of ITQ as shown in Fig 1 a discriminative properties for effective binary coding a twostage learning procedure is introduced in our MCR In the rst stage we aim to learn one bit binary code for each dimension of the original features Considering the intrinsic locality consistency between dimensi ons we associate each dimension with its neighboring dimensions to form a linear subspace Further to assess the discriminative ability each dimension is optimized by minimizing the cost function under the maximum margin criterion in their corresponding subspace and nally transformed to one bit via the obtained mapping vector Similar to SSH for each bit an indi vidua l projection is respectively learned via MCR instead of learning a whole projection matrix at one time Different from SSH learning of each bit in MCR is independent and can be parallel processed which could be more efcient in the training phase While SSH is a sequential learning procedure in which learning of each bit is dependent on previous ones In the second stage similar to  w e r earrange t h e c os t f unct i o n v al ue i e MC R cost of each dimension with the ascending order Only topranked bits are then selected and grouped as the most salient bits for retrieval tasks The experimental results demonstrate the effectiveness and efciency of the proposed MCR which leads to only one-fourth code length of ITQ but achieving competitive results with directly using original 512-d features as shown in Fig 1 It is worthwhile to note that our approach has the following advantages  It can reach competitive retrieval results with state-of-theart hashing methods but with signicantly shorter binary codes which is more feasible for realistic applications with limited computing res ources such as wearable or mobile devices  Different from some existing hashing techniques    28 once t he  r s t s t age o f b i t s l earni ng i s 


LIU et al  LEARNING SHORT BINARY CODES FOR LARGE-SCALE IMAGE RETRIEVAL 1291 accomplished there is no need to re-train the proposed MCR but only need to select a certain number of the bits when generating different lengthed binary codes This important property makes MCR more suitable and efcient for applications requiring multiple-length codes II R ELATED W ORK In terms of general visual search there exist some typical techniques The content b ased image retrieval CBIR methods  are  rs t i nt roduced t o achi e v e t h e v i s ual indexing based on using the content information e.g color texture and shape rather than visual annotations or semantics and suitable for large-scale unsupervised searching Besides with the increasing number of Internet users a large amount of multimodal web data e.g images texts and audio have been continually generated and stored Under such circumstances how to achieve cross-modality similarity search becomes an interesting but challenging problem and has attracted a lot of attention   B e yond thos e recently  w eb click i nfor mation has also been used in image re-ranking because clicks have been shown to more accurately describe the relevance of retrieved images to search queries However a critical problem for click-based methods is the lack of click data since only a small number of web images have actually been clicked on by users Inspired by this some previous methods  40 ai m to solve this problem by utilizing the multimodal hypergraphbased learning methods for image click prediction and apply the obtained click data to achieve more precise image retrieval In the rest of this section we will mainly review some more related hashing techniques and binary bit selection schemes with regards to the large-scale visual retrieval Principled linear projections like PCA Hashing PCAH 9 h as b een suggested for better quantization rather than random projection hashing More recently Spheri cal Hashing SpherH  w a s proposed to map more spatially coherent data points into a binary code compared to hyperplane-based hashing functions Meanwhile the authors also developed a new distance function for binary codes spherical Hamming distance to achieve nal retrieval tasks Iterative Quantization ITQ 10 w a s developed for more compact and effective binary coding Particularly a simple and efcient alternating minimization scheme for nding an orthogonal rotation of zero-centered data so as to minimize the quantization error of mapping this data and the vertices of a zero-centered binary hypercube To obtain high-quality hash codes researchers developed Discrete Graph Hashing DGH to directly s o lv e t he binary code without resorting to error-prone spectral relaxation 11 in wh ich binary code is relaxed to be a matrix of reals followed by a thresholding step threshold is 0  A tractable alternating maximization algorithm was propos ed to explicitly deal with the discrete constraints yielding high-quality codes to well capture the local neighborhoods Besides Random Maximum Margin Hashing RMMH 15 in tr o d u ced a n e w h a sh f u n c tio n f am ily that attempts to solve this issue in any kernel space Rather than boosting the collision probability of close points RMMH focused on data scattering By training purely random splits of the data regardless the closeness of the training samples it was indeed possible to generate consistently more independent hash functions Due to the great success of deep learning recently researchers attempt t o simultaneously achieve the discriminative feature embedding and hash code learning with a unied deep model e.g deep neural network DNN and convolutional neural network CNN Unlike most previous hashing methods which seek a single linear projection to map each sample into a binary vector deep hashing techniques aim to seek multiple hierarchi cal non-linear transformations to learn these binary codes so that the nonlinear relationship of samples can be well exploited and lead to improved performance in large-scale retrieval I n particular some representative deep hashing methods can be found in  for e i t h er single-view or cross-view retrieval applications Nevertheless above methods are all not specically designed for learning very short binary codes shorter than 100 bits so that it is difcult for them to tackle the h igh-performance searching with limited computation resources Inspired by the well-known feature selection problems   47 ai mi ng t o s e l ect t h e opt i m al s ubs et of features from a candidate feature pool in the binary coding scenario this issue can be easily transferred to a so-called bit selection problem which requires to choose a subset of salient bits with the most discriminative power In the literature there are only a few works related to the bit selection problem in large-s cale retrieval applications Mu et al  propos ed a s equent i a l b i t s e l ect i o n s cheme which can preserve the separability via greedy metric learning However in their sequential learning framework the independence between bits which is a benet for the compact hash coding 11   4 9   w as n o t e x p licitly tak e n i n t o consideration In addition to 48  a n o t h e r h ash b it selectio n architecture was introduced in  w hich can ef fecti v ely select good bits from a pool of hash bits generated by different hashing techniques with varied settings and different feature spaces However the above two methods are both regarded as the general hash bit selection schemes for any existing hashing techniques In other words binary codes generated by other hashing methods are necessarily required in advance before applying the above selection schemes on them Thus the nal performance through and  50 w i l l highly depend on the quality of hash codes Different from either purely hashing or binary bit selection schemes in this paper MCR is treated as an integrated hashing method to effectively combine unsupervised binary code learning with salient bit selection In particular the bit selection criterion in MCR is skillfully formulated as the learning objective during our binary code optimization phase Thus the binary coding and bit selection in our framework are highly correlated with each other rather than two interdependent phases similar to previous methods Our MCR is outlined in Fig 2 III M IN C OST R ANKING A Notation and Motivation Given N training data in a D dimensional space  x 1    x N  R D  1  our goal is to generate binary codes for each data point by l earning and selecting the 


1292 IEEE TRANSACTIONS ON IMAGE PROCESSING VOL 26 NO 3 MARCH 2017  Fig 2 The outline of the proposed two-stage MCR binary coding me thod discriminative bits lear ning and salient bit selection representative bits according to their discriminative ability In practice data samples are always represented by realvalued high-dimensional vectors To further obtain binary codes many hashing techniques learn appropriate hash functions directly in the original feature space 3 to preserve the similarity between data However the noise and the redundant dimensions always exist in the original feature space which would have a negative effect on measuring the similarity and fail to learn the discriminative hash codes Recently inspired by the success on feature selection methods  47 w hich motivates us to independently investigate the representative power of each dimension we intend to select the salient bits generated from the dimensions with the most discriminative ability in our architecture A simple way to assess the discriminative power of a dimension is through the separ ability via a linear classier e.g f  x   ax  b  on this dimension Obviously the separability of a linear classier on only one-dimensional data will be poor since it is demonstrated to be linearly inseparable In our work to better assess the discriminative power of each dimension we rst nd their intrinsic neighboring dimensions and then group each dimension with their neighboring dimensions t ogether into a subspace After that a linear classier as the hashing function is learned for each dimension’s subspace Since each subspace is currently spanned by multiple dimensions rather than one-dimension it can be easily separated by a linear classier 4 In fact the subspaces spanned by each di mension and their neighborhoods not only well preserve the intrinsic local structure between different dimensions but also take the independence of each dimensional data into account B Formulation of MCR In the following we rst illustrate how we construct the neighborhood for each dimensi on’s subspace and then our Min-cost Ranking MCR algorithm is described in detail for salient bits learning and selection 3 For example in ITQ the hashing function is directly learned from all dimensions of the data ignoring the inde pendence between different dimensions 4 In our learning phase since a pair-wise-label binary classication problem is involved any subspace with more than two-dimensional data can be linearly separated 1 Neighborhood Construction for Dimensions Let us denote x  1    x  D  R N  1 as the dimension vectors of x 1    x N  i.e  x  1    x  D  T  x 1    x N   A direct construction of the neighborhood for x  i is to use the k-nearest-neighbor algorithm i.e to measure the L 2 norm between dimensions However a neighborhood constructed by k-nearest-neighbor is found using the Euclidean distance which is very sensitive to data noise It means that the neighborhood structure is easy to change when unfavorable noises come in To better explore the neighborhood structure of each dimension and resist th e data noise each dimension x  i is reconstructed as the linear combination of the remaining dimensions by minimizing the  L 1 norm  of bot h the reconstruction coefcients and data noise and the nonzero coefcients indicate the L1 neighborhoods Different from using L 2 neighborhood calculated via Euclidean distance which is very sensitive to data noise L 1 neighborhood structure is more robust to data change when unfavorable noises come in and can automatically realize sparsity Besides the neighbors selected through the  L 1 norm are also dataadaptive which can discover the natural locality information of the data manifold and be a nice property for applications with uneven data distributions 51  In particular for each x  i  we need to calculate the coefcients    1    D  1  T  R  D  1   1 such that x  i  X i   where X i  x  1    x  i  1  x  i  1    x  D  R N   D  1   Thus seeking the best reconstruction for x  i leads to the following L 1 norm optimization problem arg min   x  i  X i   2  s.t    1  k  1 where k is the parameter indicating the maximum number of neighboring dimensions used to reconstruct x  i  This problem can be regarded as a sparse coding problem  and solved by the orthogonal matching pursuit OMP  In practice however a large number of training data N makes x  be a very long vector i.e x   R N  1  which would cause heavy computational cost for optimizing Eq 1 To further reduce the complexity we employ K-means clustering on original data x 1    x N to generate C data centroids denoted as  x 1     x C  In this way we can efciently construct the neighborhood for each dimension by rewriting Eq 1 as arg min    x  i   X i   2 s.t    1  k using  X i   x  1     x  i  1   x  i  1     x  D  R C   D  1  instead of 


LIU et al  LEARNING SHORT BINARY CODES FOR LARGE-SCALE IMAGE RETRIEVAL 1293 X i  R N   D  1   In addition applying K-means to reduce the dimension vectors from R N  1 to R C  1 also effectively benets the L 1 neighborhood construction phase In particular when the number of centroids in K-means satisfying C  D  1 we can construct the more robust neighborhood for each dimension on these centroid dimension vectors since the current data are overcomplete  for t he L 1 constraint optimization In the later experiments section we will intuitively discuss this point as well Having obtained the solution   we can dene that for j  i  x  j is the neighbor of x  i if  j   0 and for j  i  x  j is the neighbor of x  i if  j  1   0 Then for the i th dimension corresponding subspace is spanned by the i th dimension itself and its neighboring dimensions whose corresponding coefcients in  are nonzero For instance if   0  23  0  46  0  0  11  0   0  T for the rst dimension the corresponding subspace is then spanned by the 1-st 2-nd 3-rd and 5-th dimensions and the number of dimensions in this subspace is four 5 In this way the index of neighborhood for the subspace of each dimension will be stored We suppose the dimension of the i th subspace is d i and denote data in the i th subspace 6 by x 1  i     x N  i   R d i  1  2 Bit Evaluation We assign an MCR cost for each dimension bit depending on its discriminative power A criterion to evaluate the discriminative power of a dimension is the margin distance calculated by the optim ized linear classier for the subspace of this dimension The calculated K-means clustering centroids are not only applied to the above neighborhood construction but also used to efciently guide pseudo label assignment of pairwise data We rst assign a pseudo label p u v for the data pair  x u  x v  with K clusters as follows p u v    1  if x u and x v are in the same cluster  1  otherwise  2 Besides we also dene self-pair label p uu  1for u  1    N  Then our goal is to maximize the distances of negative pairs and minimize the distances of positive pairs Considering an example of two negative pairs  x 1  x 2  and  x 1  x 3  with pseudo label  1 both  x 1  x 2  and  x 1  x 3  are expected to be maximized However in the reduced Hamming space there is an over-tting situation that pair  x 2  x 3  will be unnecessarily mapped into a point with the same hash code as shown in Fig 3 To overcome this problem we follow  t o replace the negative pseudo label  1 with a relaxing parameter  where  1  0 The pairwise label is updated as p u v    1  if x u and x v are in the same cluster  otherwise  3 In the i th subspace we adopt the linear classier f i  x   a T i x  b for x 1  i     x N  i  where a i  R d i  1  b  R and 5 In this example  indicates the coefcients excluded the rst dimensions Thus the total number in the subspace spanned by 1-st dimension is 1  3  4 In practice the dimension of each subspace is usually larger than ve for acceptable performance 6 It is noteworthy that centroid dimension vectors  R C  D from Kmeans are only used for efciently nding the neighborhood’s index of each dimension Once the index is obtained the subspace of each dimension is still formed via the original dimension vectors  R N  D          Fig 3 Illustration of the distances in the real-valued space and the Hamming space for two-dimensional example To maximize the distance of negative pairs x 2 and x 3 will unnecessarily collide at point  1  1  in the Hamming space which may cause over-tting for the hash code learning i  1    D  In fact we can denote  x  x T  1  T  Then the classier becomes f i   x   a T i  b   x  which is equivalent to the linear classier without the bias b  For convenience we omit b in the following computation Therefore the i th bit of data x j can be acquired by sgn  f i  x j  i   i  1    D  j  1    N  4 With the above requirement for the positive and negative pairs and the maximum margin criterion we can learn the i th bit by solving the following optimization problem in the subspace of i th dimension min a i  a i  2 2  s.t p u v a T i x u  i   a T i x v i   1  u v  1    N  5 where 1 2  a i  2 is for the margin regularization similar to the SVM Note that when u  v  the constraint p u v a T i x u  i   a T i x v i   1 becomes p uu  a T i x u  i   2  1 which strictly forces every point to be out of the margin We further dene the cost function for the problem in 5 by using the hinge loss penalty as follows F i  a i    a i  2 2    u v max  0  1  p u v a T i x u  i   a T i x v i   6 where  is the balance parameter to control the weight of the two terms By minimizing the above F i  we can nd the solution a  i  Then for the i th bit its MCR cost F  i is dened as F  i  F i  a  i   D k  1 F k  a  k   i  1    D  7 In our method if a bit has a small value of the MCR cost it has relatively strong discriminative power Since the optimal solution to the Eq 6 cannot be directly obtained the gradient descent method is then applied to solve it Let us denote G u v  a i   max  0  1  p u v a T i x u  i   a T i x v i    u v 8 Then we have the following partial derivative of G u v  a i    G u v  a i   0  if 1  p u v a T i x u  i   a T i x v i   0  p u v  x u  i  x T v i   x v i  x T u  i   a i  else  9 


1294 IEEE TRANSACTIONS ON IMAGE PROCESSING VOL 26 NO 3 MARCH 2017 Algorithm 1 Min-Cost Ranking  Note that in our implementation if 1  p u v a T i x u  i   a T i x v i   0 we can set a i  a i   a i where  a i is a small nonzero random vector The same scheme has also been used in and  F i n al l y  w e can w r i t e t h e updat e rul e for a i as follows a i  a i    F i  a i   a i  a i    a i    u v  G u v  a i 012  10 where  is the step length Through repeatedly minimizi ng the Eq 6 for subspaces of all dimensions we can obtain t he bit for each dimension of the original feature as  sgn  f 1  x j  1     sgn  f D  x j  D     j  1    N  The main computational complexity of our MCR comes from three aspects 1 the cost of using K-means for generating pseudo labels in Eq 2 is O  NKt 1   2 the cost of using K-means to achieve neighborhood construction for dimensions is O  NCt 2  and the sparse coding cost is O  C  k  C   D  1   k 3   3 the cost for optimization of each bit is O  t 3 Nk 3   Thus the total theoretical complexity of our MCR is O  NKt 1   O  NCt 2   O  C  k  C   D  1   k 3   O  t 3 Nk 3  during the training phase where t 1 and t 2 indicate the iteration numbers of K-means and t 3 indicates the iteration number in gradient descent of Eq 10 during each bit’s optimization In practice the parallel computation can make the optimization cost of all bits much less than O  t 3 Nk 3   since the optimization of each bit is independent We further rearrange bits by MCR cost F  1    F  D in the ascending order The rst m bits are then selected and grouped to form the nal code where m is the target code length 3 Adaptive Gradient Descent AGD Moreover an adaptive step length is also adopted in the gradient descent procedure to accelerate the convergency Specically with the     Fig 4 Comparison of MCR cost and empirical error with respect to the number of AGD iterations and bit index on GIST-1M initialization   1 for the t th iteration we will increase the value of  by updating   1  2  in the next iteration if F i  a  t  i   F i  a  t  1  i   Otherwise we set   0  5   We summarize the whole MCR algorithm in Algorithm 1 4 MCR Cost vs Empirical Error We rst denote Er i  1 N 2    u v  sgn  f i  x u  i   sgn  f i  x v i     p u v  as the pairwise empirical error on the i th subspace Fig 4\(upper compares the average cost function value of Eq 6 i.e 1 D  D i  1 F i  a i   with the average empirical error  i.e 1 D  D i  1 Er i  during the AGD iterations As we can see both the MCR cost and the error rat e can be well converged within 50 iterations via AGD In Fig 4\(bottom we also illustrate the ascending rearrangement of the MCR cost F  1    F  D and the empirical error Er 1    Er D after nishing AGD It is observed that when the value of MCR cost is small on one bit the corresponding empirical error will be relatively low as well The consistent tendency of the blue and red lines 


LIU et al  LEARNING SHORT BINARY CODES FOR LARGE-SCALE IMAGE RETRIEVAL 1295 indicate that the MCR cost can naturally reect the separability of positive and negative pairs and represent the discriminative power of each bit IV E XPERIMENTS In this section we apply the proposed MCR method for large-scale image retrieval tasks Three realistic datasets are used to evaluate all methods GIST-1M  it contains one million 960-dim GIST feature vect ors The dataset is publicly available 7  SIFT-1M  it contains one m illion 128-dim SIFT feature vectors The dataset is publicly available 8  Tiny-1M  it contains one million 384-dim G IST feature vectors which are computed from a subset of 80M Tiny images  62 For all three datasets we respectively take 1  000 images as the queries by random selection and use the remaining to form the gallery database To construct the training set 100 000 samples from the gallery database are randomly selected for all of the compared methods Additionally we also randomly choose another 50  000 data samples as a cross-validation set for parameter tuning In the querying phase the returned points are regarded as true neighbors if they lie in the top ranked 100 200 and 1000 points for GIST-1M SIFT-1M and Tiny1M respectively We evaluate the retrieval results in terms of the Mean Average Precision 9 MAP and the precision-recall curves In addition we report the parameter sensitive analysis for our MCR method All the experiments are completed using Matlab 2014a on a workstation with a 12-core 3.2GHz CPU and 120GB of RAM running the Linux OS A Compared Methods and Settings In the experiments we denote our method as MCR-L1 since the subspace spanned by the neighborhood of each dimension is based on the L 1 norm reconstruction Beyond that we have also tested the MCR-L2 method which nds the nearest neighboring dimensions for each subspace using the L 2 norm distance a.k.a the Euclidean distance instead of L 1 norm As our MCR is the unsupervised hashing method we compared our methods with other six state-of-the-art unsupervised hashing techniques including LSH  P C A H  9 SpH  R MMH 15 I TQ 10 D G H 16 and S pherH 14 In particular we use RMMH with the triangular L 2 kernel which can produce the best performance on the nearest neighbor NN search in the original paper and the number of samples M for the RMMH hashing function is equal to 32 as recommended by  B es i d es  t o b e c ons i s t e nt w i t h the original implementation 10  P CA is ap p lied p r i o r to using ITQ Furthermore for SpherH the spherical Hamming distance is used in our experiments rather than the ordinary hamming distance We used the publicly available codes of LSH SpH ITQ DGH and SpherH and implemented PCAH and RMMH ourselves Targeting on shorter binary codes we limit the code length of evaluation up to 512 bits on the GIST1M dataset 256 bits on the Tiny-1M dataset and 128 bits on 7 http://corpus-texmex.irisa.fr 8 http://corpus-texmex.irisa.fr 9 The ground-truth is dened by the 100 200 and 1000 nearest neighbors via linear scan based on the Euclidean distance TABLE I R ETRIEVAL R ESULTS C OMPARISON OF MAP AT T OP 1000 S AMPLES ON SIFT 1M DATASET   SIFT-1M dataset respectivel y Under the same experimental setting all the parameters used in the compared methods have been strictly chosen accordin g to their original papers For our methods the number of centroids C in MCRL1 is selected from one of  100  200    1500  with the step of 100 and one of  50  200    500  with the step of 50 on the cross-validation set for GIST-1M and Tiny-1M respectively The pairwise label of each data pair is determined by their corresponding clusters and the relaxing parameter for the negative pair is set as   0  85 The balance parameter  for each dataset is then selected from one of the values in the range of  10  3  10 2   which yields the best performance on the cross-validation set We x the maximum number of the iteration of AGD at 50 which has been proved to converge as shown in Fig 4 Since bits learning on each subspace of the proposed MCR-L1/MCR-L2 is independent in our experiments the parallel computation scheme is adopted via our multi-core processor which has effectively reduced the training time Considering the uncertainty caused by K-means all the reported results by our methods are the mean accuracies from 50 runs B Evaluation Over Hashing Methods 1 Results Comparison Fig 5 compares the MAP of 100-nearest neighbor 200-nearest neighbor and 1000-nearest neighbor search of all methods on GIST-1M dataset and Tiny-1M datasets Additionally we illustrate the MAP of top 1000-nearest neighbors on a relatively low-dimensional dataset SIFT-1M in Table I Generally the accuracies on the Tiny-1M dataset are lower than those on the GIST-1M and SIFT-1M datasets since features in the Tiny-1M have larger variations than those on GIST-1M and SIFT-1M Top-1000 nearest neighbor search produce better results than those on top-100 and top-200 search Compared with all other methods our MCR-L1 and MCR-L2 can achieve better results when the code length is shorter than 128 64 bits on GIST-1M and Tiny-1M respectively For low-dimensional dataset SIFT-1M the best performance of our methods are always achieved at 64 bits as well Among the compared methods SpherH consistently gives the best performance since the spherical distance function is used in SpherH which is proved to lead a improvement of results in I TQ  D G H and R MMH s h are 


1296 IEEE TRANSACTIONS ON IMAGE PROCESSING VOL 26 NO 3 MARCH 2017          Fig 5 Performance comparison of MCR and other state-o f-the-art methods on the GIST-1M and Tiny-1M datasets                 Fig 6 Comparison of precision recall curves with different bits on GIST 1M and Tiny-1M datasets Ground truth is dened by Euclidean neighbors the similar performance on all thr ee datasets at different bits Conversely LSH PCAH and SpH achieve lower accuracies than other methods With the increase of the code length the accuracies obtained by SpherH ITQ RMMH SpH and LSH signicantly rise up However the opposite tendency occurs with PCAH The similar circumstance can also be found in  63 a nd 64 Different from these compared methods the performance of our method climbs up sharply at the short code lengths then goes down when the length of code further increases The reason is that there always e xists a trade-off between discriminative power and redundancy for ranking based selection methods such as T aki n g a n e xampl e of t h e t op-1000 nearest neighbor search on the GIST-1M dataset with the increase of the code length from the 1-st to 128-th bit more informative bits will help the hash codes to achieve higher performance The best performance can be acquired when the code length reaches an optimal number 128 bits After that from 129-th to 512-th bit the b its with low separability will be added to the code which br ings redundancy and negative effect to the performance The exactly same tendency can also be seen in due t o t h e s i m i l a r r anki ng s c heme As we can observe from Fig 5 our MCR-L1 and MCRL2 can achieve comparative ret rieval accuracies with the best-performed method SpherH but with signicantly shorter codes Besides our method with L 1 constructed subspaces i.e MCR-L1 shows better results than MCR-L2 since L 1 constructed subspaces are proved to be more robust to noise adaptive to the neighborhood and keep the intrinsic structures among different dimensions  M oreo v e r  from Table I we can discover our methods have less superiority on SIFT-1M dataset compared with other two datasets The reason is that long features   500 dimensions may include more redundant dimensions while short features have fewer redundant ones since we assume all features may have the same percentage of redundant dimensions Thus for 128-d SIFT features fewer redundant dimensions will make our MCR not signicantly better than ITQ and SpherH which are regarded as the best performed unsupervised hashing methods in this paper Fig 6 also shows a series of precision-recall curves with different code lengths on both GIST-1M and Tiny-1M datasets with the 1000-nearest neighbor as the ground truth To avoid confusion we omit LSH and PCAH in the comparison which have been demonstrated with low performance in Fig 5 By comparing the Area Under the Curve AUC our MCR-L1 achieves apparently better performance than other methods on short bits and the performance slightly goes down when long hash codes are adopted 2 Parameter Sensitivity Analysis In this section we illustrate the effect of some essential parameter settings on retrieval accuracies Fig 7 repor ts the performance by varying 


LIU et al  LEARNING SHORT BINARY CODES FOR LARGE-SCALE IMAGE RETRIEVAL 1297                                                                                                                                                                              Fig 7 Parameter sensitivity analysis of the sparse parameter k and the balanced parameter   TABLE II P ARAMETER S ENSITIVITY A NALYSIS OF K  I  E  U SING IN P SEUDO L ABEL C ONSTRUCTION  VS  MAP@64 bits ON ALL T HREE D ATASETS parameters k and  with 128-bit codes on GIST-1M and 64-bit codes on Tiny-1M When tuning the parameter k from 5 to 30 the retrieval accuracy curves of MCR-L1 appear to be more stable and insensitive to k compared with the curves of MCRL2 The best values of k on GIST-1M for both MCR-L1 and MCR-L2 are larger than those on Tiny-1M since the dimension of the original feature vector in GIST-1M is higher Furthermore the balance parameter  selected from  10  1  1  can achieve the good performance for both MCR-L1 and MCR-L2 according to Fig 7 In addition we also show the effect of performance by varying the number of the clusters  C  in K-means From Fig 8 we can observe that with C approaching the dimensionality  D  of the original features the performance of MCR-L1 will dramatically drop since when C 012 D  1 the coding procedure cannot nd a group of overcomplete basis for better reconstruction under L 1 constraint as we mentioned in Section III-B While for MCRL2 the accuracy curves rise up until n 012 300 and n 012 150 for GIST-1M and Tiny-1M respectively and after that the curves of MCR-L2 become stable At last we illustrate the effectiveness of K-means used in pseudo label construction i.e Eq 2 and Eq 3 in Table II by varying the cluster number K  From the results we can discover that for 100,000 training samples the best K of K-means clustering should be 800  1600 for GIST-1M SIFT-1M and Tiny-1M datasets C Evaluation Over Bit Selection Schemes Beyond the comparison with different hashing methods we also evaluate MCR-L1 with some existing binary bit selection schemes and report the relevant retrieval MAP on the GIST-1M Tiny-1M and SIFT-1M datasets using different code lengths One straightforward scheme is to apply random selection  Random  without considering any properties of the bits Furthermore we also compare with a  Greedy           Fig 8 Mean performance of 50 runs vs parameter C  selection 48 sch e m e wh ich c o n s id er s t h e ab ility o f sim ilar ity preserving on each bit Besides another state-of-the-art scheme called Normalized Dominant Set  NDomSet   is used in our evaluation as well Since all of the above methods are used for directly selecting dominant bits from binary codes rather than learni ng binary codes from original feature vectors we combine the se selection schemes with two top-performed hashing methods i,e SpherH and RMMH and the proposed MCR-L1 algorithm in this experiment It is noteworthy that for MCR-L1 we only complete the binary code learning without the ranking procedure From Table III we can conclude that NDomSet can achieve better performance together with SpherH and RMMH on all three datasets compared with the other two schemes since NDomSet considers both similarity preserving and independence between bits Besides t he Greedy scheme consistently leads to worse performance than the random selection Since the MCR cost is calculated for each bit in their neighborhood of dimensions the relationship between different dimensions has been considered Meanwhile  for each bit’s subspace their neighborhood of dimensions could be overlapped via L1norm and the information can be compensated each other Thus the mutual complement of different bits is considered as 


1298 IEEE TRANSACTIONS ON IMAGE PROCESSING VOL 26 NO 3 MARCH 2017 TABLE III P RECISION C OMPARISON OF MCR AND O THER bit S ELECTION M ETHODS   well In conclusion our method takes the independence and correlation between different bits into account Therefore the proposed selection scheme by ranking the MCR cost value i.e MCR-L1 can achieve comp arative retrieval accuracies with MCR  NDomSet but leads to signicantly better performance than MCR   Greedy and MCR  Random V C ONCLUSION In this paper a novel unsupervised hashing approach called Min-cost Ranking MCR has been proposed for large-scale data retrieval A two-stage procedure is involved for learning and selecting salient bits in MCR The experimental results have demonstrated that the proposed method can achieve competitive performance compared with state-of-the-art hashing methods but with signicantly shorter binary codes which is more feasible for realistic applications with limited computational resources With the recent progress on very largescale action dataset 65  w h i ch can co n t ain o v e r o n e m illio n complex action videos collected from websites in future work it would be interesting to utilize our method on such dataset for action retrieval tasks R EFERENCES  Y  L iu F  W u Y  Y a ng Y  Z huang and A  G  H auptm a nn Spline regression hashing for fast image search IEEE Trans Image Process  vol 21 no 10 pp 4480–4491 Oct 2012  J  S ong Y  Y a ng Z  H u ang H  T  S h en a nd J  L uo E f f ecti v e m ultiple feature hashing for large-scale near-duplicate video retrieval IEEE Trans Multimedia  vol 15 no 8 pp 1997–2008 Dec 2013  L  C ao Z  L i Y  Mu a nd S F  C hang Subm odular video h as hing A unied framework towards video pooling and indexing in Proc ACM Int Conf Multimedia  2012 pp 299–308 4 Y G a o M S h i D T a o  a n d C  X u  D a t a b a s e s a l i e n c y f o r f a s t image retrieval IEEE Trans Multimedia  vol 17 no 3 pp 359–369 Mar 2015  L  L iu M  Y u and L  S hao L atent s tructure pres erving has h ing  Int J Comput Vis  2016 pp 1–19  J  Y u D T a o M W a ng and Y  R ui  L earning to rank us ing u s e r c licks and visual features for image retrieval IEEE Trans Cybern  vol 45 no 4 pp 767–779 Apr 2015  M  S  C harikar  Sim ilarity es timation techniques from rounding algorithms in Proc Symp Theory Comput  2002 pp 380–388 8 B  K ulis and K  G r a um an  K e r n elized localitys ens iti v e has h ing  IEEE Trans Pattern Anal Mach Intell  vol 34 no 6 pp 1092–1104 Jun 2012  J  Z  W ang S K u m a r  and S  F  Chang Sem i-s upervis ed has h ing f or large-scale search IEEE Trans Pattern Anal Mach Intell  vol 34 no 12 pp 2393–2406 Dec 2012  Y  Gong and S  L azebnik Itera tive quantization A procrustean approach to learning binary codes in Proc IEEE Conf Comput Vis Pattern Recognit  Jun 2011 pp 817–824  Y  W e is s  A  T o r r a lba and R  F er gus   S p ectr a l h as hing  i n Proc Conf Neural Inf Process Syst  2008 pp 1753–1760  W  L i u J  W a ng S K u m a r  and S  F  Chang Has hing with graphs   in Proc Int Conf Mach Learn  2011 pp 1–8  D Z h ang J  W a ng D Cai and J  L u Self-taught hashing for fast similarity search in Proc Int ACM Conf Res Develop Inf Retr SIGIR  2010 pp 18–25  J  P  H eo Y  L ee J  He S  F  Chang and S  E  Y oon Spherical hashing in Proc IEEE Conf Comput Vis Pattern Recognit  Jun 2012 pp.2957–2964  A J o ly and O  B uis s on Random m a xim u m m ar gin h as hing  i n Proc IEEE Conf Comput Vis Pattern Recognit  Jun 2011 pp 873–880  W  L i u C Mu S  K um ar  a nd S F  C hang Dis crete g raph has h ing  in Proc Conf Neural Inf Process Syst  2014 pp 3419–3427  L  L i u M Y u and L  S hao L atent s tructure pres erving has h ing  Int J Comput Vis  2016 doi 10.1007/s11263-016-0931-4  Y  L i n R J i n D Cai S Y a n and X  L i Com pres s e d h as hing  i n Proc IEEE Conf Comput Vis Pattern Recognit  Jun 2013 pp 446–451  L  L i u M Y u  a nd L  Shao  Multi vie w alignm ent h as hing for e f  cient image search IEEE Trans Image Process  vol 24 no 3 pp 956–966 Mar 2015  D  W a ng X  G a o X  W a ng and L  H e S em antic topic m ultim odal hashing for cross-media retrieval in Proc Int Joint Conf Artif Intell  Jun 2015 pp 3890–3896  L  L i u M Y u  a nd L  Shao  Proj ection bank From high-dimensional data to medium-length binary codes in Proc IEEE Int Conf Comput Vis  Sep 2015 pp 2821–2829  J  Song H T  Shen J  W ang Z  Huang N Sebe a nd J  W a ng A distance-computation-free search scheme for binary code databases IEEE Trans Multimedia  vol 18 no 3 pp 484–495 Mar 2016  J  Song L  Gao Y  Y a n D Z h ang and N  S ebe Supervis ed has h ing with pseudo labels for scalable multimedia retrieval in Proc 23rd ACM Int Conf Multimedia  2015 pp 827–830  L  L i u M Y u  a nd L  Shao  Uns upervis ed local f eature h as hing for image similarity search IEEE Trans Cybern  vol 46 no 11 pp 2548–2558 Nov 2016  J  W a ng T  Z h ang J  Song N Sebe a nd H T  Shen  J un 2016 A survey on learning to hash O  A v a ilable https ar x i v  org/abs/1606.00185  L  L i u L  Shao a nd X L i   E v olutionary com p act em bedding for l ar gescale image classication Inf Sci  vol 316 pp 567–581 Sep 2015  X He D  C ai a nd P  Niyogi  L a pl acian score for feature selection in Proc Conf Neural Inf Process Syst  2005 pp 507–514  L  L i u a nd L  Shao  Sequential c om pact code learning for uns upervis ed image hashing IEEE Trans Neural Netw Learn Syst  vol 27 no 12 pp 2526–2536 Dec 2016  D C G Pedronette J  A lm eida  and R da S Torres A scalable re-ranking method for content-based image retrieval Inf Sci  vol 265 pp 91–104 May 2014  B X u  J  B u C Chen C  W ang D  Cai and X  H e E M R  A s calable graph-based ranking model for c ontent-based image retrieval IEEE Trans Knowl Data Eng  vol 27 no 1 pp 102–114 Jan 2015 


LIU et al  LEARNING SHORT BINARY CODES FOR LARGE-SCALE IMAGE RETRIEVAL 1299  S  M u r a la R  P  M ahes hw ar i and R  B alas ubr am anian L ocal tetr a patterns A new feature descriptor for content-based image retrieval IEEE Trans Image Process  vol 21 no 5 pp 2874–2886 May 2012  T  Dharani a nd I L  Aroquiaraj  A s urv e y o n c ontent b as ed im age retrieval in Proc Int Conf Pattern Recognit Inf Mobile Eng  2013 pp 485–490  J  W a n et al  Deep learning for content-based image retrieval A comprehensive study in Proc ACM Int Conf Multimedia  2014 pp 157–166  G H L i u a nd J  Y  Y a ng Content-bas e d i m a ge retrie v a l u s i ng color difference histogram Pattern Recognit  vol 46 no 1 pp 188–198 2013  Z  L i n G Ding M Hu a nd J  W a ng Sem antics pres erving has h ing for cross-view retrieval in Proc IEEE Conf Comput Vis Pattern Recognit  Jun 2015 pp 3864–3872  J  Song Y  Y a ng Y  Y a ng Z  Huang and H  T  S hen Inter m edia hashing for large-scale retrieval from heterogeneous data sources in Proc ACM SIGMOD Int Conf Manage Data  2013 pp 785–796  X  Z h ai Y  P eng and J  X iao Cr o s s m odality cor r e lation p r opagation for cross-media retrieval in Proc IEEE Int Conf Acoust Speech Signal Process ICASSP  Mar 2012 pp 2337–2340  F  Feng X W a ng and R  L i Cros s modal retrieval with correspondence autoencoder in Proc ACM Int Conf Multimedia  2014 pp 7–16  J  Y u  Y  R ui a nd B Chen  E xploiting c lick c ons tr aints a nd m u ltivie w features for image re-ranking IEEE Trans Multimedia  vol 16 no 1 pp 159–168 Jan 2014  J  Y u  Y  R ui a nd D T a o Click p rediction for Web image reranking using multimodal sparse coding IEEE Trans Image Process  vol 23 no 5 pp 2019–2032 May 2014  V  E  L i ong J  L u  G  W ang P  Moulin a nd J  Z hou Deep has h ing for compact binary codes learning in Proc IEEE Conf Comput Vis Pattern Recognit  Jun 2015 pp 2475–2483  F  Z h ao Y  H uang L  W a ng and T  T an  Deep s e m a ntic ranking bas e d hashing for multi-label image retrieval in Proc IEEE Conf Comput Vis Pattern Recognit  Jun 2015 pp 1556–1564  H L a i Y  P a n Y  L i u and S  Y an  Sim u ltaneous feature l earning and hash coding with deep neural networks in Proc IEEE Conf Comput Vis Pattern Recognit  Jun 2015 pp 3270–3278  B Z huang G L i n C Shen a nd I Reid  F a s t training of triplet-bas e d deep binary embedding networks in Proc IEEE Conf Comput Vis Pattern Recognit  Jun 2016 pp 431–439  Y  Cao M L ong and J  W ang Feb  2016  Correlation h as hing network for efcient cross-modal retrieval O  A v a ilable https://arxiv.org/abs/1602.06697  Q Y  J iang and W  J  L i   Feb  2016  Deep cros s m odal h as hing  O A v a ilable https arxi v  o r g abs 160 2 02255  Q Gu Z  L i and J  H an  Feb  2012 Generalized sher score for feature selection On A v a ilable https arxi v  o r g abs 120 2 3725  Y  M u  X  C hen X  L i u T  S  C hua a nd S  Y a n M ultim edia s e m a ntics aware query-adaptive hashing with bits recongurability Int J Multimedia Inf Retr  vol 1 no 1 pp 59–70 2012  J  He R  R adhakris hnan S F  C hang and C  B auer   Com p act has h ing with joint optimization of search accuracy and time in Proc IEEE Conf Comput Vis Pattern Recognit  Jun 2011 pp 753–760  X L i u J  He B  L ang and S  F  Ch ang Hash bit selection A unied solution for selection problems in hashing in Proc IEEE Conf Comput Vis Pattern Recognit  Jun 2013 pp 1570–1577  B Cheng J  Y a ng S Y a n Y  Fu a nd T  S Huang L earning with  1 graph for image analysis IEEE Trans Image Process  vol 19 no 4 pp 858–866 Apr 2010  L  L i u a nd L  S h ao  D i s c r i m i nati v e par tition s par s ity analys is   in Proc Int Conf Pattern Recognit  2014 pp 1597–1602  S Y a n a nd H W a ng Sem i-s upervis ed learning by s p ars e representation in Proc SIAM Int Conf Data Mining  2009 pp 792–801  J  W a ng J  Y a ng K  Y u  F  L v  T  H u ang and Y  G ong L ocalityconstrained linear coding for image classication in Proc IEEE Soc Conf Comput Vis Pattern Recognit  Jun 2010 pp 3360–3367  J  W r ight A  Y  Y ang A Ganes h  S  S  S as try  and Y  M a Rob us t f ace recognition via sparse representation IEEE Trans Pattern Anal Mach Intell  vol 31 no 2 pp 210–227 Feb 2009  J  Y a ng K Y u  Y  G ong and T  H uang L inear s p atial p yram id matching using sparse coding fo r image classication in Proc IEEE Conf Comput Vis Pattern Recognit  Jun 2009 pp 1794–1801  Y  C P a ti R Rezaiif ar  a nd P  Kris hnapras ad  Orthogonal m atching pursuit Recursive function approximation with applications to wavelet decomposition in Proc 27th Asilomar Conf Signals Syst Comput  Nov 1993 pp 40–44  C L e ng J  Cheng J  W u  X  Z hang and H  L u Supervis ed has h ing with soft constraints in Proc Conf Inf Knowl Manage  2014 pp 1851–1854  N Kw ak  Principal c om ponent analys is bas e d o n L 1-norm m aximization IEEE Trans Pattern Anal Mach Intell  vol 30 no 9 pp 1672–1680 Sep 2008  H W a ng X L u  Z  H u and W  Z heng Fis her d is crim inant a nalys i s with L1-norm IEEE Trans Cybern  vol 44 no 6 pp 828–842 Jun 2014  A T o rralba R Fer gus  a nd Y  W e is s  Sm a ll codes a nd lar g e i m a ge databases for recognition in Proc IEEE Conf Comput Vis Pattern Recognit  Jun 2008 pp 1–8  A  T o r r a lba R F e r gus  a nd W  T  F r eem an  80 m illion tin y i m a ges  A large data set for nonparametric object and scene recognition IEEE Trans Pattern Anal Mach Intell  vol 30 no 11 pp 1958–1970 Nov 2008  Z  J i n C L i  Y  L in a nd D Cai Dens ity s e ns iti v e has h ing  IEEE Trans Cybern  vol 44 no 8 pp 1362–1371 Aug 2014  J  W a ng S K u m a r  and S  F  Chang Sequential projection learning for hashing with compact codes in Proc Int Conf Mach Learn  2010 pp 1127–1134  A Karpathy  G  T oderici S  S hetty T Leung R Sukthankar and L Fei-Fei Large-scale video classication with convolutional neural networks in Proc IEEE Conf Comput Vis Pattern Recognit  Jun 2014 pp 1725–1732 Li Liu received the B.Eng degree in electronic information engineering from Xi’an Jiaotong University Xi’an China in 2011 and the Ph.D degree from the Department of Electronic and Electrical Engineering The University of Shefeld Shefeld U.K in 2014 He was a Research Fellow with Northumbria University He is currently a Senior Research Associate with the School of Computing Sciences University of East Anglia Norwich U.K His research interests include computer vision machine learning multimedia and data mining Mengyang Yu S’14 received the B.S and M.S degrees from the School of Mathematical Sciences Peking University Beijing China in 2010 and 2013 respectively He is currently pursuing the Ph.D degree with the Department of Computer Science and Digital Technologies Northumbria University Newcastle upon Tyne U.K His research interests include computer vision machine learning and data mining Ling Shao M’09–SM’10 was a Professor with Northumbria University from 2014 to 2016 a Senior Lecturer with the Department of Electronic and Electrical Engineering The University of Shefeld from 2009 to 2014 and a Senior Scientist with Philips Research The Netherlands from 2005 to 2009 He is currently a Professor with the School of Computing Sciences University of East Anglia Norwich U.K His research interests include computer vision image/video processing and machine learning He is a fellow of the British Computer Society and the Institution of Engineering and Technology He is an Associate Editor of the IEEE T RANSACTIONS ON I MAGE P ROCESSING  the IEEE T RANSACTIONS ON N EURAL N ETWORKS AND L EARNING S YSTEMS the IEEE T RANSACTIONS ON C IRCUITS AND S YSTEMS FOR V IDEO T ECHNOL OGY  and several other journals 


0018-9340 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/TC.2017.2669964, IEEE Transactions on Computers 2016 12 rate time time C of v  of 022  tuning eed other easing compensate also whole espondtimes educed dimechanism con\002gurations parameter v head 022 the completion iniparameter v eases the v performance parameter v deterioration head 022 performance ol parameter v to the head 022 with parameter v head 022 f job time oposed applying start heduling scheduling e under shows the The original when instart tasks Nonge tasks then difNon-delay esults eases i.e both the both scheduling Scalability time algorithm tasks map the 150 


0018-9340 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/TC.2017.2669964, IEEE Transactions on Computers 2016 13 execution each seconds is ol econ\002gindependent to components systems R D W K management cluse.g comscheduling ARN the second generation of Hadoop alent ona is developed manage ce Omega a new parallel using both scalabilMesos abstracts CPU memory  storage and other faultbe managers time optimization ests various ovisioning job scheduling and self-tuning con\002guration 12 Rao al Sail\002sh intermediate Jinda al  pr oposed a new data layout data data Dittrich al Hadoop a of incoming most start con\002guration odotou al Star\002sh an optimization framework that work con\002gurations MapRethe onment scheduling that oved techniques The default FIFO work computfor e.g ces studies start with olf al described FLEX a 003exible and intelligent allocation scheme add-on Scheduler Curino al descheduling that pr ovides the these MapReclusters C D F E W K on deterioraeservationthe design mideperformance key placement localitytimeapplications tasks ethat ovement ovetwo ARNwork scheduling paradigms ARN A T ch CNS-1422119 R S  M Zaharia M Chowdhury  T  Das A Dave J Ma M McCauley  distributed comin Design NSDI 2012  Y ahoo 223Let spark 003y Advantages and use cases for spark on g/2014 


0018-9340 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/TC.2017.2669964, IEEE Transactions on Computers 2016 14  V  K V avilapalli A C Murthy  C Douglas S Agarwal M Konar  Curino 223Apache in ACM SoCC 2013  P  C Fou ndry  223Y arn r esour ce management,\224 cemanagement.html  B Hindman A Konwinski M Zaharia A Ghodsi A Joseph for in oc Implemen\(NSDI 2011  223PUMA Pur due mapr educe benchmark suite,\224 http://web.ics due.edu 030 fahmad/benchmarks.htm  L W ang C Luo Y  He J Zhan Z K X Li Y  Zhu S Zhang benchmark in Symposium HPCA 2014  M Zaharia D Borthakur  J S Sarma K Elmeleegy  S Shenker  achieving in opean oSys 2010  S  Documentation 223spark.deploy spr eadout,\224 g/docs/1.4.0/spark-standalone.html  Y  Chen B Y ang A Abraham and L Peng 223Automatic design of evolutionary in Systems 2007  Y  W ang C Xu X Que X Li and W  Y u 223Jvm-bypas s shuf 003ing in IPDPS 2013  D Cheng J Rao Y  Gu o and X Zhou 223Impr oving mapr educe task in ere 2014  R Appuswamy  C Gkantsidis D Narayanan O Hodson and ethink?\224 in SoCC 2013  eBay  223Using spark to ignite data analytics,\224 ebaytechblog.com/2014/05/28/using-spark-to  Cloudera 223Con\002guration parameters,\224 on  M Li L Zeng S Meng J T an L Zhang N Fuller  and A R in oc Distributed HPDC 2014  Cloudera 223Apache spark r esour ce management and yarn http://blog.cloudera.com/blog/2014/05/apachece-management-and-yarn-app-models  Facebook 223Hadoop cor ona the next version of chive/hadoopona  M Schwarzkopf A Konwinski M Abd-el Malek and J W ilkes clusters,\224 in oSys 2013  C C D E Difallah C Douglas S Krishnan R Ramakrishnan dont in SoCC  2014  J W olf D Rajan K Hildr um R Khandekar  V  Kumar  allocation in the e 2010  S Rao R Ramakrishnan A Silberstein M Ovsiannikov  and ocessing,\224 in SoCC 2012  A Jinda J Quian?Ruiz and J Dittrich 223T r ojan data layouts Right in Cloud SoCC 2011  J Dittrich J.-A Quian 264 and cheetah noticing  2010  H Her odotou H Lim G Luo N Borisov  L Dong F  B Cetin analytin ch CIDR 2011 Cheng deUniersity 2009 from in essor the ree computing Zhou PhD Uni the of lies speci\002BigData autonomic net of IEEE Lama ree ing 2003 Sciado Assistant SciHis and Cloud u SciUniInComXIDIAN reersity His unicaProcessing Jiang deChinese 1995 t esComputer are net  


0090-6778 \(c This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/TCOMM.2017.2693262, IE\EE Transactions on Communications 7 1 5 9 A  G u p t a  R  K r i s h n a s w a m y  a n d K  P r u h s  g d  0 M  G u p t a a n d S  S i n g h  223 G r e e n i n g o f t h e I n t e r n e t  224 i n M M  1 R  K r i s h n a s w a m y  V  N a g a r a j a n  K  P r u h s  a n d C  S t e i n  223 C l u s t e r b e f o r e d n C   2 D  L i  Y  S h a n g  a n d C  C h e n  223 S o f t w a r e d e 002 n e d g r e e n d a t a c e n t e r n M   3 Q  L i  M  X u  Y  Y a n g  L  G a o  Y  C u i  a n d J  W u  223 S a f e a n d p r a c t i c a l 224    4 Y  L i n a n d Q  W u  223 C o m p l e x i t y a n a l y s i s a n d a l g o r i t h m d e s i g n f o r a d 224    5 F  M o g h a d d a m  P  L a g o  a n d P  G r o s s o  223 E n e r g y e f 002 c i e n t n e t w o r k i n g 224 s  6 A  O r g e r i e a n d L  L e f e v r e  223 A s u r v e y o n t e c h n i q u e s f o r i m p r o v i n g 224  s  7 T  P a n  T  Z h a n g  J  S h i  Y  L i  L  J i n  F  L i  J  Y a n g  B  Z h a n g  X  Y a n g  s 224    8 T  S h u  C  W u  a n d D  Y u n  223 A d v a n c e b a n d w i d t h r e s e r v a t i o n f o r n N   9 J  T a n g  B  M u m e y  Y  X i n g  a n d A  J o h n s o n  223 O n e x p l o i t i n g 003 o w n E M  0 C i s c o 7 6 0 3 C h a s s i s  2 0 1 6  h t t p    w w w  c i s c o  c o m  c  e n  u s  p r o d u c t s  c o l l a t e r a l  r o u t e r s  7 6 0 3 t a   1 C i s c o C R S 3 1 6 S l o t S i n g l e S h e l f S y s t e m   3   2 C i s c o C R S 4 S l o t S i n g l e S h e l f S y s t e m   3 t   3 L a w r e n c e B e r k e l e y N a t i o n a l L a b o r a t o r y  U  S  D e p a r t m e n t o f E n e r g y  e  A v a i l a b l e   4 O S C A R S  H o w I t W o r k s  2 0 1 6  h t t p s    w w w  e s  n e t  e n g i n e e r i n g  5 A  V i s h w a n a t h  K  H i n t o n  R  A y r e  a n d R  T u c k e r  223 M o d e l i n g e n e r g y 224 d   6 L  W a n g  F  Z h a n g  K  Z h e n g  A  V a s i l a k o s  S  R e n  a n d Z  L i u  223 E n e r g y r n S 226  7 X  W a n g  Y  Y a o  X  W a n g  a n d Q  C a o  223 C A R P O  C o r r e l a t i o n n E M  8 M  Z h a n g  C  Y i  B  L i u  a n d B  Z h a n g  223 G r e e n T E  P o w e r a w a r e t r a f 002 c n P 226  9 Z  Z h a n g  Y  B e j e r a n o  a n d S  A n t o n a k o p o u l o s  223 E n e r g y e f 002 c i e n t I P c o r e 224    0 Y  Z h a o  S  W a n g  S  X u  X  W a n g  X  G a o  a n d C  Q i a o  223 L o a d l n M   u r n  r  t f r r  u r r  l d g r w s   


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access  16      en-US  en-US 1 en-US D en-US ning en-US  en-US rnen-US el en-US he en-US l en-US  en-US t en-US  en-US ke en-US s en-US  en-US  en-US lly en-US r en-US reen-US o en-US  en-US ediction en-US s en-US  en-US  en-US rning en-US  en-US  en-US el en-US  en-US  en-US  en-US g en-US  en-US el en-US  en-US  en-US which en-US help en-US s en-US  en-US hink en-US  en-US  en-US  en-US rning en-US  en-US  en-US lly en-US  en-US h en-US ceen-US b en-US on en-US  en-US rning en-US  en-US 2 en-US t en-US  en-US rning en-US  en-US r en-US  en-US  en-US  en-US  en-US he en-US he en-US  en-US comen-US ion en-US ources en-US  en-US g en-US ine en-US ing en-US  en-US e en-US  en-US  en-US 3 en-US  en-US rning en-US  en-US in en-US 3    1 7 4  en-US  en-US i en-US en-US 9  cr it ica lly ex p la in s  t he p re s ent  s t a t e en-US en-US of en-US en-US elen-US in en-US rning en-US  en-US  en-US n en-US rning en-US  en-US n en-US 0  a u t hor s  d is cu s s  t he p a s t  a nd  P res ent  of  en-US  en-US rning en-US  en-US o en-US rnen-US ing en-US  en-US rning en-US  en-US n en-US 5    1 7 6  en-US  en-US e en-US ed en-US  en-US ion en-US  en-US u en-US nen-US n en-US w en-US  en-US ery en-US ce en-US rning en-US  en-US rning en-US  en-US king en-US inen-US o en-US  en-US ion en-US  en-US in en-US  en-US l en-US ly en-US  en-US l en-US  en-US ning en-US  en-US in en-US 7 en-US en-US 9 en-US  en-US en-US n en-US 4 en-US  en-US eo en-US  en-US g en-US in en-US 0 en-US  en-US  en-US  en-US rning en-US  en-US  en-US 6   d ee p  f u lly convolu t iona l neu ra l  en-US rth en-US  en-US  en-US ion en-US  en-US in en-US 1 en-US  en-US  en-US  en-US work en-US 2 en-US r en-US ion en-US 4 en-US en-US R en-US en-US he en-US  en-US ion en-US  en-US en en-US  en-US in en-US t en-US he en-US  en-US  en-US 2 en-US  en-US  en-US  en-US  en-US e en-US rning en-US  en-US  en-US ion en-US  en-US  en-US  en-US  en-US lica en-US  en-US in en-US  en-US  en-US s en-US  en-US n en-US  en-US ion en-US  en-US 3 en-US  en-US ion en-US  en-US f en-US  en-US 4 en-US  en-US l en-US ion en-US  en-US in en-US 5 en-US  en-US n en-US  en-US 6 en-US  en-US  en-US  en-US  en-US in en-US o en-US ing en-US  en-US rning en-US  en-US o en-US  en-US  en-US ion en-US  en-US  en-US  en-US en-US ing en-US  en-US  en-US rning en-US  en-US  en-US v en-US  en-US  en-US rning en-US  en-US  en-US  en-US  en-US  en-US rning en-US  en-US re en-US as en-US  en-US ion en-US  en-US  en-US 5.3  en-US  en-US  en-US es en-US  en-US en-US  en-US lif en-US n en-US en-US  en-US eren-US a en-US  en-US en-US  en-US l en-US w en-US  en-US ing en-US 7    1 8 8  en-US  en-US he en-US en-US  en-US lly en-US en-US  en-US ge en-US en-US  en-US  en-US y en-US en-US  en-US 6 en-US  en-US en-US  en-US in en-US  en-US  en-US y en-US ien-US  en-US en-US v en-US ed en-US  en-US e en-US  en-US  en-US rning en-US  en-US  en-US  en-US nen-US re en-US 9 en-US  en-US in en-US 0 en-US y en-US in en-US rt en-US e en-US  en-US recen-US  en-US en-US how en-US en-US een-US e en-US en-US who en-US  en-US en-US e en-US en-US who en-US  en-US A en-US d en-US  en-US in en-US 1 en-US  en-US  en-US  en-US  en-US 2 en-US en-US 4 en-US  en-US  en-US 5 en-US  en-US  en-US  en-US 6 en-US en-US 8 en-US  en-US 9 en-US en-US 1 en-US  en-US s en-US re en-US 2 en-US en-US 4 en-US  en-US en-US r en-US ocieen-US e en-US s en-US  en-US 5    2 0 6  en-US  


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access   17     en-US 2 en-US  en-US oT en-US en-US v en-US l en-US l en-US ions en-US  en-US 7 en-US  en-US s en-US  en-US 8 en-US  en-US 9 en-US  en-US 0 en-US en-US 3 en-US  en-US en-US ices en-US  en-US 4 en-US  en-US len-US ove en-US  en-US 5 en-US en-US 8 en-US  en-US e en-US en-US ourcing 1 en-US  2 en-US  en-US hoc en-US en-US rp en-US es en-US 6    2 1 9  en-US en-US s en-US  en-US 0 en-US s en-US 1 en-US  en-US  en-US 2 en-US  en-US cience en-US 3 en-US  en-US cy en-US 4 en-US  en-US  en-US t en-US  en-US l en-US 4 en-US  en-US 5 en-US  en-US 6 en-US  en-US  en-US nning 8 en-US  en-US  en-US hone en-US  en-US ches en-US  en-US  en-US t en-US en-US en-US g en-US  en-US la en-US ion en-US he en-US er en-US rt en-US  en-US  en-US  en-US n en-US gyroen-US  en-US  en-US nd en-US  en-US a en-US xen-US or en-US  en-US conen-US t en-US en-US heir en-US  en-US  en-US ion en-US  en-US enen-US  en-US ices en-US  en-US ion en-US 6 en-US  en-US in en-US 7    2 2 8  en-US  en-US  en-US hen-US o en-US e en-US s en-US  en-US en-US ies en-US Very en-US  en-US ion en-US  en-US on en-US he en-US  en-US here en-US r en-US t en-US en-US  en-US ap en-US d en-US ed en-US ion en-US  en-US le en-US  en-US nd en-US  en-US  en-US s en-US rt en-US c en-US  en-US f en-US len-US ove en-US en-US d en-US  en-US  en-US s en-US rt en-US c en-US m en-US en-US en-US nen-US  en-US  en-US 6  en-US C en-US O en-US LUSION en-US  en-US y en-US  en-US en-US  en-US s en-US en-US ig en-US  en-US nen-US nd en-US  en-US en-US  en-US een-US nen-US  en-US ellig en-US  en-US  en-US en-US  en-US  en-US o en-US  en-US en-US l en-US hen-US  en-US s en-US  en-US  en-US  en-US ien-US f en-US  en-US ed en-US ed en-US r en-US  en-US s en-US o en-US  en-US ic en-US en-US ed en-US en-US  en-US d en-US  en-US  en-US re en-US  en-US o en-US ed en-US  en-US en-US ed en-US ch en-US  en-US  en-US A en-US ENT en-US  en-US  en-US he en-US er en-US  en-US  en-US he en-US ing en-US iz en-US  en-US y en-US h en-US  en-US  en-US ing en-US  en-US  en-US  en-US a en-US  en-US  en-US R en-US ES en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US vey en-US en-US s en-US  en-US 7 en-US en-US  en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US  en-US 437 en-US en-US  en-US  en-US  en-US  en-US 3 en-US  en-US  en-US TU en-US en-US TU en-US en-US  en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US nc en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US  en-US en-US  en-US  en-US the en-US  en-US en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US en-US EE en-US E en-US ess en-US  en-US 5 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US i en-US en-US n en-US  en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US  en-US en-US  en-US ew en-US i   Ava i l a bl e   en-US o en-US 3 en-US en-US the en-US en-US r en-US en-US of en-US en-US the en-US en-US net en-US en-US of en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US  en-US vey en-US en-US s en-US  en-US 7 en-US en-US  en-US  en-US  en-US  en-US 10 en-US  en-US  en-US  en-US en-US  en-US  en-US a en-US vey en-US en-US  en-US  en-US 3 en-US en-US  en-US  en-US  en-US 11 en-US  en-US  en-US Al en-US en-US  en-US  en-US en-US  en-US  en-US en-US  


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access  18      en-US S en-US v en-US  en-US  en-US  en-US 12 en-US  en-US  en-US  en-US en-US  en-US ti en-US en-US  en-US en-US sors en-US  en-US 1 en-US en-US  en-US  en-US  en-US 13 en-US  en-US  en-US  en-US en-US  en-US en-US  en-US  en-US l en-US  en-US  en-US  en-US 14 en-US  en-US  en-US  en-US en-US n te en-US en-US of en-US en-US the en-US en-US  en-US en-US n en-US  en-US 28 en-US en-US  en-US  en-US  en-US 15 en-US  en-US  en-US N en-US en-US  en-US en-US  en-US en en-US  en-US en-US  en-US vey en-US en-US  en-US  en-US 4 en-US en-US  en-US  en-US  en-US 16 en-US  en-US  en-US ez en-US en-US  en-US en-US  en-US e en-US  en-US  en-US en-US sors en-US  en-US  en-US  en-US  en-US 17 en-US  en-US  en-US  en-US en-US  en-US ti en-US en-US  en-US en-US sors en-US n en-US  en-US 1 en-US en-US  en-US  en-US  en-US 18 en-US  en-US  en-US  en-US en-US  en-US en-US  en-US  en-US 3 en-US en-US  en-US  en-US  en-US 19 en-US  en-US  en-US  en-US en-US  en-US T en-US en-US  en-US  en-US  en-US 73 en-US en-US  en-US  en-US  en-US 20 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 15 en-US en-US  en-US  en-US  en-US 21 en-US  en-US  en-US  en-US en-US  en-US n en-US en-US i en-US  en-US  en-US  en-US  en-US 22 en-US  en-US  en-US  en-US i en-US en-US n en-US  en-US  en-US  en-US  en-US 23 en-US  en-US  en-US  en-US en-US  en-US n en-US en-US  en-US  en-US  en-US  en-US 24 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 540 en-US en-US  en-US  en-US  en-US 25 en-US  en-US  en-US  en-US en-US  en-US t en-US  en-US  en-US en-US l en-US  en-US 8 en-US en-US  en-US  en-US  en-US 26 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US n en-US  en-US 7 en-US en-US  en-US  en-US  en-US 27 en-US  en-US  en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US i en-US en-US B\256 en-US  en-US  en-US  en-US  en-US 29 en-US  en-US  en-US  en-US  en-US  en-US  en-US ew en-US en-US  en-US  en-US  en-US  en-US  en-US 30 en-US  en-US  en-US r en-US en-US e en-US en-US ti en-US en-US  en-US n en-US en-US  en-US ess en-US  en-US 807 en-US en-US  en-US  en-US  en-US 31 en-US  en-US  en-US  en-US  en-US  en-US en-US ted en-US en-US 2015 en-US s en-US  en-US 7 en-US en-US  en-US  en-US  en-US 32 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US no en-US  en-US 1 en-US en-US  en-US  en-US  en-US 33 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 34 en-US  en-US  en-US  en-US en-US  en-US l en-US  en-US en-US  en-US 190 en-US en-US  en-US  en-US  en-US  en-US 35 en-US  en-US  en-US  en-US en-US nt en-US en-US  en-US  en-US  en-US en-US  en-US en-US  en-US IC en-US  en-US  en-US  en-US  en-US  en-US  en-US 36 en-US  en-US  en-US  en-US en-US  en-US en-US  en-US  en-US  en-US en-US  en-US  en-US 7 en-US en-US  en-US  en-US  en-US 37 en-US  en-US  en-US  en-US en-US s en-US  en-US  en-US en-US EE en-US en-US  en-US  en-US 7 en-US en-US  en-US  en-US  en-US 38 en-US  en-US  en-US  en-US en-US er en-US en-US  en-US en-US  en-US eo en-US  en-US  en-US 6 en-US en-US  en-US  en-US  en-US 39 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US EE en-US en-US  en-US  en-US 7 en-US en-US  en-US  en-US  en-US 40 en-US  en-US  en-US  en-US en-US ti en-US en-US t en-US  en-US n en-US en-US P en-US  en-US 1 en-US en-US  en-US  en-US  en-US 41 en-US  en-US  en-US  en-US en-US ti en-US en-US e en-US en-US  en-US  en-US en-US rol en-US  en-US 7 en-US en-US  en-US  en-US  en-US 42 en-US  en-US  en-US L en-US en-US  en-US en-US t en-US  en-US  en-US en-US 0 en-US  en-US  en-US  en-US 43 en-US  en-US  en-US  en-US n en-US  en-US ey en-US en-US  en-US  en-US  en-US 44 en-US  en-US  en-US L en-US en-US  en-US en-US  en-US ti en-US en-US n en-US en-US  en-US  en-US  en-US  en-US 0 en-US en-US  en-US  en-US  en-US 45 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US n en-US en-US s en-US  en-US 6 en-US en-US  en-US  en-US  en-US  en-US 46 en-US  en-US  en-US  en-US en-US  en-US ng en-US en-US  en-US en-US  en-US ess en-US 2 en-US  en-US 2 en-US en-US  en-US  en-US  en-US 47 en-US  en-US  en-US en en-US  en-US i en-US en-US n en-US  en-US  en-US  en-US  en-US 48 en-US  en-US  en-US  en-US en-US  en-US ver en-US e en-US en-US  en-US  en-US en-US rks en-US  en-US  en-US  en-US 49 en-US  en-US  en-US  en-US en-US  en-US e en-US en-US  en-US  en-US en-US rks en-US  en-US  en-US  en-US 50 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 3 en-US en-US  en-US  en-US  en-US 51 en-US  en-US  en-US  en-US en-US r en-US n en-US en-US  en-US 7 en-US en-US  


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access   19     en-US  en-US  en-US  en-US 52 en-US  en-US  en-US  en-US en-US e en-US ti en-US en-US  en-US  en-US en-US  en-US 4 en-US en-US  en-US  en-US  en-US 53 en-US  en-US  en-US  en-US en-US  en-US  en-US DA en-US en-US  en-US  en-US  en-US  en-US 54 en-US  en-US  en-US  en-US en-US e en-US n en-US  en-US en-US  en-US v en-US en-US  en-US  en-US  en-US 55 en-US  en-US  en-US  en-US en-US k en-US en-US thm en-US en-US  en-US ron en-US  en-US 0 en-US en-US  en-US  en-US 5 en-US 6 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US n en-US  en-US  en-US  en-US 57 en-US  en-US  en-US  en-US en-US ti en-US en-US T en-US  en-US n en-US en-US  en-US  en-US  en-US  en-US  en-US 58 en-US  en-US  en-US  en-US en-US  en-US Pre en-US en-US  en-US t en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 3 en-US en-US 1 en-US  en-US  en-US  en-US 59 en-US  en-US  en-US t en-US  en-US  en-US n en-US en-US  en-US  en-US en-US  en-US OS en-US  en-US 2 en-US en-US  en-US  en-US  en-US 60 en-US  en-US  en-US  en-US en-US  en-US ti en-US  en-US t en-US en-US  en-US 4 en-US en-US  en-US  en-US  en-US 61 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US ess en-US  en-US  en-US 0 en-US en-US  en-US  en-US  en-US 62 en-US  en-US  en-US  en-US en-US X en-US en-US ng en-US en-US s en-US  en-US en-US  en-US  en-US i    en-US x en-US en-US e en-US en-US i en-US en-US r en-US en-US is en-US en-US 2 en-US en-US Dec en-US en-US 6  en-US  en-US  en-US 63 en-US  en-US  en-US i en-US  en-US en-US  en-US  en-US en-US h en-US i   Av a i l a bl e   en-US e en-US en-US is en-US en-US ng en-US en-US a en-US en-US nd en-US en-US b en-US en-US r en-US en-US the en-US en-US net en-US en-US of en-US en-US 0 en-US en-US Dec en-US en-US 6  en-US  en-US  en-US 64 en-US  en-US  en-US BI en-US en-US  en-US en-US e en-US  en-US  en-US en-US er en-US  en-US i    en-US du en-US en-US es en-US en-US new en-US en-US ai en-US en-US nd en-US en-US net en-US en-US of en-US en-US s en-US en-US ves en-US en-US 6 en-US en-US 0 en-US en-US Dec en-US en-US 6  en-US  en-US  en-US 65 en-US  en-US  en-US  en-US en-US Su en-US  en-US  en-US en-US a en-US  en-US 9 en-US en-US  en-US  en-US  en-US  en-US 66 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US en-US s en-US  en-US  en-US  en-US  en-US 67 en-US  en-US  en-US  en-US en-US  en-US a en-US en-US ve en-US en-US a en-US  en-US 383 en-US en-US  en-US  en-US  en-US 68 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US ess en-US  en-US 7 en-US en-US  en-US  en-US  en-US 69 en-US  en-US  en-US  en-US en-US  en-US vey en-US en-US  en-US ri en-US s en-US  en-US 77 en-US en-US  en-US  en-US  en-US 70 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 71 en-US  en-US  en-US o en-US  en-US  en-US ne en-US en-US  en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US 72 en-US  en-US  en-US  en-US en-US  en-US n en-US e en-US  en-US en-US  en-US n en-US  en-US 6 en-US en-US  en-US  en-US  en-US 73 en-US  en-US  en-US  en-US en-US  en-US  en-US hy en-US  en-US en-US  en-US n en-US  en-US 6 en-US en-US  en-US  en-US  en-US 74 en-US  en-US  en-US  en-US en-US  en-US t en-US en-US I en-US  en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US 75 en-US  en-US  en-US  en-US en-US  en-US ty en-US en-US  en-US en-US  en-US f en-US en-US  en-US  en-US  en-US  en-US 76 en-US  en-US  en-US a en-US  en-US en-US ti en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 96 en-US en-US  en-US  en-US  en-US 77 en-US  en-US  en-US  en-US en-US  en-US n en-US en-US 0 en-US 8 en-US en-US  en-US  en-US ess en-US  en-US 85 en-US en-US  en-US  en-US  en-US 78 en-US  en-US  en-US  en-US en-US  en-US n en-US en-US  en-US  en-US  en-US  en-US  en-US 79 en-US  en-US  en-US hen en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US 4 en-US en-US  en-US  en-US  en-US 80 en-US  en-US  en-US  en-US en-US ng en-US en-US  en-US  en-US  en-US en-US s en-US  en-US  en-US 6 en-US en-US  en-US  en-US  en-US 81 en-US  en-US  en-US N en-US  en-US  en-US en-US  en-US  en-US en-US 2011 en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 82 en-US  en-US  en-US  en-US  en-US en-US o en-US  en-US  en-US en-US 2011 en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 83 en-US  en-US  en-US  en-US en-US  en-US s en-US  en-US  en-US  en-US  en-US 9 en-US en-US  en-US  en-US  en-US 84 en-US  en-US  en-US  en-US en-US  en-US en-US  


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access  20      en-US  en-US en-US  en-US  en-US  en-US 4 en-US en-US  en-US  en-US  en-US 85 en-US  en-US  en-US F en-US  en-US en-US  en-US  en-US en-US  en-US 0 en-US  en-US 5 en-US en-US  en-US  en-US  en-US 86 en-US  en-US  en-US h en-US  en-US  en-US l en-US en-US  en-US en-US  en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 87 en-US  en-US  en-US  en-US en-US ti en-US en-US  en-US  en-US d en-US en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 88 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US 6 en-US  en-US 263 en-US en-US  en-US  en-US  en-US 89 en-US  en-US  en-US  en-US en-US tem en-US en-US OTA en-US  en-US 6 en-US  en-US 6 en-US en-US  en-US  en-US  en-US  en-US 90 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US m en-US  en-US 87 en-US en-US  en-US  en-US  en-US  en-US 91 en-US  en-US  en-US e en-US  en-US  en-US en-US  en-US  en-US en-US  en-US m en-US  en-US 87 en-US en-US  en-US  en-US  en-US  en-US 92 en-US  en-US  en-US  en-US en-US  en-US a en-US  en-US  en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 93 en-US  en-US  en-US  en-US en-US  en-US r en-US en-US  en-US  en-US en-US  en-US 195 en-US en-US  en-US  en-US  en-US 94 en-US  en-US  en-US Wei en-US en-US xi en-US ng en-US en-US eng en-US en-US  en-US en-US  en-US k en-US en-US to en-US en-US  en-US  en-US en-US ess en-US  en-US 1 en-US en-US  en-US  en-US  en-US 95 en-US  en-US  en-US  en-US en-US  en-US c en-US r en-US en-US  en-US  en-US en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 96 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US r en-US  en-US  en-US 9 en-US en-US  en-US  en-US  en-US 97 en-US  en-US  en-US  en-US  en-US  en-US en-US e en-US en-US  en-US r en-US en-US  en-US  en-US en-US  en-US en-US  en-US  en-US  en-US  en-US m en-US en-US s en-US en-US 3 en-US  en-US 3 en-US en-US  en-US  en-US  en-US 98 en-US  en-US  en-US a en-US en-US  en-US en-US  en-US ter en-US en-US  en-US  en-US en-US l en-US  en-US  en-US  en-US  en-US 99 en-US  en-US  en-US a en-US  en-US en-US  en-US  en-US  en-US en-US  en-US BE en-US  en-US 1 en-US en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US e en-US  en-US  en-US  en-US  en-US 1 en-US  en-US  en-US e en-US en-US  en-US ter en-US en-US  en-US  en-US  en-US  en-US 2 en-US  en-US  en-US e en-US en-US  en-US ter en-US en-US the en-US  en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US  en-US ter en-US en-US  en-US ti en-US en-US n en-US en-US  en-US n en-US  en-US 3 en-US en-US  en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US  en-US th en-US  en-US  en-US ter en-US en-US  en-US  en-US en-US s en-US  en-US 7 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US en en-US en-US  en-US n en-US  en-US  en-US en-US 4 en-US  en-US 4 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US e en-US en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US 4 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US  en-US ter en-US en-US  en-US tem en-US en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US  en-US en-US  en-US a en-US en-US  en-US en-US  en-US S en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US  en-US  en-US en-US ter en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 72 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US en-US  en-US n en-US en-US a en-US  en-US 5 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US W en-US  en-US en-US  en-US  en-US nty en-US en-US  en-US ON en-US  en-US 1 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US  en-US Se en-US  en-US en-US  en-US l en-US  en-US 334 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US Int en-US en-US y en-US  en-US  en-US  en-US 249 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US en-US  en-US d en-US en-US  en-US 1 en-US 1 en-US 52 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US  en-US te en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US en-US r en-US  en-US 34 en-US en-US  en-US  


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access   21     en-US  en-US 8 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US i en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US s en-US  en-US  en-US  en-US 2 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US 1 en-US 77 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US en-US d en-US en-US a en-US en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US 2 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US o en-US  en-US en-US  en-US  en-US en-US s en-US  en-US 0 en-US en-US  en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US s en-US  en-US  en-US 0 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 3 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US en-US a en-US  en-US 321 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US K en-US en-US t en-US en-US  en-US en-US  en-US  en-US s en-US en-US dy en-US en-US matics en-US  en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US  en-US res en-US  en-US  en-US  en-US 9 en-US  en-US  en-US R en-US n en-US en-US  en-US en-US  en-US  en-US en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US n en-US en-US  en-US en-US n en-US  en-US  en-US  en-US en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US en-US  en-US  en-US  en-US man en-US en-US  en-US  en-US 3 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US en-US  en-US en-US  en-US d en-US en-US s en-US 2 en-US  en-US 46 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US ti en-US en-US ti en-US en-US  en-US n en-US en-US n en-US  en-US 1 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US  en-US n en-US en-US n en-US  en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US  en-US en-US 2 en-US  en-US ron en-US  en-US 351 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US Semi en-US en-US  en-US  en-US en-US  en-US S en-US s en-US en-US  en-US  en-US  en-US 2 en-US en-US  en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US ex en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 9 en-US  en-US  en-US nt en-US en-US  en-US en-US  en-US  en-US a en-US  en-US en-US  en-US r en-US  en-US 7 en-US en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US en-US  en-US n en-US en-US EEE en-US  en-US 6 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US s en-US  en-US 7 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US en-US ti en-US en-US  en-US n en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US ene en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 7 en-US en-US  en-US  en-US 4 en-US  en-US  en-US en-US  en-US ti en-US en-US  en-US en-US  en-US  en-US 8 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US i en-US  en-US  en-US en-US  en-US dy en-US en-US  en-US n en-US y en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US k en-US en-US to en-US en-US  en-US  en-US en-US IE en-US  en-US  en-US 0 en-US en-US  en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US ter en-US en-US n en-US en-US  en-US l en-US en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 9 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US  en-US  en-US n en-US en-US  en-US en-US  en-US  en-US  en-US 7 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US k en-US en-US to en-US en-US  en-US the en-US  en-US en-US  en-US  en-US 4 en-US en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US en-US  en-US n en-US en-US  


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access  22      en-US Inte en-US  en-US  en-US 132 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US k en-US en-US to en-US en-US n en-US en-US  en-US I en-US en-US  en-US  en-US  en-US 69 en-US en-US 8 en-US  en-US  en-US  en-US 2 en-US  en-US  en-US r en-US en-US  en-US en-US k en-US en-US to en-US en-US  en-US  en-US en-US r en-US  en-US 1 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US t en-US en-US n en-US en-US n en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US  en-US i   en-US m en-US  en-US  en-US 5 en-US  en-US  en-US  en-US  en-US en-US k en-US en-US to en-US en-US  en-US tr en-US  en-US n en-US en-US  en-US  en-US 7 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US en-US a en-US i   en-US d en-US en-US a en-US en-US c en-US en-US es en-US 2 en-US en-US n en-US en-US 7  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US dy en-US en-US  en-US n en-US y en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US  en-US en-US  en-US vey en-US en-US  en-US  en-US 13 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US n en-US en-US  en-US n en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US 2010 en-US  en-US  en-US 7 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US en-US  en-US o en-US n en-US en-US  en-US  en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 4 en-US  en-US  en-US B en-US  en-US en-US A en-US ti en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 6 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US e en-US en-US s en-US  en-US  en-US  en-US en-US  en-US n en-US  en-US 1 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US f en-US en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US  en-US e en-US en-US  en-US  en-US  en-US 1 en-US 68 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US f en-US en-US  en-US ew en-US en-US e en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US en-US i en-US  en-US  en-US en-US  en-US n en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US h en-US en-US er en-US i   Av a i l a bl e   en-US p en-US en-US ten en-US en-US hn en-US y en-US en-US ends en-US en-US l en-US en-US the en-US en-US l en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US en-US  en-US en-US re en-US  en-US 6 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US y en-US en-US  en-US e en-US en-US  en-US  en-US  en-US en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US en-US re en-US  en-US 6 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US en-US t en-US  en-US earn en-US  en-US 9 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 34 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US Lef en-US  en-US en-US  en-US  en-US ti en-US en-US  en-US en-US V en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US Pro en-US  en-US  en-US  en-US 34 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US ng en-US en-US ti en-US en-US  en-US  en-US  en-US en-US  en-US 1 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US O en-US en-US M en-US  en-US en-US l en-US en-US  en-US  en-US en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US N en-US s en-US  en-US en-US  en-US  en-US 9 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US  en-US to en-US en-US  en-US en-US n en-US  en-US  en-US  en-US 5 en-US  en-US  en-US D en-US  en-US  en-US en-US nt en-US en-US  en-US en-US  en-US en-US  en-US  en-US  en-US  en-US 6 en-US en-US  en-US 2 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US edes en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US t en-US  en-US en-US  en-US  


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access   23     en-US  en-US 22 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US t en-US en-US  en-US vey en-US en-US  en-US  en-US ne en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US  en-US  en-US tbed en-US en-US  en-US en-US  en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US G en-US  en-US en-US o en-US en-US  en-US en-US  en-US  en-US  en-US 0 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US e en-US en-US ent en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US a en-US en-US z en-US en-US  en-US dez en-US en-US z en-US en-US  en-US en-US  en-US a en-US  en-US  en-US en-US  en-US 26 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US a en-US en-US a en-US en-US o en-US en-US  en-US a en-US en-US  en-US en-US o en-US en-US F en-US en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 8 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US ti en-US en-US  en-US  en-US  en-US en-US  en-US en-US  en-US 2015 en-US b en-US 5 en-US  en-US 9 en-US en-US  en-US  en-US  en-US  en-US 5 en-US  en-US  en-US o en-US en-US  en-US en-US  en-US  en-US en-US n en-US  en-US  en-US 45 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US k en-US  en-US  en-US en-US  en-US  en-US  en-US 0 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US r en-US  en-US 86 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US  en-US y en-US en-US by en-US en-US  en-US en-US  en-US s en-US  en-US 1 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US  en-US dez en-US en-US  en-US en-US n en-US en-US  en-US en-US  en-US n en-US y en-US en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US en-US  en-US tem en-US en-US  en-US ti en-US en-US ve en-US en-US  en-US  en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US ty en-US en-US  en-US en-US  en-US  en-US  en-US 5 en-US  en-US 2 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US en en-US en-US  en-US  en-US  en-US en-US ess en-US  en-US 831 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US  en-US en-US ti en-US en-US  en-US ne en-US en-US  en-US e en-US en-US  en-US  en-US 26 en-US en-US  en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US en-US  en-US ve en-US  en-US  en-US en-US  en-US rk en-US  en-US 7 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US  en-US en-US Sy en-US  en-US  en-US en-US  en-US  en-US  en-US es en-US  en-US 1 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US  en-US  en-US r en-US rks en-US  en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US n en-US  en-US  en-US 9 en-US 0 en-US  en-US 411 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US  en-US tems en-US en-US  en-US  en-US  en-US  en-US n en-US  en-US en-US  en-US v en-US es en-US  en-US  en-US 3 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US en en-US en-US  en-US  en-US en-US  en-US en-US  en-US  en-US  en-US 0 en-US  en-US  en-US m en-US en-US  en-US  en-US th en-US en-US  en-US en-US  en-US  en-US  en-US 7 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 4 en-US en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US en-US ng en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US  en-US 4 en-US  en-US  en-US  en-US  en-US rk en-US  en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 8 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US  en-US en-US  en-US t en-US dy en-US en-US n en-US s en-US  en-US  en-US 9 en-US en-US  en-US  en-US  en-US 8 en-US  en-US  en-US  en-US en-US  


2169-3536 \(c http://www.ieee.org/publications_standards/publications/rights/index.htm\l for more information This article has been accepted for publication in a future issue of this\ journal, but has not been fully edited. Content may change prior to fin\al publication. Citation information: DOI 10.1109/ACCESS.2017.2697839, IEEE Access  24      en-US  en-US i en-US  en-US en-US  en-US  en-US 9 en-US  en-US 2 en-US en-US  en-US  en-US  en-US 9 en-US  en-US  en-US  en-US en-US  en-US n en-US en-US n en-US  en-US t en-US 7 en-US  en-US  en-US 5 en-US en-US  en-US  en-US  en-US 0 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US n en-US en-US E en-US ess en-US  en-US 858 en-US en-US  en-US  en-US  en-US 1 en-US  en-US  en-US  en-US en-US  en-US  en-US en-US ess en-US  en-US 1 en-US en-US  en-US  en-US  en-US  en-US 2 en-US  en-US  en-US  en-US  en-US en-US  en-US  en-US  en-US en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 3 en-US  en-US  en-US  en-US  en-US  en-US o en-US en-US  en-US en-US in en-US  en-US  en-US  en-US l en-US en-US  en-US g en-US  en-US 0 en-US en-US  en-US  en-US  en-US 4 en-US  en-US  en-US Ben en-US  en-US  en-US en-US  en-US  en-US en-US  en-US  en-US 7 en-US en-US  en-US  en-US  en-US 5 en-US  en-US  en-US  en-US en-US f en-US  en-US  en-US dy en-US en-US  en-US 11 en-US en-US  en-US  en-US  en-US 6 en-US  en-US  en-US  en-US  en-US en-US T en-US en-US  en-US o en-US  en-US en-US  en-US  en-US 1 en-US en-US  en-US  en-US  en-US 7 en-US  en-US  en-US z en-US en-US  en-US en-US ti en-US en-US  en-US  en-US  en-US en-US sort en-US  en-US  en-US 5 en-US  en-US  en-US  en-US 8 en-US  en-US  en-US z en-US en-US  en-US en-US  en-US f en-US en-US the en-US en-US  en-US  en-US en-US  en-US  en-US  en-US  en-US 121 en-US en-US  en-US  en-US  


