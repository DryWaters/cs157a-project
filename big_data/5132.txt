Abstract Extract, transform, and load \(ETL\ is a very common and important technology for building data warehouse includes business intelligence. When people issue a very complex SQL query to acquit data from a transaction system into a data warehouse, it involves many procedures including table-joining, sort, and aggregation. Such procedures require significant retrieving step and huge data 
Ping Yang Zaiying Liu, Jun Ni School of Information Science and Technology Shanghai Sanda University, Shanghai, China, 201209 brightyang@126.com fall_water007@sina.com jun-ni@uiowa.edu 
Performance Tuning in Distributed Processing of ETL  
 
transferring from tables. The intensive querying very often causes performance issues to be concerned. Moreover, it commonly generates negative impacts on data instance resources. How to improve the performance for ETL becomes critical and challenging. This paper presents a parallel processing solution that splitting big and complex SQL query into small pieces in distributed computing manor. The proposed method aims at reducing cost of computation, while ensuring data integrity among joined tables. The innovative idea can be verified through selected test-beds of performance tuning Keywords- ETL; Data extraction; Capture data changes 
load; Performance tuning 
I I NTRODUCTION In processing of extract, transform, and load \(ETL there exist three database functions that are integrated together to operate data transfer from one table to another to form data marts or warehouses. It also functions to convert databases from one format to another. Therefore ETL is a very common technology utilized in business intelligence There is an example of unexpected ETL performance during an implementation of business analysis. The transactional data are extracted, transformed, and loaded from SIEBEL CRM system to a business intelligence data 
warehouse. As ETL 
ProC for extraction Syncsort for capture data changes \(CDC\ and Informatica for loading delta changes \(LDC\ are all functioning well in the first phase. However after the data volume rapidly increased in the system, the performance of complex extraction became worse When sophisticated queries are executed and the system retrieved huge data on the system takes about 90% of total ETL computing time. How to deal with such performance problem become critical to the business data extraction, transactions and distribution in real time. In the following section, we would like to 
s components 
demonstrate how to analyze such problem II P ROBLEM A NALYSIS First, we make sure that all data are read from the Siebel CRM system using ProC. The data in previous and current steps are compared to check whether there exists any data change. Such process can be accomplished by using Syncsort. Then captured data changes are loaded into the data warehouse. Extensive analysis can be conducted to determine bottlenecks occurring in the whole process  The major issues can be addressed as follows All transforms/aggregations can be executed in complex extract queries. For example, a most complex 
query can sometimes consists of more than 10 scalar sub queries and complex aggregations, loops, or sorts  If using the SIEBEL-CRM software system, the performance may lack of tuning. As all know, most of setups in a transactional system is OLTP-dedicated, which only returns first row. It is uncontrollable to all the return rows \(which are DW characteristics\. In addition, the system also suffers negative performance due to the influence cause by huge complex extraction of retrieving data Rapidly-developed commerce markets and business generate huge amounts of data volume significantly The 
corresponding queries become complex and not linearly scalable. The more volume increase, more surfer we may have. When dealing with big data, the intensive computing becomes necessary. Such computation causes low performance of operation using existing software. The performance issues are getting to be more concerned and complained by end-users, especially for Windows-based users on their daily PCs Not like a simple query on a huge amount of data resource, which can be accomplished within a tolerated time scale a complex query on a big data can significantly reduce computing performance Fig. 1 shows the previous solution chart in which 
CM refers to a complex query involved both base table M and referred tables A1, A2, A3, B1, B2, etc. d\(CM\ stands for the change \(or delta\ between the current and previous CM. Everyday only delta records can executed on target table, while remaining the current CM on source table. M is the base table of source. It has the same primary key as in the target table. M*, A*, and B* represent the expected columns from M, A, and B in a target table T, respectively The d\(M d\(A and d\(B*\ are the delta data 
2013 Seventh International Conference on Internet Computing for Engineering and Science 978-0-7695-5118-0/13 $26.00 © 2013 IEEE DOI 10.1109/ICICSE.2013.24 85 


Previous Solution 
SM quer y A query B query 
CM Current data Informatica Data Warehouse Target table Previous data Delta data 
3 4 4 4 
1 2 C query 
M A1 A3 A2 B1 B2 M A B M A B M A B 
Pro C 
Figure 2. The implementation steps 
Target table Implement d\(M Update d\(A Update d\(B Pro C 
Syncsort CDC Process 
Select M.rowid  select aggregation From A1,A2,A3 Where A.rowid=M.rowid  select aggregation From B1,B2 Where b.rowid=M.rowid  select aggregation From C Where C.rowid=M.rowid  From M Where  Select M.rowid A.aggregation_Values B.aggregation_Values C.aggregation_vaules From M select aggregation From A1,A2,A3 group A.rowid A  select aggregation From B1,B2 GroupbyB.rowid select aggregation From C Group C.rowid C Where M.rowid=A.rowid And M.rowid=B.rowid And M.rowid=C.rowid Select M.rowid From M  select aggregation From B1,B2 groupbyB.rowid select aggregation From A1,A2,A3 group A.rowid A select aggregation From C GroupbyB.rowid Target Table 
2 2 2 
d\(M d\(A d\(B 
M A B Execute sql on siebel DB to extract data to flat file Utilize Syncsort to Get delta data\(insert update delete by compare previous extracted file and current extract file Proposed Solution Figure 3. The proposed solution chart 
corresponding to M*, A*, and B They are obtained after the CDC previous and current data steps in daily process One flexible solution is to reduce the complexity of extract queries, ensure the data integrity, and reduce the any influences on performance on source instance Figure 1. The previous solution chart 
Where 
Siebel CRM DB Informatica Data Warehouse Siebel CRM DB Syncsort CDC Process 
s 
d CM  
Utilize Syncsort to calculate delta data current versus previous extract file Execute sql on siebel DB to extract data to flat file Result of complex query 
M A1 A3 A2 B1 B2 Previous M Current M Previous A Current A Previous B Current B R esult o f simpler Subset of refer Subset of refer Delta d\(M Delta d\(A Delta d\(B 
CM query CM query SM query B query A query 
86 


III S OLUTION How to reduce the complexity of extract query, while still ensuring data integrity is an important issue. We first analyze what the kind of data are expected to be delivered from source in SIEBEL CRM system to data warehouse during a regular daily process. In such case, we have d\(T\=d\(CM Current CM\(M*,A*,B*\ CDC Previous CM\(M*,A*,B*\. T is the target table, which has the current result from a complex query CM. It includes expected columns of M*, A*, and B*. The d\(T\ is identical to d\(CM\. The d\(T\ is executed to obtain the target table T to keep the current updated When consider ROWID as the primary key in both M and T, the d\(T\ includes the delta ROWIDS calculated in d\(CM In other words, the d\(T\ includes those ROWIDS which have been changed in the base table M or those ROWIDS possibly to be changed in M due to the influences of scalar sub queries A and B. Therefore, the d\(T\ could be split into small pieces based on ROWID, i.e  d\(T\ =d\(SM\{ Current SM CDC Previous SM       M*[impacted by d\(A   M*[impacted by d\(B    SM is the result based on a simple query. It only fetches records from base table M \(or the base table with simple joins The d\(SM\ stands for the delta data of the current SM and previous SM. It includes those that the ROWID are changed The M*\(d\(A*\\,M*\(d\(B*\includes those ROWIDS from base table M*. Whether or not being changed, they are always influenced by the delta data d\(A*\, d\(B The d\(SM\ includes those changed columns from base table M. After d\(SM\ is executed the target table T has the same ROWIDS as M has Let T have the keys A* and B referenced to M*. It is possible to be ROWID or other reference key. Then those delta data come from d\(A The d\(B*\an be calculated from T. It turns out that the calculation of d\(A*\ and d\(B*\ are on the source side. The T\(d\(A*\and T\(d\(B can be update by EQUAL-JOIN on Data Warehouse side. Fig. 2 shows the execution steps Step 1: Restructure a complex query. Put in-line scalar sub queries down to select the location to store using OUT-JOIN Step 2: Split the complex query into main query \(with base table\ and referred queries Step 3: After separating EXTRACT/CDC/LOAD jobs for preparation of distributed main and referred queries load the delta d\(SM\ into target table. The d\(M*\ must be executed firstly to ensure data integrity Step 4: After the d\(M*\ is executed, load delta M*\(d\(A*\\, M*\(d\(B  and M*\(d\(C Fig. 3 shows the proposed solution flow chart. A is a scalar sub query with tables A1, A2 and A3 which are joined each other. B is a scalar sub query with jointed tables B1 and B2 TABLE1 S HOWS THE DIFFERENT COST oading jobs can be 
s s l 
24570 14750 24570 14750 3710 2100 44328 44328 XX X X 
Sum\(SM, A, B,C CM Sum d\(M d\(A  d\(B d\(C d\(CM Sum d\(M d A d\(B  d\(C d\(CM 
Process Extract CDC Load Solution Rows p rocessed C onsistent gets 194842 355942343 XX X X P hysical reads Elapsed time 0:03:36 3:17:06 0:00:17 00:00:07 0:00:17 00:00:09 After a complex query being spitted into some simpler and smaller ones, the previous complex FULL-FULL tables OUT-JOIN can be converted to several separated simpler queries on source side. In this way, the querying can be executed in parallel. The OUT-JOIN \(full M out-joins to full referred queries\ becomes EQUAL-JOIN target table T equal-joins delta data from referred several queries\. The complexity can be balanced on two sides while the computational performance can be reduced significantly IV TEST BED OF P ROPOSED SOLUTION After implement the proposed solution in ETL three processes of extract, CDC, and loading have been changed Taking the most complex query as our test, we split a complex query CM into a simple query SM and several referred sub queries A, B, and C. Compare the results of computational cost using previous and proposed solutions we find the differences which are listed in Table1 The total elapsed time for extraction is decreased from about 3 hours to 3 minutes using proposed method Moreover, the distributed queries can be executed in parallel. Though multiple CDC 
87 


present, the total time elapsed is not significantly large compared with of single original CDC and loading jobs  V C ONCLUSION Many methods have been tried on the complex extract query \(like adding hints\ in ETL process with varying instance parameters in data transaction. The cost performance is a concern  In this work, we propose and experiment a parallel method in which a complex query is spited into several simple and small ones for execution in parallel. The results of increasing performance are very promising. The total cost can distributed and decreased, with less influences on source database In future, we would like to consider reduce such cost through reference sub queries if queries are very complex and cannot hosted on source instance The relationship between main query and reference sub queries with data integrity is the most important issue to be considered and modeled, if we split complex queries A CKNOWLEDGMENT This work is sponsored by Shanghai Education Commission Scientific Research Innovation Project 11YZ282\. The author is grateful to all colleagues in the Department of Computer Science and Technology Shanghai Sanda University for their support, help, and encouragement R EFERENCES 1 Vishal Gour, S.S. Sarangdevot, Govind Singh Tanwar Anand Sharma 
and Load \(ETL\ in Data Warehouse Vol. 02, No. 03 pp.786-789, 2010 2 Ning Zhang, Jia Ziyan,Shi Zhongzhi Research on Technology of ETL in Data Warehouse  pp. 213 216 2002.24 3 Sa Liao Design and optimization of Data Bank Increment ETL Vol.3, No.3, pp.5-9, Aug, 2008 4 Jiang Licheng The ETL Technology for Changed Data Vol.34, No.10 B, pp.219-221, 2007  5 Shu Qi The Research on Optimization of ETL Process and Incremental Data Extraction Thesis, Hunan University, 2011 6 Ralph Kimball , Joe Caserta Wiley, 2004 7 Microsoft Technical Champs for Siebel www.siebelonmicrosoft.com 8 Alkis Simitsis Kevin Wilkinson, Umeshwar Dayal, Malu Castellanos Fault-Tolerance ICDE, 2010 9 Fengjuan Yang Performance Appraisal System Vol.2, No.4, pp.116-121, November, 2009  Kommineni Sivaganesh, P Srinivasu, Dr Suresh Chandra Satapathy Warehouse International Journal on Computer Science and Engineering, Vol. 4 No. 09, pp.1579-1586, Sept. 2012 
International Journal on Computer Science and Engineering Computer Engineering and Applications Journal of Guiyang College, Natural Sciences \(Quarterly Computer Scinence The Data Warehouse ETL Toolkit Computer and Information Science 
Improve Performance of Extract, Transform           Optimizing ETL Workows for  Analysis and Design of ETL in Hospital  Optimization of  ETL Work Flow in Data  
 
88 


R EFERENCES  1  J. Dean and S. Ghemawat, "MapReduce: simplified data processing on large clusters Commun. ACM vol. 51, pp. 107-113, 2008 2  Amazon Amazon Elastic Compute Cloud \(Amazon EC2 Available http://aws.amazon.com/ec2  3  Amazon Amazon Simple Storage Service \(Amazon S3 Available http://aws.amazon.com/s3  4  Amazon Amazon Elastic MapReduce \(Amazon EMR Available http://aws.amazon.com/elasticmapreduce  5  Apache Welcome to Apache  Hadoop Available http://hadoop.apache.org  6  Nokia Disco MapReduce Available http://discoproject.org  7  M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly, "Dryad distributed data-parallel programs from sequential building blocks presented at the Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems 2007, Lisbon, Portugal 2007 8  Y.-J. Chang, C.-C. Chen, C.-L. Chen, and J.-M. Ho, "A de novo next generation genomic sequence assembler based on string graph and MapReduce cloud computing framework BMC Genomics vol. 13 pp. 1-17, 2012 9  C.-C. Chen, Y.-J. Chang, W.-C. Chung, D.-T. Lee, and J.-M. Ho CloudRS: An Error Correction Algorithm of High-Throughput Sequencing Data based on Scalable Framework," in IEEE International Conference on Big Data In press   B. Langmead, K. D. Hansen, and J. T. Leek, "Cloud-scale RNAsequencing differential expression analysis with Myrna Genome Biol vol. 11, p. R83, 2010   M. C. Schatz, "CloudBurst: highly sensitive read mapping with MapReduce Bioinformatics vol. 25, pp. 1363-9, Jun 1 2009   L. D. Stein, "The case for cloud computing in genome informatics Genome Biol vol. 11, p. 207, 2010   E. Anderson and J. Tucek, "Efficiency matters SIGOPS Oper. Syst Rev vol. 44, pp. 40-45, 2010   J. Cohen, B. Dolan, M. Dunlap, J. M. Hellerstein, and C. Welton MAD skills: new analysis practices for big data Proc. VLDB Endow vol. 2, pp. 1481-1492, 2009   D. J. DeWitt and M. Stonebraker. \(2008 MapReduce: A major step backwards Available http://databasecolumn.vertica.com/databaseinnovation/mapreduce-a-major-step-backwards    B. Irving Big data and the power of hadoop    A. Pavlo, E. Paulson, A. Rasin, D. J. Abadi, D. J. DeWitt, S. Madden  et al A comparison of approaches to large-scale data analysis presented at the Proceedings of the 2009 ACM SIGMOD International Conference on Management of data, Providence, Rhode Island, USA, 2009   M. Schroepfer Inside large-scale analytics at facebook    J. Ekanayake, H. Li, B. Zhang, T. Gunarathne, S.-H. Bae, J. Qiu et al Twister: a runtime for iterative MapReduce," presented at the Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing, Chicago, Illinois, 2010   W. Guanying, A. R. Butt, P. Pandey, and K. Gupta, "A simulation approach to evaluating design decisions in MapReduce setups," in Modeling, Analysis & Simulation of Computer and Telecommunication Systems, 2009. MASCOTS '09. IEEE International Symposium on 2009, pp. 1-11   H. Herodotou, H. Lim, G. Luo, N. Borisov, L. Dong, F. B. Cetin et al Starfish: A Self-tuning System for Big Data Analytics In Proceedings of the 5th Conference on Innovative Data Systems Research 2011   E. Jahani, M. J. Cafarella, C. R, and #233, "Automatic optimization for MapReduce programs Proc. VLDB Endow vol. 4, pp. 385-396 2011   S. Lattanzi, B. Moseley, S. Suri, and S. Vassilvitskii, "Filtering: a method for solving graph problems in MapReduce," presented at the Proceedings of the 23rd ACM symposium on Parallelism in algorithms and architectures, San Jose, California, USA, 2011   K.-H. Lee, Y.-J. Lee, H. Choi, Y. D. Chung, and B. Moon, "Parallel data processing with MapReduce: a survey SIGMOD Rec vol. 40 pp. 11-20, 2012   C. Ranger, R. Raghuraman, A. Penmetsa, G. Bradski, and C Kozyrakis, "Evaluating MapReduce for Multi-core and Multiprocessor Systems," presented at the Proceedings of the 2007 IEEE 13th International Symposium on High Performance Computer Architecture, 2007   Cloudera Optimizing MapReduce Job Performance Available http://www.slideshare.net/cloudera/mr-perf    T. White Hadoop: The Definitive Guide O'Reilly Media, 2009   J. Lin and C. Dyer Data-Intensive Text Processing with MapReduce  Morgan and Claypool Publishers, 2010   J. D. Ullman, "Designing good MapReduce algorithms XRDS vol 19, pp. 30-34, 2012   S. Gnerre, I. MacCallum, D. Przybylski, F. J. Ribeiro, J. N. Burton B. J. Walker et al High-quality draft assemblies of mammalian genomes from massively parallel sequence data Proceedings of the National Academy of Sciences 2010  A PPENDIX  A  Rules-of-thumb policy for configurations Table A1 lists our Hadoop configuration parameters depends on the rules-of-thumb policy that aims at ensuring the values are not exceed the physical limitation of each computing node. Assume that we have 10 computing nodes and each node has 8 CPU cores, 16 GB memory, and acts as Data Node and Task Tracker. We demonstrate the calculation of the first three parameter values of Hadoop framework in Table A1. To achieve the best-effort of CPU utilization, there would be assign 2 processes to utilize for each CPU core, in general. Thus, there are 16 processes execute simultaneously in a node. However, to obtain the functionality of underlying operating system, we prepare one CPU core for system routine process and I/O operations Thus, there are at most 7 CPU cores for Hadoop framework and we decide to set up at most 7 map tasks and 7 reduce tasks concurrently. Since in-memory processing is faster than performing operation with swap space or content switching we optimism the memory usage of each node is within its physical boundary. Furthermore, we preserved around 500 MB for processes of operating system, 1 GB for operations of Data Node, and 1 GB for Task Tracker. To utilize the rest 13 GB memory with at most 14 tasks concurrently, we can assign 950 MB memory for each task 5 


 TABLE  A1  A SUBSET OF JOB CONFIGURATION PARAMETERS OF H ADOOP THAT AFFECT JOB PERFORMANCE SIGNIFICANTLY  Parameter Name in Hadoop Description and Use Default Values Our Settings mapred.child.java.opts Java options for the task tracker child processes Xmx200m Xmx950m mapred.tasktracker.map.tasks.maximum Maximum number of map tasks run simultaneously by a task tracker 2 7 mapred.tasktracker.reduce.tasks.maximum Maximum number of map tasks run simultaneously by a task tracker 2 7 mapred.reduce.slowstart.completed.maps Fraction of the completed map tasks to start reduce tasks in a job 0.05 0.4 mapred.reduce.parallel.copies Number of parallel transfers 5 15 io.sort.mb Map-side buffer size \(in MegaBytes\ for buffering and sorting key-value pairs 100 384 io.sort.record.percent Fraction of io.sort.mb to store metadata of key-value pairs 0.05 0.15 io.sort.factor Number of sorted streams to merge at once when sorting files 10 38 mapred.map.tasks Default value of map tasks per job 2 10 mapred.reduce.tasks Default value of reduce tasks per job 1 10  6 


of the clusters revealed that PSCAN identies two users of a cluster although there is no direct follower/following relation exists but sharing some common interests Such capability of PSCAN helped us to nd users from a city an organization or a country It is not feasible to discuss all the clusters here one of the interesting clusters is a cluster representing twitter pages of BBC weather channel and weather alerts The cluster of 20 members all representing BBC weather related pages is found There are many such clusters that represents a group of users who share some common interests There is no base line of communities in Twitter to measure accuracy of the clustering but our manual observations found the accuracy is signicant Moreover the experiment is designed to prove the feasibility of SCAN in MapReduce framework because SCAN is proved to be accurate enough for clustering The experiment on Twitter data proved the accuracy and scalability of PSCAN V C ONCLUSIONS AND F UTURE W ORK We present a parallel structural clustering algorithm PSCAN for big networks in MapReduce in this paper PSCAN identies clusters as well as vertices playing critical roles such as outliers and hubs in big networks with billions of edges in three steps namely calculating structural similarity of edges cutting off edges with low structural similarity and nding connected components All the steps can be executed in parallel in MapReduce The time complexity of PSCAN is linear with the number of edges in the graph Our empirical evaluation demonstrated an accurate clustering result and an excellent running time in terms of scaleup sizeup and speedup Moreover we applied PSCAN for analysis of a Twitter social network with over 40 million users and 1.4 billions of follower/following relationships The result shows that PSCAN can nd interesting communities of people sharing common interests or other features In the future we plan to further investigate the performance of PSCAN by applying it for the analysis of some really big networks in real world A CKNOWLEDGMENT This project was funded by Acxiom Corporation The authors are grateful for invaluable collaboration with Kevin Liles and Derek Leonard throughout the project This work was supported in part by the National Science Foundation under Grant CRI CNS-0855248 Grant EPS-0701890 Grant EPS-0918970 Grant MRI CNS-0619069 and OISE0729792 Weizhong Zhao would like to thank the support of the National Natural Science Foundation of China No 61105052 R EFERENCES  A Lancichinetti S F ortunato and F  Radicchi 
 Physical Review E 78 046110 2008  S Y ook H Jeong and A Barabasi  In PNAS Proceedings of the National Academy of Science pages 13382-13386 October 2002  L Hubert and P  Arabie  Journal of Classication 193C 218 1985  A Strehl J Ghosh R Moone y   Proceedings of the workshop on articial intelligence for web search pp 58-64 2000  X.Xu N.Y uruk Z Feng T  Schweiger  Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining pp 824-833 2007  Bin W u Y aHong Du  2010 International Conference on Articial Intelligence and Computational Intelligence AICI vol.3 no pp.122-126 23-24 Oct 2010  Usha Nandini Ragha v an Rka Albert and Soundar K umara  Phys Rev E 76 036106 2007  M E J Ne wman and M Girv an  Phys Rev E 69 026113 2004  Jimmy Lin and Chris Dyer   Morgan and Claypool Publishers 2010 pp 94-101  Santo F ortunato  Physics Reports Volume 486 Issues 35 February 2010 Pages 75-174 ISSN 0370-1573 10.1016/j.physrep.2009.11.002  Jia wei Han Micheline Kamber  and Jian Pei  3rd edition Morgan Kaufmann 2011  B.W  K ernighan S Lin  Bell Syst Tech J 49 1970 291307  J Shi J Malik  IEEE Trans Pattern Anal Mach Intell 22 8 2000 888905  U Brandes D Delling M Gaertler  R G  orke M Hoefer Z Nikolski D Wagner  URL http://digbib ubka.unikarlsruhe.de/volltexte/documents/3255  A Clauset M.E.J Ne wman C Moore  Phys Rev E 70 6 2004 066111  G P alla I Der  enyi I Farkas T Vicsek  Nature 435 2005 814818  Martin Ester  Hans-Peter Krie gel J  org Sander Xiaowei Xu  Proceedings of the Second International Conference on Knowledge Discovery and Data Mining KDD-96 AAAI Press pp 226231 ISBN 1-57735004-9 
Benchmark graphs for testing community detection algorithms Modeling the internets large scale topology Comparing partitions Impact of similarity measures on web-page clustering SCAN a structural clustering algorithm for networks Cloud-based Connected Component Algorithm Near linear time algorithm to detect community structures in large-scale networks Finding and evaluating community structure in networks Data-Intensive Text Processing with MapReduce Community detection in graphs Data Mining Concepts and Techniques An efcient heuristic procedure for partitioning graphs Normalized cuts and image segmentation On modularity npcompleteness and beyond Finding community structure in very large networks Uncovering the overlapping community structure of complex networks in nature and society A density-based algorithm for discovering clusters in large spatial databases with noise 
868 


 Akshay U Bhat  http://www.akshaybhat.com/LPMR 2008  Hae w oon Kw ak Changhyun Lee Hosung P ark and Sue Moon  WWW 2010 April 2630 2010 
Scalable Community Detection using Label Propagation  Map-Reduce What is Twitter a Social Network or a News Media 
869 


state of innovation stakeholder  node PQ It  s a balanced node Based on this, we could calculate the  node PQ Calculation process is: set different inn ovation stakeholders state i U  and j U Value of ij 000T can be get from  innovation time difference. Innovation stakeholders social effect and industrial effect can be obtained upon ij B  and ij G set according to relation between innovation stakeholders  Model 4.1 points out  that the value of Gij  directly affects social benefits and sector benefits. Large Gij  can lead to increasing benefits of the entire industry and the entire social growth Bij reflects big organizations impact on businesses. Only strengthening the inter agent association within big organization and enhancing the str ategic partnership between enterprises can jointly promote the development of the entire industry, and bring more social benefits, so that each agent can be improved   5 Summary This paper puts forward the concept of the big organization based on the CSM t heory. It introduces the basic implication of the big organization and theoretical framework of the big organization including: the big organization's perspective  overall perspective, dynamic perspective, and new resource perspective; the big organizat ions sense  the purpose of the organizational structure is innovation, organizational activities around the flow of information, breaking the traditional organizational structure, encouraging self run structure, and blurring organizational boundaries; the big organizations platform  the platform ecosystem of the big organization ; the big organizations operation mode  borderless learning mode, and cluster effect; the big organizations theory  active management theory  leading consumers, and culture  entropy reduction theory  negative culture entropy and humanistic ecology theory  inspiring humanity, and circuit theory  a virtuous circle, and collaborative innovation theory  collaborative innovation stakeholder. This paper also discusses culture entropy reduction theory of the big organization  negative culture entropy, and coordinated innovation theory  innovation stakeholders collaboration. Culture entropy change model and collaborative in novation model are constructed   The research has just begun for the big organization. It also needs further improvement but remains the trend of the times   Reference  1  Gordon Pellegrinetti, Joseph Bentsman. Nonlinear Control Oriented Boiler Modeling A Benchmark Problem for Controller De sign [J  I E E E tr a n s a c tio n s o n c o n tr o l s y s te m s te c h n o lo g y 2 0 1 0  4 1\57 65  2  Klaus Kruger, Rudiger Franke, Manfred Rode Optimization of boiler start up using a nonlinear 457 


boiler model and hard constraints [J  E n e r gy 201 1 29   22 39 2251  3  K.L.Lo, Y.Rathamarit  State estimation of a boiler model using the unscented Kalman filter [J  I E T  Gener. Transm. Distrib.2008 2 6\917 931  4  Un Chul Moon, Kwang. Y.Lee. Step resonse model development for dynamic matrix control of a drum type boiler turbine system [J IE E E  T ra nsactions on Energy Conversion.2009 24 2\:423 431  5  Hacene Habbi, Mimoun Zelmat, Belkacem Ould Bouamama. A dynamic fuzzy model for a drum boiler turbine system [J  A u to m a tic a 2 0 0 9 39:1213 1219  6  Beaudreau B C. Identity, entropy and culture J   J o ur na l  o f  economic psychology, 2006, 27\(2 205 223  7  YANG M, CHEN L. Information Technique and the Entropy of Culture J  A cad e m i c E x ch a n g e  2006, 7: 048  8  ZHANG Zhi feng. Research on entropy change model for enterprise system based on dissipative structure J  Ind ustrial  Engineering and  Management 2007, 12\(1\ :15 19  9  LI Zhi qiang, LIU Chun mei Research on the Entropy Change Model for Entrepreneurs' Creative Behavior System Based on Dissipative Structure J  C h i n a S of t S c i e n c e  2009   8  1 62 166   458 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of Daily Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data  Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average daily operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rolling I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators  Data Element Methods  Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todays cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlights data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlights hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlights method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





