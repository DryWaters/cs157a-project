DRAC: A Direct Rule Mining Approach for Associative Classification Jinzheng Song                      Zhixin Ma*                          Yusheng Xu School of Information Science and      School of Information Science and        School of Information Science and Engineering, Lanzhou University       Engineering, Lanzhou University         Engineering, Lanzhou University Lanzhou , 730000, China            Lanzhou , 730000, China                Lanzhou , 730000, China songjz08@lzu.cn                   mazhx@lzu.edu.cn                     xuyusheng@lzu.edu.cn Abstract The application of associative rule mining in classification \(associative classification power in recent years. The current associative classifier building often adopts three phases: Rule Generation, Building Classifier and Classification. Unfortunately, in rule generation phase, a large number of rules are usually produced, which could not only slow down the mining process but also bring challenge to pruning and storing such magnitude of rules In this paper, we propose the DRAC, a Direct Rules mining approach for Associative Classification, to tackle the efficiency of associative classification problem. DRAC can mine the high quality non-redundant rule set directly. At the same time, it also adopts the multiple strong class association rules to classify the unlabeled dataset correspondingly. The experimental results show that DRAC is more efficient than traditional approach CBA without losing of accuracy Keywords: associative classification; generator non-redundant rule set I. INTRODUCTION Association rule mining and classification are two important data mining techniques used in amount application areas, including finical market, bioinformatics web analysis, and so on. The goal of association rule mining is to find the rules in the database with confidence and support greater than the user specified threshold [1 Classification is used to build a classifier by analyzing the ____________________ Corresponding Author: mazhx@lzu.edu.cn The work was supported by the Fundamental Research Funds for Central Universities under grant No lzugbky-2010-91 given training datasets with a class label, and predict the unlabeled objects. In recent years, a new approach called 


associative classification is proposed to integrate association rule mining and classification, such as CBA[7 CMAR[8]. This new technique was found to be competitive with traditional classification methods, such as C4.5 and SVM due to its higher accuracy Unfortunately, recent studies show that associative classification also suffer efficiency problem inheriting from association rule mining, the large number of rules discovered. First, mining so large number of rules is time consuming due to the exponential combination of attribute-pairs. Especially, when the datasets scale is large or minimum support threshold is low, it cannot be completed. Moreover, the last classifier building is always using a small high quality rule set, so we have to prune the class association rules \(CARs  doubt so large number of rules will bring challenge to the prune phase The computational cost raised by traditional association rule mining motivates us to investigate an alternative approach: instead of mining the complete set of CARs directly mining the high quality non-redundant rules. This leads to our proposal direct mining high quality rules for classification, DRAC Our contributions are summarized as follows 1 DRAC loss of accuracy. The experimental results show that DRAC is more efficient and accurate in companion with CBA 2 association rule mining, a large number of rules DRAC extends the efficient generator mining 2010 International Conference on Artificial Intelligence and Computational Intelligence 978-0-7695-4225-6/10 $26.00  2010 IEEE DOI 10.1109/AICI.2010.155 150 algorithm, GrGrowth[2], to mine a compact set of high quality non-redundant rules directly from the training set in comparison with traditional CBA[7], CMAR[8] which generate all CARs satisfying the support and confidence threshold. And we also adopt the multiple strong 


class association rules to classify the unlabeled dataset to avoid bias caused by single rule 3 which makes the classifier compact and efficient Moreover it uses hash table to eliminate the redundant rules during the mining process The outline of this paper is as follows: Section 2 introduces the relate works. Section 3 describes the rule generation process in DRAC. Section 4 discusses how to build the classifier. The experimental results are presented in section 5 and we conclude the study in section 6 II. RELATE WORKS In general, an association rule is the form of X ?Y expressing the semantics that occurrence of X is associated with occurrence of Y, where X and Y are collections of data items. However, the task of classification is to build a classifier from the training dataset so that it can be used to predict class label of the unknown object with high accuracy. In the training dataset except the common attributes, there is a class label. So, in associative classification the rule is the form like X ?c called class association rule. X is the attribute sets, and the c is a value of class label. Let D be the training set, I be the set of all attributes values in D, and C be the set of class label. We say that a data object d contains X, a subset of I if X?d. Given a class association rule R: X ?c, where X?I, and c? C, the number of data objects in D containing X and having class label c is called the support of R, denoted as of sup\(R objects containing X and having class label versus the total number of objects containing X is called the confidence of R, denoted as conf\(R Two important association rule based classifiers are CBA [7] and CMAR [8]. CBA first generates all the class association rules greater than the given support and confidence thresholds as candidate rules. It then selects a small set of rules from them to form a classifier. When it predicts the class label for an unlabeled object, the best rule \(i.e., with the highest confidence satisfied by the object is chosen. CMAR generates and evaluates rules in a similar way as CBA \(but uses a more efficient algorithm FP growth [11 


that it uses multiple rules in prediction, using weighted ?2 The traditional associative classification methods always generate rules from the frequent items attribute-pairs is much larger due to the exponential combination of attribute-pairs. Especially, when the datasets scale is large or minimum support threshold is low, it cannot be completed. Moreover, the rules generated contain a large number of redundant ones and are needed to prune before building the classier. So, they are time consuming Previous study ACCF[10] uses frequent closed itemsets to form the rules to lessen the number of rules successfully Frequent closed itemsets is a concise representation of the frequent itemsets, so its number is always smaller Mannila et al.[4] first propose the notion of condensed representation. Several methods[2],[3],[5] for concise representation of frequent itemsets have been previously proposed to eliminate the redundancy including frequent closed itemsets, generators and generalizations of generators. Frequent closed itemsets are the maximal among the itemsets appearing in the same set of transactions and generators are the minimal. Frequent closed itemsets mining has been well studied and several efficient algorithms have been proposed. However, little effort has been put on developing efficient algorithms for mining generator based representations The concept of generators is first introduced by Bastide et al. [6]. They use generators together with frequent closed itemsets to mine minimal non-redundant association rules In fact, generators are more preferable than closed itemsets in classification, because the later contains some redundant items that are not useful for classification and it violates the minimum description length principle In the previous study, [2] has proposed a new concise representation of frequent itemsets using generators and positive borders. In this paper we extend their method GrGrowth 151 for classification. Our experimental results show that number of rules we mined is much smaller and our classifier is more compact and general III. GENERATING RULES FOR CLASSIFICATION 


In this section, we extend GrGrowth for frequent generators mining to discover the high quality non-redundant rule set Definition 1 \(Generator there does not exist l, such that l?l and support \(l support \(l According to the definition, generator is the minimal representation of the equivalent class compared with the closed itemset which is the maximal. As described in the previous section, the generator is more representative and it is more preferable than closed itemset in classification Property 1 \(anti-monotone property generator, then ?  l? l, l is not a generator Property 1 implies that if itemset l is a generator, then all of its subsets are generators Definition 2 \(General rule X?C class association rule. We say that R1 \(X1?C1 general than R2 \(X2?C2 Definition 3 \(Non-redundant rule set R= {R1,......, Rn}.we say that R is non-redundant if and only if ,given a rule Ri\(Xi?Ci Rj\(Xj?Cj Rj Ri other word, there does not exist one rule which is more general and with higher confidence than Ri It is clear that if there exists such a rule Rj then Ri does not convey any new information. Thus we say that Ri is redundant. To guarantee this non-redundant rule set, first we use generator to form the rules. According the definition of generator, it is the minimal representation of the equivalence class. In the example 1, there are 4 itemsets {a, ad, ae, ade} in the equivalence class, we only generate one rule using the generator a?y and the other three rules are redundant because they do not convey any new information. Moreover, in the generator mining process, we use the hash table to check whether there exists at least one more general rule with higher confidence Example 1 \(Mining class association rules training data set T as shown in Table1. Let the support threshold is 2 and the confidence threshold is 50%. DRAC mines the class association rule as follows Table 1. A training data set 


Tid  Transactions          Class 1     a b c d e g            y 2     a b d e f              y 3     b c d e h i             n 4     a d e m               n 5     c d e h n              y 6     b e i o                y DRAC extends the GrGrowth algorithm to find frequent generators and generate rules in one step. To achieve this purpose, when constructing the conditional FP-tree, we add the distribution of various class labels among data objects for every node, as shown in Figure\(1  Figure 1. FP-tree in Example 1 The GrGrowth[2] algorithm explores the search space using the depth-first-search strategy. During the mining process, the GrGrowth algorithm needs to check whether an itemset is a generator by comparing the support of the itemset with that of its subsets. To guarantee that all the subsets of a frequent itemset are discovered before that itemset, the GrGrowth algorithm traverses the search space tree in descending frequency order. In the example shown in the Figure\(1 processed, and then the conditional databases of item b item a and so on. The conditional database of item i is processed last When mining itemset ls conditional database Dl, the 152 GrGrowth algorithm first traverses Dl to find the frequent items in Dl, denoted as Fl={a1, a2, , am}, and then construct a new FP-tree which stores the conditional databases of the frequent items in Fl. According to the anti-monotone property, there is no need to include item aj? Fl into the new FP-tree if l?{aj} is not a generator Non-generators are identified in two ways in the GrGrowth algorithm. One way is to check whether support\(l?{ai l performed immediately after all the frequent items in Dl are discovered and it incurs little overhead. The second way is to check whether there exists itemset l such that l? \(l?{ai l l?{ai such that support\(l?{ai l 


process, the GrGrowth algorithm maintains the set of frequent generators that have been discovered so far in a hash table to facilitate the subset checking. Further details can be obtained in[2 Differently from the GrGrowth, we form the rule using the generator discovered so far. If the rules confidence is greater than the threshold and it is non-redundant as defined in definition3, then we put it into the hash table However, if the rule does not satisfy the above two conditions but the rule body is a generator, then we only put the generator into the hash table. For example, we find the rule <bd?y,2/6,2/3> is redundant according the Definition 3 because in the hash table there exists a rule b?y,3/6,3/4> which is more general and has higher confidence, so it only stores the generator bd in the hash table. In the generator checking process, we check whether the rule generated is non-redundant or not at the same time so there is no extra time consuming. Thus, we directly mine the non-redundant rule set efficiently for building the classifier We define the form of rule in DRAC as <rule, support confidence>. In this example we only generate 7 non-redundant rules d?y,3/6,3/5>,<b?y,3/6,3/4>,<a?y,2/6,2/3> ,<c?y,2/6 2/3>,<h?n,1/6,1/2>,<i?n,1/6,1/2>,<ab?y,2/6, 1 IV. BUILDING A CLASSIFIER After the non-redundant rule set is mined, as discussed in Section 3, DRCA is ready to build a classifier based on database coverage similar to CBA[7]. Firstly, it sorts the rules according the following definition. Secondly, it chooses a set of high precedence rules in R to cover training data Definition: Given two rules, ri and rj, ri ?  rj \(also called ri precedes rj or ri has a higher precedence than rj 1 2 ri is greater than that of rj or 3 the same, but ri is generated earlier than rj DRAC for building the classifier and predict an unseen case is similar to the CBA, but there are two major differences 


1 For example, CBA discover 35 CARs but DRAC only 7 rules without lose of information 2 rule r?c whose body is satisfied by t, that is t?r However, DRAC use the best k rules of each class for prediction like CPAR[9] to avoid bias caused by single rules: 1 are satisfied the new object; 2 for each class; and 3 expected accuracy of the best k rules of each class and choose the class with the highest expected accuracy as the predicted class V. EXPERIMENTAL RESULTS AND PERFORMANCE STUDY In this section, we present some experimental results and also compare them with CBA. We conduct the experiments using the same parameter originally proposed by its authors. The support threshold is set to 1% and the confidence threshold is set to 50%. Also, we adopt the same method used by CBA to discrete continuous attributes Table2 gives the comparison of accuracy and the number of rules between DRAC and CBA. The results was obtained by 10 fold cross validation over 15 datasets from the UCI ML repository [12 As can be seen from the table 2, DRAC outperforms CBA on accuracy and it produces a much smaller number of CARs. The average accuracy increase from 0.8418 for 153 CBA to 0.8544 for DRAC and out of the 15 datasets DRAC achieves the better accuracy in 10 ones \(marked with * in table2 decreases from 23955.9 for CBA to 2396.7 for DRAC Table 2  Experiment results dataset CBA CARs DRAC CARs CBA accuracy DRAC 


accuracy heart* 52309 5975.8 0.8295 0.8519 tic 7063 1395 0.9886 0.9852 led7* 464 656.6 0.695 0.814 glass 4234 425.9 0.7698 0.7687 horse* 62745 6973 0.8061 0.812 diabetes* 3315 164 0.7587 0.773 breast* 2831 401.7 0.9599 0.971 wine 38070 1531 0.9833 0.9622 iris* 72 72.7 0.9391 0.953 labor 5565 349 0.9499 0.933 pima* 2977 741.5 0.7601 0.769 crx* 42877 4859 0.8449 0.8591 vehicle* 23446 6012 0.7195 0.7275 hepatic 63134 2321.4 0.8576 0.8327 auto* 50236 4072 0.7656 0.803 average 23955.9 2396.7 0.8418 0.8544 VI. CONCLUSIONS In this paper, we proposed a novel classification algorithm DRAC that mines high quality class association rules directly. Thus it resolves the problem inheriting from association rule mining, a large number of rules, which could not only slow down the mining process but also bring challenge to the pruning and storing the rules. We extend the GrGrowth algorithm to produces the non-redundant rule set directly and build a compact classifier. Our experiments on UCI datasets show that DRAC is consistent, highly effective and has better average classification accuracy compared to CBA. As a future work, we will continue comparing DRAC with other associative classification approaches and we will also study the other uses of the generator REFERENCES 1]. R. Agrawal and R. Srikant. Fast algorithms for mining association rules in large databases. In Proc. of the 20th VLDB Conference, pages 487499, 1994 2]. Guimei Liu, Jinyan Li, Limsoon Wong. A New Concise Representation of Frequent Itemsets Using Generators and A Positive Border. Knowledge and Information Systems 17\(1 3]. F. Bonchi and C. Lucchese. On condense representation of constrained frequent patterns. Knowledge and Information 


System,9\(2 4]. H. Mannila and H. Toivonen. Multiple uses of frequent sets and condensed representations. In Proc. of the 2nd ACM SIGKDD Conference, pages 189194, 1996 5]. A. Bykowski and C. Rigotti. A condensed representation to find frequent patterns. In Proc. of the 20th PODS Symposium, 2001 6]. Y. Bastide, N. Pasquier, R. Taouil, G. Stumme, and L Lakhal. Mining minimal non-redundant association rules using frequent closed itemsets. In Proc. of Computational Logic Conference, pages 972986, 2000 7]. B. Liu, W. Hsu and Y. Ma, Integrating Classification and Association Rule Mining. Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining, 1998, pp.8086 8]. W. Li, J. Han and J. Pei, CMAR: Accurate and efficient classification based on multiple class-association Rules. In ICDM'01, San Jose, CA, Nov.2001, pp. 369-376 9]. X. Yin and J. Han, CPAR: Classification Based on Predictive Association Rules. Proceedings of the Third SIAM International Conference on Data Mining, 2003, pp 208217 10]. ACCF: Associative Classification Based on Closed Frequent Itemsets. Proceedings of the Fifth International Conference on Fuzzy Systems and Knowledge Discovery, 2008, pages 380-384 11]. J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In SIGMOD, 2000, 1-12 12]. C.J. Merz, and P. Murphy, UCI repository of machine learning databases, 1996  154 


343 Intelligent Control System for Two-motor Synchronous Driving Yunjun Chen ,Xiuming Jiang and Gongyuan Yang 348 A Study of NC Limit Traction Technology in the Scope of Cutting Hard Materials Jiali Wu, Ming Li,Yingui Liu and Guozhi Song 352 Detecting Implicit Exceptions of Service-oriented Application Based on Context Xiangyang Jia, Shi Ying, Kai Zhao and Qing Wu 356 Fuzzy Control for the Handling Stability of Motor Drive Electric Vehicle Yongli Zhao and Yuhong Zhang 361 Acoustic-Structure Coupled Analysis of Interior Noise for Heavy Commercial Vehicle MA Tianfei,GAO Gang and ZHANG Yuntao 366 Sahana Alerting Software for Real-Time Biosurveillance in India and Sri Lanka Nuwan Waidyanatha, Gordon Gow, Chamindu Sampath, Ganesan M., Janakiraman, N., Mifan Careem,Damendra Pradeeper and Mahesh Kaluarachchi 370 A New Hybrid PSOGSA Algorithm for Function Optimization Seyedali Mirjalili and Siti Zaiton Mohd Hashim 374 xi  Control Software for Automated Microrobotic Paper Fiber Characterization Mathias von Essen, Seppo Kuikka and Pasi Kallio 378 A Visual Authoring Framework for Intelligent Flight Trainer GENG Xiaobing, LIU Sijiang, JIANG Yongshi and YANG Yiping 382 Comparison of Radiometry and Modified Periodogram Spectrum Detection inWireless Radio Networks P. Derakhshan-Barjoei, G. Dadashzadeh, F. Razzazi and S. Mohammad Razavizadeh 386 On The Miniaturization of Microstrip Line Fed Slot Antenna Using Various Slots Mustafa K. Taher Al-Nuaimi 390 An Efficient and Cost Effective Internetworking Setup for Research and Teaching Purpose Savitri Bevinakoppa 394 The Mobility Management Scheme Based on MP2P Dynamic Structure Model 


Fangfang Guo, Weiwei Xu and Bingyang Li 398 Research on Index Releasing in MP2P Based on Super-Peers Fangfang Guo, Jing Xu and Bingyang Li 402 ICCIA 2010 Session 7 Active Vibration Control of Inflated Space Stuctures using Smart Materials Kamel, M.A. and Weiliang He 406 Practical Precision Bound for Indoor Location Determination Yiming Ji 410 The Use of Genetic Algorithm for Designing Mimo Antenna Placement Peerapong Uthansakul, Danai Assanuk and Monthippa Uthansakul 414 Design and Implementation of Digital Campus Based on LBS and WSN Yunchen Jiang and Shanshan Wan 418 Process of Information Transmission in Varying Nano-microstructure Jun LU 422 An Implementation Framework of Globus Resource Management Model Based on Extended WS-Agreement QI Chao and ZHANG Xi 426 Performance of Switched-beam Antennas for Wireless Mesh Networks using Synchronous Collision Resolution Protocol Sineenart Panngam, Peerapong Uthansakul and Monthippa Uthansakul 430 A Multiple Frame Integration and Mathematical Morphology Based Technique for Video Text Extraction Lijie Li,Jin Li,Yang Song and Lei Wang 434 The Key Technologies of Painting Style Rendering Based on Image 438 xii  Li Zhen-ya Physical Layer Simulations of the Broadband Wireless LANs Based on OWSS Signalling Technique Guozhi Song and Laurie Cuthbert 442 Fluid Analysis of Gas-Liquid Two-Phase Flow Based on Semiconductor Laser 


Measurement and S Transform  446 Applying Grey Relational Analysis to Evaluating the Connection Degree of Various Factors in China Textile Industry Interior Ma Tao and Du Yuzhou 450 Automated Diagnosing of Nevus Flammeus Using OCT Raw Signal Yankui Sun and Chengkun Xue 453 Prediction of Day-ahead Electricity Price Based on Information Fusion Huixin Tian, Mu Zhang and Bo Meng 457 The Research and Realization of Multi-threaded Intelligent Test Paper Generation Based on Genetic Algorithm Ying Shan 461 Application of Virtual Reality Technology in Bridge Structure Safety Monitoring Dongwei Qiu and Lanfang Gao 465 Study on On-line Measurement System of Stone Slabs Based on OpenCV Zhao Min and Na Lihong 468 Author Index 471  Weixin Liu, Ningde Jin, Guozhi Song and Mingzhe Liu xiii          Organizing Committees Honorary Chair b f f f\f\f\b 


General Chair f!!\f  Organizing Chairs  f!!\f  f$% \f!!\f  Organizing Co-chairs f    f  f\f   f\f\f   f*!\f!\f.\f  f f  Program Committee Chairs  f\f\f   f!!\f  Publication Chair 0   


200 250 300  The size of dataset/10,000 R es po ns e tim e S    a 0 50 100 150 200  The size of dataset/10,000 R es po ns e tim e S    b 0 10 20 30 40 50 


60  The size of dataset/30,000 R es po ns e tim e S    c Fig. 9 The scalability of our algorithm compared with FP-growth  Paper [12] proposed a way to reduce times of scanning transaction database to reduce the cost of I/O IV. CONCLUSIONS AND FUTURE WORK This paper first discusses the theory of foundations and association rules and presents an association rules mining algorithm, namely, FP-growth algorithm. And then we propose an improved algorithm IFP-growth based on many association rules mining algorithms. At last we implement the algorithm we propose and compare it with algorithm FPgrowth algorithm. The experimental evaluation demonstrates its scalability is much better than algorithm FP-growth 177 Now, lets forecast something we want to do someday Firstly, we would parallelize our algorithm, because data mining needs massive computation, and a parallelable environment could high improve the performance of the algorithm; Secondly, we would apply our algorithm on much more datasets and study the run performance; At last, we would study the performance when the algorithm deal with other kinds of association rules  REFERENCES 1] S. Sumathi and S. N. Sivanandam. Introduction to Data Mining and its Applications, Springer, 2006 2] V. J. Hodge, J. Austin, A survey of outlier detection 


methodologies, Artificial Intelligence Review, 2004, 22 85-126 3] Han, J. and M. Kamber. Data Mining: Concepts and Techniques. Morgan Kaufmann, San. Francisco, 2000 4] Jianchao Han, Mohsen Beheshti. Discovering Both Positive and Negative Fuzzy Association Rules in Large Transaction Databases, Journal of Advanced Computational Intelligence and Intelligent Informatics 2006, 10\(3 5] Jiuyong Li, Hong Shen, Rodney Topor. Mining Informative Rule Set for Prediction. Journal of Intelligent Information Systems, 2004, 22\(2 6] Jianchao Han, and Mohsen Beheshti. Discovering Both Positive and Negative Fuzzy Association Rules in Large Transaction Databases. Journal of Advanced Computational Intelligence, 2006, 10\(3 7] Doug Burdick, Manuel Calimlim, Jason Flannick Johannes Gehrke, Tomi Yiu. MAFIA: A Maximal Frequent Itemset Algorithm. IEEE Transactions on Knowledge and Data Engineering, 2005, 17\(11 1504 8] Assaf Schuster, Ran Wolff, Dan Trock. A highperformance distributed algorithm for mining association rules. Knowledge and Information Systems, 2005, 7\(4 458-475 9] Mohammed J. Zaki. Mining Non-Redundant Association Rules. 2004, 9\(3 10] J.Han, J.Pei, Y.Yin, Mining frequent patterns without candidate generation, Proceedings ACM SIGMOD 2000 Dallas, TX, May 2000: 1-12 11] P.Viola, M.Jones. Rapid Object Detection Using A Boosted Cascade of Simple Features. Proc. IEEE Conf. on Computer Vision and Pattern Recognition, 2001 12] Anthony K. H. Tung, Hongjun Lu, Jiawei Han, Ling FengJan. Efficient Mining of Intertransaction Association Rules. 2003, 154\(1 178 


For each vertex b in g form j forests body\(a, g, i s.t. bodyAnt\(a, g, i a, g, i with itemsets Ant\(b b and each subset of itemsets Ant\(b b in P\(a, g, j Assign to each leaf l of trees bodyAnt\(a, g, i bodyCons\(a, g, i a fresh variable Vm,M, m, M = size\(itemset\(l Assign to each leaf l of tree headAnt\(a, g, j the variable assigned to itemset l in some leaf of some tree bodyCons\(a, g, i TABLE II.  EXPERIMENTAL DATA Conf. #rules #pruned #dftrs PtC 0.5 6604 2985 1114 0.6 2697 2081 25 0.75 1867 1606 10 0.8 1266 1176 0 0.95 892 866 1 0.98 705 699 1 DSP 0.5 2473 1168 268 0.6 1696 869 64 0.75 1509 844 89 0.8 1290 1030 29 0.95 1032 889 15 0.98 759 723 1 Arry 0.5 770 492 82 0.6 520 353 60 0.75 472 327 39 0.8 408 287 22 0.95 361 255 25 0.98 314 243 30  Our induction algorithm has been launched for each combination of thresholds. Our scheme eliminates all redundant rules in the sense of [25, 31], i.e. those association rules that are not in the covers. All the meta-rule deductive schemes implicitly included in [25] and [31] are induced by our method. The percentage of pruning, thus, outperforms [25 


The results produced for k=3, support 0.25 and confidences between 0.7 and 0.99 are shown in Fig. 3, in terms of pruning percentage \(vertical axis when applied to low confidences \(from 0.7 to 0.9 The percentage of pruning achieved diminishes as the confidence is superior to 0.9. Nevertheless, the pruning is effective with confidence of 0.99 in the majority of cases Pruning at Support = 0.25 0,00 5,00 10,00 15,00 20,00 25,00 30,00 35,00 40,00 45,00 50,00 0,7 0,8 0,9 0,95 0,99 Confidence P ru n in g L e v e l Case 1 Case 2 Case 3  Figure 3.  Pruning experiences at support 0.25  V. DISCUSSION AND CHALLENGES It is important to discuss the technique presented here with focus on the purpose the technique pursues:  to produce semantic recommendation The reader should have noticed that the algorithm presented 


relies strongly on "choice". For instance, the algorithm chooses ears in the graph to form an order for elimination, and the choice is arbitrary. This strategy is essential to maintain low complexity \(polynomial practical. Nevertheless, a warned reader may conclude that this arbitrary choice implies that there are many compactions to produce and therefore the approach as a whole does not show to produce an optimal solution. And the reader is right in this conclusion. Since the goal is compaction, the search for an optimal solution can be bypassed provided a substantial level of pruning is achieved To complete the whole view, we describe how web service descriptions are complemented with the association rules as recommendations. In effect, under our scheme, the document describing the web service is augmented with a set of OWL/RDF/S triples that only incorporate the non-pruned rules with the format of Example 1, that is, the set ARmin of the compaction program obtained by our algorithm, together with the thresholds applied to the mining process and a registered URI of a registered description service. The assumptions and defeaters are not added to the web service description. If the associations encoded in the triples are not sufficient for the client \(a search engine, for instance widening of the response to the description service identified by the given URI, and then the assumptions and defeaters are produced. The reasoning task required for deriving all the implicitly published rules is client responsibility Notice that, under this scheme, the actual rules that appear as members of the set initial ARmin set are irrelevant; the only important issue is the size of the set The developed scheme also supports an extension of the algorithm that admits the assignment of priorities to rules and to itemsets, in order to allow the user to produce a more controlled program as output. Nonetheless, the importance of the extension has not been already tested, and therefore it is beyond the subject of the present paper It would be also interesting to design a scheme that supports queries where the client provides an itemset class and values for support and confidence and the engine produces a maximal class of inferred associated itemsets as a response. This scheme is also under development, so we have not discussed this aspect here 


VI. CONCLUSION In this paper, we have presented a defeasible logic framework for managing associations that helps in reducing the number of rules found in a set of discovered associations. We have presented an induction algorithm for inducing programs in our logic, made of assumption schemas, a reduced set of association rules and a set of counter-arguments to conclusions called defeaters, guaranteeing that every pruned rule can be effectively inferred from the output. Our approach outperform those of [17], because all reduction compactions presented there can be expressed and induced in our framework, and several other patterns, particular to the given datasets, can also be found. In addition, since a set of definite clauses can be obtained from the induced programs, the knowledge obtained can be modularly inserted in a richer inference engine Abduction can be also attempted, asking for justifications that explain the presence of certain association in the dataset The framework presented can be extended in several ways Admitting defeaters to appear in the head of assumption, to define user interest Admitting arithmetic expressions within assumptions for adjustment in pruning Admitting set formation patterns as itemset constants Extending the scope, to cover temporal association rules REFERENCES 1]  R. Agrawal, and R. Srikant: Fast algorithms for mining association rules In Proc. Intl Conf. Very Large Databases. \(1994 2]  A. V. Aho, J. E. Hopcroft, J. Ullman. The design and analysis of computer algorithms, Addison-Wesley, 1974 3]  G. Antoniou, D. Billington, G. Governatori, M. J. Maher, A. Rock: A Family of Defeasible Reasoning Logics and its Implementation. ECAI 2000: 459-463 4]  G. Antoniou, D. Billington, G. Governatori, M. J. Maher: Representation results for defeasible logic. ACM Trans. Comput. Log. 2\(2 2001 5]  A. Basel, A. Mahafzah, M. Al-Badarneh: A new sampling technique for association rule mining, Journal of Information Science, Vol. 35, No. 3 358-376 \(2009 6]  R. Bayardo and R. Agrawal: Mining the Most Interesting Rules. In Proc of the Fifth ACMSIGKDD Intl Conf. on Knowledge Discovery and Data Mining, 145-154, \(1999 


7]  R. Bayardo, R. Agrawal, and D. Gunopulos: Constraint-based Rule Mining in Large, Dense Databases. Data Mining and Knowledge Discovery Journal, Vol. 4, Num-bers 2/3, 217-240. \(2000 8]  A. Berrado, G. Runger: Using metarules to organize and group discovered association rules. Data Mining and Knowledge Discovery Vol 14, Issue 3. \(2007 9]  S. Brin, R. Motwani, J. Ullman, and S. Tsur: Dynamic itemset counting and implication rules for market basket analysis. In Proc. ACMSIGMOD Intl Conf. Management of Data. \(1997 10] L. Cristofor and D.Simovici: Generating an nformative Cover for Association Rules. In ICDM 2002, Maebashi City, Japan. \(2002 11] Y. Fu and J. Han: Meta-rule Guided Mining of association rules in relational databases. In Proc. Intl Workshop on Knowledge Discovery and Deductive and Object-Oriented Databases. \(1995 12] B. Goethals, E. Hoekx, J. Van den Bussche: Mining tree queries in a graph. KDD: 61-69. \(2005 13] G. Governatori, D. H. Pham, S. Raboczi, A. Newman and S. Takur: On Extending RuleML for Modal Defeasible Logic. RuleML, LNCS 5321 89-103. \(2008  14] G. Governatori and A. Stranieri. Towards the application of association rules for defeasible rules discovery In Legal Knowledge and Information Systems, JURIX, IOS Press, 63-75. \(2001 15] J. Han, J. Pei and Y. Yin: Mining frequent patterns without candidate generation. In Proc. ACM-SIGMOD Intl Conf. Management of Data 2000 16] C. Hbert, B. Crmilleux: Optimized Rule Mining Through a Unified Framework for Interestingness Measures. DaWaK: LNCS 4081, 238247. \(2006 17] E. Hoekx, J. Van den Bussche: Mining for Tree-Query Associations in a Graph. ICDM 2006: 254-264 18] R. Huebner: Diversity-Based Interestingness Measures For Association Rule Mining. Proceedings of ASBBS Volume 16 Number 1, \(2009 19] B. Johnston, Guido Governatori: An algorithm for the induction of defeasible logic theories from databases. Proceedings of the 14th Australasian Database Conference, 75-83. \(2003 20] P. Kazienko: Mining Indirect Association Rules For Web Recommendation. Int. J. Appl. Math. Comput. Sci., Vol. 19, No. 1, 165 186. \(2009 21] M. Klemettinen, H. Mannila, P. Ronkainen, H. Toivonen, and A Verkamo: Finding interesting rules from large sets of discovered association rules. In Proc. 3rd Intl Conf. on Information and Knowledge 


Management. \(1994 22] M. J. Maher, A. Rock, G. Antoniou, D. Billington, T. Miller: Efficient Defeasible Reasoning Systems. International Journal on Artificial Intelligence Tools 10\(4 2001 23] C. Marinica, F. Guillet, and H. Briand: Post-Processing of Discovered Association Rules Using Ontologies. The Second International Workshop on Domain Driven Data Mining, Pisa, Italy \(2008 24] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal: Closed sets based discovery of small covers for association rules. In Proc. BDA'99 Conference, 361-381 \(1999 25] N. Pasquier, R. Taouil, I. Bastide, G. Stume, and  L. Lakhal: Generating a Condensed Representation for Association Rules. In Journal of Intelligent Information Systems, 24:1, 29-60 \(2005 26] P. Pothipruk, G. Governatori: ALE Defeasible Description Logic Australian Conference on Artificial Intelligence.  110-119 \(2006 27] J. Sandvig, B. Mobasher Robustness of collaborative recommendation based on association rule mining, Proceedings of the ACM Conference on Recommender Systems \(2007 28] W. Shen, K. Ong, B. Mitbander, and C. Zaniolo: Metaqueries for data mining. In Fayaad, U. et al. Eds. Advances in Knowledge Discovery and Data Mining. \(1996 29] I. Song, G. Governatori: Nested Rules in Defeasible Logic. RuleML LNCS 3791, 204-208 \(2005 30] H. Toivonen, M. Klemettinen, P. Ronkainer, K. Hatonen, and H Mannila: Pruning and grouping discovered association rules. In ECML Workshop on Statistics, Machine Learning and KDD. \(1995 31] M. Zaki: Generating Non-Redundant Association Rules. In Proc. of the Sixth ACMSIGKDD Intl Conf. on Knowledge Discovery and Data Mining, 34-43, \(2000 32] w3c. OWL Ontology Web Language Reference. In http://www.w3.org/TR/2004/REC-owl-ref-20040210 33] w3c. RDF/XML Syntax Specification. In: http://www.w3.org/TR/rdfsyntax-grammar 34] w3c. RDF Schema. In: http://www.w3.org/TR/rdf-schema      


 8   2  3\f            8  D    F  \b 1 8 & #J      b 1  1  4    2  


4 1    9  E 1  2 4 1    9 1   4      8 2  8 1  D 1        1 1  b 


     b b b b b  K            8          2 D 9   F  \b 1 8 ,+J  9 


     b 1     1 2  9 1  12 L 1   9  8       1  2      2   


     b b b b b  K            2  0 \b f  b\f      9       


  8 2   E 1   1     M13 31L 1    b  8E 1   1 #3\b?### 1  1     E 1   1 \b?###3        


1   1   b 1  2 2 18 2     8              1    2 \b 1    2  


    2          2   1 L 2 1   1   L 2 2    2 1  2        


    8  2H D \b A             2  2H D \b A 2 \f 3%\f  f   4%\f f !  , \f\b  C    2    2 


 6    3 1      253 6   1 L 2    6   1         f\b3\f       


               1     1     8 2    E       2  1   


     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


