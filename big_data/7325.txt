Abstract 226 One of the Association rule mining ARM algorithm A p riori is most popular algorithm Pruning approach used in this algorithm differentiates between potential frequent and infrequent itemset well before verifying them in the given database An alternate approach known as filtration does the same. In this paper five experiments are carried out to prove that filtration approach works as efficient as Apriori\222s pruning approach but it requires an extra data structure Keywords 226 Data mining Association Rule Mining Frequent itemset; Apriori algorithm  I INTRODUCTION Every organization deals with the saleable items in their databases Association Rule Mining ARM one of the data mining 3  te c h n iq u e   id e n ti f ie s  u n u s u a l a n d  in te r e s tin g  associations among two or more than two items Association rules helps in marketing decisions to make business more profitable Many association rule generating algorithms are available in literature namely Apriori  FP-Growth  and Elcat  etc  but Apriori algorithm is voted highest in use by research community due to its simplicity 9   Apriori  algorithm uses three steps iteratively In the first step candidate itemsets are generated; in the second step, candidate itemsets are pruned to generate potential itemsets and in the third step database is scanned to check frequentness of the potential itemsets A filtration approach [2  e x is ts  in  t h e  lite r a tu r e  a s  a n  a lte r n a te  to the pruning approach of Apriori algorithm. Both approaches have the same objective i.e they are used to remove candidate itemsets whose candidature is certain to be infrequent without scanning the database In this work, we carried out five experiments and observed that filtration approach is as efficient as apriori\222s pruning approach but it requires an extra data structure This paper is organized as follows Section I.A describes the problem statement. Section II nd summarizes the related work in th is  field In section III rd  Apriori\222s  pruning approach and filtration approach is compared with an example and both approaches are evaluated on standard data sets  In the last section, conclusion and future directions are stated A. Problem Statement The problem of mining association rules comprises two phases In the first phase itemsets which satisfy the given minimum support are generated and in the second phase, association rules which satisfy the given minimum confidence are identified The following is a formal statement of the association rule mining problem to be solved Let a database 001 002 003 004 005 006 007 005 b 007 t 007 005 n 013   co ntaining k numbers of items is known as k itemset. Itemset f  is said to be frequent when all its k items exist in the minimum number of transactions called minimum support minsup The scope of this paper is limited to list all the frequent itemsets. In other words, it is limited to first phase only  II RELATED WORK In the field of ARM Agarwal and Srikant 1  p r o p o s e d  m o s t  popular algorithm called Apriori  It is a three steps process frequent itemsets joining  candidate itemsets pruning  and potential itemsets scanning and counting  In the first step particular pairs of k frequent itemsets are joined to generate k 1-candidate itemsets in the second step k 1-candidate itemsets are pruned to generate k 1-potential itemsets and in the last step potential k 1-frequent itemsets are scanned and their support is counted in the database against  the given minimum support value. These steps are iterated until large k frequent itemsets are generated. The same pruning approach is used by Mannila et al 4    An algorithm, namely DHP used hashing technique to improve the Apriori  algorithm 6   A l te r n a te l y   M u e lle r   5   in tr o d u c e d  prefix tree to improve the Apriori algortihm A probabilistic approach to discover frequent itemsets is given by [7, 8  u s e s  t h e  c o n c e p t o f  r e c u r s i v e  m e d ia n   I t p a r titio n s  th e  frequent itemsets into two categories before the start of first step of Apriori algorithm Modified Apriori  algorithm exists in the literature 2   T h is  algorithm replaced the pruning approach used by Apriori  algorithm with a new pruning approach known as 223filtration\224 In other words, an alternate approach to second step of Apriori  algorithm is presented. Following lemmas differentiate between pruning step of [1  a n d  f iltr a ti o n  s te p  o f   2    L e m m a  1  e x p la in s  the pruning approach and Lemma 2 explains the filtration approach Lemma 1: if any itemset Z is a candidate k-itemset and at least one of its k-1-subsets are found infrequent then Z will be infrequent Proof  Let r 016 007 r 017 007 r 020 007 021 021 021 007 r 022  be all k 1  subsets for candidate k itemset Z.  Moreover, let r 023 be an infrequent itemset i e   024\025\026\026\027\030\031 032 r 023 033 034 001 035\036\n\037\025\026\001\007 027\030\001\006  036  021  1 I t is obvious that support of candidate itemset Z can\222t be greater than itemset 001 r 036   i.e   2014 5th International Conference on Computer and Communication Technolo\gy \(ICCCT 978-1-4799-6758-2/14/$31.00 ©2014 IEEE 23   be a set containing n  tr ansactions where each transaction have items from a set of items. A subset of items 001 f  Evaluation of Filtration and Pruning Approach for Ap riori Algorithm Lalit Mohan Goyal 1 M. M. Sufyan Beg 2 Department of Computer Engineering J amia Millia Islamia \(A Central University New Delhi, INDIA 2 lalitgoyal78@rediffmail.com  mmsbeg@cs.berkeley.edu 1   


 be a set of twenty transactions Ea ch tra nsaction 001\005  001 have a list of items shown in Table I. Value of m in imum support  minsup is taken five Frequency of each individual item present in database 002 is shown in Table II Fo llowing 1-frequent itemsets  016 003  004 006 013 007 004  013 007 004  013 007 004 013 007 004  013 007 004  013 007 004 0 013 007 004 1 013 007 004\006\(\013 2  are f ound while comparing the frequency of each item with the give n minimum support  Thereafter Apr iori  algorithm\222s first step is executed and following 2-frequent candidate itemsets are generated 3 017 003 4 004 006\007 013 007 004 006 007 013 007 004 006\007\013 007 004 006\007 013 007 004 006\007 013 007 004 006\0070 013 007 004 006\0071 013 007 004 006\007\006 013 007 004 007 013 007 004 007\013 007 004 007 013 007 004 007 013 007 004 0070 013 007 004 0071 013 007 004 007\006 013 007 004 007\013 007 004 007 013 007 004 007 013 007 004 0070 013 007 004 0071 013 007 004 007\006 013 007 004 007 013 007 004 007 013 007 004 0070 013 007 004 0071 013 007 004 007\006 013 007 004 007 013 007 004 0070 013 007 004 0071 013 007 004 007\006 013 007 004 0070 013 007 004 0071 013 007 004 007\006 013 007 004 0\0071 013 007 004 0\007\006 013 007 0041\007\006\(\013 5 024\025\026\026\027\030\031\032#\033 001  001 r 023     2 Us ing associative property from equation \223\(1\\224 and \223\(2\\224 it can be stated that 024\025\026\026\027\030\031 032  033 034 001 035\036\n\037\025\026   3  In other words, itemset Z will be infrequent if at least one of its k 1-subsets is found infrequent 001  Lemma 2: if any itemset Z is infrequent then none of it superset can be frequent Proof:  Let Y be an itemset containing all the items of itemset Z  and number of items in set Y is more than number of items in the itemset Z, i.e., Z  Y As if Z is infrequent, it can be stated that 024\025\026\026\027\030\031\032#\033 001 034 001\035\036\n\037\025\026    4  It is obvious that support of itemset Y  can\222t be greater than support of any of its subset in a given database because cardinality of itemset Y  is more than cardinality of itemset Z i.e  001 034 001 r It can be said that 024\025\026\026\027\030\031\032\r\033 001  001\024\025\026\026\027\030\031\032#\033   5 Us ing associative property from equation \223\(4\\224 and \223\(5\\224 it can be stated that 024\025\026\026\027\030\031\032\r\033 001 034 001\035\036\n\037\025\026    6 In  other words itemset Y  can\222t be frequent if it is superset of any itemset Z which is infrequent 001  Next section III.A and III.B demonstrates an example to  explain the work carried out in this paper and section III.C shows the experimental results  III. DIFFERENCE BETWEEN PRUNING AND FILTRATION APPROACH Difference between pruning and filtration approach is revealed in part A and part B of this section  A Pruning approach  001  L e t  003 004 006\007\b\007'\007 t 007 006 013  be a set of ten items A database 001 002 003 004 005 006 007 005 b 007 005  007 021 021 007 005 b 013   I n the second step, pruning approach is applied according to Lemma 1 and following potential 2-frequent itemsets are generated 6 017 003 4 004 006\007  013 007 004 006 007 013 007 004 006\007\013 007 004 006\007 013 007 004 006\007 013 007 004 006\0070 013 007 004 006\0071 013 007 004 006\007\006 013 007 004 007 013 007 004 007\013 007 004 007 013 007 004 007 013 007 004 0070 013 007 004 0071 013 007 004 007\006 013 007 004 007\013 007 004 007 013 007 004 007 013 007 004 0070 013 007 004 0071 013 007 004 007\006 013 007 004 007 013 007 004 007 013 007 004 0070 013 007 004 0071 013 007 004 007\006 013 007 004 007 013 007 004 0070 013 007 004 0071 013 007 004 007\006 013 007 004 0070 013 007 004 0071 013 007 004 007\006 013 007 004 0\0071 013 007 004 0\007\006 013 007 0041\007\006\(\013 5\021   24   Table I. Database D T ransaction Items Purchased Transaction Items Purchased 005 016  1,4,6,7,8,9 005 016\016  3,7 005 017  1,3,4,5,6,7,8,9 005 016\017  1,4,7,8,9 005 020  3,4,6,8,9 005 016\020  1,2,3,4,5,9,10 005 7  1,4,6 005 0167  1,4,6,7 005 8  1,5,7,8,9,10 005 0168  1,3,4,6,8,9,10 005 9  3,4,6,8,9 005 0169  4,8,10 005   2,4,6,9 005 016  1,5,8,9 005   2,3,4,8 005 016  1,4,6,9,10 005   3,7,8 005 016  1,4,5,6,9 005 016  1,2,3,4,6,7,9 005 017  1,4,6,9  Table II.  Individual Items Frequencies Item num ber Frequency Item number Frequency 004 006 013  13 004  013  12 004 b 013  4 004  013  8 004  013  9 004 0 013  11 004  013  16 004 1 013  14 004 013  5 004 006 013  5  In the third step database D  is scanned and frequency of itemsets of set 001 6 017 is counted which is sh own in Table III. This fre quency is compared with given minimum support and following 2-frequent itemsets are generated  017 003  004 006\007 013 007 004 006 007\013 007 004 006\007 013 007 004 006\007 013 007 004 006\0070 013 007 004 006\0071 013 007 004 007 013 007 004 007 013 007 004 0070 013 007 004 0071 013 007 004 007 013 007 004 007 013 007 004,\0070\013 004 0071 013 007 004 0071 013 007 004 0070 013 007 004 0071 013 007 004 0070 013 007 004 0071 013 007 004 0\0071 013 021  These itemsets ar e used to generate 3-frequent candidate itemsets in the next iteration. All the above three steps are executed iteratively In the second iteration, following 3-frequent candidate itemsets and following 3-frequent potential itemsets are generated by first and second step respectively 0013 020 003  A B 004 C\007 D\007 E 013 007 004 006\007,\007 013 007 004 006\007,\007 013 007 004 006\007,\0070 013 007 004 006\007,\0071 013 007 004 C\007 E\007 F 013 007 004 C\007 E\007 G 013 007 004 C\007 E\007 H 013 007 004 006\007-\0071 013 007 004 C\007 F\007 G 013 007 004 006\007.\0070 013 007 004 006\007.\0071 013 007 004 006\007/\0070 013 007 004 006\007/\0071 013 007 004 006\0070\0071 013 007 004 007,\007 013 007 004 007,\0070 013 004 007,\0071 013 007 004 007.\0070 013 007 004 007.\0071 013 007 004 0070\0071 013 007 004 D\007 F\007 G 013 007 004 007.\0070 013 007 004 007.\0071 013 007 004 007/\0070 013 007 004 007/\0071 013 007 004 0070\0071 013 007 004 001.\0070\0071 013 007 004 0070\0071 013 I J K 


  6 020 003 4 004 006\007 007 013 007 004 006\007,\007 013 007 004 006\007,\0070 013 007 004 006\007,\0071 013 007 004 006\007-\0071 013 007 004 006\007.\0070 013 007 004 006\007.\0071 013 007 004 006\007/\0070 013 007 004 006\007/\0071 013 007 004 006\0070\0071 013 007 004 007,\007 013 007 004 007,\0070 013 007 004 007,\0071 013 007 004 007.\0070 013 007 004 007.\0071 013 007 004 0070\0071 013 007 004 007.\0070 013 007 004 007.\0071 013 007 004 007/\0070 013 007 004 007/\0071 013 007 004 0070\0071 013 007 004 001.\0070\0071 013 007 004 0070\0071 013 5   Ta ble III 2-Frequent Itemsets Frequency Potential 2Frequent Itemsets Frequency Potential 2Frequent Itemsets Frequency 004 006 007  013  4 004  007 0 013  8 004 006 007  013  11 004  007 1 013  12 004 006 007 013  5 004  007 006 013  4 004 006 007  013  9 004 007  013  2 004 006 007  013  6 004 007  013  2 004 006 007 0 013  6 004 007 0 013  3 004 006 007 1 013  11 004 007 1 013  5 1,10 4 004 007 006 013  2 004  007  013  7 004  007  013  4 004  007 013  2 004  007 0 013  5 004  007  013  5 004  007 1 013  9 004  007  013  4 004  007 006 013  2 004  007 0 013  6 004  007 0 013  5 004  007 1 013  6 004  007 1 013  5 004  007 006 013  2 004  007 006 013  1 004  007 013  2 004 0 007 1 013  8 004  007  013  12 004 0 007 006 013  3 004  007  013  5 004 1 007 006 013  4 At this point it is to be noted here that 3-frequent candidate itemsets 004 006\007 007\013 007 004 006\007-\007 013 007 004 006\007-\007 013 007 004 006\007-\0070 013 007 004 006\007.\007 013  and 001\004,\007.\007/\013  are bo ld faced. The reason for making itemsets bold faced will be discussed in part B of this section In Table IV frequency of the 3-frequent potential itemsets is shown From this table following 3-frequent itemsets are generated while compared with given minimum support 001\001 020 003 001  004 006\007 007 013 007 004 006\007,\007 013 007 004 006\007,\0071 013 007 004 006\007.\0071 013 007 004 006\007/\0071 013 007 004 006\0070\0071 013 007 004 007,\007 013 007 004 007,\0070 013 007 004 007,\0071 013 007 004 007.\0071 013 007 004 007.\0070 013 007 004 007.\0071 013 007 004 0070\0071 013 007 004 0070\0071 013 021  Similarly in thir d iteration following 4-frequent candidate itemsets 001 3  003 001  004 C\007 D\007 F\007 G 013 007 004 006\007,\007.\0071 013 007 004 C\007 D\007 G\007 L 013 007 004 M\007 D\007 F\007 H 013 007 004 007,\007.\0071 013 007 004 M\007 D\007 H\007 L 013 007 004 007.\0070\0071 013   Eliminated 3infrequent itemsets are stored in the set N 020 003 032 004 006\007 007\013 007 004 006\007-\007 013 007 004 006\007-\007 013 007 004 006\007-\0070 013 007 004 006\007.\007 013 007 004 007.\007 013 033   4-frequent potential itemsets 6 7 003  004 006\007  007.\0071 013 007 004 007,\007.\0071 013 007 004 007.\0070\0071 013 2  and 4-frequent itemsets  7 003  004 006\007 007.\0071 013 007 004 007,\007.\0071 013 007 004 007.\0070\0071 013 2  are not 3fr equent respectively This will result the following set 001 25  are useless to scan and count be cause itemset 004 007  013  is not 2-frequent Similarly 004 006\007 007\013 007 004 006\007-\007 013 007 004 006\007-\007 013 007 004 006\007-\0070 013  are useless to scan and count be cause 001 004 007\013 007 004  007 013 007 004 007 013 007 004 0070 013  are not 2-frequent respectively Bu t this approach requires an extra data structure for storing infrequent itemsets Infrequent itemsets are used to eliminate candidate itemsets according to Lemma 2 Because of this in the third step of first iteration 2-infrequent itemset is stored in the set N 017  004 006 007  013 007 004 006\007\006 013 007 004 007\013 007 004 007 013 007 004 007\006 013 007 001 004 007\013 007 004 007\006 013 007 004 007 013 007 004 007 013 007 004 0070 013 007 004 007\006 013 007 004 007 013 007 004 007\006 013 007 004 007\006 013 007 004 0\007\006 013 007 004 1\007\006 013   Moreover in the third step of second iteration following itemsets  004 006\007 0070 013 007 004 006\007-\0071 013 007 004 006\007.\0070 013 007 004 006\007/\0070 013 007 004 007.\0070 013 007 001\004'\0070\0071\013\007 004,\007/\0070\013\007 004,\007/\0071\007 004/\0070\0071\013 2  are found infrequent Th ese infrequent itemsets are added in the set N 020  resulting into f ollo wing updated set N 020 003  004 006\007 007\013 007 004 006\007,\0070 013 007 004 006\007-\007 013 007 004 006\007-\007 013 007 004 006\007-\0070 013 007 004 006\007-\0071 013 007 004 006\007.\007 013 007 004 006\007.\0070 013 007 004 006\007/\0070 013 007 004 007.\0070 013 007 004 0070\0071 013 007 004 007.\007 013 007 004 007/\0070 013 007 004 007/\0071 013 007 004/\0070\0071\013   Similarly bold f ac ed itemsets  004 006 007 007.\007 013 007 004 006\007,\007/\0071 013 007 004 007,\007.\0070 013 007 004 007,\0070\0071 013 2  from the 4-frequent ca ndidate itemsets are useless to scan and count because 3frequent itemsets 032\004 006\007 007 013 007 004 007.\0070 013 007 004 0070\0071 013 007 004 007/\0071 013\033  Eliminating these 001 032 004 006\007,\007\013 007 004 006\007-\007 013 007 004 006\007-\007 013 007 004 006\007-\0070 013 007 004 006\007.\007 013 007 004 007.\007 013 033  ite msets from 3frequent candidate itemsets following 3-frequent potential itemsets are generated 6 020 003 4 004 006\007 007 013 007 004 006\007,\007 013 007 004 006\007,\0070 013 007 004 006\007,\0071 013 007 004 006\007-\0071 013 007 004 006\007.\0070 013 007 004 006\007.\0071 013 007 004 006\007/\0070 013 007 004 006\007/\0071 013 007 004 006\0070\0071 013 007 004 007,\007 013 007 004 007,\0070 013 007 004 007,\0071 013 007 004 007.\0070 013 007 004 007.\0071 013 007 004 0070\0071 013 007 004 007.\0070 013 007 004 007.\0071 013 007 004 007/\0070 013 007 004 007/\0071 013 007 004 0070\0071 013 007 004 001.\0070\0071 013 007 004 0070\0071 013 5  is generated by first, second and third step re spectively. At this point again, the significance of making the follo wing 4-frequent candidate itemsets  004 C\007 D\007 F\007 G 013 007 004 C\007 D\007 G\007 L 013 007 004 M\007 D\007 F\007 H 013 007 004 M\007 D\007 H\007 L 013 2  bo ld faced will be discussed in part B of this section Fre quency of 4-frequent potential itemsets is shown in Table V There will be no more 5-frequent candidate itemsets generated by Apriori algorithm for this example because none of the pairs from following 4-frequent itemsets  004 006 007 007.\0071 013 007 004 007,\007.\0071 013 007 004 007.\0070\0071 013 2  meets the joining property  B. Filtration Approach In section III.A, it is observed that potential k frequent itemsets may be generated by having an alternate approach known as 223filtration\224 In other words it replaces the apriori\222s  pruning approach. In the filtration approach, not only k frequent but k infrequent itemsets are also generated Continuing the discussion from part A of this section in the first step of second iteration bold faced 3-frequent candidate itemsets 004 006\007 007 013  and 004 007 007 013 


 algorithm objective of pruning approach is to remove infrequent itemsets from the pool of candidate itemsets that are generated during joining process. Filtration approach, an alternative to  pruning process does the same job It generates the same number of candidate itemsets as generated by the  pruning approach but it is fast in its response It is observed from the experiment that filtration approach works more efficiently than the  pruning approach but requires extra memory space In future filtration approach may be helpful in improving Eclat  and 6 7 003 032 004 006\007 007.\0071 013 007 004 007,\007.\0071 013 007 004 007.\0070\0071 013\033  as a 4-frequent potential ite mset and following set N 7 003  004 006\007,\007/\0071 013 007 004 007,\007.\0070 013 007 001 004 007,\0070\0071 013 2  as a 4infrquent itemset In the above part A and part B of this section, it is shown that filtration approach requires an extra data structure to store infrequent itemsets which is not required in Apriori\222s  pruning approach  Table IV 3-Frequent Itemsets Frequencies P otential 3Frequent Itemsets Frequency Potential 3Frequent Itemsets Frequency 004 006 007  007  013  9 004  007  007 1 013  6 004 006 007  007  013  5 004  007  007 0 013  4 004 006 007  007 0 013  4 004  007  007 1 013  5 004 006 007  007 1 013  8 004  007 0 007 1 013  4 004 006 007 007 1 013  4 004  007  007 0 013  5 004 006 007  007 0 013  3 004  007  007 1 013  10 004 006 007  007 1 013  7 004  007  007 0 013  3 004 006 007  007 0 013  4 004  007  007 1 013  4 004 006 007  007 1 013  5 004  007 0 007 1 013  6 004 006 007 0 007 1 013  6 004  007 0 007 1 013  5 004  007  007  013  5 004  007 0 007 1 013  4 004  007  007 0 013  5  C. Experimental Results Five experiments are performed on five different synth etic datasets generated as described by 1    V a lu e s  o f  th e  v a r io u s  parameters set for generating standard datasets are shown in Table VI Each experiment is executed three times for both Apriori  and proposed algorithm Average time taken in seconds by both algorithm is shown below with respect to minimum support value. Evaluated results of five experiments for the algorithms using pruning and filtration approach are shown in Fig 1.1 to Fig 1.5 All experimental results show that filtration approach is an improvement over Apriori\222s pruning approach  Table V 4-Frequent Itemsets Frequencies P otential 4-Frequent Itemsets Frequency 004 006 007  007  007 1 013  5 004  007  007  007 1 013  5 004  007  007 0 007 1 013  5  IV  CONCLUSION & FUTURE WORK Apriori\222s A  priori algorithm REFERENCES  1   Agrawal R Srikant R.,\223Fast algorithms for mining association rules\224 In Proceedings of the international conference on very large data bases VLDB\22294  Santiago, Chile, pp 487\226499, 1994 2   Goyal L.M Beg M.M.S.,\223  An Efficient Filtration A p proach for Mining Association Rules\224 In Proceedings of the 8 th  international conference on C omputing for Sustainable Global Development INDIACom\22214\,BVICAM  New Delhi pp 205-212 2014  3   Han J Kamber M D ata mining concepts and techniques 2 nd edn. Morgan Kaufmann  4   Mannila H Toivonen H Verkamo A.I.,\223Efficient algorithms for discovering association rules\224 In Proceeding of the AAAI\22294 workshop knowledge discovery in databases \(KDD\22294 Seattle,WA, pp 181\226 192, 1994 5   Mueller A 223Fast sequential and parallel algorithms for a ssociation rule mining A comparison\224 Technical CSTR-3515 University of Maryland, College Park, August 1995, pp. 1-5, 1995 6    Park J S, Chen M S, Yu P S.,\223An effective hash-based a lgorithm for mining association rules\224 In Proceeding of the ACM-SIGMOD international conference on management of data SIGMOD\22295  San Jose CA pp 175\226186, 1995 7   Sharma V., Beg M. M. S.,\223A Probabilistic Approach to A priori Algorithm\224 International Journal of Granular Computing Rough Sets and Intelligent Systems IJGCRSIS  Inderscience Publishers ISSN Online 1757-2711  ISSN Print 1757-2703 vol 2 no 3 2012, pp. 225-243, 2012 8   Sharma V., Beg M. M. S.,\223 A Probabilistic Approach to Ap riori Algorithm\224 IEEE International Conference on 26 In FP-Growth Apriori\222s Apriori\222s 


  Granular Computing \(GrC 2010 Silicon Valley, USA August 14-16 IEEE Computer Society Press pp 402408, 2010 9      W u   X    K u m a n   V    Q u in la n   J  R    G h o s h   J    Y a n g   Q   Motoda, H., Mclachlan, G.J., Ng A Liu, B., Yu, P.S Zhou Z Steinbach M.,Hand D.J  Steinberg D 2008 Top 10 algorithm in data mining Knowledge and Information Systems J Ramaan@1981  T able VI. Values of various parameters used in experiments Experiment No  Transactions  Items Correlation Level   Maximal potentially large itemsets Average size of Transactions Average size of maximal potentially large itemsets Value of minimum support  minsup  1 100 K 100 0.5 200 5 2 4% to 10% with a step increment of 1 2 100 K 100 0.5 200 10 2 10% to 20% with a step increment of 1 3 100 K 100 0.5 200 10 4 10% to 20% with a step increment of 1 4 100 K 100 0.5 200 20 4 15% to 30% with a step increment of 3 5 100 K 100 0.5 200 20 6 6%, 9%, 12 15%, 20%, and 25    27 


   Figure 1.1: Experiment 1-Pruning Vs Filtration approach  Figure 1.2: Experiment 2Pruning Vs Filtration approach Figure 1.3 Experiment 3Pruning Vs Filtration approach   001 002 001\001 003\001\001 001\001 005\001\001 001\001 001\001 b\001\001 005  b  Time in Seconds Support 001 002 001\001 003\001\001 001\001 005\001\001 001\001 002\001 002\002 002\003 002  002\005 002  002  002\b 001 005 001 001 002\003\001 002 001 003\001\001 003\005\001 003 001 003\001  001 002\001 002\002 002\003 002  002\005 002  002  002\b  Filtration approach   Pruning Vs Filtration approach   Pruning Vs Filtration approach   Figure 1.4: Experiment 4Pruning Vs Filtration approach  Figure 1.5 Experiment 5Pruning Vs Filtration approach 013 002  001 Pruning Filtration 002\b 002  002\013 003\001 Pruning Filtration 002\b 002  002\013 003\001 Pruning Filtration 001 003 001\001 005\001\001 001\001 001\001 002\001\001\001 002\003\001\001 002  002  003\002 Time in Seconds Support 001 001 001 002\001\001\001 002 001\001 003\001\001\001 003 001\001 001\001\001  001\001 005\001\001\001 005 001\001  013 002\003  Pruning Vs Filtration approach   Pruning Vs Filtration approach  003\005 003\b 001 Support Pruning Filtration 002  003\001 003  f 016\017\020\017\021 022\020\023\024 025\024\020\026\017 28 


VI Tanbeer, S. K., Ahmed, C. F., Jeong, B.-S., & Lee, Y.-K, \215Sliding window-based frequent pattern mining over data streams,\216 Information Sciences, 179\(22\, pp. 3843\2053865, 2009 9 Chang, J., & Lee, W. S, \215Finding recently frequent itemsets adaptively over online transactional data streams,\216 Information Systems, 31\(8\, pp. 849\205869, 2006 4 Agrawal, R., & Srikant, R, \215Fast algorithms for mining association rules,\216 In Proc. VLDB int. conf. very large databases \(pp. 487\205 499\, 1994 3 Tsai, P. S. M, \215Mining frequent itemsets in data streams using the weighted sliding window model,\216 Expert Systems with Applications, 36\(9\, pp. 11617\20511625, 2009  minimum change threshold Y. Chi, H. Wang, P. S. Yu and R. R. Muntz. Catch the moment maintaining closed frequent itemsets over a data stream sliding window. In KAIS, 10\(3\: pp. 265-294, 2006 6 V. kumar, S. satapathy, \215A review on algorithms for mining frequent itemsets over data stream,\216 in ijarcsse V3 I4, 2013 8 CONCLUSION AND FUTURE WORK  Considering the continuousness of a data stream, the traditional methods or techniques for finding frequent itemsets in conventional data mining methodology may not be valid in a data stream. This is because we cannot consider whole data and must identify when a data becomes obsolete or invalid As the old information of a data stream may be no longer useful or possibly invalid at present.  In order to support various requirements of mining data stream, the mining window or the interesting recent range of a data stream needs not to be defined static but must be flexible. Based on this range, a data mining method can be able to identify when a transactions becomes stale or needs to be disregarded  In this paper, we have investigated the problem of mining frequent itemset over data stream using flexible size sliding window model and proposed a new algorithm for this problem. The size of sliding window is adaptively adjusted based on the amount of observed concept change in the underlying properties of incoming data stream. The size of window enlarges or increase when there is no significant amount of change observed. While the window size reduced or decrease when there is considerable amount of concept change or significant change in set of frequent itemsets occurs Based on the value of given by user, the size of window is being controlled. After every pane insertion the set of frequent itemsets are updated and value of concept change is calculated. If the value exceeds the given minimum change threshold the window gets smaller by deleting all the obsolete information before a point defined called checkmark  Experimental results shows that our algorithm tracks the concept change efficiently while mining data stream and is more adaptive to recent frequent itemsets than fixed size sliding window models or time fading window models. For the future work, we are trying to enhance the performance by using fuzzy sets for minimum change threshold value so that the values like low, medium, high and very high instead of certain value between ranges of 0 to 1 R EFERENCES  1                    H. Li, S. Lee, and M. Shan, \215An Efficient Algorithm for Mining Frequent Itemsets over the Entire History of Data Streams\216, In Proc. of First International Workshop on Knowledge Discovery in Data Streams, 2004  F. Nori, M. Deypir, M. Sadreddini, \215A sliding window based algorithm for frequent closed itemset mining over data streams\216 journal of system and software, 2012  Zaki, M. \(2000\. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12\(3\, 372\205 390  Woo H. J., & Lee, W. S. \(2009\. estMax: Tracing maximal frequent item sets instantly over online transactional data streams IEEE Transactions on Knowledge and Data Engineering, 21\(10 1418\2051431  Mozafari B, Thakkar H, Zaniolo C, \215Verifying and mining patterns from large windows over data streams,\216 In Proc. Int. conf. ICDE pp. 179-188, 2008  Koh, J.- L., & Lin, C.- Y, \215Concept shift detection for frequent itemsets from sliding window over data streams\216, lecture notes in computer science: Database systems for advanced applications \(pp 334\205348\ DASFAA Int. Workshops, Springer-Verlag.2009  Han, J., Cheng, H., Xin, D., & Yan, X. Frequent pattern mining Current status and future directions. Data Mining and Knowledge Discovery, 15\(1\, pp.  55\20586, 2007 5 C. Giannella, J. Han, J. Pei, X. Yan, and P. S. Yu. Mining frequent patterns in data streams at multiple time granularities. In Kargupta et al.: Data Mining: Next Generation Challenges and Future Directions, MIT/AAAI Press, 2004 7 2014 IEEE International Advance Computing Conference IACC 510 J. H. Chang and W. S. Lee. estWin: Adaptively Monitoring the Recent Change of Frequent Itemsets over Online Data Streams. In Proc. of CIKM, 2003  J. Yu, Z. Chong, H. Lu, and A. Zhou. False Positive or False Negative: Mining Frequent Itemsets from High Speed Transactional Data Streams. In Proc. of VLDB, 2004      Aggarwal, C, \215A framework for diagnosing changes in evolving data streams,\216 In Proc. ACM SIGMOD int. conf. on management of data \(pp. 575\205586\ 2003 2 Manku, G. S., & Motwani, R. Approximate frequency counts over data streams. In Proc. VLDB int. conf. very large databases \(pp 346\205357\ 2002  


002 
                          
R. Agrawal and R. Srikant. Fast algorithms for mining association rules in large databases. In Proc. VLDB, pages 487499, 1994 2 R. J. Bayardo, Jr. Efficiently mining long patterns from databases SIGMOD Rec., pages 8593, 1998 3 M. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. Parallel algorithms for discovery of association rules. Data Min. and Knowl. Disc., pages 343373, 1997 4 J. Dean and S. Ghemawat. MapReduce: Simplified data processing on large clusters. In Proc. OSDI. USENIX Association, 2004 5 Apache hadoop. http://hadoop.apache.org/, 2013 6 Jiawei Han and Micheline Kamber. Data Mining, Concepts and Techniques. Morgan Kaufmann, 2001 7 M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley M. Franklin, S. Shenker, and I. Stoica. Resilient distributed datasets A fault-tolerant abstraction for in-memory cluster computing Technical Report UCB/EECS-2011-82, EECS Department University of California, Berkeley, Jul 2011 8 M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica Spark: Cluster Computing with Working Sets. In HotCloud, 2010 9 J. Han, J. Pei, and Y. Yin: Mining Frequent Patterns without Candidate Generation. In: Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data, 29\(2\:1-12, 2000 10 M. J. Zaki. Parallel and distributed association mining: A survey IEEE Concurrency, pages 1425, 1999 11 J. Li, Y. Liu, W.-k. Liao, and A. Choudhary. Parallel data mining algorithms for association rules and clustering. In Intl. Conf. on Management of Data, 2008 12 E. Ozkural, B. Ucar, and C. Aykanat. Parallel frequent item set mining with selective item replication. IEEE Trans. Parallel Distrib Syst., pages 16321640, 2011 13 B.-H. Park and H. Kargupta. Distributed data mining: Algorithms systems, and applications. 2002 14 L. Zeng, L. Li, L. Duan, K. Lu, Z. Shi, M. Wang, W. Wu, and P. Luo Distributed data mining: a survey. Information Technology and Management, pages 403409, 2012 15 Li L. & Zhang M. \(2011\. The Strategy of Mining Association Rule Based on Cloud Computing. Proceeding of the 2011 International Conference on Business Computing and Global Informatization BCGIN 11\. Washington, DC, USA, IEEE: 475- 478 16 Li N., Zeng L., He Q. & Shi Z. \(2012\. Parallel Implementation of Apriori Algorithm Based on MapReduce. Proc. of the 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel & Distributed Computing SNPD 12\. Kyoto, IEEE: 236  241 17 Lin M., Lee P. & Hsueh S. \(2012\. Apriori-based Frequent Itemset Mining Algorithms on MapReduce. Proc. of the 16th International Conference on Ubiquitous Information Management and Communication \(ICUIMC 12\. New York, NY, USA, ACM: Article No. 76 18 Yang X.Y., Liu Z. & Fu Y. \(2010\. MapReduce as a Programming Model for Association Rules Algorithm on Hadoop. Proc. of the 3rd International Conference on Information Sciences and Interaction Sciences \(ICIS 10\. Chengdu, China, IEEE: 99  102 19 S. Hammoud. MapReduce Network Enabled Algorithms for Classification Based on Association Rules. Thesis, 2011 20 Synthetic Data Generation Code for Associations and Sequential Patterns. Intelligent Information Systems, IBM Almaden Research Center http://www.almaden.ibm.com/software/quest/Resources/index.shtml 21 C.L. Blake and C.J. Merz. UCI Repository of Machine Learning Databases. Dept. of Information and Computer Science, University of California at Irvine, CA, USA. 1998 http://www.ics.uci.edu/mlearn/MLRepository.html 22 HadoopApriori. https://github.com/solitaryreaper/HadoopApriori 2 3 H.V. Nguyen, E. Muller, K. Bohm. 4S: Scalable Subspace Search Schema Overcoming Traditional Apriori Processing. 2013 IEEE International Conference on Big Data. 2013 24 S. Moens, E. Aksehirli and Goethals. Frequent Itemset Mining for Big Data. University Antwerpen, Belgium. 2013 IEEE International Conference on Big Data. 2013 25 Y. Bu et al . HaLoop: E cient iterative data processing on large clusters. Proceedings of the VLDB Endowment, 3\(1-2\:285296 2010 26 Frequent itemset mining dataset repository. http://fimi.us.ac.be/data 2004   
002 
Our experiments show that YAFIM is about 18 faster than Apriori algorithms implemented in MapReduce framework Furthermore, we can achieve a better performance in both sizeup and speedup for different datasets. In addition, we also evaluated YAFIM for medical application and revealed that YAFIM outperforms MRApriori about 25 speedup  A CKNOWLEDGMENT  This work is funded in part by China NSF Grants \(No 61223003\, and the USA Intel Labs University Research Program R EFERENCES  1 
002 
1671 


