ASSOCIATION RULES FOR SUPPORTING HOARDING IN MOBILE COMPUTING ENVIRONMENTS Y\250 ucel Sayg\021n Bilkent University Turkey saygin@cs.bilkent.edu.tr 250 Ozg\250 ur Ulusoy Bilkent University Turkey oulusoy@cs.bilkent.edu.tr Ahmed K Elmagarmid Purdue University USA ake@cs.purdue.edu Abstract One of the features that a mobile computer should provide is disconnected operation which is performed by hoarding The process of hoarding can be described as loading the data items needed in the future to the client 
cache prior to disconnection Automated hoarding is the process of predicting the hoard set without any user intervention In this paper we describe an application independent and generic technique for determining what should be hoarded prior to disconnection Our method utilizes association rules that are extracted by data mining techniques for determining the set of items that should be hoarded to a mobile computer prior to disconnection The proposed method was implemented and tested on synthetic data to estimate its effectiveness Performance experiments determined that 
the proposed rule-based methods are effective in improving the system performance in terms of the cache hit ratio of mobile clients especially for small cache sizes 1 Introduction Recent advances in computer hardware technology have made it possible the production of small computers such as notebooks and palmtops which can be carried around by users These portable computers are often equipped with wireless communication devices that enable users to access global data services from any location A considerable amount of research has recently been conducted in 
mobile database systems area with the aim of enabling mobile portable computers to ef\002ciently access a large number of shared databases on stationary/mobile data servers A detailed description of the issues related to the recent research on client server computing in mobile environments can be found in JHE99 PS98  Mobile devices are beginning to support applications such as multimedia and World Wide Web Users would like to surf the Internet and read their email while traveling However mobile computers with wireless communi 
cation often frequently disconnect from the network due to the cost of wireless communication or the unavailability of the wireless network Consider a businessperson who frequently travels around by plane Before getting on the plane he/she would have to disconnect and then continue his/her work on the air without wireless network support The same businessman may travel by car and knows that after a certain point no wireless network support in a certain region exists Thus his or her operation must go on into disconnected mode Another person may not be willing to pay 
for a continuous wireless connection but would prefer to connect intermittently in order to access 002les of interest Such scenarios suggest that the disconnection mode may be in high demand and the system should provide support for disconnected operation The process of hoarding involves loading the 002les which may be needed in the future to the client cache prior to disconnection The early work on disconnected operation done by the CODA project group in CMU KS92 w a s b ased on user s manual s peci\002cation o f the hoard set before disconnection via a script language They also had some limited form of automated hoarding 
Automated hoarding is the process of predicting the hoard set without any user intervention An approach for automated hoarding based on clustering was proposed by Kunning and Popek KP97 Thei r c l u s t eri n g m et hod w a s based on the notion of semantic distance The semantic distance between two operations was calculated by using the number of intervening references to other objects Another approach based on identifying the working sets of different programs was proposed by Tait et al TLAC95  I n t hat work the authors constructed program trees together with the 002les used by those programs For a program each ex 
ecution resulted in a tree The trees belonging to the same program were merged to obtain a 002nal hoard set for that program In previously proposed hoarding methods it is assumed that users run a program while disconnected and would like to have access to other programs and data 002les needed by the original program in the cache prior to disconnection This assumption enables researchers to use semantic infor 


mation about the data and executable 002les such as the 002le extensions txt exe and the directories these 002les are located The difference between our approaches and the previously proposed approaches is that we do not assume any knowledge about 002le extensions or directory information We propose a more generic and application independent approach to hoarding In our approach we assume that no information is available regarding which programs are running when the client requests are issued and what operations are issued on the data We introduce the notion of a session which consists of client requests and is independent of the other sessions Our approach to automated hoarding is inferencing with association rules that simply exploits the access patterns of clients to guess the future requests Association rules provide us with sound priority measures like the support con\002dence and size of the rules which can be utilized for limiting the set of items to be loaded to the client cache We performed experiments and compared our association rule based hoarding method with LRU Least Recently Used caching The results showed that rule based hoarding had a higher cache hit ratio especially for small cache sizes which is typical for thin clients and palm top computers Hoarding through association rules can be achieved through the following three phases of execution 1 mining of the history 2 construction of the candidate set and 3 construction of the hoard set In phase 1 the history of client requests are preprocessed and analyzed to come up with the association rules using data mining techniques In phases 2 and 3 these rules are used to construct the candidate set and the hoard set respectively The rest of the paper is organized as follows Section 2 and Section 3 describe the 002rst and second phases of the hoarding process respectively Section 4 describes the simulation model and provides the experimental results we obtained and 002nally Section 5 concludes the paper while discussing possibilities for future research 2 Mining Histories for Association Rules The history of previous client requests should be mined to come up with association rules Mining algorithms for association rules assume that the input data is a collection of sets of items In what follows we provide a formal de\002nition of association rules and discuss how the client request history is preprocessed so that state of the art ef\002cient and incremental association rule 002nding algorithms are utilized 2.1 Association Rules An Overview The problem of 002nding association rules among items has been clearly de\002ned by Agrawal et al AS94  T his de\002nition is provided for clarity Let I  i 1 i 2   i m be a set of literals called items and D be a set of transactions 1  such that 8 T 2 D T 022 I A transaction T contains a set of items X if X 022 T An association rule is denoted by an implication of the form X  Y where X 032 I  Y 032 I and X  Y   A rule X  Y is said to hold in the transaction set D with con\002dence c if c  of the transactions in D that contain X also contain Y Therule X  Y has support s in the transaction set D if s  of transactions in D contain X  Y  The problem of mining association rules is to 002nd all the association rules that have a support and a con\002dence greater than a user-speci\002ed thresholds AS94  T he thresholds for con\002dence and support are called minconf and minsup  respectively The problem of mining association rules has been well studied and is outside the scope of this paper In this paper we focus on how a request history can be analyzed to construct association rules that capture the af\002nity among mobile client requests We will examine the utilization of the resulting association rules in hoarding In this approach we assume that past data access requests of clients issued to the server are stored in a history log Association rules are obtained by analyzing the history of requests issued by clients To date association rules have been used in various contexts For example they are useful in determining the placement and pricing of merchandise at megamarkets AS94 World Wide Web MJHS96  a nd broadcas t i n g d at a  S U 99  However to the best of our knowledge the automated use of association rules in hoarding has not yet been considered by any researcher 2.2 The Use of Association Rules for Hoarding Data mining techniques can be involved in hoarding algorithms in order to extract association rules from client request histories The extracted association rules which represent client access patterns can be used to predict future client requests The predicted request set is what should be loaded prior to disconnection so that the future client requests are satis\002ed locally without requiring a connection to the server Association rules provide guides such as support con\002dence and size of the rules which are crucial in limiting the size of the data set to be hoarded In order to describe our approaches for hoarding we 002rst need to introduce the notion of a session A session consists of a group of continuous client requests and represents a period of user interest for a particular issue In theory such sessions are independent of each other We assume that user requests consist of sessions and that client requests could be 1 In the context of data mining a transaction is a set of items purchased at a time in market basket data and it should not be confused with the notion of transaction in the database context Transactions will be mapped to sessions for hoarding 2 


user 1  win 1 1 xzy win 1 2  z y  win 1 k pqu user 2  win 2 1 xp win 2 2  u w v  win 2 l rs   user n  win n 1 xy win n 2  u v  win n m xyz Table 1 User\255based partitioning of the client request history partitioned into sessions Each session contains some patterns of client requests Data mining techniques 002nd these patterns and produce rules that can be used to build a rule base of associations When disconnection occurs rules are used to infer a user's future requests The rest of the session is stored locally at the client's side It is assumed here that hoarding sets are limited to one session Therefore histories must be divided into a sequence of sessions 2.3 Partitioning the History into Sessions There can be two basic approaches for mining the client request history depending on how the history of user requests is partitioned 1 the 003at approach and 2 the userbased partitioning approach The 003at approach tries to extract data item request patterns regardless of who requested them The user-based partitioning approach on the other hand divides the client request history into subsets with respect to the user who requested them as shown in Table 1 Table 1 presents the windows of requests corresponding to each user such as user 1 who had k different request windows The analysis is done for each subset of the client request history corresponding to a user independent of the other subsets In order to make use of the existing data mining algorithms we need to construct sessions out of the existing history When we cannot assume any session boundaries we need to use a sliding window approach for simulating the sessions We used a gap-based approach to determine the session boundaries In gap-based approach we assume that a new session starts when the time delay between two consecutive requests is greater than a prespeci\002ed threshold called gap  In both 003at and user-based partitioning approaches we divide the request set into windows and 002nd the association rules using the data mining algorithms mentioned previously These windows correspond to the sessions for the data mining algorithm However the construction of the windows is different for the two approaches In the userbased partitioning approach we can observe the temporal patterns of a user to divide his/her request set into sessions In the gap-based method windows are clusters of item/s requested according to the times of requests if a user has  14 2 35 6 7 8 910 11 12 13 14 20 21 22 23 0 16 18 19 15 17 3 2 3 6 27 25 3 24 25 379 1 1 1 2 5 8 Timestamps Data ids Figure 1 A sample history requested items consecutively and relatively quickly then these items should be put in the same window We de\002ne a window as a group of requests where the time delay between two consecutive requests is less than a certain threshold After requesting a set of items consecutively if the user waits an extended period of time before starting another session then the sequence of items requested in the new session can be put into another window In Table 1 the requests of user 1  for instance are divided into k windows The 002rst window of user 1  denoted by win 1 1  has three requests namely data items x z and y We need to set a threshold value for the time delay in between two consecutive windows We may set this threshold to in\002nity in order to put all items requested by the same user into the same window this might be a reasonable approach if there are multiple users and each user accesses a reasonable amount of items When the number of users is small and each user accesses many documents then we need to set a reasonable threshold value For the 003at approach we determine a 002xed window length and use a sliding window approach to construct the windows The sliding window approach is based on moving the window one item further and consider the items in the current window for 002nding the large item sets In order to be able to use data mining algorithms to obtain association rules windows are mapped to sessions and data item requests are mapped to items  Standard association rule 002nding algorithms are then used to 002nd the association rules in the history The state of the art association rule 002nding algorithm called Apriori is appropriate for our purposes AS94  I n itially  t h e Ap rio r i a lg o r ith m 002 n d s all t h e large data items and computes their supports Large data items are characterized by the fact that the percentage of the windows that supports them is greater than the minimum support value Then using these large items 2-itemsets i.e itemsets of size 2 are found by combining large items by checking their support values against the minimum support Incrementally item sets of size n are constructed by using item sets of size n 000 1 where n 1 until no larger item sets could be found Figure 1 shows a sample history generated for clarifying the proposed methods The numbers above the line are the time stamps and the numbers below the line are requested items If we do not assume any session boundaries then we need to use a sliding window algorithm which results in the following sessions if the window size is set to 3 f 1  2  1 g  f 2  1  3 g  f 1  3  2 g  f 3  2  1 g  f 2  1  3 g  f 1  3  5 g  f 3  5  6 g  3 


f 5  6  2 g  f 6  2  7 g  f 2  7  2 g  f 7  2  5 g  f 2  5  3 g  f 5  3  8 g  f 3  8  3 g  f 8  3  7 g  f 3  7  9 g  If the gap-based methods will be used and the gap is set to 2 units then we have 8 sessions which are f 1  2 g  f 1  3  2 g  f 1  3 g  f 5  6 g  f 2  7 g  f 2  5 g  f 3  8 g  f 3  7  9 g  If a length-based approach is going to be used and length is set to 3 then we have f 1  2  1 g  f 3  2  1 g  f 3  5  6 g  f 2  7  2 g  f 5  3  8 g  f 3  7  9 g  Cluster-based approach depends on the clustering methods we use and will not be discussed in this paper Cluster-based methods generate session patterns similar to the sessions generated by the gap-based approaches 3 Utilization of the Association Rules for Hoarding The association rules obtained after mining the request history are used for determining the candidate set and the hoard set of the client upon disconnection The candidate set is de\002ned as the set of all candidates for hoarding for a speci\002c client Hoard set is the set of data items actually loaded to the client prior to disconnection A candidate set is constructed using inferencing on association rules as explained in Section 3.1 Some other heuristics are used to prune the candidate set to the hoard set so that it 002ts to the cache of the mobile client as explained in Section 3.2 The process of automated hoarding via association rules can be summarized as follows 1\equests of the client in the current session are used through an inferencing mechanism to construct the candidate set prior to disconnection 2 Candidate set is pruned to form the hoard set 3 Hoard set is loaded to the client The need to have separate steps for constructing the candidate set and the hoard set arises from the fact that users also move from one machine to another that may have lower resources The construction of the hoard set must adapt to such potential changes Details about how the hoarding process constructs candidate and hoard sets are provided in Sections 3.1 and 3.2 below 3.1 Construction of the Candidate Set An inferencing mechanism is used to construct the candidate set of data items that are of interest to the client to be disconnected The candidate set of the client is constructed in two steps 1 The inferencing mechanism 002nds the association rules whose heads i.e left hand side match with the client's requests in the current session 2 The tails i.e right hand side of the matching rules are collected into the candidate set The inferencing mechanism examines the current requests and predicts future ones The rules obtained from the history shown in Figure 1 as a result of gap-based partitioning approach are 1  2 and 1  3 with a minimum support of 20 and minimum con\002dence of 80 If we have the list 6  7  1  5 belonging to a current client session then the data items 2 and 3 should be placed into the candidate set as a result of the inference mechanism because data item 1 was requested and our rules tell us with a certain con\002dence level that if a user requests data item 1 then he/she will also request data items 2 and 3 in the near future Priorities need to be assigned for the items obtained as a result of the inferencing Our priority metric is based on the rule con\002dence and support values i.e items inferred by a rule with a high con\002dence or support To include both the support and con\002dence value the priority of a rule is set to be the multiplication of the con\002dence and support value for that rule 3.2 Construction of the Hoard Set The client that issued the hoard request has limited resources The storage resource is of particular importance for hoarding since we have a limited space to load the candidate set Therefore the candidate set obtained in the 002rst phase of the hoarding set should shrink to the hoard set so that it 002ts the client cache Each data item in the candidate set is associated with a priority These priorities together with various heuristics must be incorporated for determining the hoard set The data items are used to sort the rules in descending order of priorities The hoard set is constructed out of the data items with the highest priority in the candidate set just enough to 002ll the cache For an effective hoarding the cache misses during disconnection should be recorded and re\003ected to the history upon reconnection In this manner we can capture the whole picture of client requests both connected and disconnected 4 Simulation Model and Experimental Results We implemented the data mining algorithms and the rule based hoarding mechanism to show the effectiveness of the proposed methods The data set generator of the IBM Quest project was used for simulating the history of sessions AS94  T he s i mul a t i o n m odel w e u s e d a nd t h e e xperimental results we obtained are provided in Section 4.1 and Section 4.2 respectively 4.1 Simulation Model In our simulation model we make use of the history of sessions for both extracting rules and testing the effective4 


  Data Mining Program Hoard Request Hoard Set Server Rule Base Inference Engine set Candidate Incomplete Session HISTORY DATA Time 0 Request Generator Session Generator Client Cache Figure 2 Architecture of the Simulation ness of the rule based hoarding method The architecture of our simulation model is shown in Figure 2 We assume that the whole set of past requests are stored in a history Our experiments consist of two stages 1 Rule extraction  where the the history is mined for extracting association rules 2 Rule performance evaluation wherethe history is used to test the effectiveness of the extracted rules First the sessions are generated then the rule extraction mechanism performs the task of extracting the association rules from the collection of sessions Rule sets with different minimum con\002dence and support requirements can be constructed by the rule extraction program The resulting association rules are written to a 002le in a speci\002c format to be read later by the simulation program The simulation program was written in C The second stage of our experiments is performed as follows First of all the client requests are divided into two groups 1 Requests issued while disconnected 2 Requests issued while connected A request generator program takes the data generated by the IBM benchmark and converts it to a stream of client requests consisting of sessions The beginning and end of the sessions are marked and disconnect requests are inserted Hoard requests are also inserted as a client request that marks disconnection times Hoard requests are issued relative to the beginning of the sessions and have a normal distribution with mean av er ag e session l eng th 2  The main subsystems of the broadcast simulation program are the server client cache and rulebase  Server has a rule base used for constructing the hoard set The client interacts with the server by sending both data and hoard requests to it Rule base loads the association rules from a 002le generated by the data mining program The client has a cache to store a limited amount of requested items 4.2 Experimental Results We used the IBM Almaden Quest data generator to simulate the history of client requests AS94 The d at a g ener ated by this benchmark is suitable for our purposes because we could 002netune the generated data Basically the benchmark produces a collection of sets of items The sets of items correspond to the sessions in the context of hoarding We implemented the Apriori algorithm for mining association rules We extended the mining algorithm with an inverted list so that the implementation could be performed with set operations like union and intersection Data mining is performed in main memory The running time of the data mining algorithm does not exceed a few seconds for a thousand sessions Considering that the mining process is not done frequently these running times are not signi\002cant The client request log is partitioned into disconnected and connected periods The length of these periods is calculated in terms of number of requests The client part of the system loads the requests into a queue together with the disconnection period and frequency information When the disconnection period is reached the hoard set is sent to the client and the cache hit ratio is measured for the disconnection period Overall the disconnected cache hit ratio is calculated by averaging the individual disconnected cache hit ratios The resulting hit ratios can be compared to that of the LRU approaches We have performed various experiments to see the effect of different parameters on the performance Local storage done by the mobile client is referred to as its cache Our main performance criterion is the cache hit ratio since the main purpose of hoarding process is to decrease the number of cache misses during disconnection The number of sessions does not have any impact on the performance the value selected for this parameter in our experiments is 200 The number of different data items generated is 150 We assume 50 different patterns in the data set and the average pattern length is taken to be 10 items Cache size and its impact on the performance is an important parameter that we need to evaluate We measure the cache size in terms of the number of data items that the cache can hold since we did not simulate the size of data items Considering that the data items are most likely html documents or multimedia applications data items would consist of one or more pages Therefore the locality in the number of pages would not affect the system performance As a result the cache size in terms of the number of data items does not deviate very much from real life where cache size is measured in terms of kilo bytes We targeted small size devices like palmtops which have a limited memory and potentially short sessions of connec5 


 10 20 30 40 50 60 70 80 90 100 Cache Size 0 10 20 30 40 50 60 70 80 90 100 Hit Ratio NUM_ITEMS = 150, NUM_SESSIONS = 200     LRU LRU with Rules Figure 3 Cache Hit ratio as a function of the cache size tion A cache size of 50 or at most 100 items or objects used in our experiments seems reasonable considering the amount of space multimedia or web items consume Since we are assuming thin clients with a small cache size we varied the cache size from 10 to 100 items The results we obtained for the cache hit ratio under different cache sizes are displayed in Figure 3 As we observe from the 002gure in general rule-based hoarding methods are more effective for small cache sizes As the cache size gets closer to the number of distinct data items the cache hit ratio gets closer to the LRU cache hit ratio Since the mobile devices are characterized by a limited cache size we could state that our methods are effective for mobile computers The cache size is measured in terms of the number of items assuming that the items retrieved are html documents possibly with images A cache size of 50 seems to be a reasonable value Rule con\002dence also affects the cache hit ratio The performance results obtained by varying the rule con\002dence are depicted in Figure 4 As can be seen from the 002gure very high minimum con\002dence values cause the cache hit ratio to decrease This is due to the fact that a higher minimum con\002dence requirement means that we will have a lower number of rules which decreases the number of predicted items In the extreme case of a con\002dence of 100 the number of rules we can have is the minimum and the cache hit ratio is close to that of the LRU without rule extension With lower con\002dence values we have a larger number of rules However since we are using the rule con\002dence and support values for limiting the number of inferred items the cache hit ratio does not change much for the minimum con0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0 Confidence 0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0 100.0 Cache Hit Ratio CACHE_SIZE = 50, SUPPORT = 20 Figure 4 Cache Hit ratio as a function of the minimum con\002dence 002dence value in the range of 0 to 70 This is due to the fact that we are already eliminating the low con\002dence rules by the priority mechanism The number of different patterns in the data set also has an effect on the cache hit ratio as shown in Figure 5 LRU with rules consistently has a higher hit ratio as compared to LRU for all different numbers of patterns However as the number of patterns increases both LRU and LRU with rule extension inhibit a decreased cache hit ratio and the difference between the hit ratios also decreases This is due to the fact that rules better represent a dataset with a lower number of patterns and we would extract more rules with a smaller number of patterns for the same minimum con\002dence and support Overhead of mining and inferencing Rule extraction should be performed periodically to re\003ect changes in request patterns However this would lead to a processing overhead on the server in addition to the inferencing it causes We can determine a threshold value for the waiting period before restarting the history mining and development of new association rules For huge histories mining the whole history periodically is a waste of resources therefore some incremental methods can be applied to reduce the time spent for remining Ef\002cient methods for incremental rule maintenance are proposed in CHNW96 Some timing measurements are presented in Figure 6 and Figure 7 Figure 6 shows the time required for inferencing as a function of the number of rules This timing measurement is performed over 2000 sessions and CPU time gives 6 


 30 40 50 60 70 80 90 100 Number of Different Patterns 30 40 50 60 70 80 Hit Ratio NUM_ITEMS = 150, CACHE_SIZE = 50   LRU with Rules   LRU Figure 5 Cache Hit ratio as a function of the number of different patterns 0 40 80 120 160 200 240 280 320 360 400 Size of the Rulebase 0 40 80 120 160 200 240 280 320 360 400 CPU time \(msecs Figure 6 CPU time for inferencing as a func\255 tion of the number of rules in the rulebase 0 100 200 300 400 500 600 700 800 900 1000 Number of sessions 0 1 2 3 4 5 6 CPU time \(secs Support = 20, Confidence = 40, Num Items = 150 Figure 7 CPU time for rule extraction as a function of the number of session the time required to perform the inferencing 2000 times For the time being we search the whole rulebase to 002nd out the matching rules Matching is performed by a subset operation and can be performed in constant time assuming that the rule and session sizes are bounded More ef\002cient rule searching mechanisms could be employed to improve the performance since the inferencing time is directly proportional to the size of the rulebase The processing overhead of rule extraction in terms of the CPU time required to mine for rules from histories of different sizes is presented as a function of the number of sessions in Figure 7 Inferencing with rules is performed at the server therefore mobile clients are not affected by the inferencing overhead except for the server delay With ef\002cient incremental mining and inferencing techniques the server load and thus client waiting times for hoarding can be decreased 5 Conclusions and Future Work In this paper we proposed a new method for automated hoarding to support seamless disconnected operation Our method basically utilizes the association rules via an inferencing mechanism to decide what data items to hoard As an initial step a method for constructing sessions out of the client request history to feed into the data mining algorithm as transactions was described This allowed us to use data mining algorithms to analyze the history of previous client requests We described how the hoarding process could be performed via association rules together with an inferencing mechanism to determine the candidate set of data items 7 


that may be of interest to the user in the future A priority assignment method was proposed which exploits the con\002dence and support values of the association rules The priorities were used to prune the candidate set of data items in order to develop a smaller set that will 002t the limited storage of mobile clients Experiments were also conducted to gauge the effectiveness of the proposed methods in terms of the client cache hit ratio We observed that the use of association rules considerably improves the hit ratio of the client cache especially for small cache size The results are promising at this stage and can be improved further with some extensions We also performed experiments to measure the processing overhead of rule extraction and inferencing These results support our claim that neither infrequent mining nor inferencing introduces a considerable overhead to the system We believe that the performance could be improved even further with a more ef\002cient incremental mining and inferencing process One of the more appealing aspects of this work is that we bring old problems of production rules into the arena with a new method of rule gathering and application We proposed the usage of the data mining concepts for different applications such as obtaining production rules Temporality in association rules is an important aspect which captures the temporal behavior of associations We are planning to investigate how temporal association rules could be used for automated hoarding Listed below are some more heuristics for limiting the hoard set that we are planning to test 017 Considering the size of data items for determining what to hoard Hoarding 10 small items inferred by a lower priority rule might be better than hoarding a huge 002le that may consume half of the client cache inferred by higher priority rules 017 Considering the time to load a data item on the cache In case the hoarding for weak connection we can hoard big data items and let smaller ones be loaded by the client during the weak connection time The 223size\224 and 223time to load\224 heuristics seem to contradict each other However for some items transmission time might be high because of their location in the network 017 Considering if available expiration times of data items for the hoarding process We may not want to hoard data that will expire in the near future User request patterns may change over time For that reason the history should be analyzed to eliminate those patterns that no longer re\003ect more recent user request patterns The frequency of mining the history depends on the particular system in concern Ef\002cient incremental mining algorithms are proposed in CHNW96 D ynami c n at ure o f the client request patterns will be considered in the future by using incremental association rule mining algorithms References AAB  96 R Ag ra w a l A Arn i n g  T  Bo llin g e r  M Mehta J Shafer and R Srikant The quest data mining system In Proc of the 2nd Int'l Conference on Knowledge Discovery in Databases and Data Mining  Portland Oregon August 1996 AS94 R Ag ra w a l a n d R Srik an t F a st alg o r ith ms fo r mining association rules In Proc of the 20th Int'l Conference on Very Large Databases  Santiago Chile September 1994 CHNW D a vi d W ai Lok C h eung J i a w ei H a n V i ncent Ng and C Y Wong Maintenance of discovered association rules in large databases An incremental updating technique In Proceedings of the 12th International Conference on Data Engineering ICDE  pages 106\226114 1996 JHE99 J Jin g  A Helal an d A  K  E lmag armid  Client server computing in mobile environments ACM Computing Surverys  June 1999 KP G  K u enni ng and G  P opek A u t o mat e d hoarding for mobile computers In Proceedings of the ACM Symposium on Operating Systems Principles  St Malo France 1997 KS J a mes J  K i s t l e r a nd Mahade v S at yanarayanan Disconnected operation in the coda 002le system ACM Transactions on Computer Systems  10\(1\3\22625 1992 MJHS B a ms had M obas h er  N ami t J a i n  E ui H ong Han and Jaideep Srivastana Web mining Pattern discovery from world wide web transactions Technical Report 96-050 Department of Computer Science University of Minnesota September 1996 PS Ev aggel i a P i t oura a nd G e or ge S a maras  Data Management for Mobile Computing  Kluwer Academic Publishers 1998 SU99 Y u cel Say g i n a n d Ozg u r Ulu so y  Ex p l o itin g data mining techniques for broadcasting data in mobile computing environments Submitted for Journal Publication  1999 TLAC C a rl T a i t  H u i L ei  S w a rup A charya a nd Henry Chang Intelligent 002le hoarding for mobile computers In Proceedings of Mobicom'95  Berkeley CA November 1995 8 


Figure 8 Visual interface for Moridou system Search EngineTest Page 0 UI 0 5 5 Keyword plealet Figure 9 Prototype system in hcterogeneous environment 283 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


