Abstract 
li-feng,ooibc 
  
R-Store A Scalable Distributed System for Supporting Real-time Analytics Feng Li 1  M Tamer 250 Ozsu 2  Gang Chen 3  and Beng Chin Ooi 1 1 
204It is widely recognized that OLTP and OLAP queries have different data access patterns processing needs 
comp.nus.edu.sg School of Computing National University of Singapore Singapore 2 tamer.ozsu@uwaterloo.ca David R Cheriton School of Computer Science University of Waterloo Canada 3 cg@zju.edu.cn College of Computer Science Zhejiang University P.R China 
and requirements Hence the OLTP queries and OLAP queries are typically handled by two different systems and the data are periodically extracted from the OLTP system transformed and loaded into the OLAP system for data analysis With the awareness of the ability of big data in providing enterprises useful insights from vast amounts of data effective and timely decisions derived from real-time analytics are important It is therefore desirable to provide real-time OLAP querying support where OLAP queries read the latest data while OLTP queries create the new versions In this paper we propose R-Store a scalable distributed system for supporting real-time OLAP by extending the MapReduce framework We extend an open source distributed key/value 
system HBase as the underlying storage system that stores data cube and real-time data When real-time data are updated they are streamed to a streaming MapReduce namely Hstreaming for updating the cube on incremental basis Based on the metadata stored in the storage system either the data cube or OLTP database or both are used by the MapReduce jobs for OLAP queries We propose techniques to ef\223ciently scan the real-time data in the storage system and design an adaptive algorithm to process the real-time query based on our proposed cost model The main objectives are to ensure the freshness of answers and low processing latency The experiments conducted on the TPCH data set demonstrate the effectiveness and ef\223ciency of our approach I I 
NTRODUCTION Database systems implemented for large scale data processing are typically classi\223ed into two categories OLTP systems and OLAP systems The data stored in OLTP systems are periodically exported to OLAP systems through ExtractTransform-Load ETL tools In recent years MapReduce framework has been widely used in implementing large scale OLAP systems because of its scalability and these include Hive Pig 23 Most of these only focus on optimizing OLAP queries and are oblivious to updates made to the OLTP data since the last loading However with the increasing need to support real-time analytics the issue of freshness of the OLAP results has to be addressed for the simple fact that more up-to-date analytical results would be more useful for 
time-critical decision making The idea of supporting real-time OLAP RTOLAP has been investigated in traditional database systems The most straightforward approach is to perform near real-time ETL by shortening the refresh interval of data stored in OLAP systems Although such an approach is easy to implement it cannot produce fully real-time results and the refresh frequency affects system performance as a whole Fully real-time OLAP entails executing queries directly on the data stored in the OLTP system instead of the 223les periodically loaded from the OLTP system To eliminate data loading time OLAP and OLTP queries should be processed by one integrated system instead of two separate systems However OLAP queries can run for hours or even days while OLTP queries take only microseconds to seconds Due to resource 
contention an OLTP query may be blocked by an OLAP query resulting in a large query response time On the other hand since complex and long running OLAP queries may access the same data set multiple times and updates by OLTP queries are allowed as a way to avoid long blocking the result generated by the OLAP query would be incorrect the well-known dirty data problem In this work we try to address the problem of large scale RTOLAP processing using MapReduce The RTOLAP is de\223ned as follows a real-time OLAP RTOLAP query accesses for each key the latest value preceding the submission time of the query Speci\223cally  we propose and design a scalable distributed RTOLAP system called R-Store in which the storage system supports multi-versioning and each version 
is associated with a timestamp Each OLAP query operates on the version of data that exists at the time it is submitted whereas each OLTP transaction creates a new version RStore uses the MapReduce framework where the mappers of the OLAP query directly access the real-time data stored in the storage system The storage system is implemented by extending HBase HBase supports the 
HBaseScan 
operation that takes a timestamp as input and returns the version of the data with the largest timestamp before the scan operation Though this can be used to offer consistent data to OLAP queries simply using this default scan operation for querying the data stored in HBase is inef\223cient due to the following reasons 1 HBase only stores a 223xed number of versions for 
978-1-4799-2555-1/14/$31.00 
002 
each key and automatically removes the versions that exceed this cap by its default compaction policy To support real-time querying this number has to be set to in\223nity in case the old versions are removed during the running of an OLAP query However this will lead to continuously increasing of the data size and waste too much space to store the unused data 2 For each OLAP query the entire HBase table has to be scanned and shuf\224ed to the mappers which is a very costly process 40 
2014 IEEE 
ICDE Conference 2014 


A Real-Time Data Warehousing B Distributed Processing C Data Cube Maintenance To facilitate ef\223cient processing of RTOLAP queries we periodically materialize the real-time data into a data cube and implement an operation in HBase to avoid the shuf\224ing of the entire HBase table to MapReduce during real-time querying To the best of our knowledge this is the 223rst work that proposes a scalable RTOLAP distributed system based on MapReduce framework In summary the contributions of this paper are as follows 1 We propose a scalable distributed system framework called R-Store for performing RTOLAP R-Store evaluates an OLAP query by transforming it into a MapReduce job which is run on our modi\223ed HBase in the remaining of this paper we name it as HBaseR in order to differentiate it from HBase to obtain the real-time data 2 We propose an ef\223cient storage model for caching the data cube result The data cube is treated as historical data while the data updated after the refresh time of the data cube are real-time data We also propose a more ef\223cient scan operation in the storage model for obtaining the real-time data 3 We integrate streaming MapReduce into our system which maintains a real-time data cube in the reducers and periodically materializes the data cube This data cube update method is much faster than the data cube re-computation method and in turn accelerates the processing of RTOLAP since fewer real-time data are scanned during the query execution 4 We design an algorithm to ef\223ciently process the RTOLAP queries which takes both the historical data cube and the real-time table as input We also propose a cost model that guides the adaptive processing of RTOLAP 5 We perform an extensive experimental study on a cluster with more than one hundred nodes which con\223rms the effectiveness of the cost model and the ef\223ciency and scalability of R-Store The remaining of the paper is organized as follows In Section II we review some related research We subsequently present the architecture design and implementations of RStore in Section III and IV In Section V we discuss the processing of real-time OLAP We evaluate the performance of R-Store in Section VI and conclude the paper in Section VII II R ELATED W ORK Our proposal touches on a number of areas such as OLAP processing distributed processing and data cube maintenance We review related work that are most relevant to ours The growing demand for fast business analysis coupled with increasing use of stream data have generated great interest in real-time data warehousing Some ha v e proposed near real-time ETL as a means to shorten the data w arehouse refresh intervals These works require fewer modi\223cations to the existing systems but they cannot achieve 100 real-time Other studies proposed online updates in data warehouses by using differential techniques 25 or multi-v ersion concurrency control In C-store 25 tw o separate stores are used to handle in-place updates The updates are stored in a write-store WS while queries run against the readstore RS and merged with the WS during execution In existing studies the incoming updates are usually cached to improve the performance The cached data are then 224ushed to disk once the size exceeds the upper bound The performance of these studies are limited by the size of the memory and MaSM o v ercomes these limitations by utilizing SSDs to cache incoming updates MapReduce is a parallel data processing framework for large scale data processing Its programming model consists of two user-de\223ned functions and  that operate on key/value pairs A survey on database management using MapReduce can be found in and a detailed performance study for MapReduce has been proposed in In this paper  we augment MapReduce processing platform with an extended HBase for data cube storage and maintenance There have been some researches on supporting both OLTP and OLAP in one hybrid system Previously we have proposed epiC 13 an elastic po wer a w are data-itensi v e cxloud platform for supporting both OLAP and OLTP As part of the epiC work in this paper we investigate how to ef\223ciently process the RTOLAP queries in such a hybrid system The concepts proposed in this paper can also be applied to epiC though they\220re implemented using MapReduce In addition there are also some main memory database systems HyPer HYRISE 10 that tries to address both OL TP and OLAP The focus of our work is different from these systems our work tries to address the problem of ef\223ciently processing the RTOLAP queries in a large scale environment where the data are stored on disk While MapReduce provides an ef\223cient and simple platform for scalable distributed processing it is not ef\223cient for supporting online and continuous stream processing HStreaming and MapReduce Online 7 are e xtensions made to the MapReduce framework that support stream processing as follows 1 the input of the mappers could be stream data 2 the data are streamed from mappers to reducers and 3 the output of one MapReduce job can be streamed to the next job S4 is another distrib uted stream system that adopts the actor model as its computation model In these works the new data are usually appended to the system to form the data stream and the previously inserted data are not changed In contrast in our work we support the OLTP operations such as insert update and delete and these changes in turn have an effect on the result of the real-time OLAP queries Data cube maintenance has been studied for a long time The earliest works focused on ef\223cient incremental view maintenance for data warehouses 11 Ho we v er  as the number of dimension attributes increases the cost of incrementally updating data cube increases signi\223cantly To improve the performance of data cube maintenance instead of generating the delta value for all the cuboids during the update process an method of refreshing multiple cuboids by the delta value of a 41 IncrementalScan map reduce 


FullScan  IncrementalScan   A R-Store Architecture MetaStore MetaStore MetaStore MetaStore B Storage Design 1 Full and Incremental Scans region i i i 2 T T T T T T T T T T T T MapReduce  MetaStore      Refresh Cube Incremental Scan FullScan  Compaction  T Fig 1 Architecture of R-Store single cuboid has been proposed Most of these algorithms were designed for a single node con\223guration and are not scalable to a distributed environment However MapReduce has been used to construct data cube in a large scale distributed environment The MR-Cube algorithm 21 w as proposed to ef\223ciently compute the data cube for holistic measures In these works the data cube is usually used for processing OLAP queries without the real-time requirement while our system considers both the data cube and the real-time data to process RTOLAP queries III R-S TORE A RCHITECTURE AND D ESIGN In this section we present the architecture of R-Store the design philosophy of the storage system and how the data cube is maintained  42 Key/Value Store  Distributed Streaming System     1 2 1 2 1 1 2 1 1 OLAP  Real-Time Data  FullScan IncrementalScan FullScan IncrementalScan FullScan FullScan Figure III-A illustrates the architecture of R-Store The system consists of four components a distributed key/value store a streaming system for maintaining the real-time data cube a MapReduce system for processing large scale OLAP queries and a MetaStore for storing some global variables and con\223gurations The OLTP queries are submitted directly to the key/value store while the OLAP queries are processed by the MapReduce system The simplest method of supporting RTOLAP for MapReduce is to scan the whole real-time table and obtain the latest version before the submission time of the OLAP query for every key/value pair  operation as the input of the MapReduce job The key/value store has to support multi-version concurrency control in case the OLTP queries and OLAP queries are blocked by each other However this method is not ef\223cient because obtaining one version for each key/value pair is a costly operation in large scale distributed systems Note that in real applications such as social networks the updates usually follow a Zipf distribution and within a time interval only a small portion of keys are updated in the table Based on this observation we try to accelerate OLAP queries by materializing the real-time table into a data cube When an OLAP query is submitted to the system it 223rst connects to to acquire the timestamp of the query for consistency The statistics stored in are also used to optimize the query based on our proposed cost model Section V-C After the optimization by the cost model the OLAP query can be transformed to a MapReduce job that takes as input both the historical data in the data cube and the real-time data in the key/value store To ef\223ciently access real-time data the key/value store is designed to support incremental scan Section III-B1 The real-time data is scanned by the operation while the data cube is scanned by the operation The operation only shuf\224es the key/value pairs that are updated after the last building of the data cube and thus is much faster than because fewer data are shuf\224ed The data cube is also stored in the distributed key/value store and is periodically refreshed based on the real-time table The versions of the key/value pairs before the refresh time of the data cube are compacted in order to accelerate the scan time of the real-time table The performance of refreshing the data cube is crucial to our system because if the data cube is refreshed fast more data are compacted by our compaction scheme and fewer real-time data are accessed during the scan operation In an extreme case where no update is submitted since the data cube refresh the MapReduce job only needs to scan the data cube To ef\223ciently refresh the data cube the updates applied to the key/value store are streamed to the streaming system and a real-time data cube is maintained in the local storage of the streaming system The real-time data cube is periodically materialized to the key/value store to refresh the data cube Based on our experimental results this method is much faster than the method of re-computing the data cube and the throughput of this method is suf\223ciently high to process the update streams from the key/value store Once this refresh process is completed the timestamp of the latest data cube is sent to  and the compaction process is invoked to compact the real-time data The also stores other global information including the submission time of each OLAP query the frequency of materializing the data cube etc The key/value store must support multi-version concurrency control techniques to ensure that the OLAP query and the OLTP query do not block each other In addition our storage design considers many other features including ef\223cient 223le scan operations compaction scheme and load balancing which we discuss below To handle OLAP queries and to build the data cube a scan operation needs to be implemented in the key/value store Two types of scan operations are required which are used in different scenarios  For each of the table in the key/value store the operation takes a timestamp as input and returns the latest version of the value before for all the keys The data returned by this operation can be used to create or re-compute the data cube  This operation takes two timestamps and   as input and returns two versions for the keys updated after  The 223rst version is the latest value before  and the second version is the latest value before  If a new key is inserted not updated into the store after  only one version is returned This operation can be used in the RTOLAP query processing algorithm DataCube  OLTP  Real-Time DataCube 


For a A Global Compaction Local Compaction Re-Computation Incremental Update DC DC region 3 Load Balancing C Data Cube Maintenance region region regions region regions store memstore store\223les store memstore memstore memstore store\223le store\223les memstore store\223le 1 IncrementalScan region store\223les memstore i i FullScan FullScan FullScan IncrementalScan IncrementalScan IncrementalScan IncrementalScan Since each key may have several versions the scan operations read more than one version of the data to obtain the required versions incurring unnecessary I/O cost To reduce the number of stored versions for each key and to improve the scan performance the data are automatically compacted We provide two forms of compaction  The global compaction process is launched immediately following each data cube refresh For the same key all the versions inserted before the data cube refresh are merged into one version We call this version  The local compaction process is invoked on each node At 223rst the submission time of the current running scan process  is acquired by the compaction process For each key the latest version of the data before In most applications some key ranges might be updated more frequently than others causing skewed load on the nodes In addition since the update operation inserts a new version of the key into the node instead of replacing the old version there is a skew on the amount of data on the nodes This affects the performance of the scan process on those nodes The solution is to split heavily updated ranges in the key/value store and move some data to other nodes To improve the performance of OLAP queries a data cube is maintained in the key/value store A data cube could be either a full data cube an Iceberg cube or a closed cube The selection of the best suitable data cube depends on the applications which is not the focus of this work To make it general we only consider the full data cube which consists of a lattice of cuboids There are two approaches to refresh the data cube  To re-compute the data cube the operation is used It takes a timestamp as input and returns the latest version of the value before for all the keys stored in this  Each mapper of the MapReduce job takes the results of the operation on one as input For each cuboid a key/value pair is generated The map output key is the combination of the dimension attributes for the cuboid while the map output value is the numeric value The reducers compute the aggregation value for each cell of each cuboid and output the result to the key/value store  The second approach is performed in two steps propagation step and update step The propagation step computes change of the table and the update step updates data cube based on HBase is an open source distrib uted k e y/v alue store A table stored in HBase is partitioned to several  which are assigned to a certain nodes and each node runs a server to manage and serve the transactions Inside a region the data of the same column family a group of columns are stored in the same structure which is called has an in-memory structure  and several in-disk 223les  When a new version of data is about to be inserted into this  it is 223rst inserted into the and appended to the write ahead logs Once the size of the reaches its upper bound the data in the are transferred to a  The are sorted in inverse chronological order Inside the or  the data are sorted by keys and the versions for each key are sorted in inverse chronological order HBase only supports the operation so we designed and implemented in HBase-R in a  by accessing the same key across the and in parallel the operation scans the keys in ascending order For each key the version with the larger timestamp is scanned earlier For all the versions of a key the algorithm checks the timestamp of each version and returns the required two versions If the key has only one version which means the operation on the key is an insertion the only returns that version for the key For real-time queries and data cube update scanning the key/value pairs in HBase-R is the most costly step It is therefore important to improve the performance of  For this purpose we propose an adaptive incremental scan algorithm First we maintain an in-memory structure to estimate  the number of distinct keys updated since the last refresh of the data cube Estimating in a data stream has been DC DC 43  which is consistent with the data cube and is used in updating the data cube  on the is accessed by this scan process Thus the local compaction only compacts the older versions that will not be accessed by any scan process Furthermore is not changed during this compaction so that the new data cube can be computed correctly when the data cube is refreshed change of the data cube from  However not all data cubes can be incrementally updated The incremental update only works for selfmaintainable aggregate functions the ne w cell v alue can be computed from the old cell value and the updated tuples such as SUM COUNT and the algebraic functions derived from them In R-Store the re-computation approach is used to build the 223rst data cube while the incremental update approach is adopted to maintain a real-time data cube in the stream processing module The streaming system updates its data cube with the update streams coming from the key/value store and periodically materializes the data cube into the storage system As the updating of the data cube consists of two phases which can be processed by the MapReduce processing logic in natural a streaming version of MapReduce is used as the stream processing module of R-Store IV R-S TORE I MPLEMENTATIONS In this section we present the implementations of RStore Speci\223cally we show how we implement our storage system namely HBase-R on top of HBase to ful\223ll the design philosophy discussed in Section III-B 2 Global and Local Compactions A Implementations of HBase-R store store     V T T V T T T d T d T 002 002 002 scan scan 


1 2 for 3 if 4 continue 5 else  6 7 8 if 9 for 10 11 12 for 13 if 14 15 else  16 17 327 327 Q Q Q Q Q r 1 2 M h M M T T T T T T T T T T d T d T  T T   T T T w key IncrementalScan IncrementalScan IncrementalScan CostOfRandom In contrast when 212 003 004 003 003 is con\223gured before the data are inserted Thus the number of keys for a has an upper bound   which can be estimated by usually stores a range of consecutive keys a hash function can be used to map a key to a value between 0 and  and a bit array of size   is maintained in memory to indicate whether or not a key has been updated Using this bit array to compute the number of updated values on a node with even one billion distinct keys only 128 MB of memory are required To improve the performance of  the above data structure is used in the adaptive incremental scan algorithm Algorithm 1 When an request is sent to a server the 223rst parameter   is always set to the refresh time of the current data cube   equals to the submission time of the query   Instead of scanning all the key/value pairs before  the key/value pairs in are scanned 223rst Note that in  there might be several versions for a key and only the newest version is cached in line 1 The number of key/values updated after is then computed line 7 and the random read cost of these key/values is estimated If this cost is smaller than the cost of scanning all the data between  the index is used to directly read the values for these keys lines 8 to 14 In this way the latest versions for the updated keys are obtained Then by simply scanning the key/values before in-memory structure is much lower than the cost of scanning  when is large the adaptive incremental scan is almost the same as the default HBase\220s default compaction process combines all the into one 223le and retains only one version for each key The global compaction in HBase-R is similar to HBase\220s default but with a different triggering condition local compaction only compacts the versions earlier than a certain timestamp To ensure that the compaction process does not block the scan processes the compacted data are stored in different 223les instead of directly replacing the un-compacted data The 223les that contain the old versions are replaced by the compacted 223les when they are not accessed by any scan process Since the compaction process competes with OLAP queries for CPU and I/O resources there is a trade-off between the frequency of the compaction and the performance of the whole system We de\223ne a threshold so that the local compaction process is triggered when HBase has its default size which is 256MB If the size of the data for a is larger than this size it is automatically split to two sub which are distributed to other nodes In HBase\220s default setting only a  Adaptive IncrementalScan   Timestamp  DistinctKeys int NumDistinctKeys kvMap new HashMap Key Value  MemStore kvMap.contain\(kv.key  kvMap.put\(kv.key kv.value   NumKeysNotInMemory NumDistinctKeys kvMap.size kv randomRead\(key kvMap.put\(kv.key kv.value  kvMap.exist\(kv.key send kvMap\(kv.key and kv   delete kvMap invoke the default     223xed number of versions for a key are stored Once the number of versions for all the keys in this reaches the maximum number the size of the  This requires users to manually split the hot  In contrast in R-Store we do not strictly remove the old versions of the updated keys once the number of versions exceeds HBase\220s default setting We wait until the size of frequently updated reaches its upper bound and the split happens automatically R-Store adopts HStreaming for maintaining the real-time data cube note that other streaming MapReduce systems can also be used in R-Store Each mapper of HStreaming is responsible for processing the updates within a range of keys The map function of the data cube update algorithm is shown in Algorithm 2 When an update for a key arrives the old value for this key is retrieved from the local storage if exists To ef\223ciently retrieve the old value a clustered index is built for the key/values and the frequently updated keys are cached in memory In reality the updates are usually on a small range of keys and the old value of the updates have a high probability to be directly retrieved from the cache If the key is new thus does not exist in local storage for each cuboid one key/value pair is generated and shuf\224ed to the reducers The map output key is the combination of the dimension attributes and the map output value is the numeric value If the key of the update exists in local storage and the updated key/value pair falls into the same cell for a cuboid one key/value pair is shuf\224ed to the reducer and the numerical value is equal to the value change Otherwise two key/value pairs are generated one is the new value with a tag 215+\216 and the other is the old value with a tag 215-\216 The reduce function is invoked at a time interval then  do  region region region DistinctKeys region memstore memstore kvMap memstore store\223le memstore store\223le IncrementalScan 2 Compaction store\223les 3 Load Balancing region region regions region region region region region B Real-Time Data Cube Maintenance     1       DC DC DC DC DC DC DC DC exceeds this threshold SizeOf Region/SizeOf KeyV alue numberOf T uples numberOf DistinctKeys N umKeysN otInM emory  CostOfScan N umOf U pdatedKeyV alues 44 Algorithm 1 input do  then  do  then  well studied A straightforw ard method is to k eep all the keys in memory and for each key to maintain a bit value to indicate whether or not it has been updated However this method requires a considerable amount of memory to store the keys In HBase-R the size of a  Since each  and the second parameter  but not in and  the latest versions before for the updated keys are returned to the client Since the cost of scanning  Timestamp KeyValue kv key updated but not in kvMap each kv before  would not change regardless of the frequency of key updates in this is small this adaptive scan strategy incurs fewer I/O operations 


OLAP  is incrementally changed and     MapReduce       Hstreaming  Region Server  Mapper  Mapper  Mapper  Mapper  Reducer            Region for Original Table  Store  Store Region for Data Cube  StoreFile  Region Server Region for Original Table  Store  Store Region for Data Cube StoreFile  Mapper  Mapper  Reducer           002 002 002 002 DC DC DC DC DC DC DC T Map Function for Incremental Update   KeyValue kv oldkv  retrieveFromLocal\(kv.key oldkv  null CuboidK extractCuboidKey\(cuboid kv.value CuboidV extractCuboidValue\(kv.value CuboidV.setTag\(\215+\216 Emit\(CuboidK CuboidV  insertToLocal\(kv oldCuboidV extractCuboidValue\(oldkv.value oldCuboidV.setTag\(\215-\216 newCuboidV extractCuboidValue\(kv.value newValue.setTag\(\215+\216 oldCuboidK extractCuboidKey\(cuboid oldkv.value newCuboidK extractCuboidKey\(cuboid kv.value oldCuboidK  newCuobidK newCuboidV.set\(computeChangeOfCell oldCuboidV,newCuboidV Emit\(newCuboidK newCuboidV Emit\(oldCuboidK oldCuboidV Emit\(newCuboidK newCuboidV   updateToLocal\(kv   speci\223ed by the user For example if the time interval is set to one second the reducers will cache the incoming intermediate data within the past second and apply the reduce function to them Another time interval Reduce Function for Incremental Update   Key key List\241Value\277 vlist Context context i 0 sum 0 v.timestamp MergeWith\(key v MergeWith\(key v     key/value pairs that it receives from mappers which is a cell in a cuboid if these are due to an update before next cube refresh time   When the timestamps of the incoming updates on all mappers are larger or equal to since their timestamps are no less than  In streaming system to deal with fault tolerance the accumulated states of the stream computation have to be checkpointed periodically The data streams after the checkpointing time are stored in logs and will be used during the recovering process In R-Store the data cube materialized to key/value store is indeed a checkpointing of the real-time data cube Since the key/value pairs after the last data cube refresh are still stored in the storage even though some intermediate versions of the key/value pairs might be removed by the local compaction process the necessary versions for building the next data cube are still there the realtime data cube maintenance process can be recovered using the data cube and the real-time table without extra efforts of checkpointing Figure 2 illustrates the data 224ow between HBase-R HStreaming and MapReduce in R-Store Each HBase-R C Data Flow of R-Store region w T T T T Updates  MemStore  MemStore MemStore  MemStore Compact Region Refresh Cube  DC DC DC DC DC 1 2 if 3 for 4 5 6 7 8 9 else  10 11 12 13 14 for 15 16 17 if 18 19 20 else  21 22 23 1 2 for 3 if 4 5 else  6 003 003 003 003 003 003 003 003 002 002 002 002 then  Algorithm 2 input then  do  do  then  Algorithm 3 input do  Reducer  Obtain timestamp and statistics    StoreFile  StoreFile  Reducer  Fig 2 Data Flow of R-Store  cuboid in data cube cuboid in data cube  de\223nes how frequently the data cube is materialized The reduce function to incrementally update the data cube is shown in Algorithm 3 A reducer merges the local data cube   with the intermediate  Value v in vlist   Otherwise it stores these key/value pairs in  the data cube refresh process is invoked which writes the local data cube to HBase-R different cuboids are written to separate HBase-R tables The incoming cells during this refresh process are still written to  When this refresh process is completed is merged with cube Hbase-R  MetaStore              45 


IncrementalScan IncrementalScan T T T T T T T mf gr mf gr  Some of these belong to the real-time table while the others belong to the data cube An OLTP query is submitted to one of the servers and stored in of the it belongs to If the size of the reaches its upper bound the data are written into HDFS as a  Once the update is written to HBase-R it is streamed to a mapper in HStreaming based on the key of this update In the mappers of HStreaming the change of a cell for each cuboid is computed and shuf\224ed to reducers On the reduce side the real-time data cube is updated and cached in local disk At time interval HStreaming materializes its local data cube into HBase-R and noti\223es with the timestamp of the latest data cube The compaction process is then launched to compact the versions of data before data cube is refreshed When an OLAP query arrives it acquires a timestamp from the  together with the statistics of the real-time table stored in HBase-R It is then transformed to a MapReduce job based on the data statistics and submitted to the system Each mapper starts a scan operation over its input belonging to either the real-time table or the data cube At the end of the job the results of OLAP query are stored in HBase-R V R EAL T IME OLAP Section III to IV described in detail the architecture and implementation of R-Store In this section we discuss how the real-time OLAP queries are processed In R-Store if the input of the MapReduce job is only the data cube the performance of the scan phase on the map side is maximized but the result might be stale To maximize the freshness of the OLAP query all the updated key/value pairs before the submission time of the query must be considered Thus not only the data cube but also the real-time table must be scanned Suppose the creation time of the data cube is  For each updated key after running on the realtime table returns both the old version before  if its two parameters are set to respectively By merging these two versions with the numeric values of each cuboid the latest cuboid value can be computed on demand and the freshness of the OLAP query can be satis\223ed In the following subsection we present the query processing algorithm called  making use of the operation We implement so that each MapReduce job can scan the data of multiple tables and the scan operation of each table can be con\223gured as either full scan or incremental scan Using this input format the MapReduce job for can access two types of input tables one is the cuboid table for which a full scan is performed and the other is the real-time table over which the incremental scan is used  Algorithm 4 describes the map function The mappers 223lter the cell and the real-time tuple based on the 223ltering condition The cells and tuples that will be aggregated are assigned the same partition key and shuf\224ed to the same  Map Function for IncreQuerying Algorithm   KeyValueList kvlist Context context null value null kvlist.size  1 extractKe e y key is not 223ltered value  alue value.setTag\(\215Q\216 Emit\(key value  extractKe alue key is not 223ltered value extractV alue value.setTag\(\215 216 context.write\(key value value extractV alue value.setTag\(\215 216 Emit\(key value    reducer The output value for the cell is the selected numeric value while the output value for the real-time tuple is the original value which will be used to re-compute the numeric value The value is attached with a tag 215Q\216 215-\216 or 215+\216 to indicate whether it is the cell value of a cuboid the old value of a key/value pair or the new value respectively This phase is similar to the map phase of incrementally updating the data cube except that a 223ltering process is added and the partition key could be different from the dimension attributes of the data cube  The reduce function calculates the new value of each cell based on the old cell value the change of the cell and the aggregation function The cell key of the reduce function is different from that of Algorithm 3 For example for the TPCH  215Manufacturer#13\216 the key of the reduce function is the combination of the attributes  to  after removing  Figure 3 shows the data 224ow of alogrithm for an OLAP query on a two-dimensional cuboid    The query computes the summation of for each brand produced by 215M1\216 To ensure the freshness of the results all the data of the queried table and the cuboid are scanned to process the real-time query Note that the row key of the stored data cuboid is the combination of the dimension attributes Therefore if the 223ltering condition contains some attributes that could form a pre\223x of the row key such as 215Manufacturer#1\216 and 215Brand#13\216 the range scan function of HBase-R can be used to avoid scanning the entire data cube The min key for the range scan is 215Manufacturer#1,Brand#13\216 and the max key is 215Manufacturer#1,Brand#14\216 Operator  TABLE I D ATA C UBE O PERATIONS   setNumericAttribute  aggregation function name    Map Algorithm 4 input then  then  then  Reduce part 003 003 003 regions regions region memstore region memstore store\223le MetaStore MetaStore region IncreQuerying A Querying Incrementally-Maintained Cube MultiTableInputFormat IncreQuerying brand container IncreQuerying mfgr brand price 46 1 2 if 3 4 if 5 6 7 8 else  9 10 if 11 12 13 14 15 16 Parameters     server handles several and the submission time of the query is  and the latest version before and table to compute a rectangular subset of the cube  attribute name function value    group-by attribute    numeric attribute name   DC DC DC DC  key key key Q Q Q 003 003 003 003 212 addFilter  addGroupBy  setAggregationFunc  


MR  HBase 327                       T 327 f  n              IncrementalScan IncrementalScan FullScan FullScan IncrementalScan FullScan IncrementalScan IncrementalScan FullScan IncrementalScan FullScan 1 2 3 4 5 7 De\223nition     Algorithm 5 input Part  M1,B1,831  101  mfgr,brand  M2,B1  2440  v2  M1,B2,940   Mapper1  Mapper2    M2,B1,690  M2,B2  3513  M1,B2  1945 Filter Condition M1  brand  B1  B2  1945,Q   brand  B1  B2  940 B1  540 Reducer2         B1  B2  1005 Fig 3 Data Flow of IncreQuerying  HBase  HBase  Q Q k Q k Q Q k Q k T T    d T f T T    s T C d C n C Q s Q d Q n Q w c m m B  T  327  To guarantee correctness if the query needs to scan a table several times the scan process on each node always returns the data before time T T T k T T T T T T T s  T  s  T  d  T    T  d  T  C T  S s  Q  327   T  m d  Q  Q  47 number of tuples in table number of distinct keys updated    size of the tuple in table percentage of the keys that are updated since the last data cube refresh    number of cells in the selected cuboid    size of dimension attributes of cuboid    size of numeric attribute of cuboid    number of tuples in the query result    223ltering selectivity of the query    size of query result key    size of query result value    cost ratio of local I/Os    number of mappers for table T    number of mappers for the cuboid    block size   scan  L  T  C  sh sh Example Data Cube Query   DataCube cub cub.addFilter\(\215mfgr\216 215=\216 215Manufacturer#1\216 cub.addFilter\(\215brand\216 215=\216 215Brand#13\216 cub.addGroupBy\(\215type\216 cub.setNumericAttribute\(\215retailprice\216 cub.setOutputTable\(\215resultTable\216 SubmitQuery\(cub  To relieve users from having to merge the real-time data and the historic data cube we de\223ne new data cube operators and automatically translate these operators into a MapReduce job The processing of the real-time data is transparently encapsulated into the operators shown in Table I Algorithm 5 shows an example that computes the summation of the for all the parts with 215Brand#13\216 produced by 215Manufacturer#1\216 grouped by  When an OLAP query is submitted to the system a timestamp is acquired for this query from the  However in a distributed system although clocks can be synchronized to a certain extent there might still be some difference between the clocks of different nodes If the current timestamp on a certain node is smaller than  the next scan process on this node would return some data between and  which leads to an inconsistent state To avoid this inconsistency if the timestamp is larger than  the scan process is blocked for a while until is equal to or smaller than  Since clock synchronization can achieve one millisecond accuracy in local area networks under ideal conditions the delay of the scan process can be ignored compared to the processing time The algorithm discussed above is not always better The scans not only the realtime table but also the data cube incurring a higher cost In addition it shuf\224es two versions for each updated key to MapReduce When there are fewer OLTP transactions or the OLTP transactions access a small range of keys algorithm is better because only transfers a small amount of data to the mappers An alternative implementation of real-time querying is similar to re-computing the data cube a operation is used to return one version for each key/value pair regardless TABLE II P ARAMETERS   of whether or not it has been updated When the updates are uniformly distributed across all the keys this baseline implementation could be more ef\223cient To be able to select a more ef\223cient approach we propose a cost model Table II shows the parameters of the cost model The most important one is  which is the percentage of the keys that are updated after refreshing the data cube   or adaptive discussed in Section III-B1 and shuf\224ing these data to mappers The scans all the of the real-time table while the adaptive scans fewer when is small and the is activated depends on the status of each HBase-R node and cannot be easily estimated Thus we assume that the cost of reading the local data on each HBase-R node are the same for and  The difference is in the number of tuples transferred from HBase-R to mappers Thus we base our analysis on the network transfer cost The operation shuf\224es one version for each key and the cost of the scan phase of the MapReduce job is estimated as The mapper outputs one key/value pair for each tuple Thus the size of the map output is retailprice type B Correctness of Query Results MetaStore C Cost Model IncreQuerying IncreQuerying 1 Cost Analysis of the Baseline Method store\223les store\223les cub.setAggregateFunc\(\215sum\216 The baseline algorithm is a MapReduce job that is essentially similar to re-computing the entire data cube First we estimate the cost of the scan phase on the map side The scan phase consists of two parts scanning the local data on each HBase-R node  6 Parameter  sh Cube  v1  M1,B1  2954  M1,B1,540  2954,Q  831  brand  3245  MO  FullScan Incremental Scan   key  price  101  price  price  Reducer1  price  memstore cost ratio of shuf\224ing from HBase-R    cost ratio of HBase-R writes    cost ratio of shuf\224ing in MapReduce    has enough number of keys However whether the adaptive 


part MO MO MR MO MO MO MO MO MO MR sh sh sh sh 327 327 327 327  327             write write IncreQuerying part A Performance of Maintaining Data Cube part T L L R C R T C C R T L R R C C L C C L 48 reduce reduce reduce reduce 327 327 327  327 327  327 327 327 327 327 327 327 327 327  327 327 327  327 2 Cost Analysis of IncreQuerying Algorithm HBase HBase S  T n c T n Q n f C n T s n S n S C S T s n c T s n Q n and the cost of external sorting the map output is When the mappers complete the reducers start to pull the sorted key/value pairs from the mappers The cost of shuf\224ing is The pulled data are cached in the local 223le system whose cost can be ignored since the shuf\224ing process and the local writing process are pipelined The data shuf\224ed from the mappers are then merged into one sorted run using the multi-way mergesort method which only requires reading and writing the 223les once Finally the result table is written into HBase-R whose cost is computed as After the scan phase the real-time data and the data cube are sorted The size of the map output for these two types of data is and the cost of external sorting the map output is The data on all the mappers are shuf\224ed to the reducers after the mapper completes The cost of shuf\224ing is In the reduce phase the cost of sort merging process is and the cost of writing the data into HDFS is Based on the cost model discussed above the more ef\223cient approach is dynamically selected when a real-time query is submitted VI E VALUATION In this section we evaluate the R-Store on our in-house cluster of 144 nodes Each node is equipped with Intel X3430 2.4 GHz processor 8 GB of memory 2x500 GB SATA disks each of which is connected by a gigabit Ethernet and running CentOS 5.5 The cluster nodes are evenly placed onto three racks We adopt TPC-H data for the experiments However TPC-H updates only append new keys while we need online transactions that update the existing keys Therefore we write our own scripts to update the TPC-H data The scripts can update the keys based on either a uniform distribution or Zipf distribution In most of the following experiments we use the TPC-H table is loaded into HBase-R Figure 5 shows the processing time of the two methods The distribution of updated keys follows a Zipf distribution We adjust the factor of the Zipf distribution so that about 1 keys are updated while the number of updates is increased from 8 million to 1,600 million Since HBase-R does not remove the previous version of the data 0.024 GB to 4.8 GB of new data are inserted into each HBase-R node The processing time of re-computation has two parts the blue rectangle ReCompScan is the scan time of the real-time table and the yellow rectangle ReCompExe is the execution time of the MapReduce job after the scan phase As the number of updates increases the data stored on each HBase-R node increases as well Thus more data are scanned at the HBaseR side for the re-computation approach and the running time of the scan phase for re-computation is increased over time However as illustrated in Figure 5 the running time of the ReCompExe decreases as the number of updates increases which is counterintuitive We expected that the execution time of the MapReduce job should remain the same in different settings as they process the same number of key/value pairs The reason for the decrease in ReCompExe is that ReCompScan and ReCompExe are pipelined The more time ReCompScan takes the more these two phases overlap reducing the time ReCompScan takes In contrast the processing time of incremental update consists of only one part the red rectangle the time it takes to write data cube into HBase-R This is because our real-time data cube maintenance algorithm is fast enough to update the 327 327 327 327 327 327 327 327 327 327 327 327 327 327 327 327 327            2              2                        2    2                   The MapReduce job for reads both the data cube and the updated data Therefore its cost is different from that of recomputation At 223rst the mappers scan both the real-time data and the data cube The cost of shuf\224ing the real-time data and data cube to mappers is while the cost of scanning the data cube is table to build the data cube In this experiment we 223rst measure the throughput of our real-time data cube maintenance algorithm to ensure that it has suf\223ciently high processing capacity to handle the update streams from HBase-R As can be seen in Figure 4 when HStreaming is con\223gured with 10 nodes the algorithm can process more than 100K updates per second which is even higher than the throughput of HBase-R with 40 nodes the throughput of HBase-R will be discussed in Section VI-C We compare the two methods for refreshing the data cube re-computation and incremental update We deploy the system on 100 nodes with 40 nodes for MapReduce 40 nodes for HBase-R and 20 nodes for HStreaming The scale factor of the TPC-H data is set to 8000 so that there are 1,600,000,000 keys for table On each HBase-R node there are 4.8 GB data The data cube is built after the log log log C m c S B C s Q d Q Q C s Q d Q Q C w d Q Q C T T s T C d C C S s Q T m d Q Q C m s Q d Q Q C m c S  B m c S  B C s Q T C d Q Q C s Q T C d Q Q C w d Q Q 212 212 212 212 212 212 212 212 212 212 212 212 212 212 212 212 212 2 2  shuf f ling shuf f ling merge merge scan scan sort sort sort map map map B B B HBase HBase 2   1         1   1 2  2   


Manufacture B Performance of Real-Time Querying IncreQuerying Baseline Baseline IncreQuerying IncreQuerying Baseline Baseline Baseline Baseline mft IncreQuerying Baseline shipdate Lineitem shipdate return\224ag linestatus IncreQuerying Baseline IncreQuerying Baseline IncreQuerying y y IncreQuerying 11 In this experiment we investigate the performance of realtime querying First we compare the Fig 4 Throughput of Real-Time Data Cube Maintenance Algorithm                                                              Fig 5 Performance of Data Cube Refresh    Fig 6 Scalability real-time data cube with the data streams from HBase-R Thus the latency of periodically refreshing the data cube in HBaseR equals to the time of writing the real-time data cube into HBase-R This time is related to the the size of the data cube and does not change as the number of updates increases We also evaluate the scalability of R-Store In this experiment the number of nodes and the data size increase with the same ratio The percentage of updates is set to 1 for different scalability settings As can be seen in Figure 6 the running time of both re-computation the brown line and incremental update blue line do not change much as the number of nodes increase which demonstrates the scalability of R-Store 1 algorithm which optimizes the real-time query using the data cube with the algorithm implemented with the operation The cluster settings are the same as those of Figure 5 except that we 223x the number of updates to 8,000 million and vary the percentage of the keys updated Figure 7\(a shows the processing time of both algorithms for a typical data cube slice query algorithm consists of two parts the black rectangle ReCompScan is the time to scan the real-time table and the yellow rectangle ReCompExe is the execution time of the MapReduce job after the scan phase In contrast the processing time of consists of three parts the red rectangle CubeScan is the time to scan the data cube the blue rectangle UpdateScan is the time to scan the performs much better than  It outperforms the approach for two reasons 1 by using adaptive incremental scan it scans fewer data in HBase-R and shuf\224es fewer data to MapReduce 2 its MapReduce job processes fewer data than that of re-computation However as the percentage of updated keys increases more data are shuf\224ed from HBaseR to MapReduce Thus both the scan time and the execution time increase In contrast for  since the always shuf\224es one version for each key to MapReduce the amount of data shuf\224ed from HBase-R is constant As a result the running time of is almost constant Due to the existence of the 223ltering condition on attribute  most tuples of the table are 223ltered and fewer data are sorted and shuf\224ed during the execution of the MapReduce job As a result the difference between the execution times is not so signi\223cant In general algorithm outperforms algorithm when the percentage of keys being updated is low In addition to the data cube slice query we also evaluate TPC-H Q1 Figure 7\(b with the same experimental settings We did not illustrate other benchmark queries as they involve multiple tables which will not be able to illustrate as clearly the effectiveness of the basic operators supported in R-Store The parameter of TPC-H Q1 is set to 215365\216 days and only about 15 of the tuples are 223ltered  table since we only build the data cube on attributes  and  the data cube is much smaller than the real-time table and the time to scan the data cube is around 20 seconds Overall Figure 7\(b demonstrates that the performance of is signi\223cantly better than that of  To select the better querying method among the two we use the cost model Section V-C to estimate the number of I/Os Figure 8 shows the running time of  and the I/Os estimated for both and algorithms The axis on the left is the processing time of the query while the axis on the right is the estimated I/Os The estimated number of I/Os for the blue line increases linearly with almost the same slope the histogram as the processing time of the query while the estimated number of I/Os for the Baseline the brown line is constant which is around 2.52 10 FROM part WHERE mft 0 2,000 4,000 6,000 8,000 10,000 ReComp Update ReComp Update ReComp Update ReComp Update ReComp Update Processing time \(s Number of Updates 8M 400M 800M 1,200M 1,600M Update ReCompExe ReCompScan 100    200    300    400    500  10  20  30  40  50  60  70  0    1000    2000    3000    4000    5000    6000    7000    8000  50  75  100  125  145  IncrementalUpdate                            0    Updates Per Second \(K Number of Nodes Throughput          Processing Time \(s Number of Nodes ReComputation              The processing time of the table in HBase-R and the grey rectangle UpdateExe is the execution time of the MapReduce job after the scan phase When only a small range of keys are updated larger than 21512-01-1998\216 Thus the execution time of the MapReduce job after the scan phase is longer than that of Figure 7\(a For the  This result hence veri\223es the accuracy of our cost model Compared to querying only the data cube RTOLAP queries require two additional steps which incur additional cost scanning the real-time data from HBase-R and merging the real-time data with the data cube on demand in MapReduce SELECT sum   327 215 002\002    49 FullScan FullScan part shipdate prices GROU P BY brand type size container 


method increases slightly which is due to two reasons 1 the data before In this experiment we investigate the performance of OLTP queries when OLAP queries are running The workload is update-only and the keys being updated are uniformly distributed We launch ten clients to concurrently submit the updates when the system is deployed on 100 nodes Each client starts ten threads each of which submits one million updates 100 updates in batch Another client is launched to submit the data cube slice query That is one OLAP query and approximately 50,000 updates are concurrently processed in R-Store The system reaches its maximum usage in this setting based on our observation When the system is deployed on other number of nodes the number of clients submitting updates is adjusted accordingly Figure 11\(a shows the throughput of the system The throughput increases as the number of nodes increases which demonstrates the scalability of the system However when OLAP queries are running the update performance is lower than running only OLTP queries This result is expected because the OLAP queries compete for resources with the OLTP queries We also evaluate the latency of updates when the system is approximately fully used As shown in Figure 11\(b the aggregated response time for 1000 updates are similar with respect to varying scales VII C ONCLUSION MapReduce is a parallel execution framework which has been widely adopted due to its scalability and suitability in 0    500    1000    1500    2000  0  10  20  30  40  50  60  70  80  90  100  IncreQueryScan             IncreQueryExe              DC DC DC  Q i i i i T part  Q a Data Cube Slice Query                                                                                                b TPC-H Q1 Fig 7 Performance of Querying    Fig 8 Accuracy of Cost Model    Fig 9 Performance vs Freshness On each HBase-R node the key/values are stored in format Though only one or two versions of the same key are returned to MapReduce HBase-R has to scan all the of the table Since the is materialized to HDFS when it is full these 223les are sorted by time Thus instead of scanning all the and between  only the between   are scanned The value of decides the freshness of the result There is a trade-off between the performance of the query and the freshness of the result the smaller is the fewer real-time data are scanned Figure 9 shows the query processing time with different freshness ratios which is de\223ned as the percentage of the real-time data we have to scan for the query In this experiment  1600 million and 800 million updates on 1 distinct keys are submitted to HBase-R When the freshness ratio is 0 the input of the query is only the data cube Thus the cost of scanning the real-time data is 0 When the freshness ratio increases to 10 the cost of scanning the real-time data is around 1500 seconds because the cost of scanning the real-time table dominates the OLAP query As the freshness ratio increases the running time of and  and when it is not  and  We submit 800 million updates to the server each day and the percentage of keys updated is 223xed to 1 The data cube is refreshed at the beginning of each day and the OLAP query is submitted to the server at the end of the day Since the data are compacted after the data cube refresh the amount of data stored in the real-time table are almost the same at the same time of each day The processing time of and are thus almost constant In contrast when the compaction scheme is turned off HBase-R stores much more data and the cost of locally scanning these data becomes larger than the cost of shuf\224ing the data to MapReduce As a result the processing time of and increases over time and and a user speci\223ed timestamp still need to be scanned and 2 the amount of data shuf\224ed to mappers are roughly the same with different ratios Figure 10 depicts the effectiveness of our compaction scheme In this experiment we measure the processing time of the data cube slice query when the compaction scheme is applied  0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan Processing Time \(s I/Os \(X10 11  Percentage of Keys Updated CubeScan        Processing Time \(s Freshness Ratio CubeScan                                                                                                            50 0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan store\223le store\223les part memstore store\223les memstore store\223les IncreQuerying Baseline IncreQuerying Baseline-NC IncreQuerying-NC Baseline IncreQuerying Baseline-NC IncreQuerying-NC C Performance of OLTP 0    1200    2400    3600    4800    6000  1  5  10  15  20  25  0  0.8  1.6  2.4  3.2  4  IncreQueryScan        IncreQueryExe        I/Os estimated for IncreQuery                               I/Os estimated for  Baseline                 T T T T T T T T 


3000    6000    9000    12000  1  2  3  4  5  6  7  IncreQuerying                                   Baseline-NC                   IncreQuerying-NC                       51 002 Fig 10 Effectiveness of Compaction    a Throughput    b Latency Fig 11 Performance of OLTP Queries a large scale distributed environment However most existing works only focus on optimizing the OLAP queries and assume that the data scanned by MapReduce are unchanged during the execution of a MapReduce job In reality the real-time results from the most recently updated data are more meaningful for decision making In this paper we propose R-Store for supporting real-time OLAP on MapReduce R-Store leverages stable technology HBase and HStreaming and extends them to achieve high performance and scalability The storage system of R-Store adopts multi-version concurrency control to support real-time OLAP To reduce the storage requirement it periodically materializes the real-time data into a data cube and compacts the historical versions into one version During query processing the proposed adaptive incremental scan operation shuf\224es the real-time data to MapReduce ef\223ciently The data cube and the newly updated data are combined in MapReduce to return the real-time results In addition based on our proposed cost model the more ef\223cient query processing method is selected To evaluate the performance of R-Store we have conducted extensive experimental study using the TPCH data The experimental results show that our system can support real-time OLAP queries much more ef\223ciently than the baseline methods Though the performance of OLTP degrades slightly due to the competition for resources with OLAP the response time and throughput remain good and acceptable A CKNOWLEDGMENT The work described in this paper was in part supported by the Singapore Ministry of Education Grant No R-252000-454-112 under the epiC project and M.T 250 Ozsu\220s work was partially supported by Natural Sciences and Engineering Research Council NSERC of Canada We would also like to thank the anonymous reviewers for their insightful comments R EFERENCES  http://hbase.apache.or g  http://hstreaming.com  http://www comp.nus.edu.sg epic  M Athanassoulis S Chen A Ailamaki P  B Gibbons and R Stoica Masm ef\223cient online updates in data warehouses In  pages 865\205876 2011  Y  Cao C Chen F  Guo D Jiang Y  Lin B C Ooi H T  V o S W u and Q Xu Es2 A cloud data storage system for supporting both oltp and olap ICDE pages 291\205302 2011  S Ceri and J W idom Deri ving production rules for incremental vie w maintenance In  pages 577\205589 1991  T  Condie N Conw ay  P  Alv aro J M Hellerstein K Elmelee gy  and R Sears Mapreduce online In  pages 313\205328 2010  J Dean S Ghema w at and G Inc Mapreduce simpli\223ed data processing on large clusters In  pages 137\205150 2004  L Golab T  Johnson and V  Shkapen yuk Scheduling updates in a real-time stream warehouse ICDE pages 1207\2051210 2009  M Grund J Kr 250 uger H Plattner A Zeier P Cudre-Mauroux and S Madden Hyrise a main memory hybrid storage engine  4\(2 Nov 2010  A Gupta I S Mumick and V  S Subrahmanian Maintaining vie ws incrementally extended abstract In  pages 157\205166 1993  S H 264 eman M Zukowski N J Nes L Sidirourgos and P Boncz Positional update handling in column stores In  pages 543\205 554 2010  D Jiang G Chen B C Ooi and K.-L T an epic an e xtensible and scalable system for processing big data 2014  D Jiang B C Ooi L Shi and S W u The performance of mapreduce an in-depth study  3\(1-2 Sept 2010  D M Kane J Nelson and D P  W oodruf f An optimal algorithm for the distinct elements problem PODS 22010 pages 41\20552  A K emper  T  Neumann F  F  Informatik T  U Mnchen and DGarching Hyper A hybrid oltp&olap main memory database system based on virtual memory snapshots In  2011  T W  K uo Y T  Kao and C.-F  K uo T w o-v ersion based concurrenc y control and recovery in real-time client/server databases  52\(4 Apr 2003  K Y  Lee and M H Kim Ef 223cient incremental maintenance of data cubes In  pages 823\205833 2006  F  Li B C Ooi M T  250 Ozsu and S Wu Distributed data management using mapreduce In  2014  I S Mumick D Quass and B S Mumick Maintenance of data cubes and summary tables in a warehouse In  pages 100\205111 1997  A Nandi C Y u P  Bohannon and R Ramakrishnan Distrib uted cube materialization on holistic measures In  pages 183\205194 2011  L Neume yer  B Robbins A Nair  and A K esari S4 Distrib uted stream computing platform In  pages 170\205177 2010  C Olston B Reed U Sri v asta v a R K umar  and A T omkins Pig latin a not-so-foreign language for data processing In  pages 1099\2051110 2008  K Ser ge y and K Y ury  Applying map-reduce paradigm for parallel closed cube computation In  pages 62\20567 2009  M Stonebrak er  D J Abadi A Batkin X Chen M Cherniack M Ferreira E Lau A Lin S Madden E O\220Neil P O\220Neil A Rasin N Tran and S Zdonik C-store a column-oriented dbms In  pages 553\205564 2005  A Thusoo J S Sarma N Jain Z Shao P  Chakka S Anthon y  H Liu P Wyckoff and R Murthy Hive a warehousing solution over a mapreduce framework  2\(2 2009  P  V assiliadis and A Simitsis Near real time ETL In  volume 3 pages 1\20531 2009  C White Intelligent b usiness strate gies Real-time data w arehousing heats up  2012 SIGMOD VLDB NSDI OSDI SIGMOD SIGMOD Proc VLDB Endow In ICDE IEEE Trans Comput VLDB ACM Computing Survey SIGMOD ICDE ICDMW SIGMOD DBKDA VLDB PVLDB Annals of Information Systems DM Review 0    Processing Time \(s Time since the Creation of Data Cube \(day Baseline                  Updates Per Second \(K Number of Nodes Updates only                  Response Time for 1000 Updates\(s Number of Nodes Updates only                  0    20    40    60    80    100  10  20  30  40  50  60  70  Updates + OLAP                                    0    2    4    6    8    10  10  20  30  40  50  60  70  Updates + OLAP                                    Proc VLDB Endow 


  13    1  2   3   4   5   6   7   8  9  10  11   


and aeronautical engineering with degrees from Universitat Politecnica de Catalunya in Barcelona Spain and Supaero in Toulouse France He is a 2007 la Caixa fellow and received the Nortel Networks prize for academic excellence in 2002 Dr Bruce Cameron is a Lecturer in Engineering Systems at MIT and a consultant on platform strategies At MIT Dr Cameron ran the MIT Commonality study a 16 002rm investigation of platforming returns Dr Cameron's current clients include Fortune 500 002rms in high tech aerospace transportation and consumer goods Prior to MIT Bruce worked as an engagement manager at a management consultancy and as a system engineer at MDA Space Systems and has built hardware currently in orbit Dr Cameron received his undergraduate degree from the University of Toronto and graduate degrees from MIT Dr Edward F Crawley received an Sc.D in Aerospace Structures from MIT in 1981 His early research interests centered on structural dynamics aeroelasticity and the development of actively controlled and intelligent structures Recently Dr Crawleys research has focused on the domain of the architecture and design of complex systems From 1996 to 2003 he served as the Department Head of Aeronautics and Astronautics at MIT leading the strategic realignment of the department Dr Crawley is a Fellow of the AIAA and the Royal Aeronautical Society 050UK\051 and is a member of three national academies of engineering He is the author of numerous journal publications in the AIAA Journal the ASME Journal the Journal of Composite Materials and Acta Astronautica He received the NASA Public Service Medal Recently Prof Crawley was one of the ten members of the presidential committee led by Norman Augustine to study the future of human space\003ight in the US Bernard D Seery is the Assistant Director for Advanced Concepts in the Of\002ce of the Director at NASA's Goddard Space Flight Center 050GSFC\051 Responsibilities include assisting the Deputy Director for Science and Technology with development of new mission and measurement concepts strategic analysis strategy development and investment resources prioritization Prior assignments at NASA Headquarters included Deputy for Advanced Planning and Director of the Advanced Planning and Integration Of\002ce 050APIO\051 Division Director for Studies and Analysis in the Program Analysis and Evaluation 050PA&E\051 of\002ce and Deputy Associate Administrator 050DAA\051 in NASA's Code U Of\002ce of Biological and Physical Research 050OBPR\051 Previously Bernie was the Deputy Director of the Sciences and Exploration Directorate Code 600 at 050GSFC\051 Bernie graduated from Fair\002eld University in Connecticut in 1975 with a bachelors of science in physics with emphasis in nuclear physics He then attended the University of Arizona's School of Optical Sciences and obtained a masters degree in Optical Sciences specializing in nonlinear optical approaches to automated alignment and wavefront control of a large electrically-pumped CO2 laser fusion driver He completed all the course work for a PhD in Optical Sciences in 1979 with emphasis in laser physics and spectroscopy He has been a staff member in the Laser Fusion Division 050L1\051 at the Los Alamos National Laboratories 050LANL\051 managed by the University of California for the Department of Energy working on innovative infrared laser auto-alignment systems and infrared interferometry for target alignment for the HELIOS 10 kilojoule eight-beam carbon dioxide laser fusion system In 1979 he joined TRW's Space and Defense organization in Redondo Beach CA and designed and developed several high-power space lasers and sophisticated spacecraft electro-optics payloads He received the TRW Principal Investigators award for 8 consecutive years Dr Antonios A Seas is a Study Manager at the Advanced Concept and Formulation Of\002ce 050ACFO\051 of the NASA's Goddard Space Flight Center Prior to this assignment he was a member of the Lasers and Electro-Optics branch where he focused on optical communications and the development of laser systems for space applications Prior to joining NASA in 2005 he spent several years in the telecommunication industry developing long haul submarine 002ber optics systems and as an Assistant Professor at the Bronx Community College Antonios received his undergraduate and graduate degrees from the City College of New York and his doctoral degree from the Graduate Center of the City University of New York He is also a certi\002ed Project Management Professional 14 


 





 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


