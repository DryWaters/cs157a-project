IBHYS: A New Approach to Learn Users Habits Jean David Ruvini and Christophe Fagot ruvini fagot iirmm.fr LIRMM UMR 5506 CNRS-UMII 161 Rue Ada 34392 Montpeilier Cedex 5, France Tel 33 4 67 41 86 13.Fax 33 4 67 41 85 00 Abstract Learning interface agents search regularities in the user behavior and use them to predict user\222s actions We pro pose a new inductive concept learning approach called IBHYS to 
learn such regularities This approach limits the hypothesis search to a small portion of the hypothesis space by letting each training example build a local ap proximation of the global target function It allows to si multaneously search several hypothesis spaces and to si multaneously handle hypotheses described in different lan guages This approach is particularly suited for learning interface agents because it provides an incremental algo rithm with low training time and decision time which does not require from the designer of the interjCace agent to 
de scribe in advance and quite carefully the repetitive patterns searched We illustrate our approach with two autonomous software agents, the Apprentice and the Assistant, devoted to assist users of interactive programming environments and implemented in Objectworks-Smalltalk-80 The Ap prentice learns user\222s work habits using an IBHYS algo rithm and the Assistant based on what has been learnt proposes to the programmer sequences of actions the user might want to redo We show, with experimental results on real data that IBHYS outperforms ID3 both in 
computing time and predictive accuracy 1 Introduction The discovery of repetitive patterns in data is a challeng ing problem which has a lot of applications in various domains fast text searching where patterns are strings of symbols, data mining where patterns are association rules and particularly learning interface agents where patterns are sequences of users actions Learning interface agents  131 are software agents that assist users of interactive environments by learning their habits and preferences from experience and predicting what they are going to do next The problem of learning users 
habits can be modeled as the task of searching repeated patterns in the sequence of the actions the user has per formed during work sessions These repetitive patterns can be noisy \(if the user does not perform each time exactly the same sequence of actions\unordered if the user perform the same actions but not each time exactly in the same or der\or both noisy and unordered. Learning interface agents would highly benefit from an incremental and low comput ing time algorithm to learn such general regularities, able to predict user\222s action in real time Although efficient 
algorithms have been proposed to find exact or noisy repetitive patterns in strings 9 241 no algorithm to our knowledge solve the problem of finding both noisy and unordered repetitive patterns On the other hand machine-learning approaches like induc tive concept learning should theoretically be able to learn such general regularities However most of the algorithms  15,20,7 63 search hypothesis spaces to acquire the def inition of a target concept and large and complex spaces critically slow down the learning process Therefore this approach fails to provide an efficient algorithm for learn ing 
interface agents because user\222s actions can be described with a lot of attributes with large set of possible values and training examples generally belong to large hypothesis spaces Conversely paradigms like instance-based iearn ing 4 build a local approximation of the target function and limit the hypotheses search to a small portion of the hypothesis space but defer the processing of training ex amples until a new instance must be classified, and require a lot of computing time to predict the target value for a new instance These paradigms are unable to provide 
an algo rithm to predict user\222s actions in real time We provide here an alternative approach for learning interface agents called instance-based hypothesis search IBHYS As learning with radial basis functions lo which approximates the global target function by a com bination of local approximations our approach lets each training example build a set of hypotheses that locally ap proximate the global target function limiting the hypoth esis search to a small portion of the hypothesis space However as a generalization of learning with radial ba 0-7803-5214-9/98/$10.00 0 1998 IEEE 200 


sis functions  it does not restrict the approximation to a combination of Gaussian functions and allows to han dle simultaneously hypotheses described in different lan guages This approach is particularly suited for learning interface agents because it provides an incremental algo rithm with low training time and decision time, which does not require from the designer of the interface agent to de scribe in advance and quite carefully the repetitive patterns searched By specifying several hypothesis spaces he gives the algorithm the potential to find various repetitive pat terns We illustrate our approach with two interface agents the Apprentice and the Assistant that actively assist users of the Smalltalk interactive programming environment This work finds its roots in the idea of programmer appren tices 23 211 which were ambitious attempts to automat ically assist programmers in the task of code production This produced remarkable results but the task was certainly too complex in whole generality and such apprentices are not really integrated in todays standard programming en vironments We propose to apply the techniques of au tonomous software agents IO 141 to merge eventually extend, the above ideas The Apprentice and the Assistant aim at letting programmers focus on the essential part of programming design and write code by automating the achievement of repetitive tasks This paper summarizes the Apprentice-Assistant ar chitecture and focuses more particularly on the issues re lated to the learning task. Section 2 presents the Apprentice and the Assistant shows how users actions are recorded and monitored and defines the learning task Section 3 de scribes our IBHYS approach, proposes a formalism gives a general procedure and shows how the Apprentice makes use of this procedure to learn users habits Section 4 gives experimental results and shows that our IBHYS algorithm outperforms the well known ID3 algorithm 20 Related works The idea of employing machine-learning in user modeling appeared with learning interface agents 22 12 14 11 and begins to be studied in the user-modeling com munity  16 181 However, no incremental algorithm with low computing time have been proposed to solve the prob lem of learning repetitive patterns in whole generality Most of the existing learning interface agents ll 1 2 171 reduce the learning task to the prediction of a few attributes with small set of possible values and not at all try to pre dict complex actions like those performed in a program ming environment  141 studies the question of employing ID3  a de cision tree algorithm in a learning assistant for meeting calendar management However by allowing their assis tant to spend several hours learning each night the authors do not propose a low computing time solution to learn users habits The closest work to our agents are Opensesame Opensesame runs in background on Macintosh system 7 and learns repetitive tasks in opening and closing files or applications emptying trash rebuilding desktop Its first weakness is that it is disruptive and frequently solicits the user. Conversely our Assistant only makes suggestions the user is free to ignore and never request the user OpenS esame limits the learning task to a dozen of actions and only learns noisy habits Unfortunately it appeared unable to learn simple repetitive opening of folder when we have tested it and the paper describing this system does not bring any other information on this point Eager  is an interesting software that assists users of the Hypercard environment by anticipating actions Eager as Holte's assistant for browsing in information libraries  is unable to learn noisy or unordered repetitive patterns does not build a base of habits and forgets habits after it has performed them Note that our multiple description languages approach allows to integrate the graph-based induction technique used in 25 with benefit of low computing time and in crementallity 2 The Apprentice and the Assistant We illustrate the IBHYS approach with two interface agents the Apprentice and the Assistant devoted to assist users of interactive programming environments Our Ap prentice learns user's habits i.e the tasks the user performs repetitively for which he has not the opportunity or the will to write scripts or macros The Assistant's task is to ac celerate and facilitate the programmers tasks by automat ing the achievement of repetitive tasks Based on what the Apprentice has learnt the Assistant proposes in a non obtrusive window the user is free to ignore sequences of actions the user might want to redo Both of them operate without explicit intervention of the user The Apprentice and the Assistant have been developed in Smalltalk 4.02 Figure 1 shows a snapshot of a Smalltalk screen including an assistant window in which the Assistant makes a sug gestion triggered by the opening of an exception window 2.1 Monitoring User's Actions We define an action to be any interaction between the user and the interjiace that affects an interjiace tool By interface tool we mean a software component of the Smalltalk envi ronment browsers debuggers, inspectors and editors We Objectworks Smalltalk copyright Parc-Place sys tems We are currently working to adapt our agents to the newest version of Smalltalk 20 1 


Figure 1  The user has executed a program that has raised an exception The Assistant window displays a suggestion. It offers to open move and resize a debugger If the user accepts the suggestion by clicking on it, the Assistant will automatically open a debugger, move and resize it as the user uses to do naturally represent actions with Smalltalk objects For ex ample, class Act ionMenu models selection of an item in a menu  class ActionList models selection of an item in a list, class Actionselect models text highlighting class ActionError models the opening of an error noti fication window and class Act ionBut t on models mouse click on a button Each class of actions defines instance variables to store parameters for the action In the follow ing, we will note actions Class Tool Parameter Let us call truce the ordered collection of all the actions the user has performed during a work session The figure 2  ActionSelecL\(aStringHolder,'anObject cass ActionMenu\(aStringHolder,doIt ActionError\(ni1,doesNotUnderstand ActionMenu\(aDebugger,debug ActionMenu\(aDebugger,move ActionMenu\(aDebugger,resize ActionList\(aDebugger,learn  Figure 2 A sample of the trace shows an example of a trace whrre the user opens a debug ger to correct an error 2.2 The learning task Our Assistant should be able to automatically select and propose sequences of actions that the user might want to redo It has to detect situations in which these repetitive tasks are not fulfilled and it has to avoid them by offering to automate them These repetitive tasks are all the  ex act noisy or unordered both noisy and unordered  repeti tive sequences of actions of the trace The task of the Ap prentice is to build knowledge that precisely characterize the situations in which these repetitive sequences should be proposed to the user Let us call situation for such a repetitive sequence the last n actions3 of the trace that immediately precede it For a given repetitive sequence, there are as many situations as occurrences of this sequence To characterize these situa tions we make the hypothesis that they may be different occurrences of a few situation patterns which characterize them Therefore in the machine-learning framework the task of the Apprentice can be seen as a concept learning problem where each exact repetitive sequence of the trace is a concept c which training examples are all the pairs of 3The value of n clearly depends on the application field and is called description length 202 


the form 4 s c  where s is a situation associated to c The Apprentice has to induce the general definitions of sit uation patterns given a set of training examples Let R be a repetitive sequence of actions and let AI A2 A3 and lowercase letters from a to f denote actions We can distinguish 3 kinds of interesting situation patterns 1 Noisy For instance the 2 training examples 4 AlaAzbcA3,R  and  AldAzefA3,R  of the trace   AlaAzbcA3R  AldA~efA3R  can be characterized by the situation pattern A1  A2  A3 where the stars denote differences called errors or noise on the whole actions or on the values of the at tributes of the actions 2 Unordered the training examples 4 A1A3A2 R  4 A2A3A1 R  and 4 A3A2A1 R  can be char acterized by the pattern AlAzA3 3 Noisy and unordered 4 AlaA2,R  and 4 A2bA1 R  can be characterized by the pattern A1  A2 Figure 6 shows such examples of situation patterns The 3 kinds of patterns detailed above can be seen as 3 different kinds of hypotheses from 3 different hypothesis spaces which suppose 3 different description languages of the training example and the hypotheses Therefore, the task of the Apprentice is to find in this different hypothe sis spaces, the hypotheses that best explain the membership of each training example situation to the related concept repetitive sequence 3 IBHYS the Instance-based Hypothesis Search approach 3.1 A formal framework The general framework of our work is called inductive con cept learning. Let C  cl c2  cp denote a set of con cepts and   E1 U 12 U  U EP a set of training examples A training example is a pair of the form 4 x c  where x is the description of the example and c E C the concept to which the example belongs Our approach aims at ac quiring the general definition of each concept ci E C from the set Ei of positive examples and the set E  Ei of neg ative examples. Precisely it builds approximations called hypotheses of each concept ci Let U be the result of the learning process i.e the set of hypotheses that actually ap proximate c1 122    cp A hypothesis can be seen as a set of constraints on the descriptions of the training examples A hypothesis h is said to match a training example 4 x c t if x satisfies all the constraints of h conversely 4 x c  is said to satisfy h Besides h explains the membership of a positive example 4 x c  if h matches 4 z c  and h aims at approximating c Our approach has two important advantages First, it does not explore a hypotheses space but builds local approximations4 of the concepts of C by letting each training example 4 x c  choose the most relevant hy potheses that correctly explain its membership to c To do so some hypotheses are successively submitted to  x c  Using an evaluation criterion 5 4 x c  is able to compute the relevance of a hypothesis h for the concept c As a consequence a training example  x c  has to keep xx the set of the most relevant hypotheses that cor rectly explain its membership to e Second, hypotheses of U can be expressed in different description languages. The way the hypotheses are treated in the algorithm, described in section 3.2 is independent from their description and the evaluation of he relevance of the hypotheses is only based on the number of training examples they match A hypothesis h keeps two impor tant values 1 na the numbers of training examples that h matches in each c E C these values are used by the training examples to measure the relevance of h 2 T the number of elements of E that judge h relevant this attribute allows to remove from 3t a hypothesis that no training example has chosen Finally let us describe the three following operators MATCH U x   IB Data a hypothesis h a training example  z,c  Result true if h matches 4 x,c  and false if h does not This is the classical subsumption operator. As stated above, our approach allows to handle sevecal hypothe ses description languages so the MATCH operator is strongly linked to these description languages In fact MATCH can be seen as a filter among several match ing operators, one per description language MATCH 4 x c   Matchi\(h 4 x c  where h is expressed in the ith description language and Match is the matching operator of this language SUBMIT U x Ex IR  Void Data Result a hypothesis h a twining example  x c  a threshold k updates the set of 23.1 regarding the hypothesis h The training example 4 x,c  computes the rele vance of the hypothesis h using 5 for the concept c and updates its set xx The hypothesis h may be 1 relevant for  x c  h matches  x c  aitd is added to xx 2 irrelevant for 4 z c  and 4 x c  431 is a global approximation of the concepts of C and each hypothesis of 31 is a local approximation of a concept of C 203 


3 3.2 We rejects it 4 x c  may possibly remove the less rel evant hypothesis in xz if it must keep the size of xx constant. Besides r is updated Regarding to the application field the threshold IC can be used either to bound the size of xu or to set the minimum relevance accepted to add any hypothesis in xx HYPGEN  x 2O 3 2Hyp Data a training examplc x c  a set Result a set of hypotheses of objects 0 HYPGEN is the hypotheses generation operator Hyp denotes the space of all the hypotheses that can be generated HYPGEN is strongly linked to the appli cation field and allows hypotheses to be formed by comparison between a training example 4 x c  and a set of objects 0 Hypotheses can be generated by comparison with other training examples, hypotheses or any other objects useful for the hypotheses gener ation in the procedure described after 0   U 3-1 The main advantage of our approach is that it makes possible to handle simultaneously several different de scription languages of the hypotheses HYPGEN can be seen as the combination of several hypothesis gen erators one per description language HYPGEN x,c O  HypGen x,c  7 0 where HypGen is the hypothesis generator for the ith description language Suppose 0   Suppose the training examples are described both by directed graphs and conjunctions of attribute-value pairs A simple example of hypothesis generator outputs the maximal tree included in  x c  and in all the de scription of the element of 0 Another hypothesis generator compares  z c  to all the elements of 0 For each pair  x c  y d  it returns a hypoth esis which description is constituted of the attribute value pairs that x and y shares in common One of the interest of using multiple hypothesis gen erators can be evaluated easily Suppose that train ing examples are described with n boolean features This leads potentially to an hypothesis space of 2 ele ments Suppose now this set of n features can be split in 3 disjoint sets of n/3 features These 3 sets lead to 3 hypothesis spaces of 2"13 and 3 sr 2"13  2n A General Algorithm following  is the set of training examples currently avail able 3-1 the set of the currently relevant hypotheses, and the threshold k is used in the SUBMIT operator Given a new training example  x c  the procedure updates the sets  and 31 The most important steps of the IBHYS procedure are Step 1 Each hypothesis of 3-1 have to know the number of training examples it matches in each class These val ues allow the training examples to evaluate the rele vance of the hypotheses of 3t Step 2 The numbers of training examples matched by each hypothesis have been modified step 1 Some hy potheses that were relevant for a training example y E  may not be relevant any more This may happen if the hypotheses matching y are all almost as relevant as each other Step 3 Using  x c   and 3t the operator HYPGEN generates a set of hypotheses that will be individually studied only if they are not yet in 3-1 Whatever the de scription languages of the hypotheses are all the gen erated hypotheses will be dealt with in the same way described in the steps 4 and 5 Step4 Each hypothesis generated by HYPGEN have to know the amount of training examples it matches in each class, allowing the training examples to evaluate i IS relevance Step 5 The currently studied hypothesis h must be evalu ated by the training examples of  to measure its rel evance For each training example 4 y c E E this step updates the set yx of its relevant hypotheses ad the value r Step 6 This step aims at removing the irrelevant hypothe ses of 3t A hypothesis h is said to be irrelevant if none of the training examples of E has chosen it i.e r  0  3.3 The Apprentice algorithm The Apprentice learns user's habits every loo5 actions of the user It first searches the training examples that appear in the last 100 actions of the trace and invokes the pro cedure NewExample for each new training example it has found We defined 3 operators \(taking 2 situations in input to allow the Apprentice to learn the 3 kinds of situation pat  terns defined-above noisy which builds a pattern which has the commune characteristics, regarding the position of now give an Of rhe IBHYS approach through a general procedure called NewExample The main steps of this procedure are explained below In the Again this value depends on the application field 204 


I  Submit the hypothesis to the examples foreach  y d e E do SUBMIT\(h y d k I Insert h in the set of hypotheses 7-l c 3-1 U h  Delete of 3-1 the hypotheses  witch are not relevant foreach h E 3-1 do  Update the number of examples  matched by each hypothesis foreach h E 31 do if MATCH\(h z c  then L n t n  1 EU{<x,c 1  Insert  x c  in the set of examples the two situations and stars \(denoting noise for their differ ences unordered which returns the set of the actions of the first situation if and only if, these actions all appear in the second situation with no constraint on their positions and noisyunordered We also defined 3 classes of hy potheses that define their own method MATCH to test cov erage of training examples and 3 hypothesis generators based on the 3 operators defined above Let us call knowledge base the set of hypotheses pro duced by our IBHYS algorithm After each action of the user the Assistant inspects the knowledge base and se lects all habits which hypothesis cover the last actions of the user The Apprentice then displays suggestions corre sponding to the related concepts in an non-obtrusive win dow The user is free to take it into account or not A sim ple mouse-click on one of these suggestions automatically performs the related actions 4 Experimental results The Apprentice and the Assistant are currently used by the first author Experiments reported here where conducted during the development of an 223ASCII to HTML\224 translator on real data We compare our IBHYS algorithm to ID3 which is the decision tree algorithm used in  141 to 223explore the potential of machine-learning methods to automatically create and maintain  customized knowledge for personal software assistants\224 Time 450 400 350 300 250 200 1 50 100 50 0 6  8 10 12 14 Description Length lbhys ID3   Figure 3 Computing time in minutes versus de scription length I L ifr,h=Othen\222He\222H-hh Figure 3 and 4 show that IBHYS outperforms ID3 re  end garding the computing time The figure 3 plots the comput ing time versus the description length cf 2.2 on a trace of 1000 actions and the figure 4 the computing time versus the size of the trace for a description length of 10 actions Time is given in minutes 205 


Trace Figure 5 Accuracy 1 2 3 4 Accuracy Excess Hypotheses I  I ID3 I Ibhys ID3 I Ibhys ID3 I Ibhys Time 450 400 250 200 150 100 300 350 t   t  t  Figure 4 Computing time in minutes versus trace length Table in figure 5 is a direct comparison of the respec tive accuracies of IBHYS and ID3 These tests were per formed on a trace of 1000 actions with 1 x I 2 see SUBMIT in section 3 These 1000 actions were split in a training set and a test set The leftmost column lists the size of the training sets used, and column 1 lists the number of training examples repetitive sequences the algorithms have found in the training sets Column 2 shows the predic tive accuracy on new examples It shows that IBHYS had correctly predicted a repetitive sequence in 52.58 of the case versus 46.13 for ID3 Column 3 lists the 223excess rate\224 that is the number of time the algorithms have pre dicted erroneous repetitive sequences whereas no predic tion were expected This excess rate value is very impor tant Hight values means that the agent constantly bothers the user with useless suggestions IBHYS and ID3 have almost the same excess rate Finally column 4 lists the number of hypotheses the algorithms have learnt. Note that both IBHYS and ID3 have an average decision time of 10 milliseconds Due to the fact that the Apprentice and the Assistant have been implemented in Smalltalk 4.0 and that few pro grammers still use this environment we could not find pro grammers to intensively test our agents Of course, we are working to adapt our agents to the newest version of the Smalltalk environment However we can give examples of the habits learnt during the experiments reported here fig ure 6 Habit 1 means that the user systematically moves and resizes a debugger he has opened after an error habit 2 shows that the user systematically removes system com ments of new methods 5 Conclusion We have proposed a new approach, called IBMYS and an incremental algorithm with low computing time, for induc tive concept learning particularly suited for learning inter face agents This approach lets each training example build a set of hypotheses that locally approximate the global tar get function, limiting the hypothesis search to a small por tion of the hypothesis space Because training examples can choose among several description languages to form an hypothesis, and different description languages to form different hypotheses, it allows to handle simultaneously hy potheses described in different languages We presented an application of this approach to learn user\222s habits of inter active programming environments and propose an original assistance to programmers based on two software agents the Apprentice and the Assistant We showed with exper imental results on real data, that IBHYS outperforms ID3 both in computing time and predictive accuracy IBNYS seems a promising approach for data-mining Further studies will be conducted to evaluate our IB HYS approach with respect to standard Irvine collection machine-learning datasets In the context of the Appren tice and the Assistant, an important limitation of IBHYS is that it bounds in advance the length of the description and therefore, the length of the situation patterns searched We are investigating to bypass this limitation We are currently working to adapt the Apprentice and 206 


1 Situation pattern Repetitive sequence ActionErreur\(ni1  ActionMenu\(aDebugger,move ActionMenu\(aDebugger,debug ActionMenu\(aDebugger,resize Figure 6 Example of user\222s habits 2 the Assistant to the newest version of the Smalltalk envi  1 11 P Maes. Agents that reduce work and information overload ActionSelect\(aBrowser message selector and argument names ActionMenu\(aBrowser,cut 223comment stating purpose of message\224 I temporary variable names I statements ronment We hope that they will be soon available to full time programmers for intensive tests Communications of the ACM Special Issue on Intelligent Agents 37\(7 July 1994 1121 P Maes Social interface agents Acquiring comoetence by Acknowledgements We would like to thank Christophe Fiorio for his Algo rithm LaTeX style arld his help in the preparation of the final manuscript of this paper References l R Armstrong D Freitag T Joachims and T Mitchell Webwatcher A learning apprentice for the world wide web In AAAI Spring Symposium on Information Gather ing 1995 2 A Caglayan M Snorrason J Jacoby J Mazzu and R. Jones. Lessons from open sesame!, a user interface leam ing agent In Proceedings of PAAM96 pages 61-74 Apr 1996 3 A Cypher EAGER: Programming repetitive tasks by ex ample In Proceedings of ACM CHI\22291 Programming by Demonstration pages 33-39 1991 4 R 0 Duda and P E Hart Pattern Classification and Scene Analysis John Wiley and Sons New York 1973 5 0 Gascuel and G Caraux Distribution-free performance bounds with the resubstitution error estimate Pattern Recognition Letters 13:757-764 1992 6 J Hertz A Krogh and R G Palmer An Introduction to the Theory of Neural Computation Lecture Notes Volume I Addison Wesley 1991 7 J H. Holland Adaptation in natural artijicial systems Uni versity of Michigan Press Ann Arbor, 1975 8 R C Holte and C Drummond A learning apprentice for browsing In 0 Etzioni, editor Software Agents  Spring Symposium AAA1 Press Mar 1994 9 R M Karp R E Miller, and A L Rosenberg Rapid iden tification of repeated patterns in strings trees and arrays In 4th Annual ACM Symposium on Theory of computing pages 125-136 Denver, Colorado 1-3 May 1972  101 Y Lashkari, M. Metral, and P Maes. Collaborative interface agents In Proceedings ofAAAI\22294 pages 444-449 1994  learning from users and othkr agents In 6 Etzioni, editor Software Agents  Spring Symposium pages 71-78 AAAI Press Mar 1994 13 P Maes and R Kozierok Learning interface agents In Proceedings of the 1 I th National Conference on Artificial Intelligence pages 459-464 Menlo Park CA USA July 1993. AAAI Press I41 T Mitchell R Caruana D Freitag J McDermott and D Zabowski. Experience with a learning personal assistant Communications of the ACM Special Issue on Intelligent Agents 37\(7 July 1994 15 T M Mitchell Version Spaces An Approach to Concept Learning PhD thesis Electrical Engineering Dept Stan ford University, 1979 16 J. Orwant Heterogenous learning in the doppelganger user modeling system User Modeling and User-Adapted Inter action 4\(2 1995 17 T R Payne and P Edwards Interface agents that learn an investigation of leaming issues in a mail agent interface Applied Artificial Intelligence 11 1-32 1997 18 W Pohl Learning about the user  user modeling and ma chine learning In V M J Herrmann, editor ICML\22296 Work shop Machine Learning meets Human-Computer Interac tion pages 29-40 1996 I91 M J D Powell Radial basis functions for multivariable interpolation A review In Algorithms for Approximation pages 143-167 Oxford 1987. Clarendon Press 20 J R. Quinlan Induction of decision trees Machine Learn ing 1\(1 1986 21 C Rich and R C Waters The programmer\222s apprentice Computer pages 11-25 Nov 1988 22 J C Schlimmer and L A Hermens. Software agents: Com pleting patterns and constructing user interfaces Journ of AI Research 1:61-89 Nov 1993 23 R. Waters The programmer\222s apprentice: Knowledge-based program editing IEEE Trans Software Engineering SE 8\(1 Jan 1982 24 S Wu and U Manber Fast text searching allowing errors Communications of the ACM 35\(10 Oct 1992 25 K Yoshida and H Motoda Automated user modeling for intelligent interface Int J of Human Computer Interaction 3\(8 1996 207 


I Plenary Panel Session J Future Directions in Database Research  456 Chair Surajit Chaudhuri Microsoft Corporation Panelists Hector Garcia-Molina Stanford University Hank Korth, Bell Laboratories Guy Lohman IBM Almaden Research Center David Lomet Microsoft Research David Maier Oregon Graduate Institute I Session 14 Query Processing in Spatial Databases I Chair Sharma Chakravarthy University of Florida Processing Incremental Multidimensional Range Queries in a Direct Manipulation Visual Query Environment  458 High Dimensional Similarity Joins Algorithms and Performance Evaluation  466 S Hibino and E Rundensteiner N Koudas and K.C Sevcik Y Theodoridis E Stefanakis and T Sellis Cost Models for Join Queries in Spatial Databases  476 Mining Association Rules Anti-Skew Algorithms  486 J.-L Lin and M.H Dunham Mining for Strong Negative Associations in a Large Database of Customer Transactions  494 A Savasere E Omiecinski and S Navathe Mining Optimized Association Rules with Categorical and Numeric Attributes  503 R Rastogi and K Shim Chair: Anoop Singhal AT&T Laboratories S Venkataraman J.F Naughton and M Livny Remote Load-Sensitive Caching for Multi-Server Database Systems  514 DB-MAN A Distributed Database System Based on Database Migration in ATM Networks  522 T Hara K Harumoto M Tsukamoto and S Nishio S Banerjee and P.K Chrysanthis Network Latency Optimizations in Distributed Database Systems  532 I Session 17 Visualization of Multimedia Data I Chair Tiziana Catarci, Universita di Roma 223La Sapienza\224 W Chang D Murthy A Zhang and T.F Syeda-Mahmood Global Integration of Visual Databases  542 X 


The Alps at Your Fingertips Virtual Reality and Geoinformation Systeps  550 R Pajarola l Ohler P Stucki K Szabo and P Widmayer C Baral G. Gonzalez and T.C Son Design and Implementation of Display Specifications for Multimedia Answers  558 1 Session 18 Management of Objects I Chair: Arbee Chen National Tsing Hua University P Boncz A.N Wilschut, and M.L. Kersten C Zou B Salzberg, and R Ladin 0 Wolfson S Chamberlain S Dao L Jiang, and G. Mendei Flattening an Object Algebra to Provide Performance  568 Back to the Future Dynamic Hierarchical Clustering  578 Cost and Imprecision in Modeling the Position of Moving Objects  588 ROL A Prototype for Deductive and Object-Oriented Databases  598 A Graphical Editor for the Conceptual Design of Business Rules  599 The Active HYpermedia Delivery System AHYDS using the M Liu W Yu M Guo and R Shan P Lang W Obermair W Kraus and T Thalhammer PHASME Application-Oriented DBMS  600 F Andres and K. Ono S Chakravarthy and R Le S Mudumbai K Shah A Sheth K Parasuraman and C Bertram ECA Rule Support for Distributed Heterogeneous Environments  601 ZEBRA Image Access System  602 Author Index  603 xi 


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


