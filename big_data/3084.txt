A Study of Performance Evaluation of HRM: Based on Data Mining  ZHAO Xin School of Management, Zheji ang University, Hangzhou, China zhaoxin510j@yahoo.com.cn   Abstract  As an important step of knowledge discovery in databases, data mining is a process of distilling critic unclear, potential useful information or knowledge from plenty of data, which has been applied in many places. In this text, we introduce a new method of data mining that can be applied in the performance evaluation of human resource management  1. Introduction  The concept of data mining emerged in the late 1980s which is developed from the field of database and is named as knowledge discovery in database \(KDD initially. The study of KDD is aimed at automatic dealing with plenty of original dada from database to find new knowledge by uncovering potential relationship pattern As the development of KDD research, it is then called data mining   There are different definitions about data mining as follow 2  1\ining is defined as distilling interesting knowledge that is critic, unclear and potential information from big database. According to this definition, we can call data mining as knowledge refining, information discovering or data mode transaction. It involves machine learning, mode identification, statistics, intellectual database, knowledge acquirement, viewdata, and expert system, etc 2\ining is defined as distilling interesting information or knowledge that is critic, unclear and useful from mass, incomplete, noisy, blurry and random data. It is required to apply special discovery algorithm to search or produce an interesting mode or data set. The process of data mining includes three steps, such as data preparation data mining and data description 3\ining is defined as distilling potential useful information or knowledge from mass, incomplete noisy and random data. Data mining comes from the study of machine learning, which means to obtain requisite knowledge by a special learning algorithm from an existing data set. This knowledge can be applied in information management, intellectual inquiry decision-making support, process control, and so on   According above, we can define data mining as a process of distilling critic, unclear, potential useful information or knowledge from plenty of data. It is aimed at discovering the relationship among modes and data by all kinds of analysis methods. Hence, we can also call data mining as knowledge mining, knowledge distilling or knowledge discovery  2. The process of data mining  Data mining is an important step of knowledge discovery in databases \(KDD\The process of KDD is showed as figure 1. It includes five steps: selection, data preprocessing, transformation, data mining and interpretation/ evaluation                  Data Selection Interpretation/Evaluation Data mining Preprocessing Transformation  Target data Preprocessed data Transformed data Patterns Knowledge Figure 1. A KDD process 
2008 International Seminar on Future Information Technology and Management Engineering 978-0-7695-3480-0/08 $25.00 © 2008 IEEE DOI 10.1109/FITME.2008.133 45 


1\Selection This step is aimed to search and select relational data for data discovery, which includes transformation between different mode data and data gathering. The goal of data selection is to distinguish requisite data set to analyze and improve the quality of data mining  2\reprocessing and transformation This phase includes four substeps, such as data cleaning, data integration, data transformation and data reduction. Their main task is to filtrate false or incompatible data and combine data that are used in multi-document or multi-database runnable condition [4  3\ata mining This is the most important phase of KDD. It requires confirming the task or goal of data mining firstly and deciding what kind of mining algorithm should be used Then, we could start data mining to discover useful mode or knowledge 4\Interpretation/ Evaluation The task of this step is to express the final outcome and filtrate information. In addition, we should make the discovered mode visionable or intelligible, for example we can transform classifying decision tree to the classifying rule of “IF 001\002\001\002 THEN 001\002\001\002     3. The methods of data mining  The methods of data mining have been applied in many places. The most popular methods of data mining include association rules, cluster analysis, classifying visualization, ANN and decision tree, etc  1\ssociation rules For individual information searches, customers emphasize information relativity but indissimilarity Hence, it can improve the veracity and depth of data mining by establishing appropriate rule according to the psychology of customers about information requirement which includes analysis of association rules, multilayer rules and discrimination rules  2\luster analysis It means to control macroscopically the same sort data by the character of objects 3\Classifying For this method, the sort refers to the content of information metagroup. We should establish a classifying function to map the concentrated data to a given sort 4\Visualization This method enhances the traditional function of diagram to analyze data more clearly  5\Artificial neural network \(ANN ANN is based on self-study math model, which makes mode choice and trend analysis. The typical method of study is backdating, which compares the different value of output and continuously adjusts their weights to get a new output. Then, the NN gets a stable output through continuous learning process  6\ion tree It is based on information theory. This method needs us to search the field which has the most information quantity in the database to establish a crunode of decision tree. And ramification can be set up according to different value of fields. Then decision tree is formed when the crunode and ramification of each layer are built repeatedly    4. The performance evaluation of HRM  Human resource management \(HRM\ includes personnel management, human resource plan, job analysis employment policy, training, performance evaluation and salary management, etc. And the performance evaluation is a very important measure There are many ordinary methods of performance evaluation, such as scale evaluation, behavior evaluation and crucial incident method. In this study, a new method with data mining method is introduced    000\003 000\003 000\003 000\003 000\003 000\003       Figure 2. The data mining process of performance evaluation Former data N ew data O b jec t  Data preprocessing Algorithm Output Model Checkout 
46 


5. The application of data mining in performance evaluation   1\hoosing the object of data mining We choose employee’s performance as the object of data mining, because we can obtain potential and valuable information about human resource characteristic that can benefit manager to institute human resource policy to find excellent employees  2\reprocessing The aim of data preprocessing involve two sides: one is to ensure the preciseness and efficiency of original data the other is to ensure the preciseness and efficiency of the model of data mining by adjusting the format and content of data At this phase, we should collect every employee’s information as more as possibly. It includes demographic data as well as ability, attitude, belief and outstanding achievement, etc The data preprocessing is a fussy step of data mining It is needed to make the type of data consistently, unify the value range of field and delete irrespective field, etc For example, the fields of employees’ names could be deleted since they are not meaningful for the data mining of common characteristic. However, the fields of interpersonal communication, respect for occupation responsibility and learning attitude could be dispersed to three grades as high, moderate and low because it is not necessary to evaluate them accurately. Finally, we collect the preprocessed data into a single database by deleting redundant data By the way, it can be used directly in the phase of data preprocessing if the data warehouse mode has been established in enterprise in formation system, because the cost of data preprocessing is high while the data warehouse can supply requisite data for next analysis that skips the phase of data preprocessing In the case of this study, we use the data warehouse model to analysis, which is built for the former HRM system. The model is designed for the simply evaluation for employees with asteroidal structure. The data warehouse model is showed as below 000\003 000\003 000\003 000\003 000\003 000\003 000\003 000\003 000\003 000\003 000\003 000\003 000\003         3\luster analysis After finishing the phase of data preprocessing, cluster analysis is needed to classify the data more appropriately Cluster analysis is based on the similarity of data, which classify similar data into the same group. And the methods of cluster analysis include K - expectation or nearest neighbor algorithm, self-organization mapping NN algorithm, etc In this study, we use K - expectation to classify employees into the same group with the same characteristic. First, we select randomly a representation for every group. For the non-representations, we dispatch them into the nearest group according the Euclid distance to the representation. Then, we substitute non-representations for representations repeatedly to improve the quality of cluster analysis. At last, we mark employees as sorts with the cooperation value of providers and the cluster analysis results  4\The data mining model Decision tree is a kind of data mining method by establishing decision tree to find classified knowledge of training set. The data set of root node is equivalent to training set. And the content of leaf node is sort name while the middle node is aimed to the characteristic with a ramification. The ramification is a possible value of the characteristic. The typical learning system of decision tree Information Code Name Department Job Salary Harvest Code Workload Quality Efficiency Contribution Ability Code Specialty Innovation Communication Decision Organizing Performance Code Ability Attitude Spirit Harvest 001\002\001\002  Conclusion Attitude Code Present ratio Duty Learning attitude Initiative Spirit Code Policy Commitment Interpersonal relationship Upbringing Figure 3. The data warehouse model of performance evaluation 
47 


is ID3 and the algorithm C4.5 is the extension of ID3 And it is used in the research as below   1\Information gain analysis Algorithm C4.5 begins the process of learning through establishing a decision tree from above to below. And it starts by the question “which characteristic will be test at the root node?” To answer this question, we can use the information gain analysis method to solve it. The characteristic with the highest value of information gain is considered as the most relative to the object of data mining, which would be chosen to test as root node. And we can delete the characteristic with low value of information gain \(below the threshold\, which means having weak relation with the object of data mining   The information gain is described as the function Gain A\which is showed as below  I \(s 1 s 2  001\002 s m  002\010 p i log 2 p i i = 1 001\002 m a P i S i 002 S E\(A 002\010 s 1j s 2j  001\002 s mj  002 s *I\(s 1j s 2j  001\002 s mj j 1 001\002 v\                                     \(b I\(s 1j s 2j  001\002 s mj 002\010 p ij log 2 p ij i = 1 001\002 m; p ij  S ij S j  s 1 s 2  001\002 s m E\(A\               \(c   2\lishing decision tree We select the characteristic with the highest information gain as the test characteristic of given set S from the results of information gain analysis. Then, we establish the root nodes of decision tree and produce a ramification for each possible value of the characteristic of root nodes. At last, we put the training sp. below each ramification. We repeatedly select the best test characteristic with every training species of each ramification until classifying all the training sp. is finished Then, an integrated decision tree is formed  6. Discussion  In this study, we apply the data mining structure as below ability 002 attitude 002 spirit 002 harvest 002 performance We use the field of performance as the dependent variable of data mining and its value ranges as high medium and low. We use half of the samples of all the 232 employees as training set to establish decision tree model and produce 13 classifying rules. According to the result of information gain analysis, we choose the attributes of harvest as the test attribute of root nodes We exam the decision tree model with the remnant half sample of non-training set. And the result matches the former decision tree model very well, which indicates that the result of the former decision tree is perfect There is an example of classifying rule as follow Rule 002 ability =“High h arv e st  M ed i u m  U  attitude = “High spirit = “High erformance High With the data mining model, managers can classify and select the excellent employees from the applicants. For different talents, managers could dispatch them into appropriate departments according the evaluating results of performance to take advantage of their abilities  7. References  1  QIAN, Z. \(2004\. “On the development of the special information database based on data-mining in key subjects Library No. 3, pp.50-53 2  ZHANG, J. \(2008\. “A study of data mining and its application Journal of Xiangtan Normal University Social Science Edition Vol. 30, No. 1, pp.232-234 3  YAN, C., and Y. ZHANG \(2002\. “Network data mining techniques in intelligent search Journal of Library Science in China Vol. 28, No. 3, pp.49-51 4  HAN, J., and K. MICHELINE Data mining: Concepts and techniques Morgan Kaufman Publisher, San Francisco 2000 5  LI, J. \(2003\. “A model of data mining system Journal of Daqing Petroleum Institute Vol. 27, No. 3, pp.87-90 6  LUO, L., and Y. CHEN \(2004\. “Knowledge mining and personalized services of digital libraries Journal of Library Science in China Vol. 30, No. 3, pp.69-71 
48 


 Conclusion  This paper presents a semi-supervised learning algorithm based on the supervised learning information from association rules combining with unsupervised learning method ISODATA in prediction and early warning for flea beetle Semi-supervised learning algorithm takes advantage of virtue of supervised learning with high accuracy rate and unsupervised learning needn't training samples, according to past data successful predicted and early warned for vegetable crop diseases and insect pest flea beetle Semi-supervised learning algorithm can effectively improve the accuracy and effi ciency of prediction and early warning for vegetable crop diseases and insect pest flea beetle. Prediction and ear ly warning for flea beetle commonly depends on the knowledge of plant protection expert in previous. This paper constructed mathematical methods to establish prediction and early warning model have made certain signifi cance and a high accuracy for prediction and early warning Semi-supervised learning algorithm is effective, but the use of plant protection expert knowledge is also lacking In fact, the influential fact ors of the flea beetle number includes environmental factors, such as soil, but the quantitative research of flea beetle breeding environment is still in the rough, whose data including survival reproduction, and other environmental data of flea beetle is one of the research direc tion of the future research  Acknowledgements Project supported by the South China Agricultural University Principal Foundation Projects, China \(No 2007K017  References    Sun Hu. The St udy on B i ocont rol t o W h eat Takeall Disease and Resistance Assessment of Wheat Cultivars. Henan Agricultural University Master's degree thesis, 2004   W a ng Zhengjun, C h eng Ji aan, Li Di anm o  Desi gn and building of GIS database for Chilo suppressalis. Acta Entomologica Sinica, 2001, 44\(4\525-533    Li Zuoy ong, Peng Li hong. Predi c t i on m odel of agricultural plant diseases and insect pests based on artificial neural network and its verification.1999 19\(5\759-761   Zhang Ji anbi ng, Zhu Yepi ng. Di seases and pest  insects forecast by fuzzy rules. System Sciemces and Comprehensive Studies In Agriculture, 2000, 16\(4\283285   Hu Xi aopi ng Li ang C h enghua, Yang Zhi w ei et al  Development and application of the BP neural network prediction system on plantdiseases and pests. Journal of Northwest Sci-Tech University of Agriculture and Forestry, 2001 29\(2\73-76    Xi ong Xuem ei Ji C h angy i ng, C l audi o M o raga Parametric Fuzzy Neural Network Based on Genetic Algorithm Configured for Plant Disease Prediction Transactions of The Chines e Society of Agricultural Machinery, 2004, 35 \(6\110-114   R e n C hunfeng, Li M i ao. St udy  of fi t n ess funct i on in pest forecasting base d on genetic programming Computer Engineering and Applications, 2007 43\(6\197-243   Jiawei Han, Micheline Kam b er. Data Mining Concepts and Techniques. Morgan Kaufmann Pblishers.2001   B a o-Yi W a ng, Shao-M i n Zhang A M i ni ng Algorithm for Fuzzy Weighted Association Rules Proceedings of the Second International Conference on Machine Learning and Cybernetics. 2003, 11:2495-2499   Mohammadreza Kangavari Sattar Hashemi. Effect of Similar Behaving Attributes in Mining of Fuzzy Association Rules in the Large Databases. Lecture Notes in Computer Science, 2006 3980:1100-1109    R u an D.,Kerre E.E.. Fuzzy i m pl i cat i on operat o rs and generalized fuzzy method of cases. Fuzzy Sets and systems, 1993,54\(1\23-38   Veget a bl e pest s i n t h e m a i n cl assi fi cat i on criteria .2006, November, second edition  
221 
221 
221 
221 


  Proceedings of the Sev e nth Inter n ational Conference on Ma c h ine Lear ning and Cyber n etics  K u nming, 12-15 J u ly 2008          In fi g u re 1, t h e y ax is represen ts t h e tim e spen t in  readi ng i n a bat c h of st r e am const r uc t i ng i t s l o cal  DSC F C I t ree  up dat i n g t h e gl o b al DSC F C I-t ree  an d pr o duci n g f r e que nt cl o s ed i t e m s et s sati sfy i ng i t e m  constraints B The y axis i n figure 2 re presents the  me m o ry capacity used for storing the gl obal  DSCFCI-tree As ca n be see n fr om t h e t w o f i gu res whe n t h e val u e of s u pp o r t   i s fi xe d, t h e t i m e and space re qui rem e nt s of t h e algorithm grow as th e stream progre sses, but tend to sta b ilize. For e x am ple, the m e m o ry re qui red by the  global DSC F CI-tree with su ppo r t 0  00 15  in cr eases relativ ely q u i ck ly as th e first sev e n  b a tch e s o f st ream arriv e and tend s to stab ilize at ro ugh ly 3  8MB with sm a l l  bum ps. T h is is because the num b er of pote ntial fre que nt clo s ed item s e t s satisfyin g ite m co n s trai n t s g o es up  relativ ely q u i ck ly at first, and b e co m e s relativ ely stead y  later th ro ugh in tr odu cing p r un ing tech no log y Th e stab ility resu lts are qu ite n i ce as th ey p r o v id e ev id en ce th at th e algorith m can  h a n d l e lon g  d a ta strea m s. As t h e v a lu e o f suppo r t   dec r eases the e x ecution tim e in fig u re 1 a n d t h e m e m o ry requi red by  DS CFCI-tree in  figure 2 inc r ea ses quickly  This is because t h ere are m o re p ot ent i a l  f r e que nt cl ose d i t e m s et s sati sfy i ng i t e m  const r ai nt s wi t h sl ower support    s s Fi gu re 1 an d F i gu re 2 sh o w t h e e x peri m e nt al res u l t s  o f  d a ta sets T1 5I7 D 1 000k  Th e ex per i m e n t al r e su lts of  o t h e rs two d a ta sets are sim ilar to th ese fig u r es                           Fig u re 3 shows th e av erag e t i m e sp en t i n  up d a ting  t h e gl obal DS C F C I-t ree o n c e  with three dif f eren t ite m con s t r ai nt s c onst r ai nt  0 n o c onst r ai nt s co nst r ai nt  1  co nst r ai nt 2   e  can dra w a c o nclusi on that w ith t h e reinforcem en t o f th e constraints 222  degree the num b er of fre que nt close d  ite m s ets satis fyin g co rrespo nd ing item co n s t r ain t s d ecreases, and th e tim e sp en t i n  up d a ti ng  th e g l o b a l DSCFCI-tree dec r eases t o o. M o re ove r  the space  requirem ent of t h e global DSCFCI-tree go es d o wn  sim u l t a neousl y The r ef ore  i n t e g r at i ng i t e m const r ai nt s in to t h e m i n i n g algorith m can  reduce t h e e x ecution tim e  and space com p lexity of the algorithm    4 3   2 1  005 005 005 5.   Concl u si ons In this pa per   we propose a n ef ficient algorith m for m i ni ng fre q u e n t cl ose d i t e m s et s i n dat a st ream s. The co n t r i bu tio n s of ou r stud y in clu d e 1 pro posin g a no v e l  dat a  st r u ct ure  DSC F C I t re e, fo r st ori n g pot e n t i a l   fre que nt cl ose d i t e m s et s, and de vel o pi n g  a new m e t hod  for increm ental updating DSCFCI-t ree ef ficiently 2 appl y i n g a  pr u n i n g t e c hni que f o r ef fi ci ent  p r u n i n g gl obal  DSCFCI-tree  to reduce the s p ace re quire m e nt of t h e  DSCFCI-tree an d th e tim e sp en t in trav ersi n g the DSCFCI-tree dram atica l l y 3  teg r ating ite m con s t r ai nt s i n t o t h e m i ni ng a l go ri t h m and t hus  red u ci n g  furthe r t h e e x ecution tim e and sp ace co m p lex ity of th e const r ai nt 0 S=0.001 T ime\(s S=0.0015  const r ai nt 1 const r ai nt 2 S=0.002  N umber of data stream segments Fi gu re 3 c o m p ari s on of e x ecut i o n  t i m e wi t h di f f erent const r ai nt s\(s=0.0015 Figure 2.The m e m o ry usage of DSCFCI-tree   279 005 004 005 4 3 2 1 


  Proceedings of the Sev e nth Inter n ational Conference on Ma c h ine Lear ning and Cyber n etics  K u nming, 12-15 J u ly 2008  al gori t h m   Our performance st udy shows  that DSCFCI  algorithm is ef ficient and ef fective  Refer e nces 1] C Gia nnel l a, J Ha n, J  Pei, et al. Mi ning fre quent p a ttern s i n  d a t a stream s at mu ltip le tim e g r an u l arities  G]. In  H Kar g up ta, A Jo sh i, K Siv a ku m a r  et al, ed s Next  Ge nerat i on Dat a M i ni ng C a m b ri dg e, M a ss  M I T Press, 2003  2 G S Mank u, R Mo t w an i. App r ox im a t e f r e q u e n c y cou n t s ove r st ream i ng dat a  C    The 28 th Int 222 l Confere n ce on V e ry Lar g e Data Bases \(VL D B 2002 HongKong, 2002 3  Aras u A M a nk u G S  A p pr oxi m a t e co unt s a n d  q u a n tiles ov er slid in g wi n d o w s. In Pro c eed ing s  o f  th e 23 rd ACM S I G M OD SI G AC T S I G A R T  Sym posi u m on Pri n ci pl es o f  Dat a base Sy st e m s. Pari s France  AC M Press, 2004 4 Datar M, Gio n i s A, In d y k P   Mo t w an i  R. Main tain in g str eam stat is tics o v e r slid in g  w i ndow s In  Proceedi ngs of the 13 th A nnu al ACMSIA M  Sy m p o s iu m on  Discrete Al g o rith m s San Fran cisco   USA  AC M Press, 2002 5 N Pas quier  Y Bastide  R T a ouil, et al Discoveri n g freq u e n t clo s ed item s e t s fo r asso ciatio n ru les[C]. In   Beeri C, et al, eds  Proc of the 17 th I n t 222 l Co nf  on Dat a base Theory B e rl i n  Spri nger V e rl ag, 1999  6  W a n g Jian yon g. Clo s et sear ch ing fo r t h e b e st st rat e gi es f o r m i ni ng f r e que nt cl ose d i t e m s et s. In  Pr oc. N i n t h A C M SIGK DD In t\222 1 Co nf  on  K now ledg e D i scov er y an d  D a t a Min i n g  W a shi ngt on,DC Aug.2003    280 


association mining The Likelihood Ratio Test fails to extract features also belonging to common vocabulary and it makes the extraction dependent on the feature position in the sentence leading to low recall The dBNP and bBNP based methods yield low recall due to the fact that the product features do not occur with the article the in front of them very often The Association Mining approach returns all frequent nouns which decreases precision Our results suggest that the choice of algorithm to use depends on the targeted dataset If it consists of mainly on-topic content the results of Table 10 indicate that the Association Mining algorithm is better suited for this task due to its high recall If the dataset consists of a mixture of onand off-topic content our results suggest that the Likelihood Ratio Test based algorithm would perform better due to its ability to distinguish and 002lter out the off-topic features For future work we plan to extend the Likelihood Ratio Test methods especially the dBNP based approach by other determiners such as a or this  which should increase the recall of this method Another possibility which we will investigate regards the BNP patterns The current Likelihood Ratio Test approach is not capable of dealing with discontinuous feature phrases for example in 5 the quality of the pictures is great the feature would be picture quality  This problem could be addressed by introducing wildcards in the BNP patterns We will also investigate whether there are any methods in order to calculate an optimal threshold for the candidate feature extraction in order to increase the recall of the Likelihood Ratio Test based algorithm We plan to investigate whether a deeper linguistic analysis e.g with a dependency parser can improve the feature extraction Acknowledgements The project was funded by means of the German Federal Ministry of Economy and Technology under the promotional reference 01MQ07012 The authors take the responsibility for the contents The information in this document is proprietary to the following Theseus Texo consortium members Technische Universit  at Darmstadt The information in this document is provided as is and no guarantee or warranty is given that the information is 002t for any particular purpose The above referenced consortium members shall have no liability for damages of any kind including without limitation direct special indirect or consequential damages that may result from the use of these materials subject to any liability which is mandatory due to applicable law Copyright 2008 by Technische Universit  at Darmstadt References  R Agra w al and R Srikant F ast algorithms for mining association rules Proc 20th Int Conf Very Large Data Bases VLDB  1215:487–499 1994  K Bloom N Gar g and S Ar g amon Extracting a ppraisal expressions In HLT-NAACL 2007  pages 308–315 2007  R Bruce and J W iebe Recognizing subjecti vity a case study in manual tagging Natural Language Engineering  5\(02 1999  K Da v e S La wrence and D Pennock Mi ning the peanut gallery opinion extraction and semantic classi\002cation of product reviews In Proceedings of the 12th International Conference on World Wide Web  pages 519–528 New York NY USA 2003 ACM  T  Dunning Accurate methods for the statistics of surprise and coincidence Computational Linguistics  19\(1 1993  O Feiguina and G Lapalme Query-based summ arization of customer reviews In Canadian Conference on AI  pages 452–463 2007  C Fellbaum Wordnet An Electronic Lexical Database  MIT Press 1998  A Ferraresi Building a v ery lar ge corpus of english obtained by web crawling ukwac Master's thesis University of Bologna Italy 2007  M Gamon A Aue S Corston-Oli v er  and E Ringger  Pulse Mining customer opinions from free text In Proceedings of the 6th International Symposium on Intelligent Data Analysis IDA-2006  Springer-Verlag 2005  N Glance M Hurst K Nig am M Sie gler  R Stockton and T Tomokiyo Deriving marketing intelligence from online discussion In Proceedings of the 11th ACM SIGKDD International Conference on Knowledge Discovery in Data Mining  pages 419–428 New York USA 2005 ACM  M Hu and B Liu Mining opinion features in customer reviews In Proceedings of 9th National Conference on Arti\002cial Intelligence  2004  N K obayashi K Inui K T atei shi and T  Fukushima Collecting evaluative expressions for opinion extraction In Proceedings of IJCNLP 2004  pages 596–605 2004  S Morinag a K Y amanishi K T ateishi and T  Fukushima Mining product reputations on the Web In Proceedings of KDD-02 8th ACM International Conference on Knowledge Discovery and Data Mining  pages 341–349 Edmonton CA 2002 ACM Press  A.-M Popescu and O Etzioni Extracting product features and opinions from reviews In Proceedings of HLT-EMNLP-05 the Human Language Technology Conference/Conference on Empirical Methods in Natural Language Processing  pages 339–346 Vancouver CA 2005  H Schmid T reetagger a language independent part-ofspeech tagger Institut fur Maschinelle Sprachverarbeitung Universitat Stuttgart  1995  J W iebe R Bruce and T  O'Hara De v elopment and use of a gold-standard data set for subjectivity classi\002cations In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics  pages 246–253 Association for Computational Linguistics Morristown NJ USA 1999  J Y i T  Nasuka w a R Bunescu and W  Niblack Sentiment analyzer Extracting sentiments about a given topic using natural language processing techniques In Proceeding of ICDM-03 the 3ird IEEE International Conference on Data Mining  pages 427–434 Melbourne US 2003 IEEE Computer Society 
160 
151 


Figure 4 Expected and real number of extracted patterns using two promoter sequence datasets Horizontal axis minimum support vertical axis number of patterns 
85 
85 


a frequency constraint and according to the structure of the dataset These proposals are all based on a global analytical model i.e an interesting approach that needs however to develop complex and speci\036c models As a result they cannot be easily extended to handle complex conjunctions of constraints to incorporate different symbol distributions or different semantics for pattern occurrences To the best of our knowledge no method has been proposed to estimate the number of patterns satisfying a constraint while avoiding to develop a global analytical model Our approach requires only to know how to compute for a given pattern its probability to satisfy the constraint this can be obtained in many situations and it remains ef\036cient in practice by adopting a pattern space sampling scheme 6 Conclusion Using constraints to specify subjective interestingness issues and to support actionable pattern discovery has become popular Constraint-based mining techniques are now well studied for many pattern domains but one of the bottlenecks for using them within Knowledge Discovery processes is the extraction parameter tuning This is especially true in the context of differential mining where domain knowledge is used to provide different datasets to support the search of truly interesting patterns From a user perspective a simple approach would be to get graphics that depict the extraction landscape i.e the number of extracted patterns for many points in the parameter space We developed an ef\036cient technique based on pattern space sampling that provides an estimate on the number of extracted patterns This has been applied to non trivial substring pattern mining tasks and we demonstrated by means of many experiments that the technique is effective It provides reasonable estimates given execution times that enable to probe a large number of points in the parameter space Notice that domain knowledge is also exploited here when selecting the distribution model Future directions of work include to adapt the approach to other pattern domains and to different constraints Another interesting aspect to investigate is the use of more sophisticated sampling schemes e.g that could b e incorporated in the approach when more complex syntactical constraints are handled e.g a grammar to specify the shape of the patterns Acknowledgments This work is partly funded by EU contract IQ FP6-516169 Inductive Queries for Mining Patterns and Models and by the French contract ANR-MDCO14 Bingo2 Knowledge Discovery For and By Inductive Queries We thank Dr Olivier Gandrillon from the Center for Molecular and Cellular Genetics CNRS UMR 5534 who provided the DNA promoter sequences References  J F  Boulicaut L De Raedt and H  M annila e ditors Constraint-Based Mining and Inductive Databases  volume 3848 of LNCS  Springer 2005  C  B resson C K e ime C F a ure Y  Letrillard M  B arbado S San\036lippo N Benhra O Gandrillon and S GoninGiraud Large-scale analysis by SAGE revealed new mechanisms of v-erba oncogene action BMC Genomics  8\(390 2007  L  C ao and C  Z hang Domain-dri v e n actionable kno wledge discovery in the real world In Proceedings PAKDDÕ06 volume 3918 of LNCS  pages 821–830 Springer 2006  G  D ong and J  L i Ef 036cient mining of emer ging patterns discovering trends and differences In Proceedings ACM SIGKDDÕ99  pages 43–52 1999  F  Geerts B  G oethals and J  V  d en Bussche T ight upper bounds on the number of candidate patterns ACM Trans on Database Systems  30\(2 2005  U  K eich and P  A  P e vzner  S ubtle motifs de\036ning the limits of motif 036nding algorithms Bioinformatics  18\(10 2002  S  K ramer  L De Raedt and C  Helma M olecular f eature mining in HIV data In Proceedings KDDÕ01  pages 136 143 2001  L  L hote F  Rioult and A  S oulet A v e rage number of frequent closed patterns in bernouilli and markovian databases In Proceedings IEEE ICDMÕ05  pages 713–716 2005  I  M itasiunaite a nd J.-F  B oulicaut Looking for monotonicity properties of a similarity constraint on sequences In Proceedings of ACM SACÕ06 Data Mining  pages 546–552 2006  I Mitasiunaite and J F  Boulicaut Introducing s oftness i nto inductive queries on string databases In Databases and Information Systems IV  pages 117–132 IOS Press 2007  I Mitasiunaite C Rigotti S Schicklin L  M e yniel J F  Boulicaut and O Gandrillon Extracting signature motifs from promoter sets of differentially expressed genes Technical report LIRIS CNRS UMR 5205 INSA Lyon France 2008 23 pages Submitted  G Ramesh W  M aniatty  a nd M J Zaki F easible itemset distributions in data mining theory and application In Proceedings ACM PODSÕ03  pages 284–295 2003  F  Zelezn  y Ef\036cient sampling in relational feature spaces In Proceedings ILPÕ05  volume 3625 of LNCS  pages 397 413 Springer 2005 
86 
86 


