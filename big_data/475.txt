Proceedings of the Second International Conference on Machine Learning and Cybernetics Wan 2-5 November 2003 THE STUDY OF ASSOCIATION AGORITHM BGL BASED ON BINARY SYSTEM AND ORIENTED GRAPH WEN-YUAN LIU\222, YONG-SHAN LIU\222 LINA LIU3 SHU-FEN FANG4 221Department of information YanShan University; Department of managemen Harbin Institute of Technology 2\2223Department of information YanShan University 4Department of management Harbin Institute of Technology E-MAIL:wyliu@vip 163 com Abstract The actuality of current algorithms in mining association 
rule is analyzed in this paper A new algorithm based on binary and graph is put forward The main idea and realization project of the algorithm are introduced detailed here. Then, the new algorithm having improved the efficiency of mining rules is proved by comparing the performance of all kinds of algorithms Keywords Association rules;Data mining; Binary system; Graph 1 Introduction Association rule was first introduced by R.Agrawa1 and is the important part of KDD studies\223-\222 Association rule mining is 
one of the technologies that be most studied in data mining Its task is to find out the association relations between properties of the objects in database We can complete the formalization description as follows 1 Given a transaction database D\(ID,I\Where ID is the index of customers I={il  i   is the set of all the goods 2 The transaction T is the set of goods which customers have bought, namely T c I After 
a customer Idk has completed a transaction database D will add a transaction Tk 3 For a subset X of 1 if XC T then we call the transaction T include X 4 The association rule 223X 3 Y\224is a implication where X c I Y c I and X n Y X is precondition Y is conclusion 5 Support S XI 
 where XC I X.count is number of ID in transaction database ID1 is the total number of ID in transaction database the time of transaction S\(XnY  XnY 6 Confidence C\(X 2 Y S\(X  X.count 222 whereXG1 YcI XnY<=I 7 In the practical application the association rule X Y shows statistical knowledge namely if the min-support and min-confidence has been given, then S X n 
Y  min-support S X  min-support C\(X 3 Y  The problem of association rule mining can be divided into the following two sub- question 1 Among the transaction database D to find out all itemsets which are larger than or equal to the min-support appointed by customer The itemset which meet the min-support is called max-itemset The support of a itemset is the number of the itemset 2 Using the association rule that 
was required by max-itemset production, for each max-itemset A to find out all its non-empty subset a If support\(A a then a=>\(A-a support\(A a is the confidence of the rule a=>\(A-a In fact the first problem is the central problem in the whole execution course of mining association rule After find all the maitemsets the corresponding association rules will be produced easily Many effective association rule algorithms have been developed by now Among these the two algorithms Apriori 
and Fptree have represented two different methods of receiving the selected sets Performance of a lot of algorithms is all standard with these two kinds of algorithms The last several sections of this paper have been organized like this Section 2 analyzed the key thought of above two algorithms; section 3 provides the main idea of new algorithm, section 4 analyzes the performance of all these algorithm 0-7803-7865-2/03/$17.00 02003 IEEE 172 


Proceedings of the Second International Conference on Machine Learning and Cybernetics Xi\222an 2-5 November 2003 rule A->B B->A D->A D->B A,D->B B,D->A D->A,B 2 The actuality of algorithm in mining rules confidence 100 100 100 100 100 100 100 Apriori is a algorithm which was put forward by R.Agrawal,etc after they put forward the problem of mining association rules\2224\222 222I Mining rules with multi-cycle method is a very famous algorithm in mining association rules currently While Fptree Frequent Pattern Tree was introduced by Canadian professor Jiawei Han in 2000[6-71 Its characteristic is to produce frequent set straightly and does not produce selected set These two kinds of algorithms have represented the thoughts of mining association rules in two kinds of different angles For Apriori the basic idea of searching mix-itemset is as follows. The algorithm needs deal with the database for many steps. Step 1  counting the appearing frequency of all itemsets that include one element easily, then finding out these itemsets whose support not less min-support is calledl-dimension max-itemset From step2 repeating disposing until having no max-itemset been produced. The circular process is in step k producing k-dimension candidate itemset according to k-1 max-itemset then searching database getting itemset support of candidate itemset for comparing with min-support so that achieving the k-dimension max-itemset. Using transaction as in table2-1 the mining process of Apriori is showed in figure2-1 Fptree algorithm puts forward a no-candidate-itemset mining method. It takes a kind of branch tactic: compress the database which provides frequent itemset into a frequent pattem tree FP-tree but keeping itemset associated information still then divide the database which has been compressed into a set of condition database a kind of projective database with especial type and mining each database respectively For example the integral IB I D 13 I-item frequent set item 3-item fequent set FP-tree of the transaction database based on table2 1 produced is shown in figure2-2 At the condition of having been given a support threshold we can get a simpler FP-tree such that given a support threshold 50 K6 will be cut We scan the database for the first time then we can get the support number of every item A,B,C,D,E,K Arrange them in a decreasing order by support number The result is written L. Then we get L={A1:4 B2:4 D4:3, C3:2 E5:2 K6:1 a set of one frequent-itemset. The concrete step of constructing FP-tree as figure2-2 is as follows construct the root node marked with 223null\224 Scan the database for the second time Then we deal with the items of each affair set in the order of L in a decreasing order by support number and construct a branch for every affair For example the first affair: \224T100, KADB\224, including 4 items 1 2 4 6 by the order in L lead to the first branch 1 1  2 1  4 1 6 1 where 1 connect with rood node 2 connect with 1 4 connect with 2 6 connect with 4 Similarly deal with other affair in the same way We should notice that a share prefix should be use between all paths, such as 224T200,DACEB\224 produce a path l, 2,4 3  5>, it should share 1,2 with the first branch and add up the number of node For facilitating to pass all over the tree we set up a node list in order to each item can point to a node in tree through node list The tree that was gotten after scanning all the affairs in database is a FPtree Thus, the problem of frequent pattem mining of database has been transformed into a problem of FPtree mining Figue2 1 Apriori algorithm diagrammatize 173 


Proceedings of the Second International Conference on Machine Learning and Cybernetics Xi\222an 2-5 November 2003 Table2-1 Example transaction dataset TID 1 Items-List TI00 I K A D B T200 I D A C E B T300 T400 1 B A D 1 C A B E 3 algorithm BGL based on binary system and oriented graph The efficiency of above two algorithms is not high in actual mining. This because that the shortcoming of Apriori algorithm is 1 since needing scan database many times the consumption of time is so large And because that the larger data source is always used scanning database must lead to a higher requirement in time and in space 2 The number of candidate sets maybe far greater than the nu,mber of requent sets In spite of scanning times being decreasing greatly in Fptree algorithm which being faster than Apriori algorithm by one order of magnitude, FP-tree is a structure needing an enormous space to store FP-tree is unpractical if database is very large That using recursive way in the key step of the description of the algorithm must lead to an enormous consume in space at some conditions Using the binary method to profess transaction information and producing frequent items with the idea of oriented graph the writer has designed a kind of high-efficient algorithm BGL Binary Graphic Largeset  The main idea of BGL is Let\221s suppose I={i i2 in is a set of transaction database The recorders of the database is made up of several affairs that are denoted as T={tl tZ t3  t where ti={xlxc I At first we give a define3.3 Let the transaction code of one transaction data is trans yn*2n+yn.12n-1+yn-22n-2  y12I 3 1 where i=l  n Thus, every transaction code of transaction data can be expressed a binary number whose length is the number of projects. For example, the transaction data of project C in table2 1 is 22301 10\224  sk is showed as sup={ the number of 1 in B where B yI y2 A    A yk Here A is the logical and operation of It can be easily concluded that the support degree of transaction code sup can denote rightly the frequency of projects appearing in same time The k-1 subset of any a k-item frequent item must be frequent item So if we look every k-1 frequent item as a path between k-1 if ti includs s i\221 0 if ti does not includ s Yj The support of any a itemset S={slf s2 binary system 3-2 174 


Proceedings of the Second International Conference on Machine Learning and Cybernetics Xi\222an 2-5 November 2003 relative nodes in the graph which regards all items as nodes k-item frequent item must be the new path which is formatted by adding an edge on the last k-1 nodes\222 path Among a itemset S={sn s\224  sl the position number of any of a item si P=i The item graph G\(S L of a itemset is a oriented graph which takes all items as vertex Where S={sI s2  sk L={a set of all edges v Iv=\(u e if and only if u ec S and 3-3 sup\(u e e 3-4 The progress of BGL algorithm is as follows 1 Scanning database, achieving the transaction data of every item, and cutting items according to min-support to produce I-item frequent set 2\Constructing the item graph of transaction dataset of that method is as follows comparing the supports of every two items we add an orient edge between this two nodes if their supports are not less than min-support The current item graph shows straightly the information of 2-item frequent set 3 Producing the k-item frequent set according to item graph as long as k-1 frequent set is not empty Repeating this process until k-item frequent set is not empty The method is as follows for any a k-1 frequent set sl s2 k finding out the item whose position number is the largest. Examining that whether item graph existing edges pointing to other nodes For all nodes which edges point to, computing their sup\(s1 s2 I  s If they are not less than min-support they form a k-item frequent set SI SZ  sk-1 s The correctness of BGL is proved as followed Proof 3.1 1 Firstly we prove the completeness of the searching method namely this method can find out all k-item frequent sets without omission The node which is pointed to by s the item whose position number is the largest, is what the algorithm will examine Let\222s suppose that it exist a item s whose position number is smaller than s It and k-1\frequent set\(sl s2   s can form a k-item frequent set SI  s2 7 Since any subset of a frequent set must be a frequent set we know that sl s2  si  s must be a k-1\frequent set sml s being a 2-item frequent set. For the item s whose position number is the largest in the k-1\frequent set it must exist a oriented edge pointing to s So this frequent set will be get in the deriving of si  s2  S  S 1   2 Secondly we prove the correctness. That to say only the item nodes whose position numbers are largerthari that of s and which exist oriented edges can form k-item   SI  s  frequent set Because all the pointed nodes s of s are based on b=sup\(s sJ sup\(s sI sj I b being right for any item s that knows from the character and operation in binary system So we need only consider nodes whose b meet min-support The description of BGL Algorithm 3-1 BGL Input a transaction database D a min-support Output a largest itemset Largeset\which meet the Algorithm 1\Scanning the database for the first time to produce item list and transaction data, we cumulative add a 2\222 for transaction data of every field of every recorder, remaining the order of an item list. At last we cut items according to support 2 Let\222s comparing the transaction data of every two Items a and b in turn. If the number of 1 in the result of aA b is greater than support threshold it can produce a 2-item frequent set and an oriented edge. The direction of the oriented edge is from smaller position number object to larger one 3\Then we pass every the last item of every k-item frequent set. If it exists a edge pointing to other node let\222s examine whether can form a k+l frequent set after adding this node A k+l frequent set will be produced if the condition is met Let\222s take the transaction dataset case in table2-1 for example too After the first scanning database, a structure as in fig3-1 is produced After getting the transaction data table and the item graph in stepl deriving frequent mode is transformed a process as follows all the items in every k-1 frequent set form a path in the graph We select the last node on this oriented path We know from the deriving in proof3.1 that we need only searching the nodes which this node point to If the k-item oriented path which is formed after adding this node is still a frequent item of meeting support, a frequent item is find out The table 3-2 is the item graph produced after the first scanning database It is also a 2-item frequent set because that the produce condition of each oriented edge is just the condition of 2-item frequent set On this base take A1  C3  for example, viewing the largest position number item C3 it exists a edge pointing to E5 only So only A1  C3 E5 may be produced. Computing sup 1 1 1 1 A 0 1 10 A 0 1 10 sup\(O110 It meet support and produce a 3-item frequent set minsup min-support 175 


Proceedings of the Second lnternational Conference on Machine Learning and Cybernetics X'an 2-5 November 2003 A1 Item list binarv 1111 A1 B2 D4 c3 4 1 4 2 3 3 2 4 1 Binarv transaction arry 1 t B2 1111 c3 0110 D4 101 1 I E5 K6 0110 000 1 E5 1 2 Figure 3-1 Item list and binary transaction data produced by Bgl algorithm scaned database 5 Figure3-2 Item graph produced by Bgl algorithm scanning database K6 1 1 4 algorithms in mining associated rule The analyzing and comparing of all kinds of 6 Apriori algorithm produces a higher frequent set by inter-combination in frequent items which have been found with transcendental knowledge Let's suppose that the transaction number of a database is n the largest transaction item number is m. The transactions of all items are same under an ultra circumstance. This must lead to produce 2"-2 rules Subtracting 2 means taking out D and the circumstance which all transactions are the right of rules Under this circumstance, we must scan database for m times in order to get every frequent set We need to compare for c  n times in the ist canning database So it's totally n2 times comparison is needed at this condition The complexity in time is O\(n2m\Here needs much additional space to store every frequent set The number of the is frequent set is c in the worst conditions So 2 additional space is needed The complexity in space is O\(2 Under the other circumstance each of all the i transaction is not same. That to say there is on the same elements in any two transaction This lead to that any 2-item frequent sets can not be produced Scanning database is being needed only once. The number of times of comparison is mn So the complexity in time is O\(nm Since it is only 1-item frequent set that is produced, we need only m additional storage space So the complexity in space is O\(m\algorithm records frequent mode with tree Likewise we suppose that the transaction number of a database is n the largest transaction item number is m The transactions of all items are same under an ultra circumstance. Fptree scans database only for twice When achieving the support at the time of scanning database for the first time mn times comparison is needed Scanning database for the second time is used to create the structure of tree that needs mn times So it's totally 2mn times comparison is needed at this condition The complexity in time is O\(nm We need totally m list elements and m tree nodes to be created under this circumstance So the complexity in space is O\(m\the other circumstance each of all the transaction is not same. That to say there is on the same elements in any two transaction. Fptree scans database still for twice 2mn times comparison is needed 176 


Proceedings of the Second International Conference on Machine Learning and Cybernetics Xi\222an 2-5 November 2003 Timc complexity Space complexity Time complexity Space complexity So the complexity in time is O\(nm now Since all the items of every recorder are not same, the storage space of m list elements and mn tree nodes is needed. Here the complexity in space is O\(mn algorithm reduces successfully the times of scanning database to 1 It denotes information of items in all transactions with binary system, and denotes the relations between frequent sets with a item graph With the increasing of the path between items it searches frequent sets by using transcendental knowledge time after time The using of binary system simplifies greatly the structure of representing database knowledge BGL Apriori Fptree Bgl O\(n2\224 O\(m O\(mn 221\(2\224\222 221\(m 221\(n O\(m O\(m O\(m O\(m O\(m O\(n Table3-2 Complexity compared in upwards association rule algorithms __ All the Recorder Values are same All the rccorder values are not same algorithm is fit for the circumstance which the number of items is smaller than that of transactions The complexity analysis of BGL algorithm Likewise, we suppose that the transaction number of a database is n the largest transaction item number is m The transactions of all items are same under an ultra circumstance. Bgl scans database only for once too. When achieving the support at the time of scanning database mn times comparison is needed So it\222s totally mn times comparison is needed at this condition The complexity in time is O\(nm Here needs any additional space to store every frequent set. The storage space of n binary elements is needed. The complexity in space is O\(n Under the other circumstance each of all the transaction is not same That to say there is on the same elements in any two transaction This lead to that any 2-item frequent sets can not be produced The times of scanning database are still once Thus mn times comparison is needed So now the coniplexity in time is O\(nm\Although items of each recorder are all not same because of binary array remaining all the transaction information it needs only the storage space of m binary number Here the complexity in space is O\(n We has comDared comdexitv in saace and in time of above several algorithms. The result is shown as table3-2 It can be concluded that Fptree algorithms has improved the time complexity to O\(mn While the new algorithms is mainly to reduce space complexity to O\(n 5 Conclusions This paper has analyzed the status quo of associated rules in data mining and the problems and difficulty in existence emphatically. In associated rules it is based on transcendental knowledge that is Apriori algorithm mining data But it needs scanning database for many times the consumption of time being large. Fptree algorithm stores information of transaction relation with tree-like structure It reduces the time complexity but having added the additional operation to maintain the tree structure The author puts forward a method of using binary and graph to abstractive rules A high eficiency algorithm BGL is given to mining data more efficient. It is be proved that the new algorithm is high efficient by analyzing the complexity References Fayyad U M,Piatetsky-Shapiro G,Smyth PetaLAdvances in Knowledge Discovery and Data MiningMenlo Park,CA:AAAI/MIT Press 1996 Srikant R,Agrawal R.Mining Generalized Association Rules.Daya1 U,Gray P M D,Nishio Seds.Proc of the Int\222l Conf on Very Large Databases.San Francisc0,CA:Morgan Kanfinann Press, 1995:406-419 Agrawal RJmielinski T,Swami A.Mining Association Rules Between Sets of Items in Large Databases.Bunemuu P,Jajodia Seds.Proc of the 1993 ACM SIGMOD Conf on Management of Data.New York,NY:ACM Press, 1993 :207-2 16 R.Agrawa1,Etal.Mining Association Rules between Sets of Items in Large Databases.Proc.ACM SIGMOD Int\222 1 Conf.Management of Data Washington,DC,May 1993:207-2 16 R.Agrawal,R.Srikant.Fast Algorithms for Mining Association Rules.Proc 20th Int\222 1 ConfYery Large Database, Santiago, Chile, Sept, 1994: 487-499 Han J,Et A1.Mining Frequent Patterns Without Candidate Generation,\(Slides In:Proc.2000 ACM-SIGMOD 1nt.Conf.On Management of Data\(SIGMOD\222OO Dallas, TX, May2000 Jiawei Han, Micheline Kambr.Data Mining Concepts and Techniques Canada:Morgen Kaufmann Publishers 177 


Clearly the lower the above probability  the so-called p-value  the more statistically significant the rule X  Y since it is less likely that X and Y are inde pendent In our experiments we ensure that all rules have p-values of X or less, where X is a user-specified threshold Local Pruning In addition to the global pruning described above we also prune locally on various subsets of the rules discovered Specifically our tool allows the user to browse 1 rules that demonstrate reuse of a particular library class and 2 rules that are violated in a particular application \(where the tool acts like a \223reuse lint\224 In the former case we consider only the set of rules with that library class in the antecedent or consequent In the latter case we consider only those rules that are violated by at least one class in the application of interest. Either way once we have extracted the subset of rules we follow the same local pruning procedure when presenting the results to the user The motivation for local pruning is the following given the presence of a particular rule, another rule may not be surpris ing to us In that case it is desirable to additionally prune the latter rule to focus the user\222s attention on those rules that are interesting The pruning process that follows builds upon several existing techniques 2 111 Consider rules X  y and rule X\222  y, where X\222 is a sub set of X If we know the confidence c\222 for rule X\222  y then we expect the confidence for rule X  y to also be c\221 since there is no reason to believe  without prior knowl edge of the library and/or applications  that the additional items X-X\222 in the antecedent are likely to increasefdecrease the occurrence of y. Thus we shall consider pruning X  y if its confidence c is not much greater than c\221 More specifically we set an interest threshold 6 and prune any such rule X  y where c%/c\221  6 If it is not pruned we say that X  y is interesting with respect to X\222  y We also perform anotherform of pruning Suppose we have two rules X  y and X 3  where X is an ancestor of itemset X contqning the same number of items \(where 223an cestor\224 means X  X as defined in Section 2 or Q is an ancestor of y or both In such a cye we may keep the more specific rule X  y and prune X 3 6 Generally speaking we would like to show the more specific rule which tends to be more informative However we may also wish to show the more general rule if its confidence is much greater than expected If X  y has confidence c then X  y has expected confidence c since there is no reason to believe without prior knowledge, that y is more/less likely to be in a trans action with X than one with X However as  is an an cestor of y the rule X   clearly has confidence c and possibly much more To get a reasonable estimate for ex pected support we shall assume prior knowledge of the rel ative support of y and  With such knowledge we would expect X  Q to have y  c since of those transactions that support X we would expect supp\(Q y of them to support Q By the first observation X  y has expected confidence-c Given the confidence c for X  y we would expect X   to have y  c as explained above Consequently we prune the rule X  ij if and only if supp~~~~~upp~y~~~c  6 for some interest threshold 6 Finally combining this analysis with our earlier results on smaller antecedents X\222 we shall prune a rule X  ij given confidence c\222 for X\222  y if and only if  6 This follows because the ex pected confidence for X  y is the same as the confidence c\222 for X\222  y Now we are ready to describe the complete pruning proce dure Given a set of rules XI  y1    X  yn we first construct a partial order with a node for each rule The nodes in the partial order are ordered as follows Xi  yi  Xj  yj if and only if 1 the rules are not identical 2 0 E Xi C Xj and 3 X is more specific than or equal to Xj and y is a descendent of or equal to yj Pruning proceeds by considering the nodes of the partial or der in topological sort order That is, an ancestor is always processed before its descendents In this process a rule X  y is not pruned if and only if it is &interesting with respect to all ancestors in the partial order that have survived the pruning process to that point 4 BROWSING REUSE PATTERNS In this section we shall demonstrate how one might browse and learn from generalized association rules by considering code written for the KDE desktop environment. The KDE li braries provide a C application framework for developing GUI applications. In our experiment we have mined reuse patterns for the KDE 1.1.2 core libraries \(which include the Qt toolkit by analyzing 76 real-life applications Specifically, we have used our tool CodeWeb to mine for generalized association rules with confidence of at least 10 and support of at least 15 transactions There were 1365 transactions total so the support requirement as a percentage is about 1.1 The global pruning parameters were set at y  1.25 and X  0.01 The local pruning parameter 6 was set to 1.25 Only rules with one item in the antecedent and one item in the consequent were considered To contrast generalized association rule mining with our ear lier work on standard association rule mining we also in clude the data mining results where inheritance was ignored all together On a Sparc Ultra 1 mining generalized asso ciation rules took about 50 minutes while mining standard E supp~g~~supp~y~~~c E 173 


association rules took 20 minutes The rule statistics are as follows Rules Global Pruning Mined Uninteresting Misleading Insignificant Generalized 5 1308 13299 1904 6681 Standard 21594 996 320 978 Rules Left 34271 19636 Observe that while the number of rules mined by generalized association rules is significantly more than that using stan dard association rules a greater percentage of these rules is eliminated during global pruning Of course the number of rules pruned locally varied depending upon the local context and is not shown here Typically a developer just starting out with a library would identify important library classes and browse their reuse pat terns By 223important\224 we mean those library classes that are reused in many existing applications and are thus likely to be relevant in new applications also For example, a developer using our tool would notice that the KDE classes KApplica tion and QObject are reused in 99 and loo respectively of the 76 applications mined  and are thus essential in any KDE application We consider the reuse patterns for these two classes in what follows KApplication Reuse Patterns Figure 3 shows all reuse patterns predicated on the instanti ation of KApplication in an application class The support ers of a rule are those application classes for which all rule items apply For example, an application class that supports reuse pattern 1 must instantiate KApplication and calls its member function exec We also show the detractors of a rule which are those application classes for which the an tecedent items apply but where the consequent item does not hold For example, an application class that detracts from reuse pattern 1 must instantiate KApplication and not call exec Our tool allows users to browse the source code for both supporters and detractors of reuse patterns these ap plication classes illustrate characteristic and uncharacteristic reuse respectively In Figure 3 we find  among other things  that of those applications classes that instantiate the KApplication class 72.3 call its member function exec 58.5 instantiate KT~pLevelWidget 53.8 call the member function set Mainwidget of the class KApplication and 46.2 call the show member of the class KTopLevelWidget Re call that the symbol 221A\222 indicates that some application class may reuse a strict descendent of KTopLevelWidget rather than the class itself By browsing reuse patterns in combination with library ref erence documentation which is usually available\and appli cation source code a developer can learn to use a library by example in much the same way as studying manually con structed tutorials and/or toy programs both of which may not be available For example, doing this for the rules in Figure 3 reveals that the class KApplication instantiated in the main function of most KDE applications manages the application event queue We also observe that applications inherit from KTo pLevelWidget to define the main widget of the application e.g one that is not contained in any other this widget is then instantiated and a call to setMainWidget0 tells the li brary that whenever the user closes this widget that the appli cation should terminate all together Afterwards the exec member of KApplication is called to enter the main event Finally it turns out that all applications that instantiate KTo pLevelWidget always instantiate a descendent that they de fine Without taking into account the inheritance hierarchy e.g using generalized association rules we would miss reuse patterns involving this class all together QObject Reuse Patterns The class QObject is an ancestor of almost all classes in the KDE libraries According to the reference documentation this class provides facilities for event handling and timing operations Figure 4 shows some of the reuse patterns re ported by our tool for QObject of the 53 rules found for this class 47 involve a 221*\222 symbol in the antecedent and/or consequent It turns out that application classes rarely reuse QObject directly; typically they reuse it indirectly through one of its descendents Although QObject is very funda mental to the KDE libraries only six rules would have been found without taking into account the inheritance hierarchy 5 RELATEDWORK In this paper we have looked for pattems in the way library classes have been reused in practice by existing applications In this section we shall talk about several related techniques Exemplars An exemplar is an executable visual model consisting of one or more instances of at least one concrete class for each ab stract class in a library 6 By browsing these classes as well as their static relationships and dynamic interactions one can get a general understanding of how the framework works in a small example While an exemplar may be helpful it is a pre-selected toy example that may not be representative of 223real-life\224 appli cations Moreover, exemplars place an extra burden on the developers of the software library In contrast our approach allows the user to browse reuse patterns and the correspond ing supporter and detractor classes in real-life applications Moreover, the tool is automated and works on any existing code Reengineering Libraries Recently research has been done on reengineering libraries by analyzing their usage in several existing applications lo This is done by constructing a lattice that provides insights loop 174 


lllllll_l  Figure 3 KApplication reuse patterns Clicking on kasteroids a supporter of reuse pattern 1 yields the code on the right Figure 4 QObject reuse patterns 175 


into the usage of the class hierarchy in a specific context Such a lattice can be used to reengineer the library class hi erarchy to better reflect standard usage In contrast we are interested in helping novice users learn to use a library to write new applications  not reengineer the library itself This different perspective has led us to 1\initiate a new search direction for mining code for the purposes of illus trating characteristic code reuse 2 use data mining tech niques that scale to a hundred or more applications  not just a few examples for which confidence and support mea sures would not be meaningful 3 look for different kinds of reuse patterns that are more helpful for demonstrating reuse of the library classes and 4 construct a tool whose user in terface is aimed at users of a software library rather than its developers Other Work involving Data Mining Researchers have used data mining and related techniques for a variety of purposes. For example, data mining has been used to discover likely program invariants infer spec ifications in software 3 and decompose a software sys tem into data cohesive subsystems to assist developers with reengineering and maintenance tasks 4 The last of these is the only other work we are aware of that uses association rule mining in the software engineering domain 6 CONCLUSIONS AND FUTURE WORK In this paper, we have shown how data mining can be used to discover library reuse patterns in existing applications Specifically we considered the problem of discovering li brary classes and member functions that are typically reused in combination by application classes This paper improves upon our earlier research using \223associ ation rules\224  by taking into account the inheritance hier archy using \223generalized association rules\224 This has turned out to be non-trivial due to the significantly larger number of rules that arise as a result Consequently, pruning is impor tant and we showed several ways in which it can be done By browsing generalized association rules a developer can discover patterns in library usage in a way that takes into ac count inheritance relationships We have illustrated the ap proach using our tool CodeWeb by demonstrating charac teristic ways in which applications reuse classes in the KDE application framework We have observed that some impor tant rules would not have been found without taking into ac count the inheritance hierarchy I One can view our general approach to mining reuse patterns as learning from positive experience That is, library reuse that has worked in practice Presumably, one would se lect 223stable\224 applications to demonstrate reuse patterns in a library However one can also mine negative experi ence That is misunderstandings and problems that came up when reusing components from a software library For future work it would be interesting to determine if one can  mine negative experience in an automated way perhaps by analyzing application CVS logs for reuse patterns that were problematic and later corrected REFERENCES  11 R Agrawal and R Srikant Fast algorithms for mining association rules In Proceedings of the 20th Very Large Data Bases Conference pages 487499,1994 2 M Chen J Han and P S Yu Data mining An overview from a database perspective ZEEE Transac tions on Knowledge and Data Engineering 8\(6 883,1996 3 W W Cohen. Inductive specification recovery Under standing software by learning from example behaviors Automated Software Engineering 2\(2 107-129,1995 4 C Montes de Oca and D L Carver Identification of data cohesive subsystems using data mining tech niques. In Proceedings of the Zntemational Conference on Software Maintenance pages 1623,1998 5 M D Ernst J Cockrell W G Griswold and D. Notkin. Dynamically discovering likely program in variants to support program evolution. In Zntemational Conference on Software Engineering pages 2 13-224 1999 6 D Gangopadhyay and S Mitra. Design by framework completion Automated Software Engineering 3:219 237,1996 7 N Megiddo and R Srikant Discovering predictive as socation rules In Proceedings of the 4th International Conference on Knowledge Discovery in Databases and Data Mining 1998 8 A Michail Data mining library reuse patterns in user selected applications In 14th ZEEE Zntemational Con ference on Automated Software Engineering pages 24 33, 1999 9 G Piatetsky-Shapiro and W J Frawley Knowledge Discovery in Databases AAAVMIT Press 1991  101 G Snelting and F Tip. Reengineering class hierarchies using concept analysis In 6th ACM SIGSOFT Intema tional Symposium on the Foundations of Software En gineering pages 99-1 10,1998  113 R Srikant and R Agrawal Mining generalized associ ation rules In Proceedings of the 21st Very Large Data Bases Conference 1995  121 Will Tracz Confessions of a Used Program Salesman Institutionalizing Sofhyare Reuse Addison-Wesley 1995 176 


18001  balancing mechanism which requires further investi gation 4.5 Speedup Figure 12 shows the speedup ratio for pass 2 vary ing the number of processors used, 16 32 48 and 64 where the curve is normalized with the 16 processor execution time The minimum support value was set to 0.4 4.5 0.5 1 1 0 I 10 20 30 40 50 60 70 number of mxessors Figure 12 Speedup curve NPA HPA and HPA-ELD attain much higher lin earity than SPA HPA-ELD an extension of HPA for extremely large itemset decomposition further in creases the linearity HPA-ELD attains satisfactory speed up ratio This algorithm just focuses on the item distribution of the transaction file and picks up the extremely frequently occurring items Transferring such items could result in network hot spots HPA-ELD tries not to send such items but to process them locally. Such a small mod ification to the original HPA algorithm could improve the linearity substantially 4.6 Effect of increasing transaction Figure 13 shows the effect of increasing transac tion database sue as the number of transactions is increased from 256,000 to 2 million transactions We used the data set t15.14 The behavior of the results does not change with increased database size The minimum support value was set to 0.4 The num ber of processors is kept at 16 As shown each of the parallel algorithms attains linearity 5 Summary and related work In this paper we proposed four parallel algorithms for mining association rules A summary of the four database size Sizeup 0 I 0 500 loo0 1500 uxw amount of transaction thousands Figure 13 Sizeup curve algorithms is shown in Table 5 In NPA the candi date itemsets are just copied amongst all the proces sors Each processor works on the entire candidate itemsets NPA requires no data transfer when the supports are counted However in the case where the entire candidate itemsets do not fit within the mem ory of a single processor the candidate itemsets are divided and the supports are counted by scanning the transaction database repeatedly Thus Disk 1/0 cost of NPA is high PDM, proposed in 6 is the same as NPA which copies the candidate itemsets among all the processors Disk 1/0 for PDM should be also high The remaining three algorithms SPA HPA and HPA-ELD partition the candidate itemsets over the memory space of all the processors Because it better exploits the total system's memory, disk 1/0 cost is low SPA arbitrarily partitions the candidate itemsets equally among the processors Since each processor broadcasts its local transaction data to all other pro cessors the communication cost is high HPA and HPA-ELD partition the candidate itemsets using a hash function which eliminates the need for transac tion data broadcasting and can reduce the comparison workload significantly HPA-ELD detects frequently occurring itemsets and handles them separately which can reduce the influence of the workload skew 6 Conclusions Since mining association rules requires several scans of the transaction file its computational requirements are too large for a single processor to have a reasonable response time This motivates our research In this paper we proposed four different parallel algorithms for mining association rules on a shared nothing parallel machine and examined their viabil 29 


Table 5 characteristics of algorithms ity through implementation on a 64 node parallel ma chine the Fujitsu AP1000DDV If a single processor can hold all the candidate item sets parallelization is straightforward It is just suf ficient to partition the transaction over the proces sors and for each processor to process the allocated transaction data in parallel We named this algo rithm NPA However when we try to do large scale data mining against a very large transaction file the candidate itemsets become too large to fit within the main memory of a single processor In addition to the size of a transaction file a small minimum support also increases the size of the candidate itemsets As we decrease the minimum support computation time grows rapidly but in many cases we can discover more interesting association rules SPA HPA and HPA-ELD not only partition the transaction file but partition the candidate itemsets among all the processors We implemented these al gorithms on a shard-nothing parallel machine Per formance evaluations show that the best algorithm HPA-ELD attains good linearity on speedup by fully utilizing all the available memory space which is also effective for skew handling At present we are doing the parallelization of mining generalized association rules described in 9 which includes the taxonomy is-a hierarchy Each item belongs to its own class hierarchy In such mining associations between the higher class and the lower class are also examined Thus the candidate itemset space becomes much larger and its computation time also takes even longer than the naive single level association mining Parallel pro cessing is essential for such heavy mining processing Acknowledgments This research is partially supported as a priority research program by ministry of education We would like to thank the F\221ujitsu Parallel Computing Research Center for allowing us to use their APlOOODDV sys tems References l R.Agrawal T.Imielinski and ASwami 223Min ing Association Rules between Sets of Items in Large Databases\224 In Proc of the 1993 ACM SIGMOD International Conference on Manage ment of Data pp207-216 May 1993 2 R.Agrawal and RSrikant 223Fast Algorithms for Mining Association Rules\224 In Proc of the 20th International Conference on Very Large Data Bases pp.487-499 September 1994 3 J.S.Park M.-S.Chen and P.S.Yu 223An Effec tive Hash-Based Algorithm for Mining Associ ation Rules\224 In Proc of the 1995 ACM SIG MOD International Conference on the Manage ment of Data SIGMOD Record Vo1.24 pp.175 186 June 1995 4 H.Mannila H.Toivonen and A.I.Verkamo 223Ef ficient Algorithms for Discovering Association Rules\224 In KDD-94:AAAI Workshop on Knowl edge Discovery in Databases pp.181-192 July 1994 5 A.Savasere, E.Omiecinski and S.Navathe 223An Effective Algorithm for Mining Association Rules in Large Databases\224 In Proc of the 21th International Conference on Very Large Data Bases pp.432-444 September 1995 6 J.S.Park M.-S.Chen and P.S.Yu 223Efficient Parallel Data Mining for Association Rules\224 In Proc of the 4th International Conference on In formation and Knowledge Management pp.31 36 November 1995 7 T.Shintani and M.Kitsuregawa 223Considera tion on Parallelization of Database Mining\224 In Institute of Electronics Information and Com munication Engineering Japan SIG CPS Y95 88 Technical Report Vo1.95 No.47 pp.57-62 December 1995 8 T.Shimizu T.Horie and H.Ishihata 223Perfor mance Evaluation of the APlOOO Effects of message handling broadcast and barrier syn chronization on benchmark performance-\224  In S WO PP 22292 9.2 ARC 95 Information Processing Society of Japan Vo1.92 No.64 1992 9 R.Srikant and R.Agrawal 223Mining Generalized Association Rules\224 In Proc of the 21th Inter national Conference on Very Large Data Bases pp.407-419 September 1995 30 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


