A Parallel Computing Platform for Training Large Scale Neural Networks Rong Gu Furao Shen Yihua Huang National Key Laboratory for Novel Software Technology Nanjing University Nanjing China 210093 Email gurong@smail.nju.edu.cn  frshen yhuang  nju.edu.cn Abstract Artiìcial neural networks ANNs have been proved to be successfully used in a variety of pattern recognition and data mining applications However training ANNs on large scale datasets are both data-intensive and computationintensive Therefore large scale ANNs are used with reservation for their time-consuming training to get high precision In this paper we present cNeural a customized parallel computing platform to accelerate training large scale neural networks with the backpropagation algorithm Unlike many existing parallel neural network training systems working on thousands of training samples cNeural is designed for fast training large scale datasets with millions of training samples To achieve this goal rstly cNeural adopts HBase for large scale training dataset storage and parallel loading Secondly it provides a parallel in-memory computing framework for fast iterative training Third we choose a compact event-driven messaging communication model instead of the heartbeat polling model for instant messaging delivery Experimental results show that the overhead time cost by data loading and messaging communication is very low in cNeural and cNeural is around 50 times faster than the solution based on Hadoop MapReduce It also achieves nearly linear scalability and excellent load balancing Keywords parallel computing neural network big data fast training distributed storage I I NTRODUCTION Artiìcial neural networks ANNs have been used in a variety of data mining and pattern recognition applications such as protein structure analysis speech recognition image and signal processing handwriting recognition Ho we v e r  the process of training large scale neural networks is both computation-intensive and data-intensive On one hand an entire training workîow usually needs to carry out thousands of epochs iteration which makes it computationally expensive On the other hand in order to generate solid results large scale training datasets are usually used in applications As a result training large scale neural networks on a single PC is usually very time-consuming which may take several days to weeks to nish and sometimes even can not be done Thus the slow training speed of large scale neural networks has limited their use for processing many complex and valuable problems in practice On the other side the amount of data in real world has been exploding since last several years and analyzing big data becomes quite popular and necessary in many knowledge discovery related areas The big data situation is either true for neural networks From the intuition it is usually recognized that training over large scale samples leads better learning results than over small amount of samples Thus for those neural network-based applications training large scale neural networks plays an important role in achieving optimal precisions and results In this paper we design and implement cNeural a customized parallel computing platform for training large scale neural networks In cNeural a training workîow can be divided into two phases training data loading and training process executing To reduce the time cost of data loading we store the large scale training datasets in HBase and concurrently load one of them into the memory of computing nodes across the cluster when needed Also a parallel inmemory computing framework is adopted for fast iterative training During the entire training process computing nodes need to communicate with each other for cooperation and further processing We employ Apache Avro RPC to build an event-driven messaging communication framework in cNeural for its high communication efìciency and rich data structures Our platform can be deployed on commodity hardware Amazon EC2 or even general PCs interconnected with network This paper is organized in eight sections Section II describes the related work In section III we present the background of neural network along with the back propagation training algorithm In section IV we introduce the parallel training framework and algorithm used in cNeural In section V we describe data storage mechanism adopted to support fast training Then we illustrate the architecture overview and major components of cNeural in section VI Section VII performs evaluations Section VIII concludes this paper II R ELATED W ORK Many researchers have been dedicating to implementing computationally expensive ANN algorithms on parallel or distributed computing systems The related work can be traced back to the 70s of last century and the research in this area keep growing today In early time researchers prefer to adopt special-purposed hardware to improve training speed which is classiìed as neurohardware or neurocomputers in Glesner and Pochnuller presented a general o v ervie w o f special 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 376 


purpose hardware in their book Special-purposed implementations can be fast and efìcient However they offer very little exibility and scalability After the1990s designing parallel neural networks over general-purposed architectures such as parallel computing model or grid computing model became the mainstream 13 These systems are mostly implemented on clusters and multi-processor computers However these previous work made few efforts on managing large scale training datasets They usually focused on studying how to parallelize neural network training and only perform experiments with several thousands of training samples and Megabyte-class in data size 15 16 17 In recent years some researchers study on training neural networks over big data stores lar ge scale datasets on HDFS and trains them by MapReduce However Hadoop is designed for processing ofîine data-intensive problems but not for computation-intensive problems Thus the speed of training ANNs on Hadoop is slow GPU has also been used for ANN training but the training datasetês size is limited to GPUês global memory Study in 18 per formed the large scale unsupervised feature learning for building features from unlabeled data They spent a lot of effort on training algorithms such as model parallelism and asynchronous stochastic gradient descent Unlike the studies above cNeural not only considers the parallel algorithms for speedup neural network training but also spent efforts on big data management to better support the fast execution of these parallel algorithms As Hadoop is not suitable for iterative processing many studies proposed methods to improve it such as Twister  and HaLoop 20 The y try to reduce the time cost on job initialization or support caching of data in-memory at nodes between iterations proposed Spark a totally new distributed system for parallel in-memory computing Compare with these processing engines cNeural also implemented parallel neural network training algorithms in it The underlying processing engine of cNeural holds the similar idea as in-memory computing Moreover we adopted a customized implementation for better support our on-top algorithms and applications III B ACKGROUND In this section we give a brief introduction to a widely used neural network training algorithm the backpropagation algorithm We take multilayer perceptron as a typical example to describe the training algorithm The feed-forward back propagation neural network is one of the most popular neural network architectures in use today 4 pro v e d that three-layer feed-forw ard neural networks such as multilayer perceptrons trained by the backpropagation algorithm could approximate any continuous non-linear functions by arbitrary precision with enough hidden neurons Thus a three-layer feed-forward perceptron is introduced here for describing relevant algorithms The structure of a three-layer perceptron is given in Fig 1 It includes an input layer a hidden layer and an output layer The neurons in the same layer are not interlinked while the neurons in adjacent layers are fully connected with weights and biases     Figure 1 Structure of a three-layer multilayer perceptron with the backpropagation algorithm Backpropagation BP with gradient-descent technique is one of the most widely used algorithms for supervised training multilayer feed-forward neural networks The backpropagation algorithm has two phases i.e the forward phase and the backward phase In the forward phase the input layer receives input signals and propagates the information to each of neurons in the hidden layer Then the hidden layer processes the information in local and nally propagates the processed information to the output layer For an input vector x  x 1 x 2   x m   the input and output information of each neuron in hidden layer denoted as u j and h j  can be carried out by the 1 and 2 respectively u j  m  i 1 W ij x i   j j 1  2   q 1 h j  f  u j  1 1+exp  u j  j 1  2 q 2 Where W ij is the weights between input neuron i and hidden neuron j  and  j is the bias The output layer also needs to process the input information it gets from the hidden layer The input l k and output c k of each neuron in the output layer is calculated using 3 and 4 l k  q  j 1 V jk h j   k k 1  2   n 3 377 


c k  f  l k  1  1+exp  l k  k 1  2 n 4 Where V jk is the weights between hidden neuron j and output neuron k  and  k is the bias That is the end of one-pass information forward process The weights W  V and biases    does not change during forward stages If the actual output of the neural network is equal to the expected output of the input vector then a new input vector will be put into the neural network and the forward phase restarts otherwise the algorithm enters the backward phase The difference between the actual output and the expected output is called error During the backward phase errors of neurons d k in the output layer are computed using 5 at rst Then errors of neurons e j in the hidden layer are carried out using 6 d k  y k  c k  c k 1  c k  k 1  2   n 5 e j  n  k 1 d k V jk  h j 1  h j  j 1  2 q 6 The errors are propagated backward from the output layer to the hidden layer and the connection weights between the layers are updated with the back propagated errors using 7 and the weights between the hidden layer and input layer are revised using 8 V jk  N 1 V jk  N   1 d k  N  h j  k  N 1  k  N   1 d k  N  7 W ij  N 1 W ij  N   2 e j  N  x i  j  N 1  j  N   2 e j  N  8 In the above equations where i 1  2   m  j  1  2   q and k 1  2   n   1 and  2 are the learning rates ranging from 0 to 1 N is the training epoch ID Generally the BP algorithm has two modes for weights updating the online mode and the batch mode In the online mode training samples are processed one by one In the batch mode the training process is carried out for all the training samples in batch The generated  W   W here represents the changed value of W V   between two epoches of each sample is accumulated in one epoch After that the cumulative  W is used for revising the weights of connected layers together The training work continues until the terminating condition is satisìed The mostly adopted terminating condition is that the mean square error gets lower than the speciìed threshold or the training epoch reaches the limited round To calculate the total error the whole training dataset needs to be propagated through the neural network This results in slow training speed for the backpropagation training algorithm when dealing with large training datasets IV P ARALLEL N EURAL N ETWORK T RAINING A LGORITHM I NC N EURAL In this section we rstly analyze the widely used parallel training strategies Then the parallel training algorithm in cNeural along with its parallel computing framework is introduced A Analysis of Parallelization Strategies for Training Neural Network There are many parallelization approaches to accelerate training neural networks Most approaches can be classiìed into two categories namely the node parallelism and training dataset parallelism The node parallelism is neural network structure oriented These approaches achieve parallelism by mapping neurons into different computing nodes for pipeline processing Each computing node only takes charge of a part of the computation of a neural network The methods proposed by 8 and 9 adopt this approach Conversely in training dataset parallelism each computing node has the complete neural network in local and conducts computation for entire neural network The training dataset is divided into several subsets and the subsets are assigned to different computing nodes for parallel processing Different parallelization approaches t in different scenarios For the node parallelism every training sample needs to be handled down between computing nodes step by step for processing It is usually used when training datasets are small and neural network structures are complicated This kind of approaches is suitable to be implemented over the multi-core or many-core architectures which have low communication cost When implemented in distributed systems with large amount of training samples the system will be overwhelmed by the overhead of the I/O and cluster network communication costs As the I/O and network communication are usually the major time costs in distributed environments this kind of approaches is not efìcient Therefore the node parallelism approach is not appropriate to be used in a distributed computing environment Similar conclusions are also drawn in On the other side for training data parallelism each training data subset is processed on one computing node and does not need to be transmitted to the other computing nodes during the whole training process Therefore the training dataset parallelism approach is suitable for processing large scale training dataset in distributed systems as it can signiìcantly reduce data access and network communication costs B Parallel BP Alogrithm and Computing Framework in cNeural cNeural focuses on training large scale datasets For reducing the time cost of accessing and transferring training data we adopt the training dataset parallelism as our 378 


basic parallel approach We build a distributed and parallel computing framework that implements the batch mode backpropagation algorithm based on the training dataset parallelism approach This parallel computing framework is in a typical master-worker parallel model It includes one master node and n computing nodes as workers The main duty of the master node is to coordinate the whole training process The actual training process is conducted on computing nodes Before training the subsets of the training dataset are distributed to the computing nodes Each computing node contains the entire neural network and takes charge of training its local training subset The parallel training algorithm used in cNeural is described in Fig 2                               Figure 2 The parallel training algorithm used in cNeural Dash line means synchronization point As shown in Fig 2 the master node and computing nodes need to initialize themselves rstly After the initialization phase the master node broadcasts initial weights W to all computing nodes When receiving the W  each computing node simultaneously performs the training task over its local training data subset It conducts the processing of each sample in the subset for both the forward phase and backward phase During local training each computing node also needs to accumulate the  W local i of each local training sample When nishing its training task each computing node sends the  W local of its local training data subset to the master node On the master side after receiving the  W local from all the computing nodes it applies all these  W local to the W of the previous epoch to update the weights The whole training process is iterative Finally it checks whether the training termination condition is satisìed If satisìed the whole training process stops otherwise the next training epoch begins V T RAINING D ATA S TORAGE AND L OADING The time cost by loading and transferring large scale training datasets can be regarded as the major overhead of the entire training process In this section we discuss the training data storage mechanism that adopted in cNeural to support fast large scale data loading and training The storage model of cNeural is shown in Fig 3 We choose Apache HBase a distributed data management system modeled after Googleês BigTable to store the training datasets The training datasets are organized as tables in HBase Each table contains a training data set with varying number of regions according to its size For one training dataset each sample is stored as a record in a table with the sample ID as the record rowkey The content of the sample lies in the content eld of the record This way even large scale training datasets with billions of samples can be easily stored and managed During initialization of a training process computing nodes can access regions on different storage nodes simultaneously over network Therefore this underlying scalable and distributed storage can not only solve the problem for large scale dataset storage but also help to reduce the time cost of data loading by loading training datasets in parallel                       Figure 3 The data storage model of cNeural Straight arrow lines indicate the transmission of training data The loop arrows indicate that the data access repeats for many times during a training process Moreover after a dataset is loaded for training it needs to be frequently accessed during the thousands of training epochs During the training process each node resides its training data subset in memory We refer this storage mechanism as load once read many  In case the data is too large can be totally held in memory cNeural will cache part 379 


of them in the clusterês memory to avoid loading the whole data from HBase each time.In fact cNeural can always offer enough memory storage capacity to store the training dataset in use By avoiding repeated data loading and network transmission this in-memory mechanism can signiìcantly improve the training performance VI S YSTEM A RCHITECTURE A ND I MPLEMENTATION A System Overview Fig 4 shows the system architecture of cNeural as well as the processing workîow when receiving a training request from the end user Logically  cNeural consists of four types of modules including a client node a master node n storage nodes and m computing nodes The latter two can also be referred as worker nodes In reality  one machine can play multi nodes role at the same time For example in our cluster machines both act as storage nodes and computing nodes Network communications among these nodes are implemented based on Avro RPC The data storage management and transmission between computing nodes and storage nodes are supported by HBase To better explain major components of cNeural along with their functions in more detail we walk through a complete execution process of a training request from a user in next subsection B Modules 1 Client Node To perform a training job in cNeural users need to set some important training conìgurations through a simple program These conìgurations include the table name of training data stored in HBase neural network settings such as the number of hidden layers neuron numbers in each layer and training termination conditions After nishing the conìguration the client node packs related information to generate a training job and submit it to the master node If the job is submitted successfully the client node waits for the training states and results generated by cNeural Users can watch the online status of the training job on the clientês console 2 Master Node The master node is the most complicated module in cNeural It is responsible for managing and coordinating the whole training process After receiving a training job the master node splits it into several training tasks according to the size of the training dataset and the number of computing nodes The training dataset is partitioned into n data subsets Each computing node takes charge of one training task with its corresponding data subset In cNeural each machine can host k computing nodes The number k is determined by the core number and memory capacity of the machine In our implementation k  min  core  number memory capacity/quota   the quota denotes the least memory capacity reserved for each computing node and this parameter is conìgurable After nished the job splitting and task assignment the master node is responsible for coordinating the training tasks Firstly it notiìes each computing node to load its corresponding training data subset from HBase The training data loading happens among computing and storage nodes in parallel After receiving the reply informing the completion of data loading from all computing nodes then the master node starts to coordinate the parallel training process among computing nodes It also takes charge of reporting running states and nal results to the client node during the whole process 3 Computing Node The responsibility of the computing nodes is running training tasks When receiving a training task request from the master node the computing node sets up the neural network and loads its training data subset from the corresponding storage nodes through HBase interfaces After that it notiìes the master node that it has done the initialize work When all computing nodes nish the initialization work the master node starts the training work by sending weights to each computing node After that at each epoch the computing nodes execute the training tasks with the epochês weights over its training data subset During the entire training process the training data subsets are resided in the memory of the computing nodes For each training sample in a training data subset the computing node computes out  W local i and accumulates it The average of all accumulated  W local i on each computing node will be calculated denoted as  W local  and sent to the master node After receiving  W local from all computing nodes the master node updates the parameters by applying all  W local to the W of last epoch Then the master node starts a new training epoch by sending the updated parameters to each computing node When the terminating conditions are satisìed the whole iterative training process terminates and the all related resources on master node and computing nodes will be released 4 Storage Node In cNeural we utilize HBase to store training datasets The storage nodes of cNeural are region servers in HBase Each region server can interact with the computing nodes independently It can control the transmission of its local training data to the computing nodes without interacting to the master node in HBase In cNeural each training dataset is organized as a table that consists of many regions These regions are stored across the region servers and can be moved to any region servers through programming APIs We implement a utility to balance the regions of one table across the storage nodes for improving parallel data loading performance in cNeural C Features Here we conclude some key features of cNeural as below  Scalability When a new machine is added into cNeural it takes a share of the training dataset from the existing machine Adding more computing nodes leads less data for each computing node and faster processing speed  380 


                                                  Figure 4 Architecture of cNeural and an overview of the training executionês workîow The loop arrows indicate that the information are transferred continuously in a training job Users can employ cNeural from simple interfaces without knowing detailed components and internal mechanisms  Failure Recoverability At the end of each training epoch cNeural saves the most recent parameters like weights into log les A training job may fail for various faults like machine crash etc  During recovering process cNenural only needs to restart the job with the most recent parameters recorded in log By this method the training work continues running at the most recent failure point rather than totally from the scratch  Load Balance A computing node is a logical computing resource concept in cNeural In reality a machine can host several computing nodes according to its hardware capability Also in a heterogeneous cluster machines can host different suitable number depends on their hardware resource capability This way helps lead to load balance in cNeural VII E VALUATION In section we evaluate the performance of cNeural in detail In cNeural a training job has two phases large scale training data loading and executing training process Thus we design two groups of experiments for evaluating the performance of them respectively The rst group of experiments are conducted to evaluate the performance of training data loading The experiments in the second group are carried out to evaluate the training performance In addition we conduct several comparative experiments to compare the training performance of the MapReduce approach proposed by with cNeural All the e x ecution time presented here is the average of 10 runs A Experiment Setup There are 37 nodes in our experimental cluster Each node is equipped with 2 of the Intel\(R Quad Core E5620 Xeon\(R CPU at 2.4GHz with 24GB memory and 4TB 7200r/s hard disks All nodes are connected with each other by 1 Gigabit Ethernet Among them one is reserved as the master machine and the rest are slave machines The operating system of the machines is Red Hat Linux 6.0 The version of the HBase adopted in cNeural is 0.92 and the version of employed Hadoop for comparative experiments is 1.0.3 Both cNeural and Hadoop run with OpenJDK 1.6 with the same JVM heap size 16 GB 1 Datasets and Neural Network Conìguration For training dataset we use the MNIST dataset with 2 million of samples for experiments downloaded from http://www.csie ntu.edu.tw  cjlin/libsvmtools/datasets/multiclass.html MNIST dataset is a database of handwritten digits for training handwriting recognition systems Each digit is a training pattern consisting of 784 size-normalized features and a classiìcation label ranges from 0 to 9 In our test the employed training datasets contain 0.5 million 1 million and 2 million samples and their physical data sizes are 1.2 GB 2.4 GB and 4.8 GB stored in compressed coded style During the training process they need to be decoded and 381 


the corresponding sizes are 4 to 5 times larger and reach around 20 GB The training work is performed on a threelayer perceptron with the structure as 784-40-10 meaning 784 neurons in the input layer 40 neurons in the hidden layer and 10 neurons in the output layer 2 System Setup cNeural The client node and master node of cNeural both run on the master machine Each slave machine consists of 8 computing nodes and one storage node Each storage node is maintained by an RegionServer daemon in HBase Thus there are 288 computing nodes in total As each slave machine is conìgured 16 GB memory each computing node has 2 GB memory and it is enough for storing the training data subset in use MapReduce Approach The master machine acts as the JobTracker and NameNode And each of the rest 36 slaves both act as TaskTracker and DataNode Each machine is conìgured 8 slots There are also 288 slots in total and each slotês Child JVM is also conìgured 2 GB as each computing node in cNeural B Performance of Training Data Loading In this experiment cNeural is build up by 1 2 4 up to 36 machines respectively Its inner HBase consists of 36 region servers During data loading the computing nodes access HBase concurrently The experiment is performed on cNeural with different number of machines each with 8 computing nodes and different size of dataset The curves in Fig 5 shows the load time for the different sizes of training samples respectively The experiment with 2 million training samples on 1 machine failed to proceed due to insufìcient memory to load all training data Obviously as the size of input samples increases the time cost on training data loading increases too When the input sample size is xed the loading speed increases almost linearly with machine number This is because that the inner HBase provides concurrent data access and when new machines added in the system they takes a share of loading the training dataset However when the number of machines increases to some degree etc 20 in the case of test with 1 million training samples the performance of data loading can hardly be further improved This because the network transmission I/O blocking on storage nodes in HBase become bottlenecks To get valid results neural network training usually needs to run thousands of epochs And in cNeural the computing nodes only need to load data from HBase once Fig 6 illustrates the percentage of time cost on training data loading with varying training epochs The time cost on training data loading is xed Therefore the proportion of training data loading time decreases as training epochs increases With 36 machines when training epochs reach 200 the time cost in training data loading accounts only 0.3 of the total execution time The experimental results demonstrate that the time cost of data loading is very little in cNeural  0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 0 10 20 30 40 50 60 70 80     Number of machines \(each has 8 cores Load time \(seconds       Load 0.5 million training samples   Load 1 million training samples Load 2 million training samples   Figure 5 Performance of training data loading with different number of machines on different size of training samples The experiment failed to load 2 million training samples with only one machine This is the case that the training work can not be processed on a single machine 1 5 10 20 50 100 200 500 0 10 20 30 40 50 60 70 80 90 100 Percentages of time cost Number of training epoches Data loading Training Figure 6 Percentages of time cost of data loading and training with different training epochs on 1 million training samples and 36 machines C Performance of Executing Training Process The performance of executing training process is in shown in Fig 7 We can see that with the same number of machines training 0.5 million samples is always about one time faster than training 1 million samples and three times faster than training 2 million samples This indicates that time cost of our parallel algorithm increases near linear with the number of input training samples On the other perspective if the size of training data is xed the training speed of cNeural is much improved when more machines are utilized For 1 million training samples when the numbers of machines are 2 4 8 16 32 the respective time costs of one training epoch are 18.3 seconds 9.2 seconds 4.7 seconds 2.4 seconds 1.3 seconds almost getting half of time cost decrease This shows that the training process of 382 


cNeural achieves good speedup performance  0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 0 4 8 12 16 20 24 28 32 36 40     Number of machines \(each has 8 cores Training time per epoch \(seconds   Train 0.5 million training samples   Train 1 million training samples Train 2 million training samples   Figure 7 Performance of each epochês training time cost in cNeural with various numbers of machines and different sizes of training samples 124681012141618202224262830323436 0 4 8 12 16 20 24 28 32 36 40 Execution time per epoch seconds Number of machines each has 8 cores communication time computation time Figure 8 Time cost of communication and computation during a training epoch with different number of machines The histogram in Fig 8 shows the time cost by communication and computation during each epoch with different machines used respectively When more machines adopted the proportion of communication time cost increases However it always only occupies a small proportion Moreover the whole execution time still decreases D Comparative Experiments We compared the performance of cNeural with the approach proposed by as the y also tar get to w ards training neural networks with large scale training datasets This approach adopts HDFS to store large scale training data and uses MapReduce as the parallel processing engine The comparative experiments are conducted with identical number of machines and the same size of training dataset Both the execution performance and speed scalability are evaluated here Table I E XECUTION TIME  IN SECOND  ON M AP R EDUCE APPROACH AND C N EURAL WITH 1MILLION SAMPLES  Machine MapReduce appraoch cNerual Speedup ratio 1 784.0 36.3 21.6 6 177.0 6.3 28.2 12 107.0 3.1 34.1 18 95.0 2.4 40 24 82.0 1.6 50.5 30 79.0 1.4 57.9 36 60.0 1.2 52.2 1 Training Execution Performance As shown in Table I cNeural achieves around 50 times speedup over the MapReduce approach under the same conditions This improvement can be attributed to two facts First cNeural resides the training data across computer nodes memory when training however the MapReduce approach needs to reload the training data from hard disks in each training epoch Second in messaging communication cNerual adopts a event-driven model while the MapReduce approach uses the heartbeat polling model The former makes the training process more compact 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 0 5 10 15 20 25 30   Number of machines \(each has 8 cores Speed up   Hadoop MapReduce   cNeural Figure 9 Speedup salability of cNeural and Hadoop MapReduce approach with different number of machines 2 Speedup Scalability We also tested the speedup scalability of both systems with varying machine numbers The experimental results are shown in Fig 9 On one side they both achieve good speedup scalability within 10 nodes in this algorithm On the other side the speedup of the MapReduce approach failed to keep linear as cNeural because of the 383 


overhead of job initialization in Hadoop is much larger than cNeural VIII C ONCLUSION AND F UTURE W ORK The past several years have witnessed an ever-increasing growth speed of data To address large scale neural network training problems in this paper we proposed a customized parallel computing platform called cNeural Different from many previous studies cNeural is designed and built on perspective of the whole architecture from the distributed storage system at the bottom level to the parallel computing framework and algorithm on the top level Experimental results show that cNeural is able to train neural networks over millions of samples and around 50 times faster than Hadoop with dozens of machines In the future we plan to develop and add more neural network algorithms such as deep belief networks into cNeural in order to make further support training large scale neural networks for various problems Finally with more technical work such as GUI done we would like to make it as a toolbox and open source it A CKNOWLEDGMENT This work is funded in part by China NSF Grants No 61223003 the National High Technology Research and Development Program of China 863 No 2011AA01A202 and the USA Intel Labs University Research Program R EFERENCES  C Bishop Neural networks for pattern recognition  Clarendon press Oxford 1995  J Collins Sailing on an ocean of 0s and 1s  Science  vol 327 no 5972 pp 1455Ö1456 2010  S Haykin Neural networks and learning machines  Englewood Cliffs NJ Prentice Hall 2009  R Hecht-Nielsen Theory of the backpropagation neural network in Proc Int Joint Conf on Neural Networks,IJCNN IEEE 1989 pp 593Ö605  Y  Loukas  Artiìcial neural netw orks in liquid chromatography Efìcient and improved quantitative structure-retention relationship models Journal of Chromatography A  vol 904 pp 119Ö129 2000  N Serbedzija Simulating artiìcial neural netw orks on parallel architectures Computer  vol 29 no 3 pp 56Ö63 1996  M Pethick M Liddle P  W erstein and Z Huang P arallelization of a backpropagation neural network on a cluster computer in Proc Int Conf on parallel and distributed computing and systems PDCS  2003  K Ganeshamoorthy and D Ranasinghe On the performance of parallel neural network implementations on distributed memory architectures in Proc Int Symp on Cluster Computing and the Grid CCGRID  IEEE 2008 pp 90Ö97  S Suresh S Omkar  and V  Mani P arallel implementation of back-propagation algorithm in networks of workstations IEEE Trans Parallel and Distributed Systems  vol 16 no 1 pp 24Ö34 2005  Z Liu H Li and G Miao Mapreduce-based backpropagation neural network over large scale mobile data in Proc Int Conf on Natural Computation ICNC  vol 4 IEEE 2010 pp 1726Ö1730  M Glesner and W  P  ochm  uller Neurocomputers an overview of neural networks in VLSI  CRC Press 1994  Y  Bo and W  Xun Research on the performance of grid computing for distributed neural networks International Journal of Computer Science and Netwrok Security  vol 6 no 4 pp 179Ö187 2006  C Chu S Kim Y  Lin Y  Y u  G  Bradski A Ng and K Olukotun Map-reduce for machine learning on multicore Advances in neural information processing systems  vol 19 pp 281Ö288 2007  U Seif fert  Artiìcial neural netw orks on massi v ely parallel computer hardware Neurocomputing  vol 57 pp 135Ö150 2004  D Calv ert and J Guan Distrib uted artiìcial neural netw ork architectures in Proc Int Symp on High Performance Computing Systems and Applications  IEEE 2005 pp 2Ö10  H Kharbanda and R Campbell F ast neural netw ork training on general purpose computers in Proc Int Conf on High Performance Computing HiPC  IEEE 2011  U Lotri  c and e a Dobnikar A Parallel implementations of feed-forward neural network using mpi and c on  net platform in Proc Int Conf on Adaptive and Natural Computing Algorithms  Coimbra 2005 pp 534Ö537  Q V  Le R Monga and M e a De vin Building high-le v e l features using large scale unsupervised learning in Proc Int Conf on Machine Learning ICML  ACM 2012 pp 2Ö16  J Ekanayak e and H e a Li T wister a runtime for iterati v e mapreduce in Proc of the 19th ACM International Symposium on High Performance Distributed Computing  ACM 2010 pp 810Ö818  Y  Bu B Ho we M Balazinska and M D Ernst Haloop Efìcient iterative data processing on large clusters Proc of the VLDB Endowment  vol 3 no 1-2 pp 285Ö296 2010  M Zaharia M Cho wdhury  T  Das A Da v e  J  Ma M McCauley M Franklin S Shenker and I Stoica Resilient distributed datasets A fault-tolerant abstraction for in-memory cluster computing in Proc USENIX Conf on Networked Systems Design and Implementation  USENIX Association 2012 pp 2Ö16 384 


Figure 15  3D model of the patio test site Figure 16  Model of the patio test site combining 2D map data with 3D model data a Largest explored area b Smallest explored area Figure 14  Maps built by a pair of 2D mapping robots Yellow indicates area seen by both robots Magenta indicates area seen by one robot and Cyan represents area seen by the other a 3D point cloud built of the patio environment Figure 16 shows a model built combining 2D map data with 3D model data A four-robot mission scenario experiment was conducted at the mock-cave test site This included two 2D mapping robots a 3D modeling robot and a science sampling robot There was no time limit on the run Figure 17 shows a 3D model of the tunnel at the mock cave Figure 18 shows a model built combining 2D map data with 3D model data 7 C ONCLUSIONS  F UTURE W ORK The multi-robot coordination framework presented in this paper has been demonstrated to work for planetary cave mission scenarios where robots must explore model and take science samples Toward that end two coordination strategies have been implemented centralized and distributed Further a core communication framework has been outlined to enable a distributed heterogenous team of robots to actively communicate with each other and the base station and provide an online map of the explored region An operator interface has been designed to give the scientist enhanced situational awareness collating and merging information from all the different robots Finally techniques have been developed for post processing data to build 2  3-D models of the world that give a more accurate description of the explored space Fifteen 2D mapping runs with 2 robots were conducted The average coverage over all runs was 67 of total explorable area Maps from multiple robots have been merged and combined with 3D models for two test sites Despite these encouraging results several aspects have been identi\002ed that can be enhanced Given the short mission durations and small team of robots in the experiments conducted a simple path-to-goal costing metric was suf\002cient To use this system for more complex exploration and sampling missions there is a need for learning-based costing metrics Additional costing parameters have already been identi\002ed and analyzed for future implementation over the course of this study One of the allocation mechanisms in this study was a distributed system however task generation remained centralized through the operator interface In an ideal system robots would have the capability to generate and auction tasks based on interesting features they encounter Lastly the N P complete scheduling problem was approximated during task generation However better results could potentially 10 


Figure 17  3D model of the tunnel in the mock cave test site Figure 18  Model of the mock cave test site combining 2D map data with 3D model data be obtained by releasing this responsibility to the individual robots A CKNOWLEDGMENTS The authors thank the NASA STTR program for funding this project They would also like to thank Paul Scerri and the rCommerceLab at Carnegie Mellon University for lending hardware and robots for this research R EFERENCES  J C W erk er  S M W elch S L Thompson B Sprungman V Hildreth-Werker and R D Frederick 223Extraterrestrial caves Science habitat and resources a niac phase i study\\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2003  G Cushing T  T itus and E Maclennan 223Orbital obser vations of Martian cave-entrance candidates,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  M S Robinson B R Ha wk e A K Boyd R V Wagner E J Speyerer H Hiesinger and C H van der Bogert 223Lunar caves in mare deposits imaged by the LROC narrow angle camera,\224 in First Intl Planetary Caves Workshop  Carlsbad NM 2011  J W  Ashle y  A K Bo yd H Hiesinger  M S Robinson T Tran C H van der Bogert and LROC Science Team 223Lunar pits Sublunarean voids and the nature of mare emplacement,\224 in LPSC  The Woodlands,TX 2011  S Dubo wsk y  K Iagnemma and P  J Boston 223Microbots for large-scale planetary surface and subsurface exploration niac phase i.\224 NASA Innovative Advanced Concepts NIAC Tech Rep 2006  S Dubo wsk y  J Plante and P  Boston 223Lo w cost micro exploration robots for search and rescue in rough terrain,\224 in IEEE International Workshop on Safety Security and Rescue Robotics Gaithersburg MD  2006  S B K esner  223Mobility feasibility study of fuel cell powered hopping robots for space exploration,\224 Master's thesis Massachusetts Institute of Technology 2007  M T ambe D Pynadath and N Chauv at 223Building dynamic agent organizations in cyberspace,\224 IEEE Internet Computing  vol 4 no 2 pp 65\22673 March 2000  W  Sheng Q Y ang J T an and N Xi 223Distrib uted multi-robot coordination in area exploration,\224 Robot Auton Syst  vol 54 no 12 pp 945\226955 Dec 2006  A v ailable http://dx.doi.or g/10.1016/j.robot 2006.06.003  B Bro wning J Bruce M Bo wling and M M V eloso 223Stp Skills tactics and plays for multi-robot control in adversarial environments,\224 IEEE Journal of Control and Systems Engineering  2004  B P  Gerk e y and M J Mataric 223 A formal analysis and taxonomy of task allocation in multi-robot systems,\224 The International Journal of Robotics Research  vol 23 no 9 pp 939\226954 September 2004  M K oes I Nourbakhsh and K Sycara 223Heterogeneous multirobot coordination with spatial and temporal constraints,\224 in Proceedings of the Twentieth National Conference on Arti\002cial Intelligence AAAI  AAAI Press June 2005 pp 1292\2261297  M K oes K Sycara and I Nourbakhsh 223 A constraint optimization framework for fractured robot teams,\224 in AAMAS 06 Proceedings of the 002fth international joint conference on Autonomous agents and multiagent sys11 


tems  New York NY USA ACM 2006 pp 491\226493  M B Dias B Ghanem and A Stentz 223Impro ving cost estimation in market-based coordination of a distributed sensing task.\224 in IROS  IEEE 2005 pp 3972\2263977  M B Dias B Bro wning M M V eloso and A Stentz 223Dynamic heterogeneous robot teams engaged in adversarial tasks,\224 Tech Rep CMU-RI-TR-05-14 2005 technical report CMU-RI-05-14  S Thrun W  Bur g ard and D F ox Probabilistic Robotics Intelligent Robotics and Autonomous Agents  The MIT Press 2005 ch 9 pp 222\226236  H Mora v ec and A E Elfes 223High resolution maps from wide angle sonar,\224 in Proceedings of the 1985 IEEE International Conference on Robotics and Automation  March 1985  M Yguel O A ycard and C Laugier  223Update polic y of dense maps Ef\002cient algorithms and sparse representation,\224 in Intl Conf on Field and Service Robotics  2007  J.-P  Laumond 223T rajectories for mobile robots with kinematic and environment constraints.\224 in Proceedings International Conference on Intelligent Autonomous Systems  1986 pp 346\226354  T  Kanungo D Mount N Netan yahu C Piatk o R Silverman and A Wu 223An ef\002cient k-means clustering algorithm analysis and implementation,\224 IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 24 2002  D J Rosenkrantz R E Stearns and P  M Le wis 223 An analysis of several heuristics for the traveling salesman problem,\224 SIAM Journal on Computing  Sept 1977  P  Scerri A F arinelli S Okamoto and M T ambe 223T oken approach for role allocation in extreme teams analysis and experimental evaluation,\224 in Enabling Technologies Infrastructure for Collaborative Enterprises  2004  M B Dias D Goldber g and A T  Stentz 223Mark etbased multirobot coordination for complex space applications,\224 in The 7th International Symposium on Arti\002cial Intelligence Robotics and Automation in Space  May 2003  G Grisetti C Stachniss and W  Bur g ard 223Impro ving grid-based slam with rao-blackwellized particle 002lters by adaptive proposals and selective resampling,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2005  227\227 223Impro v ed techniques for grid mapping with raoblackwellized particle 002lters,\224 IEEE Transactions on Robotics  2006  A Geiger  P  Lenz and R Urtasun 223 Are we ready for autonomous driving the kitti vision benchmark suite,\224 in Computer Vision and Pattern Recognition CVPR  Providence USA June 2012  A N 250 uchter H Surmann K Lingemann J Hertzberg and S Thrun 2236d slam with an application to autonomous mine mapping,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  2004 pp 1998\2262003  D Simon M Hebert and T  Kanade 223Real-time 3-d pose estimation using a high-speed range sensor,\224 in Proceedings of the IEEE International Conference on Robotics and Automation  1994 pp 2235\2262241 B IOGRAPHY  Ammar Husain received his B.S in Mechanical Engineering Robotics from the University of Illinois at Urbana-Champaign He is pursuing an M.S in Robotic Systems Development at Carnegie Mellon University He has previously worked on the guidance and control of autonomous aerial vehicles His research interests lie in the 002eld of perception-based planning Heather Jones received her B.S in Engineering and B.A in Computer Science from Swarthmore College in 2006 She analyzed operations for the Canadian robotic arm on the International Space Station while working at the NASA Johnson Space Center She is pursuing a PhD in Robotics at Carnegie Mellon University where she researches reconnaissance exploration and modeling of planetary caves Balajee Kannan received a B.E in Computer Science from the University of Madras and a B.E in Computer Engineering from the Sathyabama Institute of Science and technology He earned his PhD from the University of TennesseeKnoxville He served as a Project Scientist at Carnegie Mellon University and is currently working at GE as a Senior Cyber Physical Systems Architect Uland Wong received a B.S and M.S in Electrical and Computer Engineering and an M.S and PhD in Robotics all from Carnegie Mellon University He currently works at Carnegie Mellon as a Project Scientist His research lies at the intersection of physics-based vision and 002eld robotics Tiago Pimentel Tiago Pimentel is pursuing a B.E in Mechatronics at Universidade de Braslia Brazil As a summer scholar at Carnegie Mellon Universitys Robotics Institute he researched on multi-robots exploration His research interests lie in decision making and mobile robots Sarah Tang is currently a senior pursuing a B.S degree in Mechanical and Aerospace Engineering at Princeton University As a summer scholar at Carnegie Mellon University's Robotics Institute she researched multi-robot coordination Her research interests are in control and coordination for robot teams 12 


Shreyansh Daftry is pursuing a B.E in Electronics and Communication from Manipal Institute of Technology India As a summer scholar at Robotics Institute Carnegie Mellon University he researched on sensor fusion and 3D modeling of sub-surface planetary caves His research interests lie at the intersection of Field Robotics and Computer Vision Steven Huber received a B.S in Mechanical Engineering and an M.S in Robotics from Carnegie Mellon University He is curently Director of Structures and Mechanisms and Director of Business Development at Astrobotic Technology where he leads several NASA contracts William 223Red\224 L Whittaker received his B.S from Princeton University and his M.S and PhD from Carnegie Mellon University He is a University Professor and Director of the Field Robotics Center at Carnegie Mellon Red is a member of the National Academy of Engineering and a Fellow of the American Association for Arti\002cial Intelligence 13 


