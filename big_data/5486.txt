Structure of Association Rule Classifiers: a Review  Koen Vanhoof  BenoÓt Depaire Transportation Research Institute \(IMOB University Hasselt 3590 Diepenbeek, Belgium koen.vanhoof@uhasselt.be benoit.depaire@uhasselt.be   Abstract This paper provides a short review of various association rule classifiers \(ARC\ that have been developed over the past decade and the common structure behind most ARCs Furthermore, different pruning and classification schemes used in various ARCs are reviewed and two ARCs are discussed which break with the standard structure behind ARC Keywords-classification, association rules, pruning I  I NTRODUCTION  In 1998, Liu et al  i n t rod uce d CB A, t h e fir s t a s s o ci a t i o n  
rule classifier \(ARC\. This new type of classification algorithm distinguishes itself from other classifiers by learning association rules from the training data and using these rules to classify new cases. As association rules typically represent local knowledge, all ARCs try to combine local knowledge to solve new cases. In the wake of Liu et al. [1 s e v e ral  oth er  authors came up with different ARC implementations, such as CMAR A RC-A C a nd A RC-B C [3 C P A R   4 C o r C l a ss 5 an d A C R I  6    2S A R C 1 an d 2S A R C 2  7 an d A R UB A S  8   At first sight, large differences seem to exist between these different ARCs in terms of generating a set of association rules and using this knowledge to classify new instances. Despite the differences, a common structure exists among the various ARC 
implementations which will be unraveled in this article The next section will introduce some concepts of frequent item sets and associati on rules, which are the basis of ARCs Next, the common structure of many ARCs is introduced and different implementations are discussed. Furthermore, two ARCs which are exceptions to the common structure are discussed and finally some conclusions are formulated II  F REQUENT I TEM S ETS AND A SSOCIATION R ULES  Research regarding frequent item sets and association rules goes back to an article of Agrawal et al. [9  H o w e v er f r e q u e n t  item set analysis typically deals with transactional data where 
each record is a transaction representing a set of items. Any two transactions can have a different size, i.e. contain a different number of items, and might have no items in common. Market basket analysis is a typical example of data analysis on transactional data. To measure how often a set of items occur together within a set of transactions, the item setês support is calculated as the number of transaction that contain the item set. If the support of the item set is above a predefined threshold, it is called a frequent item set In contrast to frequent item set analysis, classification analysis, which is what ARCês are used for, is mostly applied on rectangular data instead of transactional data. While a record in a transactional data set is represented as a set of 
items, e.g        012  the same record in a rectangular data set is represented as a set of attribute-value pairs, e.g       015     015          015    012 015    It is always possible to transform transactional data into rectangular data, by creating a binary attribute-value pair for each item. Note that a transformed rectangular record contains an attribute-value pair for all possible items, not only for those items present in the original transactional record. This is a first and important difference between transactional and rectangular 
data, i.e. transactional records differ in size as they only contain a subset of all possible items, while rectangular data records contain all possible attribute-value pairs and have equal size Another important difference between the two data set structures is that transactional data has a binary data nature, i.e either an item is present or it is not, which explains why ARC algorithms typically require nominal data to work with. On the other hand, rectangular data can also represent non-binary data such as nominal, categorical and continuous data Transforming rectangular data to transactional data can be done in two steps. First all attributes must be transformed into nominal attributes by discretizing continuous variables and 
ignoring the order of categorical variables. Next, the nominal attributes are transformed into binary dummy variables As this article focuses on ARC algorithms, the remainder of the text assumes data to be nominal and in a rectangular format. Therefore, some definitions related to frequent item sets need to be reformulated in terms of transactional data structures Definition 1: Attribute-Value Pair    An attribute-value pair         represents a nominal variable   with value  from possible values   015 
   Value  denotes a missing value. Furthermore              Definition 2: Record    A record          represents a set of   attribute value pairs Definition 3: Item set   ___________________________________ 978-1-42446793-8 10/$26.00 ©2010 IEEE  


An item set        represents a set of   attribute value pairs From these definitions, it follows that there is no structural difference between a record and an item set. However, the term record is mainly used for the original data, while item sets are mainly subsets of the original data. Furthermore, while the value  in a record represents a missing value, this value is interpreted as ènot presentê rather than èmissingê when present in an item set Definition 4: Record  contains Itemset     Record           contains item set            iff                 Definition 5: Support of Itemset       0         Definition 6: Frequent Itemset    1 is a frequent item set iff 234   0 5 where 5 is a predefined threshold Definition 7 6#7    6 7  77 such that     77 it holds that    8      9         9      III  A SSOCIATION R ULES  Association rules are nothing less than rules built from two exclusively disjunctive item sets. One item set acts as the ruleês body, which is also called the antecedent, while the second item set is the ruleês head, which is also called the consequent Definition 8: Association Rule   An association rule     consists of a body item set   and a head item set   such that       6   Traditionally, two measures are generated to evaluate the quality of an association rule, i.e. the support and the confidence. The support of an association rule measures to what extent the data supports the pattern described by the rule A low support can have different explanations. Typically, this implies that the rule only exists in a small local part of the data space. If the support is extremely low, it could also imply that the rule doesnêt really exist and is rather some kind of measurement error Definition 9: Support of Association Rule  A  B      0   C  0  The union of two disjunctive item sets is defined as follows Definition 10: Union of two disjunct itemsets C       6    C    such that      it holds that    8      9          9    While the support of an association rule measures how well the rule is supported by the data, the confidence of the rule reveals how accurate the ruleês body predicts the ruleês head The confidence of a rule measures how many percent of the records that contain the body, also contain the head and is defined as follows Definition 11: Confiden ce of Association Rule  A  B   DE   0 0F  0  For classification purposes, not all association rules are interesting. Only those rules where the ruleês head contains the class attribute and no other items can be used to classify new instances. Such rules are called classification association rules or CARs Definition 12: Classificati on Association Rule  G  A classification association rule  H     is an association rule such that     I  J     and  J   where  J represents the class attribute of the classification problem Once the CARs are learned from the data, they are used to classify new data instances. In order to do so, a distinction must be made between CARs that match a record and CARs that cover a record. A CAR covers a record if the record contains the body of the CAR, while a CAR matches a record if the record contains both the body and the head of the CAR Definition 13  G covers record    CAR  H     covers record  iff      Definition 14  G matches record    CAR  H     covers record  iff     and    Furthermore, the size of a rule is defined as the number of non-empty attribute-value pairs in the ruleês body Definition 15: The size of  G  The size of  H denoted as    H  is the number of attributevalue pairs   for which     Finally a CAR   H can be a generalization of another CAR  K H which is defined as follows Definition 16  L G generalizes  M G     H     generalizes  K H  K  K iff     K   K   and    H   K H   IV  A SSOCIATION R ULE C LASSIFIERS  A  General Structure Over the years, various authors have introduced their own association rule classifier. Although each implementation differs, a common structure of three consecutive steps can be identified across most ARCs Step 1: Learn Classification Association Rules Obviously, all ARCs must start by generating a set of CARs 


from a given training set. Various algorithms have been used such as Apriori which is used in CBA, ARC-AC, ARC-BC and ACRI Other CAR generating algorithms are FPGrowth, which is used by CMAR, a CAR generating algorithm based on FOIL used by CPAR and a CAR generating algorithm based on the Framework of Morishita and Sese used by CorClass Algorithms such as Apriori and FPGrowth are exact algorithms which provide the exhaustive set of association rules meeting specific support and confidence criteria. These CAR generating algorithms produce the same set set of CARs generated, but differ in terms of computational complexity. One exception is the FOIL-based CAR generating algorithm used in the CMAR implementation, which is a heuristic rather than an exact solution and only gives an approximation of the exhaustive set of CARs meeting specific criteria Step 2: Prune the set of Classification Association Rules Once the CARs are generated from the training set, most ARCs apply some pruning strategy to reduce the set of CARs as this can become enormous. Various strategies to prune the set of CARs exist, which will be discussed in the next subsection However, some ARCs, such as CorClass and ACRI do not apply a separate pruning step, although setting a minimum confidence and support when generating the CARs can be interpreted as a pre-pruning strategy Step 3: Classifying new instances Once the \(pruned\et of CARs is finalized, they can be used to classify new instances. Various strategies have been developed, which will be discussed in section IV.B B  Pruning the Association Rules As mentioned before, even if there is no separate postpruning step in the ARC algorithm, all ARC algorithm apply some sort of pre-pruning by setting a support and/or confidence threshold. Only CARs passing these thresholds will be kept in the set of association rules. These pruning techniques are isolated pruning techniques as they evaluate each CAR individually, in isolation from the other CARs Another isolated pruning technique, which is applied by CBA, is Pessimistic Error Pruning PEP\. PEP uses the pessimistic error rate based pruning method in C4.5 [10 I t  calculates the pessimistic error rate of the entire rule and compares it with the pessimistic error rate of the rule obtained by deleting one item from the ruleês body. If the pessimistic error rate of the complete rule is higher than the pessimistic error rate of the trimmed rule, than the complete rule is pruned The results in w t h a t  p e ssi m i st i c erro r pr uni ng ha s a strong impact on the number of CARs generated Another isolated pruning technique is Correlation Pruning CorP\, which is applied in CMAR. This pruning technique calculates the correlation between the ruleês body and the ruleês head. If the correlation is not statistically significant, the rule is pruned There are also non-isolated pruning techniques which take multiple rules into account in order to decide whether or not to prune a specific rule. A well known non-isolated pruning technique is the Data Coverage Pruning technique \(DCP which is applied in CBA, ARC-AC, ARC-BC and CMAR DCP consists of two steps. First, the rule set is ordered according to confidence, support and rule size. Rules with the highest confidence go first. In case of a tie, rules with the highest support take precedence. In case of tie in terms of confidence and support, the smaller the rule, i.e. the more general a rule is, the higher the ranking. Once the rule set is ordered, the rules are taken one by one from the ordered rule set and are added to the final rule set until every record in the data set is matched at least 5 times. For CBA, ARC-AC and ARC-BC this parameter 5 is fixed to 1, while in CMAR 5 is a parameter which needs to be set by the user Confidence Pruning ConfP\ is a second non-isolated pruning technique which is used by CMAR, ARC-AC, ARCBC. ConfP prunes all rules which are generalized by another rule with a higher confidence level C  Classifying new instances Once the CARs are generated and pruned, the ARC needs to use all these pieces of local knowledge to classify new instances. Various approaches have been developed, which can be classified as order-based classification or non-order-based classification. With order-based classification, the rules in the final rule set need to be ordered according to a specific criterion and this ordering has an influence on the predicted class, while non-order-based classifiers do not need such ordering A first order-based classification scheme is the Single Rule Classification This approach orders the rules and then uses the first rule which covers the new instance to make a prediction The predicted class is the selected ruleês head. This classification scheme is used by CBA, CorClass and ACRI CBA and CorClass order the rules according to confidence support and rule size in the same way the data coverage pruning technique does. ACRI on the other hand, provides the user four different ordering criteria to select from, i.e. a cosine measure, the support, the confidence or the coverage. The latter is defined as the number of items the body of a rule has in common with the new instance divided by the size of the rule A second order-based classification scheme is the Multiple Rule Classification This approach, which is used by CPAR retains only those rules which cover the new instance. Next, the rules are grouped per class value and ordered according to a specific criterion. Finally, a combined measure is calculated for the best N rules, where N is a parameter which needs to be set by the user. With CPAR, the rank of each rule is determined by the expected accuracy of the rule, which is calculated by the Laplace accuracy. This is defined as follows where O denotes the number of classes in the classification problem Definition 17: Laplace accuracy The Laplace accuracy of a CAR  H     is PQ.RQST   H 0  U    H 0 V\015 W FU    0 VOW  The combined measure in CPAR is nothing more than the average expected accuracy Finally, some ARCs use a non-order-based classification scheme, such as CMAR, ARC-AC, ARC-BC and CorClass 


This classification scheme selects the rules which cover the new instance, groups them per class value and calculates a combined measure per class value. This approach is almost identical to the multiple rule classification scheme, except for the ordering step. Since this non-order based classification uses all the rules, such ordering is not required. The CMAR algorithm uses a weighted X K measure as combined measure while ARC-AC and ARC-BC calculate the sum of the rules confidence levels. CorClass uses a weighted sum as combined measure and defines three different weights. The first option is to give all rules a weight equal to one, which results in a majority voting classification scheme. The second and the third option uses some kind of ranking of the rules, which turns it into an order-based classification scheme V  E XCEPTIONS  While most ARCs follow the design of learning the CARs pruning the rule set and classifying the new instances, there are some ARCs which are an exception to this rule. These exceptions are classifiers which follow a totally different scheme. Three examples of such exceptions are 2SARC1 2SARC2 [8 a n d  ARU B AS 9  These three classifiers firstly generate CARs from a training set, which makes them ARCs. However, instead of using these CARs directly to classify new instances, as the other ARCs do, the CARs are used to transform the attribute space into a new feature space. Subsequently, traditional data mining techniques are applied in this new feature space to classify the new instances In case of 2SARC1, each instance is described by class features, while as for 2SARC2, each instance is described by rule features. Once the instances are transformed into this new feature space, a neural network is trained and used to classify new instances ARUBAS on the other hand, uses the learned association rules to transform all instances into pattern space. Each CAR is considered to reveal a frequent pattern in the data which is strongly connected to a specific class. In pattern space, each CAR represents a binary attribute and instances receive value 1 if the CAR covers the instance. Next, the similarity between a new instance and all instances of a specific class is calculated in pattern space. The authors believe that pattern space can be a stronger representation space as each attribute in pattern space is actually a frequent pattern in the original attribute space with strong predictive power. The instance will be assigned to the class with the highest similarity VI  C ONCLUSIONS  Association rule classifiers are a type of classifier which have been around for more than a decade. Recently, they received quite some attention from researchers who focus on building global models from local patterns [1  r time, different ARC algorithms have been developed, resulting in a wide variety of association rule pruning techniques and CAR classification schemes. Despite the variety, it is clear that most ARC algorithms have a common three step structure, i.e first learn the association rules, next prune the set of CARs and finally classify new instances with the CAR set. The benefit of this clear common structure is that different elements from different ARC algorithms can easily be combined. Evaluating the effect of the various pruning techniques and association rule classifiers and the interaction among them could be an interesting direction for future research At the same time, some authors came up with an innovative but different approach to use association rule for classification Instead of using the association rules directly, the CARs are used to transform the original attribute space into a more powerful representation. However, this approach has not been studied as extensively as the more common three step approach and most likely provides some interesting new paths for future research R EFERENCES   1  L i u, B., H s u W  an d Y  M a I nte g r ati ng cl ass if ic at io n an d asso ci at io n  rule mining,é ACM Int. Conf. on Knowledge Discovery and Data Mining \(SIGKDD '98\,pp. 80-86 , 1998 2  L i  W   Ha n  J  and J P ei CM A R A c c u r a t e and ef f i ci en t c l a s si f i c a t i o n based on multiple class-association rules,é Proc. of the Int. Conf. on Data Mining \(ICDMê01\, pp. 369-376, 2001 3   A n to nie M.L  an d O  R. Z a  a ne  T ex t do cu m e nt c a te g o r iz atio n by te r m  association,é Proceedings of the 2002 IEEE International Conference on Data Mining \(ICDM'02\, pp. 19-26, 2002  4  Y i n X  a n d J  H a n  C P A R cl as s i f i ca tio n  bas e d o n pr e d ic tiv e  association rules,é Proc. of the SIAM Int. Conf. on Data Mining, pp 369-376, 2003 5  Zi m m e rm a n n  A  an d L  De R a ed t  C orC l a s s  c o rr ela t e d a s s o c i at i o n rule mining for classification,é Discovery Science, Springer Berlin Heidelberg, pp. 60-72, 2004  6  Ra k R   S t ac h W Z a a ne O  R a n d M  L A n to nie  C o n s i de r ing r e occurring features in associative classifiers,é Advances in Knowledge Discovery and Data Mining, Springer Berlin / Heidelberg, pp. 240-248 2005 7 A n t o n i e  M  L   Za  a n e  O R  a nd R  C Holt e Lea rn i n g t o Us e a Learned Model: A Two-Stage Approach to Classification,é Proc. of the sixth Int. Conf. on Data Mining \(ICDM 2006\ pp. 33-42, 2006 8 Dep a i re B Va nh oof K  an d G  W e t s  A R U B A S: a n a s s o ci at i o n r u le based similarity framework for associative classifiers,é Data Mining Workshops, Int. Conf. on Data mining \(ICDMW 2008\, pp. 692-699 2008 9  A g r a w a l  R., I m ie l i  s k i  T  an d A   Sw ami M i n i n g asso c i a tio n ru l e s between sets of items in large databases,é Proc. of the 1993 ACM SIGMOD Int. Conf. on Management of data, pp. 207-216, 1993 10  J  R Q u i n l a n  C 4  5   pr o g r am f o r m a ch ine l e ar n ing    Mo r g an K a uf m a n   1992    


each image is automatically segmented and the primitive visual features are computed, as it is described in section 2 for each image, the visual primitive features are mapped to semantic indicators, as it is described in section 3 the determination of pattern regions for each image belonging to the transaction set, as it is described in section 4 the rule generation algorithms are applied to produce semantic pattern rules, which will identify each semantic category from database 2  The image testing/annotation phase has as scope the automatic annotation of images each new image is automatically segmented into regions for each new image, the low-level characteristics are mapped to semantic indicators the classification algorithm is applied for identifying the image category/semantic concept In the developed system, the learning of semantic pattern rules is continuously made, because when a categorized image is added in the training database, the system continues to generate semantic rules A  The description of the algorithm for rule generation The method for the generation of the semantic pattern rule is based on A-priori algorithm for finding the frequent itemsets [1 T h e sco p e o f im ag e asso ciatio n ru le s is to f i n d  semantic relationships between image objects The rule generation algorithm takes into account the entire region features. This algorithm is based on Ñregion patternsî and necessitates some computations, being necessary a pre-processing phase for determining the visual similitude between the image regions of the same category In the pre-processing phase, the region patterns, which appear in the images, are determined. So, each image region Reg ij is compared with other image regions from the same categories. If the region Reg ij matches other region Reg km  having in common the features on n 1 n 2 n c positions then the generated region pattern is SRj \( -, -, n 1 n 2 n c   and the other features are ignored The generation of image region patterns is described in pseudo-code by the following algorithm  Algorithm IV.1   Generation of region patterns for each image Input  set of n images {I 1 I 2 I n laying in the same category; each image I is a set of regions Reg im  Output  set  of region patterns for each image Method  for i 1 1, n-1 do for j 1 1, I i1 nregions do for i 2 i 1 1, n do  for j 2 1, I i2 nregions do if Reg j1 matches Reg j2 in the components n 1 n 2 n c then  i=i+1  RSi 1 RSi 1 n 1 n 2 n c RSi 2 RSi 2 n 1 n 2 n c  end  end end end A database with five images relevant for a category is used. For this category, an example of images representation using region patterns can be observed in the Table I By applying the algorithm IV.1 on the database from Table I, the results from Table II are obtained As it can be observe, the number of region patterns is big, but it is pruned in the phase of rules elimination process, described in the next section The image modelling in terms of itemsets and transaction is the following the transactions represent the set of  region patterns determined by the previous algorithm the itemsets are represented by region patterns of the images laying in the same category the frequent itemsets represent the itemsets with the support greater than the minimum support the itemsets of cardinality between 1 and k are iteratively found the frequent itemsets are used for rule generation The algorithm for rules generation based on region patterns is described in pseudo-code  Algorithm IV.2  The generation of  semantic pattern rules based on region patterns Input the image set represented as I =\(RS 1 RS k  m is a region pattern Output the set of semantic pattern rules Method  Ck: the set of region patterns of k-length Lk: the set of frequent region patterns of k-length PatternRules: the set of pattern rules constructed from frequent itemsets, for k>1 L1= {frequent region patterns for\(k=1; Lk!=null; k Ck+1=candidates generated from the set Lk foreach transaction t in the database do Increment the number of all candidates that appear in t end Lk+1=candidates from Ck+1 that has the support greater or equal than suport_min end PatternRules=PatternRules+{Lk+1->category end T ABLE I  R ELEVANT IMAGES FOR A CERTAIN CATEGORY  ImgID Image Regions 1 R\(a,b,c,d\, R\(aí,bí,cí,d 2 R\(a,b,c,f\, R\(aí,bí, cí,d 3 R\(a,b,c,fíí\R\(aí,bí,cí,i 4 R\(a,b,c,fí\R\(aí,bí,cí,i 5 R\(a,b,e,d\, R\(m,bí,cí,d 
128 


 T ABLE II  R EGIONS P ATTERNS OF IMAGES OF A CERTAIN  CATEGOY   ID  Region Patterns  1 R\(a,b,c,-\R\(a,b,-,d\R\(aí,bí,cí,dí\ R\(aí,bí,cí,-\ R\(-,bí,cí,d  2 R\(a,b,c,-\,R\(a,b,-,-\,R\(aí,bí,cí,dí\, R\(aí,bí,cí,-\, R\(-,bí,cí,d  3 R\(a,b,c,-\  R\(a,b,-,-\  R\(aí,bí,cí,-\ R\(-,bí,c  4 R\(a,b,c,-\ R\(a,b,-,-\R\(aí,bí,cí,-\ R\(-,bí,c  5 R\(a,b,-,d\,R\(a,b,-,-\ R\(-,bí,cí,dí\ R\(-,bí,c  The category of an image  is determined by the following semantic association rules R\(a,b,c,-\ and R\(aí,bí,cí,-\>category R\(a,b,c,-\ and R\(a,b,-,-\->category R\(aí,bí,cí,-\ and R\(a,b,-,-\ ->category R\(a,b,-,-\ and R\(-,bí,cí,-\ ->category R\(a,b,c,-\ and R\(aí,bí,cí,-\d R\(a,b,-,-\ ->category The rules are represented in Prolog as facts of the form PatternRule\(Category, Score, ListofRegionPatterns The patterns from ListofRegionPatterns are terms of the form: regionPattern\(ListofPatternDescriptors The patterns from the descriptors list specify the set of possible values for a certain descriptor name. The form of this term is descriptorPattern\(descriptorName,ValueList One of the semantic pattern rules used to identify the cliff category is illustrated bellow. This semantic pattern rule has the score \(confidence\al to 100 PatternRule \(cliff,100 regionPattern descriptorPattern\(colour,[dark-brown descriptorPattern\(horizontal-position,[center,left right  descriptorPattern\(vertical-position,[center,bottom descriptorPattern\(dimension,[big descriptorPattern\(eccentricity-shape,[sma descriptorPattern\(texture-probability, [medium  descriptorPattern\(texture-inversedifference medium descriptorPattern \(texture-entropy,[big descriptorPattern\(texture-energy,[big descriptorPattern \(texture-contrast,[big descriptorPattern \(texture-correlation, [big  regionPattern descriptorPattern\(colour,[medium-brown  descriptorPattern\(horizontal-position,[center,left right descriptorPattern\(vertical-position,[center, bottom descriptorPattern\(dimension,[smal descriptorPattern\(eccentricity-shape,[sma descriptorPattern \(texture-probability,[big descriptorPattern\(texture-inversedifference,[big descriptorPattern\(texture-entropy,[medium descriptorPattern \(texture-energy, [big  descriptorPattern \(texture-contrast,[medium descriptorPattern \(texture- correlation,[big  B  Semantic pattern rules elimination The number of semantic pattern rules that could be generated is usually big. In this case two problems exist: the first one is that the set of rules can contain noise information and affect the classification time. Another problem is the big number of rules that can also affect the classification time This is an important problem in the real applications, which necessitate rapid response. On the other side, the elimination of rules can affect the classification accuracy. The elimination methods are the following elimination of  specific rules and keeping the rules with big confidence elimination of rules that can introduce errors in the classification process The following definitions introduce some notions used in this section [11   Definition 1   Given two rules R 1 C and R 2 C, the first rule is called general if R 1  R 2 The second one is called specific Definition 2  Given two rules R 1 and R 2 R 1 is called stronger than R 2 or R 2 is weaker than R 1  1\ R 1 has confidence greater than R 2  2\If the confidences are equal, but the support\(R 1  greater than support\(R 2  3\ the supports are equal, support\(R 1 upport\(R 2  and confidences confidence\(R 1 fidence\(R 2  equal, but R 1 has fewer attributes than R 2   The elimination of weak and specific semantic pattern rules is described in pseudo-code Algorithm IV.3  Elimination of weak and specific rules Input  set of semantic pattern rules, S generated for each category, C   Output  the set of rules which will be used for classification Method Sort the rules for C category in conformity to Definition 1 foreach rule in S do begin Find the most specific rules Eliminate the rules with  smallest confidence end C  Semantic image classification The classifier represents the set of semantic pattern rules used to predict the category of images from the test database. Being given a new image, the classification process searches in the rules set for finding the most appropriate category. Images are processed and are represented by means of semantic indicators as Prolog facts The semantic pattern rules are applied to the set of images facts, using the Prolog inference engine A semantic pattern rule matches an image, if all characteristics which appear in the body of the rule also appear in the image characteristics The algorithm for image categorization is described in pseudo-code 
129 


Algorithm IV.4  Semantic classification of an image Input new unclassified image and the set of semantic pattern rules; each pattern rule has the confidence R i conf Output the classified image, and the score of matching Method  S = null foreach rule R in PatternRules do begin if R matches I then Keep R and add R in S I.score = R.conf  Divide S into subsets one foreach category: S 1 S n   foreach subset S k from S do Add the confidences of all rules from S k  Add I image in the category identified by the rules from S k with the greatest confidence I.score = max 000 S k conf end end  end In the experiments realized through this study, two databases are used for testing the learning process. The database used for learning contains 200 images from different nature categories and is used to learn the correlations between images and semantic concepts. All the images from the database have JPEG format and are of different dimensions. The database used in the learning process is categorized into 50 semantic concepts. The system learns each concept by submitting appreciatively 20 images per category. The testing database contains 500 unclassified images The results of the semantic image retrieval after the sunriseî category are observed in the figure 6. These images from the test database are correctly classified by the algorithm The results of the semantic image retrieval after the mountainî category are observed in the figure 7. Two images from the test database are incorrectly classified since they represent ìclouds The performance metrics, precision and average normalized modified retrieval rate \(ANMRR\, are computed to evaluate the efficiency and accuracy of the semantic pattern rule generation and annotation methods The precision is defined as the ratio between the number of images correctly classified by system, and the total number of classified images. The precision is in the range of 0 an d g r eater v a lu es repres en t a better retriev a l performance. Averaged Normalized Modified Retrieval Rate \(ANMRR\ is an overall performance calculated by averaging the result from each query h e A N M R R is in the range of d s m a ller v a lu e s repres en t a better retrieval performance These parameters are computed as average for each image category as in the table III    Figure 6. The results of semantic image retrieval after the ìsunrise category  Figure 7. The results of semantic image retrieval after the ìmountains category  
130 


T ABLE III  T HE PRECISION AND ANMRR COMPUTED FOR EACH IMAGE CATEGORY  Category Precision ANMRR Fire 0.77 0.39 Iceberg 0.71 0.34 Tree 0.65 0.45 Sunset 0.89 0.14 Cliff 0.93 0.11 Desert 0.89 0.11 Red Rose 0.75 0.20 Elephant 0.65 0.43 Mountain 0.85 0.16 See 0.91 0.09 Flower 0.77 0.31  Also, for each category from the database, the percent of images correctly classified by the system is computed as in Figure 8 As it can be observed from the experiments, the results are strongly influenced by th e complexity of each image category. Actually, the results of experiments are very promising, because they show a small average normalized modified retrieval rate and a good precision for the majority of the database categories, making the system more reliable V  CONCLUSION  In this study we propose methods for semantic image annotation based on visual content. For establishing correlations with semantic categories, we experimented and selected some low-level visual characteristics of images. So each category is translated in visual computable characteristics and in terms of objects that have the great probability to appear in an image category  Figure 8. Category vs. percent of images correctly classified   The algorithm that generates the semantic pattern rules selects the image characteristics with the greatest probability of apparition, offering a better generality By comparison to other image annotation methods, our proposed and developed methods have some advantages: the entire process is automated, and a great number of semantic concepts can be defined; these methods can be easily extended to any domain, because the visual features semantic indicators remain unchanged, and the semantic pattern rules are generated based on the set of example labeled images used for learning semantic concepts; the spatial information is taken into account and it offers rich semantic information about the relationships of the image colour regions \(left, right, center, bottom, and upper The proposed methods have the limitation that they canít learn every semantic concept, due to the fact that the segmentation algorithm is not capable to segment images in real objects. Improvements can be brought using a segmentation method with greater semantic accuracy R EFERENCES  1  A  Be r s o n S  J  S m it h, D a ta W a r e ho us i n g  D a ta Mi ni ng a n d O L A P  McGraw-Hill. New York, 1997 2 G  C a rn e i ro A  C h a n  P  M o re n o a n d N Va s c on c e los  S u p e rvi s e d  learning of semantic classes for image annotation and retrieval IEEE Pattern Analysis Machine Intelligence, vol. 29\(3\, 2007, pp 394ñ410 3  A  H o og s  J  Ri tts c he r  G  S t e i n  a n d  J  S c hm ie de r e r   V ide o co nte n t  annotation using visual analysis and a large semantic knowledge base,î Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003, pp. 327 ñ 334 4  W  J. F r aw l e y   G  P i ate t sky S h apir o  an d C  J Mat h e u s  K n o w le dg e Discovery in Databases, chapter Knowledge Discovery in Databases  An Overview, MIT Press, 1991 5  A   W  S m e u l d e r s  M  W o r r i n g  S   S a n t i n i  A  G u p t a  a n d R  J a i n   Content-Based Image Retrieval at the End of the Early Years IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 22\(12 2000, pp. 1349ñ1380 6  J  R  S m it h S  F C h a n g   V is ual S E E k a f u l l y auto m a te d co nte n tbased image query system,î The Fourth ACM International Multimedia Conference and Exhibition, Boston, MA, USA, 1996 7  V  Me z a r i s   I   K o m p ats i ar is  an d M G  S t r i ntz   R e g io nbas e d I m age Retrieval using an Object Ontology and Relevance Feedback EURASIP JASP, 2004 8  B. S   Ma nj u n a t h  P  S a l e m b ie r  a nd T  S i ko r a I n tr o duc tio n to MP E G 7: Multimedia Content Description Standard, Wiley, New York 2001 9  O  Mar q ue s  N  B a r m a n  S e m iau t o m a ti c S e m a n t i c A nno ta tio n o f  Images Using Machine Learning Techniques,î D. Fensel, K. Sycara and J. Mylopoulos, editors, Proceedings of the International Semantic Web Conference, 2003, pp. 550ñ565, Florida 10 N. R a si w a si a  P. J. Moreno  N  Vasconcelos  Bridging the Gap Query by Semantic Example,î IEEE Transactions On Multimedia vol. 9\(5\, 2007, pp. 923-938   O   R Z a ian e  M L  A n t o ni e  A   C o m a n A pp l i cati o n o f D a ta   Minin g  Techniques for Medical Image Classificationî, Proceedings of Workshop on Multimedia Data Mining \(MDM/KDDí2002\, San Francisco, USA, 2002 
131 


     Figure 9. Argumentation Tree For Open GL  A1 The current system in flash does not have the functionality of dynamic allocation of particles like mine or clutter. It places them randomly  A1.1 That is not of much importance because it still gives a new position to mine and clutter particles A2 Current system in flash has faster response time as compared to system in Adobe Director A3 The current system doesnêt satisfy many of the features required for the new system like database A4 Adobe Flash cannot communicate with database A4.1 Flash doesnêt support database but database support is very important and critical A4.1.1 The system should be able to generate evaluation reports for trainee based on pr evious records stored in the database A5 Flash doesnêt create sound clips  A5.1 We donêt need sound creating features as the sys tem has to generate sound. We can play externally recorded sound files using Adobe Flash A6 Flash can provide good visual effects as compared to Adobe Director A7 The developer has good knowledge in development using Flash so the system can be developed quickly B1 We could reuse the system already developed for sound generation, as it is developed using Adobe Audition for analysis which is somehow related to Adobe Director B1.1 The current system is better synthesized in terms of sound production and the sound produced is also instantaneous rather than discrete B1.2 That current system has certain performance issues like slow response time B1.3 The current system in Adobe Director has the feature of producing dynamic coloring scheme on approaching a mine. This kind of scheme is highly preferable and is not present in Adobe Flash system B2 Adobe Director can provide more functionality as compared to the current flash system. E.g. Multiple sounds while detecting mines   B2.1 Adobe Director can provide better visual effects as compared to flash e.g. in case of GUIês   B2.2 A modified version of the current system in flash can also provide the same functionality B2.2.1 We cannot integrate code developed in other platforms with Flash, but Flash can be integrated in Adobe Director B3 The interface provided by flash is not professional enough. It is too simple and straight forward for doing more things in future   B4 Easily available plug-ins can help integrate the tracking system developed in C# with Adobe Director  B4.1 Code developed in Open GL/AL can also be integrated using Adobe Director using suitable stubs   B5 A new sound recognition algorithm is being developed in Adobe Audition which can be integrated with Adobe Director but not with Open GL or Flash Evidence supported B6 If the current system is reused; the project deadline can be met easily B7 The developer has very little experience in development using Adobe Director   B7.1 The developer can take help from the already developed system in Adobe Director C1 The tracking software already developed is coded in C#/NX5. We could reuse that and develop our system in Open GL/AL C1.1 Open GL has C# libraries which can be used to develop the system C2 Because the platform used is for high end application development, it can provide good GUI and database support C2.1 Open GL/AL can help us generate dynamic surfaces for mine detection and training which the original system in flash does not have C4 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C3 Open GL does not support connectivity with Adobe Audition. Adobe Audition is required for creating sound recognition algorithm C4 The time taken for developing the project using open GL will be comparatively more as the whole system would have to be developed from scratch C4.1 If Open GL has support for C# libraries, and then the system could be develope d faster as developer is quite familiar with programming languages like C 151 


   C4.2 Open GL has excellent documentation that could help the developer learn the platform with ease C4.3 Developer has very little ex perience in working with Open GL platform  For our case study, alternative B i.e. Adobe Director was the most favorable alternative amongst all the three. It catered to the reusability criteria quite well and aimed at meeting most of the desired operational requirements for the system   6. CONCLUSION & FUTURE WORK  The main contribution of this paper is to develop an approach for evaluating performance scores in MultiCriteria decision making using an intelligent computational argumentation network. The evaluation process requires us to identify performance scores in multi criteria decision making which are not obtained objectively and quantify the same by providing a strong rationale. In this way, deeper analysis can be achieved in reducing the uncertainty problem involved in Multi Criteria decision paradigm. As a part of our future work we plan on conducting a large scale empirical analysis of the argumentation system to validate its effectiveness   REFERENCES  1  L  P Am g o u d  U sin g  A r g u men ts f o r mak i n g an d  ex p lain in g  decisions Artificial Intelligence 173 413-436, \(2009 2 A  Boch m a n   C ollectiv e A r g u men tatio n    Proceedings of the Workshop on Non-Monotonic Reasoning 2002 3 G  R Bu y u k o zk an  Ev alu a tio n o f sof tware d e v e lo p m en t  projects using a fuzzy multi-criteria decision approach Mathematics and Computers in Simualtion 77 464-475, \(2008 4 M T  Chen   F u zzy MCD M A p p r o ach t o Selec t Serv ice  Provider The IEEE International Conference on Fuzzy 2003 5 J. Con k li n  an d  M. Beg e m a n   gIBIS: A Hypertext Tool for Exploratory Policy Discussion Transactions on Office Information Systems 6\(4\: 303  331, \(1988 6 B P  Duarte D e v elo p in g a p r o jec ts ev alu a tio n sy ste m based on multiple attribute value theroy Computer Operations Research 33 1488-1504, \(2006 7 E G  Fo rm an  T h e  A n a l y t ic Hier a rch y P r o cess A n  Exposition OR CHRONICLE 1999 8 M. L ease  an d J L  L i v e l y  Using an Issue Based Hypertext System to Capture Software LifeCycle Process Hypermedia  2\(1\, pp. 34  45, \(1990 9  P e id e L i u   E valu a tio n Mo d e l o f Custo m e r Satis f a c tio n o f  B2CE Commerce Based on Combin ation of Linguistic Variables and Fuzzy Triangular Numbers Eight ACIS International Conference on Software Engin eering, Artificial Intelligence Networking and Parallel Distributed Computing, \(pp 450-454 2007  10  X  F L i u   M an ag e m en t o f an In tellig e n t A r g u m e n tatio n  Network for a Web-Based Collaborative Engineering Design Environment Proceedings of the 2007 IEEE International Symposium on Collaborative Technologies and Systems,\(CTS 2007\, Orlando, Florida May 21-25, 2007 11 X. F L i u   A n In ternet Ba se d In tellig e n t A r g u m e n tatio n  System for Collaborative Engineering Design Proceedings of the 2006 IEEE International Symposium on Collaborative Technologies and Systems pp. 318-325\. Las Vegas, Nevada 2006 12 T  M A sub jec tiv e assess m e n t o f altern ativ e m ission  architectures for the human exploration of Mars at NASA using multicriteria decision making Computer and Operations Research 1147-1164, \(June 2004 13 A  N Mo n ireh  F u zzy De cisio n Ma k i n g b a se d o n  Relationship Analysis between Criteria Annual Meeting of the North American Fuzzy Information Processing Society 2005 14 N  P a p a d ias HERMES Su p p o rti n g A r g u m e n tative  Discourse in Multi Agent Decision Making Proceedings of the 15th National Conference on Artifical Intelligence \(AAAI-98  pp. 827-832\dison, WI: AAAI/MIT Press,  \(1998a 15  E. B T riantaph y llo u   T h e Im p act o f  Ag g r e g atin g Ben e f i t  and Cost Criteria in Four MCDA Methods IEEE Transactions on Engineering Management, Vol 52, No 2 May 2005 16 S  H T s a u r T h e Ev alu a tio n o f airlin e se rv ice q u a lity b y  fuzzy MCDM Tourism Management 107-115, \(2002 1 T  D W a n g  Develo p in g a f u zz y  T O P S IS app r o ach  b a sed  on subjective weights and objective weights Expert Systems with Applications 8980-8985, \(2009 18 L  A  Zadeh  F u z z y Sets   Information and Control 8  338-353, \(1965  152 


                        





