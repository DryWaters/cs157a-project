Using Text Mining to Infer Semantic Attributes for Retail Data Mining Rayid Ghani and Andrew E Fano Accenture Technology Labs 161 N Clark St Chicago IL 60601 Rayid.Ghani Andrew.E.Fano@accenture.com Abstract Current Data Mining techniques usually do not have a mechanism to automatically infer semantic features inher ent in the data being 223mined\224 The semantics are either injected in the initial stages by feature construction or by interpreting the results produced by the algorithms Both 
of these techniques have proved effective but require a lot of human effort In many domains semantic information is implicitly available and can be extracted automatically to improve data mining sysrems In this paper; we present a case study ofa system that is trained to extract semantic fea tures for apparel products and populate a knowledge base with these products and features We show that semantic features of these items can be successfully ertracted by ap plying text learning techniques to the descriptions obtained from websites of retailers 
We also describe several applica tions of such a knowledge base of product semantics that we have built including recommender systems and competitive intelligence tools and pmvide evidence that our appmach can successfully builda knowledge base with accurate facts which can then be used to create pmfiles of individual cus tomers gmups of customers or entire retail stores 1 Introduction Current Data Mining techniques usually do not automat ically take into account the semantic features inherent in the data being \223mined\224 In most data mining applications a large amount 
of transactional data is analyzed without a systematic method for 223understanding\224 the items in the transactions or what they say ahout the customers who pur chased those items The majority of algorithms used to an alyze transaction records from retail stores treat the items in a market basket as objects and represent them as cate gorical values with no associated semantics For example in an apparel retail store transaction a basket may contain a shirt a tie and a jacket When data mining algorithms such 
as association rules decision trees neural networks etc are applied to this basket they completely ignore what these items \223mean\224 and the semantics associated with them Instead. these items could just he replaced by distinct sym bols, such as A B and C or with apple, orange and banana for that matter, and the algorithms would produce the same results The semantics of particular domains are injected into the data mining process in one of the following two stages: In the initial stage where the features to be used are constructed e.g for decision trees or neural networks fea 
ture engineering becomes an essential process that uses the domain knowledge of experts and provides it to the algo rithm The second instance where semantics are utilized is in interpreting the results Once association rules or deci sion trees are generated and A and B are found to be cor related, the semantics are then used by humans to interpret them Both of these methods of injecting semantics have been proved to be effective hut at the same time are very costly and require a lot of human effort In many domains semantic information is 
implicitly available and can he automatically extracted In this paper we describe a system that extracts semantic features for ap parel products and populates a knowledge base with these products and features We use apparel products and show that semantic features of these items can he successfully ex tracted by applying text leaming techniques to the product names and descriptions obtained from wehsites of retailers We also describe several applications of such a knowledge base of product semantics including recommender systems and competitive intelligence and provide evidence that our approach can successfully build a knowledge base with ac curate facts which can then be used to create profiles of individual customers groups 
of customers or entire retail stores The work presented in this paper was motivated by dis cussions with CRM expens and retailers who currently an alyze large amounts of transactional data hut are unable to systematically understand the semantics of an item. For ex ample a clothing retailer would know that a particular cus tomer bought a shin and would also know the SKU date time, price size and color of a particular shirt that was pur 0-7695-1754-4/02 17.00 0 2002 IEEE  195 


chased While there is some value to this data there is a lot of information not being captured that would facilitate un derstanding the tastes of the customer and enable a variety of applications For example is the shirt flashy or conser vative How trendy is it  Is it casual or formal These softer attributes that characterize products and the peo ple who buy them tend not to be available for analysis in a systematic way In this paper we describe our work on a system capa ble of inferring these kinds of attributes to enhance product databases The system learns these attributes by applying text learning techniques to the product descriptions found on retailer web sites This knowledge can be used to create profiles of individuals that can he used for recommendation systems that improve on traditional collaborative filtering approaches and can also be used to profile a retailer's posi tioning of their overall product assortment how it changes over time and how it compares to their competitors Al though the work described here is limited to the apparel do main and a particular set of features we believe that this approach is relevant to a wider class of data mining prob lems and that extracting semantic clues and using them in data mining systems can add a potentially valuable dimen sion which existing data mining algorithms are not explic itly designed to handle 2 Related Work There has been some work in using textual sources to create knowledge bases consisting of objects and features associated with these objects Craven et a1.[31 as part of the Web group at Camegie Mellon University built a system for crawling the Web \(specifically websites of CS departments in universities and extract names of entities students faculty, courses research projects, departments and relations \(course X is taught by faculty Y faculty X is the advisor of student Y etc by exploiting the content of the documents as well as the link structure of the web This system was used to populate a knowledge base in an ef fort to organize the Web into a more structured data source but the constructed knowledge base was not used to make any inferences Recently Ghani et al 6 extended the WebKB framework by creating a knowledge base consist ing of companies and associated features such as address phone numbers employee names competitor names etc extracted from semi-structured and free-text sources on the Web They applied association rules decision trees rela tional learning algorithms to this knowledge base lo infer facts about companies Nahm  Mooney  111 also report some experiments with a hybrid system of rule-based and instance-based learning methods to discover soft-matching rules from textual databases automatically constructed via information extraction 3 Overview of our approach At a high level our system deals with text associated with products to infer a predefined set of semantic features for each product These features can generally be extracted from any information related to the product but in this paper we only use the descriptions associated with each item The features extracted are then used to populate a knowledge base which we call the product semantics knowledge base The process is described below 1 Collect information about products 2 Define the set of features to be extracted 3 Label the data with values of the features 4 Train a classifierlextractor to use the labeled training data to now extract features from unseen data 5 Extract Semantic Features from new products by using the trained classifierlextractor 6 Populate a knowledge base with the products and cor responding features 4 Data Collection We constructed a web crawler to visit web sites of sev eral large apparel retail stores and extract names urls de scriptions prices and categories of all products available This was done very cheaply by exploiting regularities in the html structure of the websites and manually writing wrappers We realize that this restricts the collection of data from websites where we can construct wrappers and although automatically extracting names and descriptions of products from arbitrary websites would be an interesting application area for information extraction or segmentation  we decided to take the manual approach The extracted items and features were placed in a database and a random subset was chosen to be labeled 5 Defining the set of features to extract After discussions with domain experts, we defined a set of features that would be useful to extract for each prod uct We believe that the choice of features should be made with particular applications in mind and that extensive do main knowledge should be used We currently infer values for 8 kinds of attributes for each item but are in the pro cess of identifying more features that are potentially inter esting The features we use are Age Group Functionality In our case the wrappers were simple regular expressions that took the html content of web pages into account and exmacled specific pieces of information 196 


Price point, Formality. Degree of Conservativeness Degree of Sportiness Degree of Trendiness and Degree of Brand Appeal. More details including the possible values for each feature are given in Table 1 The last four features conservative sportiness trendi ness and brand appeal have five possible values 1 to 5 where 1 corresponds to low and 5 is the highest e.g for trendiness 1 would he not trendy at all and 5 would he ex tremely trendy Rule Informal  Sportswear Informal  Loungewear Informal Juniors PricePoint=Ave  BrandAppeal=2 BrandAppeal5  Trendy=5 Sportswear  Sporty=4 AgeGroup=Mature  Trendy=l 6 Labeling Training Data Support Confidence 24.5 93.6 16.1 82.3 12.1 89.4 8.8 79.0 16.3 91.2 9.0 85.7 9.4 78.8 The data product name, descriptions categories price collected by crawling wehsites of apparel retailers was placed into a database and a small subset  600 products was given to a group of fashion-aware people to label with respect to each of the features described in the previous sec tion They were presented with the description of the prede fined set of features and the possible values that each feature could take listed in Table I 7 Verifying Training Data Since the data was divided into disjoint subsets and each subset was labeled by a different person we wanted to make sure that the labeling done by each expert was consistent with the other experts and there were no glaring errors One way to do that would he to now swap the subsets for each person and ask the other lahelers to repeat the pro cess on the other set Obviously, this can get very expen sive and we wanted to find a cheaper way to get a general idea of the consistency of the labeling process For this pur pose we decided to generate association rules on the la beled data We pooled all the data together and generated association rules between features of items using the apriori algorithm[l The particular implementation that we used was 121 By treating each product as a transaction bas ket and the features as items in the basket this scenario becomes analogous to the traditional market-basket analy sis For example a product with some unique ID say Polo V-Neck Shirt which was labeled as Age Group Teens Functionality Loungewear, Pricepoint Average Formal ity Informal Conservative 3 Sportiness 4 Trendiness:4 Brand Appeal 4 becomes a basket with each feature value as an item By using Apriori algorithm we can derive a set of rules which relate multiple features over all products that were labeled We ran apriori with both single and two feature antecedents and consequents Table 2 shows some sample rules that were generated By analyzing the association rules we found a few in consistencies in the labeling process where the labelers mis understood the features As we can see from Table 2 the association rules match our general intuition e.g Apparel items labeled as informal were also labeled as sportswear and loungewear Items with average prices did not have high brand appeal  this was probably because items with high brand appeal are usually more expensive An interest ing rule that was discovered was that items that were labeled as belonging to Mature Age group were also labeled as be ing not trendy at all Using association rules over the entire labeled data proved to he very useful in verifying the consistency of the labeling process done by several different lahelers and we believe would he a useful tool for data verification in gen eral where the labeling is performed by multiple people 8 Training from the Labeled Data We treat the learning problem as a traditional text classi fication problem and create one text classifier for each \224se mantic feature\224 For example in the case of the Age Group feature we classify the product into one of five classes Ju niors, Teens GenX, Mature All Ages The initial algo rithm used to perform this classification was Naive Bayes and a description is given below 8.1 Naive Bayes Naive Bayes is a simple but effective text classification algorithm[lO 91 Naive Bayes defines an underlying gener ative model where first a class is selected according to class prior probabilities Then, the generator creates each word in a document by drawing from a multinomial distribution over words specific to the class Thus this model assumes each word in a document is generated independently of the others given the class. Naive Bayes forms maximum a pos teriori estimates for the class-conditional probabilities for each word in the vocabulary V from labeled training data D. This is done by counting the frequency that word 221wt oc curs in all word occurrences for documents di in class cj supplemented with Laplace smoothing to avoid prohahili ties of zero 197 


Feature Name Age Group Functionality Pricepoint Formality Conservative  I clothes I I Timeless Classic to 5 Current I Is this item WDUIX now but likelv to I servative or flashy Sportiness I I to5 Trendiness Possible Values Description Juniors Teens GenX, Mature All Ages propriate Loungewear Sportswear Evening wear Business Casual Business Formal Discount. Average Luxury Informal  Somewhat Formal Very Formal l\(gray suits to 5 Loud flashy Does this suggest the person is con For what ages is this item most ap How will the item be used Compared to other items of this kind is this item cheap or expensive How formal is this item  I favorite I go out of style or is it more time I Brand Appeal I\(Brand makes the product unap pealing to 5 high brand appeal Is the brand known and makes it ap pealing  where N\(wt d is the count of the number of times word wt occurs in document d and where Pr\(c Id E 0,1 as given by the class label At classification time we use these estimated parameters by applying Bayes\222 rule to calculate the probability of each class Pr\(cJ\(d a Pr\(cJ Pr\(d,lcJ 1d.l  WJ n pr\(wd,,kl 2 k=l 8.2 Incorporating Unlabeled Data using EM In our initial data collection phase we collected names and descriptions of thousands of women\222s apparel items from websites Since the labeling process was expensive we only labeled about 600 of those, leaving the rest as unla beled Recently there has been much recent interest in su pervised learning algorithms that combine information from labeled and unlabeled data Such approaches include using Expectation-Maximization to estimate maximum a posteri ori parameters of a generative model 12 using a gener ative model built from unlabeled data to perform discrim inative classification 7 and using transductive inference for support vector machines to optimize performance on a specific test set 8 These results have shown that using un labeled data can significantly decrease classification error especially when labeled training data are sparse For the case of textual data in general, and product de scriptions in particular obtaining the data is very cheap. A simple crawler can be build and large amounts of unlabeled data can be collected for very little cost Since we had a large number of product descriptions that were collected but unlabeled we decided to use the Expectation-Maximization algorithm lo combine labeled and unlabeled data for our task 8.2.1 Expectation-Maximization If we extend the supervised learning setting to include un labeled data the naive Bayes equations presented above are no longer adequate to find maximum a posteriori parameter estimates The Expectation-Maximization EM\technique can be used to find locally maximum parameter estimates EM is an iterative statistical technique for maximum likelihood estimation in problems with incomplete data 4 Given a model of data generation and data with some miss ing values EM will locally maximize the likelihood of the parameters and give estimates for the missing values The naive Bayes generative model allows for the application of EM for parameter estimation In our scenario the class la bels of the unlabeled data are treated as the missing values EM is an iterative two-step process Initial parameter estimates are set using standard naive Bayes from just the labeled documents Then we iterate the E- and M-steps The E-step calculates probabilistically-weighted class la bels Pr\(cjld for every unlabeled document using Equa 198 


tion 2 The M-step estimates new classifier parameters us ing all the documents by Equation I where Pr\(cjIdi is now continuous as given by the E-step We iterate the E and M-steps until the classifier converges 9 Experimental Results In order to evaluate the effectiveness of the algorithms described above for building an accurate knowledge base we calculated classification accuracies using the labeled product descriptions and 5 fold cross-validation The eval uation was performed for each attribute and the table be low reports the accuracies The first row in the table base line gives the accuracies if the most frequent attribute value was predicted as the correct class The experiments with Expectation-Maximization were run with the same amount of labeled data as Naive Bayes but with an additional 3500 unlabeled product descriptions Looking at Table 3 we can see that Naive Bayes outper forms our baseline for all the attributes Using unlabeled data and combining it from the initially labeled product de scriptions with EM helps improve the accuracy even further To get a qualitative and intuitive feel for the performance of these algorithms and for the effectiveness of our approach Table 4 gives a list of words which had high weights for some of the features that we used the naive bayes classi fier to extract. There words were selected by scoring all the words according to their log-odds-ratio scores and picking the top IO words Looking at the words gives us a quali tative and intuitive idea of what type of words are indica tive of each attribute and verifies our initial hypothesis that the marketing language associated with product does corre spond to these softer attributes that we are trying to infer 9.1 Results on a new test set The results reported earlier in Table 3 are extremely en couraging but are indicative of the performance of the algo rithms on a test set that follows a similar distribution as the training set Since we first extracted and labeled product de scriptions from a retail website and then used subsets of that data for training and testing using 5 fold cross-validation the results may not hold for test data that is drawn from a different distribution or a different retailer The results we report in Table 5 are obtained by training the algorithm on the same labeled data set as before but test ing it on a small 125 items new labeled data set collected from a variety of retailers As we can observe the results are consistently better than baseline and in some cases even better than in Table 3 This results enables us to hypothe size that our system can be applied to a wide variety of data and can adapt to different distributions of test sets using the unlabeled data 1 10 Applications The knowledge base KB constructed by labeling un seen products has several applications In this section we describe some concrete applications that we have devel oped. The KB can be used to create profiles of individuals that can be used for recommender systems that improve on traditional collaborative filtering approaches and can also be used to profile a retailer\222s positioning of their overall product assortment how it changes over time and how it compares to their competitors 10.1 Recommender systems Being able to analyze the text associated with products and map it to the set of predefined semantic features in real time gives us the ability to create instant profiles of cus tomers shopping in an online store As the shopper browses products in a store, the system running in the background can extract the name and description of the items and using the trained classifiers can infer semantic features of that product This process can be used create instant profiles based on viewed items without knowing the identity of the shopper or the need to retrieve previous transaction data This can be used to suggest subsequent products to new and infrequent customers for whom past transactional data may not be available Of course if historical data is avail able our system can use that to build a better profile and recommend potentially more targeted products We believe that this ability to engage and target new customers tackles one of the challenges currently faced by commercial recom mender systems 13 and can help retain new customers We have built a prototype of a recommender system for women\222s apparel items by using our knowledge base of product semantics More details about the recommender system can be found in  knowledge base is popu lated with thousands of items and their associated seman tic attributes inferred by the learning algorithm described in earlier sections Our system monitors the browsing be havior of user browsing a retailer\222s website and in real time, extracts names and descriptions of products that they browse The description text is then passed through our learned models and the semantic attributes of the products are inferred For each product browsed, our system calcu lates P\(Ai,j PrOduct where 4,:j is the jth value of the ith attribute The attributes are the semantic features de scribed in Table l and the possible values for each attribute are also listed in the table The user profile is constructed by combining these probabilities for each product browsed User Profile     99 


Table 3 Classification accuracies for each attribute using 5 fold cross-validation Naive Bayes uses only labeled data and EM uses both labeled and unlabeled data agegroup=juniors jrs dkny jeans tee collegiate logo tommy polo short sneaker I Algonihm I AgeCroup I Funcrionality I Formality I Comervative I Spaniness 1 lrendiness  BrandAppeal 1 I Racelin 1299 I J4 I 6x4 Ti;i I496 729al-%Y7 1 Functionality=Loungewear chemise silk kimono Calvin klein august lounge hilfiger robe gown _     _  _        NaiveBayes I 66 I 57 I 76 I 80  70 169 I 82 EM 178 1 70 I 82 I 84 I 78 1 80 191 Table 4 For each class the table shows the ten words that are most highly weighted by one of our learned models The weights shown represent the weighted log-odds ratio of the words given the class 3rand Appeal=S\(high lauren ralph dkny kenneth cole imported Conservative=S\(high lauren ralph breasted seasonless trouser jones sport classic blazer Conservative I low rose special leopard chemise straps Rirty silk platform Functionality=Partywe rock dress sateen length skin shirtdress open platform plaid flower spray ormality=Informal jean tommy jeans denim sweater pocket neck tee hilfiger jponiness=S\(high sneaker camp base sole white miraclesuit athletic nylon mesh rubber Somewhat Formal jacket fully button skirt lines york seam crepe leather Trendiness 1 lo lauren seasonless breasted trouser pocket carefree ralph blazer button Table 5 Classification accuracies when trained on the same labeled data as before but tested on a new set of test data that is collected from a new set of retailers Algorithm NaiveBayes I 83 145 I 61 I 70 181 I 80 I 87 I AgeCroup  Functionality I Formality 1 Conservative I Sportiness I Trendiness I BrandAppeal 200 


The user profile is stored in terms of probabilities for each attribute value which allows us flexibility to include mixture models in future work in addition to being more robust to changes over time As the user browses products the system compares the evolving profile against the products in the knowledge base which has products classified into the same taxonomy of semantic features and recommends the closest matching ones Currently we give equal weight to all products browsed when constructing the profile In future work we plan to experiment with different weighting schemes such as weighting recent items more than older ones There are two prevalent approaches to building rec ommender systems  Collaborative Filtering and Content based Collaborative Filtering systems work by collecting user feedback in the form of ratings for items in a given domain and exploit similarities and differences among pro files of several users to recommend an item. It recommends other items bought by people who also bought the current item of interest and completely ignores what the current item of interest was. Collaborative Filtering approaches suf fer from two main problems the sparsity problem that most customers do not browse or buy most products in a store and the New Item problem that a new product cannot be recommended to any customer until it has been browsed by a large enough number of customers On the other hand content-based methods provide recommenda tions by comparing representations of content contained in an item to representations of content that interests the user A main criticism of content-based recommendation systems is that the recommendations provided are not very diverse Since the system is powered solely by the user's preferences and the descriptions of the items browsed it tends to recom mend items too similar to the previous items of interest This type of recommender system improves on collab orative filtering as it would work for new products which users haven't browsed yet and can also present the user with explanations as lo why they were recommended certain products in terms of the semantic attributes We believe that our system also performs better than standard contenl based systems Although content-based systems also use the words in the descriptions of the items they traditionally use those words to learn one scoring function For exam ple a classical content-based recommendation engine takes the text from the descriptions of all the items that user has browsed or bought and learns a model usually a binary target function recommend or not recommend In contrast our system changes the feature space from words thousands of features\to the eight semantic attributes This still enables us to recommend a wide variety of products un like most content-based systems Another potential advan tage of our system is its ability to suggests products across categories i.e. apparel styles may be predictiveof furniture for example which content-based systems are not able to do Since our goal was not to build the best recommendation system but rather to demonstrate the potential of a knowl edge base of product semantics we did not explore many approaches to building a user's profile In future work we plan to tackle the cases where a user's profile consists of a number of separate profiles For example if a user is look ing for something for herself and also for her son our sys tem should be able to recognize that the items that the user is buying or browsing are inherently different This could be done through mixture models where we construct a pro file using a mixture of different profiles Another potential solution is to monitor the users profile as they browse more and more products Since each product can be though of as a point in an n-dimensional Euclidean space where 11 is the number of features in our case 8 we can calculate the dis tance of a new product from the current profile of the user If a new product is very different from the current profile of the user using thresholds based on cross-validation it can be placed in a separate profile or treated as an outlier We also plan to conduct user studies to validate the effec tiveness of such a recommendation system based on these intermediate-level semantic features 10.2 Store Profiling Product recommendations are just one application that we have built so far. We also have a prototype that profiles retailers to build competitive intelligence applications For example by closely tracking the product offerings we can notice changes in the positioning of a retailer We can track changes in the industry as a whole or specific competitors and compare it to the performance of retailers By profiling their aggregate offerings, our system can enable retailers to notice changes in the positioning of product lines by com petitor retailers and manufacturers This ability to profile retailers enables strategic applications such as competitive comparisons monitoring brand positioning, tracking trends over time etc 11 Conclusions and Future Work We described our work on a system capable of infer ring semantic attributes of products enabling us to enhance product databases for retailers The system learns these at tributes by applying supervised and semi-supervised learn ing techniques to the product descriptions found on retailer web sites One of the main assumptions we make is the descriptions associated with the products accurately convey the semantic attributes We believe that this assumption is justified because in most cases these descriptionsare written by marketers to position the product in the consumer's mind 201 


in a manner that implicitly suggests these softer attributes The system can he bootstrapped from a small number of la beled training examples utilizes the large number of cheaply obtainable unlabeled examples product descriptions avail able from retail websites In the prototype we have built at Accenture Technology Labs we currently have several applications for this type of a knowledge base We use it to create profiles of individuals that can be used for recommendation systems that improve on traditional collaborative filtering approaches The abil ity to infer the semantics of products can also be used to profile a retailer's positioning of their overall product as sortment how it changes over time and how it compares to their competitors Although the work described here is limited to the apparel domain and a particular set of fea tures we believe that this approach is relevant to a wider class of data mining problems We believe that by going beyond the immediately available data such as the fact that a customer is looking at or bought a product and paying attention to what these products mean we can increase the effectiveness of data mining applications References I R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast discovery of association rules In U Fayyad G Piatetsky-Shapiro P Smyth and R Uthurusamy editors Advances in Knowledge Dis cnveT and Dara Mining AAA1 Pressflhe MIT Press pages 307-328.1996 21 C Borgelt apriori http://fuzzy.cs.Uni Magdeburg de/-borgelt 131 M Craven D DiPasquo D Freitag A McCallum T Mitchell K Nigam and S Slattery Learning to construct knowledge bases from the world wide web Arrifciallnrelligence I18\(1-2 41 A P Dempster, N. M. Laird and D B Rubin Max imum likelihood from incomplete data via the EM al gorithm Journal of rhe Royal Statistical Socieq Se ries 39\(1 1977 SI R Ghani and A E Fano. Building recommender sys tems using a knowledge base of product semantics In Proceedings of the Workshop on Recommendation and Personalimrion in ECommerce at the 2nd Inrerna tional Conference on Adaprive Hypermedia and Adap tive Web based Systems 2002 6 R Ghani R Jones D Mladenic K Nigam and S Slattety Data mining on symbolic knowledge ex tracted from the web In Workshop on Text Mining ar the Sixth ACM SICKDD Infernarional Conference on Knowledge Discovery and Data Mining 2000 171 T Jaakkola and D Haussler Exploiting generative models in discriminative classifiers In Advances in NIPS II 1999 181 T Joachims Transductive inference for text classi fication using support vector machines In Machine Learning Proceedings of the Sixteenth Intemtional Conference 1999 9 D D Lewis Naive Bayes at forty The indepen dence assumption in information retrieval. In Machine Learning ECML-98 Tenfh Eumpean Conference on Machine Learning pages 4-15 1998 1101 A McCallum and K Nigam A comparison of event models for naive Bayes text classification In Learning for Exr Categorizarion Papers fmm the AAA1 Work shop pages 4148.1998 Tech rep WS-98-05 AAAI Press I I U Y Nahm and R J. Mooney. Text mining with infor mation extraction In AAA1 2002 Spring Symposium on Mining Answers fmm Texts and Knowledge Bases 2002 I21 K Nigam A McCallum S Thrun. and T Mitchell Text classification from labeled and unlabeled docu ments using EM Machine Learning 39\(U3 134.2000 I31 J Schafer J Konstan and J Riedl Electronic com merce recommender applications Journal of Data Mining and Knowledge Discovery 5:115-152,2000 I41 K Seymore A McCallum and R Rosenfeld Lean ing hidden Markov model structure for information extraction In Machine Learning for Information Extracrion Papers from the AAA1 Workhop 1999 Tech rep WS-99-1 I AAAI Press 202 


association-cube, base-cube and population-cube are derived from the volume cube; the confidence-cube is derived from the association cube and population cube and the support-cube is derived from the associationcube and base-cube. The slices of these cubes shown in Figure 2 correspond to the same list of values in dimension merchant, time, area and customer_group  Multidimensional and multilevel rules Representing association rules by cubes and underlying cubes by hierarchical dimensions, naturally supports multidimensional and multilevel rules. Also these rules are well organized and can be easily queried  First, the cells of an association cube with different dimension values are related to association rule instances in different scopes. In the association cube CrossSales cell CrossSales product \221A\222, product2 \221B\222  customer_group 221engineer\222, merchant \221Sears\222, area \221Los Angeles\222, time 221Jan98\222 represents the following multidimensional rule x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x,\221B\222  275 customer_group = \221engineer\222, merchant = \221Sears\222, area 221Los  Angeles\222, time =  \221Jan98\222 If this cell has value 4500, and the corresponding cell in the population cube has value 10000, then this rule has confidence 0.45 Next as the cubes representing rules can have hierarchical dimensions, they represent not only multidimensional but also multi-level association rules. For example, the following cells CrossSales\(product \221A\222, product2 \221B\222, customer_group 221engineer\222, merchant \221Sears\222, area \221 California 222, time 221Jan98\222 CrossSales\(product \221A\222, product2 \221B\222, customer_group 221engineer\222, merchant \221Sears\222, area \221 California 222, time 221 Year98 222 represent association rules at different area levels \(i.e the city level and the state level\d different time levels \(i.e., the month level, the year level x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222  275 customer_group = \221engineer\222, merchant =  \221Sears\222, area 221 California 222, time =  \221Jan98\222 x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222  275 customer_group = \221engineer\222, merchant =  \221Sears\222, area 221 California 222, time =  \221 Year98 222 The cell CrossSales\(product \221A\222, product2 \221B\222,  customer_group 221top\222, merchant \221top\222, area \221top\222,  time \221top\222 represents the customer-based cross-sale association rule for all customers, merchants, areas, and times in the given range of these dimensions, expressed as x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222 4.3  Generating Association Rule Related Cubes The basic task of our OLAP based association rule mining framework, either at the GDOS or at an LDOS is to convert a volume cube i.e. the cube representing the purchase volumes of customers dimensioned by product  area etc, into an association cube a base cube and a population cube These cubes are then used to derive the confidence cube and the support cube of multidimensional association rule instances. The following general steps are involved in cross-sale association rule mining 267  Roll up the volume cube SaleUnits by aggregating it along merchant, time, area dimensions 267  Derive cube NumOfBuyers from SaleUnits based on the antecedent condition SaleUnits 0 267  Populate cube NumOfShoppers by the counts of customers dimensioned by merchant, area  time not by product\at satisfy the antecedent conditions 267  Derive cube CrossSales from SaleUnits based on the association conditions SaleUnits  product  p 1  0 and SaleUnits  product2  p 2 0 267  Derive cube Confidence and cube Support using cell-wise operations 214  Confidence = CrossSales  NumOfBuyers 214  Support  CrossSales  NumOfShoppers  Cubes Confidence  Support  CrossSales are dimensioned by product  product2 customer_group,merchant  time, area NumOfBuyers is dimensioned by product  customer_group, merchant time, area  NumOfShoppers is dimensioned by customer_group, merchant  time, area Rules with confidence and support that exceed specified thresholds  may be considered interesting 4.4. Rules with Conjoint Items Cubes with conjoint dimensions can be used to represent refined multidimensional association rules For example, using OLAP, we can derive association rules across time  Time-variant or temporal association rules such as 


x 316 Customers buy_product\(x,\222 A\222, \221 Jan98\222  336 buy _product\(x, \221B\222, \221 Feb98\222   275 area = \221Los Angeles\222 can be used to answer such questions as \223 How are  the sales of B in Feb98  associated with the sales of A in Jan98 224 The items in this rule are value pairs of dimensions product and time In order to specify this kind of association rule we introduce a conjoint dimension product, time and mirror it with dimension product2, time2 This allows a cell in the association cube to cross two time values. Accordingly, the cubes related to association rule mining are defined as follows Association cube  CrossSales.2 \(<product, time>, <product2, time2 customer_group, merchant, area  Population cube  NumOfBuyers.2  \(<product, time>, customer_group merchant, area Base cube  NumOfShoppers.2  \( customer_group, merchant, area Confidence cube Confidence.2 \(<product, time>, <product2, time2 customer_group, merchant, area Support  cube  Support.2  product, time>, <product2, time2 customer_group, merchant, area  The steps for generating these cubes are similar to the ones described before. The major differences are that a cell is dimensioned by, besides others product, time and product2, time2 and the template of the association condition is  SaleUnit s  product p 1 time t 1  0 and  SaleUnits  product2 p 2 time2 t 2  0 where, in any instance of this condition, the time expressed by the value of time2 is not contained in the time expressed by the value of time The template of the antecedent condition is SaleUnits   product p 1 time t 1  0 In general, other dimensions such as area may be added to the conjoint dimensions to specify more refined rules 4.5. Functional Association Rules A multidimensional association rule is functional if its predicates include variables, and the variables in the consequent are functions of those in the antecedent.  For example, functional association rules can be used to answer the following questions, where a_month and a_year are variables q  What is the percentage of people in California who buy a printer in the next month after they bought a PC x 316 Customer buy_product\(x, \221PC\222, a_ month 336 buy_product\(x, \221printer\222, a_month+1  275 area = \221California\222 q  What is the percentage of people who buy a printer within the year when they bought a PC  x 316 Customer: buy_product\(x, \221PC\222, a_ year 336 buy_product\(x, \221printer\222, a_year 275 area = \221California\222 To be distinct, we call the association rules that are not functional as instance association rules; e.g x 316 Customer: buy_product\(x,\222 PC\222, \221Jan98\222 336 buy_product\(x,\222 printer\222, \221Feb98\222  275 area =  \221California\222 Time variant, functional association rules can be derived from time variant, instance association rules through cube restructuring. Let us introduce a new dimension time_delta that has values one_day, two_day 205, at the day level, and values one_month, two_month, \205, at the month level, etc. Then, let us consider the following functional association rule related cubes Association cube  CrossSales.3 \(product, product2, customer_group merchant, area, time_delta  Population cube  NumOfBuyers.3 \(product, customer_group, merchant area Base cube  NumOfShoppers.3 \( customer_group, merchant, area Confidence cube  Confidence.3 \(product, product2, customer_group merchant, area, time_delta Support cube  Support.3 \(product, product2, customer_group, merchant area, time_delta The association cube CrossSales.3  can be constructed from CrossSales.2   The cell values of CrossSales.2  in the selected time and time2 ranges are added to the corresponding cells of CrossSales.3 For example, the count value in cell  CrossSales.2\(<PC, Jan98>, <printer, Feb98>\205 is added to cell \(bin CrossSales.3\(PC, printer, one_month,\205 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


