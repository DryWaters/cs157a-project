Preliminary Results from a Machine Learning Based Approach to the Assessment of Student Learning Salvatore Valenti, Alessandro Cucchiarelli valenti, alex}@inform.unian.it Istituto di Informatica Ö Universit‡ Politecnica delle Marche Via Brecce Bianche - 60131 - Ancona - Italy Abstract This work describes a possible approach to the problem of extracting knowledge from the analysis of questionnaires through Machine Learning. The idea guiding our research was to investigate the existence of association rules among the topics covered in a course. The data used came from the questionnaires administered to the freshmen in 
Electronic Engineering attending the course of Foundation of Computer Science at our University. Each questionnaire was coded into feature vectors that  were classified with respect to the grade obtained by the student and analysed with C4.5 Some statistical results and hints for further work are discussed in the paper 1. Introduction This work is aimed at identifing the existence of correlations among the formative deficiencies emphasized by the presence of wrong answers to Multiple Choice Questions \(MCQ\o different topics. If such correlations do exist and can be elicited, a more appropriate feedback can be provided to the students 
Furthermore, the identification of unexpected or unforeseen correlation among topics may help the teacher to revise the didactic process In our experiments we used the C4.5 package, a classification system based on top-down induction of decision trees 4 5 provid es a set of software tools able to learn inductively models of different concepts classes\from a testbed, to build a classifier in the form of a decision tree and to use it for unknown data classification. Furthermore, C4.5 provides a tool for reexpressing the classification tree via production rules in the simplified form L -> R where 
L is a conjunction of attribute-based tests and R is a class. Each invocation of the classifier over a set of cases produces an output containing its performance on the cases from which it was constructed. Thus, after classes induction, C4.5 applies the classification to the same cases used for training, and evaluates the percentage of errors made: the less this error is, the better the model is. The package contains heuristic methods for simplifying decision tree, with the aim of producing more comprehensible structures without compromising accuracy on unseen cases. Both tree \(or rules\nd error values related to the simplified tree are 
printed to the output stream. Finally, C4.5 provides a module that allows the evaluation of the accuracy of a classification model through cross-validation. In this procedure, the available data is divided into N blocks so as to make each blockês number of cases and class distribution as uniform as possible. N different classification models are the built, in each of which one block is omitted from the training data, and the resulting model is tested on the cases in that omitted block: In this way, each case appears in exactly one test set. Provided that N is not too small the average error rate over the N 
unseen test set is a good predictor of the error rate of a model built from all the data 2. Description of the Data Sample The data used for knowledge discovery came from 1322 questionnaires administered to the freshmen in Electronic Engineering attending the course of Foundation of Computer Science during the academic years ranging from 1995 to 1998. The data set has been filtered by removing low-end and high-end achievers in order to obtain a subset containing information free from border effects. After this filtering, the data set obtained contained 436 questionnaires. Ten MCQ with four answers constituted a questionnaire. Each question was given a 
weight ranging from 1 to 5 points, representing the relative importance of the question inside the curriculum Thus, the minimum score that may be obtained by a student is zero and the maximum is 30 The number of different questions in the database is 150 They have been classified according to the topic covered with a taxonomy of 15 items derived from the course syllabus. Each questionnaire was transformed in a set of feature vectors  fv  A 1 
A 2 A 3 A 15 P where  A i is 1 if the question containing the topic i is correctly answered, 2 if the answer to the question containing topic i is wrong or missing and 0 if no Proceedings of the The 3rd IEEE International Conference on Advanced Learning Technologies \(ICALTê03 0-7695-1967-9/03 $17.00 © 2003 IEEE 


question addressing topic i is present in the questionnaire  P is çpooré if the score to the questionnaire is between 12 and 17, çaverageé if the score of the questionnaire is between 18 and 20, and çgoodé if the score is between 21 and 26 The 436 questionnaires allowed generating 7884 fv this is due to the fact that the same topic may be covered by different MCQ in a questionnaire. This required to generate all the possible combinations of different answers to the same topic The analysis conducted with the C4.5 program allowed constructing a decision tree that classifies the fv under examination with an error rate of 11.9%. Furthermore, a list of about two hundred production rules was generated from the decision tree. The rules allow to classify the fv with an overall error of 13.4%. Thus, for instance, the following rule has been generated Rule 1 Scope of variables = 2  Parameters = 2  class çpooré  [98.7 This rule allows to infer that if a student does not answer correctly to questions regarding the topics: çScope of Variablesé and to çParametersé is ranked çpooré with a confidence factor of 98.7%. Therefore, the two topics seem to be related 3. Discussion Three classes of production rules have been identified by our analysis. The first category puts in relation topics that according to our knowledge are strongly related. Rule 1 is an instance of this class and is further strengthened by the existence of a complementary rule stating that "if a student answers correctly to questions regarding the same topics, his/hers questionnaire will obtain a score comprised between 21 and 26 with a confidence factor of 99.8%". Both rules represent two examples of straightforward associations, since we strongly believe that a student failing to catch the concept of çScope of variablesé will have difficulties in understanding the correct use of çParametersé and vice versa The second category of rules puts in relation topics that although not being predictable by our experience, may be understood after a deeper analysis of the questions. Thus for instance, a rule does exist allowing to infer that a student is ranked çpooré with a confidence factor of 96.6% if his/hers questionnaire contains wrong answers to questions related to Grammars & EBNF, to Side Effects and to Recursion. While we believe that it is possible to put in relation the use of functions containing Side Effects and Recursion, so that a misconception on each of the topics may affect the other, it appears difficult to understand in which way the topic Grammars & EBNF should be put in relation with the formers. After a careful analysis, we discovered that most of the questions related to this latter topic contain excerpts of production rules involving recursive definitions. Thus, the conclusion that a student failing to understand the concept of recursion may have difficulties in answering questions involving recursive production rules does not appear as an extremely odd idea The last category of rules put in relation topics that do not appear to be related each other in any way. Thus, it is very hard to understand the meaning of a rule that allowa to infer that if a student answers correctly both to a question related to the use of çIntegers and Realsé and to a question covering the topic çSide Effectsé, is ranked good with a confidence factor of 96.1 We believe that this last category of rules deserves further investigation, since it may be useful to obtain a better understanding on the way students perform We decided to adopt the cross-validation utility of C4.5 to verify that this last class of rules is not due to some sort of statistical error. The results obtained by dividing the data set into ten block of randomly selected samples are very encouraging since the average error rate is 15,9% if the decision tree is applied and 16.2% if the rules are used This seems to indicate that the inferred rules are slightly independent from the adopted data set 4. Final Remarks and hints for future work There is a number of open questions about the approach discussed in this paper that call for further investigation Thus, for instance, the same topic is covered by questions having different weights. We need to explore how, and to what extend, this aspect may influence the elicited rules Furthermore, the same question may cover different topics. In fact, a number of questions aimed to assess the scoping of variables are written using in Pascal. How the knowledge of the programmi ng language may affect the answer to these questions has not yet been investigated Moreover, for each topic there exist questions aimed at verifying different levels of competence, ranging from  a y be  related to the results obtained, is still a matter of investigation. Finally, we need to gain a deeper understanding of the rules that put in relation topics that appear to be completely independent each other References  Bloom B. S. \(19 56 y o f education al objectives The classification of educational goals: Handbook I cognitive domain.: Longmans Green, New York, NJ, USA  Quinlan J.R 19 93 5: Progr ams for Machin e Learning  Morgan Kaufmann Publishers Inc., San Mateo, CA. USA Proceedings of the The 3rd IEEE International Conference on Advanced Learning Technologies \(ICALTê03 0-7695-1967-9/03 $17.00 © 2003 IEEE 


For instance the sequence al a4 a as aq supports the pattern al,a4,*,:;a4 while the sequence alraq;a2;a3,ae does not support it since the sequence is not in compliance with the pattern on the last position Definition 2.6 Given a parrern P with period I and a se quence D of N\(N 2 I evenrs dl,dz  dN the support of P within D is the number of subsequences drxj+c that support P Intuitively the event sequence can be viewed as a list of segments each of which consists of 1 contiguous events There would be LN/I full segments, among which the segment that P matches will count for the support of P Definition 2.1 Given a pattern P  PI p riod 1 and a sequence of 1 events D  information loss of D on position j with respect to P is the information of the event pj iff D is nor in compliance with P at position j and there is no information loss otherwise The overall information loss of D with respect to P is the summa tion of the information loss of each position Definition 2.8 Given a partern P with pence D of N\(N 2 I events dl,dz mation loss of D with respect to P is the summation of the information loss of each segment drxj+i dix j+a   drxj+r with respect to P The generalized information gain of D withrespecttoPisdefinedasI\(P x SD\(P P where I\(P SD\(P and L,\(P are the information of P the support of P within D and the information loss of D with respect to P respectively In a subsequence, the first match of a paltern is viewed as an example and only subsequent matches contribute to the gen eralized information gain6 Definition 2.9 Given a pattern P a sequence D and a gen eralized information gain threshold 9 ifthere exists a subse quence D of D so that the generalized informarion gain of D with respect to P is at least y then P is a valid pattern Theoretically the period of a valid pattern could be arbitrary i.e as long as the event sequence. In reality a user can specify an upperbound of period length according to hidher domain knowledge As a result we use L to denote the maximum period allowed fora pattern. However L can be arbitrarily large, e.g ranging to several thousands. Now we can rephrase our problem model by employing the generalized information gain metric For a sequence of events D an information gain threshold y and a period bound L we want to discovery all valid patterns P whose period is less than L In addi tion for each valid pattern P we want to find the subsequence Since we aim at mining penodic patterns only repeated wcurence of a paltern are used to CCEUIIUIIC the generalized informstion gain which maximizes the generalized information gain of P In the remainder of this section we give some more definitions which enable us to present our approach and communicate to readers more effectively Definition2.10 For any hvo parterns P  plrpz2   pr and P  pi p   pi of the same period 1 P and P are complementary ifeither p   or pi  for all 1 5 j 5 1 A set of patterns of the same period are said to be comple menrary if every pair of patterns in the set are complementary Definition 2.11 Given a set II of complemenrary patterns of the same period 1 the minimum common superpattern MCSP of n is the pattern P of period I which satisfies the following WO conditions Each pattern in lI is a subpartern of P There does not exist I subparrern P of P P  P such that each patrern in Il is also a subpattern of P It follows from the definition that the information of the MCSP of a set II of complementary patterns is the summa tion of the information of each pattern inn For a given event segment D  dl d2     dt and a set II of complementary patterns the information loss of D with respect to the MCSP of Il satisfies the following equalily where LD\(P is the information loss of D with respect to P The rationale is that if D is not in compliance with a pattern P in n on position j then the jth position must be instantiated and D must not be in compliance with the MCSP of lI on position j either In general for any event sequence D the overall information loss of D with respect to the MCSP of a set of complementary patterns II is equal to the summation of the information loss of D with respect to each pattern in II Definition 2.12 A pattern P  PI p   PI is a singular pattern ifon/y one position is instantiated Intuitively a singular pattern of period 1 consists one regular event and 1  1 eternal events The event may occur at one of the 1 positions For example a   a  and a are all singular patterns of period 3 involving event a Proposition 2.1 Triangle Inequality Given an event se quence D and two complementary patterns P and P of the same period er Q be the minimum common superpattern of P nnd P Then the generalized information gain of D with respecr to Q is at most the summation of that of P and P Proposition 2.1 can be easily generalized IO a set of comple mentary patterns, which is stated as follows 727 


Proposition 2.2 Given an evenr sequence D and a ser of com plementary parrerns n ler Q be rhe minimum common super parrern of n then the generalized information gain of D with respect to Q is ar most the summation of rhar of each pattern in n 3 General Approach In this section we outline the general strategy we use to mine patterns that meet certain generalized information gain thresh old y There exist three challenges for mining patterns with information gain I The number of different patterns is I 3 11 1  O\(l 5 I O<lSL where I S I and L are the overall number of distinct events and the maximum period length respectively Since L can be quite large e.g in the thousands it is infeasible to verify each pattern against the data directly Some pruning mecha nism has to he developed to circumscribe the search space 2 By definition, the generalized information gain measure does not have the property of downward closure as the traditional suppon measure does This prevents us from borrowing ex isting algorithms developed for association rule problems to mine the qualified patterns 3 The subsequence concept in troduced in this paper poses a difficult challenge to determine when a subsequence should start and end If a pattern misses some matches it is hard to tell whether this signals the end of a subsequence or this merely means some noise within a subsequence Fortunately the triangle inequalify holds for the general ized information gain In other word for a set of comple mentary patterns Il the generalized information gain of the minimum common superpattern \(MCSP of II is always less than or equal to the sum of that of each individual pattern in II over the same subsequence of events Inspired by this ohser vation we can first collect the generalized information gain of all singular patterns and then generate candidate patterns by combining these singular patterns In the first phase, the valid singular patterns are discovered The second phase generates the candidates of valid complex pattern based on the candidates of valid singular patterns via triangle inequality. Finally InfoMiner verifies all candidates and finds the corresponding subsequence for each valid pat tern so as to maximize its generalized information gain The first phase can be further divided into two steps I identify the likely periods for each event and 2 find the valid singular pattern for each likely period In the first step the negative impact of gap penalties towards the overall GIG is taken into account to prune out disqualified periods of each event; while in the second step all possible format of singular patterns are considered and evaluated For each likely period 1 of event a there are 1 possible singular patterns e.g for 1  3 the threesingularpatternsare a a a Af Ler valid singular patterns are identified the candidate complex patterns are generated according to the triangle inequality, and then verified At the same time for each significant pattern we will find its associated subsequences Due lo space limitations we omit the detailed description of the InfoMiner+ algorithm 4 Conclusion In this paper we propose a new mining problem of partial pe riodic pattern with random replacement To qualify signifi cant patterns in a sequence we introduce a new measurement generalized information gain This new metric can seam lessly accommodate the different frequency of event occur rences in an event sequence and the gap penalties and provides us with solid theoretical foundations The triangle inequality preserved by the generalized information gain enables us to devise a linear algorithm to mine the significant pattern in any subsequence combinations References I R Blahut Principles and Pracrice of Information Theory Addison-Wesley Publishing Company 1987 21 R Durbin S Eddy A Krough and G Mitchison Biological Sequence Analysis Probabilistic Models ofPmreins and Nucleic Acids Cambridge University Press 1998 31 J Han G Dong and Y Yin Efficient mining partial periodic patterns in time series database PNC Inr Con on Datu Engineering 106-1 15 1999 141 E Keogh S Lonardi and W Chiu Finding sur prising patterns in a time series database in linear time and space Proc ACM Knowledge Discovery and Dara Mining pp 550-556.2002 51 J Yang W Wang, and P Yu Mining asynchronous periodic patterns in time series data Proc ACM SIGKDD Int ConJ on Knowledge Discovery and Data Mining SICKDD pp 275-279,2000 161 J Yang W Wang, and P Yu InfoMiner mining sur prising periodic patterns Pmc ACM Knowledge Dis covery and Datu Mining 395-400,2001 728 


then for each non-empty subset a off whether the association rule a 3 If  a holds or.inot is determined by computing the confidence  supportv a If confidence 2 minimum confidence then the rule holds a is called the ascendant of the rule and If a is called the consequent of the rule The candidate ascendant are all possible subsets of the frequent itemset whose length are from 1 to the length of the frequent itemset minus 1 and the corresponding candidate consequents are those left in the frequent itemset 4.5 Example of Mining Association Rules For this specific project the end users are assumed to be doctors specialists or anyone interested in skin cancer who have knowledge about skin'cancer and know what kind of associations they are looking for Therefore to find association rules among which attributes is totally dependent on the end user The interface is designed such that user can find association rules among any combination of the attributes In the interface every attribute in the target table is listed as shown in Figure 7. Except the Malignant attribute a list box is used to select the number of intervals to discretize the attribute values The user can select a value between 1 and 10 for the number of intervals Different attributes can be discretized with different number of intervals The attribute Malignant in the table has only three values malignant benign and pre malignant so there is no need to discretize it Two sliders allow the user to determine the minimum support and the minimum confidence After the user specifies hidher requirements and clicks the Mining button the program begins Valid association rules are displayed to the user as shown in Figure 7 Mining Skin Cancer Database Boundary-Irregularity 1.0791 to 1.1707  Malignant  pre-malignant  Boundary-Irregularity 1.1707 to 1.708 1   Malignant  malignant  Asymmetry 0.0999 to 0.1467  Malignant  benign  Asymmetry 0.2318 to 0.3857  Malignant  malignant  From the rules above we can observe that high boundary irregularity or high asymmetry of the tumor is associated with malignant This coincides with the experts experience 5 Summary In this project we developed a web-based data browsing and content-based retrieval system for a skin cancer database Users can query the database by any combination of the feature attribute values or by synthesized image colors Range queries exact match queries and similarity-based queries are allowed based on any image feature Another contribution of this project is the implementation of an association rule mining algorithm which was originally developed for mining transaction databases Users can find association rules between different skin cancer feature values which can be every useful for skin cancer diagnosis and study References l National Cancer Institute NCI What You Need to Know about Skin Cancer 1995 2 L Xu M Jackowski A Goshtasby C Yu D Roseman and S Bines, "Segmentation of Skin Cancer Images," Image and Vision Computing, 17\(1\1999 pp. 65-74 3 J E Golston W V Stoecker R H Moss and I P S Dhillon Automatic Detection of Irregular Borders in Melanoma and Other Skin Tumors Computerized Medical Imaging and Graphics 16\(3\1992 pp 188-203 4 W V Stoecker W W Li and R H Moss Automatic Detection of Asymmetry in Skin Tumors Computerized Medical Imaging and Graphics 16\(3 1992 pp I91  197 SI R Jain R Kasturi and B G Schunck Machine Vision McCraw-Hill 1995 6 D. H. Ballard and C M. Brown Computer Vision Prentice Hall 1982  P Adriaans and D Zantinge Data Mining Addison Wesley 1996 8 R Agrawal and R Snkant Fast Algonthms for Mining Association Rules Proc of VLDB Conference 1994 pp 487-499 9 M Houtsma and A Swami Set-oriented Mining for Association Rules in Relational Databases Proc of Int Conference on Data Engineering 1995 pp. 25-33 Figure 7. Valid association rules are displayed Some examples of mined association rules are listed below when the minimum support is 12 and the minimum confidence is 60 Boundary-Irregularity 0.9022 to 1.0178  Malignant  benign  615 


4.2.1 The Round Robin Algorithm The main idea behind the Round Robin Algorithm denoted by RRA is rather than selecting a unique victim item per given restrictive association rule we select different victim items in turns starting from the rst item then the second and so on in each sensitive transaction The process starts again at the rst item of the restrictive rule as a victim item each time the last item is reached The rationale behind this selection is that by removing one item at a time from the sensitive transactions it would alleviate the impact on the sanitized database and the legitimate association rules to be discovered since this strategy tries to balance the decreasing of the support of the items in restrictive association rules Selecting the sensitive transactions to sanitize is simply based on their degree of conîict Given the number of sensitive transactions to alter based on   this approach selects for each restrictive rule the sensitive transactions whose degree of conîict is sorted in descending order The rationale is that by sanitizing the conîict sensitive transactions that share a common item with more than one restrictive rule this optimizes the hiding strategy of such rules in one step and consequently minimizes the impact of the sanitization on the discovery of the legitimate association rules The sketch of the Round Robin Algorithm is given as follows Round Robin Algorithm Input D  R R   Output D  Step 1 For each association rule rr i  R R do 1 T  rr i   Find Sensitive Transactions rr i  D  Step 2 For each association rule rr i  R R do 1 Victim rr i  item v such that item v  rr i and if there are k items in rr i thei th item is assigned to item v mod k in round robin fashion Step 3 For each association rule rr i  R R do   T  rr i   is the number of sensitive transactions for rr i 1 NumbTrans rr i  T  rr i   1    Step 4 D   D For each association rule rr i  R R do 1 Sort Transactions T  rr i   in descending order of degree of conîict 2 T ransT oSanitize  Select rst NumbTrans rr i transactions from T  rr i  3 in D  foreach transaction t  T ransT oSanitize do 3.1 t   t  Victim rr i  End The four steps of this algorithm correspond to the four steps described above in the beginning of this section The rst step builds an inverted index of the items in D in one scan of the database In step 2 the victim item Victim rr i is selected in a round robin fashion for each restrictive associationrule.Line1instep3showsthat  is used to compute the number NumbTrans rr i of transactions to sanitize This means that the threshold  is actually a measure on the impact of the sanitization rather than a direct measure on the restricted association rules to hide or disclose Indirectly  does have an inîuence on the hiding or disclosure of restricted association rules There is actually only one scan of the database in the implementation of step 4 Transactions that do not need sanitization are directly copied from D to D   while the others are sanitized before copied to D   In our implementation the sensitive transactions to be cleansed are rst marked before the database scan for copying The selection of the sensitive transactions to sanitize T ransT oSanitize is based on their degree of conîict hence the sort in line 1 of step 4 When a transaction is selected for sanitization only the victim items are removed from it line 3.1 in step 4 4.2.2 The Random Algorithm The intuition behind the Random Algorithm denoted by RA is to select as a victim item for a given restrictive association rule one item of such rule randomly Like the Round Robin Algorithm the rationale behind this selection is that removing different items from the sensitive transactions would slightly minimize the support of legitimate association rules that would be available for being mined in the sanitized database Selecting the sensitive transactions to sanitize is simply based on their degree of conîict We evaluated the sanitization through the Random Algorithm by selecting sensitive transactions sorted in ascending and descending order The approach based on descending order in general yielded the best results That is why we have adopted such an approach for our algorithm The sketch of the Random Algorithm is given as follows Random Algorithm Input D  R R   Output D  Step 1 For each association rule rr i  R R do 1 T  rr i   Find Sensitive Transactions rr i  D  Step 2 For each association rule rr i  R R do 1 Victim rr i  item v such that item v  rr i and if there are k items in rr i  the item assigned to item v is random\(k Step 3 For each association rule rr i  R R do   T  rr i   is the number of sensitive transactions for rr i 1 NumbTrans rr i  T  rr i   1    Step 4 D   D For each association rule rr i  R R do 1 Sort Transactions T  rr i   in descending order of degree of conîict 2 T ransT oSanitize  Select rst NumbTrans rr i transactions from T  rr i  3 in D  foreach transaction t  T ransT oSanitize do 3.1 t   t  Victim rr i  End Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


The four steps of this algorithms correspond to those in the Round Robin Algorithm The only difference is that the Random Algorithm selects the victim item randomly while the Round Robin Algorithm selects the victim item taking turns 5 Experimental Results We performed two series of experiments the rst to measure the effectiveness of our sanitization algorithms and the second to measure the efìciency and scalability of the algorithms All the experiments were conducted on a PC AMD Athlon 1900/1600 SPEC CFP2000 588 with 1.2 GB of RAM running a Linux operating system To measure the effectiveness of the algorithms we used a dataset generated by the IBM synthetic data generator to generate a dataset containing 500 different items with 100K transactions in which the average size per transaction is 40 items The effectiveness is measured in terms of the number of restrictive association rules effectively hidden as well as the proportion of legitimate rules accidentally hidden due to the sanitization We selected for our experiments a set of ten restrictive association rules from the dataset ranging from two to ve items in length with support ranging from 20 to 42 and conìdence ranging from 80 to 100 in the database We ran the Apriori algorithm to select such association rules The time required to build the inverted le in main memory was 4.05 seconds Based on this inverted le we retrieved all the sensitive transactions in 1.02 seconds With our ten original restrictive association rules 94701 rules became restricted in the database since any association rule that contains restrictive rules should also be restricted 5.1 Measuring effectiveness In this section we measure the effectiveness of our algorithms taking into account the performance measures introduced in Section 3.3 We compare our algorithms with a similar one proposed in 4 t o h i d e r ul es by reduci ng s upport called Algo2a The algorithm GIH designed by Saygin et al 9 i s s imilar to Algo2a The bas ic dif ference is that in Algo2a some items are removed from sensitive transactions while in GIH a mark  unknowns is placed instead of item deletions Figure 4 shows a special case in which the disclosure threshold  is set to 0 that is no restrictive rule is allowed to be mined from the sanitized database In this situation 30.16 of the legitimate association rules in the case of RRA and RA 24.76 in the case of Algo2a and 20.08 in the case of IGA are accidentally hidden While the algorithms proposed in 4 9 h i d e r ul es reducing their absolute support in the database in our frame        0 5 10 15 20 25 30 35 IGA RRA RA A l g o2a Sanitizing Algorithms Misses Cost   IGA  RRA  RA  Al g o2a  Figure 4 Effect of  on misses cost work the process of modifying transactions satisìes a disclosure threshold  controlled by the database owner This threshold basically expresses how relaxed the privacy preserving mechanisms should be When  0  no restrictive association rules are allowed to be discovered When   100  there are no restrictions on the restrictive association rules The advantage of having this threshold is that it enables a compromise to be found between hiding association rules while missing legitimate ones and nding all legitimate association rules but uncovering restrictive ones Figure 5 shows the effect of the disclosure threshold  on the hiding failure and the misses cost for all three algorithms considering the minimum support threshold   5  Notice that RRA and RA yielded basically the same results That is why their curves are very identical at the scale of the gure As can be observed when  is 0 no restrictive association rule is disclosed for all three algorithms However 30.16 of the legitimate association rules in the case of RRA and RA and 20.08 in the case of IGA are accidentally hidden When  is equal to 100 all restrictive association rules are disclosed and no misses are recorded for legitimate rules What can also be observed is that the hiding failure for RA is slightly better than that for the other approaches On the other hand the impact of IGA on the database is smaller and the misses cost of IGA is the lowest among all approaches before   75  After this value all the algorithms yield similar results Regarding the third performance measure artifactual patterns one may claim that when we decrease the frequencies of some items the relative frequencies in the database may be modiìed by the sanitization process and new rules may emerge However in our experiments the problem artifactual pattern AP was always 0 with all algorithms regardless of the values of   Our sanitization indeed does not remove any transaction The same results can be observed for the algorithms presented in 4 9 We could measure the dissimilarity between the original and sanitized databases by computing the difference between their sizes in bytes However we believe that this Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


 0 20 40 60 80 100 120 0255075100 Disclosure Threshold Hiding Failure IGA RRA RA 0 5 10 15 20 25 30 35 0255075100 Disclosure Threshold Misses Cost IGA RRA RA Figure 5 Effect of  on the hiding failure and misses cost dissimilarity should be measured comparing their contents instead of their sizes Comparing their contents is more intuitive and gouges more accurately the modiìcations made to the transactions in the database To measure the dissimilarity between the original and the sanitized datasets we simply compare the difference of their histograms In this case the horizontal axis of a histogram contains all items in the dataset while the vertical axis corresponds to their frequencies The sum of the frequencies of all items gives the total of the histogram So the dissimilarity between D and D denoted by dif  D D   isgiven by dif  D D   1  n i 1 f D  i   n  i 1  f D  i   f D   i  where f X  i  represents the frequency of the i th item in the dataset X 0 1 2 3 4 5 6 7 IGA RRA RA A l g o2a Sanitizing Algorithms Dissimilarity IGA RRA RA Al g o2a Figure 6 Difference in size between D and D Figure 6 shows the differential between the initial size of the database and the size of the sanitized database when the disclosure threshold  0  To have the smallest impact possible on the database the sanitization algorithm should not reduce the size of the database signiìcantly As can be seen IGA is the one that impacts the least on the database In this particular case 3.55 of the database is lost in the case of IGA 6 in the case of RRA and RA and 5.24 in the case of Algo2a 0 1 2 3 4 5 6 7 0 25 50 75 100 Disclosure Threshold Dissimilarity IGA RRA RA Figure 7 Difference in size between D and D Figure 7 shows the differential between the initial size of the database and the size of the sanitized database for our three algorithms with respect to the disclosure threshold   Again IGA is the one that impacts the least on the database for all values of the disclosure threshold   Thus as can be seen the three algorithms slightly alter the data in the original database while enabling exibility for someone to tune them 5.2 CPU Time for the Sanitization Process We tested the scalability of our sanitization algorithms vis a-vis the size of the database as well as the number of rules to hide Our comparison study also includes the algorithm Algo2a We varied the size of the original database D from 20K transactions to 100K transactions while xing the disclosure threshold  and the support threshold to 0 and keeping the set of restrictive rules constant 10 original patterns Figure 8A shows that IGA RRA and RA increase CPU time linearly with the size of the database while the CPU time in Algo2a grows fast This is due the fact that Algo2a requires various scans over the original database while our algorithms require only two Note that our algorithms yield almost the same CPU time since they are very similar Although IGA sanitizes less sensitive transactions it has an Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


overhead to group restrictive association rules that share the same items and optimizes this process We also varied the number of restrictive rules to hide from approximately 6000 to 29500 while xing the size of the database to 100K transactions and xing the support and disclosure thresholds to   0 Figure 8B shows that our algorithms scale well with the number of rules to hide The gure reports the size of the original set of restricted rules which varied from 2 to 10 This makes the set of all restricted rules range from approximately 6097 to 29558 This scalability is mainly due to the inverted les we use in our approaches for indexing the transactions per item and indexing the sensitive transactions per restrictive rule There is no need to scan the database again whenever we want to access a transaction for sanitization purposes The inverted le gives direct access with pointers to the relevant transactions The CPU time for Algo2a is more expensive due the number of scans over the database 6 Related Work Some effort has been made to address the problem of privacy preservation in association rule mining The class of solutions for this problem has been restricted basically to randomization data partition and data sanitization In this work we focus on the latter category The idea behind data sanitization was introduced in 1 Atallah et al considered the problem of limiting disclosure of sensitive rules aiming at selectively hiding some frequent itemsets from large databases with as little impact on other non-sensitive frequent itemsets as possible Specifically the authors dealt with the problem of modifying a given database so that the support of a given set of sensitive rules mined from the database decreases below the minimum support value The authors focused on the theoretical approach and showed that the optimal sanitization is an NPhard problem In 4 th e a u t h o r s i n v estig ated co n  d e n tiality issu es o f a broad category of association rules and proposed some algorithms to preserve privacy of such rules above a given privacy threshold Although these algorithms ensure privacy preservation they are CPU-intensive since they require multiple scans over a transactional database In addition such algorithms in some way modiìes true data values and relationships by turning some items from 0 to 1 in some transactions In the same direction Saygin et al 9 i nt roduced a method for selectively removing individual values from a database to prevent the discovery of a set of rules while preserving the data for other applications They proposed some algorithms to obscure a given set of sensitive rules by replacing known values with unknowns while minimizing the side effects on non-sensitive rules These algorithms also require various scans to sanitize a database depending on the number of association rules to be hidden Oliveira and Za ane 8 i nt roduced a uni  e d frame w o rk that combines techniques for efìciently hiding restrictive patterns a transaction retrieval engine relying on an inverted le and Boolean queries and a set of algorithms to sanitize a database In this framework the sanitizing algorithms require two scans regardless of the database size and the number of restrictive patterns that must be protected The work presented here differs from the related work in some aspects as follows First we extended our previous work presented in 8 b y addi ng t w o n e w al gori t h ms Round Robin and Random to the set of sanitizing algorithms Second the hiding strategies behind our algorithms deal with the problem 1 and 2 in Figure 3 and most importantly they do not introduce the problem 3 since we do not add noise to the original data Third we study the impact of our hiding strategies in the original database by quantifying how much information is preserved after sanitizing a database So our focus is not only on hiding restrictive association rules but also on maximizing the discovery of rules after sanitizing a database Another difference of our algorithms from the related work is that our algorithms require only two scans over the original database while the algorithms presented in 4 9 requi re v a ri ous s cans depending on the number of association rules to be hidden This is due the fact that our sanitizing algorithms are built on indexes and consequently they achieve a reasonable performance 7 Conclusions In this paper we have introduced two algorithms for balancing privacy and knowledge discovery in association rule mining Our sanitizing algorithms require only two scans regardless of the database size and the number of restrictive association rules that must be protected This rst scan is required to build the index inverted le for speeding up the sanitization process while the second scan is used to sanitize the original database This represents a signiìcant improvement over the previous algorithms presented in the literature 4 9 Our algorithms are integrated to the framework presented in 8 which combines three adv ances for ef ciently hiding restrictive rules inverted les one for indexing the transactions per item and a second for indexing the sensitive transactions per restrictive association rule a transaction retrieval engine relying on Boolean queries for retrieving transaction IDs from the inverted le and combining the resulted lists and a set of sanitizing algorithms The experimental results revealed that our algorithms for sanitizing a transactional database can achieve reasonable Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


 0 20 40 60 80 100 120 140 20 40 60 80 100 DB Size \(transactions in thousands CPU Time \(sec IGA RRA RA Al g o2a 0 10 20 30 40 50 60 70 246810 Set of Restrictive Rules CPU Time \(sec IGA RRA RA Al g o2a AB Figure 8 Results of CPU time for the sanitization process results when compared with the other approaches in the literature Such algorithms slightly alter the data while enabling exibility for someone to tune them In particular the IGA algorithm reached the best performance in terms of dissimilarity and in terms of preservation of legitimate association rules On the other hand the results suggested that RA is slightly better than the other algorithms for hiding failure Although our algorithms guarantee privacy and do not introduce false drops to the data an extra cost is payed because some rules would be removed accidentally since there are functional dependencies between restricted and non-restricted rules The rationale behind this is that privacy preserving association rule mining deals with a tradeoff privacy and accuracy which are contradictory i.e improving one usually incurs a cost for the other It is important to note that our sanitization methods are robust in the sense that there is no de-sanitization possible The alterations to the original database are not saved anywhere since the owner of the database still keeps an original copy of the database intact while distributing the sanitized database Moreover there is no encryption involved There is no possible way to reproduce the original database from the sanitized one Currently we are investigating new optimal sanitization algorithms that minimize the impact in the sanitized database while facilitating proper information accuracy and mining In addition we are working on the optimization of the algorithms RRA and RA specially in terms of preservation of legitimate association rules since their results revealed they are promising 8 Acknowledgments Stanley Oliveira was partially supported by CNPq Conselho Nacional de Desenvolvimento Cient co e Tecnol ogico of Ministry for Science and Technology of Brazil under Grant No 200077/00-7 Osmar Za ane was partially supported by a Research Grant from NSERC Canada We would like to thank Y ucel Saygin and Elena Dasseni for providing us the code of their respective algorithms for our comparison study References 1 M  A tallah  E  Bertin o  A Elmag armid  M  I b r ah im an d V Verykios Disclosure Limitation of Sensitive Rules In Proc of IEEE Knowledge and Data Engineering Workshop  pages 45Ö52 Chicago Illinois November 1999  C  C l i f t on Usi ng S ampl e S i z e t o L i m i t E xposure t o Dat a Mi ning Journal of Computer Security  8\(4\:281Ö307 November 2000 3 C  C lifto n a n d D M a rk s Secu rity an d P ri v a c y Imp licatio n s o f Data Mining In Workshop on Data Mining and Knowledge Discovery  pages 15Ö19 Montreal Canada February 1996 4 E D a s s e n i  V S V e r y k i o s  A K E l m a g a r m i d  a n dE B e r t i n o  Hiding Association Rules by Using Conìdence and Support In Proc of the 4th Information Hiding Workshop  pages 369 383 Pittsburg PA April 2001 5 M  D ietzfelb in g e r  A R Karlin  K  M eh lh o r n  F  M  au f d er Heide H Rohnert and R E Tarjan Dynamic Perfect Hashing Upper and Lower Bounds SIAM Journal on Computing  23\(4\:738Ö761 1994  D E  OêL eary  Kno wledge Disco v e ry as a T hreat to Database Security In G Piatetsky-Shapiro and W J Frawley editors Knowledge Discovery in Databases AAAI/MIT Press pages 507-516 Menlo Park CA 1991 7 S  R M O l i v e i r a a n d O  R  Z a  ane A Framework for Enforcing Privacy in Mining Frequent Patterns Technical report TR02-13 Computer Science Department University of Alberta Canada June 2002 8 S  R M O l i v e i r a a n d O  R  Z a  ane Privacy Preserving Frequent Itemset Mining In Proc of the IEEE ICDM Workshop on Privacy Security and Data Mining  pages 43Ö54 Maebashi City Japan December 2002 9 Y  S aygi n V  S  V e r yki os and C  C l i f t on U s i n g U nkno w n s to Prevent Discovery of Association Rules SIGMOD Record  30\(4\:45Ö54 December 2001 Proceedings of the Seventh International Database Engineering and Applications Symposium \(IDEASê03 1098-8068/03 $17.00 © 2003 IEEE 


OM OM 006 OD8 01 012 014 016 018 02 022 False alarm demity Figure 9 Percentage of tracks lost within 200 seconds using three-scan assignment with PD  0.9 TI  O.ls Figure 11 T2  1.9s and T  Is ij  20 and 0  0.1 24 1 22  20  E fls 0  8l 16 0 n 14  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 T1/12 PD Average track life of three-scan assignment with PD varying TI  0-ls T2  1.9s T  Is X  0.02 ij LO and   0.1 mareuvenng index Figure 12 Percentage of lost tracks of 4-D assipment in 200 seconds with maneuvering index varying X  0.01 Ti  0.1 T2  1.9s and T  IS PD  0.98 Figure 10 Percentage of lost tracks of 4-D assignment in 200 SeoDllCls with TI and T2 varying PD  0.98 X  0.02 q 20 and 0  0.1 4-1607 


Figure 13 Average gate size for Kalman filter Figure is relative as compared to nq and curves are parametrized by ij/r with unit-time between each pair of samples 1.2 Iy I 1.1 0.5 I A CRLB for he unifm samiina I  0.4 0.35 d 3 03 i7 3 0.25 0 0.M 0.04 0.06 008 0.1 0.12 0.14 0.16 0.18 0.2 False A!am DemW V I    Figure 14 CramerRao Lower Boundfor Mean Square Error of uniform and nonuniform sampling schemes with Ti  O.ls T2  1.9s T  IS PD  0.9 ij  5 and U  0.25 1 unifon sampling r-ls ked i non-uniform sampling loge inlewi I ti non-uniform sampling shod interva I 0.9 0.8 I Figure 15 MSE comparison of three-scan assignment with Ti and T2 varying I'D  1 X  0.01 ij  20 and U  0.1 4-1608 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


