2002 FIRST INTERNATIONAL IEEE SYMPOSIUM "INTELLIGENT SYSTEMS SEPTEMBER 2002 Finding the Base of Global Implications for the Association Rule Problem Luminita Dumitriu Teodor Dumitriu and Cornelia Tudorie AbsbackUsing concept lattices as a theoretical background lor finding association rules 111 has led to designing algorithms like Charm 121 Close 131 or Closet 141 While they are considered as extremely appropriate when finding concepts lor association rules they do not cover a certain area 01 significant results, namely the pseudo-intents that form the base lor global implications We propose an approach that besides linding all proper partial implications also finds the pseudo-intents 
The way our algorithm is devised it allows certain important operations on eonccpt lattices like adding or extracting items meaning we can reuse previously lound results Inder Terns-Data models data processing 1 INTRODUCTION LATELY corporations have made serious investments into using information technology to increase their management quality Also large amounts of important business data are now stored in database systems and their quantum is expected to grow An important goal of data mining is to extract valuable implicit patterns from large quantities of data Association rules, introduced by 5 provide useful means to discover associations in data Let I  i i2   i he a set of m literals called ifems Let 
the database D   r b   t be a set of n transactions each transaction consisting of a set ofitemslofIand associated with a unique transaction identifier called id I is called a k-ifernsel if k is the size of I A transaction teD is said to contain an itemset I if IC t The support of an itemsetlis the percentage of transactions in D containing I support 0  I{t ED 1 I ct  I{/eD An association rule is a conditional implication among itemsets I  1 where itemsets I I'd and 
I nP O The confidence of an association rule I I Pis the conditional probability that a transaction contains r given it contains L conidence  supporr I U 1  support 0 The support of r is defined as support\(r\=supporr lu P L.Dumitriu is with the Computer Science and Engineering Department University Dunarea de Jas Galati 6200 Romania telephone 4093 161 3 14 e-mail LuminiIa.Dumitriu@ugaI.ra T Dumitriu is with he Elechical Engineering Department University Dmiaroa de SOS Galati 6200 Romania e-mad Teodor.Dumi hi a@ugal.ro C Tudorie IS 
with the Computer Science and Engineering Department University Dunarea de SOS Galati 6200 Romania c mail Comelia.Tudone@ugal.ro The problem of mining association rules in a database is defined as finding all the association rules that hold with more that a user-given support threshold minsiip and a user-given confidence threshold minconJ According to SI this problem is solved in two steps 1 Finding all frequenl itemsets in the database meaning itemsets with support higher than or equal to rninsup 2 For each frequent itemset I generating all association rules I I  I where P c I with confidence greater than or equal tominconl The second problem 
can he solved in a straightforward manner aAer the first step is completed Hence, the problem of mining association rules is reduced to the problem of finding all frequent itemsets This is not a trivial problefn since the numbcr of possible frequent itemset is equal 10 the size of the power set of I d4 There are many algorithms proposed in the literature most of them based on the Apriori mining method 6 that relies on a basic property of frequent itemsets all subsets of a.frequent ifemsel arefiequent This property can also be said as all supersets of an infTequenr itenisel are infrequent This approach works well on weakly correlated data such as market basket data. Over correlated data such as census 
data, there are other approach, as Close 3 CHARM 2 and Closet 4 which are more appropriate These approaches search for closed itemsets structured in lattices that are closely related with the concept lattice in formal concept analysis 7 The main advantage of a closed itemset approach is the smaller size of the resulting concept lattice versus the numher of frequent itemsets ie a search space reduction In this paper we propose extending our approach SI with finding the global implication base It is also a closed itemset approach but is more user-oriented it takes into account the fact that an association rule mining process leads to a large amount of results that is most of the time difficult to understand by the user To prevent this to happen we add a user-selection to the process: the interesting 
frequent items This way we build a partial easier to understand model of the data We still have to provide means to model all the data To achieve this goal our approach allows the user to pick a previously found partial data model expressed as a concept lattice and a data context and some new frequent items to he added to it thus obtaining an extended or even a full data model The results of an extension operation consist of the supplementary results the model The reverse operation to the extension of a model is reducing a model with some frequent items we have considered it to be used whenever a large data model is incomprehensible to the user The main advantages of our approach are 0-7803-7134-81021510.00 0 2002 IEEE 21s 


 a smaller size of results leads to a higher comprehensibility of a data model or a model  the data models can be reused when extending or reducing them with sets of items thus spearing the time spent building them We are enlarging the data model with the base for generating rules with confidence  1 the pseudo-intenis and    show how it works with the extension and reduction   operations I The rest of the paper is organised as Follows Section 2 reviews related work and compare it with the contribution of the paper In section 3 we define our notion of data model and we present the main operations with data models In section 4 we give some experimental results The paper is concluded in section 5 11 RELATED WORK AND CONTRIBUTIONS In this section we first present the Apriori-type approach Then we describe the use of closed itemset lattice as theoretical framework for the closed itemset-type approach In the end we briefly describe ow method TABLE1 CO OFEXAMPLE DATABASE I Transaction Item List 1 ABCD 2 ACE 3 ABCD 4 ABCE 5 ABCDE 6 ADE A Apriori-type approach Let's consider the example database in Table I The frequent itemsets, listed in Table 2 form a lattice with partial order induced by the inclusion operator as in Figure 1 TABLE I1 FREpmIsnnsm FORMINWP=M Generating all possibly frequent itemsets and testing their support is clearly an impracticable solution when m the cardinal of I is large The Apriori methodology is described as follows First, the items in I are sorted in lexicographic order The frequent items are computed in one pass over the database They are stored in the set L of frequent I-itemsets For each iteration i a candidate set C is computed joining L with itself In a database scan the support for each candidate in Ci is computed The frequent itemsets found in C are stored in  C AC a a B D E AB AI AE EC AB13 BD CE C M?4 Fig 1 The frequent itemset lattice where the partial order relationship is the set inclusion L the set OF frequent i-itemsets and C is discarded The process continues until all candidates are infrequent The join operation in iteration i selects pairs of i-1 itemsets having i-2 items in common reuniting their elements into a candidate of size i Afterwards, the candidates that do not have all their subsets of i-1 items in L are pruned The number of effective iterations is equal to the height of the frequent itemset lattice B The closed itemset approach In this section we define a context the Galois connection of a context a concept of the context a lattice ofconcepts For a more details on lattice theory see 7 A context is a triple T I D where T and I are sets and D dW The elements of T are called objects and the elements of I are called attributes For any f E T and i E I we note tDi when t is related to i i.e  t i E D Let T I 0 he a context Then the mappings s T I X II\(VteX t I p\(T Y Is Tl\(ViEY define a Galois connecrion between T and I the power sets of T and I respectively A concept in the context T I D is a pair  X U where Xc T Yc I s\(X\and t\(Y Xis called the exrenf and Y the intent of the concept X,Y This leads to s YY  s\(X Y and t O s\(X  t\(Y\being closure operators A concept X Y is asubconcept of X,Y denoted X',Y 5 X,Y iffYaY Let C he the set of concepts derived from D using the Galois connection The pair L=\(C 5 is a complete lattice called a lattice of concepts The closed itemset approach uses from the concepts only their intent Some of the frequent itemsets found in the previous section are not closed itemsets For example the itemset ABD where t\(ABD   1 3 5 hut s\({l 3 S The frequent closed itemsets for the example database in Figure 1 are C  A AC AD AE ABC ACE ABCD The lattice L is represented in Figure 2 As we can observe there are 7 frequent closed itemsets and 19 frequent itemsets Ignoring the itemsets that are not closed is a significant pruning criterion For the search space A closed itemset in the context of mining association rules is 


the maximal itemset of a collection of itemsets that share the same transaction set We will present the CHARM method for finding closed 5 e FDI i ir i 4 Fig 2 The lattice of frequent concepts for the example database itemsets First the frequent items are found, along with their transaction sets In the order defined on I the first frequent item i is picked. The algorithm attempts extending this item with in The support of the new itemset is computed while intersecting the transaction sets ofthe two items. There are 4 cases to take into account  if the new itemset is infrequent il is extended with i3 continuing in a depth-first manner if the transaction set of the new itemset is equal to the one of il then il is replaced with the new itemset and we continue extending the new itemset with the next frequent item if the transaction set of the new itemset is equal to the one of it then i2 is discarded and the new itemset is added as a child of if and we continue extending the new itemset with the next frequent item otherwise the new itemset is added as a child of if and we continue extending it with the next frequent item When there are no more items to extend the current itemset with the algorithm picks the next itemset in a depth first manner The algorithm ends finding the following closed itemsets ABC 1,3,4,5 ACE 2,4,5 ABCD 1,3,5 as in Figure 2 She CHARM algorithm does not determine the association rules    A 1,2,3.4,5,6 AC 1,2,3,4,5 AD US 61 AE 2 4,5 6 C Our approach The common part of our approach with CHARM for example is the extension idea The difference is that we extend a lattice built on a subset of I with some new frequent items The basic observation for our approach relies on a property of a non closed itemset An itemset i is not a closed itemset because there is a closed itemset c where ic c and t\(i  t\(c If we select from t\(c the subset of transactions containing some itemset i where i'is disjoint with c we find that the resulting transaction set is the one for both the i U i and c U i'itemsets Thus i ui'will never he a closed itemset any i disjoint with c We will call this property th rion closure up-propagation This means that no closu i:einset can originate in a non-closed itemset through exfwhn with a new item. Thus extending all closed itemsets in the lattice built on a subset of I with some new frequent items we will obtain a cover set for all the closed itemsets involving the new frequent items We consider the basic operation of extending the closed itemset lattice with one frequent item at a time t'4 Fig 3 Extending L with D Lets say we already have the lattice for A B C and we want to cxtcnd it with D E As wc can sec in Figurc 4 we practically make a copy of the initial lattice with new extended itemsets We eliminate itemsets D and ACD since they are not closed and we have the lattice for the  A B C D set of items We extend it with E There are only a few fiequent itemsets from the extended ones and the itemset E is not closcd bccausc of AE After eliminating E we obtain the same frequent closed itemset lattice as CHARM does Until now the main difference between our approach and CHARM is that we can start from a previously found lattice Fig 4 Extending L4 with E Now we can find all rules with confidence smaller than 1 The association rules with confidence equal to 1 can he expressed through a base from which every rule can he generated As shown in 9 The set X c\(X i X X is a pseudo-intent is a base for all global implications where c is the closure operator 217 


and X is a pseudo-inrent fX  c\(X and for all pseudo intenrs QcX c\(Q d The fact that X t c\(X tells us that X is not a closed itemset. For the initial lattice built on A B C we have 3 itemsets that are frequent but not closed: {B C BC Since c\(R  ABC c\(C and they do not include any other itemset we can say that B and C are pseudo-intents while BC is not since Bc BC but ABCQBC Thus the base for global implications consist2of B-AC and C-tA rules When we extend the initial lattice with D as in Figure 3 we find some new potential pseudo-intents D ACD and after extending with E as in Figure 4 we add E to this set too. We have c\(D c\(ACD and c\(E All of them are pseudo-intents in the context of the new lattice and the old ones remain pseudo-intents as well In fact the situation is not always this simple Let's consider another initial lattice namely the one built on B C D E and let's extend it with the A item As we can see in Figure 5 the initial lattice contains the frequent closed sets C D E BC CE BCD The frequent itemsets that are not closed are B and CD For this lattice we have c\(B and c\(CD both of them being pseudo-intents When we extend the lattice with A the itemsets C D and E become not closed due to AC AD and AE respectively and this relationship propagates over their subconcepts when extending them with A Thus all the previously closed itemsets become not closed We can prove that storing only newly constructed itemset was a closed itemset we could not calculate a pseudo-intent starting from that itemset An important observation refers situations as the one in Figure 5 If we find a rule with confidence equal to I for all the supersets of the itemset that becomes not closed we don't need to calculate the support of their extended itemsets since they are equal to one another Thus con$dence\(C+A  1 leads to confidence\(BC4A  1 conJ?dence\(CE-tA  1 conJ?dence\(DCD+A  1 so supporl\(C  supporl\(AC support\(BC  support\(ABC supporr\(CE  supporl\(ACE We consider this situation whenever it occurs as a significant pruning criterion to calculating the support of an extended itemset Another operation we have considered is reducing a lattice of closed itemsets with some items The only way the initial lattice is affected during the extension operation is as in Figure 5 when the original closed itemset becomes not closed and disappears. Keeping this observation in mind we conclude that for a closed itemset involving an item to be eliminated we have to check the existence of the reduced itemset and if it does not exist we create it with the same support If we keep track of the base of global implications we deal with two steps  we eliminate the current item from the left-side or from right-side of the rule if it exists if either the left-side or the right-side of the rule becomes empty, the rule is discarded  Fig 5 Extending the concept lattice built on B C D E with A the rules C+A D+A and EtA called generuling rules along with the old pseudo-intents we can calculate the pseudo-intents for the final lattice. The list of corresponding closed itemsets is c\(B c\(CD c\(C c\(D c\(E The 1-frequent itemsets in the list are pseudo-intents since there are no other pseudo-intents to cancel their status CD is not a pseudo-intent since there are C and D to contradict it If we add to CD the missing items from c\(C namely c\(C  A we get ACD;that is still an itemset that is not closed, belongs to the same closed itemset as CD and respects the definition of a pseudo-intent If the 111 DEFINING AND OPERATING WITH DATA MODELS For our approach a data model for the association rule problem consists of three components  the mining context  the corresponding results  There is no question on the necessity of the results in the data model The only issue is whether we keep track of the pseudo-intents or not We represent the results as the set of closed itemsets and their associated support information the observations added by user 218 


and when needed the set of pseudo-intents and their associated closed itemsets The mining context stores information on the set of frequent items and the transactions involved in building the model We also have to store the minsup value otherwise the results have no relevance The user added information is optional and it is meant to record his remarks on the contents of the data model as a remainder When choosing a data model to be extended we have to check the mining context in order to refer the same transactions with at least minsup for the support threshold while extending the model with new frequent items If we look for pseudo-intents wc can only choose a previously found model containing associated pseudo-intents The minconfparameter does not affect choosing the model since we store the set of frequent closed itemsets not the association rules The correctness of the data model extension can he proved as follows PI We extend the lattice 4 with  obtaining L'k We say that L U L 3 L Pz We eliminatc non-closed itemsets from 4 U L'k+l We say the result is k If we keep track of pseudo-intents we add to their set all generating rules found during the elimination phase P Processing the set of potential pseudo-intents we can calculate all pseudo-intents associated to the lattice of closed itemsets The correctness of the data model reduction can he proved as follows P'l We eliminate from 4 all closed itemsets involving it when the original closed itemset does not exist we create it with the same support as the extendrd one We say that the result is 4 Pz If we keep track of pseudo-intents we eliminate ik from the left-side or from the right-side of the global implication whenever a rule has an empty antecedent or consequent we discard it We say this new set is the base for global implications associated with b We have mentioned in the first section the model This kind of model is used for result displaying reasons only Whenever we enlarge a previously found model we display to the user only the differences between the initial model and the final one in order to reduce the amount of presented results to the new ones We have not yet dealt with presenting to the user the closed itemsets that become not closed during the extension The way we assume the data model processing will be useful is as follows  first the user can mine a few items getting a fast response and a small amount of results if the user is satisfied by a certain data model he can store it and enlarge it, afterwards with some other items if the user is not satisfied with the model he can reduce it by eliminating some items, until he can understand the results More we can use the extension operation on an empty   data model, building data models from scratch  219 W EXPERIMENTAL RESULTS We have performed several experiments on different data collections from the public domain We have used a computer Pentium 11 350MHz 128 MB RAM The first experiment that is revealing for this paper is about generating non-concepts We have used the data from http://lih.stat.cmu,edu datasetslboston corrected data as it is more revealing for our experiment We have run an exhaustivc Apriori-type algorithm wc will call it AI that finds all Crequent itemsets closed itemsets as well as non closed itemsets and the proposed algorithm A2 for miusup 1 1.5,2,3,4  The results as shown in Table 3 We have limited the experiment in what concerns the minimum support due to the long response time of AI As we can see from Table 3 our algorithm builds some non closed itemsets but significantly less than an exhaustive algorithm More it does not store all non-closed itemsets but pseudo-intents Nrm-closed Non-dostd Psouda ilamstls iternsets mlcnts 4 1113 1 27 26 WO I 627 171 A Yo f;.490 36 1 m I 5 11047 Se6 147 1 Yo IeSsP 8 21s In what concerns the time response of the algorithm when mining from scratch or from previous results we will show a relevant one run on the same data collection with minsup I Over a selection of 4 items we have added another 122 items in several steps recording the extension time in Textend Afterwards we have run the algorithm with the corresponding number of items from scratch recording the time in T We have afterwards computed At between successive queries in AT The results are in Table 4 We can see that sometimes the computed time is bigger than the algorithm time and sometimes is smaller We know that theoretically, adding an item depends on the initial number of closed itemsets The basic algorithm considers attributes in a specific order namely in the increasing order of support apparently and with no theoretical justification it is the best choice Adding items means considering them last This is the reason why Textend is not equal to the AT Anyway even if different Textend is considerably smaller than mining from scratch V CONCLUSION In this paper we have introduced the idea of data model expressed as frequent closed itemset lattice with a base for global implications There are no other approaches that determine the pseudo-intents The definition in 7 suggest having all non-concept itemsets and validating each of them as a pseudo-intent staring from the small one behind this idea is the necessity of generating all non-concepts We have shown that with a subset of them, significantly smaller we can still calculate the global implication base We have 


also described two important operations applicable on data models: extension and reduction with several items The main advantages of our approach are  Constructing small models of data makes them more understandable for the user the time of response is small Extending data models with a set of new items returns to the user only the supplementary results, hence a smaller amount of results the response time is considerably smaller than building the model from scratch Whenever data models are incomprehensible, some of the items can he removed thus obtaining an easier to  Extending or reducing a model spears the time spent building it thus reusing knowledge We are considering applying this approach to the more complex mining process of finding quantitative association rules in order to detect the best mappings for quantitative attributes in the context of a data model    understand data model REFERENCES I M.J Zaki M Ogihara 7heoretical Foundations of Associaion Rules in Proceedings of the 3d SIGMOD'98 Workshop an Research Issues n Data Mining and Knowlcdge Discovery DMKD\Seattle WA pp 7:l-7:8 1998 2 M.J Zaki C-J Nsiao CHARM An Efficient Algorithm for Closed Association Rule Mining RPI Technical Report 99-10 1999 N Pasquier Y Bastide R Tamil and L Lakhal Discovering frequent closed itemsets for association nrles In 7th Intl Conf On Database Theory January 1999 4 J Pei 1 Ha and R Mao CLOSET An eflcient nlgorithmfor mining frequent closed itemsets In DMKD 2000 pp 11-20 5 R Agrawal T Imielins and A Swami Mining association rules behveen sets of items in large dolabases In Proc of the ACM SIGMOD Conference on Management of Data, pages 207 216 Washington D.C May 1993 R Agrawal and R.S"kan1 Fast algorithms for mining association rules In Prac of the VLDB Conference, Santiago, Chile 1994 R Wille Restructuring lattice theory an approach based on hierarchies of concepls Ordered Sets pp 445-470 1982 L Dumitriu C Segal Query-mining of quantitative association mles in Proceedings of 1 I International Symposium on Modeling Simulation and Syslems Identification Galati October 2001 pp 194-198, ISBN 973-8139-98-8 191 B Ganter Algorirmen iur formolen begriflsanalysr Beitrage ZUT Beignffanalyse Ganter Wille, Wolf eds Wissenschaft-Verlag 1987 3 61 7 XI 220 


Clearly the lower the above probability  the so-called p-value  the more statistically significant the rule X  Y since it is less likely that X and Y are inde pendent In our experiments we ensure that all rules have p-values of X or less, where X is a user-specified threshold Local Pruning In addition to the global pruning described above we also prune locally on various subsets of the rules discovered Specifically our tool allows the user to browse 1 rules that demonstrate reuse of a particular library class and 2 rules that are violated in a particular application \(where the tool acts like a \223reuse lint\224 In the former case we consider only the set of rules with that library class in the antecedent or consequent In the latter case we consider only those rules that are violated by at least one class in the application of interest. Either way once we have extracted the subset of rules we follow the same local pruning procedure when presenting the results to the user The motivation for local pruning is the following given the presence of a particular rule, another rule may not be surpris ing to us In that case it is desirable to additionally prune the latter rule to focus the user\222s attention on those rules that are interesting The pruning process that follows builds upon several existing techniques 2 111 Consider rules X  y and rule X\222  y, where X\222 is a sub set of X If we know the confidence c\222 for rule X\222  y then we expect the confidence for rule X  y to also be c\221 since there is no reason to believe  without prior knowl edge of the library and/or applications  that the additional items X-X\222 in the antecedent are likely to increasefdecrease the occurrence of y. Thus we shall consider pruning X  y if its confidence c is not much greater than c\221 More specifically we set an interest threshold 6 and prune any such rule X  y where c%/c\221  6 If it is not pruned we say that X  y is interesting with respect to X\222  y We also perform anotherform of pruning Suppose we have two rules X  y and X 3  where X is an ancestor of itemset X contqning the same number of items \(where 223an cestor\224 means X  X as defined in Section 2 or Q is an ancestor of y or both In such a cye we may keep the more specific rule X  y and prune X 3 6 Generally speaking we would like to show the more specific rule which tends to be more informative However we may also wish to show the more general rule if its confidence is much greater than expected If X  y has confidence c then X  y has expected confidence c since there is no reason to believe without prior knowledge, that y is more/less likely to be in a trans action with X than one with X However as  is an an cestor of y the rule X   clearly has confidence c and possibly much more To get a reasonable estimate for ex pected support we shall assume prior knowledge of the rel ative support of y and  With such knowledge we would expect X  Q to have y  c since of those transactions that support X we would expect supp\(Q y of them to support Q By the first observation X  y has expected confidence-c Given the confidence c for X  y we would expect X   to have y  c as explained above Consequently we prune the rule X  ij if and only if supp~~~~~upp~y~~~c  6 for some interest threshold 6 Finally combining this analysis with our earlier results on smaller antecedents X\222 we shall prune a rule X  ij given confidence c\222 for X\222  y if and only if  6 This follows because the ex pected confidence for X  y is the same as the confidence c\222 for X\222  y Now we are ready to describe the complete pruning proce dure Given a set of rules XI  y1    X  yn we first construct a partial order with a node for each rule The nodes in the partial order are ordered as follows Xi  yi  Xj  yj if and only if 1 the rules are not identical 2 0 E Xi C Xj and 3 X is more specific than or equal to Xj and y is a descendent of or equal to yj Pruning proceeds by considering the nodes of the partial or der in topological sort order That is, an ancestor is always processed before its descendents In this process a rule X  y is not pruned if and only if it is &interesting with respect to all ancestors in the partial order that have survived the pruning process to that point 4 BROWSING REUSE PATTERNS In this section we shall demonstrate how one might browse and learn from generalized association rules by considering code written for the KDE desktop environment. The KDE li braries provide a C application framework for developing GUI applications. In our experiment we have mined reuse patterns for the KDE 1.1.2 core libraries \(which include the Qt toolkit by analyzing 76 real-life applications Specifically, we have used our tool CodeWeb to mine for generalized association rules with confidence of at least 10 and support of at least 15 transactions There were 1365 transactions total so the support requirement as a percentage is about 1.1 The global pruning parameters were set at y  1.25 and X  0.01 The local pruning parameter 6 was set to 1.25 Only rules with one item in the antecedent and one item in the consequent were considered To contrast generalized association rule mining with our ear lier work on standard association rule mining we also in clude the data mining results where inheritance was ignored all together On a Sparc Ultra 1 mining generalized asso ciation rules took about 50 minutes while mining standard E supp~g~~supp~y~~~c E 173 


association rules took 20 minutes The rule statistics are as follows Rules Global Pruning Mined Uninteresting Misleading Insignificant Generalized 5 1308 13299 1904 6681 Standard 21594 996 320 978 Rules Left 34271 19636 Observe that while the number of rules mined by generalized association rules is significantly more than that using stan dard association rules a greater percentage of these rules is eliminated during global pruning Of course the number of rules pruned locally varied depending upon the local context and is not shown here Typically a developer just starting out with a library would identify important library classes and browse their reuse pat terns By 223important\224 we mean those library classes that are reused in many existing applications and are thus likely to be relevant in new applications also For example, a developer using our tool would notice that the KDE classes KApplica tion and QObject are reused in 99 and loo respectively of the 76 applications mined  and are thus essential in any KDE application We consider the reuse patterns for these two classes in what follows KApplication Reuse Patterns Figure 3 shows all reuse patterns predicated on the instanti ation of KApplication in an application class The support ers of a rule are those application classes for which all rule items apply For example, an application class that supports reuse pattern 1 must instantiate KApplication and calls its member function exec We also show the detractors of a rule which are those application classes for which the an tecedent items apply but where the consequent item does not hold For example, an application class that detracts from reuse pattern 1 must instantiate KApplication and not call exec Our tool allows users to browse the source code for both supporters and detractors of reuse patterns these ap plication classes illustrate characteristic and uncharacteristic reuse respectively In Figure 3 we find  among other things  that of those applications classes that instantiate the KApplication class 72.3 call its member function exec 58.5 instantiate KT~pLevelWidget 53.8 call the member function set Mainwidget of the class KApplication and 46.2 call the show member of the class KTopLevelWidget Re call that the symbol 221A\222 indicates that some application class may reuse a strict descendent of KTopLevelWidget rather than the class itself By browsing reuse patterns in combination with library ref erence documentation which is usually available\and appli cation source code a developer can learn to use a library by example in much the same way as studying manually con structed tutorials and/or toy programs both of which may not be available For example, doing this for the rules in Figure 3 reveals that the class KApplication instantiated in the main function of most KDE applications manages the application event queue We also observe that applications inherit from KTo pLevelWidget to define the main widget of the application e.g one that is not contained in any other this widget is then instantiated and a call to setMainWidget0 tells the li brary that whenever the user closes this widget that the appli cation should terminate all together Afterwards the exec member of KApplication is called to enter the main event Finally it turns out that all applications that instantiate KTo pLevelWidget always instantiate a descendent that they de fine Without taking into account the inheritance hierarchy e.g using generalized association rules we would miss reuse patterns involving this class all together QObject Reuse Patterns The class QObject is an ancestor of almost all classes in the KDE libraries According to the reference documentation this class provides facilities for event handling and timing operations Figure 4 shows some of the reuse patterns re ported by our tool for QObject of the 53 rules found for this class 47 involve a 221*\222 symbol in the antecedent and/or consequent It turns out that application classes rarely reuse QObject directly; typically they reuse it indirectly through one of its descendents Although QObject is very funda mental to the KDE libraries only six rules would have been found without taking into account the inheritance hierarchy 5 RELATEDWORK In this paper we have looked for pattems in the way library classes have been reused in practice by existing applications In this section we shall talk about several related techniques Exemplars An exemplar is an executable visual model consisting of one or more instances of at least one concrete class for each ab stract class in a library 6 By browsing these classes as well as their static relationships and dynamic interactions one can get a general understanding of how the framework works in a small example While an exemplar may be helpful it is a pre-selected toy example that may not be representative of 223real-life\224 appli cations Moreover, exemplars place an extra burden on the developers of the software library In contrast our approach allows the user to browse reuse patterns and the correspond ing supporter and detractor classes in real-life applications Moreover, the tool is automated and works on any existing code Reengineering Libraries Recently research has been done on reengineering libraries by analyzing their usage in several existing applications lo This is done by constructing a lattice that provides insights loop 174 


lllllll_l  Figure 3 KApplication reuse patterns Clicking on kasteroids a supporter of reuse pattern 1 yields the code on the right Figure 4 QObject reuse patterns 175 


into the usage of the class hierarchy in a specific context Such a lattice can be used to reengineer the library class hi erarchy to better reflect standard usage In contrast we are interested in helping novice users learn to use a library to write new applications  not reengineer the library itself This different perspective has led us to 1\initiate a new search direction for mining code for the purposes of illus trating characteristic code reuse 2 use data mining tech niques that scale to a hundred or more applications  not just a few examples for which confidence and support mea sures would not be meaningful 3 look for different kinds of reuse patterns that are more helpful for demonstrating reuse of the library classes and 4 construct a tool whose user in terface is aimed at users of a software library rather than its developers Other Work involving Data Mining Researchers have used data mining and related techniques for a variety of purposes. For example, data mining has been used to discover likely program invariants infer spec ifications in software 3 and decompose a software sys tem into data cohesive subsystems to assist developers with reengineering and maintenance tasks 4 The last of these is the only other work we are aware of that uses association rule mining in the software engineering domain 6 CONCLUSIONS AND FUTURE WORK In this paper, we have shown how data mining can be used to discover library reuse patterns in existing applications Specifically we considered the problem of discovering li brary classes and member functions that are typically reused in combination by application classes This paper improves upon our earlier research using \223associ ation rules\224  by taking into account the inheritance hier archy using \223generalized association rules\224 This has turned out to be non-trivial due to the significantly larger number of rules that arise as a result Consequently, pruning is impor tant and we showed several ways in which it can be done By browsing generalized association rules a developer can discover patterns in library usage in a way that takes into ac count inheritance relationships We have illustrated the ap proach using our tool CodeWeb by demonstrating charac teristic ways in which applications reuse classes in the KDE application framework We have observed that some impor tant rules would not have been found without taking into ac count the inheritance hierarchy I One can view our general approach to mining reuse patterns as learning from positive experience That is, library reuse that has worked in practice Presumably, one would se lect 223stable\224 applications to demonstrate reuse patterns in a library However one can also mine negative experi ence That is misunderstandings and problems that came up when reusing components from a software library For future work it would be interesting to determine if one can  mine negative experience in an automated way perhaps by analyzing application CVS logs for reuse patterns that were problematic and later corrected REFERENCES  11 R Agrawal and R Srikant Fast algorithms for mining association rules In Proceedings of the 20th Very Large Data Bases Conference pages 487499,1994 2 M Chen J Han and P S Yu Data mining An overview from a database perspective ZEEE Transac tions on Knowledge and Data Engineering 8\(6 883,1996 3 W W Cohen. Inductive specification recovery Under standing software by learning from example behaviors Automated Software Engineering 2\(2 107-129,1995 4 C Montes de Oca and D L Carver Identification of data cohesive subsystems using data mining tech niques. In Proceedings of the Zntemational Conference on Software Maintenance pages 1623,1998 5 M D Ernst J Cockrell W G Griswold and D. Notkin. Dynamically discovering likely program in variants to support program evolution. In Zntemational Conference on Software Engineering pages 2 13-224 1999 6 D Gangopadhyay and S Mitra. Design by framework completion Automated Software Engineering 3:219 237,1996 7 N Megiddo and R Srikant Discovering predictive as socation rules In Proceedings of the 4th International Conference on Knowledge Discovery in Databases and Data Mining 1998 8 A Michail Data mining library reuse patterns in user selected applications In 14th ZEEE Zntemational Con ference on Automated Software Engineering pages 24 33, 1999 9 G Piatetsky-Shapiro and W J Frawley Knowledge Discovery in Databases AAAVMIT Press 1991  101 G Snelting and F Tip. Reengineering class hierarchies using concept analysis In 6th ACM SIGSOFT Intema tional Symposium on the Foundations of Software En gineering pages 99-1 10,1998  113 R Srikant and R Agrawal Mining generalized associ ation rules In Proceedings of the 21st Very Large Data Bases Conference 1995  121 Will Tracz Confessions of a Used Program Salesman Institutionalizing Sofhyare Reuse Addison-Wesley 1995 176 


18001  balancing mechanism which requires further investi gation 4.5 Speedup Figure 12 shows the speedup ratio for pass 2 vary ing the number of processors used, 16 32 48 and 64 where the curve is normalized with the 16 processor execution time The minimum support value was set to 0.4 4.5 0.5 1 1 0 I 10 20 30 40 50 60 70 number of mxessors Figure 12 Speedup curve NPA HPA and HPA-ELD attain much higher lin earity than SPA HPA-ELD an extension of HPA for extremely large itemset decomposition further in creases the linearity HPA-ELD attains satisfactory speed up ratio This algorithm just focuses on the item distribution of the transaction file and picks up the extremely frequently occurring items Transferring such items could result in network hot spots HPA-ELD tries not to send such items but to process them locally. Such a small mod ification to the original HPA algorithm could improve the linearity substantially 4.6 Effect of increasing transaction Figure 13 shows the effect of increasing transac tion database sue as the number of transactions is increased from 256,000 to 2 million transactions We used the data set t15.14 The behavior of the results does not change with increased database size The minimum support value was set to 0.4 The num ber of processors is kept at 16 As shown each of the parallel algorithms attains linearity 5 Summary and related work In this paper we proposed four parallel algorithms for mining association rules A summary of the four database size Sizeup 0 I 0 500 loo0 1500 uxw amount of transaction thousands Figure 13 Sizeup curve algorithms is shown in Table 5 In NPA the candi date itemsets are just copied amongst all the proces sors Each processor works on the entire candidate itemsets NPA requires no data transfer when the supports are counted However in the case where the entire candidate itemsets do not fit within the mem ory of a single processor the candidate itemsets are divided and the supports are counted by scanning the transaction database repeatedly Thus Disk 1/0 cost of NPA is high PDM, proposed in 6 is the same as NPA which copies the candidate itemsets among all the processors Disk 1/0 for PDM should be also high The remaining three algorithms SPA HPA and HPA-ELD partition the candidate itemsets over the memory space of all the processors Because it better exploits the total system's memory, disk 1/0 cost is low SPA arbitrarily partitions the candidate itemsets equally among the processors Since each processor broadcasts its local transaction data to all other pro cessors the communication cost is high HPA and HPA-ELD partition the candidate itemsets using a hash function which eliminates the need for transac tion data broadcasting and can reduce the comparison workload significantly HPA-ELD detects frequently occurring itemsets and handles them separately which can reduce the influence of the workload skew 6 Conclusions Since mining association rules requires several scans of the transaction file its computational requirements are too large for a single processor to have a reasonable response time This motivates our research In this paper we proposed four different parallel algorithms for mining association rules on a shared nothing parallel machine and examined their viabil 29 


Table 5 characteristics of algorithms ity through implementation on a 64 node parallel ma chine the Fujitsu AP1000DDV If a single processor can hold all the candidate item sets parallelization is straightforward It is just suf ficient to partition the transaction over the proces sors and for each processor to process the allocated transaction data in parallel We named this algo rithm NPA However when we try to do large scale data mining against a very large transaction file the candidate itemsets become too large to fit within the main memory of a single processor In addition to the size of a transaction file a small minimum support also increases the size of the candidate itemsets As we decrease the minimum support computation time grows rapidly but in many cases we can discover more interesting association rules SPA HPA and HPA-ELD not only partition the transaction file but partition the candidate itemsets among all the processors We implemented these al gorithms on a shard-nothing parallel machine Per formance evaluations show that the best algorithm HPA-ELD attains good linearity on speedup by fully utilizing all the available memory space which is also effective for skew handling At present we are doing the parallelization of mining generalized association rules described in 9 which includes the taxonomy is-a hierarchy Each item belongs to its own class hierarchy In such mining associations between the higher class and the lower class are also examined Thus the candidate itemset space becomes much larger and its computation time also takes even longer than the naive single level association mining Parallel pro cessing is essential for such heavy mining processing Acknowledgments This research is partially supported as a priority research program by ministry of education We would like to thank the F\221ujitsu Parallel Computing Research Center for allowing us to use their APlOOODDV sys tems References l R.Agrawal T.Imielinski and ASwami 223Min ing Association Rules between Sets of Items in Large Databases\224 In Proc of the 1993 ACM SIGMOD International Conference on Manage ment of Data pp207-216 May 1993 2 R.Agrawal and RSrikant 223Fast Algorithms for Mining Association Rules\224 In Proc of the 20th International Conference on Very Large Data Bases pp.487-499 September 1994 3 J.S.Park M.-S.Chen and P.S.Yu 223An Effec tive Hash-Based Algorithm for Mining Associ ation Rules\224 In Proc of the 1995 ACM SIG MOD International Conference on the Manage ment of Data SIGMOD Record Vo1.24 pp.175 186 June 1995 4 H.Mannila H.Toivonen and A.I.Verkamo 223Ef ficient Algorithms for Discovering Association Rules\224 In KDD-94:AAAI Workshop on Knowl edge Discovery in Databases pp.181-192 July 1994 5 A.Savasere, E.Omiecinski and S.Navathe 223An Effective Algorithm for Mining Association Rules in Large Databases\224 In Proc of the 21th International Conference on Very Large Data Bases pp.432-444 September 1995 6 J.S.Park M.-S.Chen and P.S.Yu 223Efficient Parallel Data Mining for Association Rules\224 In Proc of the 4th International Conference on In formation and Knowledge Management pp.31 36 November 1995 7 T.Shintani and M.Kitsuregawa 223Considera tion on Parallelization of Database Mining\224 In Institute of Electronics Information and Com munication Engineering Japan SIG CPS Y95 88 Technical Report Vo1.95 No.47 pp.57-62 December 1995 8 T.Shimizu T.Horie and H.Ishihata 223Perfor mance Evaluation of the APlOOO Effects of message handling broadcast and barrier syn chronization on benchmark performance-\224  In S WO PP 22292 9.2 ARC 95 Information Processing Society of Japan Vo1.92 No.64 1992 9 R.Srikant and R.Agrawal 223Mining Generalized Association Rules\224 In Proc of the 21th Inter national Conference on Very Large Data Bases pp.407-419 September 1995 30 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


