Data Organization and Access for Ef\002cient Data Mining Brian Dunkel Nandit Soparkar The University of Michigan Electrical Engineering  Computer Science 1301 Beal Avenue EECS Building Ann Arbor MI 48109-2122 f bedunkel soparkar g eecs.umich.edu Abstract Ef\002cient mining of data presents a signi\002cant challenge due to problems of combinatorial explosion in the space and time often required for such processing While previous work has focused on improving the ef\002ciency of the mining algorithms we consider how the representation organization and a ccess of the data may signi\002cantly affect performance especially when I/O costs are also considered By a simple analysis and comparison of the counting stage for the Apriori association rules algorithm we show that a 223column-wise\224 approach to data a ccess is often more ef\002cient than the standard row-wise approach We also provide the results of empirical simulations to validate our analysis The key idea in our approach is that counting in the Apriori algorithm with data accessed in a column-wise manner signi\002cantly reduces the number of disk accesses required to identify itemsets with a minimum support in the database 227 primarily by reducing the degree to which data and counters need to be repeatedly brought into memory 1 Introduction The algorithms and techniques of data mining DM or more generally knowledge discovery in databases KDD attempt to 002nd useful patterns that characterize very large datasets However the development of DM faces the obstacle of large requirements in time and space for processing Especially given that DM techniques are generally applicable and of interest when the target dataset is relatively large these dif\002culties are inherent in KDD We argue that the solutions will require novel approaches to data access and manipulation together with the careful application of new and existing algorithms The tasks of KDD should be supplemented by existing data management technologies in particular appropriate data representations and access methods Motivated by the particular DM technique of 002nding association rules 2 a n d a num be r o f r e p re s e ntative datasets we explore one manner in which database techniques could be used pro\002tably The growth of new sources of non-standard data and metadata bring situations where problems of combinatorial explosion become manifest The collection of HTML documents that comprise a signi\002cant portion of the Web is one source for such non-standard data Once a set of keywords has been extracted from a set of documents they can be stored in a tabular form Application of association rules to such metadata may include the identi\002cation of document characteristics based on an association with the keywords in the document For example it may happen that many documents containing the keyword football also contain some synonym for luxury car  Using this information the marketing efforts for luxury cars may direct informational e-mail to users whose homepages contain football  even if they have no mention of luxury cars Other applications of KDD to the Web include the discovery of interesting patterns in the statistics gathered as a record of Web usage Similar datasets can be found in more standard association rules tasks such as retail sales marketing An instance of such data might be a highly specialized catalog retailer whose inventory consists of a large number of different items each of which is only is purchased only by a small but loyal group of customers In these cases the number of items to be considered becomes signi\002cantly large The ef\002cient discovery of association rules in keyword metadata presents a challenge because the tables tend to be 223wide\224 i.e many different items may possibly occur in any given record rather than 223long\224 i.e any given item tends to appear in relatively few records The number of different combinations of the items grows exponentially and becomes more problematic than the size of the original database itself At the same time data may exhibit other characteristics e.g the itemsets tend to be sparsely dis 


tributed and there tends to be a limit to the size of the rows or columns that may be exploited to make the counting process more ef\002cient The problems arising from the growth in the number of items rather than the number of records is an issue even for market-basket data to which association rules have been applied in the past e.g as noted in 5 That is several types of datasets have the wide and sparse characteristics for which our research is most applicable The majority of available techniques for discovering association rules have aimed to improve the performance of row-wise algorithms i.e which scan the database by rows for counting items in the database work such as  be ing a notable exception Our approach in contrast accesses the database in a column-wise fashion which leads to a reduced number of I/O operations required for some datasets especially of the type that we discussed above The key idea is that counters which maintain the identity and frequency of occurrence for a particular set of items need not be repeatedly and expensively brought into the memory As in similar efforts we provide a comparative analysis and experimental validation against the Apriori algorithm   The remainder of this paper is organized as follows Section 2 discusses some of the related work and describes brie\003y the standard Apriori association rules algorithm Section 3 examines analytically the problem of counting the support for itemsets in data with certain characteristics using a column-wise approach with Section 4 comparing the processing and I/O costs Section 5 brie\003y describes some of our empirical results and Section 6 concludes the paper 2 Related Work A particular DM technique which has shown promise in the area of 223market-basket analysis\224 for retail sales data is the use of association rules algorithms described in  T he goal of these algorithms is to identify relationships between sets of items or simply itemsets in the database where two items are related if they appear together in the same record or row Anecdotal evidence e.g s ugge s t s t ha t a s sociation rules can be used to describe relationships within large highly structured datasets such as the sales records database of a retail outlet chain Ef\002ciency and scalability concerns continue to remain signi\002cant problems for these algorithms 2.1 The Apriori Algorithm Our description of the Apriori association rules algorithm and frequent itemsets has been adapted from 4 8  An item in a database is an attribute value pair and an itemset is a collection of items An association rule is a rule of the form A 1 A 2 A n  B 1 B 2 B m with con\014dence  c  and supp ort  s Here each of A 1 A n and B 1 B m are items The support of an itemset is the number of distinct records in the database in which the full itemset appears Let A denote the itemset containing A 1 A n and B the itemset containing B 1 B m  The support of an association rule is the support of the union of the two itemsets involved That is s  supp ort A  B The con\002dence of a rule is the percentage of records containing itemset A that also contain itemset B Thatis c  supp ort\(A  B supp ort A 002 100  Support is a measure of the likely statistical signi\002cance of an association rule while the con\002dence is an indication of the likely causality implied by the rule 1 L 1  f frequent 1-itemsets g  2 for  k 2 until L k 000 1 6   step 1 do begin 3 C k  apriori-gen  L k 000 1    New candidates 4 forall records r 2D do begin 5 C r  subset C k r   Candidates in r 6 forall candidates c 2 C r do 7 c.count  c.count  1 8 end 9 L k  f c 2 C k j c.count 025 minsup g 10 end 11 Answer   k L k  Figure 1 The Apriori Counting Algorithm The general association rules problem is to 002nd all collections of items in a database whose con\002dence and support meet or exceed user-speci\002ed minimum levels The general algorithm iteratively considers itemsets from the smaller to the larger ones pruning away unlikely candidates where possible The counting portion of the Apriori algorithm for discovering frequent itemsets is given using structured pseudo-code in Figure 1 First the support of individual items is calculated by counting the number of records or rows in which each appears The resu lting set of frequent 1-itemsets is denoted by L 1  In each subsequent pass over the database D  the frequent itemsets from pass k 000 1 are used to generate potentially frequent candidate k itemsets i.e k itemsets that could possibly be frequent for the k th pass over the database The set of candidate k itemsets C k  is generated by combining elements of the frequent itemsets identi\002ed in the previous pass i.e itemsets from the collection L k 000 1  


in two stages represented collectively by the subroutine apriori-gen  The 223join\224 step produces itemsets that are the result of combining two input itemsets sharing k 000 2 common items The single items by which the two  k 000 1 itemsets differ are added to the common items forming a k itemset The 223prune\224 step removes apriori anyof the newly generated k itemsets that could not be frequent because one of its  k 000 1 subsets is known to be infrequent For example if f AB C g and f AB D g were frequent 3-itemsets the join would produce a candidate f AB C D g and the prune would verify that all of f AC D g  f BC D g  etc are also frequent Once the candidates have been generated a row-wise pass is made over the database and the count is incremented for any subset of a row which is a candidates Once the pass is complete those itemsets that are not frequent i.e do not have a count in the database greater than or equal to minsup  the speci\002ed minimum support parameter are eliminated before the next pass giving the collection of frequent k itemsets L k  3 Association Rules over Wide Sparse Tables We present an analysis and compare to the Apriori algorithm our new strategy for counting the support of itemsets in a database Given the structure of the target data we consider a column-wise CW approach rather than a row-wise RW approach to counting This shift leads to the development of data representation and manipulation techniques that are more ef\002cient 227 which we demonstrate through an analysis of the I/O requirements for both the standard RW approach as well as the new CW approach Our new approach to the association rules counting problem is a novel departure from the standard method of accessing tabular data in conventional databases 3.1 Column\255Wise Apriori Counting Algorithm Figure 2 provides a pseudo-code description of an algorithm similar to the Apriori algorithm described in Section 2.1 where those lines that differ are marked with a prime  0  symbol An important distinction between this algorithm and the standard Apriori algorithm is that the database is accessed by column rather than by row where a column is regarded as a list of row identi\002ers for those rows in which the item appears or as a value in a bit-mapped index of the database As with the standard algorithm the set L 1 is calculated by a single pass over the database Unlike the RW algorithm however which may possibly access the entire set of candidate counters as it scans perhaps even one row the CW algorithm updates only a single counter for the column currently being counted This has signi\002cant rami\002cations with respect to the ef\002ciency of the processing as 1 0  L 1  f frequent 1-itemsets g   Column-wise scan 2  for  k 2 until L k 000 1 6   step 1 do begin 3 C k  apriori-gen  L k 000 1    New candidates 3 a 0  forall candidates c 2 C k do begin 4 0  forall  k 000 1 subsets of c do 5 0  Verify the subset is frequent 6 0  if c has no infrequent subset 7 0  Count the support of c by conjoinment 8 end 9 L k  f c 2 C k j c.count 025 minsup g  10 end 11 Answer   k L k  Figure 2 Column\255Wise Apriori Counting the number of disk accesses can be substantial It is convenient to discuss the data for association rules in terms of a bit-mapped representation each set of items is conceptually represented by a bit vector in which present items are represented as 223 1 224 and absent items are represented as 223 0 224 For sparse data a list representation where each present item is described explic itly may more ef\002cient In this latter representation a single item or row identi\002er is assumed to be the same size as a single counter in memory e.g the size of an integer or machine word but in any case more than a single bit Since each column represents a single item the number of row identi\002ers present in the column in question or equivalently the number of 1 bits in a bit-mapped index value determines the support for a given item In a manner similar to the RW algorithm until there are no more frequent itemsets of size k  the CW algorithm uses a 223join-and-prune\224 strategy to generate the set C k  For each candidate all  k 000 1 subsets are veri\002ed by lookup in L k 000 1 to con\002rm that they are frequent If any of these subsets are found to be infrequent then the candidate cannot be frequent and it is discarded If the candidate has no infrequent subsets then the columns representing its constituent items are read and used to count the support for the itemset using an operation we refer to as conjoinment  Conjoinment can be thought of as a vector operation that combines a bit-wise intersection with a series of increment operations to produce a new vector and a count such that the output 223column\224 has items present where both of the input columns contain the same item Items absent from either of the two inputs will be absent from the output For example if the input to this operation is a pair of database columns representing the 1-itemsets f X a g and f X b g where f X 1 X n g is the set of all items in the database then the result of the conjoin will be a single vec 


tor representing the 2-itemset f X a X b g  as well as a count of the support for this itemset in the database If this result and the column representing f X c g were then used as inputs for the conjoin the result will be a 223column\224 representing f X a X b X c g and the support count for this itemset Once all the candidates have been generated and counted candidates for which the support in the database meets a user-speci\002ed minimum are added to L k in preparation for the next pass of the loop If none of the candidates has suf\002cient support the loop terminates at which point the identity and support of all frequent itemsets is known 4 Cost Analysis We assume that the items in a database are represented by identi\002ers which can be lexicographically ordered and the representation of an itemset maintains the order of its items The manipulation of itemsets e.g the join preserves this order Also we assume for purposes of analysis that the representation of the data for a row column or itemset is a list of identi\002ers We suggest this representation rather than a bit-vector representation since it is more ef\002cient for sparse datasets The size of a single identi\002er is the unit of space used in the following analysis and this is also assumed to be the size of a single counter without an associated identi\002er We assume for the datasets considered here that the number of columns n  is greater than the number of rows l  We use B C max to represent the maximum number of items in any column and B R max to represent the maximum number of items in any row In most cases this should be an upper bound but for neither the CW nor the RW algorithm will it be an under-estimate and it simpli\002es our comparative analysis We further assume that B C max 034 l  B R max 034 n andthat lB R max  nB C max  and the size of the dataset are of similar magnitude which is justi\002able for the type of datasets with which we are concerned Possible values could be B C max 50  l  1000  B R max 30 and n  5000  Our analysis uses reasonable worst-case assumptions about the distribution of the data and the number of itemsets that need to be processed Such 223rough\224 analysis is warranted since it may be dif\002cult to analyze statistically the data distribution which may change frequently in practice In previous analyses e.g 4   mar k et b as k e t d at a i s assumed or in some cases observed empirically to yield fewer frequent itemsets as the size of candidate itemsets increases For our analysis we make no assumptions about the likely amount of support for itemsets of any size except to assume a maximum length for the candidate itemsets to be considered 4.1 Row\255Wise Processing We consider the processing cost of the RW strategy for counting frequent itemset support In the 002rst pass each single item must be c ounted to establish the support of the 1-itemsets and this is done by a RW scan of the entire database This requires lB R max space for the database itself For each row the RW algorithm must update the counter for each item actually present in the row Since any item may be present in any row all n items must have a counter along with a representation of the item itself which requires 2 n space for the counters and single itemsets For this comparative analysis since the two algorithms are assumed to use the same mechanism for itemset lookup and both produce the same number of candidates we assume the existence of a 002xed-time hash-based lookup scheme and drop this constant term from further expressions of time cost Thus the entire step of calculating L 1 takes in the worst case lB R max time to increment all counters and n time to determine which of the 1-itemsets are indeed frequent Assume that all potentially relevant counts are to be maintained which is necessary for the later generation of association rules involving all frequent itemsets Then the required number of counters will be j L k j for iterations k 2 k max and j C k max 1 j for the last iteration where j L k j represents the number of elements in the set L k  The total amount of space needed for the counters without their identifying candidate sets for all iterations of this loop will be 020 P k max k 2 j L k j 021  j C k max 1 j  The space required for the candidate sets themselves is described below Note that in the worst case the support in the database will be such that all k itemsets for k 024 k max will be frequent so that j L k j  000 n k 001 and j C k j  000 n k 001  and the space needed will be P k max 1 k 2 000 n k 001  Since the join step actually produces all possible extensions before pruning and each new candidate will contain k items and there will be j C k j such candidates this step requires k j C k j space for the generated itemsets At a minimum each of the j C k j candidate sets of size k must be the result of joining at least two members of L k 000 1  and producing each member of C k will require k 000 1 comparisons  k 000 2 of which are to determine equality and the last of which is to determine the order of the 002nal two items Thus at least  k 000 1 j C k j time is required for the join of items from L k 000 1 to produce C k  For the prune step each of the possible  k 000 1 subsets of each k itemset must be veri\002ed for minimum support and we assume that the amount of time to verify a single  k 000 1 subset of any member of C k is 002xed regardless of the size of L k 000 1  In the worst case all k 000 1 subsets of an element in C k will be frequent and therefore all must be veri\002ed for t 


a total cost of k j C k j time for the prune step For the counting since each row has at most B R max items it lends support to no more than 000 B R max k 001 k itemsets and itmaytakeasmuchas l 000 B R max k 001 time to increment the necessary counters assuming a unit cost for locating the counters Finally we compare each c ount with the userspeci\002ed minimum support for a cost of j C k j time Since all of the counters are being accessed in a single scan there is no additional cost for locating each c ounter 4.2 Column\255Wise Processing As in the RW algorithm each single column must be counted to calculate and verify its support The database itself may need at most nB C max space and the counters and their associated identi\002ers will take 2 n space  Counting the support of all of the columns will also take at most nB C max time and the veri\002cation that each meets the minimum support will take n time  Both algorithms use the same subroutine i.e apriorijoin of 4  to c o m pute t he s a m e c o lle c tion o f c a ndida te s e ts  and the space for the counters is as in the RW algorithm For the join step of the CW algorithm we will produce only one new candidate c  at a time and count its support if it has no infrequent  k 000 1 subsets As before the process of joining two elements of L k 000 1 requires k 000 1 time  There will be k subsets of size  k 000 1 that will need to be veri\002ed which will take k time and no additional space to verify that each is frequent As in the RW case we assume a 002xed-time lookup for each of the subsets regardless of the value of k  The conjoinment operation was described in Section 3.1 For support counting in the CW algorithm the analysis differs depending on whether the results of a conjoin operator are preserved at each stage when the itemset is shown to be frequent or the conjoin is recomputed for each candidate If the results of a conjoin are preserved a new itemset will be created together with its counter value Therefore each count will take up to 013B C max time  where the small e.g 2 or 3 constant 013 is determined by the exact algorithm used to compute the conjoin Each new frequent conjoin may also require as much as k  B C max  1 space for the conjoin itself and its related identi\002ers and counter Finally the veri\002cation of support for all candidates takes j C k j time  4.3 Disk Access Costs We now examine and compare the I/O cost of both the RW and CW representations for counting the support of frequent itemsets This comparison will allow us to draw more realistic conclusions about the relative performance of the two methods since such frequent datasets and bookkeeping information would entail many disk accesses We assume the amount of memory for use by the algorithm to be M blocks These M blocks do not include the memory needed by the operating system or disk access mechanism e.g index structures needed for lookup Where appropriate the set of M blocks is subdivided into M D blocks of memory for data into which blocks will only be read and M C blocks of memory for counters and other bookkeeping information which may need to be written again to disk In stages of the algorithm e.g the join for which three areas of memory are necessary the M D blocks of memory are further subdivided into M D 1 and M D 2 blocks of memory where M D 1  M D 2  M D  Also we assume that certain numbers of elements will 002t into a single block i.e the blocking factor for itemsets counters rows etc Each of these factors is designated by a variable f  with an appropriate subscript So the blocking factor for a unit space entity e.g a single counter without its related identi\002er is f I  the blocking factor for rows is f R  and the blocking factor for columns is f C  where it is to be understood that we mean the average number of rows or columns in a block It should be noted that these blocking factors are integer values which means that most of the expressions in the following sections should have appropriate ceiling or 003oor modi\002ers 227 which we ignore for the sake of simplicity in the expressions As before for the row-wise approach the initial step to determine L 1 requires time and space on the order of the database size and the main loop will iterate until k  k max 1  For the join let M D 1 be the number of data blocks used for the 223outer\224 loop over L k 000 1 and M D 2 be the number of 223inner\224 blocks The blocks of the outer copy are read once at a cost of k j L k 000 1 j f I I/O reads where use of the multiplier k assumes that each frequent itemset also includes space for its counter For each set of M D 1 outer blocks the remaining outer blocks must also be read Once the join and prune are complete the support of each candidate in C k must be counted Each row could require access to a minimum of 0 counters e.g if the number of items in the row were less than k and a maximum of 000 B R max k 001 counters if each k subset of the row were to support a candidate In the best case all j C k j counters will 002t into M C blocks with an I/O cost of lB R max f I  k 1 j C k j f I In the worst case as each candidate c ounter is incremented a block access is entailed giving a cost of j C k j block accesses per set of rows 002tting into M D blocks If we examine the cost of reading the entire set of counters for each set of M D rows which will be necessary in the worst case then the number of I/O block reads needed to increment all counters would be lB R max f I  l  M D f R   k 1 j C k j This expresses the cost of reading the entire database once into M D blocks and then for each set of M D blocks staging in the entire set of j C k j counters  each of which is of size k 1  Since each block read and updated will also have 


to be written this will require an additional write cost of l  M D f R   k 1 j C k j  Finally the support count for each candidate must be compared to the minimum support and the frequent itemsets L k  will need to be written out to disk for a read cost of  k 1 j C k j f I and a write cost of  k 1 j L k j f I  The total cost of the RW counting algorithm in terms of disk access is given by the cost of each phase for reading and writing summed over all values of k up to k max 1  The body of the summation can be expressed as RW algorithm cost  Read cost of join  Write cost of join  Read cost of prune  Write cost of prune  Read cost of counters  Write cost of counters  Read cost of C k  Write cost of L k We do not build this expression explicitly here since many of the terms will be common to the CW analysis and we need only compare the terms by which the two differ If we choose not to keep the results of the conjoin the I/O cost of the CW algorithm for each value of k is CW algorithm cost  Read cost of join  Write cost of join  Read cost of prune  Write cost of prune  Read cost of counters  Read cost of columns  Write cost of L k Given that the cost of the join prune and writing out of the frequent itemsets in L k are common to both the RW and the CW algorithm we need not consider them for our comparison This leaves the cost of accessing the c ounters candidates and columns as appropriate For the CW approach if we choose not to keep the results of the conjoin we have the cost of reading a single candidate k columns for each member of C k  In the worst case the columns might be accessed in an order that requires a new block read each time giving block accesses on the order of kB C max j C k j and  k 1 j C k j for each pass over the candidates of size k  where k  f 1 k max 1 g  The size of C k is the dominating factor in these expressions The cost by which the RW algorithm differs from the CW is the cost of reading and writing the counters In the worst case each block of rows may require access to every block of counters and the limited memory may force each to be rewritten so roughly speaking we 002nd that the cost of the RW algorithm is dominated by two terms of magnitude l  k 1 j C k j  Since we assumed that B C max 034 l  it can be seen that the CW approach is more ef\002cient in worstcase I/O cost than the RW approach For datasets in which the number of rows is far greater than the number of binary columns i.e the database is 223taller\224 the effect of reading the columns for the conjoin becomes more pronounced and the RW approach would at some point become more ef\002cient Of course we argue that the trade-offs in the use of materialized information such as conjoins should be considered in this comparison 5 Empirical Observations Our key observation is that with limited memory where processing costs are overshadowed by the I/O costs and a large number of items relative to the number of rows the CW approach is more ef\002cient As noted previously in this paper in many cases of interest this happens to be the case The I/O costs dominate because the processing that is done is more data-intensive than compute-intensive e.g few simple operations are performed for a large number of elements accessed from disk Note that during the c ounting phase if each block is read only when needed for a particular subset then the number of block reads and subsequent writes signi\002cantly depends on the distribution of the data the block replacement strategy the particular algorithm used to select the order of the subsets etc We provide only a few experimental results due to space constraints additional results are available in   5.1 Experimental Framework The level of uncertainty in the actual cost of I/O depending on the characteristics of the data and other related factors e.g whether certain data structures will 002t entirely into memory is not well represented in the analysis of Section 4.3 Based on I/O and processing cost analysis for the RW and CW versions of the Apriori algorithm we have implemented a simulation framework to verify experimentally the effect of various factors on these costs This framework allows us to simulate and measure various implementations of both RW and CW support-counting algorithms while controlling parameters such as the block size the number of available memory blocks the minimum required level of support for frequent itemsets whether or not the results of the conjoin are kept etc For the datasets we used a standard synthetic dataset generator 1 t o cr eat e s i m u l at ed mar k et b as k e t d at as et s  varying parameters such as the number of distinct items  n  the number of rows  l  and the available support for k itemsets We ran a number of experiments with different values for the variables in our analysis e.g M  f I etc.\,in order to verify empirically our analytical results All experiments were simulated using a framework written in C and Sparc workstations running Sun OS 4.1.4 Itemsets rows and columns were all represented as extensible arrays of identi\002ers where each identi\002er is taken to be of a unit size speci\002cally 4 bytes The number of identi\002ers which 002t into a simulated block of memory can be 


speci\002ed for each experiment and this value is then used to determine the number of itemsets rows or columns in a single block As itemsets are created each is added to an available block and new blocks are 223allocated\224 as needed If a new block is to be allocated and the number of blocks in the simulated memory also a parameter of the experiment is equal to the number of occupied blocks a victim block is chosen for replacement using one of a number of replacement strategies If the data in the victim block has been modi\002ed since last being written to disk it is 223written,\224 and a simulated data block write is counted If a data element is required from a block that has previously been written to disk e.g for a subset lookup that block is 223read\224 into the simulated memory and a data block read is counted When the counting algorithm is complete the total number of blocks written and read during the simulation is available for examination In addition to the number of data block reads and writes the simulation also counts the number of equal-cost CPU operations as described in Section 4 and the number of index block reads and writes This allows a comparison of the different algorithms based on the number of operations performed and the additional cost of index lookups The counting algorithms themselves are implemented in an I/O-ef\002cient manner using a block pinning and release strategy In all cases the join step is performed using a block nested loop 9 a l g o r i t h m w i t h each s e t o f b l o ck s i n the outer loop being pinned and released as necessary The number of blocks available for the inner and outer loops are both parameters of the experiment 5.2 Effects of Dataset Parameters The experiments were chosen to compare the effect of various distributions and relative 223dimensions\224 i.e number of rows compared to the number of distinct items or columns of the data For the RW approach subsets were identi\002ed in each row of the database and c ounters were accessed only as needed For the CW case the results of the conjoin were not kept For all experiments an LRU replacement strategy was used to determine victim blocks in memory Speci\002c parameters for each of the following 002gures are given at the end of this section in Figure 5 Figure 3 shows a comparison of the number of data block reads and writes for both the RW and CW approach as the level of minimum support is decreased leaving all other parameters 002xed Note that as the required level of minimum support is decreased the difference between the two approaches in the total number of block movements increases at an accelerating rate Figure 4 shows the results of an experiment intended to demonstrate the relative I/O cost of the RW and CW algorithms for datasets of 223equal height\224 i.e the number of  0 200,000 400,000 600,000 800,000 22 20 18 16 14 12 10 8 6 4 Minimum support RW CW Figure 3 Effect of decreasing support 100 1,000 10,000 100,000 0.10 0.20 0.40 0.60 0.80 1.00 1.20 1.40 1.60 Width ratio RW CW Figure 4 Effect of table width rows in the data held constant and varying 223width\224 i.e the number of columns The total number of blocks read or written is plotted in a logarithmic scale along the y axis including index blocks The x axis shows the relative ratio of the number of columns to the number of rows in the dataset For example a dataset with 1000 columns and 1000 rows would have a width ratio of 1 while a dataset with 100 columns and 1000 rows would have a width ratio of 0.1 That is the higher the width ratio the 223wider\224 the table is relative to its 223height\224 The number of rows i.e l as held constant at 500 while the number of columns i.e n  was varied from 50 to 1000 The other parameters of the dataset i.e B C max  B R max  were determined by the default values of the synthetic dataset generator As expected from the analysis of Section 4.3 the CW algorithm is far more ef\002cient than the RW algorithm for datasets with a relatively high width ratio Once the number of rows is suf\002ciently greater than the number of columns the advantage of the CW algorithm is no longer apparent Our experiments more results are presented in v e r ify that for datasets of the type we described accessing the database in a column-wise fashion leads to a reduction in the number of I/O operations required Again the key point 


Figure Rows  l  Items  n  M M D 1 M D 2 f I minsup 3 1000 1000 16 8 4 512 22-4 4 500 50-1000 32 10 8 512 0  2 l Figure 5 Experimental parameters per 002gure is that the counters which maintain the identity and frequency of occurrence for a particular set of items need not be repeatedly and expensively brought into the primary storage as is the case in the row-wise method 6 Discussion and Conclusions We discuss a few possible improvements not explored in our current work followed by a summary of our conclusions 017 We assumed that the RW algorithm strictly follows the strategy of identifying each subset present in a given row and then accesses the appropriate c ounter However an implementation may also read a number of blocks of counters and then 002nd all rows with an itemset to support that counter as per T h e o rde r ing o f the rows and the counters will likely have an effect on which of these two methods is more ef\002cient 017 An obvious improvement to the cost of the RW algorithm as presented is to perform the join for a pair of  k 000 1 itemsets or even a set of blocks thereof and then immediately perform the check of all its subsets Given the likelihood that many of the itemsets in ablockof L k 000 1 will share common items this could result in a signi\002cant savings since the cost of writing the join and then re-reading for the prune would be avoided probably without a signi\002cant increase in the number of counter blocks that need to be read 017 In the RW Apriori counting algorithm the primary purpose of the veri\002cation and pruning of all  k 000 1 subsets of any candidate before actually counting its support is to limit the number of times that a row or candidate need be brought in for support counting Since the CW algorithm already has all information necessary for counting at one pass it may be better to avoid the Apriori veri\002cation of subsets and simply to count the support of each candidate This is especially true if the candidates in a given block of itemsets all share a frequent common subset as the conjoin for that subset could be computed once and used directly in counting the support of candidates sharing that subset Also given the possibly larger processing cost of the CW algorithm when memory is unconstrained and the savings in I/O when memory is limited some combination of both the RW and CW algorithms may yield the best performance As organizations begin collecting their own electronic datasets and as growing sources of information become available through the Internet it is imperative that the data organization and access approaches be studied carefully to exploit ef\002cient and promising processing techniques In particular in this paper we have examined the I/O ef\002ciency considerations for association rules algorithms with respect to data organization We have shown that a column-wise strategy for mining tabular data may provide an improvement in the I/O ef\002ciency over a similar row-wise algorithm We used a simple analysis and experimental validation to support our approach This provides an indication that similar considerations may signi\002cantly bene\002t other high-cost computing problems associated with mining of datasets References 1 R  A gr a w al et al  T he Q u est D at a M i n i n g S yst e m  Technical report IBM Almaden Research Center 1996 http://www.almaden.ibm.com/cs/quest 2 R  A g r a w a l  T  I m i e lin s k i a n d A  S w a m i  M in in g A s s o c i ation Rules between Sets of Items in Large Databases In Proceedings of the 1993 ACM SIGMOD Int'l Conf on Management of Data  1993 3 R  A g r a w a l  H  M a n n illa  R  S rik a n t  H  T o i v o n e n  a n d A  I  Verkamo Fast Discovery of Association Rules In Advances in Knowledge Discovery and Data Mining  pages 307\226328 AAAI Press  MIT Press 1996 U.M Fayyad G PiatetskyShapiro P Smyth and R Uthurusamy editors 4 R  A gr a w al and R  S r i kant  F ast A l gor i t h m s f o r M i n i n g Association Rules In Proc 20th Int'l Conference on Very Large Databases  Santiago Chile 1994 5 R  J  B ayar do Ef 002ci e nt l y M i ni ng Long P a t t e r n s f r o m Databases In Proceedings of the 1998 ACM SIGMOD Conf on Management of Data  1998 6 V  C r e st ana D ec 1997 I n f o r m al cor r e spondence 7 B  D unkel a nd N  Sopar k ar  D at a O r g ani zat i o n a nd A ccess for Ef\002cient Data Mining Technical report The University of Michigan Ann Arbor 1999 8 B  D unkel  N  Sopar k ar  J  S zar o and R  U t hur usam y  Systems for KDD From concepts to practice Journal of Future Generation Computer Systems  October 1997 9 A  S i l b er schat z  H  K or t h  a nd S Sudar s han Database System Concepts  McGraw-Hill New York third edition 1997  M  J Zaki  S  P ar t h asar at hy  M  O gi har a  a nd W  Li  N e w Algorithms for Fast Discovery of Association Rules In 3rd Int'l Conf on Knowledge Discovery and Data Mining  pages 283\226286 Newport California Aug 1997 


local support count X.sup must be smaller than the local threshold s x D Following from the discus sion in Subsection 3.3 X.supq is bounded by the value min\(maxsupq X s x D  1 Hence an upper bound of X.sup can be computed by the sum x.supj  jEX.large-sites 2 min\(mazsupq\(X s x Dq  1 q=l q+?X.large-sites In FDM-LPP Si calls p-upper-bound to compute an upper bound for X.sup according to the above for mula This upper bound can be used to prune away X if it is smaller than the global support threshold 0 As discussed before both FDM-LUP and FDM LPP may have less candidate sets than FDM-LP How ever they require more storage and communication messages for the local support counts Their efficiency comparing with FDM-LP will depend largely on the data distribution 5 Performance Study of FDM An in-depth performance study has been performed to compare FDM with CD We have chosen to im plement the representative version of FDM FDM LP and compare it against CD Both algorithms are implemented on a distributed system by using PVM Parallel Virtual Machine 6 A series of three to six RS/6000 workstations running the AIX system are connected by a 10Mb LAN to perform the experi ment The databases in the experiment are composed of synthetic data In the experiment result the number of candidate sets found in FDM at each site is between 10  25 of that in CD The total message size in FDM is between 10  15 of that in CD The execution time of FDM is between 65  75 of that in CD The reduction in the number of candidate sets and message size in FDM is very significant The reduction in execution time is also substantial However it is not directly proportional to the reduction in candidate sets and message size This is mainly due to the overhead of running FDM and CD on PVM What we have ob served is that the overhead of PVM in FDM is very close to that in CD even though the amount of mes sage communication is significantly smaller in FDM From the results of our experiments it is also clear that the performance gain of FDM over CD will be higher in distributed systems in which the commu nication bandwidth is an important performance fac tor For example if the mining is being done on a distributed database over wide area or long haul net work The performance of FDM-LP against Apriori in a large database is also compared. In that case the response time of FDM-LP is only about 20 longer Interpretation transaction mean size mean size of maximal potentially large itemsets number of potentially large itemsets Number of items Clustering size Pool size Correlation level Multiplying factor Parameter ITI III ILI N sq Ps Mf Cr Value 10 4 2000 1000 5-6 50  70 0.5 1260  2400 Table 5 Parameter Table than 1/n of the response time of Apriori where n is the number of sites This is a very ideal speed-up In terms of total execution time FDM-LP is very close to Apriori The test bed that we use has six workstations Each one of them has its own local disk, and its partition is loaded on its local disk before the experiment starts The databases used in our experiment are synthetic data generated using the same techniques introduced in 2 lo The parameters used are similar to those in lo Table 5 is a list of the parameters and their values used in our synthetic databases Readers not familiar with these parameters can refer to 2  In the following we use the notation Tx.Iy.Dm to denote a database in which D  m in thousands IT1  x and 111  y T10.14.D200K s  3 4 5 6 Number of Nodes FDM CD Figure 1 Candidate Sets Reduction n  3 4 5 6 5.1 Candidate Sets and Message Size Re duction The sizes of the databases in our study range from 200K to 600K transactions and the minimumsupport threshold ranges from 3 to 3.75 Note that the number of candidate sets at each site are the same in CD and different in FDM In our experiment we witnessed a reduction of 75  90 of candidate sets on 39 


T10.14.D200K, n  3 T10.14.D200K, n  3 60  I S 8 3.00 3.25 3.510 3.75 YO  I YO Minimum support FDM kCD  gs 3.00 3.25 3.50 3.75 Minimum support FDM CD Figure 4 Message Size Reduction Figure 2 Candidate Sets Reduction average at each site when FDM-LP is compared with CD In Figure 1 the average number of candidate sets generated by FDM-LP and CD for a 200K transaction database are plotted against the number of partitions FDM-LP has a 75  90 reduction in the candidate sets The percentage of reduction increases when the number of partitions increases This shows that FDM becomes more effective when the system is scaled up In Figure 2 the same comparison between FDM-LP and CD is presented for the same database with three partitions on different thresholds In this case, FDM LP experienced a similar amount of reduction T10.14.D200K s  30/0 I 150 100 50 0 3 4 5 6 Number of Nodos FDM CB Figure 3 Message Size Reduction n  3 4 5 6 The reduction in candidate sets should have a pro portional impact on the reduction of messages in the comparison Moreover as discussed before the polling site technique guarantees that FDM only requires O\(n messages for each candidate set which is much smaller than the O\(n2 messages required in CD In our experiment FDM has about 90 reduction in the total message size in all cases when it is compared with CD In Figure 3 the total message size in FDM and CD for the same 200K database are plotted against the number of partitions In Figure 4 the same compari son on the same database of three partitions with dif ferent support thresholds are presented Both results confirm our analysis that FDM-LP is very effective in cutting down the number of messages required T10.14.D200K s  3 90 E3 28  U 70 cc Q 8 a c 50 c xs w  I 3 4 5 6 Number of Nodes FDM CD Figure 5 Execution Time n  3 4 5 6 T10.14.D200K n  3 3.00 3.25 3.50 3.75 Minimum Support E-FDM A-CD Figure 6 Execution Time 5.2 Execution Time Reduction We have also compared the execution time between FDM-LP and CD The execution time of FDM-LP and CD on a 200K database are plotted against the number of partitions in Figure 5 FDM-LP is about 40 


25  35 faster than CD in all cases In Figure 6 the comparison is plotted against different thresholds for the same database on three partitions Again FDM LP is shown to have similar amount of speed-up as in Figure 5 n  3 D  60011 s  2 I Apriori I FDM-LP response time sec I 1474 I 387 I total execution time sec I 844.7 I 842.9 I Table 6 Efficiency of FDM-LP We have also compared FDM-LP on three sites against Apriori with respect to a 600K transactions database in order to find out its efficiency in large database The result is shown in Table 6 The re sponse time of FDM-LP is only slightly 20 larger than 1/3 of that of Apriori In terms of the total ex ecution time FDM-LP is very close to Apriori For a large database FDM-LP may have a bigger portion of the database residing in the distributed memory than Apriori Therefore it will be much faster than running Apriori on the same database in a single ma chine This shows that FDM-LP on a scalable dis tributed system is an efficient and effective technique for mining association rules in large databases The performance study has demonstrated that FDM generates a much smaller set of candidate sets and requires a significantly smaller amount of mes sages when comparing with CD The improvement in execution time is also substantial even though the overhead incurred from PVM prevents FDM from achieving a speed-up proportional to the reduction in candidate sets and message size Even though we have only compared CD with FDM-LP there is enough evidence to show that FDM is more efficient than CD in a distributed environment In the follow ing sections we will discuss our future plan of imple menting the other versions of FDM 6 Discussions In this discussion we will first discuss the issue of possible extension of FDM for fast parallel mining of association rules Following that we will discuss two other related issues 1 the relationship between the effectiveness of FDM and the distribution of data and 2 support threshold relaxation for possible reduction of message overhead The CD and PDM algorithms are designed for share-nothing parallel environment. In particular CD has been implemented and tested on the IBM SP2 machine In designing algorithm for parallel mining of association rules not only the number and size of messages required should be minimized but also the number of synchronizations which is the number of rounds of message communication CD has a simple synchronization scheme It requires only one round of message communication in every iteration Besides the second iteration PDM also has the same synchro nization scheme as CD If FDM was used in the paral lel environment it has a shortcoming even though it requires much less message passings then CD it needs more synchronizations However FDM can be modi fied to overcome this problem In fact in each itera tion the candidate set reduction and global pruning techniques can be used to eliminate many candidates and then a broadcast can be used to exchange the local support counts of the remaining candidates This ap proach will generate less candidate sets than CD and has the same number of synchronization Therefore it will perform better than CD in all cases Performance studies has been carried out in a 32-nodes IBM SP2 to study several variations of this approach and the result is very promising Another interesting issue is the relationship be tween the performance of FDM and the distribution of the itemsets among the partitions From both The orem 1 and Example 1 it is clear that the number of candidate sets decreases dramatically if the distribu tion of itemsets is quite skewed among the partitions If most of the globally large itemsets were locally large at most of the sites the reduction of candidate sets in FDM would not have been as significant In the worst case if every globally large itemset is locally large at all the sites the candidate sets in FDM and CD will be the same Therefore data skewness may improve the performance of FDM in general Special partitioning technique can be used to increase the data skewness to optimize the performance of FDM Some further study is required to explore this issue The last issue that we want to discuss is the pos sible usage of the relaxation factor proposed in ll In FDM if a site sends not only those candidate sets which are locally large but also those that are almost locally large to the polling sites the polling sites may have local support counts from more sites to perform the global pruning of candidate sets For example if the support threshold is lo every site can send the candidate sets whose local support counts exceed 5 to their polling sites In this case for some candi date sets their polling sites may receive local sup port counts from more sites than the no relaxation case Hence the global pruning may be more effec tive However there is a trade-off between sending more candidate sets to the polling sites and the prun ing of candidate sets at the polling sites More study is necessary on the detailed relationship between the relaxation factor and the performance of the pruning 7 Conclusions In this paper we proposed and studied an efficient and effective distributed algorithm FDM for mining association rules Some interesting properties between 41 


locally and globally large itemsets are observed which leads to an effective technique for the reduction of can didate sets in the discovery of large itemsets Two powerful pruning techniques local and global prun ings are proposed Furthermore the optimization of the communications among the participating sites is performed in FDM using the polling sites Sev eral variations of FDM using different combination of pruning techniques are described A representative version FDM-LP is implemented and whose perfor mance is compared with the CD algorithm in a dis tributed system The result shows the high perfor mance of FDM at mining association rules Several issues related to the extensions of the method are also discussed The techniques of can didate set reduction and global pruning can be inte grated with CD to perform mining in a parallel envi ronment which will be better than CD when consider ing both message communication and synchronization Further improvement of the performance of the FDM algorithm using the skewness of data distribution and the relaxation of support thresholds is also discussed Recently there have been interesting studies on the mining of generalized association rules multiple level association rules quantitative association rules etc Extension of our method to the min ing of these kinds of rules in a distributed or parallel system are interesting issues for future research Also parallel and distributed data mining of other kinds of rules such as characteristic rules 7 classification rules, clustering 9 etc is an important direction for future studies For our performance studies an im plementation of the different versions of FDM on an IBM SP2 system with 32 nodes has been carried out and the result is very promising References l R Agrawal and J C Shafer Parallel mining of association rules Design implementation and experience In IBM Research Report 1996 2 R Agrawal and R Srikant Fast algorithms for mining association rules In Proc 1994 Int Conf Very Large Data Bases pages 487-499 Santiago Chile, September 1994 3 R Agrawal and R Srikant Mining sequential patterns In Proc 1995 Int Conf Data Engi neering pages 3-14 Taipei, Taiwan March 1995 4 D.W Cheung J Wan V Ng and C.Y Wong Maintenance of discovered association rules in large databases An incremental updating tech nique In Proc 1996 Int\222l Conf on Data Engi neering New Orleans, Louisiana Feb 1996 5 U M Fayyad 6 Piatetsky-Shapiro P Smyth and R Uthurusamy Advances zn Knowledge Dis covery and Data Mining AAAI/MIT Press 1996 6 A Geist A Beguelin J Dongarra W Jiang R Manchek and V Sunderam PVM Parallel Virtual Machine A Users\222 Guide and Tutorial for Networked Parallel Computing MIT Press 1994 7 J Han Y Cai and N Cercone Data driven discovery of quantitative rules in relational databases IEEE Trans Knowledge and Data En gineering 5:29-40 1993 Discovery of multiple-level association rules from large databases In Proc 1995 Int Conf Very Large Data Bases pages 420-431 Zurich Switzerland Sept 1995 8 J Han and Y Fu 9 R Ng and J Han Efficient and effective cluster ing method for spatial data mining In Proc 1994 Int Conf Very Large Data Bases pages 144-155 Santiago Chile, September 1994 lo J.S Park M.S Chen and P.S Yu An effec tive hash-based algorithm for mining association rules In Proc 1995 ACM-SIGMOD Int Conf Management of Data pages 175-186 San Jose CA May 1995 ll J.S Park M.S Chen, and P.S Yu Efficient par allel mining for association rules In Proc 4th Int Conf on Information and Knowledge Manage ment pages 31-36 Baltimore Maryland Nov 1995 12 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases In Proc 1995 Int Conf Very Large Data Bases pages 432-443 Zurich Switzerland Sept 1995 13 A Silberschatz M Stonebraker and J D U11 man Database research Achievements and op portunities into the 21st century In Report of an NSF Workshop on the Future of Database Sys tems Research May 1995 14 R Srikant and R Agrawal Mining general ized association rules In Proc 1995 Int Conf Very Large Data Bases pages 407-419 Zurich Switzerland Sept 1995 association rules in large relational tables In Proc 1996 ACM-SIGMOD Int Conf Manage ment of Data Montreal Canada June 1996 15 R Srikant and R Agrawal Mining quantitative 42 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


