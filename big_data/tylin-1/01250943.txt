Ef\336cient Data Mining for Maximal Frequent Subtrees Yongqiao Xiao Jenq-Foung Yao Dept of Math  Computer Science Georgia College and State University Milledgeville GA 31061  yongqiao.xiao jf.yao  gcsu.edu Zhigang Li Margaret H Dunham 001 Dept of Computer Science  Engineering Southern Methodist University Dallas TX 75275  zgli,mhd  engr.smu.edu Abstract A new type of tree mining is de\336ned in this paper which uncovers maximal frequent induced subtrees from a database of unordered labeled trees A novel algorithm PathJoin is proposed The algorithm uses a compact data structure FST-Forest which compresses the trees and still keeps the original tree structure PathJoin generates candi 
date subtrees by joining the frequent paths in FST-Forest Such candidate subtree generation is localized and thus substantially reduces the number of candidate subtrees Experiments with synthetic data sets show that the algorithm is effective and ef\336cient 1 Introduction Data mining has evolved from association rule mining 1 s e quence mi ni ng 2  4  1 0   t o t ree m i n i n g 12  5  and graph mining 11 8 7 As s o ci at i o n r ul e m i n i n g and sequence mining are one-dimensional structure mining and tree mining and graph mining are two-dimensional or higher structure mining The applications of tree mining arise from Web usage mining mining semi-structured data and bioinformatics etc The focus of this paper is on a ew type of tree mining 
As a motivating example for this new type of tree mining consider mining the Web logs at a particular Web site Several types of traversal patterns have been proposed to analyze the browsing behavior of the user[4 10  O ne dra wback of such one-dimensional traversal patterns for the Web logs is that the document structure of the Web site which is essentially hierarchical a tree or a graph is not well captured In this paper we uncover the maximal frequent subtree structures from the access sessions The access sessions 001 This material is based upon work supported by the National Science 
Foundation under Grant No IIS-9820841 and IIS-02808741 are regarded as trees instead of sequences The trees are unordered and e frequent subtrees are induced subtrees and maximal Other contributions of the paper include a compact data structure is used to compress the trees in the database and at the same time the original tree structure is kept and the proposed algorithm PathJoin uses a new candidate subtree generation method which is localized to the children of a node in a tree and thus substantially reduces the number of candidate subtrees The rest of the paper is organized as follows In Section 2 the tree mining problem is formally de\223ned and then the related work is described and compared to our work Sec 
tion 3 describes the compact data structure and the PathJoin algorithm Section 4 reports the experimental results The last Section concludes the paper and points out the future work 2 roblem Statement and Related Work 2.1 Problem Statement A tree is an acyclic connected directed graph Formally we denote a tree as T   N,B,r,L  where N is the set of nodes B is the set of branches directed edges r 001 N is the root of the tree and L is the set of labels on the nodes For each branch 
b  n 1 n 2  001 B where n 1 n 2 001 N  we call n 1 the parent of n 2 and n 2 a child of n 1 Ifthe children of each node are orderd the tree is called an ordered tree otherwise,itisan unordered tree  On each node n i 001 N  there is a label l 
i 001 L  The labels in a tree could be unique or duplicate labels are allowed for different nodes Without loss f generality the labels are represented by positive integers Paths and Root Paths A path is a sequence of connected branches i.e p   n 1 n 2   n 2 n 3  267\267\267  n k 212 1 n k  where n i 001 N 
1 002 i 002 k  and k is the number of nodes on the path For short we represent the path just by the nodes on the path n 1 n 2  267\267\267 n k  A node x is called an ancestor of another node y if there exists Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


a path starting from x to y  and accordingly y is called a descendent of x  A path starting from the root node is called a root path  Since there is only one root path to ny node in the tree each root path in a tree can be uniquely identi\223ed by the last node on the path In Figure 1 node n 3 represents the root path n 1 n 2 n 3   and the labels on the path are  1  2  3   Subtrees and Root Subtrees Atree T 001   N 001 B 001 r 001 L 001  is said to be a subtree of another tree T   N,B,r,L   if and only if there exists a mapping 001  N 001 003 N such that 1 for each node x 001 N 001  l  x  l  001  x  where l is the labeling function l  N 003 L and\(2 for each branch b  x,y 001 B 001  001  x  001  y   001 B  Such subtrees are called induced subtrees in 12  T h e embedded subtrees de\223ned in 12 a llo w t he tw o nodes after mapping to be on the same path ancestor/descendent relationship that is 001  x   267\267\267 001  y   is a path in T Inthis paper a subtree is referred to as an induced subtree unless otherwise indicated explicitly If a tree T 001 is a subtree of another tree T  we also say that tree T contains T 001 or T 001 occurs in T  If a subtree has the same root as the tree i.e r 001  r  the subtree is called a root subtree  Figure 1 shows a root subtree S of tree T  Itemset Representation r Root Subtrees A root subtree T 001 of tree T can be uniquely identi\223ed by the corresponding nodes in T of the leaf nodes in T 001  i.e a root subtree T 001 with k leaf nodes  y 1 y 2  267\267\267 y k  can be represented by the set of nodes  x 1 x 2  267\267\267 x k  in T where x i  001  y i  1 002 i 002 k   Such representation for root subtrees is called itemset representation  since the set of representative nodes for the subtree is similar to a k itemset where a k itemset consists of k items as for association rules 1 and an i t e m h ere corres ponds t o a r epres ent a t i v e node i n the tree i.e the root path ending at the node For the root subtree S of tree T in Figure 1 the itemset representation is  n 3 n 5 n 7    1 2 3 5 7 A root subtree S of T 2 4 7 6 5 n1,[1,7 n2,[2,6 n5,[5,6 n7,[7,7 A tree T 1 3 n3,[3,4 n4,[4,4 n6,[6,6 Itemset representation for S: {n3, n5, n7 1,2,3 1,2,3,4 1,2,5 1,2,5,6 1,7 1,2 1 Figure 1 A Tree Example Support and Maximal Frequent Subtrees Given a database of trees D  and a subtree S  the frequency of S in D  freq D  S   is the total number of occurrences of S in D  i.e freq D  S  001 T 002 D freq T  S  where freq T  S  is 1 if S occurs in T otherwise 0 The support of S in D  sup D  S   is the percentage of trees in D that contain S  i.e sup D  S  freq D  S   D  where  D  is the number of trees in D  Such de\223nition excludes multiple occurrences of the subtree in a tree thus it is called unweighted support  If the frequency of S includes every occurrence of S in every tree T in D i.e FREQ T  S  is n if S occurs n times in T otherwise 0  the support can be de\223ned as the ratio of the total frequency to the total size of the database total number of nodes in all trees i.e SUP  S  001 T 001 D FREQ T  S  001 T 001 D  T  where  T  is the number of nodes in T  i.e  N    Such support is called weighted support  and it is similar to that in 10  W ei ght ed s upport and frequency are shown in upper case letters to distinguish from unweighted support and frequency Given some support threshold s min for unweighted or S min for weighted a subtree is said to be frequent if the support for the subtree is not less than the threshold i.e sup  X  004 s min for unweighted support or SUP  X  004 S min for weighted support A frequent subtree is maximal if it is not a subtree of another frequent subtree The Frequent Subtree Mining Problem Given a database of trees D  and some support threshold s min or S min  the objective is to 223nd all maximal frequent subtrees in the database The trees in D are assumed to be unordered trees and the nodes in a tree could have duplicate labels however the labels for the children of every node are assumed to be unique The support could be weighted or unweighted 2.2 Related Work In 12 Zaki pres ented tw o a lgorithms  T reeMiner and PatternMatcher for mining embedded subtrees from ordered labeled trees PatternMatcher is a level-wise algorithm similar to Apriori 1 f o r m in in g asso ciatio n r u l es TreeMiner performs a depth-\223rst search for frequent subtrees and uses the novel scope-list a vertical representation for the trees in the database for fast support counting FREQT was proposed in 3 f or mi ni ng l a bel ed or dered trees FREQT uses the notion of rightmost expansion to generate candidate trees by a ttaching new nodes only to the rightmost branch of a frequent subtree The problem of discovering frequent substructures from hierarchical semistructured data was proposed in 5 It as s u mes t hat t h e h i erarchical semi-structured objects are of the same type e.g XML documents of the same DTD schema Thus it is not a general-purpose subtree mining problem Other recent work related to frequent subtree mining include mining frequent graph patterns 7 8 11 9  S uch Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


graph mining algorithms are likely to be too general for tree mining as pointed out in 12 The one-di mens i onal traversal patterns for Web usage mining include 4 10  etc These one-dimensional traversal patterns do not capture well the document structure of the Web site The maximal frequent subtree mining problem proposed by us is different from others in that 1 the uncovered subtrees are induced subtrees of the unordered labeled trees in the database 2 the subtrees are maximal The trees are assumed to be unordered because we think when analyzing the user\220s browsing from a Web page e.g home page on a Web server it would be more teresting to know which pages the user follows from the starting page regardless of the order of the access The subtrees are induced subtrees as argued in 4 10 that s uch contiguity can help analyze the user\220s browsing behavior for Web usage mining The maximality of subtrees can reduce the number of meaningful patterns Notice that we do not intend to imply that other types of frequent subtrees are not or less important 3 Algorithm PathJoin 3.1 Outline We propose an ef\223cient algorithm PathJoin for mining the maximal frequent subtrees For easier presentation it is initially assumed that there are no duplicate labels in the tree Some extensions for handling duplicate labels are described in the last subsection The main idea of algorithm PathJoin is as follows 223rst all maximal frequent paths are found then the frequent subtrees are mined by joini ng the frequent paths These maximal frequent paths are special frequent subtrees or 1 itemsets and joining k maximal frequent paths results in subtrees with k leaf nodes or k itemsets After all frequent subtrees are found by joining the maximal frequent paths the frequent subtrees that are not maximal are pruned so that we have the set of all maximal frequent subtrees One of the features of the PathJoin algorithm is the use of a new compact data structure FST-Forest Frequent SubTree-Forest  to 223nd the maximal frequent paths FSTForest consists of compressed trees representing the trees in the database with infrequent 1-itemsets pruned The idea of compressing the database was inspired by the earlier work i n mi ni ng as s o ci at i o n rul es  I n t hi s p aper  t he compact structure is used to facilitate 223nding the maximal frequent paths and with additional features it is also used in algorithm PathJoin for mining frequent subtrees FSTForest reduces the overall space requirements signi\223cantly in most cases due to the overlap among trees The PathJoin algorithm is outlined in Algorithm 1 There are only two database scans In the 223rst scan the frequent size 1 subtrees with one node are found In the second scan the trees in the database are trimmed with only frequent nodes left and then merged into the compact structure FST-Forest which is done by function constructCompressedF orest  After compressing all trees in the database to FST-Forest the maximal frequent root paths are mined in each tree in FST-Forest and then the frequent root subtrees are mined by joining the frequent paths Finally the subtrees that not maximal are pruned Algorithm 1 PathJoin Input D  database of trees s min or S min  unweighted/weighted support threshold Output MFST  all maximal frequent subtrees Method  t database scan to 336nd frequent 1-itemsets 1 minsup   D 005 s min or minsup  001 T 002 D  T   005 S min 2 F 1   frequent size 1 subtrees    Second database scan to trim trees and create FST-Forest 3 Forest  constructCompressedF orest  D F 1   4 FST  006  5 for each tree T 001 F orest.trees do begin  Find the maximal frequent root paths in T 6 computeM F P  T,Forest,minsup    Find the frequent root subtrees in T 7 computeF ST   T.root   006 FST   8 endfor  Find the maximal frequent subtrees 9 MFST  maximize  Forest,FST   10 return MFST 3.2 Compressed Tree Construction The compact structure FST-Forest\(or Forest for short consists of compressed trees The construction of Forest is a three step process identify frequent subtrees with only one node trim the original trees in the database by removing the infrequent nodes create Forest by appropriately merging these trimmed trees Prior to explaining Forest creation process we examine the Forest data structure in more detail The compressed trees in Forest are indexed by the root label of each tree For each compressed tree there is a nodelist for each label which links together all nodes in the tree with the same label Notice that nodelist is called header table in 6  a nd header t a bl e l i nks al l nodes i n t h e entire forest while in our structure the nodelist is distributed Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


to each tree in Forest o reduce overall main memory requirement during subtree mining Figure 2 shows the FSTForest structure Each node in the forest is stored using the basic Node structure shown       Forest.trees nodelist Node freq Label  root  treeids children Tree parent Figure 2 FST-Forest Data Structure The treeids 223eld in the node structure is new and is required to reconstruct the original tree structure after compression For a node in Forest the treeids keeps the ids of all original trees before being compressed to Forest which have a root path ending at the node To reduce e main memory requirement the tree ids only need be saved on the leaf nodes of the original trimmed tree since the tree ids can be merged upward\(i.e to ancestors later in mining frequent subtrees To construct the Forest each tree string i.e the labels by an depth-\223rst traversal over the tree with 1 for each backward traversal as in 12  i n the databas e i s s canned to create a tree in main memory Then the infrequent nodes those not in F 1  are removed from the tree yielding the trimmed trees When creating the trimmed database a tree in the original database may become disconnected like a forest If so each connected subtree is treated as an independent tree trimmed tree but with the same tree id as the original tree For the example database and an unweighted support of 50 there are only two subtrees of size one which are small 8 and 9 When these are removed from the database we obtain t he trimmed database Figure 3 shows the original trees upper part and the trimmed trees lower part There are 223ve trimmed trees Two of them are the same as original trees T 1 and T 4 not shown Three are modi\223ed from the original ones T 2 a  T 2 b and T 3 a  The creation of the Forest is performed by either inserting if no such tree with the same root label exists in Forest or merging a tree with the same root label already exists in Forest these trimmed trees into Forest In the 223rst case the new nodes from e trimmed tree are inserted irectly to 3 2 1 4 7 8 5 1 4 39 2 1 6 7 6 2 7 4 6 5 3 T1 7 T2b 6 4 5 3 2 1 T2a 1 1 4 3 T3a T4T3 T2 Figure 3 Original Trees vs Trimmed Trees Forest with the tree structure kept the same and the nodelist updated to include the new nodes In the second case when a trimmed tree has common nodes with an existing tree in Forest the frequency for the common nodes is incremented and the new tree id is appended to the treeids if the node in the subtree is a leaf After merging all the trimmed trees the resulting Forest is shown in Figure 4 frequency is shown f  n  2 1 6 7 f=2 f=1 f=2,\(2,4 f=1,\(4 2 1 4 6 3 7 3 4 5   f=2 f=1,\(1 f=1 f=3 f=2 f=1,\(1 f=1,\(3 f=2,\(1,2 f=2,\(2 Figure 4 Compressed FST-Forest Since the ids of the trees in the database are numbered sequentially the treeids 223eld on each node will be ordered after Forest is constructed This automatic ordering is very useful in subtree expanding described in the following subsection The treeids on the nodes are shown in the parentheses in Figure 4 For each tree string in the database the time for constructing the corresponding tree in main memory is linear to the string length Determining connected subtrees with only frequent nodes can be accomplished by a breath-\223rst traversal of the tree which requires O  n  time Merging a subtree into Forest is like a depth-\223rst traversal which also requires O  n  time Overall constructing Forest with all trees in the database needs time linear to the total number of nodes in all trees The space requirement for the compact structure depends on the structures of the trees in the database In the worst Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


case when there are no common nodes among the trees i.e each node is unique there is no compression in Forest and the required memory is as large as the database Fortunately there are usually a lot of common nodes among the trees the common structures of the trees are what we are trying to discover and thus it results in a compact Forest as shown by the experiments Since e tree ids are stored only in the leaf nodes of the original tree may not be leaf nodes in Forest due to merging the memory for storing the tree ids is reduced especially for deep trees 3.3 Maximal Frequent Path Mining The constructed Forest contains all paths found in the database which could possibly be frequent However not every path ists as a root path in Forest r example the path  2  3  is found in Figure 4 but it is not a root path To facilitate the counting of all paths we expand Forest to ensure that all paths from the original database are now root paths For those non-root paths with the same starting label X as the root label of a tree in Forest we need to expand them to the tree Actually all such non root paths are linked by nodelist  X  in each tree Each node in the nodelist can be viewed as a subtree rooted at the node and the paths starting from e node non-root in the subtree should be merged with the root paths of the tree with root label X  The treeids and frequency of these subtrees can be directly merged to the tree because 1 the subtrees are disjoint i.e they occur in different orig inal trees in the database 2 for a tree rooted at X in Forest a subtree of X rooted at node Y  and a non-root path starting from Y to node Z  the frequency of the path Y 267\267\267 Z  in the tree rooted at X is the same as that of the root path X 267\267\267 Y 267\267\267 Z   Since tree s are automatically ordered during Forest construction the merging of two sets of tree s can be done in time linear to the sum of the length of the two sets After expanding the subtree with root label 1 in the second tree the 223rst tree with root label 1 is shown in Figure 5a Other trees in the Forest are not shown since they are not relevant while mining the tree  2 1 4 3 5 f=2 f=2,\(1,2 f=4 7 3 4 f=2,\(2 f=1,\(1 f=1 f=1,\(3 f=3,\(4 6 f=1,\(1 4 2 1 3 5 f=4,\(1,2,3,4 f=3,\(1,2,4 f=2,\(1,2 f=2,\(1,2 f=2,\(1,2 a b Figure 5 Subtree Expansion and MFP As a result of expanding non-root paths subtrees from other trees the tree that is being mined in Forest has all paths in e original database which start with the root node Then the maximal frequent root paths can be found by a depth-\223rst traversal of the tree During the traversal for mining the maximal frequent paths the infrequent nodes and their descendents are all removed from the tree after their treeids are merged to the closest ancestor nodes that are frequent Such removal of infrequent nodes is valid because of the downward closure property of subtrees That is ll subtrees of frequent trees are frequent or equivalently all supertrees of an infrequent tree are infrequent At the end f such depth-\223rst traversal of the tree each leaf node left in the tree corresponds to a maximal frequent path To facilitate the subsequent frequent subtree mining the treeids are merged upward from descendents to ancestors by a post-order traversal of the tree The resulting tree after computing the maximal frequent paths in the tree with root label 1 is shown in Figure 5b The root paths in the tree with root label 1 are all frequent The paths terminating at the leaf nodes are maximal The time complexity for computing the maximal frequent paths in a tree is O  mn  ln  where m and l are the total number of nodes in the subtrees that are expanded and the number of nodes in the tree that is being mined respectively and n is the maximum number of tree ids on the nodes in the tree after expanding 3.4 Frequent SubTree Mining After expanding all subtrees with the same root label all frequent root subtrees can be mined from the tree as shown by Theorem 1 Theorem 1 Given a ree T with all frequent root paths let the label of the root node be R  All frequent subtrees with root label R are root subtrees of T  Proof  Straightforward using the downward closure property f frequent subtrees The itemset representation for root subtrees is used in the following description i.e each root subtree S of tree T is represented by the representative nodes in T of the leaf nodes of S  A root subtree with k leaf nodes\(or k root paths is a k itemset and each root path is a 1 itemset The main idea is to construct candidate k itemsets by joining k 1 itemsets For example in Figure 5b by joining two 1 itemsets n 3 and n 5  we have a candidate 2 itemset  n 3 n 5   The frequency of a candidate k itemset is the number of common tree s on the k 1 itemsets The t of common tree s f k 1 itemsets is the intersection of the ree ids of he k 1 itemset Such k way intersection of the tree ids can be done in linear time since the treeids are ordered on each node Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


To reduce the number of candidate itemsets the downward closure property is u sed to generate candidate k itemsets from frequent  k 212 1 itemsets that is a candidate k itemset is generated only if all its subsets   k 212 1 itemsets are frequent The candidate generation is done by function FST PathJoin  Function FST PathJoin is similar to function apriori gen in 1 F o r t w o item sets  r 1 r 2  267\267\267 r k 212 1  and  s 1 s 2  267\267\267 s k 212 1   a candidate k itemset  r 1 r 2  267\267\267 r k 212 2 r k 212 1 s k 212 1  is generated from them if r i  s i 1 002 i 002 k 212 2 and all subsets of it are frequent Notice that function FST PathJoin is applied to the child nodes of a node in the tree recursively thus it is localized while apriori gen is applied to all frequent itemsets Function 1 computeFST isetRecur isetF ixed s F ST  isetRecur  itemset called recursively on its child nodes isetF ixed  itemset 336xed during recursion s  minimum support count FST  the resulting set of frequent subtrees output  the set of frequent child nodes of isetRecur  1 itemset  isetRecur 007 isetF ixed   Frequency of an itemset is the number of common tree ids 2 itemset.treeids  b X 002 itemset X.treeids  3 if  itemset.treeids  s then 4 return 006  5 FST  FST 007 itemset    Find the frequent child nodes in isetRecur 6 FST 1  006  7 for each child C of X 001 isetRecur do begin 8 fx  itemset 212 X   9 re   C   10 RES  computeF ST  re fx s F ST   11 if RES t  006 then  12 FST 1  FST 1 007 C   13 endfor  Generate candidates with two frequent child nodes 14 CFT 2  FST PathJoin  FST 1   15 for  k 2 CFT k t  006  k  do begin 16 FST k  006  17 for each X 001 CFT k do begin 18 fx  itemset 212 Y.parent  Y 001 X   19 RES  computeF ST  X,fx,s,FST   20 if RES t  006 then  21 FST k  FST k 007 X   22 endfor  Generate candidates with k 1 frequent child nodes 23 CFT k 1  FST PathJoin  FST k   24 endfor 25 return FST 1  The time complexity of the function depends on the number of frequent subtrees For each candidate k itemset its frequency checking is done in time O  kn  where n is the maximum number of tree s n the nodes The cost of function FST PathJoin is kept minimal since it is applied to the child nodes of a node in the tree only that is it is localized to a node Such localization reduces the number of candidate subtrees substantially as shown in the experiments There is no extra memory requirement for the frequency checking of a candidate itemset besides 1 n for e k way intersection 3.5 Maximizing There are two steps in 223nding the maximal frequent subtrees from the set of frequent subtrees 1 local maximization in the tree and 2 global maximization in Forest The local maximization is done on each tree for the frequent subtrees with the same root label Global maximization 223lters those that are subtrees non-root of another frequent subtree The two types of maximizations are relatively straightforward thus the details are omitted After local maximization we have maximal frequent subtrees with the same root label Such maximal frequent subtrees within a tree in Forest could be interesting themselves After global maximiza tion we have the set of all maximal frequent subtrees For Web usage mining these maximal frequent subtrees provide a global view of the entire Web site 3.6 Handling Duplicate Labels So far we have assumed that there are no duplicate labels in the trees of the database The PathJoin algorithm can be modi\223ed a little bit to support duplicate labels Recall that when compressing a tree in the database into Forest we 223rst get the connected subtrees of the tree with only frequent nodes and each connected subtree has the same tree id as the original tree With duplicate labels each connected subtree with the same root label is assigned a pair of tree ids one is the original tree id and the other is a new tree id The pair of tree ids are added to all nodes of each connected subtree with the same root label Notice that for these connected subtrees with unique root labels only the original tree id is necessary or the new tree id is the same as the original tree id W hen merging a connected subtree into Forest the pair of tree ids are also merged For the nonroot nodes with duplicate label s in a connected subtree a pair of tree ids original plus new are also added the nodes and all their descendents These pairs of tree ids are merged when expanding subtrees in function computeM F P  The new tree ids are needed to prevent invalid subtree generation For the trees in Figure 3 if T 2 is replaced by Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


T 2 001 in Figure 6 e two connected subtrees have the same root label 2  In Figure 6 the pair of tree ids are shown as x  y in parentheses on each node where x and y are e original and new tree ids r espectively After merging the root node has 3 children Without the new tree ids it could generate an invalid subtree e.g 2 1 1 3 1 since children with labels 1 and 3 do not appear to be the children of the node with label 2 at the same time By checking the new tree ids such invalid subtrees are avoided since the two children have different new tree ids  1 6 T2\220 8 2 3 4 5 2 5 10 Subtree: S2 15 2 1:3 1:3 1:3 1 6 After Merging 1:2,1:3 1:2,1:3 1:2 1:2 1:3 5 2 3 1:2 4 Subtree: S1 2 3 4 5 6 1:2 1:2 1:2 1:2 1:2 Original Tree Trimed Trees After Merging Figure 6 Handling Duplicate Labels The frequency counting is changed as follows for weighted support the frequency at each node is the number of new tree ids and for unweighted support the frequency is the number of original tree ids 4 Experimental Results The performance of PathJoin was examined through a series of simulation experiments All experiments were conducted on a Sun Blade 1000 with 1GB main memory and running Sun OS 5.8 The algorithm was implemented in C using Standard Template Library Three synthetic data sets D10 F5 and T1M were tested These data sets were generated using the method in 12 The synthetic data generation mimicks the Web site browsing behavior of the user The parameters used in the data generation include the number of labels N  100  the number of nodes in the master tree M  10000  the maximum fanout of a node in the master tree F 10  F 5 for data set F5 and the maximum depth of the master tree D 10  and the total number of trees in the data set T  100000  T  1000000 for T1M Two variations of the PathJoin algorithm were compared to examine the effect of pruning in candidate subtree generation one uses pruning in candidate subtree generation i.e function FST PathJoin checks whether the subsets of a candidate itemset are frequent and the other does not We did not compare our algorithm to others because of the difference of the problem de\223ned by us from others 4.1 Execution Time and Number of Candidate Subtrees The execution time for the three data sets with varying minimum support is own in Figures 7 and 8a The ecution time increases as the minimum support decreases since there are more frequent subtrees with smaller minimum support It can be also seen that the two variations of the algorithm with or without pruning in candidate subtree generation have no big difference in execution time This is because the pruning does not prune away many candidate subtrees less than 5 as shown in Figure 8b Thus the overhead for checking e subsets of the candidate subtrees offsets the saved e for frequency counting of the pruned candidate subtrees Similar results were obtained for the other two data sets which are not shown due to space limit The reason that the pruning is not very effective is that e candidate subtree generation in function computeF ST is limited to the children of a node in a tree in Forest i.e it is localized Such localizatio n reduces the number of candidate subtrees compared to apriori gen in 1 which i s applied to all the frequent itemsets found in a pass 2.2 2.4 2.6 2.8 3 3.2 3.4 3.6 3.8 0.05 0.01 0.005 0.001 0.0005 Execution time\(seconds Support threshold PathJoin-NoPruning  PathJoin-Pruning  a set F5 0 5 10 15 20 25 30 0.05 0.01 0.005 0.001 0.0005 Execution time\(seconds Support threshold PathJoin-NoPruning  PathJoin-Pruning  b set D10 Figure 7 Execution Time 20 40 60 80 100 120 140 0.05 0.01 0.005 0.001 0.0005 Execution time\(seconds Support threshold PathJoin-NoPruning  PathJoin-Pruning  a\ecution Time 0 2000 4000 6000 8000 10000 12000 14000 0.05 0.01 0.005 0.001 0.0005 Candidate subtrees Support threshold\(unweighted PathJoin-NoPruning  PathJoin-Pruning  b of Candidate Subtrees Figure 8 Data set T1M Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


4.2 Memory Usage and Scaleup The memory usage for the compact data structure FSTForest has two parts one for the compressed tree structure the other for the tree ids The memory for the compressed tree structure is 223xed for a data set given some minimum support threshold while the memory for the tree ids will grow as the number of trees in the data set increases In the experiments the data set s with different number of trees were generated with the same parameters as for data set T1M The minimum support was 223xed to 0.5 Figure 9a shows the memory usage at two stages of the algorithm the lower curve shows the the memory before expanding all subtrees with the same label and the upper curve after computing the maximal frequent paths and merging the treeids upward It can be seen that the memory usage is about doubled after expansion and merging Figure 9 also shows that both the memory usage and the execution time scales linearly with respect to the change of the number of trees in thedataset  0 20 40 60 80 100 120 140 160 180 0 500 1000 1500 2000 2500 3000 3500 4000 Memory usage\(MBs Number of trees\(in 1000\220s FST-Forest\(before expanding  FST-Forest\(after MFP  a Memory usage 0 20 40 60 80 100 120 140 0 500 1000 1500 2000 2500 3000 3500 4000 Execution time\(seconds Number of trees\(in 1000\220s PathJoin-Pruning b Scaleup Figure 9 Memory Usage and Scaleup 5 Conclusion A new type of tree mining maximal induced subtrees in unordered trees is de\223ned in the paper A novel algorithm PathJoin is proposed to discover all maximal frequent subtrees given some minimum support threshold The algorithm uses a compact data structure FST-Forest to compress the trees in the database and at the same time still keeps the original tree structure A localized candidate subtree generation method is used in the algorithm which reduces the number of candidate subtrees substantially The algorithm is evaluated with synthetic data sets The future work includes 1 earlier identi\223cation of maximal frequent subtrees which could potentially save a lot of e for computing the non-maximal frequent subtrees 2 extension of FST-Forest for mining maximal frequent embedded subtrees which allow ancestor/descendent relationship Acknowledgement We would like to thank Prof Mohammed J Zaki for sending us the source code for e tree generation program References  R  A gra w al and R  S r i kant  F ast al gori t h ms for m i n i n g a ssociation rules in large databases In Proceedings of the Twentieth International Conference on Very Large Databases  pages 487\205499 Santiago Chile 1994  R  A gra w al and R  S r i kant  M i n i n g s equent i a l p at t e rns In Proceedings of the 11th International Conference on Data Engineering  Taipei Taiwan Mar 1995 IEEE Computer Society Press  T  A sai K Abe S  Ka w a so e H Arimura H Satamoto and S Arikawa Ef\223ciently substructure discovery from large semi-structured data In Proceedings of the 2nd SIAM Int\325l Conference on Data Mining  april 2002 4 M  S  C h en J S  Par k  and P  S  Y u  E f 223 ci ent dat a m i n i n g for path traversal patterns IEEE Transactions on Knowledge and Data Engineering  10\(2\:209\205221 1998 5 G  C ong L  Y i  B  L i u  a nd K W a ng Di sco v e r i ng f r e quent substructures from hierarchical semi-structured data In Proceedings of the 2nd SIAM Int\325l Conference on Data Mining  Arlington VA april 2002 6 J  H an J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In Proceedings of the M SIGMOD Conference  2000 7 A  I nokuchi  T  W ashi o and H Mot oda An apr i or i based al gorithm for mining frequent substructures from graph data In Proceedings of the 4th European Conference on Principles of Knowledge Discovery and Data Mining  sep 2000 8 M  K ur amochi and G Kar ypi s F r equent subgr aph di sco v ery In Proceedings of the 1st IEEE Int\325l Conference on Data Mining  nov 2001 9 D  S hasha J W a ng and R  Gi ugno Al gor i t h ms and appl i cations of tree and graph searching In Proceedings of the 21st M SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems  pages 39\20552 Madison Wisconsin june 2002  Y  Xi ao and M H Dunham E f 223 c i e nt mi ni ng of t r a v er sal patterns Data and Knowledge Engineering  39:191\205214 2001  X Y a n a nd J Han gspan Gr aphbased subst r uct ur e pat tern mining In Proceedings of the 2002 IEEE International Conference on Data Mining ICDM 2002 9-12 December 2002 Maebashi City Japan  pages 721\205724 IEEE Computer Society 2002  M J Z a ki  E f 223 ci ent l y mi ni ng f r e quent t r ees i n a f or est In Proceedings of the 8th M SIGKDD Int\325l Conference on Knowledge Discovery and Data Mining  Edmonton Canada jul 2002 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


association-cube, base-cube and population-cube are derived from the volume cube; the confidence-cube is derived from the association cube and population cube and the support-cube is derived from the associationcube and base-cube. The slices of these cubes shown in Figure 2 correspond to the same list of values in dimension merchant, time, area and customer_group  Multidimensional and multilevel rules Representing association rules by cubes and underlying cubes by hierarchical dimensions, naturally supports multidimensional and multilevel rules. Also these rules are well organized and can be easily queried  First, the cells of an association cube with different dimension values are related to association rule instances in different scopes. In the association cube CrossSales cell CrossSales product \221A\222, product2 \221B\222  customer_group 221engineer\222, merchant \221Sears\222, area \221Los Angeles\222, time 221Jan98\222 represents the following multidimensional rule x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x,\221B\222  275 customer_group = \221engineer\222, merchant = \221Sears\222, area 221Los  Angeles\222, time =  \221Jan98\222 If this cell has value 4500, and the corresponding cell in the population cube has value 10000, then this rule has confidence 0.45 Next as the cubes representing rules can have hierarchical dimensions, they represent not only multidimensional but also multi-level association rules. For example, the following cells CrossSales\(product \221A\222, product2 \221B\222, customer_group 221engineer\222, merchant \221Sears\222, area \221 California 222, time 221Jan98\222 CrossSales\(product \221A\222, product2 \221B\222, customer_group 221engineer\222, merchant \221Sears\222, area \221 California 222, time 221 Year98 222 represent association rules at different area levels \(i.e the city level and the state level\d different time levels \(i.e., the month level, the year level x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222  275 customer_group = \221engineer\222, merchant =  \221Sears\222, area 221 California 222, time =  \221Jan98\222 x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222  275 customer_group = \221engineer\222, merchant =  \221Sears\222, area 221 California 222, time =  \221 Year98 222 The cell CrossSales\(product \221A\222, product2 \221B\222,  customer_group 221top\222, merchant \221top\222, area \221top\222,  time \221top\222 represents the customer-based cross-sale association rule for all customers, merchants, areas, and times in the given range of these dimensions, expressed as x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222 4.3  Generating Association Rule Related Cubes The basic task of our OLAP based association rule mining framework, either at the GDOS or at an LDOS is to convert a volume cube i.e. the cube representing the purchase volumes of customers dimensioned by product  area etc, into an association cube a base cube and a population cube These cubes are then used to derive the confidence cube and the support cube of multidimensional association rule instances. The following general steps are involved in cross-sale association rule mining 267  Roll up the volume cube SaleUnits by aggregating it along merchant, time, area dimensions 267  Derive cube NumOfBuyers from SaleUnits based on the antecedent condition SaleUnits 0 267  Populate cube NumOfShoppers by the counts of customers dimensioned by merchant, area  time not by product\at satisfy the antecedent conditions 267  Derive cube CrossSales from SaleUnits based on the association conditions SaleUnits  product  p 1  0 and SaleUnits  product2  p 2 0 267  Derive cube Confidence and cube Support using cell-wise operations 214  Confidence = CrossSales  NumOfBuyers 214  Support  CrossSales  NumOfShoppers  Cubes Confidence  Support  CrossSales are dimensioned by product  product2 customer_group,merchant  time, area NumOfBuyers is dimensioned by product  customer_group, merchant time, area  NumOfShoppers is dimensioned by customer_group, merchant  time, area Rules with confidence and support that exceed specified thresholds  may be considered interesting 4.4. Rules with Conjoint Items Cubes with conjoint dimensions can be used to represent refined multidimensional association rules For example, using OLAP, we can derive association rules across time  Time-variant or temporal association rules such as 


x 316 Customers buy_product\(x,\222 A\222, \221 Jan98\222  336 buy _product\(x, \221B\222, \221 Feb98\222   275 area = \221Los Angeles\222 can be used to answer such questions as \223 How are  the sales of B in Feb98  associated with the sales of A in Jan98 224 The items in this rule are value pairs of dimensions product and time In order to specify this kind of association rule we introduce a conjoint dimension product, time and mirror it with dimension product2, time2 This allows a cell in the association cube to cross two time values. Accordingly, the cubes related to association rule mining are defined as follows Association cube  CrossSales.2 \(<product, time>, <product2, time2 customer_group, merchant, area  Population cube  NumOfBuyers.2  \(<product, time>, customer_group merchant, area Base cube  NumOfShoppers.2  \( customer_group, merchant, area Confidence cube Confidence.2 \(<product, time>, <product2, time2 customer_group, merchant, area Support  cube  Support.2  product, time>, <product2, time2 customer_group, merchant, area  The steps for generating these cubes are similar to the ones described before. The major differences are that a cell is dimensioned by, besides others product, time and product2, time2 and the template of the association condition is  SaleUnit s  product p 1 time t 1  0 and  SaleUnits  product2 p 2 time2 t 2  0 where, in any instance of this condition, the time expressed by the value of time2 is not contained in the time expressed by the value of time The template of the antecedent condition is SaleUnits   product p 1 time t 1  0 In general, other dimensions such as area may be added to the conjoint dimensions to specify more refined rules 4.5. Functional Association Rules A multidimensional association rule is functional if its predicates include variables, and the variables in the consequent are functions of those in the antecedent.  For example, functional association rules can be used to answer the following questions, where a_month and a_year are variables q  What is the percentage of people in California who buy a printer in the next month after they bought a PC x 316 Customer buy_product\(x, \221PC\222, a_ month 336 buy_product\(x, \221printer\222, a_month+1  275 area = \221California\222 q  What is the percentage of people who buy a printer within the year when they bought a PC  x 316 Customer: buy_product\(x, \221PC\222, a_ year 336 buy_product\(x, \221printer\222, a_year 275 area = \221California\222 To be distinct, we call the association rules that are not functional as instance association rules; e.g x 316 Customer: buy_product\(x,\222 PC\222, \221Jan98\222 336 buy_product\(x,\222 printer\222, \221Feb98\222  275 area =  \221California\222 Time variant, functional association rules can be derived from time variant, instance association rules through cube restructuring. Let us introduce a new dimension time_delta that has values one_day, two_day 205, at the day level, and values one_month, two_month, \205, at the month level, etc. Then, let us consider the following functional association rule related cubes Association cube  CrossSales.3 \(product, product2, customer_group merchant, area, time_delta  Population cube  NumOfBuyers.3 \(product, customer_group, merchant area Base cube  NumOfShoppers.3 \( customer_group, merchant, area Confidence cube  Confidence.3 \(product, product2, customer_group merchant, area, time_delta Support cube  Support.3 \(product, product2, customer_group, merchant area, time_delta The association cube CrossSales.3  can be constructed from CrossSales.2   The cell values of CrossSales.2  in the selected time and time2 ranges are added to the corresponding cells of CrossSales.3 For example, the count value in cell  CrossSales.2\(<PC, Jan98>, <printer, Feb98>\205 is added to cell \(bin CrossSales.3\(PC, printer, one_month,\205 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


