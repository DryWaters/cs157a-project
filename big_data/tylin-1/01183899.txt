Progressive Modeling Wei Fan\222 Haixun Wang\222 Philip S Yu\222 Shaw-hwa Lo2 Salvatore Stolfo3 221IBM T.J.Watson Research Hawthome NY 10532 222Dept of Statistics. Columbia Univ New York NY 10027 3Dept of Computer Science. Columbia Univ New York NY 10027 weifan,haixun,psyu}~s.ibm.com SloQStatS.columbia.edu salocs.columbia.edu Abstract Presenrly inducrive learning is srillperformed in LI frns rraring barch process The user has lirrle inreracrion wirh 
rhe sysrem and no control over rhefrnal accuracy and rrain ing rime lfrhe accuracy of rhe produced model is roo low oil the compuring resources are misspent In rhis paper we propose a progressive modeling framework In progressive modeling rhe learning olgorithm esrimares online borh rhe accuracy of rhefinnl model and remaining training rime lf the esrimred accurocy is far below expecrarion the user can rerminare rraining prior ro complerion 
wirhour wosring furrher resources If rhe user chooses ro complere rhe learn ing process progressive modeling will compure a model wirh expecred accuracy in expecred rime We describe one implemenrarion of progressive modeling using ensemble of classifiers Keywords estimation 1 Introduction Classification is one of the most popular and widely used data mining methods to extract useful information from data bases ISO/IEC is proposing an international standard to be finalized in August 2002 to 
include four data mining types into database systems; these include association rules, clus tering regression and classification Presently classifica tion is performed in a 223capricious\224 batch mode even for many well-known commercial data mining software An inductive learner is applied to the data; before the model is completely computed and tested the accuracy of the final model is not known Yet for many inductive learning algo rithms the actual training time is not known prior to learn ing either It depends on not only the size of the data and the number 
of features but also the combination of feature val ues that utimately determines the complexity of the model During this possibly long waiting period the only interac tion between the user and program is to make sure that the program is still running and observe some status reports If the final accuracy is tca low after some long training time Figure 1 An interactive scenario where both accuracy and remaining training time are es timated all the computing resources become futile The users ei ther have to repeat 
the same process using other parame ters of the same algorithm, choose a different feature sub set select a complelely new algorithm or give up There are many learners to choose from a lot of parameters to select for each learner countless ways to construct features, and exponential ways for feature selection The unpredictable accuracy long and hard-to-predict training time and end less ways to run an experiment make data mining frustrating even for experts 1.1 Example of Progressive Modeling In this paper we propose 
a \223progressive modeling\224 con cept to address the problems of batch mode learning We illustrate the basic ideas through a cost-sensitive example even though the concept is applicable to both cost-sensitive and traditional accuracy-based problems We use a charity donation dataset KDDCup 1998 that chooses a subset of population to send campaign letters The cost of a campaign letter is 0.68 It is only beneficial to send a letter if the solicited person will donate at least 0.68 As soon 
as learning starts the framework begins to compute intermediate models and report current accuracy as well as estimated final accuracy on a hold-out valida tion set and estimated remaining training time For cost sensitive problem accuracy is measured in benefits such as dollar amounts We use the term accuracy to mean tra ditional accuracy and benefits interchangeably where the meaning is clear from the context Figure 1 shows a snap 0-7695-1754-4/02 17.00 Q 2002 IEEE 163 


shot of the new learning prucess It displays that the ac curacy on the hold out validation set total donated charity minus the cost of mailing to both donors and non-donors for the algorithm using the current intermediate model is I 2840.5 The accuracy of the complete model on the hold out validation set when learning completes is estimated to he 14289.5f100.3 with at least 99.7 confidence The additional training time to generate the complete model is estimatedto be 5.40f0.70 minutes with at least 99.7 con fidence This information continuously refreshes whenever a new intermediate model is produced, until the user explic itly terminates the learning or the complete model is gener ated The user may stop the learning process mainly due to the following reasons  i intermediate model has enough accuracy, ii its accuracy is not significantly different from that of the complete model, iii\the estimated accuracy of the complete model is too low or iv the training time is unexpectedly long For the example shown in Figure 1 we would continue, since it is worthwhile to spend 6 more min utes to receive at least 1400 more donation with at least 99.7 confidence In this example we illustrated progres sive modeling applied to cost-sensitive learning For cost insensitive learning the algorithm reports traditional accu racy in place of dollar amounts Progressive modeling is significantly mbre useful than a batch mode learning process especially for very large dataset The user can easily experiment with different algo rithms parameters and feature selections without waiting for a long time for a failure result 2 Our Approach We propose an implementation of progressive modeling based on ensembles of classifiers that can be applied to sev eral inductive leaming algorithms The basic idea is to gen erate a small number of base classifiers to estimate the per formance of the entire ensemble when all base classifiers are produced 2.1 Main Algorithm Assume that a training set S is partitioned into K disjoint subsets Sj with equal size When the distribution of the dataset is uniform each subset can be taken sequentially Otherwise we can either completely 223shuffle\224 the dataset or use random sampling without replacement to draw Sj A base level model Cj is trained from Si Given an exam ple z from a validation set S it can he a different dataset or the training set Cj outputs probabilities for all possible class labels that z may be an instance of i.e pj\(tilz for class label e Details on how to calculate pj\(e,lz can be found in S].ln addition we have a benefit matrix b[ei,Ej that records the benefit received by predicting an example of class e to he an instance of class ej For cost-insensitive or accuracy-based problems Vi b[ti,l  1 and Va  j b tj  0 Since traditional accuracy-based decision making is a special case of cost-sensitive problem. we only discuss the algorithm in the context of cost-sensitive deci sion making Using benefit matrix b  I each model Ci will generate an expected benefit or risk ej\(eilz for every possible class ti ExpectedBenefit ej\(Eilz  b[ei.,ei pj\(ei81z I 1 Assume that we have trained k 5 K models Cl  C Combining individual expected benefits ae have E e,V8Id k Average Expected Benefit Eh I  2 We then use optimal Jecision policy to choose the class la bel with the maimal expected benefit Optimal Decision Lk\(z  argmax I r 3 Asruming lh3i f\(r is the true labcl of I the accuracy of the ensemble uith k classifiers is For accuracy-hasedproblems Ah is usually normalized into percentage using the size of the validation set IS,I Forcost sensitive problems it is customary to use some units to mea sure benefits such as dollar amounts Besides accuracy we also have the total time to train CI to Ck Tk  the total time to train{C    Ck 5 Next based on the performance of k 5 K base classi fiers we use statistical techniques to estimate both the ac curacy and training time of the ensemble with K models We first summarize some notations AK TK and MK are the true values to estimate Respectively. they are the accuracy of the complete ensemble the training time of the complete ensemble and the remaining training time after k classifiers Their estimates are denoted in lower case i.e a tK and mK An estimate is a range with a mean and standard deviation The mean of a symbol is represented by a bar 0 and the standard deviation is represented by a sigma a Additionally ad is standard error or the standard deviation of a sample mean 2.2 Estimating Accuracy The accuracy estimate is based on the probability that ti is the predicted label by the ensemble of K classifiers for 164 


example z P{Lx\(z  ti the probability that ti is the prediction by the ensemble of size K 6 Since each class label ti has a probability In be the predicted class and predicting an instance of class x as ti receives a benefit b[e\(x Pi the expected accuracy received for x by predicting with K base models is a  Cb[e\(x P{LK\(x  e 7 1 with standard deviation of o\(a\(x To calculate the ex pected accuracy on the validation set S we sum up the expected accuracy on each example x aK  a\(x 8 ZES Since each example is independent, according to multino mial form of central limit theorem CLT the total benefit of the complete model with K models is a normal distribu tion with mean value of  and standard deviation of Using confidence intervals, the accuracy of the complete en semble AK falls within the following range Withconfidencep,AK E ti f t o\(a IO When t  3 the confidencep is approximately 99.7 Next we discuss how to derive P{LK\(x  ti If EK\(t,lx are known, there is only one label LK\(z whose P{L,\(x  ti will he 1 and all other labels will have probability equal to 0 However EK\(P,Ix are not known wecannnly useitsestimateEt\(P,lx measuredfrom kclas sifiers to derive P{LK\(x  ti From random sampling theory 21 Et\(e,lz is an unbiased estimate of EK\(LiIx with standard error of According to central limit thereon, the true value E~\(ti1z falls within a normal distribution with mean value of p  Et\(&lx and standard deviation of U  ud\(Et\(ti1x If Et\(eilx is high it is more likely for Ex\(Pilz to he high and consequently, for P{I+\(x  e to be high For the time being we ignore correlation among different class la bels and compute naive probability P'{LK\(z   As suming that rt is an approximate of maxt area in the rangeof r w is the probability P'{LK\(z  ti whereo  od\(Et\(&lx andp  Et\(eilx When k 5 30 to compensate the error in standard error estimation we use Student-r distribution with df  k We use the average of thetwolargestEt\(tiIs EK lz The reason not to use the maximum itself is that if the asso ciated label is not the predicted label of the complete model the probability estimate for the true predicted label may he too low On the other hand P{Lk\(x  ti is inversely related to the probabilities for other class labels to be the predicted label. When it is more likely for other class labels to be the predicted label it will he less likely for to be the predicted label A common method to take correlation into account is to use normalization i Thus we have derived P{Lx\(z  ti in order to esti mate the accuracy in Eq[7 Estimating Training Time Assuming that the training time for the sampled k models are TI to rt Their mean and stan dard deviation are f and U Then the total training time of K classifiers is estimated as With confidencep TK E t f t  U\(tK where CK  K f and u\(t   14 To find out remaining training time MK we simply deduct k.ffromEq[14 With confidencep MK E lTl f t u\(mK where t  K  U\(T A rfl CK  k.f and u\(mK  o\(tK 15 2.3 Progressive Modeling We request the first random sample from the database and train the first model Then it requests the second ran dom sample and train the second model. From this point on the user will be updated with estimated accuracy remaining training time and confidence levels We have the accuracy of the current model At and the estimated accuracy of the complete model aK as well as estimated remaining train ing time mK From these statistics the user decides to continue or terminate The user normally terminates learn ing if one of the following Sropping Criteria are met 165 


   Data  benefit matrix b[&'j,tj training set S vali Result  k 5 K classifiers dation set S and K begio partition S into K disjoint subsets of equal size SI  SKh train CI from SI and TI is the training time k t 2 wbile k<K do train ck from sk and rk is the training time for z E S do calculate P{LK  Eq[131 calculate d\(z and its standard deviation Mz EqI71 end estimate accuracy OK EqlSI and Eq19l and remaining training time rnx F.qI151 if aK and mK sarisfi sropping crireria then end ktk+1 end return Cl    C return c      c end Igorithm 1 Progressive Modeling Based on Averag ing Ensemble actual 7 fraud 490 rn The accuracy of the current model is sufficiently high Assume that SA is the target accuracy The accuracy of the current model is sufficiently close to that of the complete model There won't be signif icant improvement by training the model to the end Formally t  u\(a 5 e The estimated accuracy of the final model is tm low to be useful Formally if ii  t  u\(a  BA stop the learning process The estimated training time is too long, the user de cides to abort Formally assume that OT is the target training time if m  t  u\(~K  OT cancel the leaming As a summary of all the important steps of progressive modeling, the complete algorithm is outlined in Algorithm 1 2.4 Efficiency Computing K base models sequentially has complexity of K O\(f Both the average and standard deviation can be incrementally updated linearly in the number of ex amples 0 166 


vious months as training data 406009 examples Details about this dataset can be found in 9 The third dataset is the adult dataset from UCI repository It is a widely used dataset to compare different algorithms on traditional accuracy for cost-sensitive studies we anifi cially associate a benefit of 2 to class label F and a benefit of 1 to class label N as summarized below actual N We use the natural split of training and test sets so the results can be easily duplicated The training set contains 32561 entries and the test set contains 16281 records 3.2 Experimental Setup We have selected three learning algorithms, decision tree learner C4.5 rule builder RIPPER and naive Bayes learner We have chosen a wide range of partitions K E S 16,32,64,128,256 The accuracy and estimated accuracy is the test dataset 3.3 Accuracy Since we study the capability of the new framework for both traditional accuracy-based problems as well as cost sensitive problems each dataset is treated both as a tradi tional and cost-sensitive problem The baseline traditional accuracy and total benefits of the batch mode single model are shown in the two columns under accuracy for traditional accuracy-based problem and benefits for cost-sensitive prob lem respectively in Table I These results are the baseline that the multiple model should achieve For the multiple model we first discuss the results when the complete multiple model is fully constructed then present the results of partial multiple model Each result is the av erage of different multiple models with K ranging from 2 to 256 In Table 2 the results are shown in two columns under accuracy and benefit As we compare the respective results in Tables I and 2 the multiple model consistently and significantly beat the accuracy of the single model for all three datasets using all three different inductive learn ers The most significant increase in both accuracy and total benefits is for the credit card dataset The total benefits have been increased by approximately 7.000  1 0,oOO the ac curacy has been increased by approximately I   3 For the KDDCUP'YX donation dataset. the total benefit has been increased by 1400 for C4.5 and 250 for NB Please nole hat we erpenmenled wilh different parameters for RIP PER on he donation dataset However he most specific rule produced by RlPPER contains only one rule ha covers 6 donors and one default rule hat always predict donate This succinct rule will not find any donor and will not receive any donations However RIPPER performs reason ably well far the credil card and adult darnels accuracy benefit Donation 9494 13292.7 Credit Card 87.77 1733980 Adult 84.38 Credit Card 90.14 1712541 I I Accuracy Based  Corl-sensitive accuracy 11 be"Cfi1 Donation II 94.94 II Credit Card 90.14 1712541 Addl 84.84 19725 Credit Card 85.46 704285 Table 1 Baseline accuracy and total benefits We next study the trends of accuracy when the number of partitions K increases In Figure 2 we plot the accuracy and total benefits for the credit card datasets, and the total benefits for the donation dataset with increasing number of partitions K C4.5 was the base learner for this study As we can see clearly that for the credit card dataset, the mul tiple model consistently and significantly improve both the accuracy and total benefits over the single model by at least 1  in accuracy and 4oooO in total benefits for all choices of K For the donation dataset, the multiple model boosts the total benefits by at least 1400 Nonetheless, when K increases both the accuracy and total tendency show a slow decreasing trend It would be expected that when K is ex tremely large, the results will eventually fall below the base line 3.4 Accuracy Estimation The current and estimated final accuracy are continu ously updated and reported to the user The user can termi nate the leaming based on these statistics As a summary these include the accuracy of the current model Ar the true accuracy of the complete model AK and the estimate of the true accuracy ii with U\(QK If the true value falls within the error range of the estimate with high confidence and the error range is small, the estimate is good Formally with confidencep AK E ii f t U\(QK Quantitatively, we say an estimate is good if the error bound t  U is within 5 of the mean and the confidence is at least 99 We chose k  20 K In Table 3 we show the average of estimated accuracy of multiple models with different number of part tions K     256 The true value AK all fall within 167 


Figure 2 Plots of accuracy and total benefits for credit card datasets and plot of total benefits for donation dataset with respect to K Dif.TaBn  Olbl-Bh chmc.rdobr~l  _33 Iym uLIcMD.ym.w  cmC*am  ow cm-ma Credit Adult Donation 90.37 90.08%+l.5 804964 9799876f3212 1 85.6 85.3811.48 16435 16255f142 RIPPER Accuracy Based 11 Cost-sensitive aCC"rnCY benefit Donation I1 94.9410 I1 OM Credit Card Adult 91 46f0 6 5815612f34730 86 If0 4 19875+390 Donation 94.94+0 514282f530 Credit card Adult Table 2 Average accuracy and total bene fits by complete multiple model with different number of partitions 91.46 91.24%+0.9 815612 82WIZ+3742 I 86.1 85.9%+1.3 19875 119668f258 the error range To see how quickly the error range converges with in creasing sample size we draw the entire process to sample up to K  256 for all three datasets as shown in Figure 3 There are four curves in each plot The one on the very top and the one on the very bottom are the upper and lower error bounds Thecurrent benefits and estimated total benefits are within the higher and lower error hounds Current benefits and estimated total benefits are very close especially when k becomes big As shown clearly in all three plots the error bound decreases exponentially When k exceeds 50 ap proximately 20 of 256 the error range is already within 5 of the total benefits of the complete model If we are satisfied with the accuracy of the current model we can dis Credit card Accuracy Based Cost-sensitive True Val I Estimate 11 True Val I Estimate I1 Donation I 94.94 I 94.94%fO I 14702.9 I 514913f612 88.64 89.0I%fI.Z 798943 3797749f4523 RmPER Accuracy Based Con-sensitive True Val I Estimate 11 True Val I Estimate n Donation 11 94.94 I 94.94%+0 01 of0 NB Accuracy Based Cost-sensitive True Val I Estimate 11 True Val  Estimate n dona ti 11 94.94 I 94.94%+0 11 14282 I 1438~+120 Table 3 True accuracy and estimated accu racy continue the learning process and return the current model For the three datasets under study and different number of panitions K when k  30 K the current model is usu ally within 5 error range of total benefits by the complete model For traditional accuracy, the current model is usu ally within 1  error bound of the accuracy by the complete model detailed results not shown Next we discuss an experiment under extreme situations When K becomes too big each dataset becomes trivial and will not he able to produce an effective model If the esti 168 


Figure 3 Current beneflts and estimated final benefits when sampling size k Increases up to K  256 for all three datesets. The error range Is 3 IK for 99.7 contldence ILI Im Y na 1 I Y rm Y na no lunpwS*.INmmnCi.m IO im In na 2s sb Wdd C.ul*\223l funm Sk.lNlunmolC.ul mation methods can effectively detect the inaccuracy of the complete model, the user can choose a smaller K We par titioned all three dataset into K  1024 partitions For the adult dataset, each partition contains only 32 examples but there are 15 attributes The estimation results are shown in Figure 4 The first observation is that the total benefits for donation and adult are much lower than the baseline This is obviously due to the trivial size of each data partition The total benefits for the credit card dataset is 750,000 which is still higher than the baseline of 733980 The second ob servation is that after the sampling size k exceeds around as small as 25 out of K  1024 or 0.5 the error bound becomes small enough implying that the total benefits by the complete model is very unlikely 99.7 confidence to increase At this point the user should cancel the learn ing for both donation and adult datasets The reason for the 223bumps\224 in the adult dataset plot is that each dataset is too small and most decision trees will always predict N most of the time At the beginning of the sampling, there is no variations or all the trees make the same predictions; when more trees are introduced it starts to have some diversities However the absolute value of the bumps are less than 50 as compared to 12435 3.5 Training Efficiency We recorded both the training time of the hatch mode single model plus the time to classify the test data and the training time of the multiple model with k  30 K clas sifiers plus the time to classify the test data k times We then computed ratio of the recorded time of the single and mul tiple models called serial improvement It is the number of limes that training the multiple model is faster than training the single model In Figure 5 we plot the serial improve ment for all three datasets using C4.5 as the base learner When K  256 using the multiple model not only provide higher accuracy but the training time is also 80 times faster for credit card 25 times faster for both adult and donation 4 Related Work Online aggregation has been well studied in database community It estimates the result of an aggregate query such as avg AGE during query processing One of the most noteworthy work is due to 7 which provides an in teractive and accurate method to estimate the result of ag gregation One of the earliest work to use data reduction techniques to scale up inductive learning is due to Chan I in which he builds a tree of classifiers In BOAT 6 Gehrke et al build multiple bootstrapped trees in memory to ex amine the splitting conditions of a coarse tree There has been several advances in cost-sensitive learning 3 Meta Cost 4 takes advantage of purposeful mis-labels to max imize total benefits In 181 Provost and Fawcett study the problem on how to make optimal decision when cost is not known precisely 5 Conclusion In this paper, we have demonstrated the need for a pro gressive and interactive approach of inductive learning where the users can have full control of the learning process An important feature is the ability to estimate the accuracy of complete model and remaining training time We have im plemented a progressive modeling framework based on av eraging ensembles and statistical techniques One impor tant result of this paper is the derivation of error bounds used in performance estimation We empirically evaluated our approaches using several inductive learning algorithms First, we find that the accuracy and training time by the pro gressive modeling framework maintain or greatly improve over batch mode learning Second the precision of estima tion is high The error hound is within 5 of the true value when the model is approximately 25  30 complete Based on our studies, we conclude that progressive mod eling based on ensemble of classifiers provide an effective 169 


Figure 4 Current benefits and estimated final estimates when sampling size k increases up to K  1024 for all three datasets To enlarge the plots when k is small we only plot up to k  50 The error range is 3 u\(uK for 99.7 confidence  Figure 5 Serial improvement jir4 for all three datasets when early stopping Is used 3 10 I 1 8 10 m s Irn 1m m zra m rn 1 m zra m Irn Im m ma WC4P.n WdP./Cn WdPb solution to the frustrating process of batch mode learning References 6 I Gehrke V Ganti R Ramakrishnan and W.-Y Loh BOAT-optimistic decision tree construction In Pro ceedings of ACM SICMOD International Conference on Management of Data SICMOD 1999 1999 7 J M Hellerstein P I Haas and H 1 Wang On line aggregation In Proceedings ofACM SIGMOD In ternationul Conference on Management of Data SIC I P Chan An Exrensible Meru-leurning Approach for Scalable and Accurare Inductive Learning PhD the sis Columbia University Oct 1996 21 W G Cochran Sampling Techniques John Wiley and Sons 1977 131 T Diettench D Margineatu E Provost, and P Tur ney editors Cost-Sensirive Learning Workshop ICML-00 2000 141 P Domingos MetaCost a general method for making classifiers cost-sensitive In Proceedings of Fifth In rernarional Conference on Knowledge Discovery and Dura Mining KDD-99 San Diego, California 1999 51 W Fan H Wang P S Yu and S Slolfo A framework for scalable cost-sensitive learning based on combin ing probabilities and benefits In Second SIAM In ternationul Conference on Datu Mining SDM2002 April 2002 MOD'97 1997 8 F Provost and T Fawcett Robust classification for imprecise environments Machine Learning 42203 23 I 2000 9 S Stolfo W Fan W Lee A Prodromidis and P Chan Credit card fraud detection using meta learning Issues and initial results In AAAI-97 Work shop on FraudDerecrion andRisk Management 1997 Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers In Proceedings of Eigk teenth International Conference on Machine Learning ICML'Z001 2001 IO B Zadrozny and C Elkan 170 


association-cube, base-cube and population-cube are derived from the volume cube; the confidence-cube is derived from the association cube and population cube and the support-cube is derived from the associationcube and base-cube. The slices of these cubes shown in Figure 2 correspond to the same list of values in dimension merchant, time, area and customer_group  Multidimensional and multilevel rules Representing association rules by cubes and underlying cubes by hierarchical dimensions, naturally supports multidimensional and multilevel rules. Also these rules are well organized and can be easily queried  First, the cells of an association cube with different dimension values are related to association rule instances in different scopes. In the association cube CrossSales cell CrossSales product \221A\222, product2 \221B\222  customer_group 221engineer\222, merchant \221Sears\222, area \221Los Angeles\222, time 221Jan98\222 represents the following multidimensional rule x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x,\221B\222  275 customer_group = \221engineer\222, merchant = \221Sears\222, area 221Los  Angeles\222, time =  \221Jan98\222 If this cell has value 4500, and the corresponding cell in the population cube has value 10000, then this rule has confidence 0.45 Next as the cubes representing rules can have hierarchical dimensions, they represent not only multidimensional but also multi-level association rules. For example, the following cells CrossSales\(product \221A\222, product2 \221B\222, customer_group 221engineer\222, merchant \221Sears\222, area \221 California 222, time 221Jan98\222 CrossSales\(product \221A\222, product2 \221B\222, customer_group 221engineer\222, merchant \221Sears\222, area \221 California 222, time 221 Year98 222 represent association rules at different area levels \(i.e the city level and the state level\d different time levels \(i.e., the month level, the year level x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222  275 customer_group = \221engineer\222, merchant =  \221Sears\222, area 221 California 222, time =  \221Jan98\222 x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222  275 customer_group = \221engineer\222, merchant =  \221Sears\222, area 221 California 222, time =  \221 Year98 222 The cell CrossSales\(product \221A\222, product2 \221B\222,  customer_group 221top\222, merchant \221top\222, area \221top\222,  time \221top\222 represents the customer-based cross-sale association rule for all customers, merchants, areas, and times in the given range of these dimensions, expressed as x 316 Customers: buy_product\(x, \221A\222 336 buy_product\(x, \221B\222 4.3  Generating Association Rule Related Cubes The basic task of our OLAP based association rule mining framework, either at the GDOS or at an LDOS is to convert a volume cube i.e. the cube representing the purchase volumes of customers dimensioned by product  area etc, into an association cube a base cube and a population cube These cubes are then used to derive the confidence cube and the support cube of multidimensional association rule instances. The following general steps are involved in cross-sale association rule mining 267  Roll up the volume cube SaleUnits by aggregating it along merchant, time, area dimensions 267  Derive cube NumOfBuyers from SaleUnits based on the antecedent condition SaleUnits 0 267  Populate cube NumOfShoppers by the counts of customers dimensioned by merchant, area  time not by product\at satisfy the antecedent conditions 267  Derive cube CrossSales from SaleUnits based on the association conditions SaleUnits  product  p 1  0 and SaleUnits  product2  p 2 0 267  Derive cube Confidence and cube Support using cell-wise operations 214  Confidence = CrossSales  NumOfBuyers 214  Support  CrossSales  NumOfShoppers  Cubes Confidence  Support  CrossSales are dimensioned by product  product2 customer_group,merchant  time, area NumOfBuyers is dimensioned by product  customer_group, merchant time, area  NumOfShoppers is dimensioned by customer_group, merchant  time, area Rules with confidence and support that exceed specified thresholds  may be considered interesting 4.4. Rules with Conjoint Items Cubes with conjoint dimensions can be used to represent refined multidimensional association rules For example, using OLAP, we can derive association rules across time  Time-variant or temporal association rules such as 


x 316 Customers buy_product\(x,\222 A\222, \221 Jan98\222  336 buy _product\(x, \221B\222, \221 Feb98\222   275 area = \221Los Angeles\222 can be used to answer such questions as \223 How are  the sales of B in Feb98  associated with the sales of A in Jan98 224 The items in this rule are value pairs of dimensions product and time In order to specify this kind of association rule we introduce a conjoint dimension product, time and mirror it with dimension product2, time2 This allows a cell in the association cube to cross two time values. Accordingly, the cubes related to association rule mining are defined as follows Association cube  CrossSales.2 \(<product, time>, <product2, time2 customer_group, merchant, area  Population cube  NumOfBuyers.2  \(<product, time>, customer_group merchant, area Base cube  NumOfShoppers.2  \( customer_group, merchant, area Confidence cube Confidence.2 \(<product, time>, <product2, time2 customer_group, merchant, area Support  cube  Support.2  product, time>, <product2, time2 customer_group, merchant, area  The steps for generating these cubes are similar to the ones described before. The major differences are that a cell is dimensioned by, besides others product, time and product2, time2 and the template of the association condition is  SaleUnit s  product p 1 time t 1  0 and  SaleUnits  product2 p 2 time2 t 2  0 where, in any instance of this condition, the time expressed by the value of time2 is not contained in the time expressed by the value of time The template of the antecedent condition is SaleUnits   product p 1 time t 1  0 In general, other dimensions such as area may be added to the conjoint dimensions to specify more refined rules 4.5. Functional Association Rules A multidimensional association rule is functional if its predicates include variables, and the variables in the consequent are functions of those in the antecedent.  For example, functional association rules can be used to answer the following questions, where a_month and a_year are variables q  What is the percentage of people in California who buy a printer in the next month after they bought a PC x 316 Customer buy_product\(x, \221PC\222, a_ month 336 buy_product\(x, \221printer\222, a_month+1  275 area = \221California\222 q  What is the percentage of people who buy a printer within the year when they bought a PC  x 316 Customer: buy_product\(x, \221PC\222, a_ year 336 buy_product\(x, \221printer\222, a_year 275 area = \221California\222 To be distinct, we call the association rules that are not functional as instance association rules; e.g x 316 Customer: buy_product\(x,\222 PC\222, \221Jan98\222 336 buy_product\(x,\222 printer\222, \221Feb98\222  275 area =  \221California\222 Time variant, functional association rules can be derived from time variant, instance association rules through cube restructuring. Let us introduce a new dimension time_delta that has values one_day, two_day 205, at the day level, and values one_month, two_month, \205, at the month level, etc. Then, let us consider the following functional association rule related cubes Association cube  CrossSales.3 \(product, product2, customer_group merchant, area, time_delta  Population cube  NumOfBuyers.3 \(product, customer_group, merchant area Base cube  NumOfShoppers.3 \( customer_group, merchant, area Confidence cube  Confidence.3 \(product, product2, customer_group merchant, area, time_delta Support cube  Support.3 \(product, product2, customer_group, merchant area, time_delta The association cube CrossSales.3  can be constructed from CrossSales.2   The cell values of CrossSales.2  in the selected time and time2 ranges are added to the corresponding cells of CrossSales.3 For example, the count value in cell  CrossSales.2\(<PC, Jan98>, <printer, Feb98>\205 is added to cell \(bin CrossSales.3\(PC, printer, one_month,\205 


It can also be added to cell CrossSales.3\(PC, printer one_year,\205 5  Distributed and Incremental Rule Mining There exist two ways to deal with association rules 267  Static that is, to extract a group of rules from a snapshot, or a history, of data and use "as is 267  Dynamic that is, to evolve rules from time to time using newly available data We mine association rules from an e-commerce data warehouse holding transaction data. The data flows in continuously and is processed daily Mining association rules dynamically has the following benefits 267  223Real-time\224 data mining, that is, the rules are drawn from the latest transactions for reflecting the current commercial trends 267  Multilevel knowledge abstraction, which requires summarizing multiple partial results. For example association rules on the month or year basis cannot be concluded from daily mining results. In fact multilevel mining is incremental in nature 267  For scalability, incremental and distributed mining has become a practical choice Figure 3: Distributed rule mining Incremental association rule mining requires combining partial results. It is easy to see that the confidence and support of multiple rules may not be combined directly. This is why we treat them as \223views\224 and only maintain the association cube, the population cube and the base cube that can be updated from each new copy of volume cube. Below, we discuss several cases to show how a GDOS can mine association rules by incorporating the partial results computed at LDOSs 267  The first case is to sum up volume-cubes generated at multiple LDOSs. Let C v,i be the volume-cube generated at LDOS i The volume-cube generated at the GDOS by combining the volume-cubes fed from these LDOSs is 345   n i i v v C C 1  The association rules are then generated at the GDOS from the centralized C v  214  The second case is to mine local rules with distinct bases at participating LDOSs, resulting in a local association cube C a,I a local population cube C p,I  and a local base cube C b,i at each LDOS. At the GDOS, multiple association cubes, population cubes and base cubes sent from the LDOSs are simply combined, resulting in a summarized association cube and a summarized population cube, as 345   n i i a a C C 1   345   n i i p p C C 1  and 345   n i i b b C C 1  The corresponding confidence cube and support cube can then be derived as described earlier. Cross-sale association rules generated from distinct customers belong to this case In general, it is inappropriate to directly combine association cubes that cover areas a 1 205, a k to cover a larger area a In the given example, this is because association cubes record counts of customers that satisfy   customer product merchant time area Doe TV Dept Store 98Q1 California Doe VCR Dept Store 98Q1 California customer product merchant time area Doe VCR Sears 5-Feb-98 San Francisco Joe PC OfficeMax 7-Feb-98 San Francisco customer product merchant time area Doe TV Fry's 3-Jan-98 San Jose Smith Radio Kmart 14-Jan-98 San Jose Association   population      base          confidence      support cube               cube                cube         cube                cube LDOS LDOS GDOS 


the association condition, and the sets of customers contained in a 1 205, a k are not mutually disjoint. This can be seen in the following examples 214  A customer who bought A and B in both San Jose and San Francisco which are covered by different LDOSs , contributes a count to the rule covering each city, but has only one count, not two, for the rule A  336  B covering California 214  A customer \(e.g. Doe in Figure 3\who bought a TV in San Jose, but a VCR in San Francisco, is not countable for the cross-sale association rule TV  336 VCR covering any of these cities, but countable for the rule covering California. This is illustrated in Figure 3 6  Conclusions In order to scale-up association rule mining in ecommerce, we have developed a distributed and cooperative data-warehouse/OLAP infrastructure. This infrastructure allows us to generate association rules with enhanced expressive power, by combining information of discrete commercial activities from different geographic areas, different merchants and over different time periods. In this paper we have introduced scoped association rules  association rules with conjoint items and functional association rules as useful extensions to association rules The proposed infrastructure has been designed and prototyped at HP Labs to support business intelligence applications in e-commerce. Our preliminary results validate the scalability and maintainability of this infrastructure, and the power of the enhanced multilevel and multidimensional association rules. In this paper we did not discuss privacy control in customer profiling However, we did address this issue in our design by incorporating support for the P3P protocol [1 i n  ou r data warehouse. We plan to integrate this framework with a commercial e-commerce system References 1  Sameet Agarwal, Rakesh Agrawal, Prasad Deshpande Ashish Gupta, Jeffrey F. Naughton, Raghu Ramakrishnan, Sunita Sarawagi, "On the Computation of Multidimensional Aggregates", 506-521, Proc. VLDB'96 1996 2  Surajit Chaudhuri and Umesh Dayal, \223An Overview of Data Warehousing and OLAP Technology\224, SIGMOD Record Vol \(26\ No \(1\ 1996 3  Qiming Chen, Umesh Dayal, Meichun Hsu 223 OLAPbased Scalable Profiling of Customer Behavior\224, Proc. Of 1 st International Conference on Data Warehousing and Knowledge Discovery \(DAWAK99\, 1999, Italy 4  Hector Garcia-Molina, Wilburt Labio, Jun Yang Expiring Data in a Warehouse", Proc. VLDB'98, 1998 5  J. Han, S. Chee, and J. Y. Chiang, "Issues for On-Line Analytical Mining of Data Warehouses", SIGMOD'98 Workshop on Research Issues on Data Mining and Knowledge Discovery \(DMKD'98\ , USA, 1998 6  J. Han, "OLAP Mining: An Integration of OLAP with Data Mining", Proc. IFIP Conference on Data Semantics DS-7\, Switzerland, 1997 7  Raymond T. Ng, Laks V.S. Lakshmanan, Jiawei Han Alex Pang, "Exploratory Mining and Pruning Optimizations of Constrained Associations Rules", Proc ACM-SIGMOD'98, 1998 8  Torben Bach Pedersen, Christian S. Jensen Multidimensional Data Modeling for Complex Data Proc. ICDE'99, 1999 9  Sunita Sarawagi, Shiby Thomas, Rakesh Agrawal Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications", Proc ACM-SIGMOD'98, 1998   Hannu Toivonen, "Sampling Large Databases for Association Rules", 134-145, Proc. VLDB'96, 1996   Dick Tsur, Jeffrey D. Ullman, Serge Abiteboul, Chris Clifton, Rajeev Motwani, Svetlozar Nestorov, Arnon Rosenthal, "Query Flocks: A Generalization of Association-Rule Mining" Proc. ACM-SIGMOD'98 1998   P3P Architecture Working Group, \223General Overview of the P3P Architecture\224, P3P-arch-971022 http://www.w3.org/TR/WD-P3P.arch.html 1997 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


